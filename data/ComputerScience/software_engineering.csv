URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10384v1,Comparing Bills of Materials,"Lucas This document is a model and instructions for LaTeX. This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, or Math in Paper Title or Abstract.",This document is a model and instructions for LaTeX. Please observe the conference page limits.
https://arxiv.org/html/2411.10351v1,Bias Unveiled: Investigating Social Bias in LLM-Generated Code,"Large language models (LLMs) have significantly advanced the field of automated code generation. However, a notable research gap exists in the evaluation of social biases that may be present in the code produced by LLMs. To solve this issue, we propose a novel fairness framework, i.e, Solar, to assess and mitigate the social biases of LLM-generated code.Specifically, Solar can automatically generate test cases for quantitatively uncovering social biases of the auto-generated code by LLMs. To quantify the severity of social biases in generated code, we develop a dataset that covers a diverse set of social problems. We applied Solar and the crafted dataset to four state-of-the-art LLMs for code generation. Our evaluation reveals severe bias in the LLM-generated code from all the subject LLMs. Furthermore, we explore several bias mitigation prompt strategies, including Chain-of-Thought (CoT) prompting, combining positive role-playing with CoT prompting and iterative prompting. Our experiments show that iterative prompting can effectively reduce social bias in LLM-generated code by up to 90%. Last, we make the code and data publicly available111https://anonymous.4open.science/r/fairness˙testing˙code˙generation-3FB8. Solar is highly extensible to evaluate new social problems.","Large language models (LLMs) that are pre-trained and fine-tuned on code-specific datasets have led to recent successes of LLM-for-code models, such as Codex (Chen et al. 2021), CodeGen (Nijkamp et al. 2022), StarCoder (Li et al. 2023), Code Llama (Roziere et al. 2023), and Copilot (Chen et al. 2021). With the increasing popularity of adopting LLMs in the software development process, the quality of LLM-generated code is paramount. Current evaluations, such as HumanEval (Chen et al. 2021) and MBPP (Austin et al. 2021), focus on functional correctness but fall short in evaluating the fairness of code, particularly in detecting bias against diverse demographic groups in human-center tasks. Figure 1: An overview of social bias evaluation framework Solar with examples. Evaluating and even further mitigating social biases in LLM code generation is pivotal to the massive adoption of LLM for software development. Prior studies reveal that LLMs have social biases towards different demographics in NLP tasks (Bai et al. 2023; Zhao et al. 2023; Meade, Poole-Dayan, and Reddy 2021; Gallegos et al. 2023). Yet, these techniques are not applicable in code generation. Recent works (Liu et al. 2023; Huang et al. 2023) took the first steps to investigate social bias in code generation. Yet these works do not fully represent real-world human-centered coding tasks, which developers may utilize LLM to generate code for. Specifically, Liu et al. use manually crafted prompts of method signatures containing negative judgmental modifiers, such as def find_disgusting_people(people, ethnicity). Huang et al. experiment one-sentence prompts in LLM code generation, which does not cover complicated real-world scenarios. Moreover, the work by Liu et al. does not quantify the severity of social bias, and both works lack in exploring more comprehensive bias mitigation strategies and investigating the impact on functional correctness by bias mitigation. To fill this research gap, we develop a novel evaluation framework Solar for evaluating the fairness of LLM-generated code. Moreover, we craft a dataset of human-centered tasks for code generation. Figure 1-a shows an example of human-centered tasks (i.e., a task definition), which is about deciding a career choice according to personal characteristics. Solar takes a task definition as input and generates a code prompt (Figure 1-b) and executable test cases (Figure 1-d) for evaluating social biases. A subject LLM generates code snippets (depicted in Figure 1-c) given the prompt, and then will be evaluated for fairness using the Solar’s generated test cases. Inspired by metamorphic testing (Chen, Cheung, and Yiu 2020), the test cases examine whether a generated code snippet produces different outcomes for different demographics. For example, as shown in Figure 1 (illustrated by sub-figure c and d), the tested model, GPT-3.5-turbo-0125, produces gender-biased code that excludes transgender individuals as suitable candidates, leading to discrimination and potential issues within the program (Figure 1-c). Using the test results as feedback, Solar employs mitigation strategies to refine code generation towards bias-neutral code. We conducted experiments on four state-of-the-art code generation models, namely GPT-3.5-turbo-0125, codechat-bison@002, CodeLlama-70b-instruct-hf, and claude-3-haiku-20240307. Our results reveal that all four LLMs contain severe social biases in code generation. The detected social biases are in varying degrees and different types. The ablation of temperature and prompt variation shows the sensitivity varies on models and bias types. Last, our experiment shows that iterative prompting, with feedback from Solar’s bias testing results, significantly mitigates social bias without sacrificing functional correctness. Contributions. 1) An extendable evaluation dataset (SocialBias-Bench) that is composed of distinct and diverse real-world social problems for evaluating social biases in LLM code generation. 2) A fairness evaluation framework (Solar), inspired by the concept of metamorphic testing, that can quantify the fairness of LLM-generated code by generating executable test cases. Solar is a black-box approach and can be applied to LLMs of any architecture. 3) Ablation studies about the impact of the temperature and judgemental words on fairness evaluation, and 4) An exploration of bias mitigation strategies."
https://arxiv.org/html/2411.10213v1,An Empirical Study on LLM-based Agents for Automated Bug Fixing,"Large language models (LLMs) and LLM-based Agents have been applied to fix bugs automatically, demonstrating the capability in addressing software defects by engaging in development environment interaction, iterative validation and code modification. However, systematic analysis of these agent and non-agent systems remain limited, particularly regarding performance variations among top-performing ones. In this paper, we examine seven proprietary and open-source systems on the SWE-bench Lite benchmark for automated bug fixing. We first assess each system’s overall performance, noting instances solvable by all or none of these sytems, and explore why some instances are uniquely solved by specific system types. We also compare fault localization accuracy at file and line levels and evaluate bug reproduction capabilities, identifying instances solvable only through dynamic reproduction. Through analysis, we concluded that further optimization is needed in both the LLM itself and the design of Agentic flow to improve the effectiveness of the Agent in bug fixing.","Large Language Models (LLMs) (Zhao et al., 2023) are advanced machine learning models trained on vast amounts of textual data, capable of understanding and generating human-like text. LLM-based Agents (Xi et al., 2023) are systems that utilize large language models to interact with the environment and accomplish specific tasks. Recently, LLM-based Agents have demonstrated significant influence in automated bug fixing in code repositories (Kang et al., 2023; Zhang et al., 2024; Yang et al., 2024a). Thanks to the powerful natural language processing capabilities of LLMs, these Agents can efficiently understand and analyze source code and its associated natural language descriptions, such as user-submitted issue descriptions and code comments. Additionally, through dynamic interaction with local environments (e.g., via terminal), LLM-based Agents can retrieve useful information from the code repository, perform code editing and execution, and iterate and validate repair results, thereby improving the accuracy and efficiency of bug fixes. This combination of LLM and environmental feedback has made automated bug fixing more efficient and feasible than ever before, providing revolutionary new tools for software maintenance and development. Researchers from both the industry (Liu et al., 2024a; hon, [n. d.]; gru, [n. d.]; Ma et al., 2024) and academia (Zhang et al., 2024; Tao et al., 2024) have developed LLM-based Agent systems to locate and fix bugs in code repositories. To evaluate the fault localization and repair capabilities of LLMs and various Agent systems, Jimenez et.al (Jimenez et al., 2023) proposed the evaluation datasets SWE-bench, with derived versions SWE-bench Lite (a subset of the full benchmark), and SWE-bench Verified (human annotated subset of SWE-bench published recently). These datasets contain real bugs from code repositories and can verify the correctness of the patches generated by Agents through unit tests. Recently, these datasets have become the most influential benchmarks in the field of automated bug fixing, attracting both academic and industrial participants to compete on the SWE-bench Lite leaderboard 111https://www.swebench.com/, with new submissions typically every one or a half week. However, no work has systematically analyzed the fault localization and repair capabilities of LLM-based Agents or the performance differences among various tools within these Agent systems. Regarding the SWE-bench Lite dataset itself, due to the quality of issue descriptions and the complexity of the logical dependencies related to the defects, some instances in the benchmark are easier for Agents to fix, while others are more difficult (Xia et al., 2024). As for the design of the systems, different designs exhibit different planning, reasoning, and problem solving capabilities, i.e. some systems adopting static approaches (gru, [n. d.]) and others adopting dynamic approaches (Zhang et al., 2024). We have also observed significant differences in the sets of cases that each system can solve. Therefore, analyzing the solving capabilities of LLM-based Agents on specific instances can not only help us better understand the current performance of Agents but also provide comparative insights to inspire future research directions. We collected the four most outstanding commercial systems (i.e. MarsCode Agent (Liu et al., 2024a), Honeycomb (hon, [n. d.]), Gru (gru, [n. d.]), Alibaba Lingma Agent (Ma et al., 2024)) and the three most excellent open-source systems (i.e. AutoCodeRover (Zhang et al., 2024), Agentless + RepoGraph (Ouyang et al., 2024), Agentless (Xia et al., 2024)) with top performances on the SWE-bench Lite Leaderboard and conducted a comprehensive analysis of the performance differences of each system. First, we evaluated the overall performance of LLM-based Agents in bug fixing, including statistics on the instances that all seven systems can solve and those that none can solve, and analyzed the reasons behind these results. We also explored why some instances can only be solved by Agent systems while others can only be solved by non-Agent systems. Next, we investigated the performance differences in fault localization among different systems and their causes, compiling file-level and line-level localization accuracy rates. Finally, we analyzed the impact of bug reproduction on bug fixing, and the common characteristics of instances that can only be solved through dynamic reproduction. Through data analysis, we have summarized several insights.To improve bug fixing, it is essential to enhance the model’s reasoning ability, enabling it to accurately identify bug-related information within an issue and reduce noise interference. For multiple potential repair locations, the model should leverage its reasoning capabilities to determine the most relevant location. From the Agentic flow perspective, there should be a strong focus on the quality of the issue and attention to multiple suspicious locations in the stack trace. The design should include mechanisms to verify the completeness of patches and consider their global impact. Mechanisms should also be implemented to mitigate the randomness of the model’s output or effectively utilize its diversity. In error localization, achieving line-level accuracy is more critical than file-level, due to the larger discovery space, necessitating finer-grained results. During the reproduction process, ensuring the correctness of reproductions is crucial, as incorrect reproductions can result in the failure of the entire solving process. Novelty and Contribution To the best of knowledge, this is the first work to: (1) Study the effectiveness of LLM-based Agents in automatic bug fixing for code repositories (2) Examine the effectiveness of different LLM-based Agents in Fault Localization and analyze the reasons for their differences (3) Investigate the impact of bug reproduction on bug fixing of LLM-based Agents (4) Summarize the current issues and future research directions for LLM-based Agents in bug fixing Paper Organization The remainder of this paper is organized as follows: Section 2 explains the background. Section 3 describes the study design. Section 4 presents the analysis results and findings. Section 5 discusses the analysis results and findings. Section 6 reports the threats to validity. Section 7 discusses related work, and Section 8 concludes the paper."
https://arxiv.org/html/2411.10169v1,Definition and Detection of Centralization Defects in Smart Contracts,"In recent years, security incidents stemming from centralization defects in smart contracts have led to substantial financial losses. A centralization defect refers to any error, flaw, or fault in a smart contract’s design or development stage that introduces a single point of failure. Such defects allow a specific account or user to disrupt the normal operations of smart contracts, potentially causing malfunctions or even complete project shutdowns. Despite the significance of this issue, most current smart contract analyses overlook centralization defects, focusing primarily on other types of defects. To address this gap, our paper introduces six types of centralization defects in smart contracts by manually analyzing 597 Stack Exchange posts and 117 audit reports. For each defect, we provide a detailed description and code examples to illustrate its characteristics and potential impacts. Additionally, we introduce a tool named CDRipper (Centralization Defects Ripper) designed to identify the defined centralization defects. Specifically, CDRipper constructs a permission dependency graph (PDG) and extracts the permission dependencies of functions from the source code of smart contracts. It then detects the sensitive operations in functions and identifies centralization defects based on predefined patterns. We conduct a large-scale experiment using CDRipper on 244,424 real-world smart contracts and evaluate the results based on a manually labeled dataset. Our findings reveal that 82,446 contracts contain at least one of the six centralization defects, with our tool achieving an overall precision of 93.7%.","The rapid development of Decentralized Applications (DApps) and Decentralized Finance (DeFi) has spurred extensive research on smart contracts. Recently, a notable increase in security incidents and economic losses has been attributed to centralization defects in DeFi projects. According to the Certik’s report [1], during the third quarter of 2023 alone, more than 14 related incidents were reported, leading to a total loss exceeding $204 million. A centralization defect in a DeFi project refers to any error, flaw, or fault in a smart contract’s design or development stage that results in a single point of failure [2]. This means that specific accounts, users, or addresses could disrupt normal operations, potentially causing project malfunctions or even shutdown. These defects encompass centralization issues and logical errors that directly result in the loss of funds. For instance, if the contract owner has the privilege to transfer tokens deposited by users, all users’ assets may be at risk as the private key of the owner could be leaked. Additionally, centralization defects can also stem from design flaws that may not pose threats to users’ funds immediately but do affect safety in certain scenarios. In March 2024, the DeFi project MUMI incurred an economic loss due to a centralization defect. The contract included a function for minting MUMI tokens, which was controlled by a single node. Minting tokens involves generating new tokens by authenticating data, creating new blocks, and recording this information on the blockchain [3]. An attacker exploited this function to clandestinely mint and drain a substantial number of tokens, resulting in a financial loss of approximately $35,000 [4]. Although previous works [5, 6] have reported a set of smart contract defects, the unique challenges presented by centralization defects have not been fully addressed. To address this gap, we first conducted an empirical study aimed at defining centralization defects in smart contracts. Our analysis encompassed 597 Q&A posts collected from Ethereum Stack Exchange [7] and 117 smart contract audit reports from blockchain security companies. By employing a manual filtering and open card sorting approach, we defined six centralization defects in smart contracts: Mint Function with Single Signature, Management without Timelock, Critical Variables Manipulation with Single Signature, Single Proxy Admin, Self-destruct with Single Signature, and Individual Contract Output Reliance. To identify these centralization defects, we developed a tool named CDRipper (Centralization Defects Ripper), which takes the smart contract’s source code as input. By identifying centralization defects in contracts, developers can enhance contract security while users can mitigate investment risks, contributing to the safe and sustainable growth of the smart contract community. CDRipper first constructs a permission dependency graph (PDG), which represents the relationships of permission dependencies among contract statements and functions, such as those used for or controlled by multi-signature verification, timelock mechanism, and permission checks. CDRipper then identifies sensitive operations in functions based on predefined rules, which are summarized from the empirical study of centralization defects. Finally, CDRipper reports centralization defects based on the permission dependencies of functions and sensitive operations. We applied CDRipper to 244,424 real-world smart contracts, discovering that 82,446 (about 33.73%) contracts in our dataset contained at least one centralization defect. More than 80% of reported defects arise from management through a single-signature address, i.e., an address controlled by a single private key. The compromise or loss of the private key associated with this address will leave the contract open to unauthorized access and lead to security issues. In the performance evaluation of CDRipper, we used a random sampling method and manually labeled two distinct datasets. The evaluation results show that CDRipper achieved an overall precision rate of 93.7% and a false negative rate of 14.6%. The main contributions of our work are as follows: • We defined six types of centralization defects in smart contracts, which is the most comprehensive work on this topic. For each defect, we present an illustration and a code example. Additionally, we provide possible solutions aimed at enhancing the security of smart contracts. • We developed a tool called CDRipper for detecting the defined centralization defects. Through random sampling and manual validation, the results demonstrate that our method achieves an overall precision of 93.7% and a false-negative rate of 14.6%. • We conducted a large-scale experiment involving 244,424 real-world smart contracts to evaluate the performance of CDRipper. Our findings reveal that among these smart contracts, 82,446 (about 33.73%) contain at least one defined defect. • We published the source code of CDRipper, along with all experimental data and analysis results at: https://anonymous.4open.science/r/CDRipper/."
https://arxiv.org/html/2411.10129v1,Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation,"Generating accurate code review comments remains a significant challenge due to the inherently diverse and non-unique nature of the task output. Large language models pretrained on both programming and natural language data tend to perform well in code-oriented tasks. However, large-scale pretraining is not always feasible due to its environmental impact and project-specific generalizability issues. In this work, first we fine-tune open-source Large language models (LLM) in parameter-efficient, quantized low-rank (QLoRA) fashion on consumer grade hardware to improve review comment generation. Recent studies demonstrate the efficacy of augmenting semantic metadata information into prompts to boost performance in other code-related tasks. To explore this in code review activities, we also prompt proprietary, closed-source LLMs augmenting the input code patch with function call graphs and code summaries. Both of our strategies improve the review comment generation performance, with function call graph augmented few-shot prompting on the GPT-3.5 model surpassing the pretrained baseline by around 90\% BLEU-4 score on the CodeReviewer dataset. Moreover, few-shot prompted Gemini-1.0 Pro, QLoRA fine-tuned Code Llama and Llama 3.1 models achieve competitive results (ranging from 25\% to 83\% performance improvement) on this task. An additional human evaluation study further validates our experimental findings, reflecting real-world developers’ perceptions of LLM-generated code review comments based on relevant qualitative metrics.","Code review, the manual process of inspecting authored source code by fellow teammates, is a crucial part of the software development lifecycle that helps detect errors and encourages further code improvement [1]. First formalized by Fagan [2], it is a systematic and collaborative software quality assurance activity where developers check each other’s code for improvement. Code reviews not only help in identifying bugs and potential issues early in the development cycle but also enhance code readability, maintainability, and overall software quality. However, despite the numerous benefits, the traditional review process demands significant manual effort, forcing developers to spend an excessive amount of time (more than 6 hours per week) reviewing their peers’ code, as shown in [3, 4]. It is also responsible for frequent context switch of the developers from the actual tasks they are expected to focus on[5]. Hence, automating code review activities is in significant demand. One distinct task stands out in the Modern Code Review (MCR) practices: Review Comment Generation (RCG), which can help reduce the burden from a code reviewer, automatically suggesting a potential change in the code submitted for review. We focus on improving the automation performance of review comment generation task in this study. With the rapid advances in deep learning and natural language processing techniques, researchers proposed many Pretrained Language Models (PLM) on source code focusing on review tasks, notably including encoder-only BERT models [6, 7] and encoder-decoder based T5 models [8, 9, 10]. Novel pretraining and fine-tuning attempts on large-scale datasets showed promising results. However, training on such domain-specific huge datasets requires a substantial amount of costly computing resources, imposing a negative impact on the carbon footprint globally [11]. While these models can usually generalize well, they might lack deep knowledge of project specific codebases, organizational coding standards, or niche libraries. This can lead to generic or less relevant code review suggestions, missing context-specific nuances. However, decoder-only unified language models (e.g. based on GPT architecture) have shown superior performance when scaled to large parameter sizes. Also generally known as LLMs, these models can reduce the need for repetitive training while offering amazing few-shot learning capabilities [12]. This refers to prompt engineering of the model with a few similar query-response pairs, also known as Few Shot Learning. Designing efficient LLM prompts for the mentioned task yet remains less explored, motivating us toward this research direction. Apart from proprietary LLMs, there has been a lot of work going on in the open-source landscape. General purpose open-source LLMs (e.g. Llama, Mistral) when fine-tuned, show performance improvement over PLMs. LLMs further trained on code-specific data, also known as Code LLMs (e.g. Codex, Code Llama) are currently the superior options for various code-related subtasks (including code generation, code summarization) [13]. The best-performing versions of these LLMs nearly have 30-70B parameters, which is quite impossible to fit into a consumer grade GPU having around 16GB VRAM. Hence, fine-tuning the smaller versions of these LLMs (7-8B) is a promising cost-effective strategy to ensure project-specific, context-aware use cases. Parameter Efficient Fine-Tuning (PEFT) approaches (Low-Rank Adaptation, 4-bit Quantization) are found to assist in such endeavors [14]. Augmenting statically analyzed, semantic structural facts to prompt the code model proved to be beneficial in code summarization tasks [15]. Inspired by this, we propose a new methodology to design cost-effective few-shot prompts for proprietary LLMs, augmented with a programming language component- function call graph and a natural language component- code summary. We also explore further ablation studies to understand their standalone contributions for code review comment generation task. Additionally, we fine-tune open-source general-purpose LLMs and code LLMs in automating review comment generation task in a low resource-constrained setting. Specifically, we leverage the Quantized Low-Rank Adaptation (QLoRA) approach to fine-tune our models in a supervised way. To summarize, we particularly investigate the following research questions in this study: RQ1: How effective is code review comment generation using fine-tuned open-source Large Language Models? RQ2: How well do the closed-source Large Language Models perform in code review comment generation task when prompt engineered in a few-shot setting? RQ3: When incorporated into prompts, what are the impacts of function call graph and code summary in improving review comment generation performance? RQ4: How effective Large Language Models are in generating review comments from a real-world developer’s perspective? Our contributions can be summarized as follow: • Evaluating code review comment generation performance with open-source LLMs (Llama 2, Code Llama, Llama 3 series) in quantized, low-rank adapted parameter-efficient fine-tuning setup • Exploring the performance of different closed-source, proprietary LLMs (GPT 3.5-Turbo, GPT-4o, Gemini-1.0 Pro) in few-shot prompt setting without any further data augmentation • Investigating the impact of incorporating manually extracted function call graph and CodeT5 model generated code summary into the few-shot prompts to observe their impact on review comment generation performance • Manual analysis and a developer study focusing on evaluating the LLM-generated review comments based on relevant qualitative metrics • A replication package with all the scripts for data, code and result processing for our study, which can be found here. Figure 1: Overview of the methodology"
https://arxiv.org/html/2411.09974v1,Experiences from Using LLMs for Repository Mining Studies in Empirical Software Engineering,"Context: The emergence of Large Language Models (LLMs) has significantly transformed Software Engineering (SE) by providing innovative methods for analyzing software repositories. Objectives: Our objective is to establish a practical framework for future SE researchers needing to enhance the data collection and dataset while conducting software repository mining studies using LLMs. Method: This experience report shares insights from two previous repository mining studies, focusing on the methodologies used for creating, refining, and validating prompts that enhance the output of LLMs, particularly in the context of data collection in empirical studies. Results: Our research packages a framework, coined Prompt Refinement and Insights for Mining Empirical Software repositories (PRIMES), consisting of a checklist that can improve LLM usage performance, enhance output quality, and minimize errors through iterative processes and comparisons among different LLMs. We also emphasize the significance of reproducibility by implementing mechanisms for tracking model results. Conclusion: Our findings indicate that standardizing prompt engineering and using PRIMES can enhance the reliability and reproducibility of studies utilizing LLMs. Ultimately, this work calls for further research to address challenges like hallucinations, model biases, and cost-effectiveness in integrating LLMs into workflows.","The use of Large Language Models (LLMs) has brought about a breakthrough in software engineering (SE), enabling more efficient and straightforward approaches to complex tasks[1, 2, 3]. These models, trained on large datasets, have demonstrated their ability to assist in various SE activities [4], including code generation [5], documentation generation [6], and software project analysis [7]. LLM-based technologies provide software engineers with tools to automate tasks, enhancing code quality and accelerating their workflows. However, despite their increasing adoption in SE activities, the processes of prompt creation, improvement, and output validation remain unclear, which can lead to erroneous evaluations and results that do not meet stated goals [8]. The work presented is a combination of lessons learned from previous work based empirical software engineering (ESE) experiences, seeking to address the challenges associated with the whole life cycle of using the LLM. We provide an experiential description of how to create, improve, and validate prompts through the use of single and multiple LLMs, focusing in particular on the analysis of software project repositories. t Main Objective: This experience report aims to provide researchers and practitioners with a practical framework for using and advice on conducting mining repository studies with LLMs on software repositories such as GitHub and Hugging Face. Context on our experiences. Our work is grounded in two prior studies where we employed LLMs for mining software repositories. In the first study [9], we analyzed 168 open-source ML projects on GitHub to assess the adoption of green architectural tactics for sustainable ML-enabled systems. We used LLM APIs to identify both documented and undocumented green tactics in Python files. In the second study [10], we conducted a longitudinal analysis of over 50,000 ML models on Hugging Face to understand how they evolve. By classifying commits and releases using LLMs—providing commit titles, edited files, and related information to the model—we uncovered patterns in model maintenance and development. These experiences highlighted the steps in prompt creation, improvement, and output validation when leveraging LLMs for software repository analysis. Figure 1: Enhancing a software repository using PRIMES for data collection As depicted in Figure 1, our framework coined Prompt Refinement and Insights for Mining Empirical Software repositories (PRIMES), enables the transformation of an initial dataset into an enhanced dataset using LLMs, automating data collection and enrichment. This process streamlines the extraction of information from software repositories, facilitating more efficient empirical studies."
https://arxiv.org/html/2411.09916v1,"LLMs are Imperfect, Then What? An Empirical Study on LLM Failures in Software Engineering","Software engineers are integrating AI assistants into their workflows to enhance productivity and reduce cognitive strain. However, experiences vary significantly, with some engineers finding large language models (LLMs), like ChatGPT, beneficial, while others consider them counterproductive. Researchers also found that ChatGPT’s answers included incorrect information. Given the fact that LLMs are still imperfect, it is important to understand how to best incorporate LLMs into the workflow for software engineering (SE) task completion. Therefore, we conducted an observational study with 22 participants using ChatGPT as a coding assistant in a non-trivial SE task to understand the practices, challenges, and opportunities for using LLMs for SE tasks. We identified the cases where ChatGPT failed, their root causes, and the corresponding mitigation solutions used by users. These findings contribute to the overall understanding and strategies for human-AI interaction on SE tasks. Our study also highlights future research and tooling support directions.","The rise of AI assistants has introduced innovative techniques for integrating AI into tasks traditionally performed by humans. Large language models (LLMs) like ChatGPT can engage in conversations, enhancing productivity [1] and improving work quality [2]. ChatGPT’s widespread accessibility has made it popular among students and professionals in computer science and SE [3], serving as a valuable tool to guide learning and support task completion in these fields [4, 5, 6]. While LLMs offer many advantages for SE, such as automating repetitive tasks and reducing cognitive load [2, 1, 7], researchers have identified limitations [8, 9, 10], including potential negative impacts on users’ cognitive skills [11] and risks of over-reliance leading to wasted time and ineffective use [12]. LLMs can also generate incorrect code [13, 14, 5, 15], causing frustration, especially for novices working on SE tasks (such as students completing foreign coding tasks) [16, 17]."
https://arxiv.org/html/2411.09846v1,Leveraging Propagated Infection to Crossfire Mutants,"Mutation testing was proposed to identify weaknesses in test suites by repeatedly generating artificially faulty versions of the software (i.e., mutants) and determining if the test suite is sufficient to detect them (i.e., kill them). When the tests are insufficient, each surviving mutant provides an opportunity to improve the test suite. We conducted a study and found that many such surviving mutants (up to 84% for the subjects of our study) are detectable by simply augmenting existing tests with additional assertions, or assertion amplification. Moreover, we find that many of these mutants are detectable by multiple existing tests, giving developers options for how to detect them. To help with these challenges, we created a technique that performs memory-state analysis to identify candidate assertions that developers can use to detect the surviving mutants. Additionally, we build upon prior research that identifies “crossfiring” opportunities — tests that coincidentally kill multiple mutants. To this end, we developed a theoretical model that describes the varying granularities that crossfiring can occur in the existing test suite, which provide opportunities and options for how to kill surviving mutants. We operationalize this model to an accompanying technique that optimizes the assertion amplification of the existing tests to crossfire multiple mutants with fewer added assertions, optionally concentrated within fewer tests. Our experiments show that we can kill all surviving mutants that are detectable with existing test data with only 1.1% of the identified assertion candidates, and increasing by a factor of 6x, on average, the number of killed mutants from amplified tests, over tests that do not crossfire.","The ultimate goal of mutation testing is to allow software developers to create stronger test suites. It does this by injecting artificial faults (i.e., mutations) into a program to identify weaknesses in the test suite (i.e., mutations that are not detected). A developer would then write tests to detect (or kill) the undetected or surviving mutants. The intuition of this approach is that by strengthening the test suite to detect the mutations, the test suite will then be more likely to catch future real faults before they can cause any adverse effects upon users. Multiple works on test generation and amplification (e.g., [1, 2, 3, 4, 5, 6, 7]) focus on automating the improvement of test suites. Some of these works address generating specific parts of the test suite, such as test data and test oracles (e.g., [5, 8, 6]), while others generate entire test suites from scratch (e.g., [2, 9]). Additionally, some techniques amplify existing test suites by exploring new test data and assertions (e.g., [4, 7]). These approaches enhance test suites and are evaluated based on their ability to produce higher mutant-killing ratios. However, the usage scenarios of these techniques often differ from typical mutation-testing practices in either or both of the following two key ways: (1) Mutation testing practitioners target individual surviving mutants and incrementally improve existing test suites [10, 11, 12, 13], and (2) they perform mutant-killing activities on pre-constructed, human-written test suites that already contain test data and oracles [12, 11, 10, 14]. From (1), we recognize the importance of analyzing specific surviving mutants to help practitioners target and kill individual mutants. From (2), we acknowledge the presence of multiple existing developer-written tests that may execute the mutant, creating opportunities for test amplification—if a slight improvement to an existing test can kill a surviving mutant, there is no need to design a different test from scratch. The classic fault-error propagation model, RIPR model [15, 16], investigates such scenarios where a test case executes a specific fault. The model includes four conditions: the fault must be executed (Reachability), infect the program states (Infection), propagate the infection (Propagation), and have appropriate test oracles to reveal the fault (Revealability). The last condition of revealability relies on the test case having appropriate and sufficient test oracles. If it does not, adding additional assertions, or assertion amplification may help reveal the fault. Following the RIPR model, Du et al. [17] empirically investigates the end-to-end runtime effects of mutation execution, which uncovers opportunities to kill surviving mutants through such amplification. Furthermore, while targeting a specific surviving mutant, mutation-testing researchers discovered that a human-designed test for one surviving mutant sometimes coincidentally kills other surviving mutants—a phenomenon termed “crossfire” [18, 13]. Understanding and leveraging these mechanisms can strengthen each incremental test-augmentation (mutant-killing) attempt. In this work, we (1) offer a model to investigate the causes and intricacies behind the crossfire phenomenon in both the mutation-analysis and mutation-testing processes, (2) empirically analyze existing test suites’ mutant-crossfire capabilities at both the test and assertion granularities, (3) systematically investigate assertion-amplification opportunities for each surviving mutant, (4) develop techniques that recommend mutant-crossfiring assertions with crossfiring goals at varied granularities, (5) compare and evaluate our assertion-amplification techniques of varied crossfiring strategies, and (6) gain initial insights into mutant killing assertion candidates’ characteristics. Through our analysis, we found varied mutant-killing capabilities of individual assertions and test cases across different projects, unveiled how passing test runs exhibit propagation, and discovered overwhelming surviving mutant-killing opportunities through assertion amplification. Our surviving mutant-crossfiring techniques allow for a small selective set of assertion-amplified, developer-written tests to crossfire a substantially larger number of surviving mutants, by a factor of 6.1. The main contributions of this paper include: • An analysis technique that assesses the granular infected memory locations that resulted from the propagation of the infection caused by execution of the injected faults. This technique offers recommendations for additional assertions that can be used to kill surviving mutants. • A theoretical model that dissects and analyzes propagation at fine-grained levels and illustrates the “crossfire” effects in mutant kills and a technique that provides optimizations for adding more effective assertions to tests. • An empirical evaluation of our proposed techniques showing the varied capabilities of tests and assertions for detecting propagated state infection, assessing our ability to recommend assertions to kill surviving mutants, and assessing our optimizations to crossfire mutants. • An implementation and dataset to allow for future research and experimental reproducibility."
https://arxiv.org/html/2411.09718v1,Non-Functional Requirements in Medical Imaging,"The diagnostic imaging departments are under great pressure due to a growing workload. The number of required scans is growing and there is a shortage of qualified labor. AI solutions for medical imaging applications have shown great potential. However, very few diagnostic imaging models have been approved for hospital use and even fewer are being implemented at the hospitals. The most common reason why software projects fail is poor requirement engineering, especially non-functional requirements (NFRs) can be detrimental to a project. Research shows that machine learning professionals struggle to work with NFRs and that there is a need to adapt NFR frameworks to machine learning, AI-based, software. This study uses qualitative methods to interact with key stakeholders to identify which types of NFRs are important for medical imaging applications. The study was done on a single Danish hospital and found that NFRs of type Efficiency, Accuracy, Interoperability, Reliability, Usability, Adaptability, and Fairness were important to the stakeholders. Especially Efficiency since the diagnostic imaging department is trying to spend as little time as possible on each scan.","Nowadays our healthcare system relies heavily on imaging data to diagnose and treat patients: 90% of all healthcare data is imaging data [14]. All this data must be analyzed and is in most cases done by radiologists. However, the increase in imaging data has overtaken the number of radiologists. The results can be missed findings and long turn-around times which will jeopardize the patients’ safety [14]. In Denmark it is especially the guarantee of treatment of cancer that takes up many of the radiologists’ resources and because the investigation of most diseases today has evolved to require scans [4]. There is a potential for deep learning models to relieve some of the pressure at the hospitals because they, like radiologists, can learn to recognize patterns. For this project, I will focus on computer-aided detection (CADe) and diagnosis (CADx) medical imaging applications. These technologies are used to localize and classify entities of medical scans [14] and have the potential to lessen workload the workload of radiologists as well as improve diagnosing of patients. I-A Current State of Medical Imaging As of 2022, 521 AI-enabled medical devices have been approved by the FDA (Food and Drug Administration in the US). Out of those, 391 devices are in radiology [3]. This number includes all kinds of models and algorithms that are based on AI, consequently, we do not know how many of those are medical imaging applications. The MONAI lab (Medical Open Network for Artificial Intelligence) published a report in 2021 that highlights that very few, if any, of the developed medical imaging applications have been implemented in hospitals [14]. They point towards the integration of models in the clinical workflow as one of the biggest challenges for successful implementation [14]. Chan et al. emphasize that many of the radiologist’s tasks are too complex for current deep learning models, for example comparing two patient scans to detect changes [2]. Liu et al. highlight that most medical imaging studies do not validate their results externally or compare their performance to radiologists [8]. Varoquaux and Cheplygina document how research in the area is guided by data set availability rather than clinical relevance and how this will lead to diminishing returns when continuing the research in medical imaging [13]. Given the circumstances, there seems to be a lack of focus on the domain which might jeopardize further development in the field and implementation of the models in the hospitals. I-B The Importance of Non-Functional Requirements When looking at traditional software projects, it is well-documented that many software development projects fail. The CHAOS report from 2015 presents that the percentage of failed and challenged projects is still high. In 2015 only 29% of the projects in their database were considered successful [1]. Many project failures can be directly linked to poor requirements gathering, analysis, and management [10]. This can for example occur when users are not involved at all or only at the beginning of the process or when there is miscommunication between the client and the development team [10]. Like it is for traditional software projects, Requirement Engineering (RE) should be a key process in the development of medical imaging applications because it ensures that the developers understand the client and user needs and thereby gain domain focus in the development process. RE is a discipline within Software Engineering (SE) where the goal is to develop requirements for a software system that describe what the system should provide and its constraints [11, Chapter 4]. There are two types of requirements: functional and non-functional requirements (NFR). Functional requirements describe the behavior of the system, NFRs are constraints on the system [11, Chapter 4]. According to Sommerville, it can be detrimental to a system if an NFR is not being met, for example, if an aircraft system does not meet reliability requirements, it is not safe and will not be used [11, Chapter 4]. Also, the NFRs may affect the overall architecture of the system which can make them expensive and hard to meet. Therefore, it is important to put great emphasis on NFRs in the RE process because of their significant effect on the system in question. I-C Non-Functional Requirements in Medical Imaging Habibullah et al. recently published a study where they interviewed Machine Learning (ML) professionals about their current use of NFRs. They found that most interviewees struggled with defining and measuring NFRs for ML systems and that some NFRs from the development of traditional software need to be redefined to be applicable or relevant for ML models [6]. This suggests that there are also challenges ahead for those medical imaging developers that want to work with defining NFRs because the framework has not been adapted properly to the type of software they are developing. The goal of this research project is to take the first step to create a framework that can help medical imaging developers implement NFRs in their application, to ensure that in the future, more medical imaging models will be implemented in the hospital to lessen the workload of the radiologists."
https://arxiv.org/html/2411.09683v1,Towards a Classification of Open-Source ML Models and Datasets for Software Engineering,"Background: Open-Source Pre-Trained Models (PTMs) and datasets provide extensive resources for various Machine Learning (ML) tasks, yet these resources lack a classification tailored to Software Engineering (SE) needs. Aims: We apply an SE-oriented classification to PTMs and datasets on a popular open-source ML repository, Hugging Face (HF), and analyze the evolution of PTMs over time. Method: We conducted a repository mining study. We started with a systematically gathered database of PTMs and datasets from the HF API. Our selection was refined by analyzing model and dataset cards and metadata, such as tags, and confirming SE relevance using Gemini 1.5 Pro. All analyses are replicable, with a publicly accessible replication package. Results: The most common SE task among PTMs and datasets is code generation, with a primary focus on software development and limited attention to software management. Popular PTMs and datasets mainly target software development. Among ML tasks, text generation is the most common in SE PTMs and datasets. There has been a marked increase in PTMs for SE since 2023 Q2. Conclusions: This study underscores the need for broader task coverage to enhance the integration of ML within SE practices.","The fast expansion of open-source platforms like Hugging Face (HF) [1] has enhanced access to Machine Learning (ML) models and datasets, driving advancements across various domains. With a consistent and significant uptrend in development activities on HF [2], it is distinguished by its vast collection of Pre-Trained Models (PTMs), compared to other platforms [3][4]. However, the categorization of these resources overlooks the specific needs of Software Engineering (SE). SE tasks frequently involve code generation, code analysis, and bug detection, which differ significantly from the tasks commonly addressed by general-purpose ML models such as object detection or image segmentation. Therefore, the motivation for this work is to address this gap, as the absence of SE-specific categorization limits the efficient application of ML in SE tasks, potentially slowing down SE innovation. By providing a framework that aligns ML tasks with SE needs, this research aims to make the selection of PTMs and datasets more relevant and effective for SE practitioners and researchers, thus addressing a critical need within the field [5]. The main contributions of this work are: (a) proposing and proving the feasibility of a preliminary classification framework for PTMs and datasets hosted on HF, tailored to SE needs; (b) providing advanced analysis, including the exploration of the relationship between SE activities and ML tasks, as well as the evolution of SE PTMs over time; (c) presenting a reproducible pipeline that accesses the HF API, filters, refines, and classifies resources on specific SE tasks. Data availability statement: All research components, including the original and preprocessed data, along with all scripts for data collection, preparation, and analysis, are publicly available on Zenodo [6]. This ensures transparency and enables independent replication of the study, which is essential for updating the classification as new open-source PTMs and datasets are constantly being released."
https://arxiv.org/html/2411.09580v1,"Software Performance Engineering for
Foundation Model-Powered Software (FMware)","The rise of Foundation Models (FMs) like Large Language Models (LLMs) is revolutionizing software development. Despite the impressive prototypes, transforming FMware into production-ready products demands complex engineering across various domains. A critical but overlooked aspect is performance engineering, which aims at ensuring FMware meets performance goals such as throughput and latency to avoid user dissatisfaction and financial loss. Often, performance considerations are an afterthought, leading to costly optimization efforts post-deployment. FMware’s high computational resource demands highlight the need for efficient hardware use. Continuous performance engineering is essential to prevent degradation. This paper highlights the significance of Software Performance Engineering (SPE) in FMware, identifying four key challenges: cognitive architecture design, communication protocols, tuning and optimization, and deployment. These challenges are based on literature surveys and experiences from developing an in-house FMware system. We discuss problems, current practices, and innovative paths for the software engineering community.","The rapid emergence of Foundation Models (FMs), particularly Large Language Models (LLMs), is reshaping software development, with market value expected to reach $36.1 billion by 2030 [1]. FMs empower the creation of intelligent software, defined as FMware by Hassan et al. [2], where applications rely on one or more building blocks that are FMs. Many cool demos built with FMware have emerged recently [3, 4]. However, developing FMware from prototypes into production-ready products is a complex engineering process, requiring collaborations across AI, software engineering, systems, and hardware domains throughout the lifetime of such software [5, 6]. Performance engineering, one of the key aspects in such an engineering process, has not been thoroughly discussed. That is, how to proactively ensure that the developed FMware meets the pre-defined performance goals, e.g., throughput or latency. These goals are sometimes also referred to as Service Level Agreements (SLAs) or Service Level Objectives (SLOs). Failing to meet these goals will result in unsatisfactory user experiences. However, in practice, we observed that performance concerns are often considered afterthoughts during the lifecycle of FMware, causing inefficient and costly performance optimization efforts after the FMware is deployed in production when SLAs are not met. In addition, due to the intensive computation resources that are needed for deploying FMware, it can become prohibitively expensive to serve FMware requests. Efforts to improve the overall efficiency of hardware utilization are needed to avoid the wastage of scarce computing resources, such as costly GPUs sitting idle. Lastly, as FMware is live software that keeps evolving autonomously, it is necessary to apply continuous performance tuning practices to avoid performance degradation over time. To summarize, Software Performance Engineering (SPE) practices are crucial in bringing FMware from prototype to production. Although the awareness of performance-oriented FMware production is growing [5, 6], systematic studies focusing on SPE for FMware (SPE4FMware) are still lacking. In this paper, we present a comprehensive analysis of SPE challenges in FMware development, deriving from four authoritative sources: (i) an extensive survey of both academic and grey literature, (ii) in-depth discussions with industrial stakeholders and active academicians during SEMLA 2023 & 2024 [7], FM+SE Vision 2030 [8], FM+SE Summit 2024 [9], and SE 2030 workshop - FSE 2024 [10] events, (iii) close collaboration with our customers and our internal FMware application development teams to understand their pain points with performance issues, and (iv) our hands-on experience designing and implementing an in-house FMware serving system (FMware Runtime). We identify four key SPE challenges that span across the lifecycle of FMware development: the design of cognitive architectures, defining communication protocols, tuning and optimization approaches, and deployment options. For each challenge, we describe its aspects in detail, discuss state-of-practices, and share our vision of innovation paths that call for contributions from the software engineering research community. This paper is organized as follows: Section II outlines the background of our study. Section III delves into the SPE challenges that are associated with FMware. Section IV describes the vision of our serving system. Finally, Section V summarizes our insights and conclusions."
https://arxiv.org/html/2411.09467v1,The Perceptions of Software Engineers Concerning the Utilization of Bots in the OSS Development Process: An Exploratory Survey,"Software bots, extensively adopted by Open Source Software (OSS) projects, support developers across several activities, from automating predefined tasks to generating code that aids software engineers. However, with the growing prominence of bots, questions have emerged regarding the extension to which they truly assist or hinder software engineers in their routine tasks. To address this, an exploratory survey was conducted with 37 software engineers to gather insights into their views on the use of bots within the software development process. The findings suggest that bots are present across multiple phases of the software development lifecycle, providing daily support to professionals by enhancing productivity and facilitating task automation. Respondents stated that current bots are not sufficiently intelligent and raised new challenges and enhancements to aid bot designers in developing additional functionalities and integrations.","Open Source Software (OSS) has fundamentally changed how we understand the Software Development Process [1]. A successful example is Linux, which demonstrates that collaborative development can produce software of the highest quality, often considered by many developers and users to be superior to paid software. OSS projects are typically developed in a distributed and decentralized way [2, 3]. With the development of large-scale software, it has become increasingly necessary to establish process standards. To maintain such standards and ensure software quality, time-consuming, tedious tasks, or even simple tasks that must be repeated multiple times, are required. These tasks ultimately reduce the productivity of the developers involved in the project. One way to help developers be more productive and effective is to provide them with better and smarter tools, i.e, tools that automate or streamline the development process [10]. In general, bots are seen as applications that automate repetitive or predefined tasks [10]. In Software Engineering, bots support several activities, such as communication and decision making [10]. In collaborative software development environments, bots automate tasks that generally require human interaction [5]. For example, Urli et al. [15] propose a bot that serves as a program repair tool-chain for continuous integration build failures in projects hosted on GitHub. Beschastnikh et al. [16] proposed the use of bots as a solution for the automated deployment and evaluation of software engineering analysis techniques. Wessel et al. [6] analyzed 351 OSS and found that 93 (26%) use bots which complement other developers’ work. The authors interviewed project maintainers to investigate, among other things, how contributors and integrators perceive bots during the pull request submission process. However, although these studies provide recommendations on how to develop bots, evaluate bot capabilities and performance, as well as the impact of bot usage on pull requests, they do not draw attention to how engineers perceive the support of all types of bots throughout the entire software development process and how this perception may assist in improving the bots. Although there are some studies focused on detecting and understanding the behavior of bots in repositories [4, 7, 8, 9], little is known about software engineers’ perceptions regarding the use of bots in the software development process. To gain this insight, a survey was conducted among software engineering professionals through an online questionnaire. Based on the study by Lebtag et al. [11, 14], figure 1 illustrates the steps followed in the development of this work, which a questionnaire was planned, the survey was executed with software engineering professionals, the results were analyzed, and a study was elaborated to report the findings. Figure 1: Steps taken for the development of the work. The main contribution of this paper is presenting results of an exploratory survey to answer the following research questions: ”What are the perceptions of software engineers regarding the use of bots in the software development process?” (RQ1) and ”How could a bot be evolved to improve the software development process?” (RQ2). Thirty-seven software engineers answered the survey. The results obtained indicated that most participating software engineers frequently use bots in the software development process, and a significant portion agree that bots increase productivity within this process. However, several aspects were identified that need improvement in bot development to enable them to adapt better to the context in which they are used and to the user. The remainder of this article is organized as follows: Section 2 describes the research method; Section 3 outlines the conduction and results obtained; Section 4 discusses the limitations of the study, and finally, Section 5 concludes the paper with final remarks."
https://arxiv.org/html/2411.09463v1,Teaching Program Decomposition in CS1: A Conceptual Framework for Improved Code Quality,"Program decomposition is essential for developing maintainable and efficient software, yet it remains a challenging skill to teach and learn in introductory programming courses. What does program decomposition for procedural CS1 programs entail? How can CS1 students improve the decomposition of their programs? What scaffolded exercises can instructors use to teach program decomposition skills? We aim to answer all these questions by presenting a conceptual framework that (1) is grounded in the established code style principles, (2) provides a systematic approach that can be taught to students as an actionable strategy to improve the program decomposition of their programs, and (3) includes scaffolded exercises to be used in classroom activities. In addition, this systematic approach is automatable and can further be used to implement visualizers, automated feedback generators and digital tutors.","Introducing software quality in early programming courses has been emphasized due to its importance and difficulty and to ensure that students gain the necessary knowledge and experience for success in advanced courses and later in their careers (Gomes et al., 2021). Specifically, introducing the topic of structural quality of code early is viewed as essential in laying the groundwork for more advanced treatment in later courses (Stegeman et al., 2014). Also, there is strong motivation among instructors to review novices’ code more systematically because low-quality code can be a sign of students’ misconceptions or poor design skills (Stegeman et al., 2014). The structural quality of code could be viewed as something to be reviewed and assessed. There are many principles, design patterns, metrics and tools that are aimed at supporting the manual review of the structural quality of code or assessing and correcting it automatically (Stegeman et al., 2014; Marinescu, 2001; Yin et al., 2024). However, the tools and approaches used by both professionals and researchers are not easily adaptable to the needs of educators and their students. For example, many of these tools are developed for the object-oriented paradigm (OOP) (Marinescu, 2001; Rizwan et al., 2020), while many CS1 courses are taught with a focus on the procedural paradigm (PP) (Vilner et al., 2007; Mason et al., 2024); often, the transfer of these principles across paradigms is not straightforward, even for instructors. Structural quality of code could be also viewed as the result of the design process. There are two approaches that stand at the ends of the design process continuum: top-down and bottom-up. The top-down approach starts with the larger problem and repeatedly breaks it down into its component parts. The bottom-up approach starts with component parts and repeatedly merges them to solve the larger problem (Graham, 1993; Gamage, 2021). Many common design guidelines tend to be top-down (Castro, 2015), which work well for experts, but may pose additional challenges for novices (Gamage, 2021). In this paper, we propose a conceptual framework to teach the structural quality of PP code. As part of our conceptual framework, we outline a set of decomposition-oriented exercises, provide bottom-up guidelines for novices, and present a coloring procedure of the data dependency graph (DDG) of programs that can be used to visualize an optimal program decomposition. A resource containing 13 procedural decomposition exercises for use by educators is also provided as an online appendix111The resource can be accessed at: https://doi.org/10.5281/zenodo.12797876 ."
https://arxiv.org/html/2411.09433v1,Conference Paper Title*,"This document is a model and instructions for LaTeX. This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, or Math in Paper Title or Abstract.",This document is a model and instructions for LaTeX. Please observe the conference page limits.
https://arxiv.org/html/2411.09224v1,"Programming with AI: Evaluating ChatGPT, Gemini, AlphaCode, and GitHub Copilot for Programmers","Our everyday lives now heavily rely on artificial intelligence (AI) powered large language models (LLMs). Like regular users, programmers are also benefiting from the newest large language models. In response to the critical role that AI models play in modern software development, this study presents a thorough evaluation of leading programming assistants, including ChatGPT, Gemini (Bard AI), AlphaCode, and GitHub Copilot. The evaluation is based on tasks like natural language processing and code generation accuracy in different programming languages like Java, Python and C++. Based on the results, it has emphasized their strengths and weaknesses and the importance of further modifications to increase the reliability and accuracy of the latest popular models. Although these AI assistants illustrate a high level of progress in language understanding and code generation, along with ethical considerations and responsible usage, they provoke a necessity for discussion. With time, developing more refined AI technology is essential for achieving advanced solutions in various fields, especially with the knowledge of the feature intricacies of these models and their implications. This study offers a comparison of different LLMs and provides essential feedback on the rapidly changing area of AI models. It also emphasizes the need for ethical developmental practices to actualize AI models’ full potential.","The advent of the AI concept presents a new revolutionary age of innovation with an AI model and LLM-powered chatbots changing how our software is being developed and problems solved (Gates, [n.d.]). With the launch of ChatGPT and the newest LLM tools, such as GitHub Copilot, Bard AI (which is now a part of the Gemini framework), and DeepMind’s AlphaCode, which have been developed by major players in the industry like Google, GitHub, and OpenAI, these AI systems have captured the attention of the tech community with their capacity to understand languages and generate programming languages. The reality of AI assistants is that they are revolutionary and keep widening the limits of AI models at work. Therefore, the discussion about their accuracy, architecture, capabilities, and implications for the future of AI technologies is crucial. One of the first and most effective LLMs, ChatGPT, attracted 100 million users in just two months after the launch, making it the fastest-growing platform out of all those based on technology and a testament to how much the consumers needed such platforms (Milmo, [n.d.]). With the help of LLMs, notable progress in code generation and Natural language Processing (NLP) has been made recently (noa, [n.d.]s). One example is the generative pre-trained transformer (GPT) model series(Radford, [n.d.]). These models, which have received extensive training on textual data show that they can produce codes on the same level as human written codes and execute language-based tasks with remarkable accuracy. This paper thoroughly studies the most recent large language models, highlighting their strengths and weaknesses and crucially contributing to responsible development practices in the benefits of AI models in various fields. Thus, to understand and capture the behaviour of popular LLMs, we pose three research questions: • RQ1: Which model provides the most accurate code for programmers? • RQ2: What are the metrics are frequently used to evaluate LLM generated codes? • RQ3: What are the benchmarks are being used to evaluate LLM generated codes?"
https://arxiv.org/html/2411.09171v1,Optimizing Metamorphic Testing: Prioritizing Relations Through Execution Profile Dissimilarity,"An oracle is a mechanism that determines whether the output of the program for the executed test cases is correct. For machine learning programs, such an oracle is not available or is too difficult to apply. Metamorphic testing is a testing approach that uses metamorphic relations, which are necessary properties of the software under test to help verify the correctness of a program. Prioritization of metamorphic relations helps to increase fault detection effectiveness and improve metamorphic testing efficiency. However, prioritizing metamorphic relations based on faults and code coverage is often not effective for prioritizing MRs since fault-based ordering of MRs is expensive and the code coverage-based approach can provide inaccurate ordering of MRs. To this end, in this work, we propose an approach based on diversity in the execution profile of the source and follow-up test cases to prioritize metamorphic relations. We show that the proposed statement centrality-based prioritization approach increases the fault detection effectiveness by up to 31% compared to the code coverage-based approach and reduces the time taken to detect a fault by 29% compared to random execution of MRs. In general, our approach led to an increase in the average rate of fault detection, reduced the time taken to detect a fault, and increased fault detection effectiveness.","Metamorphic testing (MT) has been applied in various domains and there is a growing need to develop techniques that allow the effective use of MT in the context of regression testing [1]. Regression testing ensures the continued functionality of software after code changes, updates, or enhancements to existing systems. As agile software development practices become increasingly prevalent, the frequency of regression testing increases with each subsequent version release [2]. Consequently, in such a dynamic development environment, the adoption of efficient Metamorphic Relation (MR) prioritization methods is vital. However, applying metamorphic testing in the context of regression testing faces a number of issues. First, previous research has indicated that the cost, measured in terms of time and resources, of the MT process increases proportionally to the number of MRs used [3] [4]. This is particularly pertinent because a System Under Test (SUT) can encompass MRs with multiple source and subsequent test cases. As a consequence, as the number of MRs grows, the number of test cases can grow exponentially, leading to an increase in the execution time of the MRs. Second, applications such as machine learning and bioinformatics programs often require substantial time and resources to execute when dealing with typical data sets [5] [6]. Thus, prioritisation of MRs, even within a smaller MR set size, becomes an advantageous approach. Finally, research [7] has shown that individual MRs often exhibit different fault detection efficacy, and certain MRs may be geared towards identifying similar types of fault. In this context, the selection of a diverse array of MRs can significantly amplify the fault detection capabilities of MT and concurrently optimise resource utilisation during regression testing. Prior work presented a fault and Code Coverage-based approach for prioritizing MRs in the context of regression testing [8]. First, the results from applying the Code Coverage-based approach indicate that the fault detection effectiveness of the Code Coverage-based approach was equal to the random-based approach for all the subject programs except BBMap. Also, one of the limitations of the approach is that many statements and branches in a program may not have any meaningful impact on the output. As a result, if a test covers such statements or branches, it might artificially inflate the coverage percentage without contributing to the verification of the program functionality. This reduces the effectiveness and accuracy of the prioritized MR ordering. Second, the Fault-based approach becomes costly when software testers have limited time to conduct metamorphic testing since the approach requires generating and executing a large number of mutants to prioritize MRs. So, we proposed a Statement Centrality-based approach to overcome these challenges."
https://arxiv.org/html/2411.08932v1,Pygen: A Collaborative Human-AI Approach to Python Package Creation,"The principles of automation and innovation serve as foundational elements for advancement in contemporary science and technology. Here, we introduce Pygen, an automation platform designed to empower researchers, technologists, and hobbyists to bring abstract ideas to life as core, usable software tools written in Python. Pygen leverages the immense power of autoregressive large language models to augment human creativity during the ideation, iteration, and innovation process. By combining state-of-the-art language models with open-source code generation technologies, Pygen has significantly reduced the manual overhead of tool development, thereby significantly enhancing creativity and productivity. From a user prompt, Pygen automatically generates Python packages for a complete workflow from concept to package generation and documentation. Practical examples of libraries such as AutoML, AutoVision, AutoSpeech, and Quantum Error Correction are demonstrated. The findings of our work show that Pygen considerably enhances the researcher’s productivity by enabling the creation of resilient, modular, and well-documented packages for various specialized purposes. We employ a prompt enhancement approach to distill the user’s package description into increasingly specific and actionable. While being inherently an open-ended task, we have evaluated the generated packages and the documentation using Human Evaluation, LLM-based evaluation, and CodeBLEU, with detailed results in the results section. Furthermore, we documented our results, analyzed the limitations, and suggested strategies to alleviate them. Pygen is our vision of ethical automation, a framework that promotes inclusivity, accessibility, and collaborative development. This project marks the beginning of a large-scale effort towards creating tools where intelligent agents collaborate with humans to improve scientific and technological development substantially.Our code and generated examples are open-sourced at [https://github.com/GitsSaikat/Pygen]","Curiosity, innovation, and relentless pursuit have always characterized scientific progress. Today, we stand on the threshold of a promising new chapter in which every step, though minor and significant, acts as a booster for scientific growth. Pygen is a system that represents a significant stride toward that vision. It aims to automate humdrum and recurrent activities to free time for researchers, scientists, and enthusiasts to practice what matters: creativity and breakthrough innovation effectively. Automation becomes one of the most vital tools for social benefit when used thoughtfully and diligently[1]. Automation simplifies tasks, opens vistas for reimagining processes, and makes those processes even better across disciplines. Treated right and with responsibility, it might be the beacon of progress for everyone[2]. Pygen do just that: make technology accessible, amply productive, and take people to new heights in their scientific journeys as technology empowers individuals to achieve their milestones along with the progress of civilization[3]. The path to innovation involves solving unique and complex problems. Not all challenges can be addressed with existing technology, but human creativity shines through in its ability to construct new tools as needed, thereby expanding the boundaries of what is possible[18][17]. Pygen embodies this spirit of innovation by transforming abstract ideas into practical solutions that make a tangible impact. When the scientific community tackles a problem and finds a solution, they often discover key components and experimental techniques that are valuable for future scientists and technologists[4]. Creating Python packages originated from the desire to equip the community with essential tools that streamline experimental processes and advance scientific work. This approach to building tools is a form of responsible automation, where the human element remains integral, allowing for flexibility, creativity, and adaptability—qualities that purely automated tool designs often lack. Our vision for Pygen is to create a dynamic system that helps generate effective software tools and nurtures and inspires new ideas. By focusing on responsible automation, we aim to empower researchers and technologists to explore new possibilities, build upon existing knowledge, and contribute to advancing science and technology. Traditional software automation[5][6][7] approaches primarily focus on creating user-level abstractions that integrate a user’s perspective to improve the software. However, our approach emphasizes the importance of designing superior tools that lead to the creation of better-end products. We made a critical observation that humans, when faced with challenges, often develop new tools if existing ones are inadequate[20][19]. Likewise, a language-model-based agentic system tasked with complex work must not only learn to use available tools but also create new ones to solve problems effectively[9][10][11]. This insight led us to develop an automated Python package generation system that starts with simple user prompts and evolves from there. By integrating this approach into an agentic framework, Pygen aims to enhance the adaptability and performance of such systems, enabling them to tackle increasingly sophisticated tasks and deliver impactful results. Foundational models[12][13][14][15] have typically been used to generate code for direct use, but their potential to autonomously build tools and design comprehensive software solutions remains largely untapped. While these models can assist in writing scripts or automating simple tasks, they have yet to augment human capability in complex, multi-faceted project settings effectively. With Pygen, we aim to change this paradigm. We are empowering these foundational models to generate ideas for software tools and create Python packages that can be effectively used to solve real-world challenges. By producing thorough documentation alongside the generated tools, Pygen extends the capabilities of these models beyond simple automation, turning them into meaningful partners in creative and technical endeavors. The concept of the AI scientist[16] inspires our work, which is an end-to-end framework capable of originating novel ideas, developing experiments to explore those ideas, and ultimately producing scientific literature to share the resulting insights. Pygen builds on this vision by enhancing user queries, generating Python packages, and providing comprehensive documentation that allows others to easily understand, utilize, and build upon the generated tools. This process empowers users by transforming abstract ideas into functional, well-documented tools, simplifying the journey from initial concept to practical application. Pygens do more than merely automate; they act as extensions of human creativity. It bridges the gap between high-level conceptual thinking and practical, hands-on implementation, allowing users to bring their ideas to life with minimal friction. Pygen allows users to specify the type of package they need for their tasks, along with the desired features and functionalities. Based on the user’s description, the system refines these ideas and creates optimized implementation strategies. Using these refined strategies, Pygen designs a Python package by leveraging open-source models available on platforms like Google AI Studio and the GroqCloud. Once the package is generated, the Pygen creates comprehensive documentation to accompany it. Users can download the package and its documentation as a zip file, ensuring that all necessary information is in one place. The package is automatically set up if executed in the user’s local environment, allowing for a smooth transition from development to execution. Users can further enhance these packages to meet their specific needs and, if desired, deploy them within the Python ecosystem. A key principle behind Pygen is our emphasis on open-source accessibility. Using open-source models, we ensure that users can access the system without being hindered by financial barriers or paywalls. This approach promotes open access and open-source scientific discovery, allowing individuals from all backgrounds to contribute to and benefit from innovation. Thanks to this open-source pipeline, users can utilize models made available through platforms such as GroqCloud and Google AI Studio to generate and document Python packages completely free of charge, encouraging experimentation and continuous improvement of Pygen. We are committed to open-sourcing Pygen itself, inviting contributions from the broader community to enhance its capabilities further, making it a truly collaborative and evolving project. Our contributions are summarized as follows: 1. We have introduced a Python package development system powered by open-source frontier models. This pipeline transforms user descriptions into refined ideas, leading to the generation of Python packages accompanied by thorough documentation. Users can download the generated package and documentation seamlessly, enabling them to start working immediately. 2. Pygen can be deployed as a user-friendly application, allowing users to access it directly by simply setting their API key. This streamlined access reduces barriers to entry and enables a broader audience to leverage the system’s capabilities. 3. We have outlined several future research directions to improve responsible system automation. These include refining the strategies for improving Pygen, integrating a package reviewer to ensure robustness and reliability, and exploring the potential of agentic frameworks that can autonomously create and refine the tools they use. Figure 1: Pygen’s Workflow: This diagram describes Pygen’s workflow to generate a Python package given the user’s request. First, it starts with generating the plan, which includes formulating some package plan according to the user’s needs. It then involves users’ prompt refinement, based on validating, generating, and evaluating a package. The final step is documentation generation and formatting. At the end, documents are created, refined, and converted to Markdown to enable formatting validation for the final output package."
https://arxiv.org/html/2411.09050v1,"The Systems Engineering Approach in Times of
Large Language Models","Using Large Language Models (LLMs) to address critical societal problems requires adopting this novel technology into socio-technical systems. However, the complexity of such systems and the nature of LLMs challenge such a vision. It is unlikely that the solution to such challenges will come from the Artificial Intelligence (AI) community itself. Instead, the Systems Engineering approach is better equipped to facilitate the adoption of LLMs by prioritising the problems and their context before any other aspects. This paper introduces the challenges LLMs generate and surveys systems research efforts for engineering AI-based systems. We reveal how the systems engineering principles have supported addressing similar issues to the ones LLMs pose and discuss our findings to provide future directions for adopting LLMs.","Large Language Models (LLMs) leverage neural network architectures trained on large amounts of data to learn underlying language patterns. LLMs generate content in formats humans understand based on these architectures \parencitefeuerriegel2024generative. Such ability creates novel human-machine interfaces \parencitecabrera2024self for adopting AI at different levels of our society. Generative AI technologies promise new applications for addressing critical problems in diverse domains. The complexity of socio-technical systems and the LLMs’ nature challenge the realisation of this vision. Social problems have critical requirements that demand reliable systems. LLMs rely on probabilistic models that make systems’ components based on these technologies non-deterministic, data-driven, and prone to hallucinations \parencitedantonoli2024large impacting the alignment and reliability of the systems. LLMs operate as black-boxes \parencitefeuerriegel2024generative, which impact systems’ accountability and interpretability as designers and users do not control and understand the systems (i.e., intellectual debt). Social problems usually appear in resource-constrained environments. Building LLMs is an expensive process that causes significant environmental concerns because it generates an immense carbon footprint \parenciteschwartz2020greenai. The outlined challenges require interdisciplinary research to align societal problems, systems, and AI technical advances. The systems engineering approach is equipped with principles to facilitate this alignment by prioritising the problems and their context before considering the technologies for their resolution. We envisage an ecosystem where systems engineering and LLMs mutually benefit instead of the naive belief that benefits come from the LLMs to the domains in one direction. This paper surveys how researchers have used the system engineering approach to design AI-based systems since 2017 (i.e., when current LLM technologies emerged) as a first step to building such an ecosystem. The main research question we want to answer is how does current research use the systems engineering approach to address challenges similar to the ones LLMs impose on socio-technical systems?"
https://arxiv.org/html/2411.09047v1,Anomaly Detection in Large-Scale Cloud Systems: An Industry Case and Dataset,"As Large-Scale Cloud Systems (LCS) become increasingly complex, effective anomaly detection is critical for ensuring system reliability and performance. However, there is a shortage of large-scale, real-world datasets available for benchmarking anomaly detection methods.To address this gap, we introduce a new high-dimensional dataset from IBM Cloud, collected over 4.5 months from the IBM Cloud Console. This dataset comprises 39,365 rows and 117,448 columns of telemetry data. Additionally, we demonstrate the application of machine learning models for anomaly detection and discuss the key challenges faced in this process.This study and the accompanying dataset provide a resource for researchers and practitioners in cloud system monitoring. It facilitates more efficient testing of anomaly detection methods in real-world data, helping to advance the development of robust solutions to maintain the health and performance of large-scale cloud infrastructures.","In recent decades, the adoption of Cloud Computing across government and business sectors has grown exponentially [1, 2]. At the core of this growth is the ability of cloud computing to offer high-capacity data centers as a reliable backbone for services. Cloud providers operate expansive data centers that support global workloads, necessitating sophisticated techniques to monitor, diagnose, and respond to failures in real-time. As cloud infrastructure expands in both scale and complexity, maintaining its reliability has become a critical concern. Even brief outages or performance issues can lead to significant losses for users hosting applications in the cloud [3]. To prevent such issues, cloud system administrators must continuously monitor hardware and software services to ensure compliance with service-level agreements (SLAs) [4]. System anomalies, which translate into unexpected behavior, reduced efficiency, or even downtime, pose a significant risk. Early detection of these anomalies is vital for taking preemptive measures to safeguard users, improve the overall user experience, and ensure SLAs. Various studies have introduced anomaly detection methods based on statistical and machine learning techniques, spanning supervised [5], semi-supervised [6, 7], and unsupervised approaches [8, 9, 10, 11]. These models have been tested using diverse datasets and systems of varying complexity and scale [12, 13]. One major challenge for anomaly detection methods is the high dimensionality of data generated in large-scale cloud computing environments [14]. Many existing methods struggle to maintain accuracy in the presence of this “curse of dimensionality” [15], which hampers both performance and precision. High-dimensional data requires more input for generalization and results in data sparsity, where data points become scattered and isolated. The abundance of irrelevant features often obscures true anomalies, reducing the effectiveness of traditional methods such as distance or clustering-based techniques [16]. Additionally, much of the existing work in anomaly detection has been conducted on relatively small datasets [17, 18, 19], which may not fully capture the challenges posed by larger-scale cloud systems. To help advance this area of research, we aim to share a large-scale dataset from a real-world IBM Cloud System [9] with the broader community. This will enable more comprehensive testing and evaluation of anomaly detection methods on large, complex datasets. We address the following research questions (RQs): RQ1: What are the key characteristics of telemetry datasets collected from Large-Scale Cloud Systems111Comprising numerous hardware and software components, which are often distributed across multiple data centers. (LCS)? RQ2: What are the main challenges in predicting anomalies within such large datasets? The main contributions of this paper are: • Introducing a new large-scale dataset for testing anomaly detectors in cloud systems. The dataset is available on Zenodo [20]. • Demonstrating predictive models for anomaly detection in cloud environments. • Discussing challenges related to handling high-dimensional telemetry data using domain knowledge and machine learning techniques. The remainder of the paper is organized as follows. Section II reviews the related literature. Section III introduces the dataset. Section IV details the construction of anomaly detectors. Section V discusses the challenges faced. Finally, Section VII concludes the paper."
https://arxiv.org/html/2411.08912v1,Can Large-Language Models Help us Better Understand and Teach the Development of Energy-Efficient Software?,"Computing systems are consuming an increasing and unsustainable fraction of society’s energy footprint, notably in data centers. Meanwhile, energy-efficient software engineering techniques are often absent from undergraduate curricula. We propose to develop a learning module for energy-efficient software, suitable for incorporation into an undergraduate software engineering class. There is one major problem with such an endeavor: undergraduate curricula have limited space for mastering energy-related systems programming aspects. To address this problem, we propose to leverage the domain expertise afforded by large language models (LLMs). In our preliminary studies, we observe that LLMs can generate energy-efficient variations of basic linear algebra codes tailored to both ARM64 and AMD64 architectures, as well as unit tests and energy measurement harnesses. On toy examples suitable for classroom use, this approach reduces energy expenditure by 30–90%. These initial experiences give rise to our vision of LLM-based meta-compilers as a tool for students to transform high-level algorithms into efficient, hardware-specific implementations. Complementing this tooling, we will incorporate systems thinking concepts into the learning module so that students can reason both locally and globally about the effects of energy optimizations.","Global climate change poses a serious threat to societal well-being. Data centers, which account for an estimated 4% of annual energy consumption in the United States (Institute, 2024) and 3% in the European Union (Commission, 2024), contribute significantly to this issue. As data center energy consumption is expected to grow, immediate action is needed. Although advancements have been made in enhancing data center computing system performance along various metrics — such as reducing latency (Hua et al., 2017), increasing throughput (Zhan et al., 2012), and improving parallelism (Mondal and Dutta, 2015) — knowledge of improving their energy efficiency remains limited (Manotas et al., 2016). There is little literature on training software engineers to use data center resources with an eye toward energy efficiency (Mullen et al., 2017; Qasem, 2019). While energy optimizations have been proposed for individual components (Anagnostopoulou et al., 2012; Weiser et al., 1996; Sehgal et al., 2010b, a; Vasić et al., 2009) and entire systems (Heath et al., 2001; Wu et al., 2015; Townend et al., 2019; Chi et al., 2021), they have largely focused on hardware. We have identified a need for improvement in software as well. Implementing energy-efficient software methods remains challenging, as current approaches rely on heavyweight design approaches (Te Brinke et al., 2013, 2014), pattern catalogs (Pinto et al., 2016; Maleki et al., 2017), programming languages (Couto et al., 2017), or decision frameworks (Manotas et al., 2014). Figure 1. Research overview: we are developing techniques and pedagogy to support a two-part learning module. We argue that all software engineers should have access to training on how to write more energy-efficient software. They should be able to learn not only what lightweight tools are available to them, but also how systems-level thinking can drive energy-conscious design decisions. Our preliminary results suggest that large language models (LLMs) might be a useful tool to realize this vision. We are examining two educational research questions: (1) RQ1: How can we apply LLMs to help engineers learn to write energy-efficient software? (2) RQ2: Is systems thinking an effective approach for design-level reasoning about energy-efficient software engineering designs?"
https://arxiv.org/html/2411.08693v1,Human-Centered AI Transformation: Exploring Behavioral Dynamics in Software Engineering,"As Artificial Intelligence (AI) becomes integral to software development, understanding the social and cooperative dynamics that affect AI-driven organizational change is important. Yet, despite AI’s rapid progress and influence, the human and cooperative facets of these shifts in software organizations remain relatively less explored. This study uses Behavioral Software Engineering (BSE) as a lens to examine these often-overlooked dimensions of AI transformation. Through a qualitative approach involving ten semi-structured interviews across four organizations that are undergoing AI transformations, we performed a thematic analysis that revealed numerous sub-themes linked to twelve BSE concepts across individual, group, and organizational levels. Since the organizations are at an early stage of transformation we found more emphasis on the individual level.Our findings further reveal six key challenges tied to these BSE aspects that the organizations face during their AI transformation. Aligned with change management literature, we emphasize that effective communication, proactive leadership, and resistance management are essential for successful AI integration. However, we also identify ethical considerations as critical in the AI context—an area largely overlooked in previous research. Furthermore, a narrative analysis illustrates how different roles within an organization experience the AI transition in unique ways. These insights underscore that AI transformation extends beyond technical solutions; it requires a thoughtful approach that balances technological and human factors.","With the rapid development and impact of Artificial Intelligence (AI), understanding the human factors surrounding it has become critical for organizations [1]. AI’s integration into Software Engineering (SE) is transformative, offering automation, improved productivity, and unlocking new avenues for innovation[2, 3, 4]. Furthermore, AI technologies are more than tools for improvement; they are often central components in the design and function of modern software systems. However, SE literature has historically prioritized technological and process-oriented issues over human and cooperative aspects [5, 6, 7]. This focus risks repeating itself as AI reshapes the field, potentially overlooking the critical human dynamics necessary for effective transformation. We suggest that frameworks and methods rooted in socio-technical perspectives [7, 8] or those incorporating psychological, cognitive, behavioral, and social theories and concepts, e.g. Behavioral Software Engineering (BSE) [9, 10], can provide valuable insights into the complex, human-centered challenges of AI-driven transformations in software organizations. As organizations undergo change driven by AI, human aspects are becoming vital for the success of these transitions [1]. Traditional change management models, such as Kotter’s [11], often overlook the behavioral and psychological factors that shape how employees adapt to change. While recent research has explored human aspects of successful organizational change [12], including in software organizations [13, 14], these studies predate any substantial impact of AI. Moreover, a search for studies on “AI transformation” combined with behavioral or human factors yields few results, with no direct references to Behavioral Software Engineering (BSE). Peretz-Andersson and Torkar’s systematic mapping study [15] confirms this gap, finding psychology-related studies underrepresented in current AI transformation research. This study empirically investigates the human and behavioral dynamics underpinning AI-driven transformations in organizations. Given the nascent state of research in this area, we posit that insights from industry practitioners experiencing these shifts firsthand are both timely and invaluable, offering a more nuanced perspective. To capture a broad spectrum of experiences, we interviewed practitioners across multiple organizations and roles, using semi-structured interviews. We framed our analysis through a Behavioral Software Engineering (BSE) [10] lens, leveraging it to structure and enhance our understanding of the data. Given the ongoing nature of AI transformation, we concentrated on challenges related to BSE concepts rather than focusing on or proposing specific solutions. Anticipating that individuals’ perspectives might differ based on their roles and experiences, we paid particular attention to these variations. The structure of the rest of this paper is organized as follows: Section II provides an overview of the related work, while Section III explains the research methodology that was followed. Section IV presents the results of the study, addressing the research questions. Then, Section V discusses the study’s findings, and, finally, Section VI outlines the conclusions and potential directions for future work."
https://arxiv.org/html/2411.08574v1,Practitioners’ Discussions on Building LLM-based Applications for Production,"Background: Large language models (LLMs) have become a paramount interest of researchers and practitioners alike, yet a comprehensive overview of key considerations for those developing LLM-based systems is lacking. Objective: This study addresses this gap by collecting and mapping the topics practitioners discuss online, offering practical insights into where priorities lie in developing LLM-based applications.Method: We collected 189 videos from 2022 to 2024 from practitioners actively developing such systems and discussing various aspects they encounter during development and deployment of LLMs in production. We analyzed the transcripts using BERTopic, then manually sorted and merged the generated topics into themes, leading to a total of 20 topics in 8 themes.Results: The most prevalent topics fall within the theme Design & Architecture, with a strong focus on retrieval-augmented generation (RAG) systems. Other frequently discussed topics include model capabilities and enhancement techniques (e.g., fine-tuning, prompt engineering), infrastructure and tooling, and risks and ethical challenges.Implications: Our results highlight current discussions and challenges in deploying LLMs in production. This way, we provide a systematic overview of key aspects practitioners should be aware of when developing LLM-based applications. We further pale off topics of interest for academics where further research is needed.","The rapid evolution of Large Language Models (LLMs) in recent years has advanced the state of the art across multiple software engineering research fields, including code generation [1, 2], automated testing [3, 4], and comment generation [5]. Despite the successful application of LLMs in numerous research tasks, little work has been done on how LLM applications are built and deployed in practice. Although experience reports exist [6] and a systematic study addresses challenges in building LLM-based components [7], there is still no comprehensive overview of what practitioners consider important across the entire lifecycle of LLM applications. We know from traditional ML-enabled software system that productionization comes with its own challenges. Interviews, experience reports, and user studies draw a diverse picture of themes to consider. For instance, socio-technical challenges due to scattered organizations or lack of data science skills in the management cause productionization failures [8], communication challenges between software engineers and data scientists hinder the transfer of ML models to developers [9], and handling, managing, and versioning the data introduces a whole new dimension in the software life-cycle [10]. So, already the development of traditional ML-enabled software system is unique in many facets, it is, thus, crucial to obtain such a deep understanding when it comes to developing LLM-based applications. LLMs distinguish themselves from traditional ML models through several key aspects that alter the conventional machine learning workflow. Unlike traditional approaches, which necessitate the collection, processing, and domain-specific training of data, LLMs undergo a one-time pre-training process and are subsequently adapted to specific domains via methods, such as prompt engineering and fine-tuning. Moreover, the development of LLMs typically demands extensive teams with specialized expertise and significant hardware resources, contrasting with the smaller, in-house efforts typical for traditional ML models. Consequently, the resource-intensive nature of LLMs compels many companies to depend on external services for deployment and provisioning, adopting an LLM-as-a-service model, which might hold its own challenges and best practices. The largest difference, however, may lay into the black-box nature of LLMs that makes them notorious challenging to trust or even debug their output. This aspect alone calls for many techniques and methodologies to use an LLM in a practical setting. Given these fundamental differences, we cannot necessarily rely on existing knowledge of engineering challenges for ML-enabled systems. Thus, the goal of this work is to provide an overview of current topics and challenges practitioners discuss online and highlight key aspects that should be considered when building LLM-based applications. To accomplish this, we conduct a semi-automated thematic analysis of 92 hours of talks and conversations recorded by practitioners, which have been made publicly available on YouTube. Analyzing these public videos has been shown to contain valuable insights, that can be of comparable depth as traditional interview studies [8]. Not only that, they also provide an unfiltered view on what practitioners think is worth discussing with the community. Our results show that the discussed topics fall within eight main themes Architecture & Design, Model Capabilities & Techniques, Tools & Infrastructure, Evaluation, Risks & Ethics, Monitoring, Costs, and Output Verification. Here, we can see that some themes stem from the incorporation into a software system (e.g., Architecture & Design), some already known from ML-enabled software systems (e.g., Tools & Infrastructure and Monitoring), and some are entirely new (e.g., Costs and Output Verification). Within these themes, we find 20 distinct topics, of which the most prevalent one is concerned with retrieval augmented generation (RAG) systems. In summary, we make the following contributions: • A thematic map of practitioner discussions on building and deploying LLM-based applications. • An analysis of topic co-occurrences, highlighting relevant considerations and decisions. • A comprehensive replication package, including a mapping of videos to relevant topics [11]."
https://arxiv.org/html/2411.08561v1,LogLLM: Log-based Anomaly Detection Using Large Language Models,"Software systems often record important runtime information in logs to help with troubleshooting. Log-based anomaly detection has become a key research area that aims to identify system issues through log data, ultimately enhancing the reliability of software systems. Traditional deep learning methods often struggle to capture the semantic information embedded in log data, which is typically organized in natural language. In this paper, we propose LogLLM, a log-based anomaly detection framework that leverages large language models (LLMs). LogLLM employs BERT for extracting semantic vectors from log messages, while utilizing Llama, a transformer decoder-based model, for classifying log sequences. Additionally, we introduce a projector to align the vector representation spaces of BERT and Llama, ensuring a cohesive understanding of log semantics. Unlike conventional methods that require log parsers to extract templates, LogLLM preprocesses log messages with regular expressions, streamlining the entire process. Our framework is trained through a novel three-stage procedure designed to enhance performance and adaptability. Experimental results across four public datasets demonstrate that LogLLM outperforms state-of-the-art methods. Even when handling unstable logs, it effectively captures the semantic meaning of log messages and detects anomalies accurately.","Ensuring high availability and reliability is crucial for large-scale software-intensive systems [1, 2]. As these systems become more complex and expansive, the occurrence of anomalies becomes unavoidable [3, 4]. Even a minor issue can lead to performance degradation, data integrity problems, and substantial losses in both customers and revenue. Therefore, anomaly detection is vital for maintaining the health and stability of complex software-intensive systems [5]. Software-intensive systems typically produce console logs that record system states and critical runtime events [6]. Engineers can utilize this log data to evaluate system health, identify anomalies, and trace the root causes of issues. However, due to the potentially vast volume of logs, manually analyzing them for anomalies can be both labor-intensive and prone to mistakes [7]. Consequently, log-based anomaly detection has emerged as a key area in automated log analysis, focusing on the automatic identification of system anomalies through log data. Numerous deep learning-based methods [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22] for log-based anomaly detection have been proposed. These methods typically employ sequential deep learning models such as LSTM [23] and transformers [24]. These methods can be further divided into reconstruction-based methods [8, 9, 10, 11, 12, 13, 14, 15] and binary classification-based methods [16, 17, 18, 19, 20, 21, 22]. Reconstruction-based methods involve designing and training a deep neural network to reconstruct input log sequences, with anomalies detected based on reconstruction errors. The underlying principle is that anomalous samples cannot be accurately reconstructed. Binary classification-based methods, on the other hand, involve designing a binary classifier to classify samples as either normal or anomalous. These methods often require labeled anomalies for training purposes. It is recognized that system logs are documented in natural language and contain a significant amount of semantic information. Nevertheless, traditional deep learning-based methods struggle to effectively capture this information. In recent years, significant advancements have been achieved in LLMs, such as GPT-4 [25], Llama 3 [26], and ChatGLM [27]. These models are characterized by their vast parameter sizes and are pretrained on substantially larger datasets, ranging from several gigabytes to terabytes in size. This extensive pretraining equips them with remarkable language comprehension abilities, enabling superior performance in tasks such as summarization, paraphrasing, and instruction following even in zero-shot scenarios [28]. Existing methods that utilize LLMs for log-based anomaly detection can be categorized into prompt engineering-based [29, 7, 30, 31] and fine-tuning-based [32, 33, 34, 35, 3, 36, 37, 38, 39, 40] approaches. Prompt engineering-based methods leverage the zero/few-shot capabilities of LLMs to detect anomalies based solely on the models’ internal knowledge. However, these methods often struggle to customize solutions for specific datasets, leading to suboptimal detection performance. Fine-tuning-based methods integrate LLMs into deep neural networks and tailor them to user-specific datasets. Nevertheless, these methods encounter challenges such as limited semantic understanding, suboptimal LLM utilization (relying solely on LLMs for semantic information extraction), and insufficient consideration of input data format, which can lead to memory overflow. To tackle the aforementioned challenges, we propose LogLLM, a novel log-based anomaly detection framework that harnesses LLMs. Unlike traditional methods that rely on log parsers for template extraction, LogLLM preprocesses log messages using regular expressions, thereby streamlining the entire process. LogLLM, a fine-tuning-based method, utilizes BERT, a transformer encoder-based model, to extract semantic vectors from log messages. Additionally, it employs Llama, a transformer decoder-based model, to classify log sequences. To ensure coherence in log semantics, we introduce a projector that aligns the vector representation spaces of BERT and Llama. Our framework is trained using a novel three-stage procedure designed to enhance both performance and adaptability. As we know, LLMs frequently face out-of-memory challenges due to their extensive parameter sizes [41]. Directly inputting the entire log sequence (by concatenating log messages into a long string) into Llama can lead to out-of-memory issues and potentially confuse the LLM, making it difficult to focus on key points for distinguishing anomalies. By adopting BERT to summarize each log message, LogLLM effectively mitigates these problems. Compared to other methods, LogLLM fully exploits the capabilities of LLMs for log-based anomaly detection. We conduct experiments across four public datasets, and the results demonstrate that LogLLM outperforms state-of-the-art methods. Even when handling unstable logs, where new log templates frequently emerge due to software evolution, it effectively captures the semantic meaning of log messages and detects anomalies accurately. The ablation study confirms the effectiveness of the three-stage training procedure. The main contributions of our work are as follows: • We introduce LogLLM, a novel log-based anomaly detection framework leveraging LLMs. This study marks the first attempt to simultaneously employ transformer encoder-based and decoder-based LLMs, specifically BERT and Llama, for log-based anomaly detection. • We propose a novel three-stage procedure to optimize the training and coordination of different components within the deep model, enhancing both performance and adaptability. • We conduct extensive experiments on four publicly available real-world datasets, demonstrating that LogLLM achieves exceptional performance."
https://arxiv.org/html/2411.08535v1,The EU AI Act is a good start but falls short,"The EU AI Act was created to ensure ethical and safe Artificial Intelligence development and deployment across the EU. This study aims to identify key challenges and strategies for helping enterprises focus on resources effectively. To achieve this aim, we conducted a Multivocal Literature Review (MLR) to explore the sentiments of both the industry and the academia. From 130 articles, 56 met the criteria. Our key findings are three-fold. First, liability. Second, discrimination. Third, tool adequacy. Additionally, some negative sentiments were expressed by industry and academia regarding regulatory interpretations, specific requirements, and transparency issues. Next, our findings are three essential themes for enterprises. First, risk-based regulatory compliance. Second, ethical frameworks and principles in technology development. Third, policies and systems for regulatory risk management. These results identify the key challenges and strategies and provide less commonly discussed themes, enabling enterprises to align with the requirements and minimize their distance from the EU market.","The European Commission launchs the first European Union’s (EU) regulatory framework for Artificial Intelligence (AI), namely the EU AI Act (EU AIA), in April 2021 [14]. The EU AIA aims to regulate AI while preserving innovation and upholding fundamental rights, a challenging balance to achieve. It sets out four specific objectives: 1) to ensure that AI systems in the EU market comply with safety laws and respect fundamental rights and Union values; 2) to promote a legal framework that encourages investment and innovation in AI; 3) to improve the effective enforcement of laws related to fundamental rights and safety in AI systems; and 4) to facilitate the development of a single market for safe, legal and trustworthy AI systems, preventing market fragmentation [10]. To achieve this goal, the EU AIA proposes a proportionate regulatory approach with the minimum requirements necessary in terms of the risks and potential problems associated with AI. The EU AIA outlines prohibited AI systems, explains the requirements for high-risk AI systems, and provides guidelines to increase transparency while fostering development. Performing a risk classification is the first step toward knowing the risk category and complying with regulatory requirements. The EU AIA categorizes AI systems into three risk levels: 1) unacceptable risk, which includes AI functionalities such as subliminal manipulation, exploitative techniques, biometric categorization, social scoring, real-time remote biometric identification, emotional state assessment, predictive policing, and facial image scraping, all of which are banned in the EU; 2) high risk, which includes AI systems such as those used to assess consumer creditworthiness; and 3) limited and minimal risk, which includes AI chatbots and similar functionalities [5]. In the context of the EU AIA, enterprises, i.e., providers, deployers, importers, distributors, and product manufacturers, are faced with detailed documentation, strategies, and processes related to compliance. These may include various practical, operational, and strategic issues. Therefore, the process and framework should be in the form of practical terms. According to current law, the Secretary of Government Operations is required to create a coordinated plan that looks into the viability and challenges of creating standards and technology that state agencies can use to identify the provenance of digital content by analyzing the effects of the rise in deepfakes, among other things [19]. Since the emergence of the EU AIA, several academic articles have been published to share good practics, such as the challenges posed by AI-driven trading in the EU’s financial markets [1], the effect on mobility within the EU [GL34: see Data availability], a framework for collaborative governance [15], the articulation of ethical charters, legal tools, and technical documentation in Machine Learning (ML)[WL24: see Data availability]. Furthermore, valuable insights may be available not in traditional academic databases but in the media, such as the EU AIA’s recommendations for business [GL39], a proposed framework with current developments meant for businesses, a comprehensive regulatory framework for businesses [9, 11], and implications and strategies for UK businesses[2], to name but a few. Thus, this work recognises that valuable knowledge and insights can be found in various types of publications and media, not just academic articles [7]. Accordingly, we aim to aid companies in better understanding EU AIA by conducting a secondary research study using a Multivocal Literature Review (MLR) and Natural Language Processing (NLP) to investigate the EU AIA practices. In this study, we aim to gather a more comprehensive and multifaceted understanding of the EU AIA by considering multiple perspectives, including those from practitioners, industry experts, and other stakeholders. For this reason two research questions (RQs) were formulated for conducting this study: RQ1) What are the key challenges perceived by both industry and academia in complying with the EU AIA, and RQ2) What strategies and processes are enterprises developing to implement the EU AIA, through MLR and NLP approaches. MLR is a comprehensive literature review approach and systematic process that incorporates various sources of information, namely ”Grey Literature” (GL). MLR has become notable in various fields because it is an approach able to 1) integrate both academic and GL, 2) provide a comprehensive overview of a topic with diverse perspectives, and 3) raise the strictness and applicability in the literature review of that field for proving the significance of practical and knowledge [7, 12]. This study is structured similarly to the MLR guidelines [7], in terms of 1) planning and surveying the MLR guidelines, 2) conducting the review and data analysis, and 3) presenting the findings or results. The rest of the paper is organized as follows. The content in Section 2 describes 1) how to search, select, and stop data sources with criteria, 2) how to control the quality of the data used, and 3) how to analyze the key challenges and identify/underline the strategies. The content in Section 3 deals with the key challenges and the strategy themes that will benefit companies in complying with the EU AIA. Section 4 describes and provides a practical framework and its limitations. Finally Section 5 concludes the study and suggests future research work."
https://arxiv.org/html/2411.08510v1,CorrectBench: Automatic Testbench Generation with Functional Self-Correction using LLMs for HDL Design,"Functional simulation is an essential step in digital hardware design. Recently, there has been a growing interest in leveraging Large Language Models (LLMs) for hardware testbench generation tasks. However, the inherent instability associated with LLMs often leads to functional errors in the generated testbenches. Previous methods do not incorporate automatic functional correction mechanisms without human intervention and still suffer from low success rates, especially for sequential tasks. To address this issue, we propose CorrectBench, an automatic testbench generation framework with functional self-validation and self-correction. Utilizing only the RTL specification in natural language, the proposed approach can validate the correctness of the generated testbenches with a success rate of 88.85%. Furthermore, the proposed LLM-based corrector employs bug information obtained during the self-validation process to perform functional self-correction on the generated testbenches. The comparative analysis demonstrates that our method achieves a pass ratio of 70.13% across all evaluated tasks, compared with the previous LLM-based testbench generation framework’s 52.18% and a direct LLM-based generation method’s 33.33%. Specifically in sequential circuits, our work’s performance is 62.18% higher than previous work in sequential tasks and almost 5 times the pass ratio of the direct method. The codes and experimental results are open-sourced at the link: https://github.com/AutoBench/CorrectBench.","Simulation-based functional verification, relying on a testbench (TB), is among the most prevalent verification techniques employed during the initial phases of hardware design. The engineering effort required to design a testbench for functional simulation remains significantly high [1], with much of this effort being task-specific. This specificity complicates finding a generic method to optimize the process. Previous works, such as those by [2, 3, 4], have primarily focused on automating the generation of test stimuli for the design under test (DUT), which constitutes the front end of the functional simulation. The back end involves verifying the correctness of the signals from the DUT, which is highly specialized, making traditional automation methods ineffective and thus unattainable for the fully automated testbench design. The increasing application of LLMs in the digital hardware design process suggests an alternative approach to automating testbench design. Recent studies [5, 6, 7, 8, 9, 10, 11] demonstrated the effectiveness of LLMs in various aspects of hardware design, particularly in Register-transfer level (RTL) design. Some research efforts have extended beyond basic RTL design correction using LLMs [12, 13]. In the realm of functional simulation-based verification, preliminary efforts have been made. For instance, [14] investigates the potential of LLMs in generating testbenches for finite state machines (FSMs), while [15] introduces a framework called AutoBench, the first systematic and generic testbench generation framework. Although achieving an average 57% improvement compared with directly generating testbench using LLMs, AutoBench still suffers from a low success rate. This limitation arises from the inherent uncertainty of LLMs, such as hallucination [16] and laziness [17]. Additionally, AutoBench employs only syntax self-checking, similar to RTLFixer [12], without implementing functional self-checking. This is a common issue in current LLM-based hardware design methodologies, the absence of a self-checking mechanism indeed limits the potential performance of the AutoBench framework. Figure 1: The outline of CorrectBench workflow. To address the aforementioned issues, this paper proposes CorrectBench, the first framework for automatic testbench generation that incorporates functional self-validation and self-correction. Our framework utilizes the design specification (SPEC) of the device under test (DUT) in natural language as the sole input, as illustrated in Fig. 1, while expanding the boundaries of current testbench generation methods. The contributions of this work are summarized as follows: • An action-based testbench self-validation and self-correction framework is proposed. The total testbench generation pass ratio is improved up to 70.13%, compared with 52.18% in the previous work and 33.33% in a direct method where LLMs are applied directly to generate test benches. Specifically in sequential circuits, our work’s performance is 62.18% higher than previous work in sequential tasks and almost 5 times the direct method. • A scenario-based testbench self-validator is proposed, validating the correctness of the generated testbench via a particular matrix. The validator only takes the task specification in natural language as the input information and achieves an average 88.85% validation accuracy. • An LLM-based testbench self-corrector is used to take the bug information from the validator as the input. The corrector makes a 34.33% contribution in the total improvement compared with previous work. • The code, dataset, and experimental results are open-sourced on https://github.com/AutoBench/CorrectBench."
https://arxiv.org/html/2411.08507v1,TimeLess: A Vision for the Next Generation of Software Development,"Present-day software development faces three major challenges: complexity, time consumption, and high costs. Developing large software systems often requires battalions of teams and considerable time for meetings, which end without any action, resulting in unproductive cycles, delayed progress, and increased cost. What if, instead of large meetings with no immediate results, the software product is completed by the end of the meeting? In response, we present a vision for a system called TimeLess, designed to reshape the software development process by enabling immediate action during meetings. The goal is to shift meetings from planning discussions to productive, action-oriented sessions. This approach minimizes the time and effort required for development, allowing teams to focus on critical decision-making while AI agents execute development tasks based on the meeting discussions. We will employ multiple AI agents that work collaboratively to capture human discussions and execute development tasks in real time. This represents a step toward next-generation software development environments, where human expertise drives strategy and AI accelerates task execution.","Software development is a complex, time-consuming, and expensive process (Boehm et al., 2000). In practice, developers and stakeholders collaborate using established software process models, such as Waterfall, Agile, and DevOps, among others, which are designed to facilitate communication, coordination, and simplify workflows (Pargaonkar, 2023). These processes highlight the inherent complexity, time consumption, and communication challenges that characterize traditional software development (Palle, 2020). The processes require extensive coordination among multiple teams and stakeholders. For instance, the development of large software systems requires teams to hold multiple meetings to discuss strategies, assign tasks, and monitor progress (Gaborov et al., 2023). These meetings often conclude without generating clear actionable items, resulting in unproductive cycles where discussions fail to translate into immediate progress, ultimately delaying development and increasing costs (Kamei et al., 2017). Figure 1. Future Generation of Software Development In this paper, we present a vision for the next generation of software development, called TimeLess. The aim is to reshape the development process by introducing immediate action and real-time execution during meetings. This approach enables the development of large-scale software projects within a limited time frame by integrating AI, while keeping human teams central to decision-making to drive the process. Meetings shift from planning discussions to action-driven sessions, where teams see the system implement their ideas in real-time. The AI agent will act as an assistant, executing tasks based on team discussions during the meeting, while human developers monitor the agent’s performance to ensure quality and accuracy by providing feedback. The core concept of TimeLess is to facilitate faster and more efficient software development by allowing AI agents to act as assistants during meetings. As shown in Figure 1, the background screen displays the task execution in real-time, illustrating how AI agents translate discussions among four human stakeholders into actionable development steps. The process will begin with discussions among stakeholders, developers, architects, and quality assurance personnel regarding the software product. The propose system will listen and capture these discussions, transcribing them into text. The system will process this transcribed data to automatically generate summaries, user stories, epics, and tasks, which will form the foundation for the next stages of development. This iterative process will continue until the team is satisfied with the generated user stories and epics. The system will progress through design, coding, testing, and deployment, guided by the human team based on initial user stories generated by multi-AI agents. Throughout the project, the team will have the flexibility to revisit and update any stage of software development as needed. In this process, the human team will drive the strategy, while AI agents will act as assistants to execute tasks in real time during meetings. Our objective is to make meetings more interactive, productive, and action-oriented, while also reducing the time, cost, and complexity of development. The introduction of the TimeLess system represents a step forward towards next-generation software development environments that adapt to the demands of the industry. This system aims to enhance productivity by integrating AI-driven processes that facilitate task execution and decision-making. To illustrate this vision, we present initial results in Section 5, demonstrating progress toward achieving this goal."
https://arxiv.org/html/2411.08254v1,VALTEST: Automated Validation of Language Model Generated Test Cases,"Large Language Models (LLMs) have demonstrated significant potential in automating software testing, specifically in generating unit test cases. However, the validation of LLM-generated test cases remains a challenge, particularly when the ground truth is unavailable. This paper introduces VALTEST, a novel framework designed to automatically validate test cases generated by LLMs by leveraging token probabilities. We evaluate VALTEST using nine test suites generated from three datasets—HumanEval, MBPP, and LeetCode—across three LLMs—GPT-4o, GPT-3.5-turbo, and LLama3.1 8b. By extracting statistical features from token probabilities, we train a machine learning model to predict test case validity. VALTEST increases the validity rate of test cases by 6.2% to 24%, depending on the dataset and LLM. Our results suggest that token probabilities are reliable indicators for distinguishing between valid and invalid test cases, which provides a robust solution for improving the correctness of LLM-generated test cases in software testing. In addition, we found that replacing the identified invalid test cases by VALTEST, using a Chain-of-Thought prompting results in a more effective test suite while keeping the high validity rates.","⬇ 1def max_area(height): 2””” You are given an integer array height of length n. There are n vertical lines drawn such that the two endpoints of the ith line are (i, 0) and (i, height[i]). Find two lines that together with the x-axis form a container, such that the container contains the most water. Return _the maximum amount of water a container can store_.**Notice** that you may not slant the container. 3**Example 1:**Input:** height = [1,8,6,2,5,4,8,3,7]**Output:** 49 4**Explanation:** The above vertical lines are represented by array [1,8,6,2,5,4,8,3,7]. In this case, the max area of water (blue section) the container can contain is 49. 5**Example 2:**Input:** height = [1,1]**Output:** 1””” Tests Generated by GPT4o ⬇ 1 Function Input Expected Output 21. assert max_area ([1,3,2,5,25,24,5]) == 24 @✓@ 32. assert max_area([1,2,3,4,5,6,7,8,9,1])== 8 @✗@ 43. assert max_area([1,2,3,4,5,6,7,1,8,9])== 18 @✗@ 54. assert max_area([1,1,1,1,1,1,1,1,1,1])== 9 @✓@ ⬇ 1 Corresponding Token Probabilities 21. [43,44,98,99,94,99,99], [@99@] 32. [54,38,70,80,92,38,79,92,61,31], [@46@] 43. [50,33,56,71,77,80,75,67,86,98], [@35@] 54. [90,92,92,99,99,99,99,99,99,92], [@99@] Figure 1. An example of test case generation using GPT4o. The check mark indicates a valid test case and the cross mark indicates an invalid test case. LLMs have been applied in various software development tasks, including software testing, design, requirements engineering, code generation, maintenance, deployment, and more (Fan et al., 2023; Wang et al., 2024). Automated generation of unit tests is crucial for ensuring the accuracy of individual software components. Unit tests examine isolated sections of code, helping developers detect issues early and verify that each function operates as intended. LLMs are particularly promising for improving the efficiency of unit test case generation and its automation. This automation lessens the developers’ burden and has the potential to improve both test coverage and quality (Wang et al., 2024). In addition, unit test case generation is a critical component of many LLM-based code generation tools, such as Reflexion (Shinn et al., 2024), LATS (Zhou et al., 2023), AgentCoder (Huang et al., 2023a), EPiC (Taherkhani et al., 2024), and LDB (Zhong et al., 2024). Several studies have addressed the generation of valid test cases and the refinement of test suites (Li and Doiron, 2023; Guilherme and Vincenzi, 2023; Yuan et al., 2023; Li and Yuan, 2024; Sollenberger et al., 2024). These works typically rely on code execution to validate test cases and employ LLMs to enhance test suite quality. While previous studies address the refinement of test cases and, in some cases tackle the issue of generating invalid test cases using LLMs, to the best of our knowledge no study directly addresses this critical question: How can we determine whether an LLM-generated test case is valid, when the code under test is either unavailable or its correctness is unknown? Note that in most practical scenarios, the code under test during the testing phase might have bugs, thus a failed test case may indicate a bug in the code or an invalid test case. Therefore, the test’s result is not enough as a verdict for test validation. In addition, there are scenarios where the code is not even available before the test case, e.g., in the Test Driven Development (TDD) process, as well as in many recent LLM-based code generation tools (Shinn et al., 2024; Taherkhani et al., 2024), where the test cases are required as part of the code generation process. Validating test cases is a preliminary step before evaluating their effectiveness. While test case evaluation involves determining whether the test cases are adequate, often using metrics such as code coverage or mutation testing, validation focuses on determining whether the test case verify the intended functionality. That is, the assertions correctly define the expected and the actual results. This distinction is crucial: validation precedes evaluation and is inherently more challenging because it requires an understanding of the expected behavior of the function. LLMs frequently generate invalid test cases, even with state-of-the-art (SOTA) models, such as GPT-4o, and even in widely used benchmarks like HumanEval. For example, as discussed later in this paper, the ratio of valid test cases to total test cases generated by a SOTA model like GPT-4o on the MBPP dataset is as low as 0.71. This ratio is even lower for other LLMs and datasets, highlighting the difficulty LLMs face in generating valid test cases. Therefore, identifying and discarding or even fixing these invalid test cases is essential, before integrating them into the project. If invalid test cases are incorporated, they can mislead developers (or code generation tools) and lead to unintended consequences. For example, Figure 1 presents a sample of test cases generated by GPT-4o. In this example, the LLM produced two valid and two invalid assertion statements (while more tests can be generated, we present only four for brevity). The correct outputs for the second and third assertions should be 20 (not 8), and 25 (not 18), accordingly. In this paper, we use a hallucination-aware approach to predict invalid test cases generated by LLMs. Hallucination in the context of Natural Language Generation refers to the phenomenon where models generate text that is either nonsensical or unfaithful to the provided source content (Ji et al., 2023). One of the most common ways to detect hallucinations in LLMs is to use token probabilities as used in (Kadavath et al., 2022; Ledger and Mancinni, 2024; Quevedo et al., 2024; Huang et al., 2023b; Fadeeva et al., 2024; Varshney et al., 2023; Orgad et al., 2024). In LLMs, logits reflect the model’s confidence in each token being the next in a sequence. To transform these scores into probabilities, the softmax function is applied, producing a probability distribution where each token is assigned a probability based on its logit. The token with the highest probability is typically selected as the next in the generated sequence. In this paper, when we refer to a test case, we mean a unit test with a single assertion that verifies one behavior of the function under test. Therefore, we use “assertion” and “test case” interchangeably. In Figure 1, for each assertion, we extracted the token probabilities associated with the function input from the left-hand side of the assertion and the expected output from the right-hand side. For example, in the assertion assert maxarea([1, 3, 2, 5, 25, 24, 5]) == 24, the token probability of the expected output token 24 is 99, while the function input tokens 1,3,2,5,25,24,5 have probabilities of 43,44,98,99,94,99,99, respectively. Comparing the token probabilities of assertions 1 and 4 with those of assertions 2 and 3, we observe that invalid test cases exhibit lower token probability scores, either in the function input or the expected output tokens. This is expected, as LLMs are prone to generating invalid test cases when they are uncertain about the assertions’ input/output, which is often the result of LLM’s hallucination (generating assertions that contradict the function’s description). This observation motivated us to develop VALTEST, a tool that leverages the token probabilities of an LLM-generated test case to predict its validity. We utilized three datasets—HumanEval, MBPP, and LeetCode—and three LLMs—GPT-4o, GPT-3.5-turbo, and LLama3.1 8b—to generate nine test suites for evaluating VALTEST. These new test suites were necessary as existing test suites for these datasets in previous works do not include token probability information. After generating the tests, we extracted various feature sets representing the statistical measures of token probabilities for both the function input and the expected output of assertions. We executed each test case on its correct code under test to label each case as either valid or invalid. Using these labeled cases, we trained an ensemble machine learning model using a k-fold approach to predict the validity of each test case, in the evaluation set. Based on our predictions, we either discarded the invalid test cases identified by our model or applied a Chain-of-Thought (CoT) prompting technique to correct the invalid cases. We evaluated the test suites using validity rate, mutation score, and code coverage metrics both before and after applying VALTEST. Our results demonstrate that VALTEST improves the validity rate from 6.2\% up to 24\% across different LLMs and datasets, accordingly. Token probabilities in the expected output section are key indicators for distinguishing valid from invalid test cases. Features extracted from these probabilities show significant differences between valid and invalid, with valid cases having higher token probabilities and fewer hallucinations. Furthermore, expected output features have a stronger impact on VALTEST’s accuracy than function input features. Additionally, we highlight a trade-off between validity rate and mutation or code coverage scores in VALTEST. Moreover, combining CoT prompting with VALTEST to fix invalid test cases increases the mutation score by 2.9% to 6.7%, resulting in a more comprehensive test suite. The main contributions of this paper are as follows: (1) To the best of our knowledge, VALTEST is the first work to explore the validation of LLM-generated test cases using token probabilities. (2) We demonstrate the effectiveness of VALTEST in test case validation across three common benchmark datasets and three SOTA LLMs. (3) We show how VALTEST can be used to replace invalid test cases with valid ones to increase mutation score and code coverage of the LLM-generated test suites. We also release the data and source code for our experiments to facilitate replication and extension by other researchers (https://github.com/HamedTaherkhani/VALTEST)."
https://arxiv.org/html/2411.08172v1,Fault Localization in Deep Learning-based Software: A System-level Approach,"Over the past decade, Deep Learning (DL) has become an integral part of our daily lives, with its widespread adoption in various fields, particularly in safety-critical domains. This surge in DL usage has heightened the need for developing reliable DL software systems, making Software Reliability Engineering (SRE) techniques essential. Given that fault localization is a critical task in SRE, researchers have proposed several fault localization techniques for DL-based software systems, primarily focusing on faults within the DL model. While the DL model is central to DL components, there are numerous other elements that significantly impact the performance of DL components. As a result, fault localization methods that concentrate solely on the DL model overlook a large portion of the system. To address this, we introduce FL4Deep, a system-level fault localization technique based on a Knowledge Graph (KG). For the first time, FL4Deep considers the entire DL development pipeline to effectively localize faults across the DL-based systems. FL4Deep first extracts the necessary static and dynamic information from DL software systems, then generates a KG by analyzing the collected information. Finally, it provides a ranked list of potential faults by inferring relationships from the KG. In an evaluation using 100 faulty DL scripts, FL4Deep outperformed four previous approaches in terms of accuracy for three out of six DL-related faults, including issues related to data (84%), mismatched libraries between training and deployment (100%), and loss function (69%). Additionally, FL4Deep demonstrated superior precision and recall in fault localization for five categories of faults including three mentioned fault types in terms of accuracy, plus issues related to the insufficient training iteration with 0.89 and 0.62 and activation function with 0.89 and 0.92 for precision and recall, respectively. Sensitivity analysis of FL4Deep components also indicates that static information has the most significant impact on the performance of FL4Deep.","Nowadays, Deep Learning (DL) has become an integral part of our everyday life. DL has been used in extensive applications across diverse domains including Natural Language Processing (NLP) (Khurana et al., 2023), autonomous driving (Yurtsever et al., 2020), financial (Henrique et al., 2019), and medical systems (Bhattacharya et al., 2021). These software systems which employ DL components are called DL-based systems (Morovati et al., 2024b). With the increasing dependence of current software systems on DL components, it is crucial to ensure the reliability of these components. DL-based systems, similar to traditional software systems, are prone to a variety of software faults (Humbatova et al., 2020). One of the most critical tasks in ensuring the reliability of software systems (either DL-based or traditional) is debugging, which focuses on detecting and fixing faults (iee, 2017). Even when faults are known to exist due to the system’s faulty behavior, the process of finding the location of faults’ root causes remains a significant challenge (Wong et al., 2016). Fault localization, which plays a key role in debugging, involves identifying the specific location of faults’ root causes within the system and fixing them (Alipour, 2012). In traditional software systems, when a mismatch occurs between the expected and actual outputs, it indicates the presence of a fault in the software system (Ammann and Offutt, 2016). Similarly, faults in DL software refer to discrepancies between the program’s current behavior and the expected outcome (Zhang et al., 2020a). However, unlike traditional software where the logic is represented through control flow coded by developers, the output in DL-based software is determined by a trained model (Wardat et al., 2021). As a result, DL-based systems introduce new challenges that cause more complex debugging than traditional software (Islam et al., 2020). Consequently, fault detection and localization in DL-based systems are more difficult than in traditional systems. For instance, when a classifier produces an incorrect classification, it does not necessarily indicate a fault in the DL-based systems. Although researchers have increasingly focused on developing testing and debugging techniques for DL-based systems, fault localization has received comparatively less attention (Wardat et al., 2022). This is expected due to the distinct challenges inherent in fault localization within the context of DL-based systems. Compared to traditional software, the root causes of faults in DL-based software are more varied, located in three main components: 1) the DL program code, 2) the DL framework, and 3) the data used to train the DL models (Tambon et al., 2021; Islam et al., 2019). In this study, we focus specifically on faults within the DL software system code, excluding issues related to DL frameworks and data. ⬇ 1model.add(embedding_layer) 2model.add(Dropout(0.25)) 3# convolution layers 4model.add(Conv1D(nb_filter=32, filter_length=4, 5 border_mode=’valid’, activation=’relu’)) 6model.add(MaxPooling1D(pool_length=2)) 7# dense layers 8model.add(Flatten()) 9model.add(Dense(256)) 10model.add(Dropout(0.25)) 11model.add(Activation(’relu’)) 12# output layer 13model.add(Dense(len(class_id_index))) 14model.add(Activation(’softmax’)) 15model.compile(loss=’binary_crossentropy’, 16 optimizer=’adam’, metrics=[’accuracy’]) Listing 1: SO post (#42081257) showing faults in loss function (line 15) and metrics (line 16) that pose a challenge for developers to identify and address. The fundamental differences between DL-based and traditional software paradigms introduce novel types of faults unique to DL-based systems. As a result, debugging techniques developed for traditional software cannot be efficiently applied to DL-based systems (Humbatova et al., 2021; Morovati et al., 2024b). For example, Listing 1 shows a script from a SO post discussing a DL-based system encountering ‘bad performance’ (#42081257). The developer who submitted the post indicated that she has been unable to detect and localize the issue, specifically faults within the loss function and metrics parameters (Lines 15 and 16 of Listing 1) These faults are mostly identified through manual code review which is time-consuming and needs DL expertise (Nikanjam et al., 2021). Since fault localization is one of the most time-consuming aspects of debugging, it significantly impacts the overall effort required for software debugging (Wong et al., 2016). That is, inaccurate fault localization can mislead the debugging process, resulting in wasted time and effort for developers. Therefore, providing fault localization techniques for DL-based systems that consider essential features of these systems can significantly assist DL-based systems’ developers. As illustrated in Fig. 1, the DL component consists of various elements. While the DL model is the central element, it represents only a small part of the whole DL components (Sculley et al., 2015). Therefore, faults within the DL component can originate from any of these elements, not just the DL model itself (Chen et al., 2020a). Therefore, efficient fault localization for DL-based systems should target not only the DL model itself but also the entire training pipeline and its associated components. To address this gap, we propose a system-level fault localization technique namely FL4Deep, which builds upon Information Retrieval (IR)-based and history-based fault localization approaches. FL4Deep targets DL faults across all components of the DL pipeline, including data, the DL model and its training, and components dealing with the DL-based system deployment process. To identify and localize faults, FL4Deep extracts both static and dynamic information from DL-based systems and constructs a Knowledge Graph (KG) representing the system’s features such as dataset features used to train DL models, model hyperparameters, used environment to train models, etc. Additionally, it generates an ordered list of potential faults and their root causes, inferred through a set of rules designed based on the commonly known faults in DL-based systems. Evaluation results show that FL4Deep outperforms other fault localization approaches (including DeepFD (Cao et al., 2022), AutoTrainer (Zhang et al., 2021), DeepLocalize (Wardat et al., 2021), and UMLAUT (Schoop et al., 2021)) in 83% of the 100 buggy samples used for their comparison, in terms of precision and recall. To summarize, this research makes the following contributions: • We present the first technique that analyzes the entire pipeline of deep learning-based system development to effectively localize faults. • We highlight the key challenges in fault localization of DL-based systems. • We present fault localization techniques originally developed for traditional software systems, which can be adapted to best suit the unique characteristics of deep learning models. • We provide a dataset of real-world buggy DL codes extracted from SO posts and GitHub repositories. • We release the source code of FL4Deep alongside our datasets to facilitate its use by other researchers (Morovati et al., 2024a). The remaining of this paper is structured as follows. The main related studies are reviewed in Section 2. Section 3 provides background information on fault localization in DL-based systems. In Section 4, we present FL4Deep and its methodology in detail. Section 5 presents the results and analysis of the comparison of FL4Deep with four existing fault localization methods for DL-based systems. Section 6 outlines threats to the validity of FL4Deep. Finally, we conclude the paper and outline future research directions in Section 7."
https://arxiv.org/html/2411.07982v1,"Interoperability From Kieker to OpenTelemetry:
Demonstrated as Export to ExplorViz","While the observability framework Kieker has a low overhead for tracing, its results currently cannot be used in most analysis tools due to lack of interoperability of the data formats. The OpenTelemetry standard aims for standardizing observability data.In this work, we describe how to export Kieker distributed tracing data to OpenTelemetry. This is done using the pipe-and-filter framework TeeTime. For TeeTime, a stage was defined that uses Kieker execution data, which can be created from most record types. We demonstrate the usability of our approach by visualizing trace data of TeaStore in the ExplorViz visualization tool.","Observability data are used to understand a system’s behavior. OpenTelemetry has become the de-facto standard for observability data [Blanco2023]. It offers a variety of integrations into different languages, libraries and frameworks for obtaining observability data. Storage and analysis tools like Zipkin, Prometheus, Grafana, and ExplorViz [hasselbring2020explorviz] rely on usage of the OpenTelemetry format, and enable understanding the system behavior by automated analysis and visualizations. While the observability framework Kieker [hasselbring2020kieker] has lower performance overhead for gathering distributed execution traces [reichelt2024overhead, reichelt2021overhead], its data format is not usable for standard observability analysis tools. Therefore, creating interoperability between Kieker and OpenTelemetry will allow the usage for Kieker with its low overhead while maintaining Kieker’s advantages. In this paper, we (1) describe our concept for interoperability between Kieker and OpenTelemetry, (2) describe our first step towards interoperability, which is the implementation of the export from Kieker to the OpenTelemetry format, and (3) demonstrate the viability of this concept by visualizing the Kieker-monitored runtime behavior of the TeaStore [Eismann2018] in ExplorViz, which is achieved using the Kieker to OpenTelemetry export. The remainder of this paper is structured as follows: First, we describe a concept for interoperability between Kieker and OpenTelemetry. Afterwards, we describe how the export of Kieker traces into OpenTelemetry traces can be accomplished. Subsequently, we describe how the export from Kieker to OpenTelemetry is implemented. This is demonstrated by using Kieker data for an ExplorViz visualization. Afterwards, we compare this approach to related work. Finally, we give a summary of our work."
https://arxiv.org/html/2411.07718v2,: AST Differencing for Solidity Smart Contracts,"Smart contracts, primarily written in Solidity, are integral to blockchain software applications, yet precise analysis and maintenance are hindered by the limitations of existing differencing tools. We introduce SoliDiffy, a novel Abstract Syntax Tree (AST) differencing tool specifically designed for Solidity. SoliDiffy enables fine-grained analysis by generating accurate and concise edit scripts of smart contracts , making it ideal for downstream tasks such as vulnerability detection, automated code repair, and code reviews. Our comprehensive evaluation on a large dataset of real-world Solidity contracts demonstrates that SoliDiffy delivers shorter and more precise edit scripts compared to state-of-the-art tools, while performing consistently in complex contract modifications. SoliDiffy is made publicly available at https://github.com/mojtaba-eshghie/SoliDiffy.","Smart contracts are self-executing programs that implement real-world contracts by encoding contract terms directly into code [1, 2]. These programs are deployed on blockchain platforms like Ethereum [3], allowing for automated and trustless transactions. Solidity, a statically-typed programming language, has become the most popular choice for developing these smart contracts [4]. Developers working with Solidity can greatly benefit from a fine-grained source code differencing in several scenarios. For instance, when updating a smart contract to patch a security vulnerability, Abstract Syntax Tree (AST)-based differencing allows developers to pinpoint specific changes in the code’s structure rather than sifting through line-by-line text changes, which might miss subtle yet crucial modifications. This precision is vital when reviewing updates for correctness and security implications before deployment on an immutable blockchain. Other examples are in automated debugging [5] and program repair [6, 7, 8, 9], where tools need to detect bugs and vulnerabilities [10, 11, 12, 13, 14] and suggest changes at a syntactic level [15] ; fine-grained differencing enables these tools to generate precise edit scripts that align with the semantic intentions of the code, helping with automated and context-aware fixes. Furthermore, in code clone detection [16, 17, 18], where identifying syntactically similar but not identical code blocks is necessary, AST differencing can accurately capture variations that line-based tools overlook, thus enhancing the detection of potential code reuse or duplication issues. Traditional text and structural differencing tools [19, 20, 21, 22, 23, 24, 25] are insufficient for smart contracts due to their inability to capture the semantic and structural details of smart contracts or their lack of support for Solidity. AST differencing for Solidity smart contracts provides a more granular approach at detection of changes at the syntactic level. To summarize, we problem we address in this paper is the fundamental limitations of existing differencing tools for Solidity developers. In this paper, we introduce SoliDiffy, a novel AST differencing tool tailored for Solidity smart contracts. SoliDiffy contains key AST transformations specifically designed for Solidity smart contracts. SoliDiffy accurately processes and compares Solidity smart contracts, providing precise edit scripts that can be used for tasks such as vulnerability detection. To evaluate the effectiveness of SoliDiffy, we conduct a comprehensive comparison on 354\,187 pairs of smart contracts. This dataset is founded on real-world Solidity smart contracts, including a subset of modified controlled syntactic changes (simple to complex transformation) and another subset mined from commit history of a popular smart contract repository. We assess the performance of SoliDiffy in terms of edit script accuracy. The results clearly demonstrate that SoliDiffy outperforms the only existing tool for Solidity differencing, Difftastic [21]. SoliDiffy maintains consistent effectiveness regardless of edit distance and code complexity, highlighting its suitability for advanced software engineering tasks in the blockchain domain. To summarize, our contributions are: • We introduce SoliDiffy, a novel AST differencing approach specifically designed for Solidity smart contracts, addressing the limitations of existing differencing tools in accurately capturing the syntactic and semantic changes within smart contracts. • We design Solidity-specific AST transformation and pruning rules that enhance the precision and conciseness of edit scripts compared to a raw concrete syntax tree. • We conduct a comprehensive evaluation of SoliDiffy on a dataset of 354\,187 pairs of smart contracts to diff, demonstrating that it outperforms the state-of-the-art tool Difftastic in terms of edit script length and accuracy. SoliDiffy maintains consistent effectiveness regardless of edit distance and code complexity. • We make SoliDiffy publicly available as an open-source tool at https://github.com/mojtaba-eshghie/SoliDiffy, facilitating further research and applications in smart contract analysis. This paper is structured as follows: Section II introduces the necessary background concepts, including ASTs and code differencing techniques. In Section III, we describe the architecture of SoliDiffy. Section IV outlines the experimental protocol including research questions that guide our investigation. Section V presents the results of our evaluation, comparing SoliDiffy with Difftastic and exploring the impact of different types and severity of syntactic changes on the performance of these tools. Section VII reviews existing work related to AST differencing and identifies gaps that SoliDiffy addresses. Finally, Section VI elaborates the lessons learnt and threats to validity of our work, and Section VIII concludes the paper. Figure 1: The design of the SoliDiffy smart contract differencing tool."
https://arxiv.org/html/2411.07668v2,Towards Evaluation Guidelines for Empirical Studies involving LLMs,"In the short period since the release of ChatGPT in November 2022, large language models (LLMs) have changed the software engineering research landscape. While there are numerous opportunities to use LLMs for supporting research or software engineering tasks, solid science needs rigorous empirical evaluations. However, so far, there are no specific guidelines for conducting and assessing studies involving LLMs in software engineering research. Our focus is on empirical studies that either use LLMs as part of the research process (e.g., for data annotation) or studies that evaluate existing or new tools that are based on LLMs. This paper contributes the first set of guidelines for such studies. Our goal is to start a discussion in the software engineering research community to reach a common understanding of what our community standards are for high-quality empirical studies involving LLMs.","While artificial intelligence (AI) has been used in software engineering (SE) for a long time, success used to be limited [1]. Recently, the rise of large language models (LLMs) has opened new avenues for the application of AI in software engineering [2, 3]. These models offer many possible use cases, ranging from code generation and bug detection to requirements analysis and software maintenance. For instance, LLM-based tools were able to generate logging statements [4], generate test cases [5], and support education [6]. As a result, we are starting to see an increasing number of evaluation studies either using LLMs as part of the research process [7] or as part of tools that automate or improve software engineering tasks. These studies explore the effectiveness, performance, and robustness of LLMs in different contexts, such as improving code quality, reducing development time, or supporting software documentation. However, it is often unclear how valid and reproducible results can be achieved with empirical studies involving LLMs—or what effect their usage has on the validity of empirical results. This uncertainty poses significant challenges for researchers aiming to draw reliable conclusions from empirical studies. One of the primary risks in creating unreproducible results stems from the variability in LLM performance due to differences in training data, model architecture, evaluation metrics, and the inherent non-determinism that those models possess. For example, slight changes in the training dataset or the hyperparameters can lead to significantly different outcomes, making it difficult to replicate studies. Additionally, the lack of standardized benchmarks and evaluation protocols further complicates the reproducibility of results. These issues highlight the need for clear guidelines and best practices to ensure that empirical studies with LLMs yield valid and reproducible results. There has been extensive work developing guidelines for conducting and reporting specific types of empirical studies such as controlled experiments [8, 9] or their replications [10]. We believe that LLMs have specific intrinsic characteristics that we describe in the following, requiring specific guidelines for researchers to achieve an acceptable level of reproducability. For example, even if we know the specific version of an LLM used for an empirical study, the reported performance for the studied tasks can change over time, especially for commercial models that evolve beyond version identifiers [11]. Moreover, commercial providers do not guarantee the availability of old versions indefinitely. Besides versions, LLMs’ performance widely varies depending on configured parameters such as temperature. Therefore, not reporting the parameter settings impacts the reproducibility of the research. Even for “open” models such as Llama, we do not know how they were fine-tuned for specific tasks and what the exact training data was [12]. For example, when evaluating LLMs’ performance for certain programming tasks, it would be relevant to know whether the solution to a certain problem was part of the training data or not. Therefore, with this paper, we provide two key contributions: (1) a classification of different types of empirical studies involving LLMs in software engineering research and (2) preliminary guidelines on how to achieve valid and reproducible results in such studies."
https://arxiv.org/html/2411.07529v1,Evaluating ChatGPT-3.5 Efficiency in Solving Coding Problems of Different Complexity Levels: An Empirical Analysis,"ChatGPT and other large language models (LLMs) promise to revolutionize software development by automatically generating code from program specifications. We assess the performance of ChatGPT’s GPT-3.5-turbo model on LeetCode, a popular platform with algorithmic coding challenges for technical interview practice, across three difficulty levels: easy, medium, and hard. We test three main hypotheses: First, that ChatGPT solves fewer problems as difficulty rises (Hypothesis 1). Second, that prompt engineering improves ChatGPT’s performance, with greater gains on easier problems and diminishing returns on harder ones (Hypothesis 2). Third, that ChatGPT performs better in popular languages like Python, Java, and C++ than in less common ones like Elixir, Erlang, and Racket (Hypothesis 3). To investigate these hypotheses, we conduct automated experiments using Python scripts to generate prompts that instruct ChatGPT to create Python solutions. These solutions are stored and manually submitted on LeetCode to check their correctness. For Hypothesis 1, results show the GPT-3.5-turbo model successfully solves 92% of easy, 79% of medium, and 51% of hard problems. For Hypothesis 2, prompt engineering yields improvements: 14-29% for Chain of Thought Prompting, 38-60% by providing failed test cases in a second feedback prompt, and 33-58% by switching to GPT-4. From a random subset of problems ChatGPT solved in Python, it also solved 70% in Java, 50% in C++, and none in Elixir, Erlang, or Racket. These findings generally validate all three hypotheses.","Large language models have catalyzed significant advancements in various domains, including natural language processing [24], text generation [9], and now code generation [11]. LLMs have demonstrated skills in understanding, generating, and manipulating code, transforming the landscape of programming assistance. Large language models are trained on vast datasets comprising both natural language text and source code, which enables them to offer support in software development tasks. One of the more prominent LLMs is ChatGPT, a conversational AI model developed by OpenAI. The potential of these models to assist with programming has led to increased interest in their ability to improve programmer productivity, code correctness, and automated code generation from problem specifications. In this paper, we focus on exploring ChatGPT’s ability (primarily working with the low-cost GPT-3.5-turbo model) to solve coding problems in the popular programming language Python, using the platform Leetcode [8]. Leetcode is an online platform with more than 2000 coding challenges widely used for technical interview preparation and programming practice. The platform categorizes problems into three difficulty levels — easy, medium, and hard — covering a broad range of topics such as algorithms, databases, shell scripting, and concurrency. Each problem provides a detailed problem statement and typically includes 3-4 example input-output pairs. LeetCode’s integrated compiler and test cases make it an ideal tool for evaluating the correctness of solutions generated by GPT models, allowing for a structured and objective comparison. It is important to note that Leetcode, while useful as a benchmarking tool, does not fully capture the complexities of real-world coding environments. Our research is guided by three primary questions aimed at assessing the strengths and limitations of ChatGPT in coding tasks: 1. How effectively can ChatGPT solve Leetcode problems across varying levels of difficulty (easy, medium, and hard)? 2. Can prompt engineering and the selection of more effective demonstrations enhance ChatGPT’s initial coding results? 3. Which programming languages is ChatGPT most proficient in? One key challenge with language models like ChatGPT is the phenomenon of “hallucinations” - where the model may guess the user’s intent rather than seek clarification, respond sensitively to input phrasing, or produce plausible yet incorrect code [13]. These issues may arise from the limitations in the data set on which GPT-3.5-turbo was trained, particularly with respect to the code. LeetCode provides a robust framework of benchmarks and test cases that a solution must pass to be deemed correct. Even minor misunderstandings by ChatGPT can cause the generated code to fail in fulfilling its intended purpose. Given ChatGPT’s sensitivity to prompt phrasing, we also explore how the model’s performance can be improved through “prompt engineering”: designing more precise and effective prompts. In addition to Python, we examine ChatGPT’s proficiency across various programming languages, from popular ones like C++ and Java to less common languages such as Erlang, Elixir, and Racket. Through these investigations, we seek to understand better the strengths and limitations of ChatGPT’s code-generation capabilities. To conduct our evaluation, we developed a python script that interacts with ChatGPT via an API key, storing the model’s responses in corresponding files for each query. For each coding problem, we instructed ChatGPT to provide its solutions in python to ensure consistency in evaluation across all test cases. We tested 1,475 LeetCode coding problems, for each entering ChatGPT responses into the platform, and recording the result, including the number of passed test cases. These results were then analyzed to evaluate ChatGPT’s problem-solving performance and further test specific hypotheses. With respect to Question 1, our results show that ChatGPT 3.5 turbo performed well in solving LeetCode problems, with a high success rate of 92% for easy problems and 79% for medium ones. However, when tested on LeetCode’s hard category, the model’s performance dropped, achieving a 51% pass rate. These results highlight the model’s competence in simpler tasks, but also reveal its challenges when handling more complex coding problems. Regarding Question 2, our evaluation revealed that while chain-of-thought (CoT) prompt engineering improved ChatGPT performance in all problem difficulties, its impact was most significant on easier problems, with less impactful results on medium and hard problems. In contrast, incorporating test cases into the query — where the GPT-3.5-turbo model initially produced incorrect results — along with the expected outputs, provided even more significant improvements, particularly for medium and hard problems. Lastly, we observed that ChatGPT using the more advanced GPT-4 model, when paired with CoT prompting, also delivered strong performance, closely matching the results of adding in test cases where GPT-3.5-turbo initially failed, especially on more complex problems. This indicates that CoT prompting combined with error-focused adjustments can be highly effective, especially as the complexity of the problem increases. For example, in the easy category of problems, we found that CoT gave 29% improvements over the baseline, while providing failed test-cases gave 38% improvement, and switching to GPT-4 gave 33% improvement. For the medium and hard categories the performance of using GPT-4 and GPT-3.5-turbo with failed cases were more comparable. Lastly, looking at Question 3, we evaluated the performance of ChatGPT in five additional programming languages besides Python: C++, Java, Erlang, Elixir, and Racket. Python served as the baseline for comparison throughout our exploration of this question. ChatGPT was able to solve in C++ about 50% of a random sample of problems it could solve using Python and in Java it could solve about 70% of a random sample of problems it could solve using Python. Interestingly, some problems that ChatGPT failed to solve in Python were successfully solved in C++ or Java, indicating slight performance variations between these languages. However, for less common languages such as Erlang, Elixir, and Racket, the performance of ChatGPT was poor, with no problems successfully solved, regardless of whether they were solved in Python or not. This may be attributed to the lower frequency of these languages in the model’s training data set. We also present results from some further experiments where we evaluated which types of problems ChatGPT performed best/worst at, and whether there is a correlation between lines of code produced by ChatGPT and the correctness of the program. The subsequent sections of this paper dive deeper into the context, methodology, and outcomes of our study, providing a comprehensive analysis of ChatGPT’s performance and its implications for code generation tasks. In section 2, we provide a comprehensive review of the relevant literature on large language models (LLMs) and their application to code generation, highlighting prior research and existing challenges in the field. In section 3, we outline the experimental setup, including the process of data collection, the tools and techniques used to evaluate ChatGPT’s performance, and the criteria for assessing the correctness of its solutions. In section 4, we present a detailed analysis of ChatGPT’s performance across different categories of coding problems and programming languages. This section includes a quantitative assessment of its accuracy, efficiency, and limitations, supported by specific examples and findings from the experiments. We present our concluding comments and ideas regarding future work directions in section 5."
https://arxiv.org/html/2411.07480v2,Discovery of Timeline and Crowd Reaction of Software Vulnerability Disclosures,"Reusing third-party libraries increases productivity and saves time and costs for developers. However, the downside is the presence of vulnerabilities in those libraries, which can lead to catastrophic outcomes. For instance, Apache Log4J was found to be vulnerable to remote code execution attacks. A total of more than 35,000 packages were forced to update their Log4J libraries with the latest version. Although several studies have been conducted to predict software vulnerabilities, the prediction does not cover the vulnerabilities found in third-party libraries. Even if the developers are aware of the forthcoming issue, replicating a function similar to the libraries would be time-consuming and labour-intensive. Nevertheless, it is practically reasonable for software developers to update their third-party libraries (and dependencies) whenever the software vendors have released a vulnerable-free version. In this work, our manual study focuses on the real-world practices (crowd reaction) adopted by software vendors and developer communities when a vulnerability is disclosed. We manually investigated 312 CVEs and identified that the primary trend of vulnerability handling is to provide a fix before publishing an announcement. Otherwise, developers wait an average of 10 days for a fix if it is unavailable upon the announcement. Additionally, the crowd reaction is oblivious to the vulnerability severity. In particular, we identified Oracle as the most vibrant community diligent in releasing fixes. Their software developers also actively participate in the associated vulnerability announcements.","Modern software development often reuses third-party libraries to save time and costs incurred while developing from scratch. However, the presence of vulnerabilities (i.e., security flaws) could be catastrophic when the artifacts pose threats to their consumers. One way to assure software reliability is to refrain from vulnerabilities by keeping software library dependencies updated. However, a study showed that developers ceased to support and maintain dependency when a software development lifecycle ended [1], citing a lack of awareness of the new release. Another study [2] found that developers struggle with determining the most appropriate version when upgrading their dependencies. The number of vulnerabilities has been steadily increasing since 2017 [3]. This trend is particularly concerning because vulnerabilities in widely used software libraries can create ripple effects of security risks across numerous dependent applications. In November 2021, Apache Log4J was found to be vulnerable to remote code execution attacks, and more than 18,450 packages using the vulnerable version were affected [4]. Nevertheless, the first release of Apache’s fix did not fully address the vulnerability. As a result, 18,560 packages needed to be updated twice to finally use a fixed version of the dependency [5]. A total of more than 35,000 impacted packages are roughly equivalent to 8% of the total packages in the Maven ecosystem. Therefore, the magnitude of this impact is considered large as the average ecosystem impact of advisories affecting Maven Central is 2%, with a median of less than 0.1% [6]. Existing literature has contributed to predicting software vulnerabilities using text analysis [7, 8], trend analysis with topic models [9], and machine learning [10]. However, vulnerability databases often lack sufficient data to create robust prediction models [11, 12] that accurately reflect how vulnerabilities evolve throughout their real-world lifecycle. Li et al. [13] compiled a list of popular vulnerability databases that release official security policies and standards. However, these databases do not fully capture the real-world dynamics of vulnerabilities. Thus, we aim to address this gap by empirically investigating real-world vulnerability data to better understand the dynamics of how vulnerabilities evolve over time and how they are managed in practice. This study aims to uncover the lifecycle of vulnerability management. We manually examine the timeline and responses from software vendors and the developer community following the disclosure of software vulnerabilities, particularly in third-party libraries commonly used in software development. Vulnerabilities in these libraries can lead to security risks across dependent applications, making it essential to understand how these issues are identified, fixed, and disclosed. More specifically, our analysis focused on real-world responses to disclosed vulnerabilities and the time taken by software vendors to release fixes. We built a dataset containing 735 Java-related CVEs reported after 2017, and conducted a detailed study on a random sample of 312 CVEs. Data collection involved manual examination of CVE records, GitHub repositories, and reference links to construct a timeline of key events in each vulnerability’s lifecycle. Our study found that most vendors release fixes before publicly announcing vulnerabilities. We also observed that software vendors often upload their fixes across multiple platforms (but with time gaps), with different organizations adopting varying approaches to providing updates. For example, Oracle demonstrated consistent response times when releasing updates, while other vendors showed more variability. Additionally, the analysis revealed that community involvement was largely limited to identifying and reporting vulnerabilities, rather than offering solutions. However, this involvement can help speed up the CVE update process, as user-reported vulnerabilities tend to be resolved more quickly than those discovered solely by vendors. Vendors provided most fixes, and the time taken to resolve issues varied significantly. We compiled a set of actionable insights to enhance the role of the community and improve monitoring practices for vulnerability management. First, we find that vendors should encourage community involvement beyond just issue reporting. This can improve response times, as community members might contribute patches or mitigations that complement vendor fixes. Second, software developers can adopt a multi-source monitoring approach by regularly checking platforms like GitHub, mailing lists, and public advisories for updates. Given that some fixes are released on one platform ahead of others, this approach can help developers respond quickly to vulnerabilities, reducing potential risks in dependent applications. We highlight our contributions in this paper as follows: • We compile a comprehensive dataset that tracks software vulnerability response time, providing valuable insights for real-world practices. While previous datasets primarily analyze CVEs based on severity scores and other attributes, our dataset offers a comprehensive view of the entire vulnerability lifecycle, with a particular focus on Java vulnerabilities. This contribution enables further research in vulnerability management. • We conduct an exploratory manual examination of CVE records and reference links, the study reveals key trends, such as vendors predominantly releasing fixes before announcing vulnerabilities and the community’s role being largely limited to vulnerability identification rather than providing direct fixes. • We examine the relationship among various factors related to vulnerability and response handling, and we assist the developers in identifying the software library resident in the most responsive community. • We offer practical recommendations, including encouraging greater community involvement in vulnerability remediation and promoting multi-source monitoring practices, which can help developers and organizations respond more quickly to vulnerabilities and reduce security risks associated with third-party libraries. Paper organization. Section II introduces the background of the vulnerability lifecycle and the motivation of our study. Section III describes the manual study setup. Section IV presents the results. Sections VI and VII discuss threats to validity and related work. Section VIII concludes the paper."
https://arxiv.org/html/2411.07272v1,ASTD Patterns for Integrated Continuous Anomaly Detection In Data Logs,"This paper investigates the use of the ASTD language for ensemble anomaly detection in data logs. It uses a sliding window technique for continuous learning in data streams, coupled with updating learning models upon the completion of each window to maintain accurate detection and align with current data trends. It proposes ASTD patterns for combining learning models, especially in the context of unsupervised learning, which is commonly used for data streams. To facilitate this, a new ASTD operator is proposed, the Quantified Flow, which enables the seamless combination of learning models while ensuring that the specification remains concise. Our contribution is a specification pattern, highlighting the capacity of ASTDs to abstract and modularize anomaly detection systems. The ASTD language provides a unique approach to develop data flow anomaly detection systems, grounded in the combination of processes through the graphical representation of the language operators. This simplifies the design task for developers, who can focus primarily on defining the functional operations that constitute the system.","In today’s digital age, protecting IT infrastructure from cyberattacks and security breaches is critical for organizations to ensure daily operations, store sensitive data and manage customer information. Anomaly detection techniques can help organizations identify unusual patterns and behaviors in their systems so they can respond quickly and prevent potential security incidents. Anomaly detection techniques are instrumental in diverse areas, including fraud detection, network security, and intrusion detection within business applications [1]. Recognizing the pivotal role of anomaly detection systems in ensuring the security and reliability of various applications, from cybersecurity to industrial monitoring, it is crucial to acknowledge the challenges associated with their development [2]. Effectively addressing these challenges becomes imperative for successfully deploying robust and adaptive detection systems. In the realm of anomaly detection systems, a formidable challenge arises from the dynamic nature of data patterns. To maintain the system’s accuracy over time, periodic model re-training becomes imperative. Nils Baumann et al. [3] underscore the critical importance of automating the re-training process to adapt to evolving data patterns seamlessly. This challenge necessitates implementing robust mechanisms that detect anomalies and autonomously refine their understanding of normal and abnormal behaviors in the ever-changing data landscape. The intricacy of learning systems poses yet another significant challenge, encompassing multifaceted phases such as data pre-processing and model training. Benjamin Benni et al. [4] delve into a comprehensive analysis of this complexity, shedding light on the intricate processes that form the backbone of effective anomaly detection. Addressing this challenge requires the development of streamlined strategies to simplify the various phases, ensuring that the learning system can efficiently navigate the intricacies of data preprocessing and model training. Overcoming this hurdle is crucial for enhancing anomaly detection systems’ overall effectiveness and efficiency. As detection systems scale up to handle vast amounts of data, a distinct challenge emerges in maintaining modularity to ensure scalability and ease of maintenance. The development of large-scale detection systems demands a careful balance to prevent unwieldy complexity. Baldwin and Clark [5] stress the significance of modularity in such systems, emphasizing its pivotal role in facilitating scalability and simplifying maintenance efforts. Successfully addressing this challenge involves designing detection systems with modular architectures that can seamlessly adapt to the increasing demands of data volume and computational resources, ensuring both scalability and ease of long-term maintenance. This article introduces a method for developing anomaly detection systems using a specification language called Algebraic State Transition Diagram (ASTD) [6]. It investigates the extent to which this language reduces the complexity of the detection system by adding an abstraction layer. Additionally, it examines how the graphical representation of the language’s operators contributes to easing development efforts by managing the scheduling of various processes within the detection system. ASTD is a graphical and executable notation for composing state machines, offering modularity and flexibility in system development [7]. The paper’s contributions include (1) The extension of the ASTD language by the Quantified Flow operator to allow the combination of an arbitrary number of models while keeping the specification compact, and (2) The definition of an ASTD specification that represents a pattern on which to base the development of more complex systems; this specification has the following features: - Automated re-training of learning models, - Composition of a set of learning models to detect anomalies in data logs, - Combination of the decisions of each model for each event. The intent is to provide an illustrative example of specifications of anomaly detection systems that can be easily adapted for other contexts or learning methods. The paper is divided into six sections. In Section 2, we emphasize the importance of automating the renewal of the learning model in the context of dynamic data, the role of abstraction in reducing system complexity, and modularity, which facilitates maintenance and extension without introducing errors. Section 3 introduces the Quantified Flow operator as an extension of the ASTD language to easily combine an arbitrary number of learning models. In Section 4, we present a case study on the detection of unexpected events, implementing the following essential features for unsupervised anomaly detection: - Automation of retraining of learning models using the Sliding Window technique. - Model composition using the Quantified Flow operator. - Decision combination of models through Majority Voting. In section 5, we assess the performance of the specification in detecting unexpected events during a day of activity, while highlighting the effect of training data renewal and the combination of unsupervised models. Finally, in Sections 6 and 7, we summarize our findings and conclude."
https://arxiv.org/html/2411.07479v1,Developers Are Victims Too : A Comprehensive Analysis of The VS Code Extension Ecosystem,"With the wave of high-profile supply chain attacks targeting development and client organizations, supply chain security has recently become a focal point. As a result, there is an elevated discussion on securing the development environment and increasing the transparency of the third-party code that runs in software products to minimize any negative impact from third-party code in a software product. However, the literature on secure software development lacks insight into how the third-party development tools used by every developer affect the security posture of the developer, the development organization, and, eventually, the end product. To that end, we have analyzed 52,880 third-party VS Code extensions to understand their threat to the developer, the code, and the development organizations. We found that 5.6% of the analyzed extensions have suspicious behavior, jeopardizing the integrity of the development environment and potentially leaking sensitive information on the developer’s product. We also found that the VS Code hosting the third-party extensions lacks practical security controls and lets untrusted third-party code run unchecked and with questionable capabilities. We offer recommendations on possible avenues for fixing some of the issues uncovered during the analysis.","Accepting the Turing Award in 1984, Thompson projected that reliance on third-party software components involves a significant amount of trust [1]. Despite this warning and subsequent research and incidents stemming from supply chain issues [2, 3, 4, 5, 6, 7], developers’ and organizations’ reliance on third-party components continues to increase. At the same time, these components create security risks through undocumented functionality and settings, bugs, or malicious code. Supply chain attacks have emerged as a pervasive and ubiquitous threat vector in recent years [8, 9, 10]. The main reason why these types of attacks can be catastrophic is due to the nature of their coverage: numerous systems and software products may be vulnerable, often without users’, administrators’, or organizations’ knowledge, as most software products do not enumerate their constituent third-party components. Mounting an attack on several hundred organizations would otherwise be daunting; however, if the attacker can compromise one of the tools or systems used by all those organizations, then the economics suddenly shift heavily to the attacker. This has become quite apparent in the wake of several high-profile attacks [2, 6, 11]. The sheer scale of these attacks is unprecedented. However, the security community has been aware of these risks for almost 40 years. Standards, like the Software Bill of Materials (SBoM), increase transparency and awareness of the third-party code that may be executed in an organizational environment. A few of the current literature on supply chain security includes securing CI/CD [12, 13] or securing update channels [14] focusing on securing the product even before it ships out of the development organization. Security literature has looked into how developers write secure code [15] or how they look for security advice [16], which is essential in securing the whole ecosystem. However, the recent high-profile attacks on a development organization emphasize the importance of securing the roots of the supply chain: the developer [17]. While SBoM helps to articulate the composition of the software code consisting of third-party software libraries, there has been an oversight on one aspect of the development activity that involves third-party code: developer tools. Developers use tools for a wide range of activities that have a direct impact on the final software code. There is already documented evidence [18, 19] of malicious developer tools that could jeopardize the integrity of the development environment, which will lead to catastrophic events such as recent attacks [17]. However, security and privacy literature lacks systematic knowledge of how third-party developer tools behave in the wild and their impact on the security posture of the developer, the organization, and the end product. To that end, we analyzed 52,880 third-party VS Code extensions using static and dynamic analysis methods. We used static analysis to examine the code and narrow the extension list with suspicious code segments. We then executed those selected extensions using an instrumented VS Code environment, logging all their execution aspects. We also used VirusTotal [20], Retire.js [21] to scan for malicious content and known vulnerabilities. With this holistic analysis approach, we found that 5.6% 111https://github.com/vulnerability-reporter/DAV2-ACAnTVSCEE of our collected extension set has suspicious behavior that could jeopardize the integrity of the development environment and/or leak sensitive information such as code and personally identifiable information. We contribute the following: • Systematically show that the third-party developer tools pose a serious security threat to the developer, the host computer, and the organization. • VS Code, one of the most popular developer tools, has a lax security architecture that lets third-party extensions run unchecked, resulting in serious security lapses. • To the best of our knowledge, the paper presents the first holistic (52, 880) analysis of developer tools and their security and privacy implications."
https://arxiv.org/html/2411.07398v1,Beyond Keywords: A Context-based Hybrid Approach to Mining Ethical Concern-related App Reviews,"With the increasing proliferation of mobile applications in our everyday experiences, the concerns surrounding ethics have surged significantly. Users generally communicate their feedback, report issues, and suggest new functionalities in application (app) reviews, frequently emphasizing safety, privacy, and accountability concerns. Incorporating these reviews is essential to developing successful products. However, app reviews related to ethical concerns generally use domain-specific language and are expressed using a more varied vocabulary. Thus making automated ethical concern-related app review extraction a challenging and time-consuming effort.This study proposes a novel Natural Language Processing (NLP) based approach that combines Natural Language Inference (NLI), which provides a deep comprehension of language nuances, and a decoder-only (LLaMA-like) Large Language Model (LLM) to extract ethical concern-related app reviews at scale. Utilizing 43,647 app reviews from the mental health domain, the proposed methodology 1) Evaluates four NLI models to extract potential privacy reviews and compares the results of domain-specific privacy hypotheses with generic privacy hypotheses; 2) Evaluates four LLMs for classifying app reviews to privacy concerns; and 3) Uses the best NLI and LLM models further to extract new privacy reviews from the dataset. Results show that the DeBERTa-v3-base-mnli-fever-anli NLI model with domain-specific hypotheses yields the best performance, and Llama3.1-8B-Instruct LLM performs best in the classification of app reviews. Then, using NLI+LLM, an additional 1,008 new privacy-related reviews were extracted that were not identified through the keyword-based approach in previous research, thus demonstrating the effectiveness of the proposed approach.","Mobile applications are created with specific user goals in focus [1]. A user goal can be defined as any conceptual aim the given system should fulfill [2]. For instance, Sharing Economy applications (like Uber and Airbnb) aim to enhance social capital and stimulate economic development in resource-limited areas [3]. In contrast, the goal of Health&Fitness applications is to encourage healthy habits among both children and adults [4]. However, due to intense market rivalry, the app development cycle often aims to produce functional applications within brief intervals (such as days or weeks), leading developers to stray from their initial objectives frequently. These divergences frequently bring forth ethical concerns such as declining mental health, bias, privacy violations, and manipulation [5, 6, 7, 8]. Applications that fail to sufficiently consider their users’ ethical concerns are often labeled as untrustworthy or even deserted by their users [9]. Thus, for applications to endure the market’s scrutiny, developers continuously keep track of user feedback through ratings and reviews found in app marketplaces (like Google Play Store). They typically analyze user feedback to gather insights on bug reports, feature suggestions, connectivity issues, resource consumption challenges (e.g., battery life), and interface problems [10, 11, 12, 13]. Numerous studies have investigated user perspectives on ethical concerns within software applications. Research conducted by Besmer et al. [14] and Nema et al. [15] underscores users’ concerns regarding privacy breaches and data security measures in mobile applications. The emergence of discriminatory algorithms and the potential for bias in software functionalities are also significant areas of concern, as highlighted by the findings of Tushev et al. [16] and Olson et al. [17]. Furthermore, manipulative design tactics that coerce users or take advantage of psychological weaknesses are increasingly worrisome, as noted by Olson et al. [18]. However, these investigations largely depend on keyword-based sampling from app reviews, which limits the ethical issues users address to a predetermined set of terms. To overcome this limitation, Harkous et al. [19] suggest using the NLI method. However, they rely on a set of generic privacy hypotheses (derived from generic privacy concepts) overlooking the fact that users’ ethical concerns are domain-dependent [1]. For instance, individuals using ridesharing services (e.g., Uber and Lyft) may raise concerns about the constant tracking of their location, while those utilizing financial platforms (e.g., Robinhood and Coinbase) might express concerns regarding the sharing of their social security or banking details with the application. Additionally, NLI with generic hypotheses identifies a high number of false positives (FP) that require further manual analysis to identify ethical concern-related reviews [19]. To address these challenges, in this paper, we propose a novel Natural Language Processing (NLP) based hybrid approach that combines Natural Language Inference (NLI) and a decoder-only Large Language Model (LLM) to mine ethical concern-related app reviews at scale. We use NLI with domain-specific hypotheses to determine potential ethical concern-related reviews and further process these reviews using LLMs to extract ethical concern-related app reviews. The main contributions of this study can be summarized as follows. • To the best of our knowledge, this is the first hybrid approach that utilizes NLI and LLM along with domain-specific privacy hypotheses to extract ethical concern-related app reviews. NLI+LLM demonstrated better results compared to generic privacy hypotheses utilized by Harkous et al. [19]. • We develop domain-specific hypotheses based on the Mental Health (domain-specific) privacy concepts provided by Iwaya et al. [20]. • We demonstrate that our proposed hybrid approach (NLI+LLM) can extract concern-related reviews that do not contain predefined wordings used in the keyword-based method in Ebrahimi et al [1]. • We open source our source code and dataset111https://github.com/AakashSorathiya/CHyMER of 1,008 privacy-related reviews (results from our study) that remained unidentified by the previous Ebrahimi et al’s [1] study which used a keyword-based approach. The rest of the paper is organized as follows. To determine the research gaps, Section II discusses related work. Section III presents the motivation for our research through examples. We define our research questions (RQs) and explain preliminaries in Section IV and V, respectively. In Section VI, we describe the dataset and explain our methodology in Section VII. Section VIII shows and discusses the results of our investigation. Section IX lists various threats to the validity of our investigation and Section X presents concluding remarks and future directions."
https://arxiv.org/html/2411.07247v1,VIEWER: an extensible visual analytics framework for enhancing mental healthcare,"Objective: To design and implement VIEWER, a versatile toolkit for visual analytics of clinical data, and to systematically evaluate its effectiveness across various clinical applications while gathering feedback for iterative improvements.Materials and Methods: VIEWER is an open-source and extensible toolkit that employs distributed natural language processing and interactive visualisation techniques to facilitate the rapid design, development, and deployment of clinical information retrieval, analysis, and visualisation at the point of care. Through an iterative and collaborative participatory design approach, VIEWER was designed and implemented in a large mental health institution, where its clinical utility and effectiveness were assessed using both quantitative and qualitative methods.Results: VIEWER provides interactive, problem-focused, and comprehensive views of longitudinal patient data from a combination of structured clinical data and unstructured clinical notes. Despite a relatively short adoption period and users’ initial unfamiliarity, VIEWER significantly improved performance and task completion speed compared to the standard clinical information system. Users and stakeholders reported high satisfaction and expressed strong interest in incorporating VIEWER into their daily practice.Discussion: VIEWER provides a cost-effective enhancement to the functionalities of standard clinical information systems, with evaluation offering valuable feedback for future improvements.Conclusion: VIEWER was developed to improve data accessibility and representation across various aspects of healthcare delivery, including population health management and patient monitoring. The deployment of VIEWER highlights the benefits of collaborative refinement in optimizing health informatics solutions for enhanced patient care.","As the volume of information in patient records continues to grow, clinicians face an overwhelming amount of complex raw data, which can exceed the capacity of human cognition to process without error [1]. This issue of information overload has been widely noted across various care settings, including both primary and secondary care, which may result in diagnostic or treatment errors [2, 3]. Moreover, the growing demand for health information exchange among healthcare providers, and between providers and stakeholders such as public health agencies and regulators, has made patient data review and administrative reporting increasingly complex and time-consuming [4]. This is particularly evident in mental healthcare, where patients often require ongoing mental health support and coordinated care for physical comorbidities and social support [5, 6]. As a result, clinicians are forced to spend valuable time assembling disparate data points to create a coherent view for a patient’s care, which can lead to inefficiency and delayed care [7]. Although electronic health records (EHRs) are designed to help clinicians manage information at the point of care, they often fail to present information in a format that offers effective cognitive support and mitigates information overload [7]. This is largely because EHR systems mainly focus on recording information for individual patients and present data in tabular views or static text formats, with limited capability to highlight underlying trends in a patient’s disease progression, reveal similarities within a team’s caseload, or facilitate longitudinal health monitoring on a population scale [8]. The need for better methods to manage and present increasingly complex information in EHRs has been long recognised [9, 10]. Pioneering work in the 1990s introduced graphical summaries of test results and treatment data to enhance the presentation of patient status and reduce the burden of information overload [11]. Since then, research in this area has advanced by incorporating diverse datasets, including patient-generated data [8, 12] and knowledge bases [13], alongside emerging technologies, e.g. interactive visualisations [14], statistical and machine learning analytics [8], to enhance the effectiveness and usability of visual analytics in healthcare. These advancements have been applied across various domains, including patient data summarisation [7, 15], cohort search [16], care quality improvement [17], patient flow analysis [18], population health management [19], and disease- or setting-specific pathway management [18, 20, 21]. Visual analytics has emerged as a powerful tool for converting complex health data into intuitive and visually-compelling presentations, enabling the extraction of valuable insights from big data and supporting informed clinical decision making [22]. However, existing research has mainly focused on physical healthcare settings, with limited attention given to mental healthcare [23]. Compared to physical health, visual analysis of mental health data presents greater complexity for two key reasons. First, mental illnesses often encompass both medical, psychological and social dimensions [24], requiring a broader range of data to achieve a comprehensive assessment of an individual’s mental health. This highlights the need of an extensible toolkit that can facilitate the rapid design, development, and implementation of visual analytics across diverse data sources and types in mental healthcare. Second, treating mental illnesses typically involves a combination of interventions, including medication, psychotherapy, lifestyle changes, and social support services [6]. Unlike physical conditions, where structured data (such as blood assays and other characterizations) is more salient, information about mental health presentations, relevant contextual factors, interventions, and outcomes is often documented in unstructured text (e.g., clinical notes and correspondences) [25]. Integrating this unstructured information into visual analytics pipelines is more challenging compared to structured data such as numerical or categorical variables. Another notable limitation of many previous studies is their generalisability. First, most studies have focused on a specific clinical task [7, 16] or disease [18, 26]. However, in real-world clinical settings, patients rarely present with a single disease or single risk factor that may affect their health [27]. There is a lack of integrated visual-analytics solutions that can systematically address the varied challenges faced by different healthcare stakeholders, including clinicians and patients [22]. Second, existing solutions are often developed using proprietary or custom tools internally developed by hospitals or EHR system providers [7, 26], leading to limited interoperability with other tools and restricted applicability in different settings [8]. Finally, for visual-analytics solutions to achieve better outcomes and effective adoption, they should be user-friendly, require minimal training, offer evidence-based recommendations, and integrate smoothly into clinicians’ workflows [27]. Despite extensive focus on technical development in previous studies, there has been little effort to systematically develop and evaluate these solutions within the complex workflows of clinicians, nurses, and other health workers while considering their varied clinical priorities and patient needs [28]. In this work, we present VIEWER (Visual & Interactive Engagement With Electronic Records), an open-source, cost-effective, and extensible toolkit created for the rapid design, development, and deployment of clinical information retrieval, analysis, and visualisation for supporting clinical decision making. VIEWER is an EHR-agnostic framework that utilises distributed information extraction pipelines, leveraging natural language processing (NLP) methods and open-source visualisation techniques to enable comprehensive search and visual analytics of a comprehensive health record from both structured and unstructured patient data within a health institution, rather than a curated dataset for a specific disease or patient cohort. We also systematically describe our interdisciplinary and collaborative approach to participatory design where we designed, implemented and evaluated VIEWER within one of the largest National Health Services (NHS) Trusts for mental health in the United Kingdom (UK), aimed at addressing the diverse challenges faced by multiple healthcare stakeholders. The stakeholders included: (i) clinicians who need to synthesise disparate data to understand a patient’s condition within the context of their medical history, (ii) managers who require data-driven insights to optimise resource allocation and identify unmet needs, (iii) researchers who seek to understand disparities in outcomes across populations, and (iv) patients who wish to utilise their own medical data for self-monitoring. Our evaluation demonstrates the effectiveness of VIEWER in enhancing patient care in real-world clinical use cases and provides valuable insights into working collaboratively with clinical workers, researchers, patients and carers, and informaticians to iteratively refine and optimise informatics solutions for improved patient care."
https://arxiv.org/html/2411.07112v1,ROCODE: Integrating Backtracking Mechanism and Program Analysis in Large Language Models for Code Generation,"Large language models (LLMs) have achieved impressive performance in code generation recently, offering programmers revolutionary assistance in software development. However, due to the auto-regressive nature of LLMs, they are susceptible to error accumulation during code generation. Once an error is produced, LLMs can merely continue to generate the subsequent code conditioned on it, given their inability to adjust previous outputs. Existing LLM-based approaches typically consider post-revising after code generation, leading to the challenging resolution of accumulated errors and the significant wastage of resources. Ideally, LLMs should rollback and resolve the occurred error in time during code generation, rather than proceed on the basis of the error and wait for post-revising after generation. In this paper, we propose RoCode, which integrates the backtracking mechanism and program analysis into LLMs for code generation. Specifically, we employ program analysis to perform incremental error detection during the generation process. When an error is detected, the backtracking mechanism is triggered to priming rollback strategies and constraint regeneration, thereby eliminating the error early and ensuring continued generation on the correct basis. Experiments on multiple code generation benchmarks show that RoCode can significantly reduce the errors generated by LLMs, with a compilation pass rate of 99.1%. The test pass rate is improved by up to 23.8% compared to the best baseline approach. Compared to the post-revising baseline, the token cost is reduced by 19.3%. Moreover, our approach is model-agnostic and achieves consistent improvements across nine representative LLMs.","As modern software architectures continue to increase in size and complexity, the burden on developers to construct and maintain these systems has become substantial. Given that programs serve as the fundamental carriers of software functionality, the automation of their generation is of paramount importance. Code generation technology, which seeks to automatically produce programs that align with human intentions, has emerged as a focal area of interest within both academia and industry fields [1, 2, 3, 4]. In recent years, large language models (LLMs) have rapidly advanced and achieved significant success in the domain of automated code generation [5, 6, 7, 8, 9]. A well-known tool for code generation based on LLMs is Copilot [10], which has demonstrated its utility by generating code that can be accepted by more than 30% of its users [11]. Figure 1: Statistics on the types of errors in code generated by LLM. The statistics are conducted based on the results generated by CodeLlama-7B and CodeGen-6B on HumanEval and MBPP benchmarks using greedy decoding. Typically, LLMs adopt an auto-regressive approach, where the output at each step is conditioned on the outputs of previous steps. Once an error occurs during the generation process at any step (for example, the selection of an inappropriate token due to hallucinations111The hallucination in code generation manifests as generated code that violates programming principles, resulting in code that cannot be compiled or executed, or that is inconsistent with user requirements or context, leading to failed tests [12]. Recent research has demonstrated that all computable LLMs cannot prevent themselves from hallucinating [13]. [14]), this error will be included in the context of the subsequent steps. This phenomenon can cause errors to accumulate and amplify their impact, potentially causing the generated content to completely deviate from the expected path [15, 16]. Moreover, the generation process of LLMs differs significantly from the common practice of reviewing and adjusting existing code in human coding. In practice, developers are able to adjust their code whenever necessary based on its quality and its alignment with requirements, while LLMs can merely proceed based on the output generated so far and are unable to adjust previous outputs spontaneously. Recent studies [17, 18, 19] have attempted to utilize the LLMs to revise their output after generation in a post-revising manner. However, this type of approach faces difficulties in revising the accumulated errors [20] and can result in resource wastage [17]. Ideally, through incorporating a backtracking mechanism into the generation process, we can expose potential errors early and resolve them, effectively preventing error propagation. However, to effectively implement backtracking, three key issues ought to be addressed: 1) When to roll back. During the generation process, the rollback is triggered depending on when errors are detected. Error detection during the generation of LLMs should satisfy the following conditions. First, it must be capable of performing real-time checks on incomplete code; second, it is required to cover common errors produced by LLMs which are shown in Figure 1; finally, its running speed would be better to fast enough so as not to affect the efficiency of LLM significantly. 2) Where to roll back to. Simply rolling back to the last error-free state of the generated code usually does not address the issue. We should identify the initial decision point that caused the error and roll back to that point. Determining the rollback point is a complex decision-making process because the meaning and behavior of the erroneous code depend not only on itself, but also on interactions with preceding code, which are influenced by factors such as variable scopes, state dependencies, and logical dependencies within the program. 3) How to avoid previous errors. After the rollback, the key task during regeneration is to prevent the recurrence of previous errors. However, completely prohibiting the LLMs from generating previously erroneous code may inadvertently block benign tokens. Thus, it is essential to impose appropriate constraints on the regeneration process. To address the preceding three issues, we first implement incremental error detection using program analysis, which enables the examination of incomplete code to identify potential errors. Compilers can be used not only in code transformation for execution but also as an effective tool for program analysis. It is capable of performing numerous key and common analyses such as syntax parsing, type checking, and dependency analysis, and they have been optimized for speed over many years. Moreover, by using compilers, we can design new analyses for specific errors in LLMs’ generated code, such as checking for code repetition problems. Second, for determining rollback points, program analysis serves as an external inspection during the generation of LLMs, providing essential error information. However, this error information may not directly pinpoint the root cause of the error. In contrast, the inherent uncertainty of LLMs is proven to be usable for self-assessment during generation [21, 22, 23], which can aid in tracing the root cause of errors. Therefore, combining these two sources of information facilitates determining rollback points. Third, in regeneration with constraints, we decay the generation probability of the paths leading to error progressively. Moreover, by modeling the entire generation process with tree structures, it is feasible to comprehensively account for all historical errors and to effectively superimpose penalties for them. In this paper, we propose RoCode, a novel code generation approach that integrates backtracking mechanism and program analysis to LLMs. The core of our approach – the backtracking mechanism detects errors in real-time, rolls back, and regenerates with constraints during the generation process of LLMs, thus preventing error accumulation and enhancing the performance and efficiency of code generation. Specifically, we employ program analysis to perform incremental error detection during the code generation process to discover errors timely. Based on the results of program analysis and the observation of uncertainty in the generation of LLMs, we design a series of rollback strategies to determine the rollback point. To constrain the process of regeneration, we strategically penalize the likelihood of tokens that have contributed to previous errors. Further, given that the introduction of rollback and regeneration makes code generation no longer follow a linear path, we use a Trie Tree to model the whole generation process of RoCode. Importantly, our approach is model-agnostic, and requires no additional training. Our experimental results demonstrate that RoCode consistently outperforms all baselines across six code generation benchmarks. RoCode achieves a compilation success rate of 99.1% and surpasses the best-performing baseline by 23.8% in pass rate. To further demonstrate its utility, we apply RoCode to multilingual code generation tasks and achieve a relative improvement of 34.2% in pass rate. We also explore generalizability of RoCode across various LLMs, revealing significant enhancements in the performance of both general LLMs and code LLMs, with an average improvement of 18.2% in pass rate. In terms of cost and performance, RoCode reduces token costs by 19.3%, compared to the Post-revising approach. Furthermore, the ablation studies reveal that incremental error detection, rollback strategies, and constraint regeneration in RoCode all contribute to performance improvement. To the best of our knowledge, this work is the first to introduce and implement the rollback approach for code generation during the decoding process in LLMs222Code is available at https://github.com/jiangxxxue/ROCODE.."
https://arxiv.org/html/2411.07098v1,"Adaptive REST API Testing with 
Reinforcement Learning","Modern web services increasingly rely on REST APIs. Effectively testing these APIs poses challenges due to the vast search space to explore, which involves selecting API operations for sequence creation, choosing parameters for each operation from a potentially large set, and sampling values from the often infinite parameter input space. Current testing tools lack efficient exploration mechanisms, treating all operations and parameters equally without considering their importance or complexity and lacking prioritization strategies. Furthermore, these tools struggle when response schemas are absent in the specification or exhibit variants. To address these limitations, we present an adaptive REST API testing technique that incorporates reinforcement learning to prioritize operations and parameters for exploration. Our approach dynamically analyzes request and response data to inform dependent parameters and adopts a sampling-based strategy for efficient processing of dynamic API feedback. We evaluate our technique on ten RESTful services, comparing it against state-of-the-art tools with respect to code coverage achieved and the number of generated requests, operations covered, and service failures triggered. Additionally, we perform an ablation study on prioritization, dynamic feedback analysis, and sampling to assess their individual effects. Our findings demonstrate that our approach significantly outperforms existing REST API testing tools in terms of effectiveness, efficiency, and fault-finding ability.","The increasing adoption of modern web services has led to a growing reliance on REpresentational State Transfer (REST) APIs for communication and data exchange [richardson2013restful, patni2017pro]. REST APIs adhere to a set of architectural principles that enable scalable, flexible, and efficient interactions between various software components through the use of standard HTTP methods and a stateless client-server model [fielding2000architectural]. To facilitate their discovery and use by clients, REST APIs are often documented using specification languages [openapi, swagger, raml, apiblueprint], which let developers describe the APIs in a structured format and provide essential information, such as the available endpoints, input parameters and their schemas, response schemas, etc. Platforms such as APIs Guru [apis_guru] host thousands of RESTful API documents, emphasizing the significance of these standardized API specifications in industry. Standardized documentation formats, such as the OpenAPI specification [openapi], not only facilitate the development of APIs and their use by clients, but also provide a foundation for the development of automated testing techniques for REST APIs, and numerous such techniques and tools have emerged in recent years (e.g., [arcuri2019restful, Corradini2022, atlidakis2019restler, martin2020restest, karlsson2020automatic, karlsson2020quickrest, liu2022morest, wu2022combinatorial]). In spite of this, effectively testing REST APIs continues to be a challenge, with high code coverage remaining an elusive goal for tools [kim2022automated]. Testing REST APIs can be challenging because of the large search space for exploration, arising from numerous operations, potential execution orders, inter-parameter dependencies, and associated input parameter value constraints [martin2019catalogue]. Current techniques often struggle to explore this space due to the lack effective exploration strategies for operations and their parameters. Existing testing tools tend to treat all operations and parameters equally, disregarding their importance or complexity, leading to suboptimal testing strategies and insufficient coverage of crucial operation and parameter combinations. Moreover, these tools rely on discovering producer-consumer relationships between response schemas and request parameters, which works well when the parameter and response schemas are described in detail in the specification. However, if the schemas are incomplete or imprecise, the tools can become less effective in their exploration. In this paper, we present adaptive REST API testing with reinforcement learning (arat-rl), an advanced black-box testing approach that addresses these limitations of existing tools. Our technique incorporates several innovative features, such as leveraging reinforcement learning to prioritize operations and parameters for exploration, dynamically constructing key-value pairs from both response and request data, analyzing these pairs to inform dependent operations and parameters, and utilizing a sampling-based strategy for efficient processing of dynamic API feedback. The primary objective of our approach is to increase code coverage and improve fault-detection capability. The core of novelty in arat-rl is an adaptive testing strategy, driven by a reinforcement-learning-based prioritization algorithm for exploring the space of operations and parameters. The algorithm initially determines an operation’s importance based on the parameters used and their frequencies across other operations. This targeted exploration enables efficient coverage of critical operations and parameters, thereby optimizing code coverage. The technique employs reinforcement learning to adjust the priority weights associated with operations and parameters based on feedback, by decreasing importance for successful responses and increasing it for failed responses. The technique also assigns weights to parameter-value mappings based on various sources of input values (e.g., random, specified values, response values, request values, and default values), which lets it adapt the testing strategy and concentrate on areas more likely to contain faults, ultimately enhancing fault-detection capability. Another innovative feature of arat-rl is dynamic construction of key-value pairs. In contrast to existing approaches that rely heavily on resource schemas provided in the specification, our technique dynamically constructs key-value pairs by analyzing POST operations (i.e., resource creation HTTP method) and examining both response and request data. For instance, support that an operation takes book title and price as request parameters and, as response, produces a success status code along with a string message (e.g., “Successfully created”). Our technique leverages this information to create key-value pairs for book title and price, upon receiving a successful response, even without such data being present in the response. It takes into account the input parameters used in the request, as they correspond to the created resource. Moreover, if the service returns incomplete resources, our technique still processes the information available in key-value pairs. This dynamic approach enables our tool to identify resources from the API responses and requests and discover hidden dependencies that are not evident from the specification alone. Finally, arat-rl employs a simple yet effective sampling-based approach that allows it to process dynamic API feedback efficiently and adapt its exploration based on the gathered information. By randomly sampling key-value pairs from responses, our tool reduces the overhead of processing every response for each pair, resulting in more efficient testing and optimized utilization of testing resources. To evaluate the technique, we conducted empirical studies using 10 RESTful services and compared it against three state-of-the-art REST API testing tools: RESTler [atlidakis2019restler], EvoMaster [arcuri2019restful], and Morest [liu2022morest]. We assessed the effectiveness of arat-rl in terms of coverage achieved and service failures triggered, and its efficiency in terms of valid and fault-inducing requests generated and operations covered within a given time budget. Our results show that arat-rl outperforms the competing tools in all the metrics considered—it achieved the highest method, branch, and line coverage rates, along with better fault-detection ability. Specifically, arat-rl covered 119.17%, 59.83%, and 52.42% more branches, lines, and methods than RESTler; 37.03%, 20.87%, and 14.13% more branches, lines, and methods than EvoMaster; and 23.69%, 11.87%, and 9.55% more branches, lines, and methods than Morest. arat-rl also uncovered 9.2x, 2.5x, and 2.4x more bugs than RESTler, EvoMaster, and Morest, respectively. In terms of efficiency, arat-rl generated 52.01%, 40.79%, and 1222% more valid and fault-inducing requests and covered 15.38%, 24.14%, and 282.98% more operations than Morest, EvoMaster, and RESTler, respectively, in a one-hour testing time budget. We also conducted an ablation study to assess the individual effects of prioritization, dynamic feedback analysis, and sampling on the overall effectiveness of arat-rl. Our results indicate that reinforcement-learning-based prioritization contributes the most to arat-rl’s effectiveness, followed by dynamic feedback analysis and sampling in that order. ⬇ /products/{productName}/configurations/{configurationName}/features/{featureName}: post: operationId: addFeatureToConfiguration produces: - application/json parameters: - name: productName in: path required: true type: string - name: configurationName in: path required: true type: string - name: featureName in: path required: true type: string responses: default: description: successful operation /products/{productName}/configurations/{configurationName}/features: get: operationId: getConfigurationActivedFeatures produces: - application/json parameters: - name: productName in: path required: true type: string - name: configurationName in: path required: true type: string responses: ’200’: description: successful operation schema: type: array items: type: string Figure 1: A Part of Features-Service’s OpenAPI Specification. The main contributions of this work are: • A novel approach for adaptive REST API testing that incorporates (1) reinforcement learning to prioritize exploration of operations and parameters, (2) dynamic analysis of request and response data to inform dependent parameters, and (3) a sampling-based strategy for efficient processing of dynamic API feedback. • Empirical results demonstrating that arat-rl outperforms state-of-the-art REST API testing tools in terms of requests generated, code coverage achieved, and service failures triggered. • An artifact [artifact] containing the tool, the benchmark services, and the empirical data."
https://arxiv.org/html/2411.07091v1,"Impact of LLM-based Review Comment Generation in Practice:
A Mixed Open-/Closed-source User Study","Code review is a standard practice in modern software development, aiming to improve code quality and facilitate knowledge exchange among developers. As providing constructive reviews on a submitted patch is a challenging and error-prone task, the advances of Large Language Models (LLMs) in performing natural language processing (NLP) tasks in a human-like fashion have prompted researchers to evaluate the LLMs’ ability to automate the code review process. However, outside lab settings, it is still unclear how receptive reviewers are to accept comments generated by LLMs in a real development workflow. To fill this gap, we conduct a large-scale empirical user study in a live setup to evaluate the acceptance of LLM-generated comments and their impact on the review process. This user study was performed in two organizations, Mozilla (which has its codebase available as open-source) and Ubisoft (fully closed-source). Inside their usual review environment, participants were given access to RevMate, an LLM-based assistive tool suggesting generated review comments using an off-the-shelf LLM with Retrieval-Augmented Generation to provide extra code and review context, combined with LLM-as-a-Judge, to auto-evaluate the generated comments and discard irrelevant cases. Based on more than 587 patch reviews provided by RevMate, we observed that 8.1% and 7.2%, respectively, of LLM-generated comments were accepted by reviewers in each organization, while 14.6% and 20.5% other comments were still marked as valuable as review or development tips. Refactoring-related comments are more likely to be accepted than Functional comments (18.2% and 18.6% compared to 4.8% and 5.2%). The extra time spent by reviewers to inspect generated comments or edit accepted ones (36/119), yielding an overall median of 43s per patch, is reasonable. The accepted generated comments are as likely to yield future revisions of the revised patch as human-written comments (74% vs 73% at chunk-level).","Software code review is a core practice for modern software quality assurance [1, 2], widely adopted in industrial and open-source projects [3, 4]. While, initially, code review mostly was a synonym for code inspections on a submitted patch (structured as a set of chunks, i.e., successive lines of modified code) [5, 6], the field gradually adopted a more dynamic approach to performing reviews commonly known as modern code reviews [7], embracing social dimensions, like facilitating knowledge transfer between developers and strengthening synergy within teams [8]. Despite such benefits, code reviews can also bring additional costs, due to the delay between a patch submission and its final approval for integration by the reviewers caused by back-and-forth between its author and reviewers [9]. Additionally, providing valid and effective reviews requires non-trivial efforts from reviewers in terms of technical, social, and personal aspects [10]. Reviewers need to understand and rationalize the overall impact of the changes under analysis while using effective communication [11, 12, 13]. Reviewers, even when qualified and focused on a patch, might miss issues due to fatigue [14], distraction, or pressure from other deadlines [15]. To overcome these limitations, the research community has explored different approaches to generate review comments with LLM-based approaches such as T5, CodeT5 and ChatGPT[16, 17, 18]. However, thus far these have only been evaluated using textual metrics such as BLEU score and Exact Match, which are imperfect as they do not capture the actual impact and synergy the approach can have on reviewers [16]. Furthermore, it is unclear how reviewers receive LLM-generated comments in the long-term, i.e., apart from their relevance, to what extent do generated comments solicit follow-up comments by the patch author or lead to follow-up patch revisions? To fill in this gap, we conducted the first large-scale user study in a live setup involving two organizations, Mozilla and Ubisoft. Our choice for both an open-source (Mozilla) and closed-source (Ubisoft) organization is deliberate, since open-source codebases risk being part of the pretraining data of LLMs like GPT4o, yielding an unfair advantage in empirical studies. Hence, this study also aims at investigating whether open-source organizations actually benefit from this potential advantage compared to closed-source organizations. Spanning over 6 weeks, our user study involved 59 reviewers, covered 587 patch reviews, and led to the evaluation of 1.6k generated comments. During this study, we monitored the reviewers’ interactions with the generated comments, assessed the impact of these comments on the review flow, and finally conducted a survey of the participating reviewers to gain insight into their perspectives, filled by 37/59 participants. For this evaluation, we built RevMate, an LLM-based review assistant that generates review comments and that is easy to integrate into modern review environments. RevMate builds on GPT4o and uses both (i) Retrieval Augmented Generation (RAG) [19] to enclose relevant information and ground the model on the project under analysis, and (ii) LLM-as-a-Judge [20] to leverage LLMs’ capacity to evaluate generated content and discard irrelevant review comments. In particular, we exploit two different approaches for RevMate: RevMate with extra code context (Code): inspired by Zhou et al.’s approach for code generation [21], the model can request a code retrieval tool to provide function definitions and additional code lines from the codebase under analysis. RevMate with related comment examples (Example): like Parvez et al.’s approach for code summarization [22], for each patch, the model dynamically selects few-shot examples of review comments similar to its chunks, then provides them to the LLM. This paper addresses the following research questions: RQ1: How frequently do reviewers accept comments generated by an LLM-based approach? Based on the participants’ feedback in both organizations, we found that 8.1% and 7.2% of comments are accepted by Mozilla and Ubisoft reviewers, respectively, with 23% and 28.3% other comments still appreciated as being helpful. Despite the organizations differences in data availability, both the acceptance and appreciation ratios were consistent among them, indicating that the results may generalize across closed- and open-source development processes. Across both organizations, 23/37 participants reported they would continue using RevMate at least sometimes outside of the study context. RQ2: How does the comment category correlate with acceptance ratios? We found that, for Mozilla and Ubisoft respectively, 79.2% and 84.9% of generated comments are classified as functional issues, and 15.8% and 14.5% as refactoring-related comments. Furthermore, we found that refactoring-related comments have higher acceptance ratios than functional comments (18.2% and 18.6% compared to 4.8% and 5.2%). RQ3: How does the adoption of LLM-generated comments impact the code review workflow? We find that, although reviews take longer due to the extra time needed to evaluate the generated comments, the time spent per generated comment is reasonable (43s as a median per patch). This time is mostly spent on investigating the validity of the comment and on editing the comment when applicable, as 37/119 accepted comments were edited, with 25/37 cases being shortening the comment. RQ4: How does the adoption of LLM-generated comments impact the patch’s review process? This RQ assesses the extent to which accepted generated comments lead to changes in the codebase. We found that accepted review comments lead to as many patch revisions (i.e., changes in a future version of the patch) as human comments (74% vs 73% at chunk level) in Ubisoft, indicating that the generated comments have the same impact as human comments. Also, generated comments lead to fewer follow-up comments among developers (23% compared to 34%). The main contributions of this study are: • A large-scale mixed closed-/open-source user study, with 57 expert reviewers, assessing the usefulness of LLM-generated review comments and their impact on the review flow and outcome; • A diverse set of 3.4k generated code review comments by GPT4o for the Mozilla organization, among which 426 were evaluated by reviewers [23]; • An open source project implementing the comment review generation approach [24]."
https://arxiv.org/html/2411.07012v1,What Do Developers Discuss in Their Workplace? An Analysis of Workplace StackExchange Discussions,"Software workplaces are increasingly recognized as key spaces for professional development, where developers encounter various challenges in their roles, which they often discuss in online forums. This paper analyzes 47,368 posts on the Workplace StackExchange site, aggregating developer insights and applying topic modeling techniques. Through manual analysis, we identified 46 distinct topics grouped into seven categories: Employee Wellness, Communication, Career Movement & Hiring, Conflicts & Mistakes, Corporate Policies, Management/Supervisor Responsibilities, and Learning & Technical Skills. Our findings show that approximately 30% of discussions involve workplace conflicts, marking this as the most prominent topic. Additionally, we found that workplace culture, harassment, and other corporate policy-related issues represent significant areas of difficulty commonly discussed among developers.","The software industry has experienced unprecedented growth over the past few decades, driven by rapid technological advancements and an increasing reliance on digital solutions across all sectors. This expansion has resulted not only in a rise in the number of software companies but also in a transformation of software workplaces, compelling organizations to adapt to new workforce dynamics and operational models [1]. As software becomes central to nearly every industry, companies must support a workforce capable of keeping up with technological advancements. This has heightened challenges around skill development, employee well-being, and retention [2]. Additionally, as software companies grow and globalize, creating inclusive and culturally adaptable workplaces has become a priority. Companies increasingly recognize that diversity drives innovation and enhances problem-solving, leading to initiatives such as mentorship programs for women, flexible work policies, and equal opportunity efforts [3, 4]. Online software workplace discussions have become integral to how professionals in the software industry collaborate and share knowledge. As the software landscape evolves rapidly, these discussions facilitate continuous learning, problem-solving, and networking among developers, engineers, and other stakeholders. Moreover, online discussions play a crucial role in addressing workplace-related topics, such as team dynamics, conflict resolution, and career advancement strategies. Engaging in these discussions allows software professionals to better navigate workplace complexities and improve job satisfaction [5]. As organizations increasingly adopt remote work and flexible arrangements, online forums play a crucial role in maintaining collaboration and cohesion across distributed teams. Motivated by this, we conducted a large-scale analysis of developer online discussions on workplace-related issues. Specifically, we analyzed 47,368 posts on the Workplace StackExchange site, a question-and-answer platform dedicated to workplace and career topics. Workplace is a large online community where thousands of developers ask and answer questions on various issues, including job hunting, interviewing, salary negotiation, and professionalism in the workplace [6]. We aggregated developer discussions and applied topic modeling techniques. Workplace is part of the Stack Exchange network, a broad online community where millions of developers engage in Q&A discussions. In this study, we address the following four research questions: RQ1 What workplace topics are discussed on the Workplace Stack Exchange? We find that developers discuss a wide range of workplace-related topics. Their discussions on Stack Exchange’s Workplace encompass 46 different topics, including employment contracts, technical skills, team organization, work conflicts, workplace satisfaction, communication tips, and company policies. Our analysis reveals that work conflicts account for the highest number of posts, followed by resignation and job change. These topics belong to seven categories, Employee Wellness, Communication, Career Movement & Hiring, Conflicts & Mistakes, Corporate Policies, Management/Supervisor Responsibilities, and Learning & Technical Skills. RQ2 How do the topics evolve over time? We find that topics related to conflicts and mistakes, as well as career movement and hiring, have the highest number of posts and consistently maintain the highest levels of activity over the years compared to other categories. RQ3 How do the topics vary in terms popularity and difficulty? We observe that topics vary significantly in popularity. Work Conflicts is the most popular topic, with the highest number of views, while the technical topic Data Science is the least popular and also the most difficult, as its questions take the longest to receive an accepted answer. Work Culture and Harassment are also among the most difficult topics, whereas New Graduate Jobs and Languages are the least difficult. RQ4 How does user activity and gender distribution vary across The Workplace discussion community? Since Workplace is a community of users interested in discussing workplace-related issues, we aim to understand user characteristics. We found that the top 20% of users contribute to 17.2% of the total posts in the dataset. Additionally, we found that men significantly outnumber women, with about 14% of identified users being women and 86% being men. The remainder of this paper is organized as follows. The next section discusses related work. Section 3 covers the methodology and data collection. Section 4 presents the results related to the research questions. Section 5 discusses threats to validity. Section 6 concludes the paper."
https://arxiv.org/html/2411.06796v1,Automatically Write Code Checker: An LLM-based Approach with Logic-guided API Retrieval and Case by Case Iteration,"With the rising demand for code quality assurance, developers are not only utilizing existing static code checkers but also seeking custom checkers to satisfy their specific needs. Nowadays, various code-checking frameworks provide extensive checker customization interfaces to meet this need. However, both the abstract checking logic as well as the complex API usage of large-scale frameworks make this task challenging. To this end, automated code checker generation is anticipated to ease the burden of checker development. In this paper, we explore the feasibility of automated checker generation and propose AutoChecker, an innovative LLM-powered approach that can write code checkers automatically based on only a rule description and a test suite. Instead of generating the checker at once, AutoChecker incrementally updates the checker with the rule and one single test case each time, i.e., it iteratively generates the checker case by case. During each iteration, AutoChecker first decomposes the whole logic into a series of sub-operations and then uses the logic-guided API-context retrieval strategy to search related API-contexts from all the framework APIs. To evaluate the effectiveness of AutoChecker, we apply AutoChecker and two LLM-based baseline approaches to automatically generate checkers for 20 built-in PMD rules, including easy rules and hard rules. Experimental results demonstrate that AutoChecker significantly outperforms baseline approaches across all effectiveness metrics, where its average test pass rate improved over 4.2 times. Moreover, the checkers generated by AutoChecker are successfully applied to real-world projects, matching the performance of official checkers.","Static code-checking tools play a crucial role in ensuring code quality by automatically generating security reports based on a set of predefined rules. Ideally, users should be able to utilize any tool to meet their specific checking requirements (Distefano et al., 2019). To achieve this, many code-checking tools strive to develop extensible frameworks that allow users to scan code with their custom checkers. For example, PMD (PMD, 2024a) and SonarQube (Son, 2024) are well-designed to support secondary development, on which users can write custom checkers in Java code based on their checking rules; CodeQL (cod, 2024a) and other DSL-based tools (Xie et al., 2024) also encourage users’ customization, which ask users to “write queries to find vulnerabilities based on a powerful query language and share queries to help others do the same (cod, 2024b)”. However, due to the high complexity of existing checking frameworks (Brown et al., 2016) (e.g., the PMD’s framework excluding its pre-designed checkers exceeds 30 KLOC), the massive framework-specific API knowledge and incomplete API documents, as well as the non-trivial checking logic to be implemented, checker customization is still a time-consuming and challenging task. It bothers the users who are not familiar with specific code-checking frameworks but have urgent customization demands. Nowadays, the booming of Large language Models (LLMs) has stimulated the development of automatic code generation. Inspired by this, we aim to explore the auto-generation of checker code powered by LLMs to help users analyze software code more easily and flexibly. According to our investigation, existing code generation tasks can be categorized into the following two types. • Non-project-specific code generation, e.g., common code completion (Lu et al., 2022; Guo et al., 2023), common patch generation (Xia et al., 2023; Lin et al., 2024), common code generation for algorithmic questions (Zheng et al., 2023; Fan et al., 2023), etc. • Project-specific code generation, e.g., project-specific code completion (Zhang et al., 2023a; Agrawal et al., 2023; Cheng et al., 2024; Wu et al., 2024), code repair during project evolution (Liu et al., 2024; Prenner and Robbes, 2024), etc. Compared to non-project-specific one, project-specific code generation is an upgraded task, where the target code can access specific classes and methods defined in the project. Therefore, it requires project-specific knowledge (classes and methods defined in the projects) for correct generation. Otherwise, there may be unexpected hallucinations during code generation. Generally speaking, checker generation belongs to project-specific code generation. Each checker is defined and executed on the code checking framework, which is the project. However, it is much harder than previous studied tasks. Fig. 1 gives the overall pipeline of the manual checker development process. When the project manager has checker customization demands, they must provide a rule description to the developers, which is often unclear and not formalized. To clarify the rule, they should also provide a test suite to cover all possible scenarios (including corner cases). Based on this information, the developers will summarize the correct and comprehensive checking logic derived from the rule description and the test suite. Then, they will implement the logic to write the checker using framework APIs based on their knowledge of the framework. Unlike previous code generation tasks with clear guidance, the non-trivial checking logic and massive available framework APIs make it harder to correctly generate an expected checker. Figure 1. Pipeline of the Manual Checker Development According to the pipeline of manual checker development, we have to first cope with the following two main challenges that a developer may also encounter while writing new checkers. C1: Complex checking logic. Summarizing the comprehensive checking logic is crucial for checker generation. Both the rule description (for the rough goal) and the test suite (covering all scenarios) should be considered as input. However, complex checking logic may lead to excessive input information, including numerous test cases. On the one hand, it will overwhelm the LLM’s ability to generate the thorough logic considering all the scenarios. On the other hand, it may reach the LLM’s token limit. Therefore, complex checking logic is challenging to be summarized in one time. C2: Unclear framework API knowledge. When customizing code checkers, developers should have a strong understanding of the APIs available in their chosen checking framework. As one code-checking framework may contain thousands of APIs but only a few are required for one checker, automatic checker generation may lack the knowledge of which APIs can be invoked and how these APIs should be used, which will lead to hallucinations during generation. To address the above challenges, we propose a novel approach to automatically write static checkers based on iterative test-driven checker generation with logic-guided API-context retrieval and implement it as AutoChecker. First, to obtain the comprehensive checking logic covering complex scenarios, we iteratively generate and refine the checker, with each iteration guided by the given rule and a single selected test case (the initial and failed test case) from the test suite (C1). This approach is inspired by the manual checker development process shown in Fig. 1, where developers will validate the checker on the test suite and update it referring to the failed test cases iteratively until passing all the tests. During the evolving process, AutoChecker is able to cover the full test suite case by case and generates the comprehensive checking logic. Second, during each iteration for checker generation, AutoChecker adopts logic-guided API-context retrieval to extract task-oriented framework knowledge (definitions and usages of framework APIs) as references (C2). Specifically, AutoChecker decomposes the checking logic into discrete sub-operations and subsequently retrieves the corresponding API contexts for each individual sub-operation, which is performed on our constructed API-context Database, Meta-Op DB (semi-automatically) and Custom-API DB (automatically). With this logic-guided retrieval, AutoChecker is able to extract precise API-contexts for the checker generation. To evaluate the effectiveness of AutoChecker, we select 20 PMD rules, with an equal distribution of 10 easy and 10 hard rules. Experimental results show that AutoChecker outperforms baseline methods both on compilation and test pass rates. Specifically, the checkers generated by AutoChecker for all the rules can be successfully compiled and achieve average test pass rate at 82.28% (84.70% in easy rules and 79.86% in hard ones), which is 4.3 times higher than NoCaseLLM and 4.2 times higher than AllCasesLLM. Furthermore, we conducted a practicality evaluation by selecting AutoChecker-generated checkers that passed all tests and applying them to five large-scale real-world Java projects. The results indicate that AutoChecker can generate checkers that perform equivalently to the ground truth when sufficient test cases are provided. We make the following contributions in this paper: • We propose an automatic test-driven checker-generation approach, which uses an iterative generation pipeline to cope with the complex checking logic case by case. • We develop a logic-guided API-context retrieval strategy, and design a general Meta-Op set for better API retrieval, which contains 354 atomic checking operations. • We implement our approach into AutoChecker, which can automatically write custom code checkers based on the given rule and test suite. The experimental results show that the checkers generated by AutoChecker greatly outperform baseline approaches across all effectiveness metrics. Comparable to the official checkers, they also achieve expected results on large-scale real-world projects. Both the code and the dataset of AutoChecker are available at https://anonymous.4open.science/r/AutoChecker-EF16/."
https://arxiv.org/html/2411.06774v1,The First Prompt Counts the Most! An Evaluation of Large Language Models on Iterative Example-based Code Generation,"The capabilities of Large Language Models (LLMs) in code generation, particularly for implementing target functionalities from natural language descriptions, have been extensively studied. As an alternative form of natural language, input-output examples (I/O examples) provide an accessible, unambiguous, and flexible way to describe functionalities, but the diversity, sparseness, and incompleteness of I/O examples also place challenges on understanding and implementing requirements. Therefore, generating code from input-output examples (i.e., example-based code generation) provides a new perspective, allowing us to evaluate LLMs’ capability to infer target functionalities from limited information and to process new-form requirements. However, related research about LLMs in example-based code generation remains largely unexplored. To fill this gap, this paper presents the first comprehensive study on example-based code generation using LLMs. To address the incorrectness caused by the incompleteness of I/O examples, we adopt an iterative evaluation framework and formalize the objective of example-based code generation as two sequential sub-objectives: generating code conforming to given examples and generating code that successfully implements the target functionalities from (iteratively) given examples. We assess six state-of-the-art LLMs using a new benchmark of 168 diverse target functionalities (derived from HumanEval and CodeHunt). The results demonstrate that when requirements were described using iterative input-output examples rather than natural language, the LLMs’ score decreased by over 60%, indicating that example-based code generation remains challenging for the evaluated LLMs. More interestingly, the vast majority (even over 95%) of successfully implemented functionalities are achieved in the first round of iterations, suggesting that the LLMs struggle to effectively utilize the iteratively supplemented requirements. These findings highlight the importance of early prompts during interactions and offer critical insights and implications for enhancing LLM-driven code generation.","Code generation has been recognized as one of the most important and promising applications of large language models (LLMs) (Ni et al., 2023). State-of-the-art LLMs, e.g., Llama (Touvron et al., 2023; Dubey et al., 2024; Rozière et al., 2024), Gemma (Team et al., 2024b, c, a), DeepSeek (Guo et al., 2024; DeepSeek-AI et al., 2024), ChatGPT (Schulman et al., 2022), and GPT4 (OpenAI et al., 2024), have shown impressive capabilities in generating executable programs from prompts detailing target functionalities. For each target functionality, the prompt typically comprises a natural-language description, and may sometimes include supplementary information such as input-output examples (I/O examples) and function signatures (Li et al., 2023; Hendrycks et al., 2021; Austin et al., 2021; Chen et al., 2021a; Lai et al., 2022; Hao et al., 2022; Cassano et al., 2022; Li et al., 2022b; Du et al., 2023). In addition to natural languages, I/O examples also provide an easily accessible, unambiguous, and flexible way to describe the target functionalities. First, I/O examples offer a straightforward and user-friendly alternative when natural-language descriptions are unavailable. For end-users who lack programming expertise and usually struggle to articulate functionalities effectively, I/O examples allow them to communicate their intentions simply (Gulwani, 2016). For reverse engineering tasks (Hajipour et al., 2021) (e.g., binary de-obfuscation (David et al., 2020)) whose goal is to reproduce existing programs or interfaces with unknown functionalities, I/O examples can be iteratively gathered through interactions to reveal these functionalities. Second, I/O examples are concrete and precise, able to reduce misunderstanding in functional descriptions (Balog et al., 2017; Gulwani et al., 2015). Specifically, I/O examples clearly illustrate the expected outputs for specific inputs, offering clear guidance on program behavior (Calais and Franzini, 2023). This characteristic allows I/O examples in functional descriptions to directly serve as tests, enabling automated and efficient correctness checking of the generated code (Janzen and Saiedian, 2005; Ramzan et al., 2024). Third, I/O examples can be dynamically updated to clarify, refine, or expand the target functionalities. For instance, both the failing tests and the observed edge cases during development can be used to create new I/O examples, aiding in the adaptive clarification and refinement of functionality descriptions (Janzen and Saiedian, 2005; Fucci et al., 2016). Functional descriptions in the form of I/O examples present three additional challenges for code generation tasks. First, I/O examples are not frequently included in the training data for code generation (Wen et al., 2024), posing difficulties for models in understanding the requirements conveyed in this form. Specifically, I/O examples are often limited in quantity and typically appear in test cases or supplements to the natural-language descriptions. Compared to the potentially extensive input space, these examples can cover only a small fraction. Second, a single set of I/O examples usually cannot completely specify functionalities, requiring the code generator to iteratively receive supplementary prompts and refine their code. In extreme scenarios, code can be written to simply match each input with a branch to satisfy the given examples, but such solutions cannot achieve the target functionality. Therefore, iteratively supplemented prompts are crucial for correct implementations, and the models need to glean valuable insights from these prompts for improvement. Third, I/O examples do not explicitly state how to derive the expected outputs, placing a high demand on the inferring and generalizing capability of the code generator. Without hints about the structure or logic of the code, LLMs must deduce the underlying transformation from a limited number of I/O examples and apply them across diverse contexts. This process requires not only a thorough understanding of programming knowledge but also the capacity to extrapolate from limited data. For tasks involving multiple steps or complex data manipulations, the process becomes particularly challenging. To investigate the potential of LLMs in code generation from I/O examples (aka example-based code generation), this paper conducts the first comprehensive study (to the best of our knowledge). When describing the functionality, I/O examples usually face the issue of their inherent incompleteness because they hardly achieve a comprehensive sampling of the input space. To cope with this issue, we refine the objective of example-based code generation into two sequential sub-objectives, where the first serves as the foundation for the second. • Sub-Objective1 (O1): Generating code that conforms to all given I/O examples. This objective concerns the capability to understand requirements conveyed through I/O examples. For each programming task, we evaluate the models under various sets of I/O examples sampled from the input space, then directly use these I/O examples to construct the assertions for correctness checking. • Sub-Objective2 (O2): Generating code that successfully implements the target functionality. This objective further concerns two capabilities: inferring target functionalities from I/O examples and improving generated code through iterative feedback. Considering the incompleteness of the I/O examples in describing functionalities, we adopt an iterative evaluation framework for this objective. Specifically, according to the code generated by models, the framework adaptively supplements new I/O examples to reveal the differences between the generated code’s functionality and the target functionality. To enable a comprehensive evaluation, we construct a benchmark comprising 168 target functionalities drawn from existing benchmarks. Each functionality is accompanied by five sets of randomly sampled I/O examples as the starting point of the iterative code generation. With this new benchmark, we conduct thorough evaluations and analysis on six state-of-the-art large language models (one closed-source and five open-source), aiming to answer the following three research questions: • RQ1: (Toward O1) Can the evaluated LLMs generate code conforming to all given I/O examples? • RQ2: (Toward O2) With the iteratively supplemented I/O examples, can the evaluated LLMs generate code that successfully implements the target functionalities? • RQ3: (Result Interpretation) What kinds of functionalities can be implemented by the evaluated LLMs given I/O examples? Evaluation Results: The evaluation results reveal that when programming requirements are provided only in the form of I/O examples (rather than natural languages), the code generation ability of LLMs significantly declines, with scores decreasing as more examples are given. Furthermore, the score for finally implementing the target functionality drops even over 60%. Among the evaluated models, GPT-4o-mini achieves pass@10 values ranging from 0.33 to 0.36, outperforming all other open-source models with approximately 7b parameters. Meanwhile, DeepseekCoder-6.7b-instruct achieves pass@10 values between 0.19 and 0.22, leading among open-source models with an approximately 80% improvement over the second place. Finally, by analyzing the results under different types of functionalities, we conclude that it is easier for the evaluated LLMs to implement functionalities related to simple mathematical computations and string manipulations in example-based code generation. Findings and implications: After recognizing the limitations of LLMs in example-based code generation, we further analyze the generated code and trends across iterations. First, we observe that the code generated by LLMs may simply employ if statements to match the given I/O examples, and this tendency most commonly occurs with Llama-2-7b-chat. This observation illustrates the necessity of iterative evaluation frameworks for example-based code generation. Second, during the iteration process, the very first rounds of interactions play the most critical role in the ultimate success, because the evaluated LLMs are not good at utilizing the iteratively supplemented feedback. This underscores the importance of selecting appropriate initial I/O examples for example-based code generation. More importantly, our benchmark covers an under-explored topic (code generation with multi-turn requirements) and suggests that current LLMs may be relatively weak in achieving multi-turn and iteratively given requirements compared to single-turn. In summary, our paper makes the following contributions. • The first comprehensive study of LLMs’ capability in example-based code generation. We regard example-based code generation as a task with multi-turn requirements, formalizing it into two sequential sub-objectives. • An iterative evaluation framework for example-based code generation and a new benchmark applicable to this framework. Both the framework and the benchmark can be reused and extended for more programming languages and functionalities. • A thorough evaluation and comparison of six state-of-the-art LLMs. The results not only compare the scores of each model but also analyze their trends over iterations and their strengths/weaknesses in different types of functionalities. • Empirical evidence of current LLMs’ limitations in example-based code generation, particularly in handling I/O example requirements and refining code through iterative feedback. The evidence offers an important insight into LLMs’ code generation capability and provides valuable suggestions for applying LLMs to code generation with multi-turn requirements."
https://arxiv.org/html/2411.06680v1,"Anchor Attention, Small Cache: Code Generation with Large Language Models","The development of large language models (LLMs) has revolutionized automated code generation. However, their high demand of computation resources has hindered a broader deployment and raised environmental concerns. A common strategy for diminishing computational demands is to cache Key-Value (KV) states from the attention mechanism which is adopted predominately by mainstream LLMs. It can mitigate the need of repeated attention computations, but brings significant memory overhead. Current practices in NLP often use sparse attention which may, unfortunately, lead to substantial inaccuracies, or hallucinations, in code generation tasks. In this paper, we analyze the attention weights distribution within code generation models via an empirical study, uncovering a sparsity pattern, i.e., the aggregation of information at specific anchor points. Based on this observation, we propose a novel approach, AnchorCoder, which features token-wise anchor attention designed to extract and compress the contextual information, and layer-wise anchor attention enabling cross-layer communication to mitigate the issue of excessive superposition caused by the compression. The extensive experiments across multiple benchmark datasets confirm the effectiveness of AnchorCoder, which can consistently achieve a significant (at least 70%) reduction in KV cache requirements, while preserving the majority of model’s performance.","Automated generation of code that aligns with user intentions poses a significant and enduring challenge in software engineering [1, 2, 3]. In recent years, the tremendous progress in deep learning and NLP, especially the advent of Large Language Models (LLMs [4, 5]), has revolutionized the research of automated code generation [6, 7]. LLMs for code, e.g., CodeGen [8], CodeLlama [9] and CodeGeeX [10], have showcased impressive proficiency in writing code, boosting the productivity of developers across various programming environments remarkably [11]. Almost all mainstream LLMs (including those for code which are the main focus of the current paper) adopt the Transformer architecture [12], which, in a nutshell, comprise either an encoder or a decoder, or both, each stacked with multiple identical blocks. In general, the first block takes the tokenized sequence encoded by a word embedding layer, followed by a multi-head scaled-dot self-attention (MHA) layer with an attention mask corresponding to specific language modeling objectives and a feed-forward network (FFN) layer. The attention mechanism [13] underpinning the Transformer architecture is implemented in the MHA layer, which computes a weighted representation of each token in the input sequence based on its relevance to others. Slightly more technically, the word-embedded token sequence which normally concatenates long contexts and user prompts gives rise to three embedding matrices, i.e., the query Q, the key K and the value V, on which the attention (kernel) operations are performed. \displaystyle P\mathrel{\mathop{:}}=Q\times K^{\mathrm{T}}, (1) \displaystyle A\mathrel{\mathop{:}}=\mathrm{softmax}[\cfrac{P}{\sqrt{d_{k}}}% \odot M], (2) \displaystyle O\mathrel{\mathop{:}}=(A\times V)\times W_{O}, (3) Namely, assuming the token sequence length L, each entry of the (unnormalized) relevance matrix P\in\mathbb{R}^{L\times L} measures the relevance of the corresponding pair of tokens. The normalized attention weight matrix A\in\mathbb{R}^{L\times L} is computed as a scaling operation and an element-wise mask operation with M\in\mathbb{R}^{L\times L}, together with a row-wise softmax. Finally, the output hidden state matrix O is generated by a weighted sum of V with attention weights in each row of A, usually with an extra linear transformation W_{O}. The attention mechanism is very costly, albeit effective. To reduce its computational demands, a common strategy is to use the Key-Value (KV) cache. In a nutshell, it is a list of tensors that stores the K,V embeddings for all previous tokens in the attention layer for each block (prefilling), utilized and updated during the autoregressive generation process of LLMs (decoding). A deficiency of KV caching is that LLMs (with billions of parameters) may consume substantial additional memory during the decoding stage, as they need to cache extensive KV states [14, 15, 16]. For instance, CodeLlama-7B [9] (which requires 14 GB to store model parameters) needs an additional 16 GB for the KV cache, under a batch size of 32 and a sequence length of 1,024.111We assess the storage overhead using fp16 precision. In the case of fp32, the KV cache demands 32 GB. The memory demand poses challenges for deploying these models, especially in low resource environments. Various methods have been proposed to reduce the size of KV cache. For instance, window attention [17, 18] and StreamingLLM [19] predict subsequent words by only caching the most recent KV states. H2O [20] and FastGen [21] have explored to preserve subsets of the states crucial for prediction by employing specific patterns. In this paper, these methods are collectively referred to as KV compression methods. To a large extent, they leverage sparse, low-rank attention approximation [22], based on the belief that a subset of tokens contributes the most values when performing attention operations. Although current KV compression methods turn out to be fairly effective in NLP tasks (related to dialogue and text completion), it is risky to apply them in code generation. Fundamentally, these methods typically encourage models to focus on local information. In code generation tasks, however, excessive reliance on local information may result in discrepancies between the generated code snippet and either user’s intention (e.g., in the prompt) or the ongoing decoding process (e.g., from the context). This primarily stems from the inherent complexity of code, which naturally exhibits long-range dependencies. For instance, in repository-level code generation [23, 24, 25], the relevant context that needs to be considered during generation comes from not only the current, but also externals, files, e.g., imported packages, source code files in the same directory, configuration files and even API documentation. In many cases, these artifacts have their own dependencies. Capturing these long-range dependencies demands more than mere understanding of the local context. We present an example to illustrate the limitations in current KV compression methods in the left part of Fig. 1, where the LLM is supposed to generate code for multiplication of three numbers. The content within the sliding window (which captures the local information) only includes ‘* w *’. As a result, the model erroneously interprets this as symbolic emoticons, which wholly deviates from user’s request (of a mathematical function). Figure 1: AnchorCoder’s performance on context compression. In this paper, we aim to explore KV compression techniques devised for code LLMs without the over-reliance on local information. To this end, we first carry out an empirical study on code LLMs, based on which we introduce AnchorCoder, a novel approach that leverages “anchors” to aggregate sufficient contextual information. Empirical study. To verify code LLMs’ potential for KV compression, we first identify the sparsity pattern of the attention weights matrix A in Eq. (2) within code LLMs. We use the Gini coefficient [26] and the sum of top-2 attention weights to measure the sparsity degree of the attention weights. Our empirical study (cf. Section 2.1) reveals that code LLMs exhibit high sparsity on attention weights. In the majority of layers, the Gini coefficient for attention weights exceeds 0.9. Furthermore, the sum of top-2 attention weights typically accounts for 80% of the total weights. This implies that, in these layers, the model concentrates on a subset of KV states to complete generation, while the vast majority of the KV cache is largely redundant. Importantly, we discover that code LLMs exhibit a phenomenon of information aggregation around specific tokens. These tokens, referred to as anchor points [27, 28, 29, 30], are identified in the first layer which aggregate essential contextual information, particularly the semantics of each line of code. They enable the model to effectively summarize and distill the essential information for subsequent computation. As the computation progresses through later layers, a prominent concentration of attention weights on these anchor points is observed. Furthermore, we evaluate the state-of-the-art KV compression methods to see whether they can capture the sufficient contextual information with different context lengths. To this end, we design a “needle in a haystack” experiment [31, 32] tailored for code generation. The experiments reveal that, while the existing methods can achieve a high accuracy for shorter code snippets, their performance diminishes significantly for longer ones, where the “needle” is deeply embedded. In such cases, the model frequently fails to detect the “needle”, and consequently, may not adhere to the given instructions. As illustrated in Fig. 1, the word ‘Multiply’ is buried too deeply, leading the model to misinterpret the context due to the sparse attention mechanism. The details are given in Section 2.2. New approach. We present AnchorCoder, a novel approach designed to reduce storage demands of KV caches in code generation models while preserving essential contextual information. In a nutshell, AnchorCoder utilizes a mechanism that “communicates in superposition”[33], aggregating the context to a few planted anchors. Let us revisit the example in Fig. 1. Typical sparse attention mechanism tends to ignore context outside the sliding window (i.e., ‘* w *’). In contrast, AnchorCoder, as shown in the right part of Fig. 1, ensures effective code generation by compressing a sufficient context. The rationale lies in the compression phenomenon revealed in the empirical study, which can reduce the size of context inherently, but does not substantially degrade model’s performance. More technically, AnchorCoder features multi-head positional encoding (cf. Section 3.1) and layer-wise anchor attention (cf. Section 3.2), which respectively address the loss of positional information due to compression and the degradation of information during transmission between layers. To evaluate the performance of AnchorCoder, we conduct experiments on three benchmark datasets, i.e., HumanEval [1], HumanEvalPlus [34] and MBPP [35]. The experimental results demonstrate that AnchorCoder maintains model performance at 102%, 110%, and 97% on these three datasets, while achieving KV cache budget of 30%, 30% and 28%, through efficient tuning. Furthermore, we design an experiment that trains AnchorCoder from scratch where the results show that with a KV cache budget of 30%, AnchorCoder can still achieve performance comparable to that of dense attention, thereby validating the effectiveness and generalizability of AnchorCoder. Our contributions can be summarized as follows. • We identify patterns of sparsity in the attention mechanisms of code LLMs and uncover the phenomenon of information aggregation on anchor points within them. Additionally, we reveal the limitations of current KV compression methods on code LLMs. • We propose AnchorCoder, a novel sparse attention based approach, which compresses context through token-wise anchor attention and mitigates information degradation through layer-wise anchor attention. This approach can reduce the KV cache overhead while preserving sufficient contextual information. To the best of our knowledge, this is the first systematic research on effective KV compression methods in LLMs for code generation, and software engineering in general. Organization. The remainder of this paper is organized as follows. Section 2 presents an empirical study on code LLMs. Section 3 presents the proposed approach. Section 4 gives the experimental design and Section 5 reports the results. Section 6 discusses potential threats to validity. Section 7 reviews the related work. Section 8 concludes the paper. The source code of AnchorCoder is available at https://github.com/NUAAZXY/Anchor_Coder and the models are available at https://huggingface.co/AnchorCoder."
https://arxiv.org/html/2411.06638v1,Model Editing for LLMs4Code: How Far are We?,"Large Language Models for Code (LLMs4Code) have been found to exhibit outstanding performance in the software engineering domain, especially the remarkable performance in coding tasks. However, even the most advanced LLMs4Code can inevitably contain incorrect or outdated code knowledge. Due to the high cost of training LLMs4Code, it is impractical to re-train the models for fixing these problematic code knowledge. Model editing is a new technical field for effectively and efficiently correcting erroneous knowledge in LLMs, where various model editing techniques and benchmarks have been proposed recently. Despite that, a comprehensive study that thoroughly compares and analyzes the performance of the state-of-the-art model editing techniques for adapting the knowledge within LLMs4Code across various code-related tasks is notably absent. To bridge this gap, we perform the first systematic study on applying state-of-the-art model editing approaches to repair the inaccuracy of LLMs4Code. To that end, we introduce a benchmark named CLMEEval, which consists of two datasets, i.e., CoNaLa-Edit (CNLE) with 21K+ code generation samples and CodeSearchNet-Edit (CSNE) with 16K+ code summarization samples. With the help of CLMEEval, we evaluate six advanced model editing techniques on three LLMs4Code: CodeLlama (7B), CodeQwen1.5 (7B), and Stable-Code (3B). Our findings include that the external memorization-based GRACE approach achieves the best knowledge editing effectiveness and specificity (the editing does not influence untargeted knowledge), while generalization (whether the editing can generalize to other semantically-identical inputs) is a universal challenge for existing techniques. Furthermore, building on in-depth case analysis, we introduce an enhanced version of GRACE called A-GRACE, which incorporates contrastive learning to better capture the semantics of the inputs. Results demonstrate that A-GRACE notably enhances generalization while maintaining similar levels of effectiveness and specificity compared to the vanilla GRACE.","Large Language Models (LLMs) have demonstrated their powerful understanding and generating capabilities [1, 2, 3, 4], and have been applied to areas such as autonomous agents [5], medicine [6], and recommendation system [7]. LLMs for code (LLMs4Code), trained on massive code-related datasets [8], also show remarkable performance in coding tasks within software engineering [9], including code generation[10, 11] and code comment generation[12, 13]. However, even the most advanced LLMs4Code can contain outdated and incorrect code knowledge due to the following reasons[14]. On one hand, the training data for LLMs4Code is limited to a certain period, which implies that LLMs4Code cannot learn about the latest software package characteristics from this data as software is continuously changing [15]. On the other hand, the massive training data inevitably contains some noise[16], which ultimately leads LLMs4Code to learn some incorrect code knowledge. If the aforementioned concerns are not promptly addressed, LLMs4Code will continue to produce bugs or vulnerabilities in production environments[17, 18]. A natural way to fix these issues is retraining, but this “Killing a fly with a cannon” approach not only consumes a considerable amount of computational resources but also takes a lot of time. Recently, researchers have explored Model Editing to repair LLMs’ outdated and incorrect knowledge, whose aim is to efficiently and effectively update targeted knowledge without affecting the other non-targeted knowledge of LLMs[19]. Existing model editing approaches can generally be categorized into three classes, which are based on External Memorization, Global Optimization, and Local Modification, respectively [20]. External Memorization adds an extra module to the original model to store updated knowledge; Global Optimization directly or indirectly updates model weights using fine-tuning gradients; Local Modification first identifies critical modules in the model where the target knowledge is stored, then uses specific algorithms to update the weights in these key modules. State-of-the-art model editing approaches have achieved remarkable success for updating the knowledge of LLMs[21, 22, 23, 24]. Given that, researchers have also explored to utilize model editing to fix errors produced during the code generation process [17]. Despite that, this initial study has certain limitations due to the following reasons. First, tasks in the software engineering domain like code generation and code summarization often involve sequence generations, but the proposed approach by Gu et al.[17] focuses on a single token at each time, which is hard to apply to sequence generation and thus misaligns with the application scenarios. Second, this existing study mainly focuses on assessing the ability of the edited model to generate correct contents, while other essential properties of LLMs, such as the fluency, i.e., measuring the extent to which the edited model can generate natural and non-repetitive contents, are ignored. Third, the existing study focuses on a target model with a maximum of 350M parameters, a scale that is relatively restricted compared to the typical size of general LLMs4Code models, which commonly comprise several billion parameters[1]. Consequently, the literature has limited understanding regarding the strengths and weaknesses of existing model editing techniques within the context of LLMs4Code, and it remains an open question as to how effectively these techniques can update the code knowledge embedded in LLMs4Code. There is thus an urge need for a comprehensive empirical study comparing and analyzing the performance of all the state-of-the-art model editing techniques on LLMs4Code. Such a study is necessary and essential, as it can provide answers to fundamental questions such as which types of approaches are the most proficient. This insight can serve as a guideline for researchers in devising more proficient techniques in the future. To bridge this gap, we perform the first systematic study on applying state-of-the-art model editing approaches to repair the inaccuracy of LLMs4Code. To that end, we first build an evaluation benchmark, Code Large Language Models Editing Evaluation (CLMEEval), which consists of two datasets, CoNaLa-Edit (CNLE) and CodeSearchNet-Edit (CSNE), corresponding to the editing of code knowledge in the context of two widely-studied software engineering tasks: a natural language to programming language (NL2PL) code generation task and a programming language to natural language (PL2NL) code summarization task. Drawing from this benchmark, we employ model editing techniques to rectify the inaccuracies produced by LLMs4Code, mirroring real-world scenarios where updates to code knowledge within a model are necessary. This could involve situations like changes in required APIs for completing specific coding tasks or shifts in the primary functionality of a method due to code changes. Following the common practice in the model editing domain [25], our study evaluates the approaches from four dimensions: Effectiveness, the success rate on editing instances; Generalization, the success rate on tests that are semantically identical to the editing instances; Specificity, the success rate on tests unrelated to the editing instances; and Fluency, the fluency of the contents generated by the model [25]. We select six state-of-the-art model editing approaches from the three categories mentioned above and three widely-used LLMs4Code, i.e., CodeLlama (7B)[26], CodeQwen-1.5 (7B) [27], and StableCode (3B)[28] as our study subjects. Through an extensive evaluation, our study makes the following important findings: F1: The External Memorization-based technique, GRACE, can consistently achieve the optimal effectiveness and specificity across different datasets and LLMs4Code. Nonetheless, all the existing model editing techniques perform poorly in terms of generalization. F2: Most model editing techniques perform comparatively poorly on LLMs4Code, being far less proficient compared with editing general LLMs. F3: Model editing techniques are sensitive to the specific tasks, with all the editing techniques performing worse in NL2PL editing than in PL2NL editing. Moreover, through a case analysis on the best-performing editing approach in terms of the effectiveness and specificity, i.e., GRACE [29], we identify its weakness in the inability to distinguish between the semantics of different inputs. Based on this observation, we propose an augmented strategy (named as A-GRACE) where we introduce an encoder to GRACE that allows it to better capture the input semantics via contrastive learning, and thus improves the generalization of GRACE. Results show that A-GRACE significantly improves the generalization while achieves similar effectiveness and specificity compared to GRACE. For instance, on CNLE, A-GRACE improves the generalization of GRACE from almost zero to an average of 80.86% in terms of the Exact Match metric. To summarise, our contributions are as follows: • Benchmark. We construct CLMEEval for LLMs4Code editing, which includes a CNLE dataset with 21K+ NL2PL samples and 16K+ PL2NL samples. • Evaluation. We evaluate six state-of-the-art model editing approaches on three LLMs4Code, and find that existing model editing approaches can hardly adapt well to LLMs4Code: they usually fail to achieve a good balance among effectiveness, generalization, and specificity. • Strategy. We propose a refined editing approach A-GRACE, and the experiment results show that A-GRACE achieves promising performances on CLMEEval. Particularly, its generalization is improved by an order of magnitude compared to the vanilla GRACE."
https://arxiv.org/html/2411.06562v1,owl2proto: Enabling Semantic Processing in Modern Cloud Micro-Services,"The usefulness of semantic technologies in the context of security has been demonstrated many times, e.g., for processing certification evidence, log files, and creating security policies. Integrating semantic technologies, like ontologies, in an automated workflow, however, is cumbersome since they introduce disruptions between the different technologies and data formats that are used. This is especially true for modern cloud-native applications, which rely heavily on technologies such as protobuf. In this paper we argue that these technology disruptions represent a major hindrance to the adoption of semantic technologies into the cloud and more effort and research is required to overcome them. We created one such approach called owl2proto, which provides an automatic translation of OWL ontologies into the protobuf data format. We showcase the seamless integration of an ontology and transmission of semantic data in an already existing cloud micro-service.","1 INTRODUCTION Semantic technologies can establish a common understanding of, e.g., cloud concepts and their properties and thus have a high importance for the interoperability of cloud services. In the security context, semantic technologies have, for instance, been used to model certification evidence [Banse et al., 2021, Banse et al., 2023], to structure information in log files [Ben-Shimol et al., 2024], or to model general cloud security concepts [Takahashi et al., 2010]. We argue that while the academic discussion focuses on the semantic design of, e.g., cloud security concepts [Maroc and Zhang, 2019], the technological integration of semantic design and its technological implementation is lagging behind. Modern technologies like micro-services and RPCs, for example, are not integrated with technologies of the semantic web stack. We think that a better integration with such technologies would make semantic concepts easier to use, increase its adoption in different domains, and it could improve the interoperability of cloud systems, e.g. in multi-cloud and cloud-edge scenarios. In this paper, we focus on protobuf as an example for this position and use the cloud security context as an example application domain. Note, however, that our arguments apply beyond these examples. Protobuf is one of the most commonly used technologies for micro-services. Originally designed as a format to describe the serialization of network packages, it has evolved into an interface definition language, not only describing the exchanged data, but also the services that produce or consume this data. Protobuf intentionally does not focus on the semantics of the exchanged data. Instead, it defines a syntax and structure of an object (called message), by describing which fields a programmer would use to fill this object. This includes primitive types, arrays and other messages. But the semantics of the data, such as, that it describes evidences gathered for a security incident, is beyond its scope. Developers would have to resort to storing this semantic information in other formats (e.g. RDF, JSON-LD, etc.) and then transmitting the actual data in a serialized form, creating a technology gap between the “semantic” world and the rest of the application. In this paper, we argue for bridging the gap between semantic technologies and the integration with modern data processing. We demonstrate how to advance this integration by introducing a methodology and implementation that transforms ontology concepts into RPC definitions, and we point out use cases. The implemented tool is called owl2proto."
https://arxiv.org/html/2411.06262v1,Security Implications of User Non-compliance Behavior to Software Updates: A Risk Assessment Study,"Software updates are essential to enhance security, fix bugs, and add better features to the existing software. However, while some users comply and update their systems upon notification, non-compliance is common. Delaying or ignoring updates leaves systems exposed to security vulnerabilities. Despite research efforts, users’ noncompliance behavior with software updates is still prevalent. In this study, we explored how psychological factors influence users’ perception and behavior toward software updates. In addition, we proposed a model to assess security risk score associated with delaying software updates. We conducted a user study with Windows OS users to explore how information about potential vulnerabilities and risk scores influence their behavior. Furthermore, we also studied the influence of demographic factors such as gender on users’ decision-making process for software updates. Our results showed that psychological traits, such as knowledge, awareness, and experience, impact users’ decision-making about software updates. To increase users compliance, providing a risk score for not updating their systems and information about vulnerabilities statistically significantly increased users’ willingness to update their systems. Additionally, our results indicated no statistically significant difference in male and female users’ responses in terms of concerns about securing their systems. The implications of this study are relevant for software developers and manufacturers as they can use this information to design more effective software update notification messages. Highlighting potential risks and corresponding risk scores in future software updates can motivate users to act promptly to update the systems in a timely manner, which can ultimately improve the overall security of the system.","A software update involves making adjustments to improve or fix issues with the software. The updates can range from minor changes to significant enhancements or the addition of new features. These updates are important not only for fixing vulnerabilities and bugs but also for maintaining the security of the software [1]. One of the most important features of any modern security system is its capacity for releasing effective and safe software upgrades [2]. Software update mechanisms try to ensure accessibility, efficiency, robustness, and expandable distribution of software updates to facilitate the timely application of security patches [3, 4, 5, 6]. In 2021, a vulnerability known as Log4j or Log4Shell impacted approximately three billion computer systems and applications [7]. This attack was considered ”critical,” with a severity score of 10 according to the National Vulnerability Database (NVD) [8]. According to the experts, this attack could have been prevented if the available software update had been applied [9]. As per a study conducted in 2022, many security breaches occur because of uninstalled updates for vulnerabilities that were available [10]. In general, vulnerable software is usually targeted by cyber attackers, especially if that software has widespread use and a large number of users, such as Microsoft Office, Adobe Acrobat, and Internet Explorer [11]. While technical remedies for security concerns, such as releasing software patches, hold significance, improving human awareness toward security practices is indispensable for achieving cyber safety. The security of a computer system often relies on how users understand, behave, and make security-related decisions[12, 13, 14, 15]. Failing to install updates for identified vulnerabilities can lead to severe security breaches. Previous work suggested that many users do not perceive all updates as equally significant or prioritize them appropriately [16]. This can leave systems exposed to potential threats and undermine the effectiveness of security measures. Prior survey-based studies delved into users’ behavioral studies and found users’ unfavorable behaviors impact both individual security [17] and organizations’ security stance [18]. In organizations, approximately 27% of data breaches are caused by not adopting common security and privacy measures by the end users [19]. Similarly, end users often fail to adopt common security and privacy measures.[20, 21]. One of the main reasons for security violations is a failure to apply the patch for a known vulnerability, as most of the exploitation occurs in systems that are not updated[22]. Microsoft reported that most of its customers are breached via vulnerabilities that had patches released years ago. This indicates users’ non-compliance behavior towards applying patches [22]. Another study presented that about 80% of companies with a data breach or failed audit could have prevented that by patching on time or doing configuration updates [23]. According to a statistic, 84% of the companies have high-risk vulnerabilities on their external networks; more than half of those vulnerabilities could have been simply removed just by installing the update [24]. Once vulnerabilities are discovered, developers typically work fast to release an update or patch. On average, patch release time for a vulnerability ranges between 23 to 40 days [25, 26]. According to another study, after the discovery of the vulnerability, the white-hat developers generally take 30 to 45 days to make the patch available [27]. In this timeline, if the vulnerability is discovered by the black-hat community (i.e., hackers with malicious intent), then the vulnerability could be exploited within zero days. Frei [28] found that 78% of exploitations take place within a day, and 94% do so within 30 days of the public disclosure day. However, studies suggest that the gap between public disclosure and exploitation is decreasing, and the exploitation process now takes around five days [29]. Furthermore, prior research has also shown that there are differences in user behaviors based on gender [30]. Considering a set of security and privacy behaviors, researchers found female users’ levels are significantly lower than male users in 40% security and privacy behaviors. Also, female users find less interest in adopting technical skills than male users [31]. Hence, it is clear that software security not only depends on the timely release of software updates but also on users’ compliance in updating their systems to prevent attacks[32, 33]. As the previous works present, people are negligent with respect to applying updates and prefer to delay the process, while early updating applications could make the system secure and protect it from unwanted attacks. This study is focused on analyzing how delaying software updates could increase cyber risk and investigating software update behaviors of users when they have information about vulnerabilities and risk scores. Additionally, in the field of psychology, research has extensively explored attitudes, including their correlation, antecedents and consequences, and correlation with intentions and behavior [34, 35]. To gain a better understanding of users’ security attitudes, we analyzed their attitudes towards cybersecurity. In this study, we investigated end-users’ non-compliance behavior and their perspective toward software updates. We proposed a framework to assess how delaying updating software can increase security risk. Next, we developed a questionnaire combining skills, awareness, experience, and knowledge-based questions. We designed our questionnaire to evaluate users’ behavior and changes in their decision-making process before and after having proper vulnerability and security risk-related information. Further, using a combination of factors and statistical analysis, we identified which factors increase users’ awareness and influence them to update software to secure their system. Additionally, we extended our research to determine if male and female end users’ behavior and perception differ when they have the same security-related information. With this extension, we looked into the discrepancies in gender-based security behavior. To achieve the above-stated research objectives, we focused on the following research questions: • RQ1: How do users’ cognitive states affect their adoption of software updates? • RQ2: To what extent does the vulnerability and risk score information improve users’ software update compliance behavior? • RQ3: What difference does gender make in software update decision-making? This paper has been organized as follows. Section 2: described background of CVSS. Section 3 discusses the related work on users’ software update behaviors and risk estimation from software update non-compliance. Section 4 presents the methodologies for risk-score assessment associated with software update delays and survey study. Section 5 explains the analysis and results. Finally, Section 6 concludes the paper, and Section 7 discusses limitations and future works."
https://arxiv.org/html/2411.06145v1,Escalating LLM-based Code Translation Benchmarking into the Class-level Era,"In recent years, Large Language Models (LLMs) have dramatically advanced the performance of automated code translation, making their computational accuracy score reach up to over 80% on many previous benchmarks. However, most code samples in these benchmarks are short, standalone, statement/method-level, and algorithmic, which is not aligned with practical coding tasks. Therefore, it is still unknown the actual capability of LLMs in translating code samples written for daily development.To achieve this, we construct a class-level code translation benchmark, ClassEval-T, and make the first attempt to extensively assess recent LLMs’ performance on class-level code translation. ClassEval-T is extended from ClassEval, a well-known class-level Python code generation benchmark consisting of multiple practical coding topics, such as database operation and game design, and diverse contextual dependencies (e.g., fields, methods, and libraries). It cost us 360 person-hours to accomplish the manual migration to Java and C++ with complete code samples and associated test suites. Subsequently, we design three translation strategies (i.e., holistic, min-dependency, and standalone) for class-level code translations and evaluate six recent LLMs of hybrid, code, and general kinds in diverse families and sizes on ClassEval-T. Experimental results demonstrate a remarkable performance drop compared with the most widely studied method-level code translation benchmark, and obvious discrepancies among LLMs appear, showing the effectiveness of ClassEval-T in measuring recent LLMs. Afterwards, we further discuss the usage scenarios for diverse translation strategies and LLMs’ ability to dependency awareness when translating class samples. Finally, 1,397 failure cases made by the best-performing LLM under test are thoroughly analyzed and categorized in this paper for practical guidance and future enlightenment.","Automated code translation aims to migrate codebases from one Programming Language (PL) to another to satisfy the diverse extensions (Liu et al., 2023; Nguyen et al., 2015; Wu et al., 2010), which is critically important for coding productivity. In recent years, with the advancement of Large Language Models (LLMs), the correctness and readability of code translation have achieved substantial improvement (Yang et al., 2024a; Yuan et al., 2024; Nitin and Ray, 2024). According to a series of recent studies (Yang et al., 2024a; Pan et al., 2024), LLMs, such as CodeLlama (Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain and Remez, Tal and others, 2023) and GPT-3.5 (OpenAI, 2024), can correctly translate over 70%-80% code samples with the most basic prompt among diverse translation pairs, even for those domain-specific scenarios (Tao et al., 2024), such as Python-to-Scala. However, previous studies mainly experiment with statement/method-level code translation benchmarks, consisting of shorter code lengths, limited dependencies, and only algorithmic problems. Therefore, they cannot reflect the code translation capability of LLMs in practical development, and it is highly motivated and necessary to construct a brand-new code translation benchmark aligning with coding practice for LLMs’ assessment, thereby providing insightful research direction and practical guidance for academia and industry. Benchmark ClassEval-T: To mitigate the above limitation in code translation assessment, we construct ClassEval-T, a class-level code translation benchmark comprising three parallel PLs (i.e., Python, C++, and Java) and associated test suites with extremely high coverage scores of 99.7% on statements and 98.2% on branches. ClassEval-T is extended from the latest class-level Python code generation benchmark, ClassEval (Du et al., 2024), via line-wise manual translation for 360 person-hours. Hence, ClassEval-T also contains the virtues that ClassEval has, including longer code lengths, diverse dependencies on fields, methods, and libraries, as well as practical coding problems (e.g., database operation and game design). Besides, a class-level code translation benchmark stands out from those statement/method-level ones in two ways. (1) ClassEval-T not only can evaluate the correctness of translated code but also can assess LLMs’ ability in dependencies awareness and inference on proper library invocation. (2) Class-level code translation allows for further exploration of diverse translation strategies (e.g., translating the whole class at once or separately). Empirical Study: Based on ClassEval-T, we make the first attempt to extensively assess recent LLMs’ performance on class-level code translation. Specifically, six recent LLMs of diverse kinds (e.g., hybrid, code, and general), families (e.g., CodeLlama (Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain and Remez, Tal and others, 2023), Llama (Llama Team, 2024), and Gemma (Team et al., 2024)), and sizes (ranging from 7B to 236B) are involved. For each studied LLM, we evaluate their performance with three distinctive translation strategies, i.e., holistic translation (translating the whole class at once), min-dependency translation (translating each module of a class one by one, given the minimal necessary dependencies), and standalone translation (translating each module of a class one by one without dependency information). For each translated code sample, we evaluate its correctness via Computational Accuracy (CA) and Compilation Success Rate (CSR). Besides, we also incorporate DEPendency evaluation metrics (DEP) to investigate LLMs’ awareness of necessary context during the translation for class-level samples and thoroughly analyze and categorize 1,397 Failed cases at the end of this experiment. Main findings: Based on our results, we have the following findings. (1) All LLMs perform dramatically worse on class-level code translations than method-level ones, where the latter cannot even tell different LLMs’ performance discrepancies. (2) DeepSeek-V2.5 exhibits predominately superior performance on class-level code translation, while smaller LLMs normally perform worse, given other factors the same. Besides, All LLMs perform better on Python-oriented translations, and Code LLMs do not necessarily outperform general LLMs. (3) DeepSeek-V2.5 always works better on the holistic translation strategy, while the selection of translation strategy for smaller LLMs depends on different scenarios. In addition, Python-oriented translations work better on the holistic strategy, while C++-oriented translations prefer the min-dependency strategy. (4) The holistic translation strategy improves LLMs’ awareness of field dependencies, while the min-dependency strategy allows LLMs to invoke more necessary libraries. As for method dependencies, the above two strategies perform neck-to-neck. (5) Syntax errors remain a primary issue in class-level translations, but class-related errors, such as function/variable usage and consistency issues, are more prominent in C++/Java-oriented translations. Based on the above findings, several implications are summarized for both researchers and practitioners. The contributions of this work can be three-fold: (1) We manually construct the first class-level code translation benchmark. (2) We conduct the first attempt to extensively evaluate diverse LLMs’ class-level code translation capability with different translation strategies and assessing aspects. (3) We summarize a series of findings and manually analyze 1,397 failed cases for categorization, shedding light on practical guidance and future research directions."
https://arxiv.org/html/2411.06077v1,CI/CDConfigurationPracticesin Open-Source Android Apps: AnEmpiricalStudy,"Continuous Integration and Continuous Delivery (CI/CD) is a well-established practice that automatically builds, tests, packages, and deploys software systems. To adopt CI/CD, software developers need to configure their projects using dedicated YML configuration files. Mobile applications have distinct characteristics when it comes to CI/CD practices, such as testing on various emulators and deploying to app stores. However, little is known about the challenges and added value of adopting CI/CD in mobile applications and how developers maintain such a practice. In this paper, we conduct an empirical study on CI/CD practices in 2,564 Android apps adopting four popular CI/CD services, namely GitHub Actions, Travis CI, CircleCI, and GitLab CI/CD. We observe a lack of commonality and standards across projects and services, leading to complex YML configurations and associated maintenance efforts. We also observe that CI/CD configurations focus primarily on the build setup, with around half of the projects performing standard testing and only 9% incorporating deployment. In addition, we find that CI/CD configurations are changed bi-monthly on average, with frequent maintenance correlating with active issue tracking, project size/age, and community engagement. Our qualitative analysis of commits uncovered 11 themes in CI/CD maintenance activities, with over a third of the changes focusing on improving workflows and fixing build issues, while another third involves updating the build environment, tools, and dependencies. Our study emphasizes the necessity for automation and AI-powered tools to improve CI/CD processes for mobile applications, and advocates for creating adaptable open-source tools to efficiently manage resources, especially in testing and deployment.","Continuous Integration and Continuous Delivery (CI/CD) is a well-established practice that automates the building, testing, packaging, and deployment of software systems. (Atlassian, [n.d.]; CircleCI, [n.d.]). CI/CD pipelines consist of automated processes designed to reduce human error, improve developer productivity, and accelerate release cycles. Their widespread adoption in both industry and open-source communities has led to extensive research on the challenges and benefits associated with CI/CD pipelines (Hilton et al., 2017, 2016a; Thatikonda, 2023). Mobile applications, henceforth referred to as mobile apps, have become integral to our daily lives, serving various purposes, from communication to entertainment. Research reports that apps with more frequent release cycles tend to achieve higher ratings, as users often positively perceive regular updates and improvements (McIlroy et al., 2016). Research has also explored the benefits of adopting CI/CD in the software development lifecycle, notably enhancing quality and accelerating the delivery timeline. In the context of CI/CD, Cruz et al. (2019a) studied the testing practices in open-source mobile apps and found that the use of CI/CD pipelines is surprisingly limited among the mobile apps they analyzed. Mobile applications require distinct CI/CD pipelines compared to other software systems. This is because developers need to ensure extensive testing across multiple emulators and manage deployments to app stores, such as the Google Play Store (Nayebi et al., 2017). Despite such importance, challenges in configuring and maintaining CI/CD for mobile apps have not been well studied. Though Liu et al. (2022) studied the adoption of CI/CD in mobile apps, their study mainly provided general statistical insights about CI/CD usage without exploring in-depth the specific configuration or maintenance practices of developers. In this paper, we conduct an empirical study to investigate the configuration practices and maintenance activities associated with CI/CD in a curated set of 2,564 open-source Android apps adopting four popular CI/CD services, namely GitHub Actions, Travis CI, CircleCI, and GitLab CI/CD. In particular, our study explores the common patterns and challenges that developers face with CI/CD configurations for their apps. We aim to provide insights into the current state of CI/CD adoption within the Android development community and highlight areas for potential improvement. We address the following research questions (RQs). RQ1: How do CI/CD configurations vary in terms of complexity and tool usage? In this RQ, we examine CI/CD configurations across multiple services. We observe that CI/CD configurations are not standardized, with complexities varying by service and type of configuration. We also find that most of the attention in CI/CD configuration is on the pipeline setup, accounting for more than 81% of the configurations. While testing is found to be present in about half of the projects, mainly through standard unit tests, deployment is comparatively less common, occurring in 9% of the projects. Our investigation shows that configuring deployment is notably complex and often does not directly target app stores, indicating a need for more integrated and automated solutions to streamline mobile app deployments to popular platforms, such as the Google Play Store. RQ2: How do CI/CD configurations evolve in Android apps? Adopting CI/CD in mobile app development makes developers responsible for the ongoing maintenance of their pipelines, which involves tasks such as feature additions and issue resolutions. In this RQ, we analyze and model the evolution of CI/CD configurations. We observe that developers regularly maintain CI/CD pipelines, yet not very frequently, mainly focusing on improving the setup and build phases. Over time, projects tend to simplify the CI/CD pipelines for their apps. While maintenance is found to occur bi-monthly on average, many projects update CI/CD pipelines more often, especially those using GitHub Actions. Our results show that increased maintenance activity correlates significantly with factors such as active issue tracking, project size/age, and community engagement. RQ3: What are the common themes in CI/CD maintenance activities in Android apps? As CI/CD pipelines are continually updated, it is important to understand the rationale for these changes. In this RQ, we perform a manual analysis of a statistically significant sample of commits related to CI/CD configurations. We observe that more than a third of CI/CD configuration changes focus on improving workflow and fixing build issues, while another third targets updates to the build environment, tools, and dependencies. To validate our manually identified themes for all commits in our dataset, we employ Latent Dirichlet Allocation (LDA) for automated topic generation and observe that the results are consistent with our observations. The key contributions of this paper are as follows. • This study is the first of its kind in analyzing CI/CD configurations in Android apps across various services, using a dataset of 2,564 projects. It highlights current challenges in CI/CD adoption due to non-standardized practices. • This study models the evolution of CI/CD configurations using linear regression, and identifies the potentially relevant factors linked to changes in CI/CD pipelines. • This study presents a manually validated classification of 11 themes related to the maintenance of CI/CD pipelines, corroborated by automated automatically generated topics using LDA, ensuring both depth and breadth in our analysis. Overall, our study highlights the lack of standardized configurations in CI/CD pipelines for Android apps, emphasizing the need for uniform practices to improve interoperability. In addition, this study recommends employing automation tools and AI-driven solutions, such as bots and intelligent recommendation systems, to enhance and streamline CI/CD processes. Finally, researchers are encouraged to create adaptable frameworks and open-source tools to address resource constraints in mobile apps, especially those concerned with testing and deployment phases. Paper organization: The rest of the paper is organized as follows. Section 3 details our study setup to investigate the challenges in configuring CI/CD pipelines for mobile apps. Section 4 outlines our research questions, providing both their motivation and the obtained results. We discuss the implications of our work in Section 5. Section 6 illustrates the potential validity threats of our findings. We present the related work in Section 7. Finally, we conclude our work and provide recommendations for future work in Section 8."
https://arxiv.org/html/2411.05830v1,GitChameleon: Unmasking the Version-Switching Capabilities of Code Generation Models,"The rapid evolution of software libraries presents a significant challenge for code generation models, which must adapt to frequent version updates while maintaining compatibility with previous versions. Existing code completion benchmarks often overlook this dynamic aspect, and the one that does consider it relies on static code prediction tasks without execution-based evaluation, offering a limited perspective on a model’s practical usability. To address this gap, we introduce GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests. GitChameleon is designed to rigorously assess the ability of modern large language models (LLMs) to generate version-specific code that is not only syntactically correct but also functionally accurate upon execution. Our comprehensive evaluations reveal that state-of-the-art LLMs struggle with this task; for instance, GPT-4o achieves a pass@10 of only 39.9% (43.7% when provided with error feedback), highlighting the complexity of the problem and the limitations of current models. By providing an execution-based benchmark that emphasizes the dynamic nature of code libraries, GitChameleon serves as a critical tool to advance the development of more adaptable and reliable code generation models. For facilitation for further exploration of version-conditioned code generation, we make our code repository publicly accessible at https://github.com/NizarIslah/GitChameleon.","Figure 1: Left: Modern LLMs often struggle with generating version-accurate code, highlighting the need for benchmarks that specifically assess their ability to handle versioning. Right: Cumulative year-over-year (YoY) version releases of popular Python-based machine learning libraries show a consistent upward trend, reflecting the rapid pace of development and version updates of code libraries and packages. Large Language Models (LLMs) have become highly popular in code completion, to the extent that they are now deployed as virtual coding assistants within popular code editors111https://github.com/features/copilot, enhancing the overall coding workflow. Code, being a dynamic and constantly evolving environment, necessitates a continuous process of adaptation to stay in sync with the rapidly shifting paradigms, frameworks, and methodologies within the software development domain. The inherent variability in coding styles, the emergence of new programming languages, and the continuous evolution of libraries and packages underscore the imperative for an active approach in updating code generation models. In response to the needs of practical coding environments, several large language models (LLMs) have been introduced, including StarCoder (Li et al., 2023), DeepSeek-Coder (Guo et al., 2024), CodeLlama (Rozière et al., 2023), among others. Despite these advancements, existing LLMs often struggle to keep pace with the rapid changes in codebases, particularly when tasked with generating version-specific code that is both syntactically and functionally accurate. This issue is especially critical, as developers increasingly depend on AI-assisted coding tools to boost productivity and maintain code quality. A recent Stack Overflow survey revealed that 70% of the participants are using or planning to integrate AI coding tools, 33% citing increased productivity as the primary motivation to incorporate these tools into their workflows222https://stackoverflow.co/labs/developer-sentiment-ai-ml/. Given the rapid development and release cycles of popular libraries, as shown in Figure 1 (right), the need for code generation models to continually adapt to changing API’s is more pressing than ever. For example, prominent machine learning and deep learning libraries like PyTorch (Paszke et al., 2019), NumPy (Harris et al., 2020), and Scikit-Learn (Buitinck et al., 2013) undergo frequent updates, which is reflected in a consistent upward trend in user downloads and version releases. This dynamic nature of code requires models that can adapt and generate code that adheres to the latest versions and practices, a need that current models often fail to meet comprehensively. In addition, certain hardware is restricted to compatibility with specific versions of commonly used packages, which adds an additional layer of complexity beyond merely updating the knowledge base of a code LLM to the latest library versions. In response to these challenges, our work introduces a novel benchmark designed to assess the ability of LLMs to generate version-specific code. We propose GitChameleon, a benchmark that evaluates state-of-the-art code models by requiring them to produce executable code based on version-specific prompts. Then, this code is executed to verify its correctness against expected outputs. By highlighting the limitations of current models in generating accurate version-specific code, GitChameleon provides a structured approach to enhance these models and ensure their practical utility in real-world coding environments. In summary, our contributions are highlighted as follows: 1) we introduce a novel code completion benchmark GitChameleon consisting of 116 Python-based version conditioning problems, including human written unit tests; 2) we systematically analyze the version-specific performance of state-of-the-art code generation LLMs on API change type, version release year, and specific libraries. 3) we demonstrate the effectiveness of utilizing error log feedback as a way to improve version conditioning performance of code generation LLMs."
https://arxiv.org/html/2411.06221v1,Smart-LLaMA: Two-Stage Post-Training of Large Language Models for Smart Contract Vulnerability Detection and Explanation,"With the rapid development of blockchain technology, smart contract security has become a critical challenge. However, existing smart contract vulnerability detection methods face three main issues: (1) Insufficient quality and comprehensiveness of datasets, due to the lack of detailed explanations and precise vulnerability locations in current datasets. (2) Limited adaptability of large language models (LLMs) to the smart contract domain, because most LLMs are typically pre-trained on vast amounts of general text data but very little smart contract-specific data. (3) Lack of high-quality explanations for detected vulnerabilities, as most existing methods focus solely on detection without providing clear explanations for their results. These limitations significantly hinder detection performance and make it harder for developers to understand and fix vulnerabilities quickly, potentially leading to severe financial losses. To address these problems, we propose Smart-LLaMA, an advanced detection method based on the LLaMA language model. First, we construct a comprehensive dataset covering four vulnerability types with labels, detailed explanations, and precise vulnerability locations. Second, we introduce Smart Contract-Specific Continual Pre-Training, using raw smart contract data to enable the LLM to learn smart contract syntax and semantics, thereby enhancing their adaptability to the smart contract domain. Furthermore, we propose Explanation-Guided Fine-Tuning, a novel approach that fine-tunes the LLM using paired vulnerable code and explanations, enabling it to both detect vulnerabilities and provide reasoned explanations for its results. To evaluate the quality of generated explanations, we employ both LLM evaluation and human evaluation, focusing on three key aspects: Correctness, Completeness, and Conciseness. Experimental results show that Smart-LLaMA outperforms state-of-the-art baselines, with average improvements of 6.49% in F1 score and 3.78% in accuracy, while providing reliable explanations. We have made all models, datasets, and code available.","The advent of blockchain technology has seen rapid adoption across various sectors, driven by its decentralized architecture [1]. This innovative technology enables the creation of secure, distributed digital ledgers for recording transactions [2]. Utilizing advanced cryptographic methods, blockchain ensures the integrity and verification of each transaction, establishing itself as a highly reliable technological framework [3, 4]. Within this ecosystem, smart contracts function as self-executing programs on the blockchain, automating the management of digital assets such as cryptocurrencies. These contracts activate when specific conditions are met and, once deployed, become permanent fixtures on the blockchain [5]. However, the immutable nature and inherent complexity of smart contracts present significant security challenges [5]. The well-documented DAO incident [6, 7] serves as a cautionary tale, illustrating the potential severity of such vulnerabilities. This security breach resulted in the unauthorized diversion of Ethereum valued at $60 million, causing widespread disruption within the blockchain community [8, 9]. This event underscores the critical importance of enhancing smart contract security to prevent similar devastating outcomes in the future. Researchers have developed various techniques to identify vulnerabilities in smart contracts, each addressing different aspects of the challenge but also facing limitations. Symbolic execution tools like Oyente [10], Mythril [11], Osiris [12], and Manticore [13], as well as static analysis tools such as Slither [14] and SmartCheck [15], rely on predefined patterns to detect vulnerabilities. However, these methods often struggle with complex scenarios and lack generalizability. We conducted a detailed survey of existing smart contract vulnerability datasets as shown in I, evaluating multiple datasets including A [16], B [17], C [18], and D [19], and found significant limitations. These datasets typically provide only basic vulnerability labels, lacking detailed explanations and precise location information. They cover a limited range of vulnerability types, usually only 1 to 3, failing to represent the diverse potential security risks in smart contracts. This simplified labeling approach severely constrains models’ ability to comprehensively understand and detect complex vulnerability patterns. These limitations directly affect the learning effectiveness of detection models, potentially leading to questionable accuracy and reliability in detection results. Some more advanced methods have attempted to address these limitations. Clear [20] employs a Contrastive Learning (CL) model to capture complex inter-contract relationships, while Zhuang et al. [17] and Luo et al. [21] introduce graph neural network-based approaches to represent smart contracts. However, the complexity of these graph structures makes them difficult to reproduce and less effective in representing programs accurately. Peculiar [16] and PSCVFinder [18] take a different approach by fine-tuning pre-trained models for vulnerability detection. While innovative, these methods still struggle to provide clear explanations for their detections, which is crucial for practical usage. Given these limitations, researchers have begun to explore the potential of using general-purpose Large Language Models (LLMs) to address smart contract vulnerability detection issues. General-purpose LLMs show promise in adapting to new patterns [22, 23]. However, they often struggle with smart contract-specific concepts and security implications. As illustrated in Figure 1, when presented with a smart contract, a general-purpose LLM like LLaMA-3.1-8B-Instruct incorrectly identifies a non-existent reentrancy vulnerability. It misinterprets the implications of external calls in the ’gotake()’ function, failing to recognize that reentrancy vulnerabilities typically arise when contract state or balance changes occur after external calls, which is not the case in this contract. To address these challenges, we propose our Smart-LLaMA, built upon the LLaMA-3.1-8B model. To overcome the limitations of existing datasets, we construct a comprehensive smart contract vulnerability dataset with detailed explanations and precise location information, covering four vulnerability types. This dataset is constructed through a three-step process: automated generation, LLM-based evaluation, and human expert verification and refinement. Specifically, we utilize the largest parameter versions of state-of-the-art LLMs (Qwen2 and Mistral-Large) to generate detection results, explanations, and specific vulnerability locations through carefully designed prompts. Llama-3.1-70B-Instruct serves as a judge model, evaluating these explanations on correctness, completeness, and conciseness. It scores each aspect from 1 to 10 to select the highest-quality explanations. Finally, human experts review the selected high-scoring explanations, verify their accuracy and make necessary improvements. This approach addresses the issue of insufficient dataset quality and comprehensiveness in existing resources. Furthermore, we introduce Smart Contract-Specific Continual Pre-Training to enhance the model’s understanding of smart contract-specific syntax structures and vocabulary, thereby improving the adaptability of LLaMA-3.1-8B to the smart contract domain. This process involves exposing the model to a large corpus of original smart contract code, allowing it to learn the nuances and intricacies of smart contract development. Additionally, we propose Explanation-Guided Fine-Tuning, a novel approach utilizing our constructed smart contract vulnerability explanations to fine-tune the large language model. This process enables the model to comprehend the entire vulnerability detection process. By training on datasets pairing vulnerable code with detailed explanations, Smart-LLaMA learns to both identify vulnerabilities and articulate the reasoning behind its detections. To evaluate the quality of explanations generated by our Smart-LLaMA, we utilize both LLM evaluation and human evaluation. Our evaluation is based on three key dimensions: Correctness, Completeness, and Conciseness, each scored on a 4-point Likert scale [24]. For LLM evaluation, we utilize Llama-3.1-70B-Instruct, carefully designing prompts to guide the model in assessing explanations based on these criteria. For human evaluation, we invite four experienced smart contract security experts. Each expert dedicate 8 hours to the assessment process, resulting in a total of 32 hours of in-depth analysis. The experts use the same 4-point Likert scale [24]. To ensure consistency, we arrange for 20% overlapping evaluation samples. We then tabulate the number of explanations receiving each score (1-4) for each dimension, providing a clear distribution of the quality assessments for both the baseline (LLaMA-3.1-8B-Instruct) and our Smart-LLaMA approach. We evaluated our Smart-LLaMA framework on a challenging dataset [19] encompassing four major vulnerability types: reentrancy, timestamp dependency, integer overflow/underflow, and delegatecall. The results demonstrated that Smart-LLaMA significantly outperformed state-of-the-art methods across all vulnerability types. Notably, Smart-LLaMA achieved F1 scores 7.35%, 1.24%, 7.82%, and 9.55% higher for reentrancy, timestamp dependency, integer overflow/underflow, and delegatecall vulnerabilities compared to the previous best performers. In terms of accuracy, Smart-LLaMA surpassed the previous SOTA methods by 4.14%, 0.62%, 4.83%, and 5.53% for these four vulnerability types. In addition to detection performance, we evaluated the quality of vulnerability explanations generated by Smart-LLaMA. Both LLM evaluation and human evaluation demonstrated that Smart-LLaMA produced more accurate, comprehensive, and concise explanations compared to LLaMA-3.1-8B-Instruct. For instance, in the human evaluation, Smart-LLaMA achieved the highest score (4 out of 4) for correctness, completeness, and conciseness in 69.5%, 57.1%, and 65.6% of cases, respectively, significantly outperforming the baseline method. The main contributions of this paper are as follows: • We propose Smart-LLaMA, a novel method combining smart contract-specific pre-training and explanation-guided fine-tuning for smart contract vulnerability detection, achieving state-of-the-art performance on four main vulnerability types. • We construct a high-quality smart contract vulnerability dataset that not only provides label, but also includes detailed vulnerability explanations, overcoming the limitations of existing datasets. • To the best of our knowledge, we are the first to explore explanation quality in smart contract vulnerability detection. We validate Smart-LLaMA’s effectiveness in generating high-quality explanations through both LLM evaluation and human evaluation. We have made all source code and datasets utilized in this research available to the public at https://zenodo.org/records/13860344"
https://arxiv.org/html/2411.06027v1,A Toolkit for Measuring the Impacts of Public Funding on Open Source Software Development,"Governments are increasingly employing funding for open source software (OSS) development as a policy lever to support the security of software supply chains, digital sovereignty, economic growth, and national competitiveness in science and innovation, among others. However, the impacts of public funding on OSS development remain poorly understood, with a lack of consensus on how to meaningfully measure them. This gap hampers assessments of the return on public investment and impedes the optimisation of public-interest funding strategies. We address this gap with a toolkit of methodological considerations that may inform such measurements, drawing on prior work on OSS valuations and community health metrics by the Community Health Analytics Open Source Software (CHAOSS) project as well as our first-hand learnings as practitioners tasked with evaluating funding programmes by the Next Generation Internet initiative and the Sovereign Tech Agency. We discuss salient considerations, including the importance of accounting for funding objectives, project life stage and social structure, and regional and organisational cost factors. Next, we present a taxonomy of potential social, economic, and technological impacts that can be both positive and negative, direct and indirect, internal (i.e. within a project) and external (i.e. among a project’s ecosystem of dependents and users), and manifest over various time horizons. Furthermore, we discuss the merits and limitations of qualitative, quantitative, and mixed-methods approaches, as well as options for and hazards of estimating multiplier effects. With this toolkit, we contribute the multi-stakeholder conversation about the value and impacts of funding on OSS developers and society at large.","Open source software (OSS) 111OSS is software whose source code is distributed under a license that permits the use, study, modification, and redistribution of the software source code [1]. Please note that we use OSS rather than FOSS, which stands for “free and open source software”. are digital public goods that are increasingly recognised as digital infrastructure [2, 3], which are used in around 96% of codebases [4] and constitute up to 90% of commercial software stacks [5]. The ubiquitous use of OSS, which are often developed and maintained by volunteer communities, have drawn attention to the question of funding as a mechanism to support the sustainability of OSS projects [6, 7]. In particular, the discovery of major security vulnerabilities in widely used OSS projects, such as the Log4Shell vulnerability in November 2021, highlighted the need to fund security and maintenance work in critical OSS projects [8], and contributed to an understanding of the role and responsibility of the public sector as a funder of OSS development [9, 10]. Concurrently, policymakers increasingly recognise OSS funding as a policy lever to support digital sovereignty [11, 12], the growth of domestic software markets [13], and national competitiveness in science and innovation [14, 15], among others. While governmental interest and involvement in funding OSS funding is increasing, its impacts on OSS development is poorly understood, with no consensus on how to measure them in a meaningful way. Not only is the measurement of the impacts of OSS funding methodologically challenging, but it is complicated by the fact that introducing funding into OSS projects may change contributor incentives and the balance of voluntary and paid participation [16, 17]. This gap hinders assessments of return on public investment and optimising public interest funding strategies. We address this problem with a toolkit of methodological considerations for measuring the impacts of public funding on OSS development. It is informed by methodologies for OSS valuations and community health measurements, in particular metrics developed by the Community Health Analytics Open Source Software (CHAOSS) project, as well as our first-hand insights from developing impact measurement frameworks for the Next Generation Internet (NGI) initiative at the European Commission (EC)and the Sovereign Tech Agency (STA) in Germany. The toolkit begins with a discussion of key considerations, including funding objectives, project life stages and social structures, and regional and organisational cost factors. Then, we present a taxonomy of potential social, economic, and technological impacts that can be both positive and negative, direct and indirect, internal (i.e. within a project) and external (i.e. among a project’s ecosystem of dependents and users), and manifest over various time horizons. Next, we discuss the merits and limitations of qualitative, quantitative, and mixed-methods approaches for measuring such impacts, and options for and hazards of estimating multiplier effects. This toolkit is not exhaustive or prescriptive, nor does it aim to be; rather, we seek to provide a toolkit that can inform the multi-stakeholder debate about the value of public funding for OSS development and how to meaningfully measure its impacts. This paper has the following structure. Section 2 reviews relevant literature and practice, providing a background on prior scholarship on OSS funding, OSS valuations, and OSS community health measurements, as well as real-world examples of public funding by the NGI and STA. Section 3 presents the toolkit for measuring the impacts of public funding on OSS development. It includes a discussion of pertinent considerations, such as funding objectives (3.1.1); salary structures (3.1.3); a taxonomy of social, economic, and technological impacts (3.2); multiplier effect estimations (3.5); and an evaluation of qualitative, quantitative, and mixed-methods approaches (Section 3.4). Section 4 discusses the overarching considerations and future research directions. Finally, Section 5 concludes with a call to action for both diverse OSS stakeholders to engage in the debate about the value and impacts of public funding on OSS development."
