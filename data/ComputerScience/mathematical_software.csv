URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.09859v1,Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures,"The factorization of skew-symmetric matrices is a critically understudied area of dense linear algebra (DLA), particularly in comparison to that of symmetric matrices. While some algorithms can be adapted from the symmetric case, the cost of algorithms can be reduced by exploiting skew-symmetry. A motivating example is the factorization X=LTL^{T} of a skew-symmetric matrix X, which is used in practical applications as a means of determining the determinant of X as the square of the (cheaply-computed) Pfaffian of the skew-symmetric tridiagonal matrix T, for example in fields such as quantum electronic structure and machine learning. Such applications also often require pivoting in order to improve numerical stability. In this work we explore a combination of known literature algorithms and new algorithms recently derived using formal methods. High-performance parallel CPU implementations are created, leveraging the concept of fusion at multiple levels in order to reduce memory traffic overhead, as well as the BLIS framework which provides high-performance gemm kernels, hierarchical parallelism, and cache blocking. We find that operation fusion and improved use of available bandwidth via parallelization of bandwidth-bound (level-2 BLAS) operations are essential for obtaining high performance, while a concise C++ implementation provides a clear and close connection to the formal derivation process without sacrificing performance.","The reduction of a skew-symmetric matrix (i.e. X^{T}=-X) to tridiagonal form is an important technique in the manipulation of such matrices in dense linear algebra (DLA), for example enabling the rapid computation of \operatorname{det}(X), solution of systems of equations XY=B, and computation of the matrix inverse X^{-1}, among others. As the eigenvalues of a skew-symmetric matrix are purely imaginary, reduction to diagonal form is not possible in real arithmetic, and while reduction to a 2x2 diagonal-blocked form is possible, tridiagonalization provides a convenient middle ground. Such a factorization can be computed by applying and accumulating Gauss transforms, X=LTL^{T} or with column pivoting PXP^{T}=LTL^{T} with lower unit-triangular matrix L and permutation matrix P, or via Householder reduction X=QTQ^{T} with orthogonal Q. As the former decomposition can be obtained in approximately n^{3}/3 FLOPs for an n\times n matrix X and can utilize efficient level-3 BLAS operations for the majority of the computation [13, 22, 25], we do not further consider the Householder approach. The Pfaffian of the tridiagonal factor T satisfies \operatorname{Pf}(T)^{2}=\operatorname{det}(X), with \operatorname{Pf}(T)=\prod_{i=0}^{\lceil n/2\rceil-1}\tau_{2i+1,2i}. Note that we use uppercase roman letters to denote matrices and submatrices, lowercase roman letters to denote column (sub)vectors, and lowercase Greek letters to refer to scalars. Thus, the computation of the Pfaffian via tridiagonal factorization is a critical kernel in scientific applications working with skew-symmetric matrices, such as in machine learning (Markov random fields [10]), physics (partition functions of Ising spin models [15]), and quantum chemistry/materials science (electronic structure quantum Monte Carlo [3]). Previously, the FLAME (Formal Linear Algebra Methods Environment) methodology [9, 6, 16] was used to derive a family of algorithms for X=LTL^{T} factorization [17]. The basic idea is a goal-oriented approach, where one represents the post-condition X=LTL^{T} (and other conditions as required) in a partitioned form: the partitioned matrix expression (PME), \displaystyle\left(\begin{array}[]{c | c}X_{TL}&\star\\ \hline\cr X_{BL}&X_{BR}\\ \end{array}\right)=\left(\begin{array}[]{c | c}L_{TL}&0\\ \hline\cr L_{BL}&L_{BR}\\ \end{array}\right)\times (5) \displaystyle\quad\quad\quad\left(\begin{array}[]{c | c}T_{TL}&-\tau_{BL}e_{l}% e_{f}^{T}\\ \hline\cr\tau_{BL}e_{f}e_{l}^{T}&T_{BR}\\ \end{array}\right)\left(\begin{array}[]{c | c}L_{TL}^{T}&L_{BL}^{T}\\ \hline\cr 0&L_{BR}^{T}\\ \end{array}\right) (10) where \star indicates redundant data (due to the skew-symmetry of X), and e_{f}/e_{l} are column Euclidean unit vectors with a non-zero entry in the first and last position, respectively. For later use, we also define \bar{E}_{f} (\bar{E}_{l}) as the collection of all column unit vectors except e_{f} (e_{l}), i.e. the identity matrix with the first (last) column dropped. Essentially, inspection of the PME gives rise to a family of algorithmic invariants, which each then leads mechanically to a concrete algorithm. In this paper, we present high-performance parallel shared-memory CPU implementations of this family of algorithms leveraging the concept of fusion at multiple levels in order to reduce memory traffic overhead, as well as the BLIS framework [19, 18] which provides high-performance gemm kernels, hierarchical parallelism, and cache blocking. As in [25], we add basic skew-symmetric BLAS operations as modifications of existing operations, while also re-implementing certain key operations (e.g. skr2) using in-house code. The specific contributions of this paper are: • High-performance shared-memory parallel implementation of six skew-symmetric LTL^{T} factorization algorithms, of which only two have been previously implemented (skew-symmetric Parlett-Reid and Aasen’s algorithms). All algorithms include optional partial pivoting, except the “two-step” and blocked left-looking algorithms. • Implementation of key skew-symmetric level-2 and level-3 BLAS operations in the BLIS framework. • Additional hand-optimized level-2 BLAS implementations, including shared-memory parallelization and operation fusion. • Implementation of fused “sandwich product” level-3 operations using the BLIS framework. • Algorithmic modifications which expose additional opportunities for operation fusion. • An expressive C++ interface for implementing both blocked and unblocked DLA operations which closely mirrors the FLAME notation while enabling extensive compiler optimization. • Benchmarking of our own and related implementations on AMD “Milan” EPYC 7763 CPUs. These contributions impact not only the efficient implementation of X=LTL^{T} factorization, but also showcase the benefits of our approach, leveraging formal derivation together with expressive APIs and flexible frameworks, to DLA in general. Algorithm: [L,T{\color[rgb]{.75,.5,.25},p}]:=\text{\sc ltlt\_\{blk,unb\}\_{\emph{var}}}(X) L=I,\quad T=0,\quad{\color[rgb]{.75,.5,.25}p=0}"
https://arxiv.org/html/2411.10143v1,Cascaded Prediction and Asynchronous Execution of Iterative Algorithms on Heterogeneous Platforms,"Owing to the diverse scales and varying distributions of sparse matrices arising from practical problems, a multitude of choices are present in the design and implementation of sparse matrix-vector multiplication (SpMV). Researchers have proposed many machine learning-based optimization methods for SpMV. However, these efforts only support one area of sparse matrix format selection, SpMV algorithm selection, or parameter configuration, and rarely consider a large amount of time overhead associated with feature extraction, model inference, and compression format conversion. This paper introduces a machine learning-based cascaded prediction method for SpMV computations that spans various computing stages and hierarchies. Besides, an asynchronous and concurrent computing model has been designed and implemented for runtime model prediction and iterative algorithm solving on heterogeneous computing platforms. It not only offers comprehensive support for the iterative algorithm-solving process leveraging machine learning technology, but also effectively mitigates the preprocessing overheads. Experimental results demonstrate that the cascaded prediction introduced in this paper accelerates SpMV by 1.33x on average, and the iterative algorithm, enhanced by cascaded prediction and asynchronous execution, optimizes by 2.55x on average.","Conjugate gradient (CG) and global minimum residual (GMRES), both grounded in the Krylov subspace, are the mainstream iterative algorithms for solving large-scale sparse linear equations. These algorithms include sparse matrix-vector multiplication (SpMV), vector inner product, and other computational kernels. Among them, SpMV comprises the largest proportion of the overall running time of the two iterative algorithms [1], thereby garnering extensive attention from researchers over the past two decades [2]. Typically, sparse matrices from real-world engineering problems are rich in zeros, necessitating storage in a compressed format. The irregular distribution of non-zeros in sparse matrices has attracted different compression format designs. The considerable research effort is focused on designing new sparse matrix compression formats and their associated SpMV algorithms [2]. However, according to existing studies, no compression format has been found that consistently delivers good performance across all sparse matrices [3], and there is significant variation in the performance of different SpMV algorithms designed for the same sparse format. The most notable example is the implementation of SpMV in the CSR format. Algorithms based on CSR employ diverse strategies such as sparse matrix partitioning, data cache optimization, and load balancing, resulting in significant disparities in SpMV’s computational efficiency across different sparse matrices. Moreover, even within a single algorithm, issues of parameter setting arise, with optimal parameter configurations differing for various sparse matrices. For instance, Gao et al. [4] examined the thread configuration issue of the CSR-Vector algorithm [5, 6] in CUSP [7]. The SpMV using the optimal thread configuration achieved an average speedup of 3x and a maximum speedup of 20x compared with the SpMV with default settings. As machine learning (ML) technology becomes more widespread, researchers address these issues by gathering performance data and training machine learning models. These efforts can be classified mainly into three categories: the optimal compression format prediction [8][9][10][3][11], the optimal SpMV algorithm prediction [12][13][14], and the optimal parameter setting prediction [12][15][16][17][4]. Although these methods have improved SpMV performance to some extent, they still face the following challenges: (1) Current ML-based optimization efforts only cover the prediction of a single area within the compressed formats, SpMV algorithms, or parameter settings. The search space is so small that there is still a gap from the best SpMV performance. (2) The ML-based SpMV and iterative algorithm optimization introduce significant preprocessing overhead, which offsets the performance benefits from ML-based optimization. Addressing the aforementioned challenges, this paper introduces an ML-based cascade prediction approach for typical iterative algorithms like CG and GMRES, targeting heterogeneous platforms. This optimization spans three key areas: selecting sparse matrix compression formats, SpMV algorithms, and parameter configurations. Building on this, an asynchronous execution model is proposed, integrating model inference with iterative computation. This model conceals the costs associated with model inference and matrix preprocessing, fully leverages the heterogeneous computing resources of CPU-GPU, and improves the operational efficiency of iterative algorithms on these platforms. This paper’s main contributions include: • We propose a lightweight cascading prediction framework for ML-based SpMV to mitigate the suboptimal performance caused by single area prediction. • We propose an asynchronous execution model for iterative algorithms on heterogeneous platforms to hide the preprocessing overheads. • We take the GMRES algorithm as an example to prove the efficiency of our method using 22 sparse matrices."
https://arxiv.org/html/2411.06631v1,SequentialSamplingModels.jl: Simulating and Evaluating Cognitive Models of Response Times in Julia,"Sequential sampling models (SSMs) are a widely used framework describing decision-making as a stochastic, dynamic process of evidence accumulation. SSMs popularity across cognitive science has driven the development of various software packages that lower the barrier for simulating, estimating, and comparing existing SSMs. Here, we present a software tool, SequentialSamplingModels.jl (SSM.jl), designed to make SSM simulations more accessible to Julia users, and to integrate with the Julia ecosystem. We demonstrate the basic use of SSM.jl for simulation, plotting, and Bayesian inference.","Sequential sampling models (SSMs) are widely used in cognitive science due to their ability to describe dissociable processes underlying a wide variety of capacities, including memory retrieval, visual perception, and decision making (Forstmann et al., (2016)). These models typically describe decision-making as a stochastic, dynamic evidence accumulation process which evolves until evidence for one option reaches an evidence threshold (Smith, (2000); Ratcliff & McKoon, (2008); Ratcliff, (1978)). By doing so, SSMs provide a generative model for response time (RT) distributions of actions made by organisms capturing both the speed of the response and the response itself. Figure 1 illustrates the latent evidence accumulation process for a hypothetical decision between a Margherita pizza and a pineapple pizza. Evidence accumulates stochastically (i.e., randomly) until it reaches either the upper boundary, triggering the selection of the Margherita, or the lower boundary, triggering the selection of the pineapple pizza. While these models hold many promising applications, their implementation and use remain a technical challenge. The widespread interest and continuous use of SSMs across cognitive science have spurred the development of various software packages that lower the barrier for simulating, estimating, and comparing existing SSMs. These software implementations range widely, from those designed to provide maximum flexibility to end-users (Shinn et al., (2020)) to those that streamline the entire analysis of SSMs (Fengler et al., (2022)). Toolboxes have also been developed across multiple programming languages commonly used for scientific computing, including MATLAB (Vandekerckhove & Tuerlinckx, (2008)), Python (Wiecki et al., (2013); Shinn et al., (2020); Fengler et al., (2022); Murrow & Holmes, (2024)), and R (Wabersich & Vandekerckhove, (2014); Stevenson et al., (2024); Ahn et al., (2017); Singmann et al., (2018); Hartmann & Klauer, (2021)). Figure 1: An example of applying the sequential sampling modeling framework to a choice between two pizzas. The decision-maker samples evidence in favor of both options until reaching a decision boundary."
