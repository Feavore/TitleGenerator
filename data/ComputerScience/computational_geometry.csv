URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10043v1,Constant Workspace Algorithms for Computing Relative Hulls in the Plane,"The constant workspace algorithms use a constant number of words in addition to the read-only input to the algorithm stored in an array. In this paper, we devise algorithms to efficiently compute relative hulls in the plane using a constant workspace. Specifically, we devise algorithms for the following three problems: (i) Given two simple polygons P and Q with P\subseteq Q, compute a simple polygon P^{\prime} with a perimeter of minimum length such that P\subseteq P^{\prime}\subseteq Q. (ii) Given two simple polygons P and Q such that Q does not intersect the interior of P but it does intersects with the interior of the convex hull of P, compute a weakly simple polygon P^{\prime} contained in the convex hull of P such that the perimeter of P^{\prime} is of minimum length. (iii) Given a set S of points located in a simple polygon P, compute a weakly simple polygon P^{\prime}\subseteq P with a perimeter of minimum length such that P^{\prime} contains all the points in S. To our knowledge, no prior works devised algorithms to compute relative hulls using a constant workspace and this work is the first such attempt.","A polygon is the image of a piecewise linear closed curve. A simple polygon is the image of a piecewise linear simple closed curve such that any two successive line segments along its boundary intersect at their endpoints. A polygon P with at least three sides is called a weakly simple polygon whenever each of the vertices of P can be perturbed by at most \epsilon to obtain a simple polygon for every real number \epsilon>0. A simple polygon P in \mathbb{R}^{2} is convex whenever the line segment pq is contained in P for every two points p,q\in P. Computing the convex hull in the plane is a fundamental problem in computational geometry. Specifically, the following two problems are of interest: (i) Given a set S of n points in the plane, finding a convex (simple) polygon CH(S) with perimeter of minimum length so that CH(S) contains all the points in S. (ii) Given a simple polygon P in the plane defined with n vertices, finding a convex (simple) polygon CH(P) with perimeter of minimum length that contains P. It is well known that both CH(S) and CH(P) are unique. The famous algorithms for the first problem include Jarvis’s march (a specialization of Chand and Kapur’s gift wrapping), Graham’s scan, Quickhull algorithm, Shamos’s merge algorithm, Preparata-Hong’s merge algorithm, incremental and randomized incremental algorithms. Many of these algorithms are detailed in popular textbooks on computational geometry, Preparata and Shamos [21] and de Berg et al. [15]. The worst-case lower bound on the time complexity of the first problem is known to be \Omega(n\lg{h}). Several optimal algorithms that take \Theta(n\lg{h}) worst-case time are known; for example, algorithms by Kirkpatrick and Seidel [18] and Chan [13]. For the second problem, several algorithms with O(n) time complexity, where n is the number of vertices of P, are known; the one by Lee is presented in [21]. The relative hull, also known as the geodesic convex hull, has received increasing attention in computational geometry, which appears in a variety of applications in robotics, industrial manufacturing, and geographical information systems. The relative hulls and their related structures based on geodesic metrics have been used to approximate curves and surfaces in digital geometry. The following two specific problems on relative hulls are famous: (i) Given two simple polygons P and Q with P\subset Q, computing a simple polygon, known as the relative hull of P with respect to Q, denoted by RH(P|Q), with a perimeter of minimum length such that P\subseteq RH(P|Q)\subseteq Q. (ii) Given a set S of points located in a simple polygon Q, computing a weakly simple polygon, known as the relative hull of S with respect to Q, denoted by RH(S|Q), with a perimeter of minimum length such that RH(S|Q) contains all the points in S and is contained in Q. Toussaint [23] devises algorithms for both of these problems. The main idea of their algorithm for the first problem is as follows. For every edge e of CH(P), if e does not intersect any edge of Q, then e is included in the relative hull. Otherwise, their algorithm identifies a simple polygon P^{\prime} formed by the pocket defined by e sans the exterior of Q, and all the edges of the geodesic shortest path between the endpoints of e in P^{\prime} are included in the relative hull. More details of this algorithm are given in Section 2. For the second problem, the algorithm given in [23] computes a weakly simple polygon Q^{\prime} and then transforms that polygon into a simple polygon Q^{\prime\prime} with a perimeter of minimum length. The polygon Q^{\prime} is computed by triangulating the polygon Q, finding convex hulls of points of S lying in each triangle of that triangulation, and using the dual-tree of that triangulation to connect specific points on the boundaries of some of these hulls with geodesic shortest paths. This algorithm is further detailed in Section 4. The polygon Q^{\prime\prime} is obtained by finding a geodesic shortest path between two special points such that this path is a simple cycle located in Q\backslash Q^{\prime}. In this paper, we consider three problems on relative hulls and devise algorithms to compute them using a constant workspace. The workspace of an algorithm is the additional space, excluding both the input and output space complexities, required to execute that algorithm. Traditionally, workspace complexity has been playing only second fiddle to time complexity; however, due to the limited space on chips, algorithms need to use space efficiently, for example, in embedded systems. For such reasons, algorithms using a small footprint are desired. The following are the three computation models that are popular in designing algorithms focusing on workspace efficiency: algorithms that work using a constant workspace, in-place algorithms, and algorithms that trade-off between time and workspace. In constant workspace algorithms, input is read-only, output is not stored but streamed, and these algorithms use an O(1) workspace. The following are the well-known constant workspace algorithms: shortest paths in trees and simple polygons by Asano et al. [5], triangulating a planar point set and trapezoidalization of a simple polygon by Asano et al. [4], and separating common tangents of two polygons by Abrahamsen [1]. In the constant workspace model, Reingold [22] devised an algorithm to determine whether a path exists between any two given nodes in an undirected graph, settling a significant open problem. In-place algorithms also use O(1) workspace; however, the input array is not read-only. That is, at any instant during the algorithm’s execution, the input array can contain any permutation of elements of the initial input array. Strictly speaking, these algorithms use O(n) workspace, where n is the input size. Here are some of the notable in-place algorithms from the literature: computing convex hulls in the plane by Bronnimann et al. [12] and Bronnimann et al. [11]. For algorithms that provide trade-off between time and workspace, s is given as an input parameter, saying the worst-case upper bound on the workspace is O(s). The time complexity of such algorithms is expressed as a function of n and s, making it viable to provide an algorithmic scheme to achieve a trade-off between workspace complexity and time complexity, a lower asymptotic worst-case time complexity as s grows and larger asymptotic worst-case time complexity as s gets smaller. The notable results in this model include the algorithm by Asano et al.’s algorithm to compute all-nearest-larger-neighbors problems [3], Darwish and Elmasry’s planar convex hull algorithm [14], Asano et al.’s algorithm for computing the shortest path in a simple polygon [3], Peled’s algorithm for computing the shortest path in a simple polygon [16], Aronov et al.’s algorithm for triangulating a simple polygon [2], and algorithms for triangulation of points by Korman et al. [20] and Banyassady et al. [9], an algorithm for computing the k-visibility region of a point in a simple polygon by Bahoo et al. [6], and Banyassady et al.’s algorithm for computing the Euclidean minimum spanning tree [7]. Barba et al. [10] devises a framework that trades-off between time and workspace for applications that use the stack data structure. (A variation of this model considers O(s\lg{n}) number of bits as the workspace.) Note that when s is 1, these algorithms are essentially constant workspace or in-place algorithms, depending on whether the input array is read-only or has a permutation of input objects. A detailed survey of these models and famous algorithms in these models can be found in [8]. As mentioned earlier, in this paper, we devise algorithms for computing relative hulls in the plane using a constant workspace. The convex hull of a set S of points in the plane is denoted by CH(S). The convex hull of a simple polygon P is the convex hull of vertices of P, denoted by CH(P). We denote the number of vertices of any (weakly) simple polygon P with |P|. For any simple polygon P, the boundary (cycle) of P^{\prime} is denoted by bd(P). The interior of a (weakly) simple polygon P is denoted by int(P). The relative interior of a (weakly) simple polygon P is the region resultant from the union of int(P) and the boundary of P. For any edge e of CH(P) with e\notin P, the pocket of e is the simple polygon in CH(P)\backslash P with e on its boundary. And, e is said to be the lid of pocket induced by e. Let r^{\prime} and r^{\prime\prime} be two rays with origin at p. Let \overrightarrow{v_{1}} and \overrightarrow{v_{2}} be the unit vectors along the rays r^{\prime} and r^{\prime\prime}, respectively. A cone C_{p}(r^{\prime},r^{\prime\prime}) is the set of points defined by rays r^{\prime} and r^{\prime\prime} such that a point q\in C_{p}(r^{\prime},r^{\prime\prime}) if and only if q can be expressed as a convex combination of the vectors \overrightarrow{v_{1}} and \overrightarrow{v_{2}} with positive coefficients. When the rays are evident from the context, we denote the cone with C_{p} or C. Our Contributions Traditional algorithms for finding relative hulls proceed by designing data structures that use a workspace whose size is linear in the input complexity. Since there is no restriction on the memory, these algorithms use data structures, such as stack, queue, or a doubly-connected edge list, to save the intermediate structures such as hulls or sub-polygons. Saving information in these data structures helps to avoid re-computations. However, the workspace constraints in a constant workspace setting do not permit the liberal use of such data structures to store processed information. Hence, using a constant workspace makes the problem of computing relative hulls harder, requiring engineering a solution with special techniques devoted to constant workspace algorithms. Usually, a constant time operation using a linear workspace could take linear time when only a constant workspace is allowed, increasing the overall time complexity to quadratic in the input size and even beyond in some cases. This paper proposes algorithms for finding relative hulls in the plane for three problems. Each of these algorithms uses a constant workspace. The first problem computes the relative hull of a simple polygon P, which is contained in another simple polygon Q. The algorithm by Toussaint [23] for this problem takes O(|P|+|Q|) time while using O(|P|+|Q|) workspace. Our algorithm for this problem takes O(|P|^{2}+|Q|^{2}) time in the worst-case while using O(1) workspace. Using some of the ideas from this algorithm, we proposed an O(|P|^{2}+|Q|^{2}) time and O(1) workspace algorithm for computing the relative hull of a simple polygon P when another simple polygon Q is placed such that int(P)\cap Q=\phi but int(CH(P))\cap Q\neq\phi. We call this problem a simple polygon placed alongside another simple polygon. Toussaint [24] gave an algorithm for this problem which takes O(|P|+|Q|) time and O(|P|+|Q|) workspace. We modify the constant workspace algorithm given in Asano et al. [4] to compute the geodesic shortest path in a simple polygon and use this modified version to provide constant workspace algorithms for these two problems. The third problem computes the relative hull of a set S of points in a simple polygon Q. The algorithm by Toussaint [23] for this problem takes O(|Q|+(|S|\lg{|S|})) time but uses O(|Q|+|S|) workspace. The constant workspace algorithm we present for this problem is more involved. As part of devising this algorithm, we engineered a solution by carefully combining constant workspace algorithms for several problems: Eulerian tour of trees, constrained Delaunay triangulation of a simple polygon, constructing the triangulation in an online fashion, computing the shortest geodesic path in a sleeve, and partially computing the hulls of points lying in each triangle of triangulation, to name a few. However, unlike the algorithm in [23], our algorithm for this problem outputs a weakly simple polygon instead of a simple polygon. Our algorithm takes O(|Q|^{3}+|S|^{2}) time in the worst case. In these three algorithms, at several junctures, we predominantly exploit computing instead of storing design paradigm. This paradigm is both natural and commonly used in space-constrained algorithms, wherein instead of storing information computed in a data structure, to save workspace, we re-compute those elements as and when needed. To our knowledge, the algorithms presented in this paper are the first algorithms to compute relative hulls using constant workspace. Section 2 presents an algorithm for computing the relative hull of a simple polygon located inside another simple polygon. Section 3 gives an algorithm for computing the relative hull of simple polygon P when another simple polygon is placed alongside P. An algorithm for computing the relative hull of a set of points located in a simple polygon is given in Section 4. Conclusions are in Section 5."
https://arxiv.org/html/2411.10207v1,Insights from a workshop on gamification of research in mathematics and computer science,"Can outreach inspire and lead to research and vice versa? In this work, we introduce our approach to the gamification of research in mathematics and computer science through three illustrative examples. We discuss our primary motivations and provide insights into what makes our proposed gamification effective for three research topics in discrete and computational geometry and topology: (1) DominatriX, an art gallery problem involving polyominoes with rooks and queens; (2) Cubical Sliding Puzzles, an exploration of the discrete configuration spaces of sliding puzzles on the d-cube with topological obstructions; and (3) The Fence Challenge, a participatory isoperimetric problem based on polyforms. Additionally, we report on the collaborative development of the game Le Carré du Diable, inspired by The Fence Challenge and created during the workshop Let’s talk about outreach!, held in October 2022 in Les Diablerets, Switzerland. All of our outreach encounters and creations are designed and curated with an inclusive culture and a strong commitment to welcoming the most diverse audience possible.","To set the stage, we begin this contribution by introducing one of our research topics through a small puzzle. After all, if you are reading this article, there is a good chance you enjoy learning about and solving puzzles. The Fence Challenge with tetrominoes Using the following five pieces (called tetrominoes), enclose the biggest area possible. ilnot (1) Using the five pieces depicted above in (1), which you might recognise from the game Tetris [pajitnov1984tetris], try to enclose the largest possible area on a square grid. To clarify the objective of the puzzle, we present a hypothetical dialogue between an imaginary reader and us. Figure 1: Left, a corner-connection, not closing the fence; right an edge-connection with the rook-movement — Reader: The puzzle asks us to enclose the largest area, but how do we define “enclosing”? — Us: Great question! We want to use the five tetrominoes… — Reader (interrupting): Tetrominoes? — Us: Yes, that’s the term we use for the pieces! “Tetro-” means “four,” and “mino” comes from “domino,” so this means “like a domino but with four tiles”. — Reader: Got it! Please continue. — Us: Right! An area composed of a set of unit squares on the grid is considered “enclosed” if it is bounded by an edge-connected fence made of tetrominoes. — Reader: By “edge-connected,” do you mean I can’t connect the pieces by their corners? — Us: More or less! While it’s possible for some pieces to touch at the corners, they still need to be connected through their edges. To clarify, the fence you build with the five tetrominoes must be rook-connected, meaning you should be able to move a chess rook from any square of the fence to any other square, as if the entire grid were a chessboard. — Reader (taking two tetrominoes): I see! If I place two tetrominoes together like this (Figure 1, right), it is rook-connected, but not in this other arrangement (Figure 1, left), right? — Us: Exactly! Now try to build a fence using four of the five pieces, say i, l, n, t. — Reader: Hmm, like this (Figure 2)? Is that allowed? — Us: Yes! And see, you’ve created two holes! It is allowed, but do you think it will give you the largest possible enclosed area? — Reader: Probably not! Hmm, I’ll work on it a bit more and come back to you when I find a good solution using all five pieces. Puzzles, such as the one above, are a good way to engage people in thinking logically and using mathematics as a way to model them and solve them, but we wish to do so on a different level, finding ways to go beyond puzzles and really to “gamify” mathematical research to let it be experienced by the public. In its ideal form, it would also enable the participation of the public in the research itself, providing a sort of feedback loop between the researchers and the people experiencing the game, making it a collaborative endeavour. Figure 2: A tentative of tetromino fence (without the “o”). The primary example of gamification of research presented in this paper, the Fence Challenge, was inspired by a generalization of the tetromino fence problem described above [LRMR23]. Additionally, we discuss a collaborative game, Le Carré du Diable, developed with input from participants of the workshop Gamification of Research, held during the conference Let’s talk about outreach!. During the same workshop, we also introduced two other fully developed instances of gamification of research: one based on the computational complexity of art-gallery problems on polycubes [AR21, LRMR22], and another stemming from the study of higher-dimensional cubical sliding puzzles [BMRV23] (see Sections 2.1 and 2.2). It is not the goal of this contribution to provide fully developed theoretical arguments regarding the benefits of gamification. The concept of gamification and its implications form a rich field of study that requires far more space to address comprehensively. Here, we aim, instead, to highlight certain design choices we made and share insights gained during our outreach activities. In the future, we hope to systematically analyse our setups and the behaviour of participants to better understand the impact of our approach. Our outreach efforts aim to engage the widest audience possible, with particular emphasis on participants who might not typically enjoy mathematics. Beyond merely exposing them to mathematical concepts, we aim to immerse them in contemporary research and even invite them to contribute to it. Games serve as a powerful medium for achieving these goals by breaking down barriers and fostering curiosity. The ludic potential of games can transform attitudes toward mathematics [BHMW85], and research shows that enjoyment of mathematics is one of the strongest predictors of student success [DDYF20]. Furthermore, games enable collective engagement, allowing participants and their surrounding communities to share the experience of mathematical discovery [MSJCJCSS17, QP22]. The idea of presenting mathematics through games has a long and rich history. From the educational tools of the tutor in Rousseau’s Émile [Rousseau1762] to Martin Gardner’s Mathematical Games columns [GardnerMathGames], games have been used to make mathematics accessible and engaging. Foundational works like Berlekamp, Conway, and Guy’s Winning ways for your mathematical plays [BCG04] study games mathematically, inspiring further gamification efforts. Recent examples, such as Ravi Vakil’s “bedtime story” for understanding long exact sequences [Ra21], Lee and Hua’s Homotopy Type Theory quests [HoTT], and Buzzard and Pedramfar’s Natural Number Game [Buzz], demonstrate how games can bridge the gap between recreational engagement and mathematical research. These examples illustrate the transformative potential of games, and we hope that the experiences shared here will inspire others to explore the interplay between games and research-level mathematics. We now provide an overview of the sections in this contribution. Section 2 briefly presents our first two examples of gamification along with some of our design choices. Section 3 focuses on the Fence Challenge, the gamification exercise proposed to participants of Let’s talk about outreach!, and the collaborative game developed during this workshop. In Section 4, we synthesise our observations and outline the next steps in our research. Lastly, readers interested in the puzzle introduced earlier in this section can find its complete solution in Appendix A, as well as supplementary figures for the game in Appendix B."
https://arxiv.org/html/2411.08204v1,A well-separated pair decomposition for low density graphs,"Low density graphs are considered to be a realistic graph class for modelling road networks. It has advantages over other popular graph classes for road networks, such as planar graphs, bounded highway dimension graphs, and spanners. We believe that low density graphs have the potential to be a useful graph class for road networks, but until now, its usefulness is limited by a lack of available tools.In this paper, we develop two fundamental tools for low density graphs, that is, a well-separated pair decomposition and an approximate distance oracle. We believe that by expanding the algorithmic toolbox for low density graphs, we can help provide a useful and realistic graph class for road networks, which in turn, may help explain the many efficient and practical heuristics available for road networks.","1.1 Motivation Advances in technology have made the collection of geographic data easier than ever before. Nowadays, continent-sized road networks are stored in graphs with up to a billion vertices and edges. To analyse these large graphs, highly efficient algorithms and data structures are required. Many heuristics for analysing road networks are highly efficient in practice. One explanation as to why many heuristics are efficient on road networks but inefficient on general graphs is that these heuristics can exploit the underlying properties of road networks. For example, experiments show that road networks have desirable underlying properties such as small separators [30, 31] and bounded maximum degree [19, 35]. A common structure that exists for road networks that does not exist for general graphs is an efficient shortest path data structure [7, 17, 45, 52]. Many attempts have been made to explain why road networks have desirable properties such as small separators and efficient shortest path data structures. In the theory community, a popular approach is to argue that road networks belong to a certain graph class, and then to prove a set of desirable properties for the graph class. Numerous graph classes have been proposed for road networks. When assessing these graph classes, it is natural to consider two criteria [36]. Criterion 1 (Usefulness). How well does this graph class explain the desirable properties of road networks? Do graphs in this graph class have small separators, or efficient shortest path data structures? Criterion 2 (Realism). How well does this graph class model real-world road networks? Do all, or most, real-world road networks belong to this graph class? Planar graphs, bounded highway dimension graphs, and spanners are among the most popular graph classes for road networks111In Section 1.4, we provide a more extensive overview of existing graph classes for modelling road networks.. Next, we will assess these popular graph classes using Criteria 1 and Criteria 2. Then, we will introduce low density graphs, which is our preferred graph class for road networks. Planar graphs. Planar graphs are graphs with no edge crossings. An advantage of planar graphs is that it has a broad algorithmic toolbox, including separators [68], shortest path separators [62, 79], cycle separators [71], r-divisions [41], and tree covers [6, 23]. Shortest path separators have been used to construct efficient approximate shortest path data structures in planar graphs [62, 79]. Unfortunately, real-world road networks are far from planar [18, 35]. Non-planar features of road networks include bridges, tunnels and overpasses. Bounded highway dimension. Highway dimension [1], introduced in 2010, is graph parameter for road networks. A graph has highway dimension h if, for every r and every ball of radius 4r, there exists h vertices that cover all shortest paths with length at least r inside the ball. Bounded highway dimension guarantees provable running times for shortest path data structure heuristics, such as Contraction Hierarchies [45], Transit Nodes [7], and Hub Labelling [26]. For many researchers, bounded highway dimension is the preferred graph class for modelling road networks [27, 39, 51, 59, 65]. Unfortunately, it is unknown whether real-world road networks have bounded highway dimension. It is difficult to verify the exact highway dimension of a graph as it is NP-hard to compute [13]. Experiments suggest that even a lower bound for the highway dimension is non-constant, and may be superlogarithmic [16]. Moreover, road networks that follow a grid layout [84] may have a large highway dimension [15]. For example, in Figure 1 (left), the \sqrt{n}\times\!\sqrt{n}-grid has a highway dimension of h=\Theta(\sqrt{n}). Spanners. Spanners are a popular class of geometric networks. For any pair of vertices in a geometric t-spanner, the shortest path distance in the graph between the two vertices is at most t times the Euclidean distance between the vertices. Spanners represent good network design, through the absence of large detours. Geometric spanners have been studied extensively, see the reference textbook [72]. Efficient approximate shortest path data structures have been constructed for spanners [49]. Unfortunately, many road networks are not spanners. Rivers, mountains and valleys impose physical constraints on road networks [74], which can lead to large detours [57]. Even without physical constraints, the suburban cul-de-sac layout [85] prioritises buildable area over connectivity [47, 87], which can also lead to large detours. In Figure 1 (right), the \sqrt{n}\times\!\sqrt{n} comb models a suburban cul-de-sac network, and has a spanning ratio of t=\Theta(\sqrt{n}). Figure 1: Left: A \sqrt{n}\times\!\sqrt{n} grid has a highway dimension, skeleton dimension, and treewidth of \Theta(\sqrt{n}). Right: A \sqrt{n}\times\!\sqrt{n} comb has a spanning ratio of \Theta(\sqrt{n}) and a doubling constant of \Theta(\sqrt{n}). Low density graphs. Low density [81], introduced in 1994, is a versatile geometric property that has been used to model obstacles [82], trajectories [33] and road networks [25]. Low density states that not too many large objects can be in any given area. We formally define low density in Section 2. Low density graphs are considered to be a realistic graph class for modelling road networks. An advantage of low density graphs is that realistic features of road networks, such as edge crossings, grid-like structures (Figure 1, left) and comb-like structures (Figure 1, right), can all be modelled with low density graphs. Experiments support the claim that road networks are low density. It has been verified that the city-sized road networks of San Francisco, Athens, Berlin and San Antonio are all low density graphs [25]. Unfortunately, low density graphs only have a narrow algorithmic toolbox, limiting its usefulness as a graph class. Nonetheless, we believe that low density graphs have the potential to be a useful graph class for road networks. It was shown in 2022 that low density graphs have small separators [66]. Map matching under the Fréchet distance has been studied on low density graphs [20, 25]. But little else is known. In particular, low density graphs lack an efficient shortest path data structure, which is an extremely useful structure in practice. Applications such as navigation systems rely heavily on the existence of efficient shortest path data structures. Open Problem 3. Is there an efficient shortest path data structure for low density graphs? Resolving Open Problem 3 would help bridge the gap, in terms of usefulness, between low density graphs and other popular graph classes. 1.2 Contributions We resolve Open Problem 3 in the affirmative. We construct an approximate shortest path data structure for low density graphs, which has O(n\varepsilon^{-4}\log n) size and can answer (1+\varepsilon)-approximate shortest path queries in O(1) time. Along the way, we construct a (1/\varepsilon)-well-separated pair decomposition of O(n\varepsilon^{-4}\log n) size. The well-separated pair decomposition is a highly influential structure in computational geometry, and has led a wide range of approximation algorithms for proximity problems in Euclidean spaces [21, 22], doubling spaces [54, 78], and unit disc graphs [43]. We hope that our results will pave the way for more tools to be developed for low density graphs, which would further increase the usefulness of low density as a graph class. 1.3 Previous work Gao and Zhang [43] construct a well-separated pair decomposition for unit disc graphs. We regard this to be one of the most relevant results to our work. To the best of our knowledge, their result is the only previous work that constructs a well-separated pair decomposition for a graph class. Gao and Zhang [43] show how to transform a well-separated pair decomposition of a graph into an approximate shortest path data structure. In Section 6.2, we apply essentially the same transformation. The well-separated pair decomposition is a highly influential structure in computational geometry, and is a fundamental tool for designing geometric approximation algorithms. Applications of the well-separated pair decomposition include spanners [21], approximate nearest neighbour queries [4], tree covers [3], and approximate distance oracles [43]. For a thorough treatment of the topic, refer to the textbook chapters by Har-Peled [53, Chapter 3] and by Narasimhan and Smid [72, Chapter 9]. Since the well-separated pair decomposition leads to efficient approximation algorithms for a wide range of proximity problems, a natural open question [72, Chapter 9.6] is: for which metrics spaces does there exist a well-separated pair decomposition of subquadratic size? To the best of our knowledge, the only metrics in which the well-separated pair decomposition has previously been studied are Euclidean spaces, unit disc graphs and doubling spaces. Callahan and Kosaraju [22] constructed a well-separated pair decomposition with O(n/\varepsilon^{d}) pairs and separation constant (1/\varepsilon), for point sets in d-dimensional Euclidean space. For unit disc graphs, Gao and Zhang [43] constructed a well-separated pair decomposition with O(n\varepsilon^{-4}\log n) pairs and a separation constant of (1/\varepsilon). Talwar [78] provided a well-separated pair decomposition with O(n\log\Delta/\varepsilon^{ddim}), where \Delta is the spread of the point set, ddim is the doubling dimension, and (1/\varepsilon) is the separation constant. Har-Peled and Mendel [54] provided an improved, randomised construction, with O(n/\varepsilon^{ddim}) pairs. Low density was among the first realistic input models to be considered in computational geometry, see the introduction of [29]. In van der Stappen’s thesis [81], he introduced low density and proved that non-intersecting fat objects formed a low density environment. A \lambda-low-density environment has at most \lambda large objects in any given area, where an object is large if it has greater diameter than that of the given area. Schwarzkopf and Vleugels [75] constructed a range searching data structure for low density environments. Van der Stappen, Overmars, de Berg and Vleugels [82] studied motion planning in low density environments. Berretty, Overmars and van der Stappen [11] considered moving obstacles in dynamic low density environment. Har-Peled and Quanrad [55] studied the intersection graphs of low density environments. It is worth noting that the definition of low density graphs used in [55] is based on intersection graphs, and is distinct from our definition of low density graphs. Driemel, Har-Peled and Wenk [33] introduced a definition of low density for a set of edges. A set of edges is \lambda-low-density if there are at most \lambda long edges intersecting any given disc, refer also to Section 2. Driemel and Har-Peled [33] used low density to study polygonal curves under the Fréchet distance. They provided a near-linear time algorithm to compute the Fréchet distance between a pair of low density curves, by showing that after simplifying both curves, the free space diagram between the two curves has linear complexity. They also introduced a similar but stricter condition known as c-packedness. Chen, Driemel, Guibas, Nguyen and Wenk [25] were the first to consider low density graphs. They considered the map matching problem under the Fréchet distance on low density graphs. They additionally required that the matched trajectory is c-packed. Under these assumptions, they provided a near-linear time algorithm for the map matching problem. Buchin, Buchin, Gudmundsson, Popov and Wong [20] constructed a data structure for map matching under the Fréchet distance. Their data structure has subquadratic size, and answers map matching queries in sublinear time, provided that the road network is a low density spanner. De Berg, Katz, van der Stappen and Vleugels [29] were first to consider the problem of computing the \lambda-low-density value. Given a set of n non-intersecting objects, their algorithm computes the parameter \lambda in O(n\log^{3}n+\lambda n\log^{2}n+\lambda^{2}n) time. Chen, Driemel, Guibas, Nguyen and Wenk [25] provide an efficient implementation for computing the \lambda-low-density value of a graph, and verified that \lambda\leq 28 for the city-sized road networks of San Francisco, Athens, Berlin and San Antonio. Gudmundsson, Huang and Wong [48] provided an O(n\log n+\lambda n/\varepsilon^{4}) time, (2+\varepsilon)-approximation algorithm for computing \lambda. Low density graphs have small separators. Le and Than [66] introduced the \tau-lanky property, which is similar to and implies the \lambda-low-density property. See Definitions 5 and 6. They proved that, in \mathbb{R}^{d}, any \tau-lanky graph contains a separator of size O(n^{1-1/d}). Their result implies that a low density graph embedded in the Euclidean plane has an O(\sqrt{n})-sized separator. Approximate distance oracles are highly efficient shortest path data structures. Typically, an approximate distance oracle has a constant approximation ratio, subquadratic size, and constant query time. See Definition 10. For general graphs, Thorup and Zwick [80] constructed an approximate distance oracle with an approximation ratio of 2k-1, a size of O(kn^{1+1/k}), and a query time of O(k). They also showed that for general graphs, any approximate distance oracle with an approximation ratio of 2k+1 requires \Omega(kn^{1+1/k}) size. For planar graphs, Thorup [79] and Klein [62] independently constructed a (1+\varepsilon)-approximate distance oracle with O(n\varepsilon^{-1}\log n) size and O(\varepsilon^{-1}) query time. Follow up works [60, 61, 86] improved the size and query time trade-off. The state of the art result of Le and Wulff-Nilsen [67] is a (1+\varepsilon)-approximate distance oracle with O(n\varepsilon^{-2}) size and O(\varepsilon^{-2}) query time. For geometric t-spanners, Gudmundsson, Levcopoulos, Narasimhan and Smid [49] constructed a (1+\varepsilon)-approximate distance oracle with O(nt^{4}\varepsilon^{-4}\log n) size and O(t^{6}\varepsilon^{-4}) query time, see also [72, Chapter 17.3]. For unit disc graphs, Gao and Zhang [43] constructed a (1+\varepsilon)-approximate distance oracle with O(n\varepsilon^{-4}\log n) size and O(1) query time. For bounded highway dimension graphs, Abraham, Delling, Fiat, Goldberg and Werneck [1] constructed a distance oracle with O(nh\log\Delta) size and O(h\log\Delta) query time, where h is the highway dimension and \Delta is the spread of the point set. 1.4 Other graph classes In Section 1.1, we compared low density graphs to three popular graph classes, that is, planar graphs, bounded highway dimension graphs, and spanners. In this section, we will discuss some of the other graph classes that have also been considered for modelling road networks. The treewidth of a graph measures how close to a tree the graph is. Specifically, a graph has treewidth tw if the size of the largest vertex set in its tree decomposition is tw. Graphs with treewidth tw have separators of size O(tw) [34] and have a shortest path data structures of O(tw^{3}n) size and O(tw^{3}\alpha(n)) query time [24]. Here, \alpha(n) denotes the inverse Ackermann function. Unfortunately, the treewidth of the grid graph in Figure 1 (left) is \Theta(\sqrt{n}), and experiments suggest that tw=\Omega(\sqrt[3]{n}) on real-world road networks [31, 69]. The doubling dimension of a metric measures its dimensionality by using a packing property. Formally, a metric has doubling dimension ddim if any ball of radius r can be covered with at most 2^{ddim} balls of radius r/2. Talwar [78] and Har-Peled and Mendel [54] constructed (1+\varepsilon)-approximate distance oracles for doubling metrics, with O(n\log\Delta/\varepsilon^{ddim}) size and O(n/\varepsilon^{ddim}) size, respectively. However, verifying the doubling dimension of a graph is NP-hard [46], and the comb graph in Figure 1 (left) has a doubling constant of 2^{ddim}=\Theta(\sqrt{n}) [14, Chapter 5]. The skeleton dimension of a graph is a closely related parameter to the highway dimension. The skeleton dimension is the maximum width of the so-called “skeleton” of all shortest path trees. Kosowski and Viennot [65] proved that k<h, where k is the skeleton dimension and h is the highway dimension. They also constructed a shortest path data structure of size O(nk\log\Delta) and query time of O(k\log\Delta), where \Delta is the spread of the point set. Unfortunately, the skeleton dimension of a grid graph in Figure 1 (left) is k=\Theta(\sqrt{n}). Blum and Storandt [16] computed the skeleton dimension for real-world road networks and found that k\leq 114, which for their road network sizes corresponded to \sqrt{n}\gg k\gg\log n. It is worth noting that low density graphs can have non-constant highway dimension, non-constant skeleton dimension and non-constant treewidth, see Figure 1 (left). They can also have non-constant spanning ratio and non-constant doubling dimension, see Figure 1 (right). Low density graphs can be non-planar. Therefore, any results for these other graph classes do not immediately extend to low density graphs. Previous works have considered the shortcomings of popular graph classes. Eppstein and Goodrich [35] noted that road networks are non-planar, and empirically measured the number of edges crossings to be proportional to \sqrt{n}. Blum, Funke and Storandt [15] claimed that road networks contain grid-like structures. They also noted that, for grid graphs, highway dimension, skeleton dimension, and treewidth cannot explain the excellent performance of shortest path heuristics. Eppstein and Gupta [36] stated the need for a graph class that is realistic, i.e. accurately models real-world road networks, and is useful, i.e. leads to efficient algorithms. Eppstein and Goodrich [35] proposed a realistic alternative to planar graphs. They defined a disc neighbourhood system to be a subgraph of a disc intersection graph where the set of discs have at most constant ply. They showed that disc neighbourhood systems admit O(\sqrt{n})-sized separators, similar to planar graphs and real-world road networks. They also verified that on real-world road networks, the maximum ply is indeed constant, provided that large enough discs may be ignored. A follow up result of Eppstein and Gupta [36] showed that real-world road networks do not have too many crossings, and used this property to show that road networks admit O(\sqrt{n})-sized separators. Blum, Funke and Storandt [15] proposed a realistic alternative to highway dimension, skeleton dimension, and treewidth. A graph has bounded growth if, for all vertices u and all radii r, the number of vertices within distance r of u is at most cr^{2} for some constant c. The main advantage of bounded growth graphs is that they include grid graphs. Blum, Funke and Storandt [15] showed that bounded growth graphs have a shortest path data structure of size O(n\sqrt{n}) and a query time of O(\sqrt{n}). Funke and Storandt [42] verified experimentally that the German road network is bounded growth for c=1. Finally, we summarise our comparison between graph classes. Numerous graph classes have been proposed, some of these graphs are useful while others are realistic. Many of the graph classes have shortest path data structure of O(n\operatorname{polylog}n) size and O(\operatorname{polylog}n) query time, which matches the practical performance of the most efficient heuristics [17]. Some of the graph classes admit an O(\sqrt{n}) sized separator, which is a well-known property of real-world road networks [30, 31]. Only a few of the graph classes have been verified on real-world road networks. Features such as grid graphs, comb graphs, and edge crossings can be modelled by many graph classes, but only a few can model all three features. We propose a model that captures all three features. The SELG graph class is a generalisation of both the grid and comb graphs when \ell=1, and includes edge crossings for \ell\geq\sqrt{2}. Definition 4 (SELG). A short-edged lattice graph, or SELG, is a graph where all vertices are lattice vertices in \mathbb{Z}\times\mathbb{Z}, and all edges have length at most \ell, where \ell is a constant. To the best of our knowledge, low density graphs are the only existing graph class that simultaneously admits an approximate distance oracle of O(n\operatorname{polylog}n) size and O(\operatorname{polylog}n) query time, admits an O(\sqrt{n})-sized separator, is supported by experimental evidence, models grid, comb and edge-crossing features, and subsumes the SELG graph class. See Table 1. Useful Realistic Distance oracle Separator Experiments SELG Grid Comb Cross Planar ✓ ✓ ✓ ✓ Highway dimension ✓ ✓ ✓ Spanners ✓ ✓ ✓ Treewidth ✓ ✓ ✓ ✓ Doubling dimension ✓ ✓ ✓ Skeleton dimension ✓ ✓ ✓ Disc neighbourhood ✓ ✓ ✓ ✓ ✓ ✓ Bounded growth ✓ ✓ ✓ ✓ ✓ Low density ✓ ✓ ✓ ✓ ✓ ✓ ✓ Table 1: Comparing graph classes for road networks, using Criterion 1 and Criterion 2."
https://arxiv.org/html/2411.06986v1,A Sparse Multicover Bifiltration of Linear Size,"The k-cover of a point cloud X of \mathbb{R}^{d} at radius r is the set of all those points within distance r of at least k points of X. By varying the order k and radius r we obtain a two-parameter filtration known as the multicover bifiltration. This bifiltration has received attention recently due to being parameter-free and its robustness to outliers. However, it is hard to compute: the smallest known equivalent simplicial bifiltration has O(|X|^{d+1}) simplices, where d is the dimension. In this paper we introduce a (1+\varepsilon)-approximation of the multicover that has linear size O(|X|), for a fixed d and \varepsilon. The methods also apply to the subdivision Rips bifiltration on metric spaces of bounded doubling dimension to obtain analogous results.","This paper aims to approximate the multicover bifiltration of a finite subset X of \mathbb{R}^{d}. The k-cover \textrm{Cov}_{r,k} for a scale r\geq 0 is given by all those points covered by at least k balls of radius r around the points of X: \textrm{Cov}_{r,k}\coloneqq\{p\in\mathbb{R}^{d}\nonscript\>|{}\allowbreak{}% \nonscript\>\mathopen{}\left\lVert x-p\right\rVert\leq r\text{ for at least $k% $ points of $X$}\}. For any r\leq r^{\prime} and k\geq k^{\prime}, we have that \textrm{Cov}_{r,k}\subset\textrm{Cov}_{r^{\prime},k^{\prime}} and as such the \textrm{Cov}_{r,k} assemble into a bifiltration known as the multicover bifiltration, see Fig. 1. The multicover bifiltration offers a view into the topology of the point cloud X across multiple scales, in a way that is robust to outliers, which can be made precise [6]. Figure 1. Illustration of the multicover bifiltration \textrm{Cov}_{r,k} for various values of r and k. The left column has k=1 and is thus the union of balls around the points, the middle column show those points covered by at least two balls, k=2, and the right column has k=3. The top row has a smaller scale parameter r than the bottom row. However, computing the multicover bifiltration remains a challenge. Current exact combinatorial (simplicial or polyhedral) methods [30, 24] are expensive, of size O(\lvert X\rvert^{d+1}), and previous approximate methods, although linear on \lvert X\rvert, have an exponential dependency on k [10]. For general metric spaces, instead of subsets of \mathbb{R}^{d}, an analogue of the multicover is the subdivision Rips bifiltration [50]. It has also received attention recently, both in its theoretical [6] and computational aspects [37, 38, 36]. Still, its computation faces similar challenges as the multicover. Previous approximation schemes are at least polynomial in size [39, 6, 37, 38, 36], see the related work below. In this paper, we define a simplicial bifiltration, the sparse subdivision bifiltration (Definition 4.2), that (1+\varepsilon)-approximates the multicover and that is of linear size O(\lvert X\rvert), independently of k and for fixed \varepsilon and d. It can be computed in time O(\lvert X\rvert\log\Delta), where \Delta is the spread of X, the ratio between the longest and shortest distances between points in X. In addition, the methods extend to obtain analogous results for the subdivision Rips bifiltration for metric spaces of bounded doubling dimension (Section 8). The sparse subdivision bifiltration is (homotopy) equivalent to the sparse multicover bifiltration SCov (Definition 3.5), a bifiltration of subsets of \mathbb{R}^{d}. It is via SCov that the sparse subdivision bifiltration (1+\varepsilon)-approximates the multicover, by which we mean that \textrm{Cov}_{r,k}\subset\textrm{SCov}_{(1+3\varepsilon)r,k}\text{ and }% \textrm{SCov}_{r,k}\subset\textrm{Cov}_{(1+\varepsilon)r,k}, as in Theorem 3.7. In other words, the sparse subdivision bifiltration and the multicover are (multiplicatively) (1+\varepsilon)-homotopy-interleaved [7]. Instead of balls of radius r, the sparse multicover bifiltration is based on sparse balls (Definition 3.1), understood as balls with a lifetime of three phases: on the first phase they grow normally (at scale r they have radius r), and then they are slowed down (they keep growing but at scale r they have radius less than r), before eventually disappearing. We guarantee that when a sparse ball disappears it is approximately covered by another present sparse ball, and we keep track of which present sparse balls covers which disappeared sparse ball via a covering map (Definition 3.3). A sparse ball “counts as” as many sparse balls it approximately covers according to the covering map—the sparse k-cover \textrm{SCov}_{r,k} consists of those points covered by sparse balls that at least “count as” k balls. 1.1. Motivation and related work For k=1, the 1-cover is the union of balls of radius r around points of X. The union of balls is commonly used in reconstructing submanifolds from samples [43, 3] and is a cornerstone of persistent homology methods in Topological Data Analysis, driving the Čech complex and the alpha complex [27, 28], among others. The union of balls assembles into a 1-parameter filtration: by increasing the scale parameter r we obtain inclusions \textrm{Cov}_{r,1}\hookrightarrow\textrm{Cov}_{r^{\prime},1}, for r\leq r^{\prime}. Taking the homology of each \textrm{Cov}_{r,1}, we obtain a persistence module, an algebraic descriptor of the topology of the filtration across the different scales. Such a persistence module is stable to perturbations of the points [22], but is not robust to outliers (already appreciated in Fig. 1) and insensitive to differences in density in the point cloud. There are multiple methods that address the lack of robustness to outliers and changes in density within the 1-parameter framework. These include density estimation [44, 18, 8], distance to a measure [16, 34, 17, 1, 9], and subsampling [5]. However, they all depend on choosing a parameter. The problem is that it is not clear how to choose such a parameter for all cases, and such a choice might focus on a specific range of scales or densities. We refer to [6, Section 1.7] for a complete overview of the methods and its limitations. It is then natural to consider constructions over two parameters, scale and density. Examples include the density bifiltration [12], degree bifiltrations [39], and the multicover, closely related to both the distance to a measure [16] and k-nearest neighbors [50]. The advantage of the multicover is that it does not depend on any further choices (like choosing a density estimation function) and that it is robust to outliers, as a consequence of its strong stability properties [6]. However, one currently problematic aspect of the multicover is its computation. Sheehy [50] introduced a celebrated simplicial model of the multicover called the subdivision Čech bifiltration, based on the barycentric subdivision of the Čech filtration. It has exponentially many vertices on the number of input points, making its computation challenging. A crucial ingredient in the theory is the multicover nerve theorem [50, 13, 6] that establishes the topological equivalence (see Section 4.1 for a precise definition) of the subdivision Čech and multicover bifiltrations. Such a multicover nerve theorem has its analogue for the sparse multicover, the sparse multicover nerve theorem (Theorem 4.3). Dually to an hyperplane arrangement in \mathbb{R}^{d+1}, Edelsbrunner and Osang [30, 29] define the rhomboid tiling and use it to compute the multicover bifiltration. Corbet, Kerber, Lesnick and Osang [24, 23] build on the rhomboid tiling to define two bifiltrations topologically equivalent to the multicover, that are of size O(\lvert X\rvert^{d+1}) and that can be efficiently computed. Most recently, Buchet, Dornelas and Kerber [10] introduce an elegant (1+\varepsilon)-approximation of the multicover whose m-skeleton has size that is linear on \lvert X\rvert but that incurs an exponential dependency on the maximum order k we want to compute. A crucial difference between our work and the Buchet-Dornelas-Kerber (BDK) sparsification is that BDK work at the level of intersection of balls, rather than at the level of balls, as we do with the sparse balls. This starting point is what allows us to ultimately remove the exponential dependency on k. BDK work by freezing the lenses: at a certain scale they stop growing. It is not clear how to compute exactly the first scale at which freezing lenses first intersect—a problem they sidestep by discretizing the scale parameter (at no complexity cost). In contrast, by letting sparse balls grow slowly at a certain rate, rather than freezing them completely, we can compute their first intersection time exactly, as we explain in Section 6.3. 1.1.1. Sparsification As far as sparsification is concerned, our methods are part of a line that can be traced to the seminal work of Sheehy [51] to obtain a linear size approximation of the Vietoris-Rips filtration. Subsequently, Sheehy’s methods have served as inspiration for sparsifying other constructions, like the Čech filtration [14], the Delaunay triangulation [52] or, as already mentioned, the multicover itself [10]. A fundamental ingredient in many of these constructions is the greedy permutation [47, 33, 26], or variants of it, that we also use in the form of persistent nets (Section 2). 1.1.2. General metric spaces In general metric spaces, an analogue of the subdivision Čech bifiltration (and multicover) of Sheehy is the subdivision Rips bifiltration. Its computation faces similar challenges, and, in fact, no subexponential size combinatorial model of it exists [37]. Thus, there has been recent interest in approximations of subdivision Rips. Indeed, the degree Rips bifiltration [39], which can be shown to be a \sqrt{3}-approximation [6] whose k-skeleton has size O(\lvert X\rvert^{k+2}), and has been implemented [39, 46, 48]. Furthermore, it was recently shown that subdivision Rips admits \sqrt{2}-approximations whose k-skeleta has the same size as degree Rips [36, 37]. Most recently, Lesnick and McCabe [37, 38], in the more particular case of metric spaces of bounded doubling dimension (which include Euclidean spaces), give a (1+\varepsilon)-approximation of subdivision Rips whose k-skeleton has O(\lvert X\rvert^{k+2}) simplices, for fixed \varepsilon and dimension. In Section 8, we extend the methods of the sparse multicover to this case, and, as in the sparse multicover, obtain a (1+\varepsilon)-approximation of linear size O(\lvert X\rvert). 1.1.3. Miniball In Section 6.3, we compute the first scale at which a set of sparse balls have non-empty intersection, for Euclidean space. If instead of sparse balls we would be using usual balls, such a scale would be the radius of the minimum enclosing ball of the centers of the balls—the miniball problem, first stated by Sylvester in 1857 [54]. Our solution to the analogous problem for sparse balls has its origin in Welz algorithm [56], which takes randomized linear time on the number of centers. The miniball problem is of LP-type as introduced later by Matoušek, Sharir and Welz [42], and Welz algorithm for the miniball can be generalized to the Matoušek-Sharir-Welz (MSW) algorithm for LP-type problems. Fischer and Gärtner [31] solve the problem of computing the minimum enclosing ball of balls—generalizing the miniball—via the MSW-algorithm in a way that is practical and efficient, as in the implementation in the CGAL library [32]. Here, we frame the problem of computing the first scale of intersection of sparse balls as an LP-type algorithm and show how to solve it as an extension of Fischer and Gärtner’s methods. The miniball and similar problems have also been stated and solved through the geometric optimization point of view, see [25, 15] and references therein. 1.2. Acknowledgements The author would like to thank his advisor Michael Kerber for helpful comments on an early draft. This research was funded in whole, or in part, by the Austrian Science Fund (FWF) 10.55776/P33765."
https://arxiv.org/html/2411.06584v1,On inside-out Dissections of Polygons and Polyhedra,"In this work we study inside-out dissections of polygons and polyhedra. We first show that an arbitrary polygon can be inside-out dissected with 2n+1 pieces, thereby improving the best previous upper bound of 4(n-2) pieces. Additionally, we establish that a regular polygon can be inside-out dissected with at most 6 pieces. Lastly, we prove that any polyhedron that can be decomposed into finitely many regular tetrahedra and octahedra can be inside-out dissected.","In computational geometry, the term dissection refers to the concept of decomposing (2D or 3D) shapes into smaller pieces of the same dimension, which can then be reassembled via continuous motions to form new shapes. Over the years the notion of dissections has been examined in various settings by formulating restrictions on the continuous motions that are allowed to rearrange the smaller pieces of a given decomposition into a new shape. For various examples of dissections we refer to [1, 6, 7, 8, 9, 10] to name only a few. In this work we study so-called inside-out dissections of polygons and polyhedra which were introduced by Joseph O’Rourke in [12]. Here, we recall the definition of the inside-out dissection of a polygon (polyhedron). Definition 1.1. Let P be a polygon (polyhedron). An inside-out dissection of P is a decomposition of P into finitely many polygons (polyhedra) P_{1},\ldots,P_{k} such that 1. the polygons P_{1},\ldots,P_{k} can be rearranged by only applying rotations and translations to form a polygon (polyhedron) P^{\prime} that is congruent to P, 2. the boundary of the polygon (polyhedron) P^{\prime} is composed of internal cuts of P. In the case that such a decomposition of P exists, we denote it by (P_{1},\ldots,P_{k}) and say that P can be inside-out dissected. We refer to the polygons (polyhedra) P_{1},\ldots,P_{k} as pieces of P. Furthermore, we define \mathcal{I}(n) as the smallest natural number such that every n-gon P can be inside-out dissected with k\leq\mathcal{I}(n) pieces. We note that, in contrast to Dudeney and hinged mirror dissections, the pieces of an inside-out dissection of a polygon (polyhedron) are not required to be connected by hinges. Figure 1 shows a possible inside-out dissection of an obtuse angled triangle. Figure 1: An inside-out-dissection of an obtuse triangle using four pieces by Aaron Meyerowitz.[11] The illustrated inside-out dissection of the given triangle consists of 4 pieces. It can be shown that every triangle can be inside-out dissected with 4 pieces, see [11]. It is well-known that a polygon P with n edges can be triangulated with exactly n-2 triangles (proof by induction). Therefore by the same result the inequality \mathcal{I}(n)\leq 4(n-2) holds. This instance yields the question whether this existing bound on the number \mathcal{I}(n) can be improved. Question 1. Let P be an arbitrary n-gon. Can we show that P can be inside-out dissected with k<4(n-2) pieces? Here, we show that the inequality \mathcal{I}(n)\leq 2n+1 is true for all n-gons. We also prove that if P is a regular polygon, then it can be inside-out dissected with at most six pieces. Moreover, we also investigate inside-out dissections in the three-dimensional case. In particular, we examine the following problem. Question 2. Can every (or any) tetrahedron be inside-out-dissected? Both Question 1 and Question 2, were posed by Joseph O’Rourke at the 36th Canadian Conference on Computational Geometry, see [4]. In this paper, we answer these questions and illustrate our current results."
https://arxiv.org/html/2411.06114v1,Hyperplane Distance Depth,"Depth measures quantify central tendency in the analysis of statistical and geometric data. Selecting a depth measure that is simple and efficiently computable is often important, e.g., when calculating depth for multiple query points or when applied to large sets of data. In this work, we introduce Hyperplane Distance Depth (HDD), which measures the centrality of a query point q relative to a given set P of n points in \mathbb{R}^{d}, defined as the sum of the distances from q to all \binom{n}{d} hyperplanes determined by points in P. We present algorithms for calculating the HDD of an arbitrary query point q relative to P in O(d\log n) time after preprocessing P, and for finding a median point of P in O(dn^{d^{2}}\log n) time. We study various properties of hyperplane distance depth, and show that it is convex, symmetric, and vanishing at infinity.","Depth measures describe central tendency in statistical and geometric data. A median of a set of univariate data is a point that partitions the set into two halves of equal cardinality, with smaller values in one part, and larger values in the other. Various definitions of medians exist in higher dimensions (multivariate data), seeking to generalize the one-dimensional notion of median (e.g., [6]). For geometric data and sets of geometric objects, applications of median-finding include calculating a centroid, determining a balance point in physical objects, and defining cluster centers in facility location problems [7]. A median is frequently used in statistics to describe the central tendency of a data set. It is particularly useful when dealing with skewed distributions or datasets that contain outliers. By using a median, analysts can obtain a representative value that is less affected by extreme values and outliers [10]. In 1975, Tukey introduced the concept of data depth for evaluating centrality in bivariate data sets [12]. The depth of a particular query point q in relation to a given set P gauges the extent to which q is situated within the overall distribution of P; i.e., when q’s depth is large, q tends to be near the center of P. Since the introduction of Tukey depth (also called half-space depth), many more depth functions have been proposed. Data depth functions should ideally satisfy specific properties, such as convexity, stability (small perturbations in the data do not result in large changes in depth values), robustness (depth is not heavily influenced by outliers or extreme values in the data), affine invariance (the depth function remains consistent under linear transformations of the data, such as translation, scaling, and rotation), maximality at the center (points closer to the geometric center of the data set have higher depth values), and vanishing at infinity (depth values approach zero as a query point moves away from the data set) [14]."
https://arxiv.org/html/2411.07030v1,Hyperplanes Avoiding Problem and Integer Points Counting in Polyhedra,"In our work, we consider the problem of computing a vector x\in\operatorname{\mathbb{Z}}^{n} of minimum \norm{\cdot}_{p}-norm such that a^{\top}x\not=a_{0}, for any vector (a,a_{0}) from a given finite set \operatorname{\mathcal{A}}\subseteq\operatorname{\mathbb{Z}}^{n}. In other words, we search for a vector of minimum norm that avoids a given finite set of hyperplanes, which is natural to call as the Hyperplanes Avoiding Problem. This problem naturally appears as a subproblem in Barvinok-type algorithms for counting integer points in polyhedra. More precisely, it appears when one needs to evaluate certain rational generating functions in an avoidable critical point.We show that:With respect to \norm{\cdot}_{1}, the problem admits a feasible solution x with \norm{x}_{1}\leq(m+n)/2, where m=\abs{\operatorname{\mathcal{A}}}, and show that such solution can be constructed by a deterministic polynomial-time algorithm with O(n\cdot m) operations. Moreover, this inequality is the best possible. This is a significant improvement over the previous randomized algorithm, which computes x with a guaranty \norm{x}_{1}\leq n\cdot m. The original approach of A. Barvinok can guarantee only \norm{x}_{1}=O\bigl{(}(n\cdot m)^{n}\bigr{)};The problem is \operatorname{N\!P}-hard with respect to any norm \norm{\cdot}_{p}, for p\in\bigl{(}\operatorname{\mathbb{R}}_{\geq 1}\cup\{\infty\}\bigr{)}.As an application, we show that the problem to count integer points in a polytope \operatorname{\mathcal{P}}=\{x\in\operatorname{\mathbb{R}}^{n}\colon Ax\leq b\}, for given A\in\operatorname{\mathbb{Z}}^{m\times n} and b\in\operatorname{\mathbb{Q}}^{m}, can be solved by an algorithm with O\bigl{(}\nu^{2}\cdot n^{3}\cdot\Delta^{3}\bigr{)} operations, where \nu is the maximum size of a normal fan triangulation of \operatorname{\mathcal{P}}, and \Delta is the maximum value of rank-order subdeterminants of A. It refines the previous state-of-the-art O\bigl{(}\nu^{2}\cdot n^{4}\cdot\Delta^{3}\bigr{)}-time algorithm.","Let \operatorname{\mathcal{A}}\subseteq\operatorname{\mathbb{Z}}^{n+1} be a set of pairs (a,a_{0}) with a\in\operatorname{\mathbb{Z}}^{n}\setminus\{\operatorname{\mathbf{0}}\} and a_{0}\in\operatorname{\mathbb{Z}}, and denote m:=\abs{\operatorname{\mathcal{A}}}<\infty. Consider the system \begin{cases}a^{\top}\cdot x\not=a_{0},\quad\forall(a,a_{0})\in\operatorname{% \mathcal{A}}\\ x\in\operatorname{\mathbb{Z}}^{n}.\end{cases} (HyperplanesAvoiding) The system (HyperplanesAvoiding) has infinitely many solutions, and it is interesting to find solutions having small norm (we are mainly interested in the \norm{\cdot}_{1}-norm). The latter motivates the following problem, which is natural to call the Hyperplanes Avoiding Problem: \displaystyle\norm{x}_{p}\to\min \displaystyle\begin{cases}a^{\top}\cdot x\not=a_{0},\quad\forall(a,a_{0})\in% \operatorname{\mathcal{A}}\\ x\in\operatorname{\mathbb{Z}}^{n}.\end{cases} (p-HyperplanesAvoiding) In other words, we are just trying to find an integer vector of the smallest norm that does not lie in any of the m given hyperplanes. It is also interesting to consider the Homogeneous forms of the system (HyperplanesAvoiding) and problem (p-HyperplanesAvoiding), when a_{0}=0 for any (a,a_{0})\in\operatorname{\mathcal{A}}. In this case, we are trying to find an integer vector of the smallest norm that does not lie in any of the m given (n-1)-dimensional subspaces. 1.1 Motivation: The integer Points Counting in Polyhedra The problem (p-HyperplanesAvoiding) naturally appears as a subproblem in algorithms for integer points counting in polyhedra. Let us give a brief sketch of how it appears. Consider a rational polytope \operatorname{\mathcal{P}} defined by a system of linear inequalities. The seminal work of A. Barvinok [5] (see also [3, 4]) proposes an algorithm to count the number of points inside \operatorname{\mathcal{P}}\cap\operatorname{\mathbb{Z}}^{n}, which is polynomial for a fixed dimension. His approach is based on a representation of \operatorname{\mathcal{P}}\cap\operatorname{\mathbb{Z}}^{n} via some rational generating function. More precisely, the Barvinok’s algorithm computes a set of indices \operatorname{\mathcal{I}}, and for each i\in\operatorname{\mathcal{I}}, it computes a number \epsilon^{(i)}\in\operatorname{\mathbb{Z}} and vectors v^{(i)},u_{1}^{(i)},\dots,u_{n}^{(i)}\in\operatorname{\mathbb{Z}}^{n} such that \sum\limits_{x\in\operatorname{\mathcal{P}}\cap\operatorname{\mathbb{Z}}^{n}}z% ^{x}=f_{\operatorname{\mathcal{P}}}(z):=\sum\limits_{i\in\operatorname{% \mathcal{I}}}\epsilon^{(i)}\cdot\frac{z^{v^{(i)}}}{\bigl{(}1-z^{u_{1}^{(i)}}% \bigr{)}\cdot\ldots\cdot\bigl{(}1-z^{u_{n}^{(i)}}\bigr{)}}. (1) Here, the notation z^{x} means z^{x}=z_{1}^{x_{1}}\cdot\ldots\cdot z_{n}^{x_{n}}. The right-hand-side of (1), i.e. the function f_{\operatorname{\mathcal{P}}}(z), is called the short rational generating function of \operatorname{\mathcal{P}}\cap\operatorname{\mathbb{Z}}^{n}. Since the left part of (1) is a finite sum, the point z=\operatorname{\mathbf{1}} is an avoidable critical point of f_{\operatorname{\mathcal{P}}}(z). Therefore, \abs{\operatorname{\mathcal{P}}\cap\operatorname{\mathbb{Z}}^{n}}=\lim\limits_% {z\to\operatorname{\mathbf{1}}}f_{\operatorname{\mathcal{P}}}(z). (2) One possible approach to find this limit, is to compute a vector c\in\operatorname{\mathbb{Z}}^{n} such that c^{\top}u^{(i)}_{j}\not=0, for any i\in\operatorname{\mathcal{I}} and j\in\left\{1,\dots,n\right\}. Note that c is a solution of the system (HyperplanesAvoiding) with \operatorname{\mathcal{A}}=\bigl{\{}u^{(i)}_{j}\bigr{\}}, and m=\abs{\operatorname{\mathcal{A}}}=(n+1)\cdot\abs{\operatorname{\mathcal{I}}}. Using the substitution z_{i}\to e^{\tau\cdot c_{i}}, the function f_{\operatorname{\mathcal{P}}}(z) transforms to the function \hat{f}_{\operatorname{\mathcal{P}}}(\tau), depending on the single complex variable \tau, defined by \hat{f}_{\operatorname{\mathcal{P}}}(\tau)=\sum\limits_{i\in\operatorname{% \mathcal{I}}}\epsilon^{(i)}\cdot\frac{e^{\langle c,v^{(i)}\rangle\cdot\tau}}{% \bigl{(}1-e^{\langle c,u_{1}^{(i)}\rangle\cdot\tau}\bigr{)}\cdot\ldots\cdot% \bigl{(}1-e^{\langle c,u_{n}^{(i)}\rangle\cdot\tau}\bigr{)}}. (3) Now, since \hat{f}_{\operatorname{\mathcal{P}}} is analytical, the limit (2) just equals to the [\tau^{0}]-term of the Tailor’s series for \hat{f}_{\operatorname{\mathcal{P}}}(\tau): \abs{\operatorname{\mathcal{P}}\cap\operatorname{\mathbb{Z}}^{n}}=\lim\limits_% {\tau\to 0}\hat{f}_{\operatorname{\mathcal{P}}}(\tau)=[\tau^{0}]\hat{f}_{% \operatorname{\mathcal{P}}}. We note that it is preferable to calculate the vector c satisfying c^{\top}u^{(i)}_{j}\not=0 with the smallest possible norm, because it will reduce the size of the numbers \langle c,v^{(i)}\rangle and \langle c,u_{j}^{(i)}\rangle, which in turn will speed up practical computations. However, the norm of c does not affect the computational complexity in terms of the number of operations. It only reduces the variable sizes. Assuming that the polyhedron \operatorname{\mathcal{P}} is defined by a system Ax\leq b, for given A\in\operatorname{\mathbb{Z}}^{m\times n} and b\in\operatorname{\mathbb{Q}}^{m}, the computational complexity of the Barvinok’s algorithm in terms of operations number can be bounded by \nu\cdot\bigl{(}O(\log\Delta)\bigr{)}^{n\ln n}, (4) where \nu is the maximum size of a normal fan triangulation of \operatorname{\mathcal{P}}, and \Delta is the maximum value of the rank-order subdeterminants of A. Thus, finding a good solution to (HyperplanesAvoiding) has only effect on the variable sizes of the Barvinok’s algorithm. However, there is an alternative algorithmic approach to integer point counting, which allows obtaining complexity bounds of the type \operatorname{poly}(\nu,n,\Delta). It was developed in a series of works [10, 11, 8, 9, 7].111For the latest perspective see [9], for the parametric case see [7], the paper [8] is a correction of [11]. In this alternative approach, the norm of the solution to (HyperplanesAvoiding) is a multiplicative factor in the bound on its computational complexity. More precisely, the following result was obtained in [9]. Proposition 1 (D. Gribanov, I. Shumilov, D. Malyshev & N. Zolotykh [9]) Assume that, for any collection \operatorname{\mathcal{A}} of vectors of size m, there exists a solution x of the system (HyperplanesAvoiding) with \norm{x}_{1}\leq L(m,n). Assume additionally that such x can be calculated for free. Then the number \abs{\operatorname{\mathcal{P}}\cap\operatorname{\mathbb{Z}}^{n}} can be calculated with O\bigl{(}\nu\cdot L(\nu\cdot n,n)\cdot n^{2}\cdot\Delta^{3}\bigr{)}\quad\text{% operations}. It was shown in [9] that L(m,n)\leq n\cdot m, and such x can be constructed by a randomized polynomial-time algorithm with O(n\cdot m) operations. It means that the counting complexity can be estimated by O\bigl{(}\nu^{2}\cdot n^{4}\cdot\Delta^{3}\bigr{)}. In the current paper, we show that L(m,n)\leq(m+n)/2, and such x can be constructed by a deterministic O(n\cdot m)-time algorithm. The latter yields the counting complexity O\bigl{(}\nu^{2}\cdot n^{3}\cdot\Delta^{3}\bigr{)}, which is the main application of our results. Additionally, we hope that our result can significantly accelerate the evaluation part of the Barvinok-type algorithms. To this end, we propose some experimental results showing that the new algorithm constructs solutions of significantly lower norm than random sampling in a cross-polytope, see Section Experimental Evaluation. Finally, we note that this paper is not considering the dual-type algorithms for counting integer points in polyhedra. A great survey of this approach could be found in the book [12] of J. Lasserre. 1.2 Complexity Model Assumptions All the algorithms that are considered in our work correspond to the Word-RAM computational model. In other words, we assume that additions, subtractions, multiplications, and divisions with rational numbers of the specified size, which is called the word size, can be done in O(1)-time. In our work, we chose the word size to be equal to some fixed polynomial on the input size of the corresponding computational problem. More precisely, considering the problem (p-HyperplanesAvoiding), we assume that the input size is bounded by n\cdot m\cdot(1+\lceil\log_{2}\alpha\rceil), where \alpha is the maximum absolute value of coordinates of a, for a\in\operatorname{\mathcal{A}}. 1.3 Main Results and Related Work Let us summarize our results below. 1. With respect to \norm{\cdot}_{1}, the problem (p-HyperplanesAvoiding) admits a feasible solution x with \norm{x}_{1}\leq(m+n)/2, where m=\abs{\operatorname{\mathcal{A}}}, and we show that such solution can be constructed by a deterministic polynomial-time algorithm with O(n\cdot m) operations, see Theorem 2.3 of Section Approximate Solution via Combinatorial Nullstellensatz. The inequality is the best possible, see the discussion afterward. This is a significant improvement over the previous O(n\cdot m)-time randomized algorithm of [9], which computes x with a guaranty \norm{x}_{1}\leq n\cdot m. In contrast, the original approach of A. Barvinok searches x in the form x=(1,t,t^{2},\dots,t^{n-1}). Since, for each a\in\operatorname{\mathcal{A}}, a^{\top}x=\sum_{i=1}^{n}a_{i}\cdot t^{i-1} is a polynomial of degree at most n-1, there exists a suitable t with t\leq n\cdot m. However, this reasoning can guaranty only \norm{x}_{1}=O\bigl{(}(n\cdot m)^{n}\bigr{)}. 2. For any p\in\bigl{(}\operatorname{\mathbb{R}}_{\geq 1}\cup\{\infty\}\bigr{)}, the problem (p-HyperplanesAvoiding) is \operatorname{N\!P}-hard with respect to any norm \norm{\cdot}_{p}, even in its homogeneous form. See Theorem 3.1 of Section Computational Complexity of the Exact Solution; 3. We show that the problem to calculate the value \abs{\operatorname{\mathcal{P}}\cap\operatorname{\mathbb{Z}}^{n}} for a polyhedron \operatorname{\mathcal{P}} defined by the system Ax\leq b, for agiven A\in\operatorname{\mathbb{Z}}^{m\times n} and b\in\operatorname{\mathbb{Q}}^{m}, can be solved with O\bigl{(}\nu^{2}\cdot n^{3}\cdot\Delta^{3}\bigr{)} operations, where \nu is the maximum size of a normal fan triangulation of \operatorname{\mathcal{P}}, and \Delta is the maximum value of rank-order subdeterminants of A. It refines the O\bigl{(}\nu^{2}\cdot n^{4}\cdot\Delta^{3}\bigr{)}-time algorithm of [9]. See Subsection Motivation: The integer Points Counting in Polyhedra, more specifically, see the discussion alongside Proposition 1; It is easy to see that the guaranty \norm{x}_{1}\leq(m+n)/2 on an existing solution x of the system (HyperplanesAvoiding) is the best possible. Proposition 2 There exists a family of systems (HyperplanesAvoiding) such that \norm{x}_{1}\geq(m+n)/2 for any solution x. Proof Fix some positive integer k. The desired system consists of the constraints x_{i}\not=j, for any i\in\left\{1,\dots,n\right\} and j\in\left\{-k,\dots,k\right\}. So, the total number of constraints is m=(2k+1)\cdot n. It is easy to see that \abs{x_{i}}\geq k+1, for any i\in\left\{1,\dots,n\right\} and any solution x of the system. Therefore, \norm{x}_{1}\geq(k+1)\cdot n=(m+n)/2. However, for the homogeneous form of the system (HyperplanesAvoiding), the asymptotics of the solution quality with respect to the parameter m can be slightly improved. This observation is based on the following result of I. Bárány, G. Harcos, J. Pach, & G. Tardos [2]. Let \operatorname{\mathbb{B}}_{1} be the unit ball with respect to \norm{x}_{1} and g(r) be a minimal number of subspaces needed to cover all points of the set r\cdot\operatorname{\mathbb{B}}_{1}\cap\operatorname{\mathbb{Z}}^{n}. Theorem 1.1 (I. Bárány, G. Harcos, J. Pach, & G. Tardos [2]) There exist absolute constants C_{1} and C_{2} such that C_{1}\cdot\frac{1}{n^{2}}\cdot r^{\frac{n}{n-1}}\leq g(r)\leq C_{2}\cdot 2^{n}% \cdot r^{\frac{n}{n-1}}. Note that the original work [2] contains a more general result concerning arbitrary convex bodies in \operatorname{\mathbb{R}}^{n}, albeit with a worse dependence on n. The Theorem 1.1 is a straightforward adaptation of the original proof to the case of \operatorname{\mathbb{B}}_{1}. As a corollary, it follows that the system (HyperplanesAvoiding) always has a solution with an asymptotics that is slightly better in m, but worse in n. Corollary 1 The system (HyperplanesAvoiding) has a solution x, such that \norm{x}_{1}=O\bigl{(}n^{2}\cdot m^{\frac{n-1}{n}}\bigr{)}. At the same time, the theorem implies that solutions of significantly smaller norm do not exist in general. In particular, it implies that our constructive bound \norm{x}_{1}\leq(m+n)/2 is almost optimal with respect to m even in the homogeneous case. Corollary 2 There exists a system (HyperplanesAvoiding) such that, for any solution x, \norm{x}_{1}=\Omega\bigl{(}\frac{1}{2^{n}}\cdot m^{\frac{n-1}{n}}\bigr{)}."
