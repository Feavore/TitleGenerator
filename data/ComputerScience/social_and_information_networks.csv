URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10294v1,Static network structure cannot stabilize cooperation among Large Language Model agents,"Large language models (LLMs) are increasingly used to model human social behavior, with recent research exploring their ability to simulate social dynamics. Here, we test whether LLMs mirror human behavior in social dilemmas, where individual and collective interests conflict. Humans generally cooperate more than expected in laboratory settings, showing less cooperation in well-mixed populations but more in fixed networks. In contrast, LLMs tend to exhibit greater cooperation in well-mixed settings. This raises a key question: Are LLMs about to emulate human behavior in cooperative dilemmas on networks? In this study, we examine networked interactions where agents repeatedly engage in the Prisoner’s Dilemma within both well-mixed and structured network configurations, aiming to identify parallels in cooperative behavior between LLMs and humans. Our findings indicate critical distinctions: while humans tend to cooperate more within structured networks, LLMs display increased cooperation mainly in well-mixed environments, with limited adjustment to networked contexts. Notably, LLM cooperation also varies across model types, illustrating the complexities of replicating human-like social adaptability in artificial agents. These results highlight a crucial gap: LLMs struggle to emulate the nuanced, adaptive social strategies humans deploy in fixed networks. Unlike human participants, LLMs do not alter their cooperative behavior in response to network structures or evolving social contexts, missing the reciprocity norms that humans adaptively employ. This limitation points to a fundamental need in future LLM design—to integrate a deeper comprehension of social norms, enabling more authentic modeling of human-like cooperation and adaptability in networked environments.","Over the last few years, the advances in artificial intelligence have ignited hopes of new methods for the behavioral and social sciences[1, 2]. In particular, chatbots powered by so-called large language models (LLM)[3] are believed to be able to emulate humans in conversations well enough to eventually replace them in experiments[4]. Even though experiments with human participants are ultimately needed to advance the behavioral and social sciences, there are expectations AI agents could aid in experimental design and provide experimental agents with programmable behavior (so-called “digital twins”) [1, 2]. LLMs are trained on extensive datasets of human-generated text and semantic knowledge from various societies [1, 4]. These models operate as conditional probability distributions, where altering the context or narrative can steer them toward more desirable outcomes by influencing the likelihood of specific responses while reducing others [4]. Consequently, LLMs are highly skilled at following instructions and embodying assigned personalities [5]. When given personalities, LLMs can display traits that resemble human nature, almost as if they possess their mind [6, 7]. This training process can also improve LLMs’ understanding and reasoning regarding cooperation, defection, and balancing individual and collective interests [8]. Recent behavioral experiments have shown that LLM agents can effectively substitute human participants in certain contexts, particularly within Western, Educated, Industrialized, Rich, and Democratic (WEIRD) societies [4]. However, this substitution does not extend to other cultural contexts [9]. Beyond cultural differences, within the same cultures, individuals’ social and strategic preferences depend on the network they are part of [10, 11, 12, 13]. Individuals can infer underlying network structures and adapt their social behavior, even without a complete overview of the network. Through interactions with neighbors and social learning, individuals discern the network they are part of, and this structure significantly influences their social behavior. A fascinating question arises: can large language models discern these network structures and adjust their behavior similarly to humans? This inquiry becomes especially intriguing when we consider their potential to help solve societal challenges, like promoting cooperation in social dilemmas. For this vision to come to life, it is crucial that AI agents consistently demonstrate behavior akin to that of humans. We have some understanding of LLMs’ capabilities in repeated prisoners’ dilemma games in well-mixed populations [5, 14, 8, 15, 16, 17], but we lack insight into how they perform in network settings. There, individuals consider not only strategies but also their interactions with specific network members [18, 19]. Previous research has shown that individuals can establish cooperation when they have the opportunity to adjust their social ties based on their experiences with neighbors [19]. Interestingly, it has been found that in a prisoner’s dilemma game, individuals can achieve stable cooperation within specific static network structures when the benefit-to-cost ratio exceeds the number of connections. However, the same parameter settings do not foster cooperation in a well-mixed population. The findings suggest that when individuals are aware of their neighborhood and the consequences of their cooperative actions, they can adapt their strategies based on the success of their neighbors, leading to stable cooperation even in networks with a few defectors [13]. In this study, we investigate whether LLMs can reliably adjust their social behavior in response to the network structure they operate within, as humans do. We study the capabilities of LLM agents, as of 2024, in the context of networked social dilemmas [18, 20, 5]. These stylized situations are designed to explore how humans prioritize between short-sighted egoistic and long-term prosocial choices. Such dilemmas have a wide range of social science applications, from behavioral economics, which explores questions like how to best incentivize people to follow policies, to anthropology and evolution, which search for the unique behavioral mechanisms behind human cooperation among known lifeforms."
https://arxiv.org/html/2411.10290v1,"The ParClusterers Benchmark Suite (PCBS): 
A Fine-Grained Analysis of Scalable Graph Clustering 
[Experiment, Analysis & Benchmark]","We introduce the ParClusterers Benchmark Suite (PCBS)—a collection of highly scalable parallel graph clustering algorithms and benchmarking tools that streamline comparing different graph clustering algorithms and implementations. The benchmark includes clustering algorithms that target a wide range of modern clustering use cases, including community detection, classification, and dense subgraph mining. The benchmark toolkit makes it easy to run and evaluate multiple instances of different clustering algorithms, which can be useful for fine-tuning the performance of clustering on a given task, and for comparing different clustering algorithms based on different metrics of interest, including clustering quality and running time.Using PCBS, we evaluate a broad collection of real-world graph clustering datasets. Somewhat surprisingly, we find that the best quality results are obtained by algorithms that not included in many popular graph clustering toolkits. The PCBS provides a standardized way to evaluate and judge the quality-performance tradeoffs of the active research area of scalable graph clustering algorithms. We believe it will help enable fair, accurate, and nuanced evaluation of graph clustering algorithms in the future.","Clustering is a critical tool in almost any scientific field that involves classifying and organizing data today. Examples of fields leveraging clustering range from computational biology and phylogenetics to complex network analysis, machine learning, and astrophysics (Manning et al., 2008; Shalita et al., 2016; Camerra et al., 2014; Patwary et al., 2015). Clustering has proven particularly useful in fields transformed by AI and machine learning because of its utility in understanding and leveraging high-dimensional vector representations (embeddings) of data (Monath et al., 2021; Bateni et al., 2017; Douze et al., 2024; Dhulipala et al., 2021, 2022). In this paper, we are interested in carefully characterizing the behavior (e.g., measuring quality, running time, and scalability) of parallel clustering algorithms for shared-memory multi-core machines that are scalable in the size of the dataset and the number of threads. Our specific focus is on graph clustering, which is a versatile and scalable clustering approach that can be used with different input types. On one hand, graph clustering is a natural approach whenever the input is a graph (e.g., friendships, interactions, etc.). On the other hand, graph clustering can also be applied in the other popular scenario, when the input is a collection of points in a metric space (e.g., embeddings). In this case, one can obtain a graph by computing a weighted similarity graph, where continuous or complete phenomena can be cast into sparse similarity graphs, e.g., by keeping only edges between nearby points or only the most significant entries of a similarity matrix. Despite substantial prior works that study the quality (e.g., precision and recall) and scalability of individual graph clustering methods (Dhulipala et al., 2023, 2021, 2022; Shi et al., 2021; Staudt et al., 2016; Tsourakakis et al., 2017; Tseng et al., 2021a), no prior works have systematically compared a large collection of different graph clustering methods (and their corresponding implementations) to understand how different methods compare against each other under different metrics. For example, celebrated and widely-utilized graph clustering algorithms, such as modularity clustering are well understood to be highly effective in community detection tasks on unweighted natural graphs, but little is known about their performance for clustering on vector embedding clustering tasks. This paper addresses this gap by performing a systematic comparison of a large and diverse set of graph clustering methods. Our evaluation includes methods tailored to both weighted and unweighted graphs and incorporates a diverse set of natural graphs and similarity graphs derived from point sets. We focus on undirected graphs, as converting directed graphs to undirected graphs is a common practice in many graph tasks, such as community detection (see, e.g., (Fortunato, 2010)). We stratify our evaluation based on four unsupervised clustering tasks that are commonly found in the literature and in practice—(1) community detection, (2) vector embedding clustering, (3) dense subgraph partitioning, and (4) high resolution clustering. Due to insisting on scalability, we focus our evaluation on the most scalable parallel graph clustering methods currently available in the literature. To make our evaluation easily reusable and extensible by future researchers, we designed a graph clustering benchmark called the ParClusterers Benchmark Suite (PCBS). PCBS enables users to accurately measure the scalability and accuracy of different shared-memory parallel graph clustering algorithms. In addition to providing a simple and easy to use benchmarking platform, we have also incorporated eleven parallel graph clustering methods into PCBS. The algorithms include algorithms from our recent prior work, as well as several new implementations. In addition to classic graph clustering methods such as modularity-based clustering (Shi et al., 2021), structural clustering (Tseng et al., 2021b), and label propagation methods (Raghavan et al., 2007), we include recently developed hierarchical agglomerative graph clustering methods (Dhulipala et al., 2022) and connectivity-based methods such as k-core and low-diameter decomposition (Dhulipala et al., 2018). Finally, unlike much of the existing work on graph clustering, which typically focuses on optimizing a specific graph clustering metric (e.g., modularity or conductance) that a clustering method is usually designed to optimize, PCBS supports evaluating any clustering algorithm using a very broad set of metrics, which helps us understand what different clustering algorithms are able to optimize for on real-world datasets, and help inform users of the best clustering algorithm for a given metric. Besides PCBS’s clustering implementations, PCBS also supports running many clustering implementations in other graph clustering frameworks and systems such as NetworKit (Staudt et al., 2016), Neo4j (neo, [n.d.]), and TigerGraph (tig, [n.d.]). PCBS can also be easily extended to include new datasets, algorithms, and parameter search methods. The datasets we study include both widely-used graph datasets from the SNAP repository, as well as several new graph clustering datasets that we have generated from spatial and embedding datasets using a simple nearest-neighbor-based graph building process, and which we will open-source as part of this work. We also contribute a new graph dataset for clustering, which represent similarities between 1.2M short texts. As far as we know, this is the first large-scale graph clustering dataset that provides a large number of ground-truth clusters. Our datasets cover a wide range of scales and clustering tasks, including community detection, vector embedding clustering, and dense subgraph partitioning. Key Contributions. The key contributions of our work include: • A comprehensive library that implements eleven state-of-the-art scalable graph clustering algorithms, providing a unified codebase for researchers and practitioners. • A benchmarking toolkit that facilitates the systematic evaluation of graph clustering algorithms across diverse datasets, parameter settings, and experimental configurations, enabling rigorous and comprehensive comparative analyses. • A new large graph clustering dataset containing many ground-truth clusters. • The first extensive evaluation of parallel graph clustering algorithms, encompassing their runtime performance, clustering quality, and the trade-off between these two critical dimensions. We also compare our library against other existing libraries and graph databases. Key Results. Some of our key takeaways and findings of our study of graph clustering algorithms include: • Our clustering implementations in PCBS are very fast compared to other clustering implementation in state-of-the-art graph libraries and databases. While graph databases, such as Neo4j (neo, [n.d.]) and TigerGraph (tig, [n.d.]), provide a richer functionality, on different graph clustering implementations, they are slower. For example, on the LiveJournal graph from SNAP (Leskovec and Sosič, 2016), PCBS is on average 32.5x faster than Neo4j and 303x faster than TigerGraph. Compared with state-of-the-art parallel graph library NetworKit (Staudt et al., 2016), PCBS is on average 4.54x faster. We compute the average using the geometric mean. • Correlation clustering (Shi et al., 2021) obtains the highest quality on three out of four tasks. ParHAC (Dhulipala et al., 2022) obtains the best quality on the fourth task. We consider this finding surprising, given that these two methods are not included in many popular graph clustering frameworks. The best performing method commonly found in existing graph clustering packages is modularity clustering. However, we observe that on a vast majority of datasets, correlation clustering obtains strictly better quality. • Parallel affinity clustering obtains high quality on the vector embedding clustering task and is consistently faster than correlation clustering and ParHAC on large graphs. Our code and the full version of our paper can be found at https://github.com/ParAlg/ParClusterers."
https://arxiv.org/html/2411.09675v1,Citation Sentiment Reflects Multiscale Sociocultural Norms,"Modern science is formally structured around scholarly publication, where scientific knowledge is canonized through citation. Precisely how citations are given and accrued can provide information about the value of discovery, the history of scientific ideas, the structure of fields, and the space or scope of inquiry. Yet parsing this information has been challenging because citations are not simply present or absent; rather, they differ in purpose, function, and sentiment. In this paper, we investigate how critical and favorable sentiments are distributed across citations, and demonstrate that citation sentiment tracks sociocultural norms across scales of collaboration, discipline, and country. At the smallest scale of individuals, we find that researchers cite scholars they have collaborated with more favorably (and less critically) than scholars they have not collaborated with. Outside collaborative relationships, higher h-index scholars cite lower h-index scholars more critically. At the mesoscale of disciplines, we find that wetlab disciplines tend to be less critical than drylab disciplines, and disciplines that engage in more synthesis through publishing more review articles tend to be less critical. At the largest scale of countries, we find that greater individualism (and lesser acceptance of the unequal distribution of power) is associated with more critical sentiment. Collectively, our results demonstrate how sociocultural factors can explain variations in sentiment in scientific communication. As such, our study contributes to the broader understanding of how human factors influence the practice of science, and underscore the importance of considering the larger sociocultural contexts in which science progresses.","In the modern era, science has often been assumed to be an objective process [1]. By this descriptor, scholars have variously meant faithfulness to facts, absence of normative commitments, and freedom from personal biases [2]. Yet with an increasing understanding of human psychology has come an acknowledgement that even the most scrupulous of researchers cannot achieve various types of objectivity; for example, (1) the mechanical objectivity of suppressing the universal human propensity to judge and aestheticize, or (2) the aperspectival objectivity of eliminating individual idiosyncracies [3]. The growing appreciation of human subjectivity has motivated a turn from the personal to the community, where objectivity is considered a quality that characterizes a collection or population of studies [4], and hence is a feature of scientific communities and their practices [5]. In communities, objectivity can further be thought of as occurring in degrees, such that any method of inquiry is only objective to the degree that it permits transformative criticism through shared standards, equality of intellectual authority, avenues for criticism, and uptake of criticism [6]. Figure 1: Sentiment analysis in citation and author networks. A two-layer network comprising a network of citations and a network of collaborators. In the citation network, nodes represent scientific articles, and edges represent citations. Sentiment analysis categorizes each edge as favorable (blue), neutral (grey), or critical (red). In the collaboration network, nodes represent scholars, and scholars who co-author articles are connected by edges. Yet, the above discourse begs the question: Need objectivity always be fundamentally at odds with subjectivity? In a complementary line of work, it has been noted that strong objectivity can be maintained without the requirement of value-neutrality [7] or the suppression of all aspects of the self [8], including one’s epistemological standpoint [9] or ethical commitments [10]. The potential coexistence of certain objectivities and certain subjectivities has proven useful in evaluating the scientific self, which exists among values, standpoints, cognitive biases, and human perceptions [11]. Indeed, scientists specifically, and scholars more broadly, exist in multiply-dependent cultural contexts—from the local collaborative group or research institution to the scientific discipline or the geographically-defined culture—that each evince distinct norms, values, currencies, and artifacts [12]. For example, separate disciplines recommend distinct forms of scientific rationality, methods to investigate the world, and factors that others in the community would appreciate [13]; separate countries can have social and funding norms that impact collaborative tendencies [14]; and even different fields of thought work under distinct notions of theoretical aesthetics, the value of parsimony, and preferred characteristics of explanatory structures [15]. Hence, scientific selves are not fundamentally autonomous, self-creating, culture-free individuals; rather, they are co-produced by interactions within the networks, communities, and social movements of scientific culture [11], with its set of standards, mores, practices, languages, and dialects [12]. A key locus of this cultural production is the accumulation of cultural archives through the publication of scientific papers [12]. Although a large fraction of such papers will not be referenced (or quickly become obsolete [16]), the remainder will be canonized into scientific knowledge proper through the act of citation [17]. Factors that determine which papers get cited include epistemic values (e.g., empirical support, simplicity, generality, precision, rigor, testability, and explanatory power [6, 18]) and non-epistemic values (e.g., common interest, shared gender, same ethnicity, social capital, and institutional privilege [19, 20, 21, 22, 23, 24]), as well as seemingly trivial factors such as punctuated titles [25]. Collectively, these factors provide an explanation for who cites who, how often, and when. In other words, citation dynamics are socio-cognitive processes [26]. Citation networks have played a key role in the sociology of science and its critical evaluation [27, 24, 28, 29]. Yet, due to various challenges of data accessibility and limitations in algorithmic efficacy, a major gap in knowledge exists: precisely how, or in what way, do we cite? Citations can be differently valued [30]; can be epistemic or procedural [18]; can be positive or negative, indicate either support or contrast, attribute creativity or indicate status as a “classic” [31, 32]; can either represent or misrepresent the work cited [33]; and can be chosen for social reasons (such as name-dropping [33] or soliciting favor from a scholar who might have influence in the review process), for mercantile reasons (such as bartering for a citation in return), and for alignment reasons (such as transmitting a specific self-image, e.g., as part of the mainstream, the avant garde, or a given school) [34, 35]. Because not all citations are created equal, citation analysis could be meaningfully expanded to account for citation type [28, 34]. Although a few early studies attempted such an expansion, they were limited in scope by the lack of computational infrastructure to assess citation type automatically from large databases. Here we overcome this fundamental limitation in order to examine citation sentiment and its dependence upon the sociocultural factors in which science exists and progresses (Fig. 1). To ensure that our assessment is computationally tractable while remaining sensitive to local cultural norms, we focus on a single discipline—neuroscience—and build a large database of papers from 56,000 last authors published in 185 journals (impact factor \geq 3 in 2022). As a candidate field, neuroscience has particular affordances for our purpose: its relative youth allows for the emergence and growth of not-yet-crystallized subdisciplines, a fundamental interdisciplinarity as it draws on earlier-defined disciplines, and a marked intersection of opinions, disciplinary methods, and approaches to study [36, 37]. All of these factors can support a diversity, heterogeneity, and complexity of the space of ideas, and the network of citations among papers espousing those ideas. We chose to focus on citation sentiment (neutral, favorable, or critical), as a kind or type, for functional and pragmatic purposes. Functionally, sentiment can track coalescence (or fragmentation) of people and ideas; pragmatically, sentiment can be measured quantitatively in extensive databases using large language models. Using this approach, we show how citation sentiment varies across people groups, scientific disciplines, and whole countries, providing a paper-trail lens into the socio-cognitive processes of science. Figure 2: Citation sentiment depends on distance and years since/to first collaboration. (A) Citation sentiment relative to the null model as a function of the collaboration distance between the last authors of the citing and cited works. Shaded regions denote one standard deviation from the mean at a given collaboration distance. (B) Citation sentiment as a function of the number of years since the first collaboration between the two last authors. A negative year implies that at the time of a citation, the two scholars had yet to collaborate but would do so in the future. Shaded regions denote one standard deviation."
https://arxiv.org/html/2411.09389v1,Less is More: Unseen Domain Fake News Detection via Causal Propagation Substructures,"The spread of fake news on social media poses significant threats to individuals and society. Text-based and graph-based models have been employed for fake news detection by analyzing news content and propagation networks, showing promising results in specific scenarios. However, these data-driven models heavily rely on pre-existing in-distribution data for training, limiting their performance when confronted with fake news from emerging or previously unseen domains, known as out-of-distribution (OOD) data. Tackling OOD fake news is a challenging yet critical task. In this paper, we introduce the Causal Subgraph-oriented Domain Adaptive Fake News Detection (CSDA) model, designed to enhance zero-shot fake news detection by extracting causal substructures from propagation graphs using in-distribution data and generalizing this approach to OOD data. The model employs a graph neural network-based mask generation process to identify dominant nodes and edges within the propagation graph, using these substructures for fake news prediction. Additionally, CSDA’s performance is further improved through contrastive learning in few-shot scenarios, where a limited amount of OOD data is available for training. Extensive experiments on public social media datasets demonstrate that CSDA effectively handles OOD fake news detection, achieving a 7%\sim16% accuracy improvement over other state-of-the-art models.","The popularity of social media has enabled rapid news dissemination, for both true and fake news. Given the potential impact of fake news, robust fake news detection methods are needed to debunk such news in a timely manner. In real-world scenarios, out-of-distribution news from unseen domains emerges over time. This brings substantial challenges to fake news detection models. Graph-based fake news detection methods using graph neural networks (GNN) have garnered much attention recently for modelling news propagation patterns (Gong et al. 2023a). Despite their success, existing GNN-based methods are generally built on the assumption that both training and testing data are independently sampled from an identical data distribution (i.i.d.), which often does not hold true nor reflect the real challenges of fake news detection (Li et al. 2022). Emerging and hitherto unseen fake news and their associated propagation graphs can and do appear. From an empirical perspective, these methods focus on minimising the average training error and incorporating correlations within the training data (which is considered to be in-distribution) to improve fake news detection accuracy (Liu et al. 2021). However, real-world graph-based fake news data is often mixed with biased domain-specific information in the training data. The detection model may thus learn these domain-specific biases resulting in misclassification of cross-domain news items (Li et al. 2022). To detect fake news across different domains (e.g., sports and politics), some early studies (Ma, Gao, and Wong 2018; Bian et al. 2020) focus on capturing content-independent propagation patterns. However, it has been shown (Min et al. 2022) that not only the news contents but also the propagation patterns can vary across different news domains. More recent approaches (Li et al. 2023; Lin et al. 2022) collect and manually label a small dataset from emerging news domains. They utilise domain adaptation methods to adapt the trained models to the emerging domains in a few-shot manner. However, these approaches require labelled data from emerging domains which is not always available and could be expensive and time-consuming. To address the limitations above, we focus on extracting causal subgraphs from news propagation graphs to eliminate potential domain biases. The patterns of such subgraphs are learnt for fake news detection in emerging domains. News from an emerging domain is considered as the out-of-distribution (OOD) data, and we generalise our model trained on in-distribution data to OOD data by capturing causal subgraphs in an unsupervised manner. From a causal analysis perspective, each propagation graph is composed of causal subgraph and biased subgraph which are initially entangled. Our intuition is that not all nodes in the propagation graph of a given news item are helpful for fake news detection. Instead, only some causal subgraphs of the propagation graph carry critical clues that can be used to identify fake news, as illustrated in Fig. 1 with an example. If we can identify and capture such causal subgraphs, we can improve fake news detection accuracy and subsequently improve the way we generalise the model to OOD data. Figure 1: Illustration of the causal subgraphs and the Structure Causal Models (SCMs). In the SCMs, the grey and white variables represent unobserved and observed variables. Further explanations on SCMs are given in Preliminaries. Based on this intuition, a cross-domain model – the Causal Subgraph Oriented Domain Adaptive Fake News Detection (CSDA) model, is proposed. This model extracts subgraphs from propagation graphs and performs detection based on the subgraphs. In CSDA, a binary mask is learned for each node and each edge of the propagation graph of a news item to classify them into causal or biased elements. For the subgraph formed by each type of element, a graph encoder and a multilayer perceptron (MLP) classifier together encode the subgraphs and classify the news item according to the subgraph embeddings. In the training process, we utilise a data augmentation strategy by concatenating the causal subgraph embedding and the permuted biased subgraph embedding. We then train CSDA with both embeddings to enhance the effectiveness of causal subgraph learning. In the testing process, only the causal branch of the CSDA model is utilised to predict news veracity. Following recent works (Li et al. 2023; Lin et al. 2022), we also consider a scenario where limited OOD data becomes available through manual labelling. In this scenario, CSDA’s performance on OOD data is further enhanced with a supervised contrastive learning-based approach and achieves state-of-the-art (SOTA) classification accuracy. In summary, our contributions include: • We propose a zero-shot cross-domain fake news detection model named CSDA based on extracting causal subgraphs related to news propagation patterns. • We further explore a few-shot scenario in cross-domain fake news detection where a small number of OOD examples are available, and we utilise contrastive learning to enhance CSDA’s cross-domain performance. • Extensive experiments are conducted on four real datasets. The results confirm the effectiveness of CSDA in cross-domain fake news detection, outperforming SOTA models by 7.69\sim 16.00\% in terms of accuracy."
https://arxiv.org/html/2411.09100v1,General Linear Threshold Models with Application to Influence Maximization,"A number of models have been developed for information spread through networks, often for solving the Influence Maximization (IM) problem. IM is the task of choosing a fixed number of nodes to “seed” with information in order to maximize the spread of this information through the network, with applications in areas such as marketing and public health. Most methods for this problem rely heavily on the assumption of the known strength of connections between network members (edge weights), which is often unrealistic. In this paper, we develop a likelihood-based approach to estimate edge weights from the fully and partially observed information diffusion paths. We also introduce a broad class of information diffusion models, the general linear threshold (GLT) model, which generalizes the well-known linear threshold (LT) model by allowing arbitrary distributions of node activation thresholds. We then show our weight estimator is consistent under the GLT and some mild assumptions. For the special case of the standard LT model, we also present a much faster expectation-maximization approach for weight estimation. Finally, we prove that for the GLT models, the IM problem can be solved by a natural greedy algorithm with standard optimality guarantees if all node threshold distributions have concave cumulative distribution functions. Extensive experiments on synthetic and real-world networks demonstrate that the flexibility in the choice of threshold distribution combined with the estimation of edge weights significantly improves the quality of IM solutions, spread prediction, and the estimates of the node activation probabilities.","The emergence of large-scale online social networks has led to a new surge of interest in information diffusion models. These networks supply incredibly rich data, which can include connections between users, user covariates, e.g., demographics, and the paths of information propagation between users, e.g., retweets or reposts. Information diffusion paths, also known as propagation traces, are especially valuable in modeling information spread since they provide direct data on the influence users have on their network neighbors. “Information” in this context can be interpreted broadly and refer to anything that can spread from node to node, be it a news item or a virus (and sometimes these are connected – Dinh and Parulian (2020) showed that the spread of Covid-19 through people’s social networks can be well predicted by their interactions around Covid-related posts on Twitter). For example, Liu and Wu (2018) used propagation traces for fake news detection while Saito et al. (2008) proposed to estimate information diffusion probabilities from the same data. Goyal et al. (2011) proposed a model assigning credits to users based on how well they propagate information and subsequently using these weights to solve the influence maximization (IM) problem, that is, identifying the best “seed nodes” for spreading information. While the IM problem and spread prediction are the main applications of the present paper, the method we propose for estimating edge weights from propagation traces is general and can be applied to any other downstream problem in network analysis after the weights are estimated, such as testing for network differences (Tantardini et al., 2019) or community detection in weighted graphs (Lancichinetti and Fortunato, 2009). Since the IM problem was formulated by Richardson and Domingos (2002) and formalized by Kempe et al. (2003), many efficient approaches for identifying the best seed sets have been proposed. All of them require a probability model for how information (“influence”) spreads over the network, known as the diffusion model, usually with edge- or vertex-specific parameters governing information spread. Perhaps the most popular is the independent cascade (IC) model (Goldenberg et al., 2001), which assumes that each edge is associated with a transmission probability and all transmission events are independent. Another popular choice is the linear threshold (LT) model (Granovetter, 1978)), which assumes that each edge has a deterministic weight and each node uses a uniformly distributed random threshold to decide whether to “accept” incoming information; this will be stated formally in Section 2. There is substantial literature on solving the IM problem given a specific diffusion model (see Banerjee et al. (2020) for a survey), and some on more efficient but less general approaches, for example, mixed integer programming for the IC model (Farnad et al., 2020). However, in most applications, the underlying diffusion parameters are unknown and can significantly change the IM problem solution under misspecification. Work by Goyal et al. (2011) highlighted this issue, showing that the IM solutions improve significantly when the edge weights are estimated, for example using the EM approach of Saito et al. (2008) for the IC model. One contribution we make in this paper is to develop an EM approach analogous to Saito et al. (2008) for edge weight estimation under the LT model (Section 4.6). However, the LT and IC model classes are not especially rich, and the natural next step would be to estimate the parameters of more sophisticated and flexible network diffusion models. While there are well-known flexible generalizations of these two models, such as the triggering and general threshold (GT) models studied in Kempe et al. (2003), their additional flexibility is achieved at the cost of exponentially many additional parameters (see Proposition 3.1), making parameter estimation impossible with realistic amounts of data. To address this, we propose the general linear threshold (GLT) model, which has only O(|E|+|V|) parameters but is much more flexible than the LT model; importantly, it allows for heterogeneity in how readily users accept new information. Our generalization is a special case of the GT model but not of the triggering model. We propose a likelihood-based approach for estimating its parameters and show the estimator is consistent under mild regularity conditions. In Section A.6 of the Appendix, we extend this technique by showing the IC model parameters are identifiable and can be consistently estimated under similar conditions. While there have been several papers (He et al., 2016; Narasimhan et al., 2015) establishing Probably Approximately Correct (PAC) learnability guarantees for nodes’ activation probabilities under the IC, LT, and several similar models, surprisingly, there has been very little work establishing theoretical guarantees for the diffusion model parameters. The only paper we are aware of that addresses these questions is the work by Rodriguez et al. (2014) where authors derive identifiability conditions and establish consistency for the parameters of several continuous-time diffusion models. In such a way, to the best of our knowledge, this work is the first one to prove consistency for the estimates of discrete diffusion model parameters based on the propagation traces. Finally, we show that under some additional conditions on the GLT model, the IM problem under the GLT assumption can be solved by the natural and widely used greedy strategy described in Section 2, with standard optimality guarantees. The rest of this manuscript is organized as follows: in Section 2, we briefly introduce the background on the IM problem and the relevant diffusion models and fix notation. In Section 3 we propose the Generalized Linear Threshold (GLT) model, describe its relationship to other diffusion models, and derive the conditions under which the IM problem under the GLT model can be optimally solved by the greedy strategy. In Section 4, we establish identifiability conditions, derive a likelihood approach to weight estimation under the GLT model, and establish consistency. We also present an EM-based improvement of our estimation algorithm for the standard LT model. Finally, Section 5 presents experiments on synthetic and a real-world network showing that using the proposed weight estimation procedure outperforms standard heuristics and estimating the threshold distributions under the GLT model instead of using the standard LT model can substantially improve the IM solutions."
https://arxiv.org/html/2411.08608v1,Comparative study of random walks with one-step memory on complex networks,"We investigate searching efficiency of different kinds of random walk on complex networks which rely on local information and one-step memory. For the studied navigation strategies we obtained theoretical and numerical values for the graph mean first passage times as an indicator for the searching efficiency. The experiments with generated and real networks show that biasing based on inverse degree, persistence and local two-hop paths can lead to smaller searching times. Moreover, these biasing approaches can be combined to achieve a more robust random search strategy. Our findings can be applied in the modeling and solution of various real-world problems.","Random walk is a ubiquitous concept that describes wandering in certain space in which the location where the walker will be in the next moment is chosen randomly. In complex networks it can applied for modeling diverse phenomena like searching through information networks [1], diffusion of information, ideas and viruses in social networks, stock market fluctuations, and solving various problems such as page ranking in the web [19], semi-supervised graph labeling [29, 10], link prediction in graphs [2], and graph representation learning [13, 17]. Since the onset of interest in complex networks, various models of random walk on top of them have been proposed. The standard uniform random walk is based on randomly choosing the next node in the walk with equal probability from all neighbors of the node where the walker currently is. By applying master equation approach [18] or Markov chain theory [12] one can obtain theoretical results for a key quantity in the random walk – the mean first passage time (MFPT), that represents the expected number of steps needed for the walker to reach randomly chosen target for the first time. Using the same formalism, various modifications of the uniform random walk have been applied that exploit the local properties of the network, aimed at improving the search time. One approach is based on the degrees of the neighbors [9], particularly when biasing proportionally to the inverse degree of the next node [6, 4]. Some authors have considered local neighborhood exploration by random walks using marking as well as biasing based on neighbors degrees[5]. In another approach memory is applied where the probability to jump to some next node depends on the current, but also on the previously visited one [3, 4, 7]. Other problems that have recently received attention are random walk on networks with resetting [21], multiple simultaneous random walks [20], and random walk on hypergraphs [8]. The theoretical expressions for calculating MFPT in random walks with one-step memory presented in [4] provide a useful testbed that can be employed for comparing various biasing strategies in relatively small networks. Nevertheless, the findings can be then applied to networks with arbitrary sizes. In this work, we aim to study and combine different approaches with local information in order to see whether further improvement is possible. We study five types of random walks with one-step memory: simple forward going, inverse degree biased, two-hop paths based, persistent, and we introduce a combination of persistent and inverse degree biased. For comparison in our study we also include two standard random walks without memory: uniform and inverse degree biased. Our findings can be applied for potential improvements in the study of a wide range of problems mentioned at the beginning of this introduction. In Section II we describe the theoretical expressions for calculating MFPTs in random walks with one-step memory on complex networks represented as graphs. Several graph searching strategies using such random walks are described in Section III. In Section IV we present the results obtained with the theoretical expressions and numerical simulations on several synthetic and real complex networks, while in Section V we give some general conclusions."
https://arxiv.org/html/2411.08638v1,"Gaussian Mixture Models Based 
Augmentation Enhances GNN Generalization","Graph Neural Networks (GNNs) have shown great promise in tasks like node and graph classification, but they often struggle to generalize, particularly to unseen or out-of-distribution (OOD) data. These challenges are exacerbated when training data is limited in size or diversity. To address these issues, we introduce a theoretical framework using Rademacher complexity to compute a regret bound on the generalization error and then characterize the effect of data augmentation. This framework informs the design of GMM-GDA, an efficient graph data augmentation (GDA) algorithm leveraging the capability of Gaussian Mixture Models (GMMs) to approximate any distribution. Our approach not only outperforms existing augmentation techniques in terms of generalization but also offers improved time complexity, making it highly suitable for real-world applications. Our code is publicly available at: https://github.com/abbahaddou/GMM-GDA.","Graphs are a fundamental and ubiquitous structure for modeling complex relationships and interactions. In biology, graphs are employed to represent complex networks of protein interactions and in drug discovery by modeling molecular relationships. Similarly, in social networks, graphs capture relationships and community interactions, offering insights into social structures and interactions (Zeng et al., 2022; Gaudelet et al., 2021; Newman et al., 2002). To address the unique challenges posed by graph-structured data, GNNs have been developed as a specialized class of neural networks designed to operate directly on graphs. Unlike traditional neural networks that are optimized for grid-like data, such as images or sequences, GNNs are engineered to process and learn from the relational information embedded in graph structures. GNNs have demonstrated state-of-the-art performance across a range of graph representation learning tasks such as node and graph classification, proving their effectiveness in various real-world applications (Vignac et al., 2022; Corso et al., 2022; Duval et al., 2023; Castro-Correa et al., 2024; Chi et al., 2022). Despite their impressive capabilities, GNNs face significant challenges related to generalization, particularly when handling unseen or out-of-distribution (OOD) data (Guo et al., 2024; Li et al., 2022). OOD graphs are those that differ significantly from the training data in terms of graph structure, node features, or edge types, making it difficult for GNNs to adapt and perform well on such data. This challenge is also faced when GNNs are trained on small datasets, where the limited data diversity hampers the model’s ability to generalize effectively. To address these challenges, the community has explored various strategies to improve the robustness and generalization ability of GNNs (Abbahaddou et al., 2024; Yang et al., 2022). One promising approach is data augmentation, which involves artificially expanding the training dataset by introducing variations of the original graph data. Data augmentation has shown its benefits across different types of data structures such as images (Krizhevsky et al., 2012) and time series (Aboussalah et al., 2023). For graph data structures, generating augmented versions of the original graphs, such as by adding or removing nodes and edges or perturbing node features (Rong et al., 2019; You et al., 2020), allows for the creation of a more varied training set. Inspired by the success of the Mixup technique in computer vision (Rebuffi et al., 2021; Dabouei et al., 2021; Hong et al., 2021), additional methods such as \mathcal{G}-Mixup and GEOMIX have been developed to adapt the Mixup technique for graph data (Ling et al., 2023; Han et al., 2022). These techniques combine different graphs to create new, synthetic training examples, further enriching the dataset and enhancing the GNN’s ability to generalize to new unseen graph structures. In this work, we introduce a novel graph augmentation technique based on Gaussian Mixture Models (GMMs), which operates at the level of the final hidden representations. Specifically, guided by our theoretical results, we apply the Expectation-Maximization (EM) algorithm to train a GMM on the graph representations. We then use this GMM to generate new augmented graph representations through sampling, enhancing the diversity of the training data. Contributions. The contributions of our work are as follows: • Theoretical framework for generalization in GNNs: We introduce a theoretical framework that rigorously analyzes how graph data augmentation impacts the generalization capabilities of GNNs. This framework offers new insights into the underlying mechanisms that drive performance improvements through augmentation. • Efficient graph data augmentation via GMMs: We propose GMM-GDA, a fast and efficient graph data augmentation technique, leveraging GMMs. This approach enhances the diversity of training data while maintaining computational simplicity, making it scalable for large graph datasets. • Comprehensive theoretical analysis using influence functions: We perform an in-depth theoretical analysis of our augmentation strategy through the lens of influence functions, providing a principled understanding of the approach’s impact on generalization performance."
https://arxiv.org/html/2411.08052v1,Mobility-based Traffic Forecasting in a Multimodal Transport System,"We study the analysis of all the movements of the population on the basis of their mobility from one node to another, to observe, measure, and predict the impact of traffic according to this mobility. The frequency of congestion on roads directly or indirectly impacts our economic or social welfare. Our work focuses on exploring some machine learning methods to predict (with a certain probability) traffic in a multimodal transportation network from population mobility data. We analyze the observation of the influence of people’s movements on the transportation network and make a likely prediction of congestion on the network based on this observation (historical basis). Keywords: Optimization, Modeling, Transportation, Mobility, Road Safety, Machine Learning.","The government of Senegal had expressed the importance to transport sector and urban mobility. The vision in transport sector explains the importance of the investments planned in the emerging senegalese plan. In particular, the priority action plan, in relation to urban mobility: the etablishing of the Regional Express Train (TER), the Bus Rapid Transit (BRT), and the reinforcement of the Dakar Dem Dikk fleet (400 city buses out of 475 vehicles) (CETUD, 2024). In addition, we have the continued renewal of the urban fleet of fast buses and ndiaga ndiaye (more than 60%), together with the professionalization of informal or artisanal actors. Urban transport malfunctions (congestion, pollution, and accidents) are recorded in some African countries at about 4% of GDP. The movement of an individual in a network is simply a succession of nodes and arcs. For a given individual, let us assume that we only know his path to a certain node. Many urban areas in the world, especially in developing countries, are facing a rapid increase in population density, that generates a transport demand that cannot be supported by transport infrastructures. Between 1976 and 2022, the population of the Dakar region increased approximately 67 times while at the same time the transport network and urban planning were not sufficiently adapted to this development (ANSD, 2024). This leads to congestion problems and a reduction in urban accessibility defined as the ability to access certain given resources or activities, within a given time frame (Gueye et al., 2015). Is it possible, by analyzing all movements, to predict (with a certain probability) the next node where this individual will go? The scientific fields associated with this question are related to pattern recognition and machine learning. These two fields make extensive use of statistical and operations research techniques. The underlying application is traffic forecasting. This new research direction is based on the idea that there are recurrent trajectory patterns hidden in CDR files, the knowledge of which allows to reduce considerably the amount of files needed. And these patterns can be discovered by Machine Learning on the data corpus (and others). Being able to make such predictions gives a definite advantage on the operational management. This work is positioned on resolution approaches. There are currently few methods, especially exact, to solve this type of problem by considering the system in its globality. Knowing where individuals start from (origins), where they go (destinations), what they do (activities), which paths they follow, and by which means of transport (mode) is essential information whose real-time knowledge can inform urban management policies. We aim at, is a system that exploits in real-time this kind of files in. However, such an objective poses many computational and mathematical challenges. - Challenge 1: Exploration of data mining methods. First of all, the data analysis proposed by Gueye in (Gueye et al., 2015), Baldé in (Baldé et al., 2021; Baldé and Ndiaye, 2016), Kone in (Koné et al., 2019; 2020) consisted of a series of algorithms based on simple and improvable heuristic rules. However, the extraction of knowledge from data is precisely the domain of data mining and multi-agent systems, using statistical techniques. We have not exploited all the possibilities. One of this research directions will be to explore this path. - Challenge 2: Traffic prediction by machine learning. Secondly, having real-time information based on the exploitation of data files must take into account the momentary absence of these files. It is indeed difficult to envisage, because of the construction delays and their sizes, to have a system regularly fed, on very short time steps by data. And capable, on equally short time steps, of extracting the relevant information. We have also experimentally observed, in the challenge, that not all trajectories are relevant to study. Indeed, some of them contain too many information gaps to be exploitable because the individuals are only detected if they receive or send a call/sms or by Google if they have turned on the Location History setting, Google Mobility Data (Aktay et al., 2020). And the trajectories can be grouped into clusters (of trajectories) sharing approximately the same characteristics: almost the same origins, same destinations, same activities, same modes. Only one trajectory among each cluster is then needed to report on all of them. For example, let us assimilate the transportation network to a graph whose arcs represent the roads and nodes their intersections. The movement of an individual in this network is just a succession of nodes and arcs taken. For a given individual given, let us admit that we only know his path to a certain node. Is it possible, by analyzing all the trips, to predict (with a certain probability) the next node where this individual will go? The article is organized as follows. In Section 2, we present the mathematical model, error measurement and some advantages for the Prophet forecasting. In Section 3, we numerically determine the Dakar traffic forecast by analyzing the mobility time series and improving the model prediction. Finally, in Section 4, we present conclusions and perspectives."
https://arxiv.org/html/2411.07907v1,When Randomness Beats Redundancy: Insights into the Diffusion of Complex Contagions,"How does social network structure amplify or stifle behavior diffusion? Existing theory suggests that when social reinforcement makes the adoption of behavior more likely, it should spread more—both farther and faster—on clustered networks with redundant ties. Conversely, if adoption does not benefit from social reinforcement, then it should spread more on random networks without such redundancies. We develop a novel model of behavior diffusion with tunable probabilistic adoption and social reinforcement parameters to systematically evaluate the conditions under which clustered networks better spread a behavior compared to random networks. Using both simulations and analytical techniques we find precise boundaries in the parameter space where either network type outperforms the other or performs equally. We find that in most cases, random networks spread a behavior equally as far or farther compared to clustered networks despite strong social reinforcement. While there are regions in which clustered networks better diffuse contagions with social reinforcement, this only holds when the diffusion process approaches that of a deterministic threshold model and does not hold for all socially reinforced behaviors more generally. At best, clustered networks only outperform random networks by at least a five percent margin in 18% of the parameter space, and when social reinforcement is large relative to the baseline probability of adoption.","Introduction How does social network structure amplify or stifle behavior diffusion? Existing theory suggests this relationship between structure and diffusion depends on the micro-foundations of how a behavior is adopted from peer to peer in the process of social influence [centola2007complex, guilbeault2018complex]. For some behaviors, the chance of adoption increases as individuals are exposed to multiple influential neighbors who serve as socially reinforcing sources. For other behaviors, the chance of adoption remains constant regardless of the number socially reinforcing neighbors they are exposed to. When the socially reinforced adoption rate is greater than the non-socially reinforced adoption rate, such that a behavior “benefits” from social reinforcement, the behavior is called a complex contagion. Existing theory suggests it spreads more—both faster and farther—on clustered networks [centola2007complex]. On the other hand, if the likelihood of adopting a behavior does not “benefit” from social reinforcement we speak of a simple contagion. In this case, existing theory suggests the behavior will spread more on random networks [centola2007complex, granovetter1973strength, watts1998collective, hebert2010propagation]. These stylized results that form the basis of current understanding describe individuals as deterministic [centola2007complex], subject to changing their behavior based on fixed rules such as adopting a behavior when a threshold of neighbors have adopted the behavior.222The paper that develops the original theory [centola2007complex] does incorporate some probabilistic features including stochastic thresholds as robustness checks but treats individual-level adoption as strictly deterministic. While the idea that complex contagions spread faster and further on clustered networks holds with such additions, it is unclear whether this pattern persists in cases of probabilistic adoption, which is our focus here. However, humans are not deterministic rule followers; they are probabilistic decision-makers. This is supported by many studies of peer influence where non-socially reinforced and socially reinforced adoption are probabilistic [bakshy2012role, bakshy2012social, centola2010spread, romero2011differences, leskovec2007dynamics, lee2022complex, fink2016investigating]. It remains unclear whether this dichotomy between random networks better diffusing simple contagions and clustered networks better diffusing complex contagions generalizes to the probabilistic nature of real human behavior. That is, do these results hold when we move from the notion that adoption depends on social reinforcement (i.e., does not happen without it) to a less restrictive version where adoption is simply more likely when it is socially reinforced?333An additional benefit of probabilistic models besides better aligning with the probabilistic nature of human behavior is that they can account for noise or mistakes in behavioral data (e.g., “trembling hand”). As a result, such probabilistic models can be fit to empirical data that contains observations that have a probability of zero in a deterministic model. In the deterministic case there is a clear advantage to random networks better spreading simple contagions and clustered networks better spreading complex contagions. Random networks are characterized by short path lengths and a lack of clustering. This allows the diffusing behavior to reach a greater number of unique individuals without “wasting” redundant social ties on encouraging the same individual to adopt [watts1998collective, granovetter1973strength]. Conversely, the redundant ties in clustered networks enable repeated exposure to multiple influential neighbors at the expense of reaching fewer individuals [centola2007complex]. Fundamentally, this presents a trade-off. The very lack of clustering in random networks that enables more unique individuals to be exposed is also the clustering that enables redundant exposures. In a deterministic setting, simple and complex contagions fall cleanly on either side of this trade-off. Deterministic simple contagions are equally likely to be adopted from exposure to one as opposed to multiple influential neighbors, so there is no benefit from socially reinforcing, redundant ties. They spread faster on random networks that avoid such redundant ties. Deterministic complex contagions cannot be adopted with exposure to only one adopting neighbor so the ability to reach many unique individuals without redundant exposure is not beneficial [centola2007complex, guilbeault2021topological]. Instead, they can only spread on clustered networks and not random networks [centola2007complex, guilbeault2021topological]. When the decision to adopt a behavior is probabilistic, however, the spread of simple and complex contagions both benefit from the ability to reach more unique individuals through short path lengths and the redundant exposure to influential neighbors [dodds2005generalized, lee2022complex]. This is because, by nature of basic probability, the cumulative probability of adopting a behavior increases with repeated exposures even if the chance of adoption remains constant. Hence, it is unclear whether random or clustered networks are more advantageous to the diffusion of behaviors with probabilistic adoption. Already, a growing body of work introducing probabilistic elements to the canonical deterministic complex contagion model have found instances where the original theory does not hold [eckles2024long, sassine2023does, keating2022multitype, lu2011small, cui2014message, zheng2013spreading, de2009role]. Two recent papers are particularly relevant [eckles2024long, sassine2023does]. Both papers make important contributions by introducing some probabilistic non-socially reinforced adoption in their models, and find that such additions can lead to either faster [eckles2024long] or farther [sassine2023does] spread on random networks in contrast to results by Centola & Macy [centola2007complex]. However, they do not systematically vary probabilistic adoption for both socially reinforced and non-socially reinforced adoption together. Without providing a systematic investigation of the interplay of non-socially reinforced and socially reinforced adoption rates, neither quantifies the conditions under which random networks always spread faster and farther compared to clustered networks. As both papers [sassine2023does, eckles2024long] and other existing studies [de2009role, o2015mathematical, centola2010spread] are choosing exemplar points within the parameter space of stochastic contagions, it also remains unclear how representative certain diffusion patterns are in characterizing complex contagions more generally. Additionally, neither paper includes variable threshold dynamics, or examines analytically how far a behavior spreads based on variable levels of non-socially reinforced and reinforced adoption. To address this, we introduce a novel conceptual model of a contagion process with both tunable probabilistic adoption rates, and social reinforcement parameters. Our model relaxes the deterministic assumption of the original theory [centola2007complex, centola2010spread] and opens up a parameter space of non-socially reinforced and socially reinforced adoption probabilities that describe both stochastic and deterministic simple and complex contagions. Such a model has only been partially explored in past work [lu2011small, cui2014message, zheng2013spreading, de2009role, eckles2024long, sassine2023does, keating2022multitype]. We compare the diffusion of different contagion types, parameterized by the model, on clustered ring lattice networks [watts1998collective] to that of regular random networks constructed by rewiring clustered networks [maslov2002specificity], while holding network size and node degree (number of neighbors each individual in the network has) constant across network types. Using both agent-based modeling and analytical techniques, we are able to identify precise thresholds (or lower bounds of thresholds for certain cases) of adoption and social reinforcement, demarcating regions in which behavior on random networks spreads faster, further, or equally compared to clustered networks. We find that by introducing probabilistic non-socially reinforced and socially reinforced adoption, most instances of complex contagion spread equally or more on random networks even though the behavior exhibits positive social reinforcement. The key mechanism driving this result is that the gains in diffusion from reaching a greater number of unique individuals through the short paths and non-redundant ties of random networks outweighs the gains repeated exposure enabled socially reinforcing, redundant ties of clustered networks. The canonical result by Centola & Macy [centola2007complex] of greater spread of complex contagions on clustered networks only occurs among a small subset of complex contagions, namely those that approach a deterministic spreading process, which are unrepresentative of empirical social contagions [lee2022complex]. This subset shrinks further when individuals have more connections, when an individual needs proportionally more exposure to influential neighbors to themselves adopt, or when an individual remains influential for longer periods of time after adopting a behavior. In summary, that complex contagions spread faster and farther on clustered networks only holds true for specific, highly deterministic, regions of the behavioral parameter space. In most other areas, random networks spread a behavior equally or better. This suggests that greater diffusion on clustered networks is not a defining feature of complex contagions. Past experimental work [centola2010spread] that confirms the original theory, while contributing important and valid insights, may not be entirely representative of complex contagion more broadly when the assumption of deterministic behavior is relaxed. By developing a framework that systematically varies non-socially reinforced as well as socially reinforced adoption probabilities we can clearly demarcate this region of greater spread. This allows us to fully characterize model behavior as a function of other attributes of the network structure and behavior, thus building on other modeling work in the area [eckles2024long, sassine2023does, keating2022multitype, lu2011small, cui2014message, zheng2013spreading, de2009role]. Establishing Micro-foundations of Social Influence: A Model of Stochastic Contagion We introduce a model that describes the micro-level process of social influence, formalizing the differences between simple and complex contagions. All individuals in the network begin having not adopted a behavior (they are “susceptible”), except for several randomly chosen “seed” individuals who have already adopted and can influence their immediate neighbors to adopt (the seeds are “infected” individuals; Figure 1A). Those who have adopted a behavior remain influential towards their neighbors for a set time length (T) after which they can no longer influence others (they are “recovered”). This mirrors the Susceptible-Infective-Recovered (SIR) model from epidemiology [anderson1991infectious, barrat2008dynamical]. For each time step, all susceptible individuals are simultaneously exposed to any neighboring individuals who are currently influential. With every exposure to an influential neighbor, an individual may adopt the behavior with a certain “per-exposure” probability. This per-exposure probability of adoption is defined by p(c), where c indexes the number of different influential neighbors an individual has been in exposed to from the start of the simulation. p(c)=\begin{cases}0,&\text{if }c=0\\ p_{1},&\text{if }1\leq c<i\\ p_{2},&\text{if }c\geq i.\end{cases} All individuals follow this adoption rule identically and there is no heterogeneity among individuals except for network position. When an individual does not have contact with any influential neighbors, they cannot adopt the behavior. If an individual has been exposed to less than c different influential neighbors, they will adopt with a non-socially reinforced probability of p_{1}, which we call the below threshold adoption probability. If the number of different adopting neighbors an individual is in contact with equals or exceeds i, which we call the social reinforcement threshold, an individual adopts the behavior with a socially reinforced probability of p_{2}, which we call the above threshold adoption probability. In practice, even if an individual is exposed to multiple neighbors within one time step, the number of exposures is still counted serially. For instance, if i=2 and an unexposed individual is exposed to three influential neighbors for the first time within one time step, one neighbor “transmits” the behavior with the below threshold probability of p_{1} while the other two transmit the behavior with the above threshold probability of p_{2}. The difference between p_{1} and p_{2} quantifies the amount of social reinforcement the adoption of a behavior is sensitive to, the idea being that multiple exposures reinforce the likelihood of adoption beyond that of the baseline, below threshold adoption rate p_{1}. Setting different values of p_{1} and p_{2} can parameterize behaviors with different levels of below and above threshold adoption rates. This allows us to recover well studied forms of complex and simple contagions, while at the same time allows us to examine overlooked regions of the space (Figure 1C). When p_{1}=p_{2}, the threshold parameter i has no effect and p(c) remains constant across all additional contacts c. Increasing the number of influential neighbors an individual is exposed to does not increase an individual’s per-exposure probability of adoption, so the behavior is considered a simple contagion. The behavior is a deterministic simple contagion when p_{1}=p_{2}=1, and a stochastic simple contagion when 0<p_{1}=p_{2}<1. When p_{1}\neq p_{2}, the behavior is a complex contagion and is sensitive to social reinforcement. Social reinforcement can be positive when the per-exposure adoption probability increases with exposure to multiple influential neighbors, p_{1}<p_{2} (as theorized in complex contagion about costly behaviors such as attending a protest) or negative if exposure to additional influential neighbors somehow dampen each other, p_{1}>p_{2} (e.g., spreading a rumor may become less satisfying if many people already know it). Under both positive and negative social reinforcement, the complex contagion can be deterministic (p_{1}=0,p_{2}=1 in the positive case; p_{1}=1,p_{2}=0 in the negative case) or stochastic (0\leq p_{1}<p_{2}\leq 1 but not including p_{1}=0,p_{2}=1 or p_{1}=1,p_{2}=0). We focus on simple contagions and complex contagions with positive social reinforcement that are either deterministic or stochastic, where p_{1}\leq p_{2}. Among complex contagions with positive social reinforcement, the social reinforcement threshold i parameterizes how many different neighbors an individual must be in contact with in order to adopt at p_{2} instead of p_{1}, “activating” this positive reinforcement effect. Holding constant the total number of neighbors an individual has (formalized as the individual’s degree k), while increasing i increases the costliness of adopting a behavior, in the sense that contact with more socially reinforcing neighbors relative to the total number of neighbors is required to adopt at the higher, above threshold adoption probability. This is not unlike various existing threshold models [schelling1969models, granovetter1978threshold, valente1996social] where individuals adopt a behavior based on whether a certain threshold of neighbors adopts. However, rather than governing deterministic adoption, surpassing i only increases the likelihood of adoption from p_{1} to p_{2}. As we are interested in providing a minimal model that systematically varies adoption and social reinforcement, we model adoption in probabilistic terms while retaining a homogeneous social reinforcement threshold i that serves as a model parameter. The length of time an individual remains influential for after adopting, or what we call the “time of influence” T, models a distinction between behaviors that remain transmissible for longer periods of time as opposed to shorter periods of time. For instance, behaviors that remain highly visible, salient, or relevant over time (such as changing a highly visible profile picture on social media) may exhibit longer times of influence compared to behaviors where visibility quickly diminishes with time (such as changing a highly visible profile picture on social media).444This is similar to incorporating memory parameters into a contagion [dodds2005generalized, cui2014message, sassine2023does]. At the extreme, such a distinction between diffusion processes with longer or shorter times of influence is analogous to the differences between the canonical Susceptible-Infective (SI) model, where the time of influence is infinite, and Susceptible-Infective-Recovered (SIR) model, where the time of influence is some finite value. Research from epidemiology has shown divergent diffusion patterns from SI and SIR models, giving reason to believe that varying time of influence may have a significant role in how a behavior spreads [anderson1991infectious, barrat2008dynamical, dorogovtsev2008critical]. Core to understanding the difference between simple and complex contagions is making a distinction between gains in diffusion from social reinforcement on the one hand, and gains from receiving repeated exposures to influential neighbors on the other. The former, benefiting from social reinforcement, refers to an increase in the per-exposure probability of adoption of a behavior as exposure to the number of influential neighbors increases (adopting at p_{2} instead of p_{1}). This is characteristic of complex contagions with positive reinforcement studied here. The latter, benefiting from repeated exposures to influential neighbors, refers to the extent to which the cumulative probability increases with more exposures, simply from the nature of probability (the chance of observing at least one coin toss to come up heads is higher when we flip two coins than when flipping just one (i.e., p(\texttt{at least one head})=1-(1-0.5)^{2}=0.75)). While benefiting from social reinforcement is only possible when the number of different influential neighbors exceeds the threshold i, benefiting from redundant exposures occur with every exposure, regardless of whether they are from the same neighbor or different neighbors. Non-socially reinforced stochastic simple contagions benefit only from increasing exposure to influential neighbors, but complex contagions with positive social reinforcement benefit from both redundant exposure to influential neighbors and the socially amplified adoption probability p_{2} (when exposures exceed the threshold i). This difference can be formalized by the cumulative probability F(c) of the per-exposure probability of adoption p(c), where F_{C}(c)=P(C\leq c) (Figure 1B). The cumulative probability of adopting a simple contagion can be expressed as, F(c)=1-(1-\beta)^{c} where p_{1}=p_{2}=\beta. When the behavior is a deterministic simple contagion, p_{1}=p_{2}=1, F(c)=1, and the likelihood of adopting the behavior does not increase with additional exposures after the first exposure. However, when 0<\beta<1 and the behavior is a stochastic simple contagion, F(c) increases with additional exposures to influential neighbors, similarly to that of complex contagions, even though the behavior is not more likely to be adopted with socially reinforcement. In the case of stochastic complex contagions though, F(c) increases at a faster rate compared to stochastic simple contagions with the same below threshold probability p_{1}. This is visible in the bottom right panel of Figure 1B: while the simple stochastic contagion experiences increasing cumulative adoption probability from exposure to more influential neighbors (albeit with diminishing returns), the increase for the stochastic complex contagion is higher. Given that both simple and complex probabilistic contagions benefit from repeated exposures through redundant ties, but can also be transmitted along non-redundant ties, it becomes theoretically ambiguous as to whether the presence of clustering and redundant ties would be beneficial for spread in either case. Stochastic simple contagions benefit from redundant exposures, while stochastic complex contagions with non-zero below threshold adoption probabilities can benefit from reaching more unique individuals even through non-redundant ties (Figure 1B). This stands in contrast to the clean cut deterministic case where complex contagions spread better on clustered networks because they only benefit from redundant ties, and simple contagions spread better on random networks because they only benefit from non-redundant ties. The relative strengths of these two effects, gains from redundant ties as opposed to gains from non-redundant ties, will determine which network spreads behavior “better”. By exploring this model, we will show that socially reinforced complex contagions spread farther and faster on clustered networks only in a small area of the p_{1}\leq p_{2} parameter space whereas in the majority of the parameter space the random network either performs equally or better. We additionally test the effects of differing degree (k), social reinforcement threshold (i), and time of influence (T; see Methods). Figure 1: Micro and Macro Views of Diffusion on Clustered and Random Networks. A. The seeding structure and early time diffusion in random and clustered networks, with example parameters k=6 and i=3. Random networks are able to reach more individuals (ki-i+1 in the first time step), but all adopt at the lower, below threshold adoption rate p_{1}. Clustered networks reach less individuals (k in the first time step) but more individuals receive reinforcing signals and may adopt at the above threshold, p_{2}. This illuminates a fundamental trade-off of having more or less redundant ties. B. Per-exposure and cumulative adoption probabilities for stochastic and deterministic simple and complex contagions. Deterministic simple contagions do not benefit from social reinforcement, but deterministic complex contagions, as well as stochastic simple and complex contagions do. C. The space of all possible p_{1}\leq p_{2} values that uniquely define a behavior, or adoption trajectory (p(c),F(c))."
https://arxiv.org/html/2411.07475v1,Degree Matrix Comparison for Graph Alignment,"Graph alignment considers the optimal node correspondence across networks. To advance unsupervised graph alignment algorithms on plain graphs, we propose Degree Matrix Comparison (DMC). Through extensive experiments and mathematical motivations, we demonstrate the potential of this method. Remarkably, DMC achieves up to 99\% correct node alignment for 90\%-overlap graphs and 100\% accuracy for isomorphic graphs. Additionally, we propose a reduced version of DMC (Greedy DMC) that provides a solution to the graph alignment problem with lower time complexity. DMC could significantly impact graph alignment, offering a reliable solution for the task.","Graph alignment is a critical task with applications across various domains, like social networks, biology, and cybersecurity. It is also highly related to the subgraph isomorphism problem in mathematics and computer science. Hence, improving graph alignment methods holds significant value. Supervised or semi-supervised methods typically exploit node attributes, such as semantic features of users [1, 2], but these methods break down when users disguise their identities [3]. WAlign is a recent unsupervised method, but also harnesses the power of attributes [4]. We focus on the unsupervised graph alignment problem for unattributed plain graphs, leveraging geometric properties. Our proposed method, Degree Matrix Comparison (DMC), is a method that fits the task. Unsupervised methods with a focus on graph geometry include: REGAL [5], FINAL [6], Klau [7], and IsoRank [8]. For a more comprehensive view of past unsupervised alignment methods, see [9]. DMC was constructed with two core ideas in mind: degree is the most accurate and direct description of local structure; moreover, in representing geometric structure, it makes sense to include both local and global information, like in [10, 11], especially in compact matrix form (adjacency matrices and Laplacian matrices). We propose foregoing the need to know exactly which nodes are connected, like in an adjacency matrix, and record accurately the degree of a node and its neighbors’ degrees in a matrix. We show that DMC works well for aligning real world graphs with high clustering and link complexity."
https://arxiv.org/html/2411.07663v1,Is Graph Convolution Always Beneficial For Every Feature?,"Graph Neural Networks (GNNs) have demonstrated strong capabilities in processing structured data. While traditional GNNs typically treat each feature dimension equally during graph convolution, we raise an important question: Is the graph convolution operation equally beneficial for each feature dimension? If not, the convolution operation on certain feature dimensions can possibly lead to harmful effects, even worse than the convolution-free models. In prior studies, to assess the impacts of graph convolution on features, people proposed metrics based on feature homophily to measure feature consistency with the graph topology. However, these metrics have shown unsatisfactory alignment with GNN performance and have not been effectively employed to guide feature selection in GNNs. To address these limitations, we introduce a novel metric, Topological Feature Informativeness (TFI), to distinguish between GNN-favored and GNN-disfavored features, where its effectiveness is validated through both theoretical analysis and empirical observations. Based on TFI, we propose a simple yet effective Graph Feature Selection (GFS) method, which processes GNN-favored and GNN-disfavored features separately, using GNNs and non-GNN models. Compared to original GNNs, GFS significantly improves the extraction of useful topological information from each feature with comparable computational costs. Extensive experiments show that after applying GFS to 8 baseline and state-of-the-art (SOTA) GNN architectures across 10 datasets, 83.75% of the GFS-augmented cases show significant performance boosts. Furthermore, our proposed TFI metric outperforms other feature selection methods. These results validate the effectiveness of both GFS and TFI. Additionally, we demonstrate that GFS’s improvements are robust to hyperparameter tuning, highlighting its potential as a universal method for enhancing various GNN architectures.","Graph Neural Networks (GNNs) are widely used for processing graph-structured data, such as recommendation systems (Wu et al., 2022; 2019b), social networks (Li et al., 2023a; Awasthi et al., 2023; Luan et al., 2019), telecommunication (Lu et al., 2024a) and bio-informatics (Zhang et al., 2021; Kang et al., 2022; Hua et al., 2024). Although graph convolution has been shown effective to enrich node features with topological information through message propagation, the performance gain is found to be restricted by the assumption of homophily, i.e., similar nodes are more likely to be connected in a graph (McPherson et al., 2001). On the other hand, when a graph exhibits low homophily, i.e., heterophily, the graph convolution operation can lead to performance degradation and sometimes even underperform convolution-free models, such as Multi-Layer Perceptrons (MLPs) (Zhu et al., 2020a; Luan et al., 2022b; 2024c). Therefore, to measure the impact of graph convolution operation, label-based homophily metrics (Pei et al., 2020a; Zhu et al., 2020a) are proposed to measure the label consistency along graph topology. However, they neglect the effects on node features, which is crucial for graph learning. Feature homophily metrics (Yang et al., 2021a; Jin et al., 2022) are then proposed to measure the feature consistency along the graph. Although these existing metrics can capture the feature similarity between connected nodes, they overlook that different feature dimensions may exhibit different levels of compatibility with graph structures, and thus may gain different amounts of benefits or negative impacts from graph convolution. For example, as illustrated in Figure 1 (right), the GNN-favored feature exhibits uniform values among intra-class nodes while differing across inter-class nodes. This characteristic enables graph convolution operation to effectively improve the distinguishability among nodes from different classes, as demonstrated in (Luan et al., 2024b). Conversely, graph convolution on GNN-disfavored features may hinder the learning process of GNNs. This example raises a crucial question: How can we determine whether graph convolution is beneficial or not for a specific feature? Figure 1: Improvements in GNN performance at node level and feature level. Different colors denote node labels, while the direction and magnitude of arrows denote node features. To address this issue, in this paper, we propose Topological Feature Informativeness (TFI), which measures the mutual information between each dimension of aggregated node features and labels. TFI can identify features that are either favored or disfavored by GNNs, provably providing an upper bound on the performance gap between graph convolution and convolution-free models, which is then supported by both theoretical analysis and empirical observations. Additionally, TFI overcomes the “good heterophily” issue, which is a serious misalignment of existing feature homophily metrics and GNN performance. Besides, TFI can work well in sparse label scenarios. Motivated by the principle of “feed the right features to the right model” Luan et al. (2022a), we propose a simple yet effective method called Graph Feature Selection (GFS). GFS first uses TFI to identify GNN-favored and GNN-disfavored features. Then, to enhance the extraction of useful information, the GNN-favored features are processed by GNNs, while MLPs handle the GNN-disfavored features. Last, a final linear layer fuses the embeddings from both models to obtain the final node representation. GFS can be seamlessly integrated into almost any GNN architecture, improving overall model performance with comparable computational costs. Our experiments on real-world datasets demonstrate that GFS significantly boosts the performance of 8 GNNs across 10 datasets in node classification tasks and the improvement is robust to hyperparameter tuning. Moreover, we demonstrate that TFI outperforms other statistical and optimization-based metrics for feature selection in GNNs, validating its superiority. Besides, we surprisingly find that GFS is much more effective on node embeddings encoded by Pretrained Large Models (PLMs) than other methods. This implies that the advantages of PLMs in understanding graph-structured data might be rooted in their ability to disentangle the topology-aware and topology-agnostic information into separate feature dimensions. In summary, our main contributions are as follows. • We introduce a novel metric, Topological Feature Informativeness (TFI), to distinguish between GNN-favored and GNN-disfavored features. We validate its effectiveness through both empirical observations and theoretical analysis. • We propose Graph Feature Selection (GFS) based on TFI, a simple yet powerful method that significantly boosts GNN performance. To the best of our knowledge, this is the first study to address the feature selection problem based on GNN-favored and GNN-disfavored disentanglement. • Our extensive experiments demonstrate that applying GFS to 8 baseline and state-of-the-art (SOTA) GNN architectures across 10 datasets yields a significant performance boost in 83.75\% (67 out of 80) of the cases."
https://arxiv.org/html/2411.06138v1,: A Harmful Content Detection and Mitigation Architecture for Social Media Platforms,"The mental health of social media users has started more and more to be put at risk by harmful, hateful, and offensive content. In this paper, we propose StopHC, a harmful content detection and mitigation architecture for social media platforms. Our aim with StopHC is to create more secure online environments. Our solution contains two modules, one that employs deep neural network architecture for harmful content detection, and one that uses a network immunization algorithm to block toxic nodes and stop the spread of harmful content. The efficacy of our solution is demonstrated by experiments conducted on two real-world datasets.","From hate speech and misinformation to verbal violence and death threats, social platforms like X (formally Twitter) have become a favorable space for content that can cause harm through online aggression [10]. Harmful content is no longer perceived only as a form of immorally expressed opinions but as a recognized global danger that must be prevented for the mental, emotional, and physical safety of online content consumers [32]. It is necessary to be able to detect the sources that generate such toxic behavior on the Internet and to reduce the influence they have. The more the spread of hate-speech posts is decreased, the more users will be saved from emotional and psychological damage. To address these issues, we propose StopHC, a harmful content detection and mitigation architecture for social media platforms. The main objectives of this paper are: (1) to detect problematic behavior, and (2) to minimize the spread of such behaviors. To detect harmful content, we train multiple deep neural network models using different embeddings that consider the syntax (i.e., word embeddings such as Word2Vec [11, 12] and GloVe [13]), context (i.e., transformed embeddings such as BERT [5] and RoBERTa [8]), and network information (i.e., Node2Vec [6]). Using these approaches, we aim to improve the StopHC detection module by employing the model that better understands both textual content and network structures. To minimize the spread of such behaviors, we employ 3 different immunization strategies: (1) naïve (i.e., Highest Degree [9]), (2) pro-active (i.e., NetShield [3]), and (3) contra-active (i.e., DAVA [33]). Using these strategies, we aim to improve StopHC mitigation module and offer a graph-dependent solution. The main contributions of this work are four-fold: C_{1} We propose StopHC, a novel architecture for harmful content detection and mitigation; C_{2} We develop new deep neural network models for harmful content detection; C_{3} We employ immunization strategies to stop the spread of harmful content on social media platforms; C_{4} We perform extensive evaluation testing on two real-world datasets. This work is structured as follows. Section II provides insights into the recent literature. Section III presents StopHC’s architecture Section IV offers the experimental evaluation of our solution. Finally, Section V presents the conclusions of this work and discusses future work."
https://arxiv.org/html/2411.06122v1,Characteristics of Political Misinformation Over the Past Decade,"Although misinformation tends to spread online, it can have serious real-world consequences. In order to develop automated tools to detect and mitigate the impact of misinformation, researchers must leverage algorithms that can adapt to the modality (text, images and video), the source, and the content of the false information. However, these characteristics tend to change dynamically across time, making it challenging to develop robust algorithms to fight misinformation spread. Therefore, this paper uses natural language processing to find common characteristics of political misinformation over a twelve year period. The results show that misinformation has increased dramatically in recent years and that it has increasingly started to be shared from sources with primary information modalities of text and images (e.g., Facebook and Instagram), although video sharing sources containing misinformation are starting to increase (e.g., TikTok). Moreover, it was discovered that statements expressing misinformation contain more negative sentiment than accurate information. However, the sentiment associated with both accurate and inaccurate information has trended downward, indicating a generally more negative tone in political statements across time. Finally, recurring misinformation categories were uncovered that occur over multiple years, which may imply that people tend to share inaccurate statements around information they fear or don’t understand (Science and Medicine, Crime, Religion), impacts them directly (Policy, Election Integrity, Economic) or Public Figures who are salient in their daily lives. Together, it is hoped that these insights will assist researchers in developing algorithms that are temporally invariant and capable of detecting and mitigating misinformation across time.","Misinformation is a statement that contains false or misleading information, and can result in serious consequences, including the erosion of civil discourse, political paralysis, uncertainty, in addition to alienation and disengagement (Kavanagh & Rich, 2018; Hook & Verdeja, 2022). Despite its serious impact on individuals and society, misinformation is known to be shared more than valid information (Vosoughi, et al, 2018), and the reasons that misinformation is propagated are diverse and include cognitive factors (Del Vicario, et al , 2016; Ecker, et al, 2022), socio-affective factors (Ecker, et al, 2022), incentives (Ceylan, et al, 2023) and changes in the information system (Kavanagh & Rich, 2018, Chen et al, 2023). As a result, finding scalable solutions to detect and mitigate the impact of misinformation has proven challenging, although many efforts have demonstrated some promise (Conroy, et al., 2015; ,Aldwairi & Aldwairi, 2018, RAND, 2018). Part of the challenge facing researchers is that misinformation can be propagated through many modalities (e.g., text, image and video), thereby increasing the algorithmic complexity and computational resources necessary to deploy scalable solutions. Moreover, the sources from which misinformation is spread can effortlessly adapt to any mitigation attempts (e.g., account removal), which can quickly become a real-life version of the Whac-A-Mole game. In order for robust and effective solutions to be deployed, first we must understand the features of misinformation that are temporally invariant, since the content, sources and modalities associated with misinformation are likely to change across time. Although some researchers have explored temporal patterns of misinformation on social media (Allcott, et al, 2019), they focused on relatively narrow timelines that were less than five years in range. Therefore, this effort explores the trends associated with political misinformation over a twelve year period, leveraging tools from natural language processing to uncover common themes in misinformation across time. By uncovering these insights, it may allow researchers to develop more robust tools to detect and mitigate misinformation, and the next section will detail the data used to this end."
https://arxiv.org/html/2411.06116v1,Supernotes: Driving Consensus in Crowd-Sourced Fact-Checking,"X’s Community Notes, a crowd-sourced fact-checking system, allows users to annotate potentially misleading posts. Notes rated as helpful by a diverse set of users are prominently displayed below the original post. While demonstrably effective at reducing misinformation’s impact when notes are displayed, there is an opportunity for notes to appear on many more posts: for 91% of posts where at least one note is proposed, no notes ultimately achieve sufficient support from diverse users to be shown on the platform. This motivates the development of Supernotes: AI-generated notes that synthesize information from several existing community notes and are written to foster consensus among a diverse set of users. Our framework uses an LLM to generate many diverse Supernote candidates from existing proposed notes. These candidates are then evaluated by a novel scoring model, trained on millions of historical Community Notes ratings, selecting candidates that are most likely to be rated helpful by a diverse set of users. To test our framework, we ran a human subjects experiment in which we asked participants to compare the Supernotes generated by our framework to the best existing community notes for 100 sample posts. We found that participants rated the Supernotes as significantly more helpful, and when asked to choose between the two, preferred the Supernotes 75.2% of the time. Participants also rated the Supernotes more favorably than the best existing notes on quality, clarity, coverage, context, and argumentativeness. Finally, in a follow-up experiment, we asked participants to compare the Supernotes against LLM-generated summaries and found that the participants rated the Supernotes significantly more helpful, demonstrating that both the LLM-based candidate generation and the consensus-driven scoring play crucial roles in creating notes that effectively build consensus among diverse users.","Misinformation has become a pervasive characteristic of discourse online, leading to widespread negative consequences. Addressing its spread is a complex challenge that requires a range of mitigation strategies. Platforms have experimented with various approaches including professional fact-checking, domain filtering, and encouraging attention to accuracy (Pennycook and Rand, 2019; Pennycook et al., 2021; Lazer et al., 2018). One approach that has recently attracted considerable attention is crowd-sourced fact-checking. Multiple academic studies have demonstrated that a panel of regular users could be as effective at fact-checking misinformation as professional fact-checkers (Bhuiyan et al., 2020; Allen et al., 2021; Resnick et al., 2023). Figure 1. An example post, along with three community notes proposed to provide additional context, and the Supernote produced by our framework. Motivated by these findings, X has developed and launched a crowd-sourced fact-checking system called Community Notes (Wojcik et al., 2022). The system allows users to create notes that can be attached to potentially misleading posts and to rate the helpfulness of proposed notes. It uses a matrix factorization algorithm to score the overall helpfulness of the notes based on the individual ratings. Notes rated helpful by many users with diverse views, as measured by their latent representations, are scored higher. Only notes that cross a certain helpfulness threshold are considered helpful and attached to the post. As of October 2, 2024, users have added more than 1.28 million notes on over 738,000 posts, and more than 78.8 million ratings. X provides continuous public access to the code and data behind the system. Figure 2. Overview of our framework for generating Supernotes. (1) We prompt an LLM to generate many candidate Supernotes using the post text and the existing community notes. (2) We score the helpfulness of each candidate by simulating a jury of raters, predicting their ratings, and aggregating them using the Community Notes algorithm. (3) We filter out candidates that do not follow key principles of effective fact-checking. (4) Finally, we rank and select the candidate with the highest score. Both internal pilot experiments and external analyses find that community notes have a significant impact on users’ engagement with and perceptions of misinformation (Wojcik et al., 2022; Chuai et al., 2024a; Renault et al., 2024; Chuai et al., 2024b). Once a community note is attached to a post, the post is more likely to be deleted and less likely to be reposted or liked. Users are also less inclined to agree with the substance of misleading posts when presented with a community note (Wojcik et al., 2022) and tend to trust community notes more than warnings by third-party fact-checkers (Drolsbach et al., 2024). While the community notes have a significant impact on the posts to which notes are attached, only 12.5% of all posts for which a note has been proposed have a note that has reached a helpful status and thus has been attached to the post. For 91% of the posts for which a note has been proposed, the note(s) have failed to gather enough helpful ratings by users with diverse views and are never attached to the post. Although some of the proposed notes may be inaccurate, many contain valuable information that offers important additional context to the post. We postulate two reasons why otherwise valuable notes fail to reach helpful status and are never displayed on the platform. First, individual notes may present biased or one-sided perspectives, failing to achieve the neutral tone and information necessary for broad acceptance. This can lead to polarized ratings, with users aligned with the note’s perspective rating it as helpful, while others find it unpersuasive or even misleading. For example, in Figure 1, the second note might be perceived as argumentative or partisan, hindering its adoption by a wider audience. Second, essential context is often fragmented across multiple notes, each offering a piece of the puzzle but failing to provide a holistic understanding in isolation. Figure 1 illustrates this, with separate notes addressing President Biden’s authority on debt cancellation and the administration’s proposal on medical debt reporting. These fragmented insights, while individually valuable, may not achieve sufficient visibility on their own. The present work. We propose a framework for generating Supernotes: notes that synthesize information from several existing community notes and are written to foster consensus among a diverse set of users. The key idea behind the framework is to use LLMs to generate many candidate Supernotes and use a helpfulness model trained on millions of publicly available Community Notes ratings to select the candidates that are most likely to be rated helpful by a diverse set of users (Figure 2). The framework consists of two components: candidate generation and candidate scoring (Section 2). The candidate generation component leverages an LLM to generate many diverse candidate Supernotes. The candidate scoring component ranks the proposed candidate by how likely it is to be rated as helpful by a diverse set of users and whether it follows core principles of effective fact-checking. To estimate whether a proposed Supernote would be rated helpful broadly, we simulate a jury of randomly sampled raters, predict their individual ratings, and aggregate them in a single helpfulness score. We leverage the millions of publicly-available ratings on existing community notes to train a model that given the post, note, and rater information predict the rater’s helpfulness rating. To aggregate the predicted ratings, we use the Community Notes matrix factorization algorithm (Wojcik et al., 2022). The candidate scoring component also ensures that the Supernotes follow core principles for effective fact-checking (e.g., are unbiased and non-argumentative) and do not introduce any new links. To test our framework, we evaluate the performance of the individual components on historical data (Section 3) and conduct human subject experiments (Section 4). At the individual rating level, we show that our personalized helpfulness model accurately predicts ratings on community notes (AUC = 0.85) that were held out during training. At the jury level, we show that aggregated predictions of a jury of raters accurately predicts which note among all notes on a held out post is most helpful according to the empirical ratings (P@1 = 0.7). To evaluate the helpfulness of the Supernotes produced by our framework, we sample 100 posts and recruit participants to rate the helpfulness of the Supernotes and the best existing note on the post, without revealing which one is which. We find that participants rate the Supernotes as significantly more helpful than the best existing note and, when asked to choose among the two, select the Supernote 75.2% of the time. When asked to rate the note’s quality, clarity, coverage, context, and argumentativeness, participants rated the Supernotes significantly more favorably than the best existing notes across all five dimensions. Finally, to evaluate the impact of the Candidate Scoring component, we ran a follow-up experiment in which we asked participants to compare the Supernotes generated by the full pipeline with an LLM summary generated by our Candidate Generation component. We find that the participants rated the Supernotes as more helpful than the LLM summaries 61.5% of the time, demonstrating the impact of the Candidate scoring component. We release the code needed to implement our framework and replicate our analyses at: https://github.com/saveski-lab/supernotes."
https://arxiv.org/html/2411.06011v1,Exploring the impact of reflexivity theory and cognitive social structures on the dynamics of doctor-patient social system,"Conventional economic and socio-behavioural models assume perfect symmetric access to information and rational behaviour among interacting agents in a social system. However, real-world events and observations appear to contradict such assumptions, leading to the possibility of other, more complex interaction rules existing between such agents. We investigate this possibility by creating two different models for a doctor-patient system. One retains the established assumptions, while the other incorporates principles of reflexivity theory and cognitive social structures. In addition, we utilize a microbial genetic algorithm to optimize the behaviour of the physician and patient agents in both models. The differences in results for the two models suggest that social systems may not always exhibit the behaviour or even accomplish the purpose for which they were designed and that modelling the social and cognitive influences in a social system may capture various ways a social agent balances complementary and competing information signals in making choices.","Conventional economic and behavioural studies have long assumed that an agent has complete access to information and adheres to rational behaviour to model interactions between social agents [KorobkinUlen2000, Soros2013]. However, evidence challenging these assumptions has accumulated, pointing to other, more plausible theories [BromileyPapenhausen2003, Crotty2017]. This paper presents a novel model of social system design and analysis. Our model incorporates ideas from the theory of reflexivity [Soros2013] and cognitive social structures [Krackhardt1987a] offering a socio-cognitive approach to understanding the behaviours of individual social agents and emergent social phenomena. Reflexivity theory, considered formally proposed for economic analysis by George Soros [Soros2013],[Umpleby2018], is similar to the concepts found in the studies and literature of second-order cybernetics [Scott2004]. The theory, in a nutshell, suggests that social agents and the environment, which constitute a single system, are involved in a feedback loop with not just negative feedback, as usually considered to be the case in classical economic analysis of markets and social phenomena, but also positive feedback to and from the agents and the environment driving the overall system towards a specific, attractor state [Davis2020]. The critical insight from the theory that we have utilized in our model is that feedback loops between the agents and their environments change both the agents and the environments; once the agents act, the environments that they are in change as well, which influences the following actions the agents take and so it continues. Furthermore, the social agents can only access their own subjective realities of these changes, which are different from an objective reality (See Figure 1). These subjective realities can often drive many social phenomena, including business cycles and stock market bubbles [Beinhocker2013],[Soros2013], despite their possible deviations from the objective reality. Figure 1: Outline of Reflexivity Theory [Soros2013] Cognitive social structures, an extension of the theory of social structures, was instrumental in understanding that social agents are situated in a social network with relationship dynamics and that the position of social agents in a network influences the perceptions, decisions and actions of each agent in that network [Brands2013],[Krackhardt1987a]. Once again, similar to what we observe in the theory of reflexivity, the social agents do not have direct access to objective reality according to this framework. Instead, social agents perceive their interactions with others through a subjective lens, forming the basis for their actions and decisions. However, the critical difference between the two theories is that while the theory of reflexivity explores the dynamics of the interactions of social agents in a system, cognitive social structures uncover the matrix of relationships that influence the subjective perceptions of each agent [Frank2015]. Our primary motivation behind pursuing this research is the intuition that socially embedded, reflexive agents have different sets of behaviours that lead to the emergence of social phenomena which are closer to what we observe in our social systems compared to what we observe from classical economic and sociological models where rationality and perfect information assumptions are made. It is essential to realize and recognize that perceptions of individual agents are real in their consequences, even if there is no direct, one-to-one relationship between observed behaviours and objective reality. To flesh out our intuition, we compare and contrast observations from the simulation of two types of models, which we call the ”classical” and ”cognitive social system”, respectively. In particular, we explore the behaviours of doctors and patients in a primary healthcare network. We have chosen this social system partly because of the abundance of literature regarding the choices and behaviours of patients and doctors [Djulbegovic.etal2014],[Harris2003],[Kozikowski.etal2022] and also because of its utilitarian nature; better comprehension of this system would lead to more robust and precise healthcare services that will help in saving lives and improving standards of living [Cabrera.etal2011],[Comis.etal2021]. The classical model employs perfect information and rational behaviour assumptions, while the cognitive social system integrates the theory of reflexivity and cognitive social structures. We want to find out whether the best doctors, as determined by certain traits in the models, such as their credentials and abilities to conduct research, among others, get the most patients. In classical models, the best doctors would ideally have the best reputation because patients would know the best doctors and choose them accordingly to receive treatments. However, in the cognitive social system, the best doctors may not have the most favourable reputations because of social ties, which modulate the perception of their abilities and, thus, alter the judgements of patients choosing them. We implement a microbial genetic algorithm to optimize the agents in both models [Harvey2011]. We use this variation of the genetic algorithm because both of our models are relatively small in scale, and using a complete, conventional genetic algorithm may detract from the focus of our analysis. While a microbial genetic algorithm is more skeletal than a conventional one, it is more than adequate for our purposes in this paper. Ultimately, we are convinced that cognitive social systems can be generalized to understand other social systems, such as those found in areas such as education and defense, among others, and even design new ones. The potential of cognitive social system modelling to capture the dynamics of agents’ interactions embedded in social systems more accurately than conventional models is immense, as long as the modeller can rely on sound literature and verify the models with well-founded data."
https://arxiv.org/html/2411.06878v1,GraphRPM: Risk Pattern Mining on Industrial Large Attributed Graphs,"Graph-based patterns are extensively employed and favored by practitioners within industrial companies due to their capacity to represent the behavioral attributes and topological relationships among users, thereby offering enhanced interpretability in comparison to black-box models commonly utilized for classification and recognition tasks. For instance, within the scenario of transaction risk management, a graph pattern that is characteristic of a particular risk category can be readily employed to discern transactions fraught with risk, delineate networks of criminal activity, or investigate the methodologies employed by fraudsters. Nonetheless, graph data in industrial settings is often characterized by its massive scale, encompassing data sets with millions or even billions of nodes, making the manual extraction of graph patterns not only labor-intensive but also necessitating specialized knowledge in particular domains of risk. Moreover, existing methodologies for mining graph patterns encounter significant obstacles when tasked with analyzing large-scale attributed graphs. In this work, we introduce GraphRPM, an industry-purpose parallel and distributed risk pattern mining framework on large attributed graphs. The framework incorporates a novel edge-involved graph isomorphism network (EGIN) alongside optimized operations for parallel graph computation, which collectively contribute to a considerable reduction in computational complexity and resource expenditure. Moreover, the intelligent filtration of efficacious risky graph patterns is facilitated by the proposed evaluation metrics. Comprehensive experimental evaluations conducted on real-world datasets of varying sizes substantiate the capability of GraphRPM to adeptly address the challenges inherent in mining patterns from large-scale industrial attributed graphs, thereby underscoring its substantial value for industrial deployment.","Figure 1: Example risk patterns. Risk pattern A describes the behavior of fraudsters who defraud funds from multiple victim users and quickly transfer them to different downstream bank cards. Risk pattern B describes that the fraudster collects the victim’s funds multiple times in the name of investment through a shop, giving rewards in the early stage but no longer paying in the later stage. Precision and recall metrics are the evaluation criteria for measuring risk patterns in the industry. Graph pattern mining constitutes a pivotal task within the ambit of mining and machine learning, with profound applications extending to various industrial and business domains such as social network analysis [13], financial fraud detection [2, 10], and computational bioinformatics [21]. Taking the financial transaction scenario as an example, fraudsters would try to cheat normal users and make illegal money transfers. The distinctive behavioral patterns of these fraudsters, termed ’risk patterns’, are critical for the detection of fraudulent activity and the prevention of financial fraud, as exemplified in Fig. 1. Compared to black-box neural network models used for identifying fraudsters [12], industry experts express a preference for summarizing these risk patterns, as they provide more granular insight into the conduct of fraudulent entities, thereby facilitating a more explainable approach to fraud detection. Nonetheless, the manual delineation or construction of these patterns by experts is a labor-intensive process that demands considerable domain-specific knowledge. Consequently, the automation of risk graph pattern mining is an avenue warranting exploration. GRAMI [3] presents a method for frequent subgraph mining by leveraging a novel canonical labeling technique to efficiently discover patterns within a single large graph. Bliss [8] introduces an optimized tool for canonical labeling, specifically designed to handle the challenges posed by large and sparse graph structures, enhancing the performance of graph mining tasks. T-FSM [20] outlines a task-based framework that enables massively parallel processing for frequent subgraph pattern mining, addressing the scalability issues associated with big graph data. Despite this interest, extant automated graph pattern mining algorithms [3, 8, 14, 20] are impeded by two principal limitations: 1. Challenges in processing attributed graphs. In numerous real-world applications, simplistic representations of graph topology fall short of accurately depicting risk scenarios. There is a necessity to leverage high-dimensional attributes associated with nodes or edges for a nuanced characterization of entities, which is beyond the capabilities of methods that are restricted to or can only process one-dimensional attributes. 2. Deficiencies in scalability. Graph data within industrial environments is characteristically voluminous, spanning millions or even billions of nodes. Existing methodologies lack the integration of computational optimization strategies that are critical for the effective and efficient management of data at such an industrial scale. This shortfall in capability significantly undermines the suitability of these methods for application in industrial tasks, which necessitate robust data manipulation and analytical capacity to handle the sheer volume and complexity of the data involved. In this paper, we address the problem of Risk Patterns Mining on large transaction attributed graphs (GraphRPM). Although our research is primarily focused on financial fraud detection, the versatility of the proposed framework allows for its extension to a multitude of industrial applications, including but not limited to analysis within social network contexts. The challenge of managing and processing large-scale attributed graphs in industrial settings is a nontrivial hurdle, particularly in the realm of data mining. The primary objective of this study is to establish a robust and efficacious methodological framework capable of discerning distinct graph patterns as discriminative entities, enabling the differentiation of various graphical structures and the identification of fraud risk patterns. GraphRPM introduces a pioneering Edge-Involved Graph Isomorphism Network (EGIN) that addresses the challenge of fuzzy matching in attributed graph patterns, striking a balance between computational complexity and accuracy. Furthermore, this study implements a two-stage mining strategy coupled with a parallel distributed processing framework to diminish computational redundancy and enhance efficiency. Additionally, we present a Pattern Risk Score as an evaluative measure for identifying salient risk patterns. Comprehensive evaluations across diverse real-world datasets, varying in size, corroborate GraphRPM’s proficiency in resolving pattern mining issues within expansive industrial attributed graphs. Our research represents a significant advancement in the application of data mining and machine learning to industrial and business analytics. We contribute to the field in two pivotal ways. 1. We meticulously conceptualize and address the hitherto underexplored issue of discerning risk patterns on large-scale attributed graphs. 2. We introduce an all-encompassing analytical framework that not only incorporates the cutting-edge EGIN algorithm but also integrates a scalable distributed computation system, thereby enhancing computational efficacy. To our knowledge, this is the first proposition of an approximation algorithm based on graph neural networks for risk pattern mining on large transaction-attributed graphs."
https://arxiv.org/html/2411.06866v1,Subgraph Retrieval Enhanced by Graph-Text Alignment for Commonsense Question Answering,"Commonsense question answering is a crucial task that requires machines to employ reasoning according to commonsense. Previous studies predominantly employ an extracting-and-modeling paradigm to harness the information in KG, which first extracts relevant subgraphs based on pre-defined rules and then proceeds to design various strategies aiming to improve the representations and fusion of the extracted structural knowledge. Despite their effectiveness, there are still two challenges. On one hand, subgraphs extracted by rule-based methods may have the potential to overlook critical nodes and result in uncontrollable subgraph size. On the other hand, the misalignment between graph and text modalities undermines the effectiveness of knowledge fusion, ultimately impacting the task performance. To deal with the problems above, we propose a novel framework: Subgraph REtrieval Enhanced by GraPh-Text Alignment, named SEPTA. Firstly, we transform the knowledge graph into a database of subgraph vectors and propose a BFS-style subgraph sampling strategy to avoid information loss, leveraging the analogy between BFS and the message-passing mechanism. In addition, we propose a bidirectional contrastive learning approach for graph-text alignment, which effectively enhances both subgraph retrieval and knowledge fusion. Finally, all the retrieved information is combined for reasoning in the prediction module. Extensive experiments on five datasets demonstrate the effectiveness and robustness of our framework.","Commonsense question answering (CSQA) is a critical task in natural language understanding, which requires systems to acquire different types of commonsense knowledge and possess multi-hop reasoning ability [27, 19, 22]. Though massive pre-trained models have achieved impressive performance on this task, it is difficult to learn commonsense knowledge solely from the pre-training text corpus, as the commonsense knowledge is evident to humans and rarely expressed explicitly in natural language. Compared with unstructured text, structured data like knowledge graphs is much more efficient in representing commonsense [26]. The incorporation of external knowledge aids PLMs in comprehending question-answer (Q-A) pairs, while the entity relations enhance the model’s reasoning capabilities. Therefore, various commonsense knowledge graphs (CSKGs) (e.g., ConceptNet [25]) have been adopted in previous studies. Existing KG-augmented models for CSQA primarily adhere to a extracting-and-modeling paradigm [36, 26, 29, 32, 28, 35]. First, the knowledge subgraphs or paths related to a given question are extracted by string matching or semantic similarity, which indicate the relations between concepts or imply the process of multi-hop reasoning. Subsequently, diverse strategies emerge for the efficient representation and fusion of the extracted structural knowledge. One research path [12, 8] involves elaborately crafting graph neural networks for better modeling the extracted subgraphs, whereas another [34, 26] explores the efficient incorporation of knowledge from KG into language models by enhancing the interactions between PLMs and GNNs. Despite their success, these approaches still have several limitations. First, the subgraph’s quality suffers when retrieved through a simple string or semantic matching, posing limitations for subsequent operations. To obtain sufficient relevant knowledge, the number of nodes will expand dramatically with the increase of hop count, inevitably raising the burden of the model. Despite its ample size, certain crucial nodes might remain elusive, since some entities are not learned during the pre-training. Besides, the edges linked to the peripheral nodes within the subgraph are pruned, causing the message-passing mechanism of GNN to be blocked and impairing the attainment of effective representations, consequently undermining valuable information. Second, the misalignment between graph and text encoders presents a challenge for PLMs to internalize the knowledge contained in the acquired subgraph, especially in scenarios with limited data, leading to a reduced task performance [35]. Though Dragon [33] proposes a pre-training method to align GNNs and PLMs, it requires additional corpus, and the text-to-graph style to construct semantically equivalent graph-text pairs is challenging. The necessity for substantial computational resources poses another hurdle, prompting the search for a more efficient alignment method. In this paper, we propose a novel framework: Subgraph REtrieval Enhanced by GraPh-Text Alignment (SEPTA), for CSQA. To mitigate the shortcomings of the subgraph extraction process, we establish a database of subgraph vectors derived from the knowledge graph. Consequently, the challenge shifts from retrieving a pertinent subgraph to obtaining relevant subgraph vectors. A BFS-style sampling method is employed to obtain the connected graph for each node and the embedding of the subgraph is subsequently stored in the database. Drawing on the parallels between BFS and the message-passing mechanism of GNNs, the central node’s representation learned from the subgraph could be closely aligned with that derived from the entire graph, with almost no information loss. Besides, to further improve the retrieval accuracy and facilitate knowledge fusion during the prediction, we consider aligning the semantic space of the graph and text encoders, proposing an effective approach for graph-text alignment. A novel graph-to-text method is proposed to construct high-quality semantically equivalent training pairs, with no requirement of external corpus and easy to train. Finally, all the information retrieved is combined by a simple attention mechanism to facilitate the model in commonsense reasoning. Our contributions can be summarized as follows: • We propose a novel and effective framework SEPTA, where we convert the knowledge graph into a subgraph vector database and retrieve relevant subgraphs to facilitate commonsense reasoning. • We design a bidirectional contrastive learning method to align the semantic space of the graph and text encoders, with a graph-to-text method to construct high-quality graph-text pairs, which facilitates subgraph retrieval and knowledge fusion. • We propose a BFS-style subgraph sampling strategy for subgraph construction. Drawing on the parallel between BFS and the message-passing mechanism, our method can preserve complete neighbor information for each node. • We conduct extensive experiments on five datasets. Our proposed approach achieves better results than the state-of-the-art approaches and has promising performance in weakly supervised settings."
https://arxiv.org/html/2411.06239v1,Web Scale Graph Mining for Cyber Threat Intelligence,"Defending against today’s increasingly sophisticated and large-scale cyberattacks demands accurate, real-time threat intelligence. Traditional approaches struggle to scale, integrate diverse telemetry, and adapt to a constantly evolving security landscape. We introduce Threat Intelligence Tracking via Adaptive Networks (TITAN), an industry-scale graph mining framework that generates cyber threat intelligence at unprecedented speed and scale. TITAN introduces a suite of innovations specifically designed to address the complexities of the modern security landscape, including: (1) a dynamic threat intelligence graph that maps the intricate relationships between millions of entities, incidents, and organizations; (2) real-time update mechanisms that automatically decay and prune outdated intel; (3) integration of security domain knowledge to bootstrap initial reputation scores; and (4) reputation propagation algorithms that uncover hidden threat actor infrastructure. Integrated into Microsoft Unified Security Operations Platform (USOP), which is deployed across hundreds of thousands of organizations worldwide, TITAN’s threat intelligence powers key detection and disruption capabilities. With an impressive average macro-F1 score of 0.89 and a precision-recall AUC of 0.94, TITAN identifies millions of high-risk entities each week, enabling a 6x increase in non-file threat intelligence. Since its deployment, TITAN has increased the product’s incident disruption rate by a remarkable 21\%, while reducing the time to disrupt by a factor of 1.9x, and maintaining 99\% precision, as confirmed by customer feedback and thorough manual evaluation by security experts—ultimately saving customers from costly security breaches.","In today’s cybersecurity landscape, threat actors continuously evolve their techniques to infiltrate networks by leveraging a vast array of interconnected infrastructure. This has created an urgent demand for high-quality, real-time threat intelligence (TI). However, traditional TI approaches often struggle to scale, relying on manual investigation, signature matching, static analysis, and behavioral monitoring (Networks, 2024; Security, 2024; VirusTotal, 2024). These methods are further hindered by their siloed nature, lacking broader context across the entire enterprise security landscape, resulting in a fragmented view of threat actor infrastructure (Chau et al., 2011; Tamersoy et al., 2014). Unified security operation platforms platforms, such as Microsoft USOP, are uniquely positioned to break down these silos by acting as the centralized security hub. These platforms aim to enhance efficiency and effectiveness by correlating alerts across first and third party security products, such as endpoint, email, and identity, into cohesive security incidents (Einav, 2023; Freitas and Gharib, 2024). With TITAN, we advance Microsoft USOP threat intelligence capabilities by introducing a real-time, dynamic TI graph that captures the complex relationships between millions of entities, incidents, and organizations, providing a unified view of threat activity. By infusing this graph with security domain knowledge and leveraging a guilt-by-association framework (Koutra et al., 2011), we propagate reputation scores to unknown entities, enabling early detection and disruption (i.e., pre-damage mitigation) of threat actor infrastructure. Figure 1. Overview of the TITAN architecture: an industry-scale graph mining framework that generates real-time TI by propagating reputation scores across millions of interconnected entities, incidents, and organizations. Built on a time evolving 5-partite graph, the system operates through four key components: (1) dynamic graph construction and updates, (2) integration of known TI and security domain knowledge to bootstrap reputation scores for unknown entities; (3) reputation propagation to iteratively update risk scores; and (4) model calibration to probabilistically align scores for use by security analysts. Threat intelligence at scale. Generating scalable and accurate threat intelligence presents multiple unique and exciting challenges: (1) Evolving threat environment. Adversaries continually evolve their tactics and infrastructure, creating a rapidly shifting threat landscape. Generating up-to-date intelligence while identifying and pruning stale data is a substantial challenge. (2) Complex security landscape. The vast array of commercial security products, each with thousands of custom and built-in detection rules, creates an intricate and fragmented enterprise environment. Integrating diverse security telemetry into a unified TI framework requires careful application of domain knowledge to ensure accurate and meaningful insights. (3) Scalable and robust architecture. Modern security systems generate enormous volumes of alerts across interconnected domains such as network, cloud, endpoint, and email. Scaling to analyze millions of entities and terabytes of data in real time demands a robust, low-latency, and efficient architecture. The emergence of USOP as a relatively new industry underscores the timeliness of these challenges and positions scalable threat intelligence as a pivotal frontier in cybersecurity. Innovative solutions that drive real-time TI generation will be essential to safeguarding organizations against continuously evolving threats. 1.1. Contributions We introduce TITAN (Figure 1), a novel framework designed to address the challenges of generating high-fidelity threat intelligence at scale and in real time. Our framework makes significant contributions in the following areas: • TITAN architecture. TITAN transforms the cybersecurity industry’s approach to threat intelligence by introducing advanced methods for real-time, large-scale TI generation. Key innovations include: (1) dynamic k-partite graph that captures complex relationships between entities, incidents, and organizations; (2)the integration of security domain knowledge to bootstrap initial reputation scores; (3) reputation propagation algorithms to uncover hidden threat actor infrastructure; and (4) model calibration to probabilistic align reputation scores. We also disclose key architectural design elements and operational processes, setting a precedent as the first USOP cybersecurity company to openly discuss advanced TI capabilities in such comprehensive detail. • Extensive Evaluation. We conduct a comprehensive evaluation of TITAN’s performance across three key pillars: internal assessments, collaborations with security experts, and customer feedback. In internal testing on hundreds of thousands of held out entities, TITAN achieves an impressive average cross-region macro-F1 score of 0.89 and a precision-recall AUC of 0.94. • Impact to Microsoft Customers and Beyond. TITAN is integrated into Microsoft USOP, a market leader (Mellen et al., 2024), deployed across hundreds of thousands of organizations worldwide. Each week, TITAN identifies millions of high-risk entities, enabling a 6x increase in non-file threat intelligence. This research has transformed the product’s approach to detection and disruption, increasing the overall incident disruption rate by 21\% while reducing the time to disrupt by a factor of 1.9x—saving customers from costly breaches. Collaboration with Microsoft security research experts and feedback from customers further validates the effectiveness of our TI, demonstrating 99\% precision in attack disruption scenarios. Term Definition Alert Potential security threat that was detected Detector A security rule or ML model that generates alerts Entity File, IP, etc. evidence associated with an alert Correlation A link between two alerts based on a shared entity Incident Related alerts that are correlated together Organization Company containing a USOP product Disrupted Early threat mitigation (e.g., disable user) Reputation Likelihood of an entity being malicious or benign USOP Unified Security Operations Platforms (USOP) are used to protect organizations across the entire 1st and 3rd party enterprise landscape Table 1. Terminology and definitions."
https://arxiv.org/html/2411.06070v1,"GFT: Graph Foundation Model with 
Transferable Tree Vocabulary","Inspired by the success of foundation models in applications such as ChatGPT, as graph data has been ubiquitous, one can envision the far-reaching impacts that can be brought by Graph Foundation Models (GFMs) with broader applications in the areas such as scientific research, social network analysis, drug discovery, and e-commerce. Despite the significant progress of pre-trained graph neural networks, there haven’t been GFMs that can achieve desired performance on various graph-learning-related tasks. Building GFMs may rely on a vocabulary that encodes transferable patterns shared among different tasks and domains. Unlike image and text, defining such transferable patterns for graphs remains an open question. In this paper, we aim to bridge this gap by rethinking the transferable patterns on graphs as computation trees – i.e., tree structures derived from the message-passing process. Based on this insight, we propose a cross-task, cross-domain graph foundation model named GFT, short for Graph Foundation model with transferable Tree vocabulary. By treating computation trees as tokens within the transferable vocabulary, GFT improves model generalization and reduces the risk of negative transfer. The theoretical analyses and extensive experimental studies have demonstrated the transferability of computation trees and shown the effectiveness of GFT across diverse tasks and domains in graph learning. The open source code and data are available at https://github.com/Zehong-Wang/GFT.","Foundation models such as Large Language Models (LLMs) and Large Vision Models (LVMs) keep reshaping our view of the world [7, 100, 51, 112, 50]. These models are designed to be general-purpose, adaptable across various tasks and domains through fine-tuning or prompting, such as GPT-4 [1] in Natural Language Processing (NLP) and Sora [46] in Computer Vision (CV). Research attributes the success of foundation models to the uniformity of tasks and a general vocabulary that defines basic transferable patterns across tasks [98, 76, 112, 3, 50]. For example, LLMs [1, 112] treat language tasks as question-answering or next-word prediction and deconstruct sentences using a word vocabulary. Similarly, LVMs [100, 98, 3] reformulate image tasks as image question-answering, converting images into discrete tokens using a vision vocabulary. Inspired by the success of LLMs and LVMs, as graph-structured data (e.g., citation networks, social networks, computer networks, molecular structures, and recommender systems) have become ubiquitous, one can envision the far-reaching real-world impacts that can be brought by pre-trained Graph Foundation Models (GFMs). Although there has been significant progress of pre-trained Graph Neural Networks (GNNs), there haven’t been GFMs that can achieve desired performance on a wide range of graph-learning-related tasks. Unlike CV and NLP, as graphs represent complex, non-Euclidean relationships among entities [92, 48, 104, 58, 107], a grand challenge of building GFMs lies in identifying transferable patterns across graphs [50, 93, 25]. There have been extensive studies aiming to tackle this challenges, which can mainly be categorized into two groups: (1) Utilizing graphon theory: Ruiz et al. [62] provide theoretical evidence of transferability between two graphs generated from the same graphon. Cao et al. [8] further extend these findings by both empirically and theoretically analyzing graph transferability in pre-training and fine-tuning scenarios. Despite these theoretical guarantees, the stringent assumptions of graphon theory often prove difficult to satisfy in real-world, cross-domain datasets [42], thus limiting its applicability in defining transferable graph vocabularies. (2) Exploring graph transferability using subgraph structures [114, 59, 50]: Zhu et al. [114] demonstrate that the transferability between graphs is linked to the ego-graph patterns of individual nodes and establish a stability bound using the graph Laplacian. They suggest that localized subgraphs could serve as transferable patterns within graph vocabularies. Building on this finding, Sun et al. [68] develop a GFM by reformulating graph tasks as subgraph classification, enabling a single model to be applied to diverse tasks. Huang et al. [30], Liu et al. [45] expand GFMs to cross-domain scenarios by unifying the node feature space across different graphs through LLMs [60, 76]. Despite these successes, the process of explicit subgraph extraction remains time and memory intensive [30]. More importantly, numerous studies such as [20, 10, 53, 103] show that message-passing GNNs [40, 24, 21] fail to detect critical substructures or motifs within subgraphs, reducing the feasibility of using subgraphs to define graph vocabularies. How to identify a vocabulary that can encode transferable patterns shared among different tasks and domains for the construction of GFMs? In this paper, we aim to address the limitations of existing works by answering this question. Specifically, based on message-passing mechanism of GNNs, we have observed that the learned embeddings of each node can be essentially captured in the form of its computation tree. Based on this insight, we bridge the research gap by rethinking the transferable patterns on graphs as computation trees – i.e., subtree structures derived from the message-passing process. Using computation tree as a transferable pattern across graphs will bring three primary advantages: (1) Efficiency: As the extraction and encoding of computation trees are integrated within a single message-passing GNN process [20], it eliminates the need for the explicit subgraph extraction for GFMs [30, 45]. (2) Expressiveness: Since computation trees are capable of capturing localized patterns [52], it’s able to represent a graph as a multiset of computation trees [23]. (3) Learnability: As the information of computation trees is completely captured by message-passing GNNs, it can tackle the issue that certain motifs within subgraphs remain elusive. We theoretically investigate the transferability of computation trees and empirically demonstrate a strong correlation between computation tree similarity and transfer learning performance across various graphs. Based on the key idea above, by treating computation trees as graph vocabulary tokens, we develop a cross-task, cross-domain graph foundation model – namely GFT – short for Graph Foundation model with transferable Tree vocabulary. GFT consists of pre-training and fine-tuning phases, enabling it to handle datasets across different tasks and domains effectively. During pre-training, we introduce a computation tree reconstruction task to acquire generalized knowledge from cross-domain graphs. We obtain a discrete tree vocabulary of prototypical tree tokens by quantizing the embedding space of computation trees, which theoretically improves model generalization. In the fine-tuning phase, we utilize this learned tree vocabulary to unify various graph-related tasks into computation tree classification, thereby preventing negative transfer [89, 87]. Extensive experimental results demonstrate the effectiveness of GFT in graph learning on cross-task and cross-domain datasets."
https://arxiv.org/html/2411.05922v1,Bridging Nodes and Narrative Flows: Identifying Intervention Targets for Disinformation on Telegram,"In recent years, mass-broadcast messaging platforms like Telegram have gained prominence for both, serving as a harbor for private communication and enabling large-scale disinformation campaigns. The encrypted and networked nature of these platforms makes it challenging to identify intervention targets since most channels that promote misleading information are not originators of the message. In this work, we examine the structural mechanisms that facilitate the propagation of debunked misinformation on Telegram, focusing on the role of cross-community hubs—nodes that bridge otherwise isolated groups in amplifying misinformation. We introduce a multi-dimensional ‘bridging’ metric to quantify the influence of nodal Telegram channels, exploring their role in reshaping network topology during key geopolitical events. By analyzing over 1,740 Telegram channels and applying network analysis we uncover the small subset of nodes, and identify patterns that are emblematic of information ‘flows’ on this platform. Our findings provide insights into the structural vulnerabilities of distributed platforms, offering practical suggestions for interventions to mitigate networked disinformation flows.","In the past decade, private messaging platforms have emerged as powerful vehicles for information dissemination, fundamentally altering the landscape of digital communication through the introduction of anonymity (Walther and McCoy, 2021; Kireev et al., 2024). However, this transformation has brought with it unprecedented challenges, particularly in the realm of misinformation propagation. Telegram, with its encrypted channels and vast user base, has become a focal point for researchers and policymakers alike, as it represents a complex ecosystem where information—both accurate and misleading—can spread rapidly and with far-reaching consequences (Urman and Katz, 2022; Shehabat et al., 2017). The distributed111As a platform, Telegram operates on a centralized computing systems architecture but its channel-based structure decentralizes the information feed for users, and for the purpose of this research, this lack of a central information proliferation is what the term distributed will refer to.(Ng et al., 2024) nature of information proliferation on Telegram, characterized by interconnected channels and groups, creates an environment ripe for the formation of echo chambers and information silos(Törnberg, 2018; Baumgartner et al., 2020). Within this intricate network structure, certain nodes play a pivotal role in bridging disparate communities, acting as conduits for information flow across ideological and thematic boundaries. These ”bridge nodes” are gateways for both, the dissemination of reliable information and the amplification of misinformation (Urman et al., 2021). Past research has advanced our understanding of misinformation dynamics in social media platforms (Ng and Carley, 2022; Mehta et al., 2022; Cinelli et al., 2020). This includes content-based analyses (Fan et al., 2020) and network metrics to identify influential nodes and information flow patterns. Building on this foundation, our research examines the structure of private messaging ecosystems like Telegram, where the dynamics of information propagation may differ significantly from centralized ‘feed-based’ platforms, posing unique risks (Walther and McCoy, 2021). Our research takes a multidimensional approach in identifying and analyzing the role of critical nodes in Telegram’s network structure. We propose a ”bridge score” metric that aggregates measures across varied network characteristics to provide a comprehensive understanding of each node’s potential to act as an ‘information hub’. This approach builds upon traditional graph-centrality measures, offering new insights into the structural underpinnings of information flow in distributed messaging platforms."
