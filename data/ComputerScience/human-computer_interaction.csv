URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10234v1,"Generative AI in Multimodal User Interfaces: Trends, Challenges, and Cross-Platform Adaptability","As the boundaries of human-computer interaction expand, Generative AI emerges as a key driver in reshaping user interfaces (UIs), introducing new possibilities for personalized, multimodal, and cross-platform interactions. This integration reflects a growing demand for more adaptive and intuitive UIs that can accommodate diverse input types—text, voice, video—and deliver seamless experiences across devices. This paper explores the integration of Generative AI in modern UIs, examining historical developments and focusing on multimodal interaction, cross-platform adaptability, and dynamic personalization. A central theme is the “interface dilemma,” which addresses the challenge of designing effective interactions for multimodal large language models (LLMs), assessing the trade-offs between graphical, voice-based, and immersive interfaces. The paper further evaluates lightweight frameworks tailored for mobile platforms, spotlighting the role of mobile hardware in enabling scalable, multimodal AI. Technical and ethical challenges, including context retention, privacy concerns, and balancing cloud and on-device processing, are thoroughly examined. Finally, the paper outlines future directions, such as emotionally adaptive interfaces, predictive AI-driven UIs, and real-time collaborative systems, underscoring Generative AI’s potential to redefine adaptive, user-centric interfaces across platforms.","User interfaces have significantly evolved over the last few decades. From early text-based systems to modern graphical and multimodal interfaces, the developments in human-computer-driven interactions were strongly driven by advancements in software and hardware. This review examines how Generative AI, particularly multimodal large language models (LLMs) [1], is poised to further transform UI design by enabling dynamic personalization, context retention, and efficient scalability. It seems inevitable that with the advent of highly capable LLMs being accessible on every device, the way people interact with technology is going to adapt. This raises a few critical questions: What is the ideal interface for users to interact with AI-powered systems? Is there going to be a single trend or will interfaces adapt to each type of application specifically? How will the availability of immersive technologies like Virtual Reality (VR) glasses influence this? How long can it take people to adapt to a completely new interface experience? Some of the biggest technology companies have been experiencing with introducing new ways to interact with technology, but in the end, they converge to very similar styles and ideas. We will also address the limitations of current frameworks and the challenges of implementing AI in this domain, particularly when constrained by hardware like mobile phones. I-A Problem Statement: The Interface Dilemma Since the release of Chat GPT by Open AI, there has been a massive spike in interest in AI-driven applications. The chatbot-like interface, which was popularized by Chat GPT has quickly become the standard for human-AI interactions. Despite recent developments of multimodal LLMs, chat-based interactions are still the most popular ones, despite their limitations. As a result, most applications are limited to a very similar user interface and require high-quality user input to help the LLM understand the context or mood. One example of such applications can be voice assistants such as Alexa, Google Assistant, or Siri. Apple first integrated Siri into its ecosystem with the iPhone 4S in 2011. Since then, both the hardware and software it runs, as well as its capabilities, have improved massively. However, the way users interact with Siri has stayed exactly the same. Similarly, Google Assistant and Alexa, despite being competing products, offer exactly almost identical experiences when it comes to interacting with their device. We already possess powerful multimodal LLMs capable of processing text, images, and voice. However, the challenge lies in determining the best interface for human-computer interaction. Should it be console-based, GUI-driven, or even integrated into VR glasses? The core issue revolves around formulating an interface that is intuitive and leverages the full capabilities of AI-driven multimodal systems. I-B Objectives This review article aims to critically analyze and synthesize the state-of-the-art advancements in AI-driven user interface (UI) design, with a particular emphasis on optimizing interfaces for multimodal LLMs. The central focus is to explore how various forms of interaction, including text, voice, and video, can be seamlessly integrated into UIs to enhance human-computer interaction. Additionally, this article investigates the application of lightweight frameworks, particularly in the context of mobile devices, and assesses the potential of mobile phone hardware (voice, video, text) as a primary platform for scalable, efficient, and user-friendly AI interfaces. TABLE I: List of key acronyms used in the paper. Acronym Definition AI Artificial Intelligence AR Augmented Reality BCI Brain-Computer Interface CLI Command-Line Interface GUI Graphical User Interface IoT Internet of Things LLM Large Language Model NLP Natural Language Processing NPU Neural Processing Unit UI User Interface VR Virtual Reality TABLE II: Comparison of review articles about Generative AI in multimodal user interfaces. ✓, ✗, and ✱indicate that the topic is well-covered, uncovered, and partially covered, respectively. Reference Year Focus Multimodal Interaction Generative AI Cross-Platform Challenges & Trends Bieniek et al. (This Paper) 2024 A review of integrating Generative AI in modern computer UIs, focusing on multimodal interaction, cross-platform compatibility, and dynamic personalization ✓ ✓ ✓ ✓ Jones et al. [2] 2024 Examines how multimodal LLMs ground language in comparison to human interactions ✓ ✗ ✗ ✱ Munikoti et al. [3] 2024 Reviews architectures, challenges, and opportunities in multimodal AI, including transformer-based architectures for fusion ✓ ✓ ✱ ✓ Huang et al. [4] 2024 Focuses on adaptive user experience with Generative AI and dynamic personalization ✗ ✓ ✓ ✱ Kim et al. [5] 2021 Reviews multimodal interaction systems based on IoT and augmented reality ✓ ✗ ✓ ✓ Lu et al. [6] 2024 Reviews AI in user experience design, with an emphasis on human-centered AI applications ✗ ✓ ✓ ✗ Pyarelal et al. [7] 2018 Discusses automating the design of UIs using AI technologies ✱ ✓ ✱ ✱ Su et al. [8] 2023 Reviews advancements in multimodal human-robot interaction, with emphasis on combining voice, gestures, and facial recognition ✓ ✗ ✗ ✓ Zhang et al. [9] 2024 Evaluates the impact of multimodal interactions on user engagement in AI-driven conversations ✓ ✓ ✗ ✓ Bandi et al. [10] 2023 Reviews generative AI models, input-output formats, evaluation metrics, and technical challenges ✗ ✓ ✱ ✓ I-C Scope This article focuses on the intersection of AI and UI design, with particular attention to multimodal interaction with LLMs. The review encompasses the technological frameworks that enable AI-driven UI optimization, especially for mobile platforms, and evaluates lightweight approaches for integrating voice, video, and text modalities. The scope is limited to UIs that support multimodal interaction and dynamic personalization, excluding non-interactive or single-modal systems. Furthermore, the review assesses the feasibility of deploying such systems on mobile hardware, considering processing limitations, scalability challenges, and the need for efficient resource management. I-D Related Papers Table II provides a structured comparison of various review articles on generative AI, multimodal interfaces, and cross-platform adaptability. Each reference contributes uniquely to the body of research, highlighting different aspects of AI-driven user interfaces. For instance, Jones et al. [2] focus on the comparison between multimodal LLMs and human language grounding, providing valuable insights into how AI systems perceive and interpret multimodal inputs. However, their work lacks a detailed discussion on cross-platform adaptability and the role of generative AI in modern interfaces. In contrast, Munikoti et al. [3] delve deep into the architecture of multimodal AI systems, such as transformer-based models, and discuss their potential across different platforms. While their review thoroughly covers multimodal interaction, it only partially addresses the cross-platform challenges. Huang et al. [4] contribute by emphasizing adaptive user experiences through generative AI. Their review offers insights into how dynamic personalization can be achieved using generative AI but lacks an extensive exploration of multimodal systems. Similarly, Kim et al. (2021) focus on multimodal interaction systems in Internet of Things (IoT) and Augmented Reality (AR), emphasizing cross-platform challenges and offering a robust view of system interoperability but without a focus on generative AI [5]. On the other hand, Lu et al. [6] concentrate on human-centered AI in user experience design, which is highly relevant to AI-driven interfaces but does not deeply explore multimodal interaction. While some works, such as Su et al. [8], offer a comprehensive review of multimodal human-robot interaction, they miss out on the cross-platform adaptability required in broader AI-driven systems. Bandi et al. [10], though thorough in covering generative AI models and technical challenges, only partially address cross-platform issues and multimodal interaction. While these articles contribute significantly to the fields of generative AI and multimodal user interfaces, my attached paper offers a more comprehensive review by covering multimodal interaction, generative AI integration, cross-platform adaptability, and the emerging trends and challenges in AI-driven UIs. This distinguishes the paper as a broader, more inclusive review, bridging gaps not fully addressed by other works. I-E Paper Structure The structure of this paper is as follows: Section II covers the interface dilemma, highlighting challenges in designing intuitive multimodal LLM interfaces. Section III reviews the evolution of user interfaces, noting limitations of current AI-driven platforms. Section IV discusses application frameworks and AI integration, focusing on Generative AI’s role in cross-platform adaptability and mobile integration. Section V examines the development of new multimodal UIs enabled by Generative AI, addressing privacy, mobile processing efficiency, and context retention challenges. Section VI explores limitations and challenges in AI-driven interfaces and outlines future trends. Section VII identifies evaluation metrics for AI-driven multimodal UIs and Section VIII concludes the paper. Lastly, the list of key acronyms used in this paper is given in Table I."
https://arxiv.org/html/2411.10230v1,Exploring Augmented Table Setup and Lighting Customization in a Simulated Restaurant to Improve the User Experience,"This study explored a concept for using Augmented Reality (AR) glasses to customize augmented table setup and lighting in a restaurant. The aim was to provide insights into AR usage in restaurants and contribute to existing research by introducing an extendable and versatile concept for scholars and restaurateurs. A controlled laboratory study, using a within-subjects design, was conducted to investigate the effects of a customizable augmented table setup and lighting on user experience (UX), perceived waiting time, psychological ownership and social acceptability. A simulated restaurant environment was created using 360∘ image in Virtual Reality (VR). The study implemented default and customizable table setup and lighting. Results from a paired samples t-test showed a statistically significant effect of table setup and lighting on pragmatic quality of UX, hedonic quality of UX, overall UX, Valence, Dominance, psychological ownership and affect. Furthermore, table setup had a significant effect on Arousal and perceived waiting time. Moreover, table setup significantly affected AR Interaction-, Isolation-, and Safety acceptability, while lighting only affected AR Interaction acceptability. Findings suggest that these investigated variables are worth considering for AR applications in a restaurant, especially when offering customizable augmented table setup and lighting.","Augmented Reality (AR) and Virtual Reality (VR) technologies are rapidly growing and are expected to reach 3.728 million people worldwide by 2029 [1]. While immersive technologies were adopted across various fields such as e-commerce and gaming [1], their use in restaurants is still uncommon. Present use cases include immersive experiences [2, 3, 4, 5], interactive menus, and 3D representation of dishes [6, 7, 8] to enhance the dining experience. However, there is an additional application of AR in the restaurant context: allowing customers to personalize aspects of their environment. This study delves into the potential of AR to enable positive emotional states and hedonic benefits through a customizable augmented table setup and lighting in a simulated restaurant environment in VR. Prior research has successfully utilized VR to simulate AR across various contexts, showing minimal differences between real-world and simulated conditions [9, 10, 11]. Also, studies showed that the physical environment in restaurants and its interior design are important factors for customer experience [12, 13], customer emotion [13, 14, 15, 16], and customer behavior [17, 18, 14, 15, 19, 16]. The relevance of the table setup and lighting in a restaurant is given by them being an element of the restaurant’s interior and environment. In this study, table setup refers to the virtual objects on the dining table, while lighting refers to the illumination of the simulated restaurant environment. Examining the social acceptability of new emerging technologies such as AR glasses in public use cases is essential to determine their future success and adoption rate [20, 21, 22, 23, 24]. Social acceptability can be defined as the phenomenon of judging a technology introduced in the future from the comfort or discomfort of both the user’s and the observer’s perspective. It can be evaluated through various dimensions including Public VR, User, Public Communication, VR Interaction, Isolation, Privacy, and Safety [25, 20]. This work provides insights into the social acceptability of AR glasses in a restaurant context and investigates psychological ownership toward augmented table setup and lighting. Psychological ownership can be defined as “the state of the mind in which individuals feel as though the object or target of ownership is theirs” [26]. It is related to relevant variables for restaurants such as customer-company identification, word-of-mouth, and willingness to pay more [27]. Relevant research on psychological ownership in AR can be found in [26, 28]. Waiting is also an intrinsic part of the restaurant experience. Investigating the perceived waiting time is relevant, given its influence on customers’ evaluation of services [29]. Additionally, studies have demonstrated that the environment in waiting situations plays a crucial role in customers’ perceptions and behavior [30, 15]. If customizing elements of the restaurant’s interior contributes to creating a favorable environment, it could prevent or mitigate a negative waiting experience [15], which, in turn, would be reflected in the perceived waiting time. In this study, perceived waiting time refers to the subjective time spent in the simulated restaurant environment waiting for the order, covering the in-process phase, from ordering an appetizer until its delivery [29, 31, 32, 33]. The concept of this work centers around customers using AR glasses inside a restaurant to customize virtual objects on their dining table and the lighting. In the concept, the restaurant is a physical environment consisting of interior elements like the table setup and lighting. The table setup consists of decorative elements like flowers and a table light. Customers can customize table setup elements in terms of type (and color) and the lighting in terms of intensity and color. Central to this concept are the customers who can benefit from the customization, whether dining alone, in pairs, or groups. This study investigates the solo experience, shown in black in Fig. 1. A shared experience and an exemplary extension of the table setup are implied in gray. This study draws from Obrist’s approach [34], which suggests that multisensory experiences are impressions formed by events whose sensory characteristics were crafted intentionally. Figure 1: Concept Model"
https://arxiv.org/html/2411.10192v1,Tangi: a Tool to Create Tangible Artifacts for Sharing Insights from 360° Video.,"Designers often engage with video to gain rich, temporal insights about the context of users, collaboratively analyzing it to gather ideas, challenge assumptions, and foster empathy. To capture the full visual context of users and their situations, designers are adopting 360° video, providing richer, more multi-layered insights. Unfortunately, the spherical nature of 360° video means designers cannot create tangible video artifacts such as storyboards for collaborative analysis. To overcome this limitation, we created Tangi, a web-based tool that converts 360° images into tangible 360° video artifacts, that enable designers to embody and share their insights. Our evaluation with nine experienced designers demonstrates that the artifacts Tangi creates enable tangible interactions found in collaborative workshops and introduce two new capabilities: spatial orientation within 360° environments and linking specific details to the broader 360° context. Since Tangi is an open-source tool, designers can immediately leverage 360° video in collaborative workshops.","To better understand the needs and wants of potential users, designers111Someone engaged in the processes of (re)designing a product or service, regardless of profession or title. engage in Contextual Inquiry, gaining insights into the context around the user and the user themselves (Beyer and Holtzblatt, 1999). One method for gathering information about a context is the use of video, which enables prolonged and unobtrusive observation of a context (Ylirisku and Buur, 2007, p. 19) or observation of contexts that are difficult or dangerous to observe in person – for example, logging equipment operators (Sitompul and Wallmyr, 2019) or emergency medical services (Schlosser and Matthews, 2022). \Description A few cars are on a street in Paris. (a) A subsection of a 360° video cropped to match the FoV (Field of View) of conventional video. \Description A distorted picture of someone biking around the street, much more visual information is shown. (b) The same frame of video using a modified Little Planet projection (Nguyen et al., 2017). Figure 2. The difference in visual information of a frame when using conventional video and 360° video for an exemplar use case of studying cycling behavior, similar to Porcheron et al. (2023). Illustrating that 360° images, and videos, (2b) contain significantly more contextual information, at the disadvantage of being significantly more distorted than conventional video. Video from Velo Mondial - CC-BY. The process of designers engaging with video as user research material is referred to as Video Design Ethnography (VDE) (Ylirisku and Buur, 2007), this iterative process centers around designers viewing and annotating videos individually and then engaging in sense-making to align their understanding of user needs and in turn design goals. A crucial component that supports this collaborative sense-making are “video artifacts”222Not to be confused with compression artifacts, this term has been used by Ylirisku and Buur (2007) to describe similar boundary objects made from video. – tangible representations of designers’ insights, such as storyboards or clusters of screenshots (Section 2.2). Designers use the artifacts to represent insights during discussions, documentation, VDE (Buur et al., 2010; Ylirisku and Buur, 2007; Nova, 2014; Lucero et al., 2016) and as the output of the process (Nova, 2014). The increasing ubiquity of 360° cameras has the potential to provide designers with richer and more immersive insights (Kramer, 2022). With a Field of View (FoV) of 360°, these cameras capture their entire visual context, solving issues with framing (Jokela et al., 2019; Tojo et al., 2021) and enabling viewers to understand more complex interactions – such as how a cyclist reacts to events in front or behind them (Porcheron et al., 2023) or the interaction between the conductor and their orchestra (Vatanen et al., 2022). Designers are able to use this additional visual context to gather richer insights (Meijer et al., 2024) into the context of their users (See Figure 2). Unfortunately, as seen in Figures 1a and 2b the spherical nature of 360° video makes it challenging to view and share (Jokela et al., 2019) using tools designed around conventional video, such as monitors and video artifacts (Section 2.5). To work with 360° video designers need to either discard most of the visual information – converting it back to conventional video (Figure 2a) or suffer from a heavily distorted image (Figure 2b). Thus, in order to take advantage of the benefits of 360° video, it is necessary to create tangible artifacts that enable the kinds of interactions offered by conventional video artifacts (Meijer et al., 2024). In this paper we discuss Tangi, a web-based tool for creating tangible artifacts from 360° video frames, in order to support 360° Video Design Ethnography (Section 4). To understand the utility of Tangi and the artifacts it creates, we conducted reflection sessions with experienced designers (Section 5). These sessions demonstrated that the artifacts Tangi produces enable tangible interactions that Buur et al. (2000) describe as essential to collaborative video analysis. Additionally, participants were able to easily modify and create new artifacts using the base elements provided by Tangi, showing the flexibility of paper-based artifacts to evolve to meet the needs of diverse design tasks (Dalsgaard et al., 2014). Finally, we discuss the implications of Tangible 360° Video Artifacts, limitations of this early work, and future steps to further understand how 360° artifacts evolve over a longer design process (Section 7). To summarize, this paper’s key contributions are: (1) Tangi - an open source tool to quickly create Tangible 360° Video Artifacts. (2) Demonstrating the utility of these artifacts to support collaborative sense-making. (3) Examples of more complex artifacts that show the ability of paper-based 360° video artifacts to adapt to the needs of specific design teams."
https://arxiv.org/html/2411.09997v1,DBenVis: A Visual Analytics System for Comparing DBMS Performance via Benchmark Programs,"Database benchmarking is an essential method for evaluating and comparing the performance characteristics of a database management system (DBMS). It helps researchers and developers to evaluate the efficacy of their optimizations or newly developed DBMS solutions. Also, companies can benefit by analyzing the performance of DBMS under specific workloads and leveraging the result to select the most suitable system for their needs. The proper interpretation of raw benchmark results requires effective visualization, which helps users gain meaningful insights. However, visualization of the results requires prior knowledge, and existing approaches often involve time-consuming manual tasks. This is due to the absence of a unified visual analytics system for benchmark results across diverse DBMSs. To address these challenges, we present DBenVis, an interactive visual analytics system that provides efficient and versatile benchmark results visualization. DBenVis is designed to support both online transaction processing (OLTP) and online analytic processing (OLAP) benchmarks. DBenVis provides an interactive comparison view, which enables users to perform an in-depth analysis of performance characteristics across various metrics among different DBMSs. Notably, we devise an interactive visual encoding idiom for the OLAP benchmark to represent a query execution plan as a tree. In the process of building a system, we propose novel techniques for parsing meaningful data from raw benchmark results and converting the query plan to a D3 hierarchical format. Through case studies conducted with domain experts, we demonstrate the efficacy and usability of DBenVis.","1 Related Work We introduce the research areas related to DBenVis, primarily focusing on two domains: benchmark result visualization and query plan visualization. Our work gets inspiration from existing studies and attempts to overcome some of the limitations in these areas. 1.1 Benchmark Result Visualization Benchmark result visualization is a key aspect in the analysis of database performance. Several popular libraries are used in the visualization process, each with unique features and requirements. Matplotlib [6] is among the most popular Python libraries aimed at creating interactive visualizations. D3 [4] is one of the most famous web-based JavaScript libraries for visualization. Numerous other visualization libraries exist, including Seaborn [10] and FusionCharts. Although these libraries serve as a powerful tool for visualization, the challenge is that they require users to preprocess the raw benchmark results. This requires users to have prior knowledge of result data and the library, making it time-consuming. HammerDB [1] is an open-source database benchmarking tool that allows loading and evaluating DBMS performance using TPC benchmarks and facilitates result visualization. However, its utility is constrained since it supports only a limited range of benchmarks and database systems. These limitations highlight the need for more versatile and user-friendly tools in database benchmark visualization. 1.2 Query Plan Visualization The query planner in DBMS generates possible plans for submitted queries and examines each of these possible execution plans, ultimately selecting the execution plan that is expected to run the fastest. The selection of the right plan to match the query structure and the properties of the data is critical for good performance, so the system includes a complex planner that tries to choose the good plan. EXPLAIN command is used to see what query plan is chosen. The query plan generated from EXPLAIN structure follows a tree format, with scan nodes at the bottom level of the tree. For queries that require joining, aggregation, sorting, or other operations, additional nodes are present to execute these operations. There are only a few query plan visualizers available. The PostgreSQL Explain Visualizer (PEV) [3] provides an interactive interface for visualizing query plans in a tree format. Users can engage with the visualization through clicking, zooming, and panning interactions. Although not as comprehensive as PEV, MySQL does provide its own visualization tool for explaining plans. This tool facilitates a basic understanding of query execution plans within the MySQL environment. Additionally, as far as we surveyed, no query plan visualizer is available for MariaDB. This notable absence highlights the need for a unified visualizer to compare and analyze query plans across various DBMSs."
https://arxiv.org/html/2411.09969v1,Steering AI-Driven Personalization of Scientific Text for General Audiences,"Digital media platforms (e.g., social media, science blogs) offer opportunities to communicate scientific content to general audiences at scale. However, these audiences vary in their scientific expertise, literacy levels, and personal backgrounds, making effective science communication challenging. To address this challenge, we designed TranSlider, an AI-powered tool that generates personalized translations of scientific text based on individual user profiles (e.g., hobbies, location, and education). Our tool features an interactive slider that allows users to steer the degree of personalization from 0 (weakly relatable) to 100 (strongly relatable), leveraging LLMs to generate the translations with given degrees. Through an exploratory study with 15 participants, we investigated both the utility of these AI-personalized translations and how interactive reading features influenced users’ understanding and reading experiences. We found that participants who preferred higher degrees of personalization appreciated the relatable and contextual translations, while those who preferred lower degrees valued concise translations with subtle contextualization. Furthermore, participants reported the compounding effect of multiple translations on their understanding of scientific content. Given these findings, we discuss several implications of AI-personalized translation tools in facilitating communication in collaborative contexts.","Science communication refers to the process of “communicating complex scientific information with general audiences to improve public awareness, interest, and understanding of science” (Burns et al., 2003). Both academic and industry professionals strive to make their ideas more accessible. As a result, science communication platforms, such as science blogs (Hogan, 2024) and science magazines (Times, 2024), have been translating scientific information into more accessible formats for decades. However, a single version of a text often falls short of meeting the diverse needs of a general audience (August et al., 2024). For instance, a journalist with an interest in quantum physics would have vastly different context and comprehension needs than a college student majoring in physics. Scientific research articles often employ domain-specific language, including jargon and complex sentence structures, which can pose significant barriers to comprehension for general audiences. Large language models (LLMs) offer the capability to personalize content to various user contexts, transforming both the style (e.g., casual to formal (Das et al., 2023)) and content (e.g., simplifying complex ideas (August et al., 2024)). This adaptive personalization creates opportunities for enhancing audience engagement and understanding through more effective content dissemination (Kim et al., 2024). For example, LLMs can tailor the tone and style of complex concepts like scientific information to align with individual language preferences, thereby improving readability and comprehension across various fields (August et al., 2023; Das et al., 2023; Ding et al., 2023). Notably, AI can achieve this personalization at scale (i.e., AI-scalable personalization), generating multiple tailored variations of content for diverse audiences (August et al., 2024; Riedl, 2010; Kim et al., 2024). This scalability positions LLMs as powerful tools in advancing inclusive and effective science communication (Razack et al., 2021). Among the many strategies in science communication, analogies have proven effective in translating technical content into more accessible forms (August et al., 2020). For instance, the structure of the solar system is often used as an analogy to explain the structure of an atom. While such analogies can broaden understanding of scientific information, they are typically generalized and may not resonate equally well with all audiences, as individual comprehension is shaped by societal, cultural, educational, and personal backgrounds. For example, the space analogy assumes familiarity with the solar system (Kim et al., 2024). LLMs have demonstrated the ability to generate personalized analogies that can help people understand complex concepts and ideas within their own context (Ding et al., 2023). Building on this capability, we leverage LLMs to generate multiple personalized analogies tailored to individual readers’ contexts and comprehension levels. We designed and implemented TranSlider (Translate through Slider), an interactive reading interface that enables users to steer the degree of personalization in scientific text through an adjustable slider. TranSlider allows the user to specify a degree of personalization from 0 to 100 and utilizes their background information (e.g., education, hobbies, location) to present relevant analogies. The slider enables intuitive exploration of various personalization degrees, allowing users to quickly review multiple translations. We employed TranSlider as a research probe to ask the following research questions: RQ1: What is the utility of AI-driven personalized translations of scientific text? RQ2: What is the impact of interactive reading features on user experience? Specifically: 2.1: How does exploring multiple translations influence readers’ comprehension and engagement with scientific text? 2.2: How does the slider interaction to steer the degree of personalization influence readers’ comprehension and engagement with scientific text? To answer these questions, we conducted a user study with 15 non-expert participants who used the tool to understand two scientific texts by exploring multiple personalized translations. We conducted post-session semi-structured interviews to elicit feedback. Participants generally found the analogies useful in understanding the content. Some favored general analogies (e.g., a participant liked a construction analogy describing the body’s cells as building sites—low personalization), while others preferred detailed, personalized analogies (e.g., a participant appreciated a baking analogy to explain harmful emissions from lithium-ion batteries—high personalization). Participants noted that reading multiple translations with varied analogies allowed them to piece together a fuller understanding of the scientific text, correcting misunderstandings along the way. Overall, users found the tool beneficial for learning about unfamiliar topics but were cautious about the reliability of AI-generated content. Based on these findings, we discuss the need for HCI techniques to steer models towards human pluralistic preferences, rather than solely relying on machine learning methods that assume uniformity in human experiences. In summary, our study has the following three contributions: (1) A novel slider-based interaction to enhance understanding of scientific content through the exploration of analogy-driven personalized translations (2) An investigation to examine the utility, benefits, and limitations of personalized translations in science communication (3) Implications for designing such AI-personalized translation tools and broader applications in collaborative and education contexts such as facilitating cross-disciplinary communication"
https://arxiv.org/html/2411.09879v1,A Multi-Label EEG Dataset for Mental Attention State Classification in Online Learning,"Attention is a vital cognitive process in the learning and memory environment, particularly in the context of online learning. Traditional methods for classifying attention states of online learners based on behavioral signals are prone to distortion, leading to increased interest in using electroencephalography (EEG) signals for authentic and accurate assessment. However, the field of attention state classification based on EEG signals in online learning faces challenges, including the scarcity of publicly available datasets, the lack of standardized data collection paradigms, and the requirement to consider the interplay between attention and other psychological states. In light of this, we present the Multi-label EEG dataset for classifying Mental Attention states (MEMA) in online learning. We meticulously designed a reliable and standard experimental paradigm with three attention states: neutral, relaxing, and concentrating, considering human physiological and psychological characteristics. This paradigm collected EEG signals from 20 subjects, each participating in 12 trials, resulting in 1,060 minutes of data. Emotional state labels, basic personal information, and personality traits were also collected to investigate the relationship between attention and other psychological states. Extensive quantitative and qualitative analysis, including a multi-label correlation study, validated the quality of the EEG attention data. The MEMA dataset and analysis provide valuable insights for advancing research on attention in online learning. The dataset is publicly available at https://github.com/GuanjianLiu/MEMA.","Attention refers to the ability to focus on task-related information stimuli for a relatively long period of time while consciously attempting to ignore other stimuli [1]. It is essentially governed by three networks in the brain: alertness, orientation, and execution [2]. Attention is closely related to a learner’s education quality as a crucial cognitive process in the learning and memory environment. With the rise of online learning, the online learning environment, characterized by a massive number of learners online simultaneously and a lack of supervisors, contrasts sharply with traditional offline learning scenarios. This has made the need for automated assessment of learners’ states more urgent and has also drawn increased research focus to this area. Traditional methods for classifying the attention states of online learners are based on behavioral signals observed in online videos, including speech, facial expressions, gestures, and body movements [3, 4]. Although these methods can somewhat reflect the learners’ attention states, they are easily influenced by their subjective actions, allowing for the possibility of deliberately deceiving the classification algorithms through active disguises. On the contrary, the classification of online learners’ attention states based on electroencephalography (EEG), a physiological signal, has garnered widespread attention due to its authenticity. Meanwhile, EEG has a high temporal resolution by directly measuring brain activity and has been proven to accurately reflect changes in attention states [5]. With the increasing portability and affordability of EEG devices, the prospects of employing EEG to monitor learners’ attention in online learning scenarios are becoming increasingly promising. Figure 1: Overall collection procedure consists of two parts: preparation before recording and EEG data recording. Despite some progress, the field of EEG-based attention state classification in online learning still faces several challenges. Firstly, a significant drawback is the scarcity of publicly available datasets, and those that are available often suffer from inadequate data quality validation. This deficiency severely hampers the reproducibility and comparability of research in the field [6, 7]. Secondly, the lack of standardized or uniform data collection paradigms and arbitrary processes further compounds the challenge. Various stages lack the incorporation of pertinent psychological and physiological knowledge, such as the selection of collection times, the design of collection processes, and the formulation of experimental tasks [8, 9]. Consequently, adhering to a consistent collection paradigm becomes difficult. Thirdly, research about learners’ attention is often overly simplistic and idealized, leading to limited data, labels, and gathered information. It is worth noting that attention is closely intertwined with other psychological states, such as emotional states [10]. The above discussion indicates that this field is still hungry for fundamental data construction research. In light of this, we propose a Multi-label EEG dataset for classifying Mental Attention states (MEMA) in the context of online learning. Specifically, we have meticulously designed a reliable paradigm with three distinct attention states: neutral, relaxing, and concentrating. The details of the paradigm, such as task and duration for each state, have been carefully established, considering human physiological and psychological characteristics to ensure standardization and rationality throughout. Employing this paradigm, we gather EEG signals from a cohort of 20 subjects, each participating in 12 trials, resulting in a total of 1,060 minutes of data. Additionally, to investigate the relationship between attention and other psychological states, we collect emotional state labels [11, 12], basic personal information, and subjects’ personality traits. Furthermore, we conducted extensive quantitative and qualitative analysis involving a multi-label correlation study to validate the quality of the EEG attention data. The contributions of this study are as follows: • We have made a fundamental research achievement by constructing a multi-label EEG dataset for classifying mental attention states in the context of online learning. • We meticulously designed a reliable and standardized paradigm tailored for attention, integrating physiological and psychological principles, acquiring EEG signals from 20 subjects for more than 1,000 minutes. • We validated the quality of the EEG attention data through extensive quantitative and qualitative analysis, including a multi-label correlation study. The baseline algorithms achieved accuracy rates of up to 85.12% and 64.84% for subject-dependent and cross-subject attention state classification tasks, respectively."
https://arxiv.org/html/2411.09875v1,EEG Spectral Analysis in Gray Zone Between Healthy and Insomnia,"This study investigates the sleep characteristics and brain activity of individuals in the gray zone of insomnia, a population that experiences sleep disturbances yet does not fully meet the clinical criteria for chronic insomnia. Thirteen healthy participants and thirteen individuals from the gray zone were assessed using polysomnography and electroencephalogram to analyze both sleep architecture and neural activity. Although no significant differences in objective sleep quality or structure were found between the groups, gray zone individuals reported higher insomnia severity index scores, indicating subjective sleep difficulties. Electroencephalogram analysis revealed increased delta and alpha activity during the wake stage, suggesting lingering sleep inertia, while non-rapid eye movement stages 1 and 2 exhibited elevated beta and gamma activity, often associated with chronic insomnia. However, these high-frequency patterns were not observed in non-rapid eye movement stage 3 or rapid eye movement sleep, suggesting less severe disruptions compared to chronic insomnia. This study emphasizes that despite normal polysomnography findings, EEG patterns in gray zone individuals suggest a potential risk for chronic insomnia, highlighting the need for early identification and tailored intervention strategies to prevent progression.","I INTRODUCTION Insomnia is a prevalent sleep disorder that can significantly impair various aspects of an individual’s life. The condition has been associated with a range of detrimental effects, including difficulties in emotional regulation, decreased cognitive performance, and increased risk of mood disorders such as anxiety and depression [1]. Research indicates that individuals suffering from insomnia often experience heightened emotional reactivity and impaired coping mechanisms, leading to a diminished quality of life. Given the growing prevalence of insomnia and its associated repercussions, understanding the underlying mechanisms of this disorder is of paramount importance. By elucidating the emotional and physiological characteristics of insomnia, researchers can better address its impact and develop effective interventions for those affected. In recent years, research has increasingly focused on the relationship between sleep disorders like insomnia and brain function. Given the central role the brain plays in regulating sleep, brain-computer interface (BCI) technologies have emerged as a promising area of study in understanding and treating insomnia [2, 3]. BCI research investigates how neural activity correlates with sleep disturbances, aiming to create systems that can monitor and potentially modulate brain states to improve sleep quality [4]. This approach is particularly relevant for identifying biomarkers that distinguish different types of insomnia and could lead to personalized interventions. Despite the advancements in understanding insomnia, the majority of existing research has predominantly focused on individuals with chronic insomnia. This narrow scope presents several limitations, particularly the significant demographic differences between control and patient groups, which may confound the interpretation of findings [5]. Furthermore, there is a notable paucity of studies investigating individuals who experience insomnia-like symptoms yet do not meet the criteria for chronic insomnia. These individuals may endure disturbances in sleep patterns and daytime functioning but are often overlooked in both clinical and research contexts. In light of these gaps in the literature, the present study aims to explore the characteristics of a gray zone group—individuals who do not qualify as chronic insomnia patients but report experiencing discomfort in daily functioning due to sleep disturbances. This gray zone group may represent a transitional state, placing individuals at increased risk of developing chronic insomnia. By analyzing this unique cohort, the current research seeks to identify specific features that distinguish them from both healthy individuals and those with chronic insomnia. Ultimately, this study aspires to lay the groundwork for targeted interventions and care strategies for this at-risk population, thereby potentially mitigating the progression to chronic insomnia and enhancing overall sleep health. Figure 1: Experimental setting. a) Experimental procedure. Q=questionnaire, b) Experimental environment. c) Segmentation of brain regions. The brain region is divided into three groups: F=frontal, C=central, and O=occipital."
https://arxiv.org/html/2411.09873v1,LLM-Powered AI Tutors with Personas for d/Deaf and Hard-of-Hearing Online Learners,"Intelligent tutoring systems (ITS) using artificial intelligence (AI) technology have shown promise in supporting learners with diverse abilities; however, they often fail to meet the specific communication needs and cultural nuances needed by d/Deaf and Hard-of-Hearing (DHH) learners. As large language models (LLMs) provide new opportunities to incorporate personas to AI-based tutors and support dynamic interactive dialogue, this paper explores how DHH learners perceive LLM-powered ITS with different personas and identified design suggestions for improving the interaction. We developed an interface that allows DHH learners to interact with ChatGPT and three LLM-powered AI tutors with different experiences in DHH education while the learners watch an educational video. A user study with 16 DHH participants showed that they perceived conversations with the AI tutors who had DHH education experiences to be more human-like and trustworthy due to the tutors’ cultural knowledge of DHH communities. Participants also suggested providing more transparency regarding the tutors’ background information to clarify each AI tutor’s position within the DHH community. We discuss design implications for more inclusive LLM-based systems, such as supports for the multimodality of sign language.","In the United States, about two per 1,000 children have some level of hearing loss in one or both ears (Gaffney et al., 2010), and one out of 8 people (13%) aged 12 or above has hearing loss in both ears (Lin et al., 2011). As one of the diverse ability populations, there are educational technologies designed to support d/Deaf and Hard-of-Hearing (DHH) learners to address their accessibility needs. Various AI technology has been explored to support DHH learners through automated caption generation (Butler et al., 2019; Kushalnagar et al., 2012; Lasecki et al., 2014), emotional experience (Chen et al., 2024e), and sign language translation (Papastratis et al., 2021; Bragg et al., 2019). Over the years, intelligent tutoring systems (ITS) using artificial intelligence (AI) technology has shown promise in education to create a personalized and adaptive learning experience (Carbonell, 1970) as well as supporting learners with diverse abilities (Michaud et al., 2000). AI-based tutors (Graesser et al., 2001; Rus et al., 2013), for example, allows automated tutoring through conversation dialogue, where the AI serves as tutors. AI tutors were also developed to support DHH individuals learning written language (e.g., English) (Michaud et al., 2000) and sign language (e.g., ASL - American Sign Language) (Xu et al., 2007; Ritchings et al., 2012). With the rapid growth of large language models (LLMs) in recent years, AI tutors saw new opportunities in providing dynamic interactive dialogue that combines the knowledge from LLMs and course materials for a more personalized learning experience (Schmucker et al., 2024; Kakar et al., 2024b). LLMs also enabled incorporating personas, fictional characters that represent a specific group of people (Pruitt and Grudin, 2003), to AI tutors, which shows promise in improving students’ interaction experience and educational performance (Ruan et al., 2019). However, there is a general lack of attention on how LLM-powered ITS may or may not address specific needs from DHH learners (Chen et al., 2024d). While LLMs brought new opportunities to support DHH learners, it also brought risks such as biased training data and flawed AI models (Desai et al., 2024). In addition, while there are works in designing personas for DHH individuals, to our knowledge, there is limited work on how LLM-powered persona may be incorporated in AI tutors to support DHH learners. Additionally, there is limited understanding on the specific needs that may or may not be addressed by current LLMs. To address the research gaps and understand the interaction between DHH learners and LLM-powered ITS in online learning, we evaluated a prototyped online learning tool for DHH learners that allows interacting with ChatGPT as well as three AI tutors with LLM-powered personas while watching online educational videos. The three AI tutors had different backgrounds in educating DHH learners: Jamie, an AI tutor who was from Gallaudet University, a DHH-centric university; Casey, an AI tutor who is a DHH access specialist; and Alex, an AI tutor. Both Jamie and Casey had experience educating DHH students with different levels of knowledge toward DHH learners’ preferences, whereas Alex had experience educating hearing students. To simulate the knowledge in DHH education experiences, we incorporated comments collected from DHH and hearing students and preferences of DHH learners into the prompt design of each AI tutor. Through a user study with 16 DHH learners, we answer the following research questions: • RQ1: How do DHH learners perceive the interaction with AI tutors with LLM-powered personas using experience in DHH education as background information? • RQ2: What are the design suggestions from DHH learners to improve their interaction with LLM-powered AI tutors? Our paper makes the following contribution to the CSCW and HCI communities. 1) We found the perception of DHH learners toward LLM-powered AI tutors with personas. For example, DHH learners perceived AI tutors with experience in DHH education to be more trustworthy and human-like. 2) We identified the diverse needs that were not addressed by current LLMs. For example, the multimodality of sign language was yet to be supported by LLMs. 3) We discuss the ethical and design implications for more inclusive LLMs. For example, we highlighted the importance of credible and detailed ability-related background information regarding the persona within a diverse ability population as well as potential biases raised from ability-related persona design."
https://arxiv.org/html/2411.09788v1,"AI-Driven Human-Autonomy Teaming in Tactical Operations: Proposed Framework, Challenges, and Future Directions","Artificial Intelligence (AI) techniques, particularly machine learning techniques, are rapidly transforming tactical operations by augmenting human decision-making capabilities. This paper explores AI-driven Human-Autonomy Teaming (HAT) as a transformative approach, focusing on how it empowers human decision-making in complex environments. While trust and explainability continue to pose significant challenges, our exploration focuses on the potential of AI-driven HAT to transform tactical operations. By improving situational awareness and supporting more informed decision-making, AI-driven HAT can enhance the effectiveness and safety of such operations. To this end, we propose a comprehensive framework that addresses the key components of AI-driven HAT, including trust and transparency, optimal function allocation between humans and AI, situational awareness, and ethical considerations. The proposed framework can serve as a foundation for future research and development in the field. By identifying and discussing critical research challenges and knowledge gaps in this framework, our work aims to guide the advancement of AI-driven HAT for optimizing tactical operations. We emphasize the importance of developing scalable and ethical AI-driven HAT systems that ensure seamless human-machine collaboration, prioritize ethical considerations, enhance model transparency through Explainable AI (XAI) techniques, and effectively manage the cognitive load of human operators.","The convergence of AI and autonomous technologies has revolutionized various industries, including defense and tactical operations. The rise of HAT can be attributed to several factors, including rapid advancements in autonomous technologies and AI [1], the increasing complexity of tasks and environments, the development of more capable autonomous systems, and the increasing availability of data and computing power [2]. As these technologies have become more sophisticated and capable, there has been a growing recognition of the potential collaborations that can be achieved by combining human cognitive abilities with the computational power and efficiency of autonomous systems [3]. The rise of modern HAT systems has also been driven by the need to address the complexities and challenges of rapidly evolving and dynamic environments. As tasks become more complex, time-sensitive, and data-intensive, the collaboration between humans and autonomous agents becomes crucial for effectively navigating and responding to these challenges. HAT is an emerging field that explores collaborative partnerships between humans and autonomous systems to perform tasks or achieve common goals [2, 4, 5, 6]. This involves a collaborative arrangement in which at least one human worker collaborates with one or more autonomous agents [2]. This collaborative approach has the potential to revolutionize how tasks are accomplished across various sectors and pave the way for a future where humans and intelligent autonomous systems will work hand in hand to tackle complex problems and achieve shared goals. HAT systems are designed to allow humans to delegate tasks to intelligent autonomous agents while maintaining overall mission control [7]. Autonomous agents, in this context, refer to computer entities with varying degrees of self-governance in decision-making, adaptation, and communication. This definition has been supported by studies conducted by the research works in [8, 9]. The integration of human cognitive capabilities with the computational power and efficiency of autonomous systems in HAT enhances performance, decision-making, and overall system capabilities. Here, we define and clarify some key concepts that are fundamental to understanding the scope and context of this study. These concepts include AI, Autonomy, Autonomous Systems, and Tactical Autonomy. By providing clear definitions and distinguishing between these terms, we aim to establish a common understanding among our readers. Autonomy. Autonomy in the context of HAT describes the ability of intelligent autonomous systems or agents to operate and make decisions independently in a team setting with varying degrees of self-governance [3, 10]. This involves a higher degree of decision-making capability in autonomous systems based on learning, adaptation, and reasoning. It is a property of a system, not a technology itself [10]. An autonomous entity can perceive, reason, plan, and act in pursuit of specific goals or objectives without constant human intervention. It is important to note that the level of autonomy can vary, ranging from fully autonomous systems that make all their decisions to semi-autonomous systems that require human input at certain points [10]. In the context of tactical autonomy, HAT involves the integration of autonomous capabilities into tactical operations. This integration can include various applications, such as using autonomous systems to gather intelligence, perform surveillance, and perform other critical activities. Autonomy enables systems to operate in complex and uncertain environments, learn from experience, and make decisions without explicit human intervention in every scenario. However, it is important to distinguish this from traditional automation, which typically follows pre-programmed rules, decision trees, or logic-based algorithms to perform tasks or make decisions. Traditional automation has limited adaptability and flexibility to handle dynamic or unforeseen situations without explicit programming. This paper discusses how AI-driven autonomy differs from traditional automation by emphasizing learning, adaptation, and decision-making capabilities. These capabilities ultimately enhance the overall effectiveness and agility of human-autonomy teaming in tactical operations. Autonomous Systems. Autonomous systems can perform tasks or operations without constant human control. They utilize AI algorithms and sensors to perceive and navigate their environment, achieving a high degree of autonomy [11]. Tactical Autonomy. In this study, tactical autonomy refers to autonomous systems’ ability to make real-time decisions and take actions in dynamic and complex operational environments [12]. This involves the seamless coordination and interaction between humans and autonomous systems, enabling them to function as a unified team with complementary strengths [12]. HAT focuses on achieving shared mission goals through seamless coordination and collaboration between human operators and intelligent autonomous systems [13]. This paper introduces an AI-driven HAT, which integrates AI into HAT frameworks. This approach improves decision-making, situational awareness, and operational effectiveness by combining the strengths of human expertise and AI capabilities. Tactical autonomy, which combines human cognitive abilities, such as adaptability, intuition, and creativity, with the computational power, precision, and dynamic execution of autonomous systems, has the potential to revolutionize various fields, including defense, emergency response, law enforcement, and hazardous environments [12]. It is important to differentiate between tactical and strategic autonomy to clarify how AI-driven human-autonomy teaming contributes to both levels of autonomy in military and operational contexts. Strategic autonomy refers to a nation or organization’s ability to make autonomous choices regarding broad security goals, whereas tactical autonomy, in contrast to strategic autonomy, focuses on individual units or teams acting independently within a specific mission [14]. Strategic autonomy involves higher-level decision-making and planning that considers long-term goals, overall mission objectives, and broader situational awareness. It addresses the coordination, allocation of resources, and strategic decision-making processes that guide the overall mission or campaign [14]. Tactical Operations. Tactical operations involve coordinated activities in a specific area or environment, typically in a military, law enforcement, or strategic context, focusing on achieving short-term objectives through rapid decision-making, adaptation to dynamic situations, and the application of military skills and resources within a localized area and timeframe [15]. In recent years, advancements in AI, Machine Learning (ML), robotics, and sensor technologies have paved the way for realizing the potential of tactical autonomy [12]. These technological advancements have enabled autonomous systems to perform complex tasks, process vast amounts of data in real-time, make informed decisions, and collaborate with human team members seamlessly [12]. This has opened new possibilities for augmenting human capabilities, optimizing resource allocation, and improving overall operational efficiency. However, effective tactical autonomy requires a comprehensive understanding of the dynamics between humans and autonomous systems. Human factors, including trust, communication, shared situational awareness, and decision-making, play a vital role in ensuring successful HAT. Challenges such as establishing appropriate levels of trust, addressing potential cognitive biases, managing workload distribution, and maintaining effective communication channels must be carefully addressed to ensure seamless collaboration and maximize the potential benefits of tactical autonomy. HAT for tactical autonomy is a collaborative approach to using humans and autonomous systems to operate and control weapons and other military systems. In HAT, the human operators and autonomous systems work together to achieve common goals. The human operators are responsible for the overall mission and making high-level decisions. Autonomous systems are responsible for performing assigned tasks. As explained in detail in Section IV, human operators contribute strategic insight, context, and high-level decision-making capabilities based on their experience and understanding of the mission’s goals. The interaction and communication represent the interfaces and communication channels through which each component exchanges information, collaborates, and makes joint decisions. Within the context of a shared decision-making process, human operators and autonomous systems engage in a collaborative decision-making process, sharing insights, data, and recommendations to formulate effective strategies. The autonomous system is responsible for real-time data processing, analysis, and execution of specific tasks supporting human operators with timely and pertinent information. Subsequently, once decisions are made, the autonomous system performs specific tasks, including reconnaissance, navigation, or data collection, in alignment with the directives of the shared decision-making process. This paper comprehensively explores the historical development and current state of HAT and delves into the opportunities, challenges, and potential future directions in leveraging AI for tactical autonomy. It emphasizes the transformative impact of AI on tactical autonomy and presents opportunities for improved decision-making, situational awareness, and resource optimization. By acknowledging and addressing the challenges associated with AI adoption, and by charting future directions for research, we can pave the way for a future where humans and autonomous systems seamlessly collaborate, ultimately leading to safer, more efficient, and successful missions in tactical environments. I-A Scope and Contributions The main contribution of this paper is its forward-looking study of the applications, trends, and disruptive technologies that will drive the HAT revolution in complex and dynamic environments. This provides a clear picture of HAT services and practical recommendations for future work. I-B Contributions This paper makes the following key contributions to the field of HAT. • We propose a comprehensive conceptual framework for AI-driven HAT in tactical operations, describing critical components such as trust and transparency, function allocation, situational awareness, and ethical considerations. The proposed framework provides a foundation to understand and advance the integration of AI into HAT for tactical environments. • We provide a comprehensive overview of the opportunities and key challenges associated with incorporating AI-driven HAT into tactical operations. • We explore the symbiotic relationship between AI and HAT, presenting a thorough analysis of how AI-driven HAT enhances decision-making, situational awareness, and operational effectiveness in tactical environments. • We identify several research directions for future work in AI-driven HAT, emphasizing ethical considerations, building transparent AI models, and advancing human-centric design principles to fully realize the potential of tactical autonomy. Table I compares our work to existing studies. In this paper, we explore and address research questions related to AI-driven HAT to enhance tactical operations, covering various aspects and challenges. • How do AI and HAT benefit each other when achieving tactical autonomy? • What are the main opportunities and challenges associated with incorporating AI-driven HAT in the context of tactical operations? • How can AI-driven HAT be best used in tactical operations to improve success and decision-making? • What is the plan for AI-driven HAT and how can it improve the collaboration between humans and autonomous systems in tactical situations? • How can AI-driven HAT help humans and autonomous systems work together smoothly to achieve common goals in tactical environments? • What ethical concerns must be considered when developing and using AI-driven HAT systems? • How can we make AI models in HAT more understandable, and why does this matter for better decision-making and trust in autonomous systems? • What design principles should be followed to create user-friendly AI-driven HAT systems for human operators in tactical settings? TABLE I: Comparison of our work to existing works. Year Publications Main Research Focus and Scope 2018 Ref [16] • Explores the relationship between team coordination dynamics and team performance for human-autonomy teams using an extended version of nonlinear dynamical systems methods. 2018 Ref [17] • Proposed a framework for HAT, incorporating three key tenets: transparency, bi-directional communication, and operator-directed authority. 2019 Ref [18] • Discusses what function allocation and challenges in allocating tasks between humans and autonomous machines. 2020 Ref [19] • Provides a framework for practitioners to make informed decisions regarding the integration and training of human-autonomy teams in applied settings. 2020 Ref [20] • Proposes a new approach to using ML agents in real-time strategy games to collaborate with human players rather than competing against them. 2021 Ref [3] • Examines the differences between automation and autonomy and how insights from human-human teaming can be applied to HAT. The authors have identified research gaps that need to be addressed to improve the understanding of HAT. 2022 Ref [2] • Provides a comprehensive understanding of the research environment, dependent variables, independent variables, key findings, and future research directions related to human-autonomy teamwork. 2022 Ref [21] • Emphasizes the need for humans and AI to work together effectively, particularly in complex situations. It examines the factors affecting the design and implementation of AI systems for human interaction. In addition, it provides a detailed roadmap for future HAT research, particularly emphasizing the perspectives of human factors, which aligns well with our focus on enhancing tactical operations through AI-driven HAT. 2024 Our Paper • Proposes a comprehensive conceptual framework for AI-driven HAT in tactical operations, detailing critical components, such as trust and transparency, function allocation, situational awareness, and ethical considerations. • Explores the advantages and challenges associated with integrating AI-powered HAT into tactical operations. • Provides a thorough exploration of the symbiotic relationship between AI and HAT in the context of tactical operations. • Identifies several research directions, including ethical considerations, building transparent AI models, and advancing human-centric design principles, for future work in AI-driven HAT. I-C Methodology This study investigates the potential of AI-driven HAT to revolutionize tactical operations. To achieve this, we conducted a systematic literature review to identify and analyze relevant academic research. Our search primarily targeted prominent academic databases such as Google Scholar, IEEE Xplore, ACM Digital Library, and ScienceDirect for scholarly articles published up to 2024. We focused on studies published up to May 2024 that emphasized empirical research and theoretical frameworks to explore the application of AI in human-autonomy teaming for tactical operations. Note that studies that focused on general AI applications without a tactical operation context were excluded. We employed a combination of keywords, including “AI-driven human-autonomy teaming,” “tactical operations,” “situational awareness,” “automated decision-making,” “Integrating AI and HAT,” “situation models,” and “shared situational awareness in HAT.” We included studies that focused on the application of AI in HAT for tactical operations, explored the use of Natural Language Processing (NLP) and reinforcement learning for improved communication, collaboration, and threat assessment, and addressed challenges related to trust, explainability, and ethical considerations. Furthermore, we included studies that explored the impact of AI-driven HAT on trust, explainability, and ethical considerations. We employed thematic analysis to identify key themes emerging from the reviewed literature, focusing on the opportunities and challenges associated with AI-driven HAT, with a particular emphasis on enhancing situational awareness, decision-making, and human-machine collaboration. The remainder of this paper is organized as follows. Section II discusses the integration of AI solutions into HAT. In Section III, we discuss the concept of delegated autonomy in HAT, exploring different levels and the balance between human decision-making and automated systems in teaming scenarios. Section IV presents the key components and characteristics defining HAT systems. Next, Section V identifies and discusses the practical applications of HAT, presenting real-world examples where HAT has proven advantageous. Section VI explores the economic aspects of AI integration in HAT. VII provides a detailed discussion of situation models and shared situational awareness in HAT. Section VIII outlines the specific roles and contributions of AI in enabling tactical autonomy in HAT, emphasizing its ability to enhance human decision-making. The opportunities and challenges associated with using AI to enhance HAT in tactical autonomy are discussed in Section X. The design of user interfaces and interaction mechanisms for HAT systems in tactical autonomy settings is explored in Section IX. Section XI introduces a proposed framework for AI-driven HAT in tactical operations, describes the key components, and provides guidance for future research and development. Finally, Section XII provides practical recommendations for implementing and optimizing HAT systems. The paper concludes in Section XIII with indications for future work."
https://arxiv.org/html/2411.09751v1,"Analyzing the AI Nudification Application
Ecosystem","Given a source image of a clothed person (an image subject), AI-based nudification applications can produce nude (undressed) images of that person. Moreover, not only do such applications exist, but there is ample evidence of the use of such applications in the real world and without the consent of an image subject. Still, despite the growing awareness of the existence of such applications and their potential to violate the rights of image subjects and cause downstream harms, there has been no systematic study of the nudification application ecosystem across multiple applications. We conduct such a study here, focusing on 20 popular and easy-to-find nudification websites. We study the positioning of these web applications (e.g., finding that most sites explicitly target the nudification of women, not all people), the features that they advertise (e.g., ranging from undressing-in-place to the rendering of image subjects in sexual positions, as well as differing user-privacy options), and their underlying monetization infrastructure (e.g., credit cards and cryptocurrencies). We believe this work will empower future, data-informed conversations — within the scientific, technical, and policy communities — on how to better protect individuals’ rights and minimize harm in the face of modern (and future) AI-based nudification applications.Content warning: This paper includes descriptions of web applications that can be used to create synthetic non-consensual explicit AI-created imagery (SNEACI). This paper also includes an artistic rendering of a user interface for such an application.","Computer vision and generative AI techniques can undress111We define undress functionality as that which takes a representation (image) depicting a clothed individual and produces an representation of that individual without clothes. someone depicted in a picture or video [61, 25, 20, 4]. Such technology is increasingly publicly accessible, leading to a proliferation of “nudification” applications available to end-users online. Nudification applications enable end-users without technical or even Photoshop skills to artificially generate intimate imagery of someone without their consent. We refer to such a resulting image as a synthetic non-consensual explicit AI-created imagery, or SNEACI222Synthetic non-consensual explicit AI-Created imagery, or SNEACI, refers to images or videos that depict a nude or semi-nude subject, including those that contain intimate body parts and/or depict the subject engaged in a sexual act, without the subject’s consent.. Non-consensual imagery created through the use of AI has a starkly different paradigm compared to that of images created through Photoshop or technical skills. AI makes the creation of these images easier, faster, and more realistic because of how advanced generative AI has become. Similar to malware-as-a-service, synthetic non-consensual explicit AI-created imagery brings non-experts the ability to harm at scale. The creation of SNEACI is a form of sexual abuse against the subject who, by definition, is non-consensually depicted in the resulting content [32]. In addition to the serious mental health impacts victim-survivors sustain from the violation of being depicted in SNEACI [18], SNEACI may be used by the creator to extort and/or otherwise harass the subject of the image [30]. As such, there is increasing concern amongst policymakers and new legislation about SNEACI and the applications that facilitate its creation [37, 22, 2]. Despite the severity of abuse possible with nudification applications, little is known about this software ecosystem. In order to secure potential victims of an abusive software ecosystem, we must first understand how that ecosystem operates in practice [3]. We take a first step toward filling this gap through measuring and characterizing the ecosystem by answering three key research questions. Our first research question is: • RQ1. How do nudification applications position themselves to clients via text and visual descriptions? Acknowledging that nudification applications could be used with the consent of the image subject, we seek to understand whether existing applications foster and support those (and only those) use cases. Additionally, we seek to understand the general experience of users as they interact with these sites. Among the questions we ask: do users need to confirm that they are adults, do the sites support the nudification of all people, and how do the sites communicate about the role of consent from the image subject? Understanding the user experience on these sites, and how these sites position themselves and communicate with their users, is essential for having an informed conversation about their dominant use cases, the benefits and harms of these applications, and, to the extent that harms dominate benefits, how the broader research, industry, and policy communities might go about mitigating those harms. Next, we seek to answer the following research question: • RQ2. What features do nudification applications advertise? In our study, we seek to understand and catalogue the advertised features of popular nudification applications — these are the features that they purport to offer, and hence the features that users believe that the applications will provide. We chose not to experimentally verify that each nudification site actually provides all (and has no hidden features) the features that they claim, nor did we upload images of people for nudification to these sites to test these features, as we did not believe it would be ethical to upload images of real people to potentially adversarial entities, including images of people on our research team, or even already-public stock photos or images of celebrities. Instead, for our purposes, we focus on what features users believe they will have access to if they choose to use the nudification site. Even if not all nudification platforms provide all the features that they claim to, if users seek those features today, we conjecture that future instantiations of these platforms will provide those features. Hence, knowing the features that these applications advertise, and what features these applications believe that their users want or will want, is valuable. Lastly, we seek to answer the following research question: • RQ3. How do nudification applications monetize? Prior to the full initiation of our research study, we gained preliminary experience through the interaction with several nudificaiton sites. Our preliminary interactions uncovered a diverse monetization ecosystem, including both conventional payment systems (like credit cards) as well as newer, less conventional payment systems (like cryptocurrencies). Under the hypothesis that at least some nudification sites are problematic, one approach toward curbing their existence might be to challenge their ability to monetize. Additionally, as part of studying the monetization ecosystem, we sought to assess these sites’ monetization strategies, e.g., do certain features (like full nudification) require a paid subscription whereas other features (like changing clothing) do not? To answer our research questions, we collected a sample of 20 nudification applications (websites) and systematically analyzed them using the application walkthrough method [24] — a methodology used in several fields within and beyond computer science (e.g., [31, 45, 39, 55, 21, 12]). In analyzing the applications, we found a problematic ecosystem: • 19 out of 20 applications explicitly specialize in the undressing of women; only half of the websites mention that they expect the user to have the image subject’s consent and fewer ask for affirmation that consent has been obtained. • Most of the applications allowed for additional features beyond “undressing” (e.g., making the image-subject nude with their breasts and vulva visible in the imagery). For example, half of the applications allowed users to put image-subjects into sexual acts. • These nudification applications make up a commercial ecosystem and, hence, targeting their commercialization features might be one way to protect against SNEACI. Furthermore, we see purposeful repackaging of these nudification features with 5 out of the 20 applications offering API access to their highest paying customers. Stepping back, the computer security research community, as it is often defined, focuses on computing in the presence of adversaries. In some cases, the research focuses on studying adversarial features and the adversarial ecosystem, and in other cases, the research focuses on studying defenses against said adversaries. The ecosystem of image-based sexual abuse is an example of an adversarial usage of technology, and this work sits within that context and builds on prior research in and adjacent to the computer security research community on understanding and studying the emerging realm of synthetic explicit non-consensual AI-created imagery [6, 56, 54]."
https://arxiv.org/html/2411.10176v1,Let people fail! Exploring the influence of explainable virtual and robotic agents in learning-by-doing tasks,"Collaborative decision-making with artificial intelligence (AI) agents presents opportunities and challenges. While human-AI performance often surpasses that of individuals, the impact of such technology on human behavior remains insufficiently understood, primarily when AI agents can provide justifiable explanations for their suggestions. This study compares the effects of classic vs. partner-aware explanations on human behavior and performance during a learning-by-doing task. Three participant groups were involved: one interacting with a computer, another with a humanoid robot, and a third one without assistance. Results indicated that partner-aware explanations influenced participants differently based on the type of artificial agents involved. With the computer, participants enhanced their task completion times. At the same time, those interacting with the humanoid robot were more inclined to follow its suggestions, although they did not reduce their timing. Interestingly, participants autonomously performing the learning-by-doing task demonstrated superior knowledge acquisition than those assisted by explainable AI (XAI). These findings raise profound questions and have significant implications for automated tutoring and human-AI collaboration.","The maturation of artificial intelligence (AI) techniques has facilitated their extensive utilization across various domains. The integration and refinement of explainable AI (XAI) methods have further empowered non-expert users to incorporate AI models into decision-making settings Waldman and Martin (2022). The resultant dynamics of human-AI collaboration have become a focal point of interest for the human-computer interaction (HCI) community and society at large Buçinca et al. (2021). While human-AI collaboration in decision-making has predominantly been addressed in HCI in recent years, with individuals interacting with artificial agents or receiving suggestions and explanations from recommendation systems Malhi et al. (2020); Lai et al. (2021), the study of the human-robot collaboration received the scientific community’s attention since the dawn of the human-robot interaction (HRI) research field. However, recent years have witnessed implementing and testing explainable techniques with robots in collaborative contexts Anjomshoae et al. (2019); Wallkötter et al. (2021). Differently from the HCI context, the HRI one provides richer interaction modalities, offering a more diverse range of opportunities for personalizing XAI and the modality of the explanations delivery Matarese et al. (2021). An emerging challenge addressed by both the HCI and HRI communities is examining how AI technologies influence human behavior in the context of human-AI collaboration Green and Chen (2019). Multidisciplinary efforts have investigated the impact of AI suggestions on human decision-making, exploring implications related to human cognitive biases Bertrand et al. (2022). Moreover, the introduction of XAI techniques has a dual effect, enabling non-expert users to benefit from such powerful technology while also raising concerns about over-reliance on AI models and the reinforcement of negative human heuristics, such as automation bias Vered et al. (2023). This work investigates the impact of interacting with virtual and robotic explainable agents on people’s behavior and performance during a learning-by-doing task Anzai and Simon (1979); Schank et al. (2013). In our experiments, participants had to learn an unknown task with the assistance of an explainable artificial agent, specifically a virtual talking agent and a social humanoid robot. Additionally, a separate group performed the task autonomously without assistance. During the experiments, we employed an assessment task to directly and quantitatively measure the utility of the human-agent explanatory interactions, building on prior work111M. Matarese, F. Rea, K. Rohlfing, A. Sciutti. How informative is your XAI? Assessing the Quality of Explanations through Information Power (under review).. We aimed to compare the effect of different explanation strategies and explainable agents on participants’ behavior, focusing on their final knowledge of the task. The subsequent sections are organized as follows. Section 2 reviews related works, categorizing them into three parts: human-AI collaboration, explanations in human-AI decision-making, and explanations evaluation. Section 3 outlines the methods employed in the user study, presenting the peculiar methodologies and the technology used during the experiments. Section 4 details the results of the user study, showing comparisons between the experimental conditions. These results are extensively discussed in Section 5, with reference to the existing literature. Finally, Section 6 summarizes our work, highlighting its limitations."
https://arxiv.org/html/2411.09955v1,Instruction-Guided Editing Controls for Images and Multimedia: A Survey in LLM era,"The rapid advancement of large language models (LLMs) and multimodal learning has transformed digital content creation and manipulation. Traditional visual editing tools require significant expertise, limiting accessibility. Recent strides in instruction-based editing have enabled intuitive interaction with visual content, using natural language as a bridge between user intent and complex editing operations. This survey provides an overview of these techniques, focusing on how LLMs and multimodal models empower users to achieve precise visual modifications without deep technical knowledge. By synthesizing over 100 publications, we explore methods from generative adversarial networks to diffusion models, examining multimodal integration for fine-grained content control. We discuss practical applications across domains such as fashion, 3D scene manipulation, and video synthesis, highlighting increased accessibility and alignment with human intuition. Our survey compares existing literature, emphasizing LLM-empowered editing, and identifies key challenges to stimulate further research. We aim to democratize powerful visual editing across various industries, from entertainment to education. Interested readers are encouraged to access our repository at https://github.com/tamlhp/awesome-instruction-editing.","Visual design tools have become essential in various multimedia fields, although they often require prior knowledge to use effectively. Recent research has emphasised text-guided image editing as a way to make these tools more accessible and controllable (Li et al., 2020a; Patashnik et al., 2021; Gal et al., 2022; Crowson et al., 2022), as in Fig. 1. Studies have shown the effectiveness of diffusion models in creating realistic images and their application in image editing through techniques like swapping latent cross-modal maps for visual manipulation (Ho et al., 2020; Kim et al., 2022). Additionally, specific region editing is made possible through guided masks (Nichol et al., 2022; Avrahami et al., 2022). Moving away from complex descriptions and masks, instruction-based editing has gained traction for its straightforward approach, allowing users to directly command how and what aspects of an image to edit (Hertz et al., 2023; Mokady et al., 2023; Kawar et al., 2023). This paradigm is noted for its practicality, aligning closely with human intuition (Fu et al., 2024; El-Nouby et al., 2019; Fu et al., 2020). The latest text-to-image generative models offer impressive image quality and accuracy in reflecting the given captions, marking a significant leap in content generation technologies (Alayrac et al., 2022; Ramesh et al., 2022; Rombach et al., 2022). Among these advancements, instructional image editing has emerged as a particularly promising application (Brooks et al., 2023). This method streamlines the editing process by eliminating the need for detailed before-and-after captions (Avrahami et al., 2022; Wallace et al., 2023). Instead, users can provide simple, human-readable instructions, such as “change the dog to a cat”, making the editing process more intuitive and aligned with how humans naturally approach image modification (Zhang et al., 2024f). In recent years, advancements in large language models (LLMs) (Touvron et al., 2023; Brown et al., 2020) have dramatically reshaped the landscape of image and video manipulation. The convergence of these technologies has enabled more intuitive, flexible, and high-fidelity editing processes, largely driven by natural language instructions (Wu et al., 2023c; Feng et al., 2024b; Chakrabarty et al., 2023a). These innovations span various applications, from fashion image editing and 3D scene manipulation to video-to-video synthesis and audio-driven editing, empowering users to achieve fine-grained control over visual content. Moreover, Multimodal large language models (MLLMs), building upon the foundational capabilities of traditional LLMs, have extended the boundaries of vision-language tasks (Zhang et al., 2024b). By integrating latent visual knowledge and treating images as input, MLLMs enhance performance in tasks requiring both textual and visual reasoning. The emergence of diffusion models, such as LLaVA (Liu et al., 2024a) and MiniGPT-4 (Zhu et al., 2024a), has further elevated the potential of these frameworks by improving image-text alignment through instruction tuning. These models, including GILL (Koh et al., 2024) and SEED (Ge et al., 2023), facilitate coherent image generation from textual input while preserving rich visual semantics, marking a pivotal evolution in instruction-based editing. This review paper explores the evolution and diversity of techniques underpinning instruction-based image and video editing, synthesizing cutting-edge approaches that integrate human feedback, multimodal signals, and advanced neural architectures. The focus spans from early models leveraging generative adversarial networks (GANs) (Patashnik et al., 2021) to the latest innovations using diffusion models, including frameworks like Pix2Pix (Brooks et al., 2023), InstructBrush (Zhao et al., 2024), and FlexEdit (Nguyen et al., 2024a). Additionally, specialized models for audio- and video-driven editing, such as Noise2Music (Huang et al., 2023a) and Fairy (Wu et al., 2023a), are examined, demonstrating the versatility and creativity unlocked by these methods. By analyzing over 100 recent key publications, this review delves into key technological breakthroughs, evaluates their effectiveness, and considers potential avenues for further innovation. From 3D image editing (Sabat et al., 2024) to fashion editing (Wang and Ye, 2024), this paper highlights how these models are reshaping industries ranging from entertainment and fashion to education and remote sensing (Han et al., 2024b). Through this comprehensive overview, we aim to identify emerging trends, challenges, and opportunities in the growing field of text-driven, instruction-guided image and video editing. Differences with Existing Surveys. Our survey differs from existing surveys in its specific focus on instruction-based image and video editing empowered by LLMs. While Li et al. (Li et al., 2024b) focus on the integration of various modalities for retrieval tasks, our paper highlights the use of instructions for precise visual editing. Qin et al. (Qin et al., 2024) evaluate instruction-following abilities in LLMs but does not address their application in visual manipulation, which is a key focus of our review. Similarly, Yin et al. (Yin et al., 2023) address instruction-following in language models with a broader emphasis on ethical concerns, whereas our review emphasizes the technical advancements in using these capabilities for visual content generation and editing across various domains, including image, video, and 3D manipulation. Closest to our review is (Zhan et al., 2023), which explores generative AI techniques but lacks the detailed exploration of instruction-following in visual editing contexts, as seen in our paper. Especially, we consider caption-based image editing (Chen et al., 2018; Couairon et al., 2022b; Lin et al., 2023a) is a part of instruction-based image editing but we do not fully focus on the former. Rather, we are interested in user-friendly instructions that have practical implications for broad audience when editing images. Table 1 summarises the difference between our surveys and existing ones. Table 1. A comparison between existing surveys Survey Focused Task Focused Modality Key Contents (Qin et al., 2024) Editing Text Instruction development, Evaluation concerns (Yin et al., 2023) Editing Text LLM-empowered instructions, Instruction tuning (Li et al., 2024b) Retrieval Image, Video, Audio Image-text composite retrieval, Multimodal composite retrieval (Zhan et al., 2023) Generation Image Text guidance, Audio guidance, Sketch guidance, etc. Ours Editing Image, Video, Audio Instruction mechanisms, Augmentations, Learning stragies, Model designs, Loss functions Paper Collection Methodology. To map the research landscape on this subject, we used a range of keyword searches and combinations such as “image editing”, “image manipulation”, “text-guided”, “instruction-followed”, and “instruction-guided”. Initially, we relied on platforms like Google Scholar, Semantic Scholar, and the AI-enhanced tool Scite.ai to compile an initial set of studies. We then expanded this collection by conducting backward searches, reviewing the references in the selected papers, and forward searches to identify works that cited them. To ensure accuracy, we manually evaluated the relevance of each study, given that some focused on related areas like image generation or retrieval but employed similar techniques. This thorough process ultimately resulted in the identification of over 100 pivotal papers relevant to the field. Contributions. The main contributions of this survey are: • Comprehensive Review: This study provides a comprehensive review of LLM-empowered image and media editing. We have gathered and summarised an extensive body of literature, including both published works and pre-prints up to October 2024. • Process-based Taxonomy: We have organised the literature according to the developmental stages of an image editing framework. Fig. 2 presents the taxonomy we developed to structure the existing works in the field. • Optimisation Tools: We have curated a set of optimisation tools for developing end-to-end image editing frameworks, covering model designs, learning strategies, instruction mechanisms, data augmentations, and loss functions. • Practical Applications: We discuss various practical applications across multiple domains, including style, fashion, face editing, scene manipulation, charts, remote sensing, 3D, speech, music, and video editing. • Challenges and Future Directions: Instruction-guided visual design remains an emerging area of research. Based on the surveyed literature, we identify several unresolved challenges and propose future research directions to explore more editing use cases and user-friendly editing controls. • Sources, Datasets, and Metrics: To support empirical research, we provide a comprehensive overview of available source codes, datasets, and evaluation metrics that have been utilised in the field. • Online Updating Resource: To support ongoing research in LLM-empowered visual design, we have created an open-source repository111https://github.com/tamlhp/awesome-instruction-editing, which consolidates relevant studies, including links to papers and available code. Figure 2. Process-based taxonomy of instruction-guided image editing."
https://arxiv.org/html/2411.09789v1,Can EEG resting state data benefit data-driven approaches for motor-imagery decoding?,"Resting-state EEG data in neuroscience research serve as reliable markers for user identification and reveal individual-specific traits. Despite this, the use of resting-state data in EEG classification models is limited. In this work, we propose a feature concatenation approach to enhance decoding models’ generalization by integrating resting-state EEG, aiming to improve motor imagery BCI performance and develop a user-generalized model. Using feature concatenation, we combine the EEGNet model—a standard convolutional neural network for EEG signal classification with functional connectivity measures derived from resting-state EEG data. The findings suggest that although grounded in neuroscience with data-driven learning, the concatenation approach has limited benefits for generalizing models in within-user and across-user scenarios. While an improvement in mean accuracy for within-user scenarios is observed on two datasets, concatenation doesn’t benefit across-user scenarios when compared with random data concatenation. The findings indicate the necessity of further investigation on the model interpretability and the effect of random data concatenation on model robustness.","Data-driven approaches in Brain-Computer Interfaces (BCIs) are bringing neurotechnology closer to real-world applications[15]. These interfaces have a broad range of applications, from restoring motor functions in individuals with severe motor impairments to providing new modes of interaction for able-bodied users in virtual environments. The core functionality of BCIs relies on decoding neural activity, often recorded via electroencephalography (EEG), to infer user intent and translate it into actionable commands. Motor Imagery (MI), a paradigm in which users control external devices by imagining specific motor movements, such as moving a hand or foot is one of the most researched paradigm for BCIs with availability of large and multiple public datasets [14]. MI-BCIs leverage the ability of individuals to modulate their brain activity voluntarily, typically in the sensorimotor cortex, without executing actual movements. This allows for the control of assistive technologies through thought alone, making MI-BCIs particularly valuable for patients with conditions such as amyotrophic lateral sclerosis (ALS) or spinal cord injuries. However, the practical deployment of MI-BCIs is often challenged by the significant variability in EEG signals across sessions and subjects, driven by factors such as differences in electrode placement, neurophysiological conditions, and external noise. Recent data-driven approaches with deep learning models trained on datasets from large number of participants show a promising approach for generalisability, requiring lesser or no calibration for new users [21, 9]. The complex deep learning architectures at the same time are a hurdle to interpret the computations and decision making behind the model predictions [14]. This necessitates the incorporation of neurophysiological principles into model design and training to ensure that the models can generalize effectively beyond the specific conditions under which they were trained. Recent efforts towards generalisation has focused on learning disentangled representations with complex encoder and decoder architectures [7, 2] to separate the signals from noise and user-specific signals. Another approach is towards the alignment of the covariance matrix that generalises the common spatial patterns for user-agnostic motor imagery decoding [8, 13]. Our research introduces an innovative approach to achieve generalization across sessions and users by utilizing resting-state EEG data. Prior studies have demonstrated the effectiveness of resting-state EEG as a distinctive biometric for individual identification [12, 4, 20], due to its reflection of spontaneous brain activity when no specific tasks are being performed. This brain activity captures the brain’s inherent functional organization and has been associated with individual differences in cognitive and behavioral characteristics. Moreover, the correlated predictors of BCI performance, are also based on extracted features from resting state EEG data. [19, 17, 1]. Therefore, integrating resting-state EEG features with task-related features from motor imagery (MI) may represent a potential pathway to enhance the generalization capabilities of decoding models. This approach is designed with a foundation in neurophysiology, requiring minimal modifications to the existing standard architecture. The key contributions of this paper are as follows: • We introduce a novel approach that combines resting-state EEG features with task-related EEG features extracted from the decoding models across sessions and subjects in MI-BCI applications. • We investigate the effectiveness of this approach and perform ablation studies with feature concatenation; to multiple datasets comprising multiple subjects and sessions. The rest of the paper is organized as follows. Section II details the dataset and preprocessing methods. Section III describes the model architecture and training procedures. Section IV presents the results and discusses the implications of our findings. Finally, Section V concludes with a summary of our contributions and future directions for research."
https://arxiv.org/html/2411.09707v1,Decoding Fatigue Levels of Pilots Using EEG Signals with Hybrid Deep Neural Networks,"The detection of pilots’ mental states is critical, as abnormal mental states have the potential to cause catastrophic accidents. This study demonstrates the feasibility of using deep learning techniques to classify different fatigue levels, specifically a normal state, low fatigue, and high fatigue. To the best of our knowledge, this is the first study to classify fatigue levels in pilots. Our approach employs the hybrid deep neural network comprising five convolutional blocks and one long short–term memory block to extract the significant features from electroencephalography signals. Ten pilots participated in the experiment, which was conducted in a simulated flight environment. Compared to four conventional models, our proposed model achieved a superior grand–average accuracy of 0.8801 (\pm0.0278), outperforming other models by at least 0.0599 in classifying fatigue levels. In addition to successfully classifying fatigue levels, our model provided valuable feedback to subjects. Therefore, we anticipate that our study will make the significant contributions to the advancement of autonomous flight and driving technologies, leveraging artificial intelligence in the future.","I INTRODUCTION Brain–computer interface (BCI) enables interaction between humans and devices by interpreting user’s cognitive status and intentions [1, 2, 3]. BCIs are generally categorized into invasive and non–invasive types [4]. Invasive BCIs record neural activity through implanted electrodes positioned near target neurons [5]. This approach offers the benefit of a high signal–to–noise ratio (SNR) due to direct brain activity measurement but necessitates surgical intervention. Conversely, non–invasive BCIs acquire brain activity data without the need for surgery [6, 7]. While this method avoids surgical procedures and is more cost–effective, it suffers from a relatively lower SNR [5]. As a result, non–invasive BCIs have been utilized in various applications, including the control of external devices such as drones [8], robotic arms [9], wheelchairs [10], and spellers [11]. In the field of BCI, the accurate detection of humans’ abnormal mental states with high performance remains a critical concern [12]. Recently, numerous technologies related to autonomous flight and driving have been developed for application in real–world environments. Given that pilots’ or drivers’ mental state is a key factor in ensuring passenger safety, the ability to reliably detect these states is crucial for enhancing safety measures. Flight operation is a demanding task due to the significant energy expenditure involved [13]. Fatigue is commonly induced by extended cognitive tasks, particularly those that are repetitive or monotonous [12]. According to the British Airline Pilots’ Association, 280 out of 500 commercial pilots reported having fallen asleep during night flights. In the context of detecting humans’ mental states, electroencephalography (EEG) signals are highly informative as they directly reflect an individual’s cognitive status and intentions [14]. Several studies have focused on detecting fatigue using solely EEG signals. Wu et al. [15] proposed the nonparametric prior–induced deep sum–logarithmic–multinomial mixture model for identifying pilots’ fatigue through the development of a brain power map, along with the adaptive topic–layer stochastic gradient Riemann Markov chain Monte Carlo inference method for estimating global parameters without relying on heuristic assumptions. Yang et al. [16] introduced the novel complex network–based broad learning system for the EEG–based fatigue detection. Their results demonstrated that the method could reliably distinguish between fatigue and alert states with high stability. The contributions of this study are as follows: i) We developed an experimental paradigm to acquire EEG signals related to fatigue in a flight environment involving pilots. We effectively induced a normal state (NS), low fatigue (LF), and high fatigue (HF) based on task variations. ii) We introduced the novel model comprising five convolutional blocks and one long short–term memory (LSTM) block for classifying fatigue levels. Our proposed model demonstrated a superior accuracy compared to the conventional models. To the best of our knowledge, our study represents the first attempt to classify fatigue levels using the deep learning architecture. Figure 1: Data configuration of EEG and EOG channels."
https://arxiv.org/html/2411.09577v1,SimTube: Generating Simulated Video Comments through Multimodal AI and User Personas,"Audience feedback is crucial for refining video content, yet it typically comes after publication, limiting creators’ ability to make timely adjustments. To bridge this gap, we introduce SimTube, a generative AI system designed to simulate audience feedback in the form of video comments before a video’s release. SimTube features a computational pipeline that integrates multimodal data from the video—such as visuals, audio, and metadata—with user personas derived from a broad and diverse corpus of audience demographics, generating varied and contextually relevant feedback. Furthermore, the system’s UI allows creators to explore and customize the simulated comments. Through a comprehensive evaluation—comprising quantitative analysis, crowd-sourced assessments, and qualitative user studies—we show that SimTube’s generated comments are not only relevant, believable, and diverse but often more detailed and informative than actual audience comments, highlighting its potential to help creators refine their content before release.","Audience feedback is crucial for video content creators to shape and refine content. Various modern platforms [11, 57, 60] allow creators to share videos and gather crowd-sourced input [74, 71, 82, 57]. For instance, YouTubers and TikTok creators frequently adjust their content based on audience reactions and comments from previous episodes [26, 14]. However, such audience feedback is often delayed, typically provided only after the content has been published, limiting creators’ ability to make timely improvements and dynamically adjust the content. That said, creators must wait until their content is posted to receive input, which can only influence future episodes rather than the current one. For novice creators, this challenge is even more pronounced due to the low visibility of their content, making it difficult to gather substantial, actionable feedback. To address these limitations, we aim to develop methods to enable video content creators to obtain diverse and meaningful audience feedback before publishing their videos. Specifically, we focus on simulating audience comments, a prevalent form of feedback on video-sharing platforms. Video comments allow viewers to directly share their thoughts with creators, often sparking broader discussions as other users engage by liking or replying [70]. These engagements offer valuable perspectives and foster a sense of community [70, 17], making video comments a rich source of feedback for creators. To this end, we present SimTube, a full-stack AI system capable of generating diverse, relevant, and believable audience comments based on video content. We proposed a computational pipeline that leverages generative AI models, including vision language models (VLMs) for understanding visuals, speech recognition for transcribing audio, and large language models (LLMs) for generating natural language feedback. This pipeline first integrates the multimodal data in videos—including visuals, audio, and metadata—to produce a video summary and keywords. Subsequently, the video summary and keywords are combined with various persona descriptions—representing different audience demographics and backgrounds—to simulate video comments from diverse perspectives. In addition to this pipeline, we designed a user interface that allows creators to upload videos, obtain simulated feedback, and interactively explore the results. Users can also customize personas to tailor the feedback to specific viewpoints or audience backgrounds as they see fit. To understand the effectiveness of SimTube and the quality of the comments it generates, we conducted a comprehensive set of assessments, including quantitative analysis, crowd-sourced ratings, and qualitative studies. Our results indicate that SimTube produces relevant, believable, and helpful comments for creators across various video genres. Notably, in many instances, AI-generated comments were rated as more informative and beneficial to creators than those left by actual users. Additionally, the user study provided insights into how SimTube can integrate into creators’ video production workflows, revealing user perceptions of generative video comments. Collectively, this paper makes the following contributions: • SimTube, an interactive system that supports the automatic generation of diverse video comments, enabling creators to receive valuable feedback before publishing their video content. • A multimodal AI pipeline that integrates multiple data modalities in video—such as visuals, audio, and metadata—along with user personas sampled from a large dataset, to produce diverse, relevant, believable, and helpful video comments for creators. • A thorough evaluation involving automatic metrics, crowdsourcing, and qualitative user studies, demonstrating SimTube’s effectiveness and providing insights for the development of future AI-assisted feedback tools in content creation."
https://arxiv.org/html/2411.09066v1,A multidimensional measurement of photorealistic avatar quality of experience,"Photorealistic avatars are human avatars that look, move, and talk like real people. The performance of photorealistic avatars has significantly improved recently based on objective metrics such as PSNR, SSIM, LPIPS, FID, and FVD. However, recent photorealistic avatar publications do not provide subjective tests of the avatars to measure human usability factors. We provide an open source test framework to subjectively measure photorealistic avatar performance in ten dimensions: realism, trust, comfortableness using, comfortableness interacting with, appropriateness for work, creepiness, formality, affinity, resemblance to the person, and emotion accuracy. We show that the correlation of nine of these subjective metrics with PSNR, SSIM, LPIPS, FID, and FVD is weak, and moderate for emotion accuracy. The crowdsourced subjective test framework is highly reproducible and accurate when compared to a panel of experts. We analyze a wide range of avatars from photorealistic to cartoon-like and show that some photorealistic avatars are approaching real video performance based on these dimensions. We also find that for avatars above a certain level of realism, eight of these measured dimensions are strongly correlated. In particular, for photorealistic avatars there is a linear relationship between avatar affinity and realism; in other words, there is no uncanny valley effect for photorealistic avatars in the telecommunication scenario. We provide several extensions of this test framework for future work and discuss design implications for telecommunication systems. The test framework is available at https://github.com/microsoft/P.910.","Photorealistic avatars are human avatars that look, move, and talk like real people. Photorealistic avatars can be used for various applications, such as: • Telecommunication: Photorealistic avatars can be used instead of two-dimensional webcam videos to create a virtual meeting space. Users can interact while maintaining correct eye gaze, allowing participants to know who is looking at whom, which increases trust (Nguyen and Canny, 2007) while reducing video fatigue by reducing hyper-gaze (Gale et al., 1975; Fauville et al., 2021). • Health care: Photorealistic avatars can be used to provide virtual consultations, training, therapy, and education for patients and medical professionals. For example, a photorealistic avatar of a doctor can explain a diagnosis, prescribe a treatment, or demonstrate a procedure to a patient. • Education: Photorealistic avatars can be used to create immersive, personalized, and interactive learning environments for students and teachers. For example, a photorealistic avatar of a teacher can guide students through a lesson, provide feedback, or answer questions. • Retail and e-commerce: Photorealistic avatars can be used to enhance the (online) shopping experience for customers, sellers, and customer service. For customers, photorealistic avatars can enable virtual try-on experiences for clothing, accessories, or makeup, allowing customers to visualize how products will look on them before making a purchase. • Entertainment: Photorealistic avatars can be used to create realistic and engaging characters for games, movies, shows, and social media. For example, a photorealistic avatar of an actor can perform scenes, interact with fans, or promote brands. While there are many applications of photorealistic avatars, our focus is on evaluating them for the telecommunication scenario to improve trust and reduce video fatigue. Therefore the avatar test sequences we use are targeted for telecommunication, which includes people talking and expressing a wide range of emotions (happy, sad, surprised, fear, anger, and disgust). The performance of photorealistic avatars has been improving significantly recently based on objective metrics such as PSNR (Gonzalez and Woods, 2006), SSIM (Wang et al., 2004), LPIPS (Zhang et al., 2018), FID (Heusel et al., 2017), and FVD (Unterthiner et al., 2019). However, recent avatar publications do not provide subjective tests of the avatars to measure human usability factors (e.g., (Saito et al., 2024; Tian et al., 2024; Xu et al., 2024b; Huang et al., 2024; Shao et al., 2024; Zhou et al., 2024; Deng et al., 2024; Kirschstein et al., 2024; Xu et al., 2024a; Liu et al., 2024)). We believe this is because such subjective tests are challenging, but also because there is no standardized or readily available method to do so. The usability factors that have been previously suggested and studied for avatars include realism (Inkpen and Sedlins, 2011), comfortableness using (Inkpen and Sedlins, 2011), comfortableness interacting with (Inkpen and Sedlins, 2011), appropriateness for work (Inkpen and Sedlins, 2011), creepiness (Inkpen and Sedlins, 2011), formality (Inkpen and Sedlins, 2011), resemblance to the person (Inkpen and Sedlins, 2011), trust (ITU-T Recommendation P.1320, 2022), and emotion accuracy (ITU-T Recommendation P.1320, 2022). We also include affinity to study the uncanny valley effect (Mori et al., 2012). The research questions we want to answer in this work are: • RQ1: Are the objective metrics currently used to develop avatars (PSNR, SSIM, LPIPS, FID, FVD) sufficient to achieve the performance goals of avatars, especially the human usability factors for avatars? • RQ2: Which human usability factors are the most important for photorealistic avatars? • RQ3: Can we develop an accurate and reproducible test framework to measure human usability factors for avatars? • RQ4: Is there an uncanny valley effect for photorealistic avatars in the telecommunication scenario? Our contributions in this work are: • We provide an open source test framework to subjectively measure photorealistic avatar quality of experience in ten dimensions using crowdsourcing. • The crowdsourced subjective test framework is highly reproducible and accurate compared to a panel of experts. • We show that the correlation of nine of these subjective dimensions to PSNR, SSIM, LPIPS, FID, and FVD is weak and one (emotion accuracy) is moderate, motivating the need for an available subjective test framework as well as improved objective metrics to measure avatar performance. • We analyze a wide range of avatars from photorealistic to cartoon-like and show some photorealistic avatars are approaching real video quality based on these subjective metrics. • We show that for avatars with a realism > 2 (out of a 1-5 Likert scale) eight of the ten measured dimensions are strongly correlated, which leads to a dimensionality reduction of ten to three for the survey. • We show that for photorealistic avatars there is a linear relationship between avatar affinity and realism. In other words, there is no uncanny valley effect for photorealistic avatars in the telecommunication scenario; the more realistic the avatar is, the more affinity there is to the avatar. In Section 2, we review related work in this area. In Section 3, we describe the test framework design to measure avatar quality of experience, and in Section 4, we show that the test framework is both reproducible and accurate. Using the test framework, we show the results and analysis in Section 5. Finally, we provide conclusions, future extensions, system design implications, and limitations in Section 6."
https://arxiv.org/html/2411.08901v1,"SoccerGuard: Investigating Injury Risk Factors for 
Professional Soccer Players with Machine Learning","We present SoccerGuard, a novel framework for predicting injuries in women’s soccer using Machine Learning (ML). This framework can ingest data from multiple sources, including subjective wellness and training load reports from players, objective GPS sensor measurements, third-party player statistics, and injury reports verified by medical personnel. We experiment with a number of different settings related to synthetic data generation, input and output window sizes, and ML models for prediction. Our results show that, given the right configurations and feature combinations, injury event prediction can be undertaken with considerable accuracy. The optimal results are achieved when input windows are reduced and larger combined output windows are defined, in combination with an ideally balanced data set. The framework also includes a dashboard with a user-friendly Graphical User Interface (GUI) to support interactive analysis and visualization.","The training intensity of professional soccer players has experienced a tremendous increase in the past decade, in parallel to the growing athleticism in the sport. From the 2006/2007 to the 2012/2013 English Premier League seasons, the mean distances of high-intensity running increased by 28% with ball possession and by 31% without ball possession (Barnes et al., 2014). Players performed twice as many sprints during the Union of European Football Associations (UEFA) Champions League in the 2018/2019 season compared to prior years (UEFA, 2019). With an elevated number of training sessions, matches, and tournaments coupled with a larger number of injuries (Nassis et al., 2020; Ekstrand et al., 2016), the reduction of injury risk factors has become an increasingly important aspect of professional soccer teams (Gabbett et al., 2017). Given the comparatively limited research conducted on female athletes, it is crucial to gain a deeper understanding of the risk factors associated with injuries in female athletes (Bodenner, 2015). In this paper, we make several contributions to the state of the art in injury risk analysis for the domain of women’s soccer. The central contribution is the SoccerGuard framework consisting of three major pipelines: The Preprocessing Block, the Automated Machine Learning Block, and the Soccer Dashboard. The study also includes 90 unique ML experiments, each with different settings across four experiment components, focused on predicting an injury event in a Time Series Classification (TSC) scenario. Moreover, the Soccer Dashboard offers an interactive tool that facilitates a more comprehensive understanding of the characteristics of the athletes’ data and potential injury risks. SoccerGuard is designed to be flexible in order to facilitate the integration of multiple data sources. The possibilities of monitoring data in soccer clubs and related work, including Artificial Intelligence (AI) in prediction systems in the domain of soccer are demonstrated. With the GPS data and pmSys data included in this study, several studies base their work and results on a similar subset of these instances of data (Van Eetvelde et al., 2021; Rossi et al., 2018; Lövdal et al., 2021). It is proposed that the fusion of different data sources can enhance the prediction accuracy of injury detection by TSC."
https://arxiv.org/html/2411.08889v1,Multilingual Standalone Trustworthy Voice-Based Social Network for Disaster Situations,"In disaster scenarios, effective communication is crucial, yet language barriers often hinder timely and accurate information dissemination, exacerbating vulnerabilities and complicating response efforts. This paper presents a novel, multilingual, voice-based social network specifically designed to address these challenges. The proposed system integrates advanced artificial intelligence (AI) with blockchain technology to enable secure, asynchronous voice communication across multiple languages. The application operates independently of external servers, ensuring reliability even in compromised environments by functioning offline through local networks. Key features include AI-driven real-time translation of voice messages, ensuring seamless cross-linguistic communication, and blockchain-enabled storage for secure, immutable records of all interactions, safeguarding message integrity. Designed for cross-platform use, the system offers consistent performance across devices, from mobile phones to desktops, making it highly adaptable in diverse disaster situations. Evaluation metrics demonstrate high accuracy in speech recognition and translation, low latency, and user satisfaction, validating the system’s effectiveness in enhancing communication during crises. This solution represents a significant advancement in disaster communication, bridging language gaps to support more inclusive and efficient emergency response.","In today’s interconnected world, effective communication is essential, particularly during disaster situations where timely information dissemination can save lives. However, linguistic diversity often poses significant challenges in ensuring that crucial messages are understood by all stakeholders. Traditional communication platforms frequently fail to address these language barriers, leaving gaps in understanding that can have dire consequences during emergencies. The advent of AI technologies and decentralized systems like blockchain offers new possibilities for creating secure, multilingual communication networks that can function reliably even in the most challenging circumstances. Motivated by the need for a robust solution to bridge language gaps in crisis scenarios, this paper presents a novel, multi-platform application that integrates advanced AI with blockchain technology to enable trustworthy, asynchronous voice-based social media communication across language barriers. The application is designed to run seamlessly on various devices, including mobile phones, tablets, and desktops, and is compatible with different operating systems, ensuring accessibility and reliability in diverse environments. The significance of addressing multilingual communication barriers in disaster situations cannot be overstated. Traditional approaches to disaster communication often fail to account for the linguistic diversity inherent in many societies, leading to what is termed ”disaster linguicism.” This phenomenon exacerbates the vulnerability of Indigenous, Tribal, Minority, and Minoritized peoples, as critical information is often not accessible in their native languages. By proposing a solution that integrates advanced AI with blockchain technology, this paper directly responds to the urgent need for more inclusive and effective disaster risk reduction strategies that consider the linguistic needs of all communities, thereby reducing social vulnerability and enhancing resilience during crises [1]. Despite advancements in communication technology, there remains a critical gap in enabling asynchronous, multilingual voice communication during disaster situations. Current social media platforms and communication tools often rely on centralized servers and lack the necessary security features to ensure message integrity and authenticity, which are particularly vulnerable in disaster scenarios where external networks may fail. Additionally, the language translation tools available are not typically integrated into a system that provides both translation and secure message verification. This lack of integration can lead to misinformation, miscommunication, and delays in critical information sharing, which can exacerbate the effects of disasters. Furthermore, many existing solutions are platform-dependent, limiting their effectiveness across different devices and operating systems. This paper presents significant advancements in the field of disaster communication and multilingual technology through the following key contributions: • Innovative Application for Disaster Scenarios: We introduce a novel, multi-platform application specifically designed to function reliably in disaster situations, where communication infrastructure may be compromised. The application’s ability to operate offline on a local network without requiring an internet connection ensures its deployment in critical environments. Its lightweight design allows it to run on portable devices like laptops, making it highly adaptable and infrastructure-free. • AI-Driven Asynchronous Multilingual Translation: The application leverages advanced AI technology to facilitate asynchronous voice communication by automatically translating spoken messages across multiple languages. Furthermore, it synthesizes these translations back into voice, providing a natural and effective method of communication that transcends language barriers. • Blockchain-Enabled Secure Communication: A key feature of this application is its implementation of secure storage and verification mechanisms using Ethereum blockchain. This ensures the integrity and authenticity of translated messages, critical in maintaining trustworthy communication during emergencies. • Cross-Platform Usability and Security: The application is designed to operate independently of external servers, with all AI processing and blockchain management performed locally. This not only enhances security and reliability but also ensures a seamless user experience across various devices and operating systems, making it accessible in diverse disaster environments. • Robust and Portable Communication System: By focusing on portability, trustworthiness, and transparency, this application provides a robust communication solution in emergency situations. It guarantees that communication remains secure, reliable, and effective, even when traditional systems fail."
https://arxiv.org/html/2411.09436v1,"Robot Tasks with Fuzzy Time Requirements
from Natural Language Instructions","Natural language allows robot programming to be accessible to everyone. However, the inherent fuzziness in natural language poses challenges for inflexible, traditional robot systems. We focus on instructions with fuzzy time requirements (e.g., “start in a few minutes”). Building on previous robotics research, we introduce fuzzy skills. These define an execution by the robot with so-called satisfaction functions representing vague execution time requirements. Such functions express a user’s satisfaction over potential starting times for skill execution. When the robot handles multiple fuzzy skills, the satisfaction function provides a temporal tolerance window for execution, thus, enabling optimal scheduling based on satisfaction. We generalized such functions based on individual user expectations with a user study. The participants rated their satisfaction with an instruction’s execution at various times. Our investigations reveal that trapezoidal functions best approximate the users’ satisfaction. Additionally, the results suggest that users are more lenient if the execution is specified further into the future.","Automating household tasks and production in small and medium enterprises is attracting considerable attention in robotics (e.g., [1, 2]). Robot programming is still mostly relegated to specialized robotics experts, resulting in high costs and slow adaptation to new situations [2]. One response to this is to increase the accessibility of robot programming [3, 4], for example, with natural language [5]. Natural language contains inherent fuzziness that reduces the user’s cognitive load. However, this contrasts with the rigid parameters (like concrete execution times) demanded by traditional robot systems [6]. For example, the instruction “Prepare some food in about ten minutes!” (Fig. 1) contains fuzziness regarding parameters (“some food”) and execution time (“about ten minutes”). To handle fuzzy parameters, fuzzy-logic [7] is commonly used to deduce exact parameters (e.g., weight in grams) for the operating robot system [8, 9]. For time-dependent parameters, fuzzy-logic is extended to temporal fuzzy logic [10]. However, previous research focused hardly on the user’s perception of fuzziness regarding execution time. For instance in Fig. 1, there is no immediate loss if the robot prepares the food five minutes earlier or later. Nevertheless, the user may be dissatisfied if the operation is not performed around the specified time, which depends on the instruction and the context. In this example, execution after 15 minutes leads to higher user satisfaction than after 20 minutes. Figure 1: Natural language instructions are inherently fuzzy, requiring an interpretation within the context of the instruction and instructor. Here, the user’s satisfaction varies over time based on the start of the task execution. This paper investigates instructions with fuzzy time requirements. An instruction describes a fuzzy skill that encodes the manipulation of an object by the robot (in continuation of [11, 12]). If the user instructs only one fuzzy skill, it can be executed at maximum satisfaction by default. However, suppose the user instructs several fuzzy skills combined into one superordinate plan, the fuzzy task. In that case, the task execution may require compromises, i.e., the robot must perform some skills at a suboptimal time. For example, if the user issues another command that should also start in ten minutes (Fig. 1), rigid robot systems lack the knowledge of which operation to prefer. The satisfaction function provides this required knowledge – enabling the scheduling to maximize overall satisfaction. Precisely identifying the expectations of an individual user before execution is challenging. In addition to the given instruction itself, other aspects could also influence satisfaction, e.g., the context of the scenario, the user’s previous experience, or the expected abilities of the actor. Another challenge is that several users with (partly) divergent expectations may instruct the robot system. One response to this could be the creation of user profiles. However, this does not account for frequent user changes or public scenarios with previously unknown users. Hence, we aim to provide general statements about deriving the satisfaction functions from the instructions and context. We present two central contributions: (i) We formalize fuzzy tasks, their inference from language, and the scheduling (Section III-A and Section III-B). (ii) We deduce an overall satisfaction function from individual user satisfaction (Section III-C). On this basis, we examine fuzzy time requirements regarding their modeling, the difference between human and robot actors, and the influence of time until the required execution starts (Section IV). For this, we exploit subjective satisfaction data gathered with an online user study."
https://arxiv.org/html/2411.09266v1,"How Good is ChatGPT at Audiovisual Deepfake Detection: A Comparative Study of ChatGPT, AI Models and Human Perception","Multimodal deepfakes involving audiovisual manipulations are a growing threat because they are difficult to detect with the naked eye or using unimodal deep learning-based forgery detection methods. Audiovisual forensic models, while more capable than unimodal models, require large training datasets and are computationally expensive for training and inference. Furthermore, these models lack interpretability and often do not generalize well to unseen manipulations. In this study, we examine the detection capabilities of a large language model (LLM) (i.e., ChatGPT) to identify and account for any possible visual and auditory artifacts and manipulations in audiovisual deepfake content. Extensive experiments are conducted on videos from a benchmark multimodal deepfake dataset to evaluate the detection performance of ChatGPT and compare it with the detection capabilities of state-of-the-art multimodal forensic models and humans. Experimental results demonstrate the importance of domain knowledge and prompt engineering for video forgery detection tasks using LLMs. Unlike approaches based on end-to-end learning, ChatGPT can account for spatial and spatiotemporal artifacts and inconsistencies that may exist within or across modalities. Additionally, we discuss the limitations of ChatGPT for multimedia forensic tasks.","Figure 1: Illustration of audiovisual deepfake manipulations. Original video content is represented as RVRA (real video with real audio. Through deepfake manipulation techniques, three manipulated types are generated: FVRA (fake video with real audio), RVFA (real video with fake audio), and FVFA (fake video with fake audio). Blue text represents the “real modality” of the video content, while red text represents the “fake modality”. Synthetic multimedia content has become both innovative and a significant threat in recent years. Deepfake images and videos created using artificial intelligence (AI) and deep learning (DL) techniques have attracted public and academic attention. This synthetic content is generated by generative adversarial networks (GANs) [1] and more sophisticated AI techniques such as diffusion models [2]. While deepfake technology has many innovative applications in education, entertainment, and other fields [3], it is a double-edged sword that can be used for unethical purposes, such as pornography, political defamation, identity theft, fraud, misinformation, and disinformation [4, 5, 6]. Unethical use of this technology can lead to political instability and social violence [6]. On the one hand, deepfake technology continues to evolve to create more convincing and realistic fake multimedia content. Social media, on the other hand, plays a catalytic role in spreading such content. Therefore, timely detection of deepfake content is crucial to avoid any damage and loss to human society [4]. Audiovisual deepfakes that involve multimodal manipulation are a more convincing type of forgery, with attackers attacking audio, video, or both modalities. Unimodal video forgery detectors [7, 8, 9, 10] and spoofed audio detectors [11, 12, 13, 14] are generally unable to identify forgeries across multiple modalities, although they may be good at detecting forgeries in the specific modality they focus on. To address this challenge, the research community has developed sophisticated tools and algorithms to detect audiovisual forgeries in videos. These specialized tools require knowledge of multimedia forensics as well as knowledge of deep learning. Furthermore, these tools do not generalize well to other unseen datasets and manipulations. Large language models (LLMs) are a major advancement in the field of artificial intelligence. They are trained on a large amount of data and can perform well in various natural language processing (NLP) tasks such as text generation, summarization, classification, completion, sentimental analysis, machine translation, and question answering. Their applications even go beyond the aforementioned NLP tasks and can be used as writing assistants, learning tools, productivity tools, coding assistants, software development, healthcare, legal assistance, entertainment, and more. Despite being primarily designed for NLP tasks, OpenAI’s ChatGPT can analyze image, audio, and video content. Taking advantage of its support for multimodal input, we studied the potential and limitations of ChatGPT for audiovisual deepfake detection. The research questions we aimed to address in this study are as follows: • Can ChatGPT perform multimedia forensic tasks? • Is ChatGPT capable of detecting forgery based on artifacts in audio and visual modalities? • What is the role of prompt engineering in using ChatGPT to detect audiovisual deepfakes? • Which performs better at identifying forgeries in audiovisual deepfakes, ChatGPT, humans, or AI models? • How interpretable is ChatGPT for forgery detection? • What are the limitations of ChatGPT in detecting multimodal deepfakes? The main contributions of our work are threefold: • We explore for the first time the potential of ChatGPT for audiovisual forgery detection tasks. • We compare the performance of ChatGPT with human and state-of-the-art AI models on audiovisual forgery detection tasks. • We highlight the strengths and limitations of ChatGPT on audiovisual forgery detection tasks."
https://arxiv.org/html/2411.08859v1,Designing a Virtual Reality Training Apprenticeship for Cold Spray Advanced Manufacturing,"Apprenticeship and training programs in advanced manufacturing frequently encounter safety and accessibility concerns due to using heavy machinery. Virtual Reality (VR) training addresses such constraints while maintaining the spatial and procedural learning requirements of such training. However, designing effective VR training is challenging because advanced manufacturing processes are complex and require experts to train novices for a long time. This paper presents a VR Training Apprenticeship (VRTA) tailored for cold spray, which we carefully designed to teach novices step-by-step this particular advanced manufacturing process. To assess its effectiveness, we conducted an exploratory study (n = 22). We evaluated user experience (UX) measures in the form of quantitative scales, users’ qualitative insights, and task performance with real-world machinery after the VR training. We discuss how the VRTA design contributed to the effectiveness and the challenges of considering VR training for advanced manufacturing.","1 VR Training Apprenticeship Design Cold spray is an advanced manufacturing technology that applies coatings of metallic or non-conductive substances to another surface through gas-powered high-velocity spray [25]. Safety is a real concern in the process, especially for novice operators. Hence, VR can provide valuable alternative training. Working collaboratively with experts, we dissected the cold spray process into six distinct modules (powder feeder, applicator, system leak check, substrate preparation, nozzle, and spray). Here, we outline the key considerations of our VRTA design, which, after several iterations, resulted in a training that takes approx. two hours to complete.111Videos and the VRTA itself are accessible here: https://xert.co/ Visual and Audio Realism: Based on recent findings highlighting the influence of graphical realism in VR [24], and leveraging CAD models from cold spray equipment, we recreated the cold spray process on a 1-1 scale. We further simulated the audio in a typical cold spray environment (e.g., the sound of dust collectors) to improve the sense of immersion and presence. Task Instructions: Providing task instructions improves the cognitive engagement of participants compared to unguided training [26]. For the task instructions in our VRTA, we first decomposed the tasks, inspired by the Four-Component Instructional Design (4C/ID) model [32], to optimize step size and break the process down based on a logical order and our learning goals. To help users understand the consequences of incorrect task completion, we included Why Statements (Fig. 1). Moreover, operators frequently need to convey information on names and functions. Hence, we consistently mentioned part names and their uses in relevant steps to help users become familiar with cold spray parts. Importantly, we placed the task instructions directly in front of the users so they could read the instructions while simultaneously engaging with the task itself (Fig. Designing a Virtual Reality Training Apprenticeship for Cold Spray Advanced Manufacturing). Figure 1: (A) Hint system: green and red highlight correct and incorrect interactions. (B) Task instruction: the gray box is task decomposition, and the blue box is the why statement. Hint System: Task instructions alone are insufficient [16]. We created two hint systems: automatic and manual. Prior research indicates that users are reluctant to seek assistance and frequently ignore unclear hints [2]. To tackle this, automatic hints are activated 30 seconds into tasks, highlighting essential objects visually to guide users efficiently (Fig. 1). Manual hints are available through the user interface to repeat text-to-speech audio for task instructions and highlight relevant objects. Task Interactions: While we visually (and audibly) simulated the cold spray process on a 1-1 scale, we designed the actual Task Interactions to highlight and focus on the important learning goals. For instance, the powder feeder (PF) assembly and disassembly steps involve 23 steps, 8 of which align with our learning goals. In these 8 steps, users must meticulously follow task instructions and engage with the system similarly to real-life scenarios. Minimal interaction is necessary for the remaining 15 steps as the VRTA auto-completes these steps. In addition, based on the literature [7], to reinforce what has to be done in each step, we included Confirmation Animations, which are animations that are triggered at the end of each step that demonstrate the most efficient method to complete the task."
https://arxiv.org/html/2411.08641v1,DipMe: Haptic Recognition of Granular Media for Tangible Interactive Applications,"While tangible user interface has shown its power in naturally interacting with rigid or soft objects, users cannot conveniently use different types of granular materials as the interaction media. We introduce DipMe as a smart device to recognize the types of granular media in real time, which can be used to connect the granular materials in the physical world with various virtual content. Other than vision-based solutions, we propose a dip operation of our device and exploit the haptic signals to recognize different types of granular materials. With modern machine learning tools, we find the haptic signals from different granular media are distinguishable by DipMe. With the online granular object recognition, we build several tangible interactive applications, demonstrating the effects of DipMe in perceiving granular materials and its potential in developing a tangible user interface with granular objects as the new media.","Granular materials are commonly seen in our daily lives. As a continuous deformable media, granular materials such as sand or beads has been introduced to tangible user interface (TUI) to preview the landscape or adjust the stiffness of input devices [1, 2, 3, 4]. While different rigid objects have been used in TUI to give flexible control of virtual content [5, 6, 7, 8, 9], to the most of our knowledge, the potential of interacting with different types of granular objects has not been exploited. TUI has proved to play a vital role in everyday tasks. It makes things more concrete and provides embodiment effects through physicality [10]. For example, typical studies such as Project Zanzibar [11], which showed a flexible mat to communicate with tangible objects placed on its surface. It also supports sensing a user’s touch and hover hand gestures, which opened up the possibility of novel digital experiences. De Tinguy et al. [12] proposed that portable devices could be used to simulate the experience of interacting with different objects. Schmitz et al. [13] proposed a fabrication pipeline and sensing approach that enabled object recognition of tangibles on capacitive touchscreens. Yan et al. use the LaserShoes [14] to achieve real-time inference, which cooperate with human under different circumstances. At the same time, they used the laser speckle imaging technique and the LaserShoes could distinguish the surface textures that appear. However, existing TUI with granular materials only involves a pre-specified type of granular media. Ishii et al. [1] proposed the concept of continuous tangible user interface (Sandscape) using granular materials. They argued that granular materials could bridge the gap between physical and digital forms because of their continuous physical properties. Users could interact with models made of sand and the shape of the granular media was converted into a digital height field to preview land scapes. Kazi et al. [2] developed a digital canvas (Sandcanvas) to simulate sand drawings. They studied the gestures on touchscreens and incorporated sand simulators to reproduce sand drawings on a virtual screen. Follmer et al. [3] proposed using granular materials to build Jamming user interface. They exploited computer-controlled jamming of granular particles to show its ability in controlling the stiffness of shape-changing objects. In this work, we will introduce dipping as a method to sense different types of granular media and use multiple types of granular media for interactive applications. If users would like to fully exploit the interaction experience or map different types of granular objects into the virtual world, existing solutions are not ready for users to interact with different granular objects. One of the missing features is to enable the input device to understand the type of granular media. To understand the different types of media, various methods have been developed for object recognition based on vision [15, 14], inertia data [16, 17, 18], acoustic signals [19, 20], or electromagnetic signals [21, 22] for various human-computer interactions. Vision-based methods are typical ways for object recognition. However, during the interaction with the granular media, the shape changes of the material, the occlusion or the varying lighting conditions may all affect the recognition results. Furthermore, vision-based solutions require capturing the interaction scene, which may also bring concerns to users in terms of privacy. Haptic recognition is also an important modality for object recognition in the field of computer-human interaction (CHI). Force or tactile signals are used in previous works with machine learning techniques. For example, tactile information has been used for learning the grasp signature for object recognition [23] or human-environment interactions [24]. Wu et al. [25] proposed to encode contact information for object recognition on an interactive fabric. VibEye [26] used the vibration passing through the object to determine the identity of an object. The vibrotactile information received by the finger was represented with a spectrogram and used for object recognition. Researchers also proposed to install proximity sensors on objects to recognize the grasping events [27]. However, it is costly to customize different objects by embedding sensors inside them. While humans naturally learn to recognize types of granular material through physical interactions, it is challenging for computers to classify them, especially in user interaction scenarios. Soil, a typical granular medium, is commonly studied in geoscience using the cone penetrometer test (CPT) to ascertain geotechnical properties and identify soil stratigraphy [28, 29]. Geologists increasingly employ force data collected from experiments along with machine learning techniques for tasks such as soil spatial mapping, recognition, and classification [30, 31]. However, traditional CPT is costly and time-consuming, requiring heavy and precise tools and strict perpendicular testing directions [32]. In contrast, DipMe provides an inexpensive and easily deployable alternative, allowing for testing with a casual dipping operation, making it more suitable for tangible interactions with particulate media. Machine learning has revolutionized object recognition in recent years, with techniques such as Time Series classification (TSC) [33] utilizing inception modules with a fully convolution network (FCN) [34], a robust temporal feature network (RTFN) [35], and a convolution neural network (CNN) [36]. Additionally, researchers have proposed Multivariate Time Series Classification (MTSC) [37]. These machine learning tools have also been leveraged for object recognition in the human-computer interaction community using multimodal signals [38, 39, 40, 41, 42]. We propose the use of a more advanced machine learning algorithm, based on the encoder part of the transformer model and multi-channel mechanism, to achieve autonomous recognition of force signal series, surpassing conventional algorithms in performance. Figure 1: DipMe: system overview (left) and applications (right). A user is allowed to dip the device into different granular media. We collect the force and torque signals and use machine learning techniques to recognize the type of the granular material from the multichannel time series data. With the tracked motion of DipMe and the recognized type of granular material, we demonstrate several applications including a virtual drawing interface and a virtual music instrument built with DipMe. In this work, we propose DipMe, a haptic solution for recognizing different types of granular media. Our insight is that humans may probe fingers into the granular particles to feel their haptic experience. Even if they cannot see the granular media, they can still recognize its type based on the force perceived by the fingers. To this end, we develop a device to simulate the probing process and equip the device with force-sensing ability. During the interaction, users are allowed to first probe to test the granular material. We employ modern machine learning techniques to encode the differences of the force signals collected from probing granular materials. We test our method in various interaction conditions and positive recognition results (92.78% accuracy with 10 users over 6 granular media) are obtained. With the capability of recognizing types of granular materials, we demonstrate several interactive applications to show the potential of DipMe in developing new tangible user interface with granular media. Our contributions can be summarized as follows: • We propose a new device to intelligently recognize the types of granular material based on haptic information; • We develop new tangible interactive applications that allows user to interact with different types of granular media."
https://arxiv.org/html/2411.08621v1,Thought Experiments in Design Fiction for Visualization,"Thought experiments are considered valuable tools in science, enabling the exploration of hypotheses and the examination of complex ideas in a conceptual, non-empirical framework. These thought experiments can be useful in design fiction for speculating future possibilities, examining existing and alternate scenarios in new ways or challenging current paradigms. In visualization, speculating future possibilities or exploring new ways of interpreting existing scenarios can provoke critical reflection and envision novel approaches. In this paper we present such thought experiments for visualization. We conceptualize and define a thought experiment to consist of a situation, a story, and a scenario. Situations are derived from different tools of thought experiments and visualization practice; a story is an AI-generated fiction based on the situation and the scenario is the grounding of the situation and story in visualization research. We present ten such thought experiments and demonstrate their utility in visualization by deriving critiques from them.","Design fiction, despite encountering challenges [Lindley and Coulton(2016a)], has gained increasing popularity within the HCI community [Blythe(2014), Markussen and Knutz(2013), Misra et al.(2023), Muller et al.(2020), Lindley and Coulton(2016b)] due to its ability to explore speculative futures and provoke critical reflection on emerging technologies. By employing narrative-driven prototypes and scenarios, design fiction facilitates an examination of potential societal impacts, ethical considerations, and user experiences that traditional methodologies may overlook. This approach not only enriches design research and practice by envisioning innovative possibilities but also stimulates discussions about future implications and challenges. The growing popularity of design fiction in HCI reflects a broader trend towards integrating speculative and critical thinking into the design process, enhancing the discipline’s ability to address complex, future-oriented issues. Thought experiments are an important method in scientific inquiry, though one which is less reliant on empirical observation and more inclined towards conceptual exploration and theoretical analysis [Brown and Fehige(2023)]. Rooted in the philosophical tradition, thought experiments serve as tools for explaining complex scientific concepts, probing the boundaries of existing theories, or generating novel hypotheses that sometimes are not feasible by using empirically backed methods. Unlike empirical experiments, which involve physical manipulation, data collection and observation of phenomena, thought experiments operate within the realm of imagination, allowing researchers to mentally construct hypothetical scenarios and extrapolate their implications [Sorensen(1992), Brown and Fehige(2023)]. By engaging in thought experiments, researchers can interrogate the logical coherence of their theories, identify potential flaws or inconsistencies, and stimulate innovative insights [Brown and Fehige(2023)]. Notably, thought experiments are not confined to any specific scientific discipline but find application across a broad spectrum of fields, from physics and mathematics to psychology and ethics [Horowitz and Massey(1991), Helm et al.(1985), Domenech-Pascual(2021)]. An instance of an application in AI and ethics is the trolley problem, which presents the following scenario: “A train is heading towards five people, who will be killed unless a switch is thrown to divert it towards a single person instead. Is it justifiable to save five lives by deliberately causing one death?"" [Thomson(1984)] These thought experiments are well-aligned with many methods pursued in science fiction [Schneider(2016), Wiltsche(2021), Seed(2008)]. A critical point of difference is as Wiltsche suggests, thought experiments target “ “physical understanding”, science fiction novels typically have “existential understanding” as their target"" [Wiltsche(2021)]. These thought experiments can be aligned and adapted to the methods of design fiction. Researchers have utilized these two areas interchangeably, occasionally characterizing and defining one in relation to the other. For instance, Design fiction has been thought of as “materialized thought experiments”[Bleecker(2022), Blythe et al.(2018)]. Blythe and Encinas report, “In Speculative Everything Dunne and Raby describe fictional worlds and thought experiments as methodological playgrounds. They note that their two favourite forms of thought experiment are the reductio ad absurdum (where a particular claim is taken to extremes in order to test it) and the counterfactual where a historical fact is changed to see what would have happened"" [Blythe et al.(2018), Dunne and Raby(2024)]. Thought experiments hold significant promise in enriching the landscape of visualization by offering a structured framework for exploring theoretical concepts [Chen et al.(2017)] and pushing the boundaries [Kosara(2007)] of traditional data visualization methods. When applied to visualization, these experiments serve as cognitive tools for conceptualizing abstract concepts and testing the limits of visual representation. By engaging in thought experiments, practitioners can probe the implications of different data interpretations [Hullman and Diakopoulos(2011)] and visualization techniques, uncovering insights that may not be readily apparent through empirical observation of users alone. Moreover, thought experiments can provide a means to interrogate the underlying assumptions and biases [Valdez et al.(2017), Mansoor and Harrison(2018)] inherent in visualization practices, fostering a deeper understanding of the foundations that underpin visualization. In this paper, we present thought experiments for visualization. We define thought experiments as consisting of a situation, story and scenario; where the situations are derived from the different tools of thought experiments and from reflecting upon visualization practice. A story is based upon the situation and is generated by AI and a scenario based on the situation and the generated story while being grounded in visualization research. We present 10 such thought experiments in this work. Finally, we derive three generic visualization critiques, following Kosara’s process and rules [Kosara(2007)] from these thought experiments."
https://arxiv.org/html/2411.08588v1,I Can Embrace and Avoid Vagueness Myself: Supporting the Design Process by Balancing Vagueness through Text-to-Image Generative AI,"This study examines the role of vagueness in the design process and its strategic management for the effective human-AI interaction. While vagueness in the generation of design ideas promotes diverse interpretations and prevents fixation, excessive vagueness can lead to scattered results. Designers attempt to use image search tools or generative AIs (e.g., Dall-E) for their work but often fail to achieve satisfactory results because the level of vagueness is not properly managed in these technologies. In this work, we identified how designers coordinate vagueness in their design process and applied key components of the process to the design of CLAY, an interactive system that balances vagueness through iterative prompt refinement by integrating the strengths of text-to-image generative AI. Results from our user study with 10 fashion designers showed that CLAY effectively supported their design process, reducing design time, and expanding creative possibilities compared to their existing practice, by allowing them to both embrace and avoid vagueness as needed. Our study highlights the importance of identifying key characteristics of the target user and domain, and exploring ways to incorporate them into the design of an AI-based interactive tool.","For designers, the notion of vagueness in design plays various roles. Designers’ initial design ideas often begin as uncertain and abstract mental images (Stacey et al., 1999; Eckert and Stacey, 2000; Howell, 2016). This vagueness allows for multiple interpretations of the design, such as providing a rich array of design strategies, fostering the transformation of ideas, and preventing early fixation that limits creativity to pre-existing concepts (Minneman, 1991; Fish and Scrivener, 1990; Tseng, 2018; Goel, 1995). However, when vagueness becomes too excessive, it can lead to scattered or directionless results due to the lack of clear constraints in the design development process (Stacey and Eckert, 2003). Therefore, effective management of vagueness—within its appropriate range, which tends to vary from person to person—is crucial to the success of a project (Steinmann, 1997; Leite et al., 2011; Howell, 2016). In this paper, we refer to this process as balancing vagueness. Vagueness facilitates the exploration of meaningful information (Tseng, 2018; Wu et al., 2022), allowing designers to explore a wide range of alternatives and gradually reduce vagueness as the designers uncover more specific information (Kuhlthau, 2004; Chowdhury et al., 2011). Image search tools (e.g., Google Image Search, Pinterest) are commonly used for this process by providing diverse search results that aid in the exploration of relevant information (Linder et al., 2014). However, even for specific queries, these tools often produce inaccurate or irrelevant results, complicating the search process and increasing the time required to find meaningful information (Spink et al., 2001; Woll, 2005; Liu et al., 2022). This can lead to an excessive increase in vagueness, which complicates design tasks and hinders effective decision-making (Whittemore and Yovits, 1973). On the other hand, the emergence of text-to-image generative AI offers the potential to quickly generate meaningful and high-quality images through prompts, bypassing the need for multiple search steps in the design process (Liu et al., 2022). However, achieving meaningful results requires specific, well-designed prompts, which may be beyond the capabilities of designers and limit the creative advantages designers can gain from exploring various possibilities when embracing vagueness (Ko et al., 2023). Therefore, it is important to explore the best ways to leverage the advantages of text-to-image generative AI within the vagueness balancing process. Previous research has explored the use of text-to-image generative AI to support the design process by refining prompts to better capture users’ creative intent (Wang et al., 2024; Brade et al., 2023; Mahdavi Goloujeh et al., 2024). However, these studies have largely overlooked the creative design aspect of exploiting or reducing vagueness. It is also important to balance the level of vagueness, avoiding making it too large or too small, otherwise, users are likely to stop using the technology. With these motivations, this study addresses the gap between the unique characteristics of design processes, the requirements of technology use, and user experience (expectation, satisfaction, tolerance) in two important ways: (1) by leveraging the advantages of text-to-image generative AI to mitigate the excessive vagueness that is often encountered with traditional image search tools, and (2) by helping designers gain valuable design strategies and support the vagueness balancing process. While advances in generative AI show great potential for creative work, its application remains limited because users often struggle to align their mental models with the prompts required by these AI models (Wu et al., 2022; Zamfirescu-Pereira et al., 2023). The HCI community has emphasized the importance of designing generative AI systems that align with users’ existing mental models (Weisz et al., 2024). Thus, our study seeks to contribute to the field of HCI by exploring how integrating text-to-image generative AI with the mental model of the vagueness balancing process can more effectively support the creative process. In this paper, we present CLAY, an interactive system designed to support the process of vagueness balancing through iterative prompt refinement, metaphorically likened to molding undefined ideas like clay into a complete design. Given the importance of vagueness in the fashion domain (Stacey et al., 1999), the design rationale of CLAY was developed through a formative study with six senior fashion professionals, all of whom had experience with text-to-image generative AI. We conducted interviews with the participants focusing on (1) how designers move from initial vagueness to a final design when creating a moodboard; (2) the exploration process designers undertake to gather meaningful information from image search tools while balancing vagueness, and the challenges they face; and (3) the difficulties they encounter when using text-to-image generative AI in creative processes. Based on the interview results, we identified three key insights: (1) the role of hierarchical and combinational structures in balancing vagueness, (2) the challenges of navigating both intentional and unintentional interactions during the vagueness balancing process, and (3) the difficulties of designing prompts for text-to-image generative AI in the context of vagueness balancing. From these insights, we derived three design goals for CLAY: (1) support designers in balancing vagueness through hierarchical and combinational structures; (2) facilitate the transition from hierarchical, unintended outcomes to combinational, intended ones; and (3) support prompting by maintaining vague prompts and helping to specify them to better balance vagueness. CLAY operates with the following main structure: the user starts by creating a vague prompt, explores the hierarchical results generated by the text-to-image generative AI to take advantages of vagueness, refines the prompt, and receives combination results from the AI to reduce vagueness. The main structure is divided into two sub-structures: (1) When the user enters a vague prompt related to the initial concept, CLAY generates sub-styles and sub-fashion elements in text format. The user refines the prompt, and CLAY creates a moodboard. (2) When the user enters a moodboard, CLAY suggests fashion elements that can be used in the design. The user then refines the prompt based on these suggestions, and CLAY generates corresponding designs. Through this iterative process, CLAY aims to support the user’s design workflow. We conducted a within-subjects study with 10 fashion professionals to compare CLAY with the baseline system (Dall-E). The results showed that (1) the steps in CLAY allowed users to control the results according to their intentions, leading to a reduction in the number of interactions; (2) CLAY helped expand the scope of the moodboard and even allowed users to modify its components, enabling them to create the desired moodboard more proactively; and (3) users applied the generated images more actively in their final designs, exploring by adding keywords that were not suggested by the system, and entering more specific keywords to create the desired images more actively. Based on these results, we discussed the importance and role of text-to-image generative AI in improving the clarity of vagueness. Specifically, (1) understanding the vagueness explore structure to let users actively embrace vagueness; (2) understanding the appropriate timing and prompt guidance to let users actively avoid vagueness; (3) providing users to select the system based on vagueness level; and (4) providing the real images in materializing the design. In summary, this paper makes the following contributions: • We identified the hierarchical combinational structure in the vagueness balancing process, the challenges posed by traditional image search tools and input prompting, and how these issues can be addressed using text-to-image generative AI. • We developed CLAY, a system that efficiently supports the design process by incorporating the vagueness balancing process into text-to-image generative AI. • Our process confirmed that users can embrace or avoid vagueness as needed, and we discussed the possible directions and design implications for text-to-image generative AI based on these findings. Our study contributes to expanding the current research on creative support tools (CST) by demonstrating how text-to-image generative AI can be applied to creative processes that require balancing vagueness. We suggest that future systems designed with these characteristic processes in mind can be more effectively applied to similar domains. Identifying key elements (e.g., vagueness in a fashion domain in our case) that can be considered important for characterizing domain users, tasks, or the domain itself, and extending the concept to design elements for the interactive computer system would be important for making the system more meaningful and adaptable to domain users."
https://arxiv.org/html/2411.08583v1,An Empirical Examination of the Evaluative AI Framework,"This study empirically examines the “Evaluative AI” framework, which aims to enhance the decision-making process for AI users by transitioning from a recommendation-based approach to a hypothesis-driven one. Rather than offering direct recommendations, this framework presents users pro and con evidence for hypotheses to support more informed decisions. However, findings from the current behavioral experiment reveal no significant improvement in decision-making performance and limited user engagement with the evidence provided, resulting in cognitive processes similar to those observed in traditional AI systems. Despite these results, the framework still holds promise for further exploration in future research.","In recent years, AI has gained substantial attention for their increasingly sophisticated performance in various applications (Barredo Arrieta et al., 2020; Rong et al., 2022; Albrecht, 2016; MacCarthy, 2019). However, their significant limitation compared to simpler methods is their commonly opaque “black box” nature, making it difficult to understand how inputs generate outputs (Guidotti et al., 2018). This is particularly problematic in high-stakes areas like medicine, economics, or law, where understanding the decision-making process is crucial (Rudin, 2019). As a result, the lack of transparency and comprehensibility often leads to distrust and underreliance among potential users, despite the accuracy of these decision-support systems (Jacovi et al., 2021; Mahmud et al., 2022; Zhang et al., 2020). This challenge has spurred the development of several explanatory methods and a surge in interest in Explainable AI (XAI). Initially, it was hoped that XAI would enhance understanding and trust in AI models, thereby improving decision-making quality among users. However, as summarized by recent studies (Lai et al., 2023b; Schemmer et al., 2022; Vasconcelos et al., 2023; Bertrand et al., 2023; Rogha, 2023; Schemmer et al., 2023), the results are mixed. While XAI might indeed improve understanding (Ribeiro et al., 2018), higher transparency can make models less comprehensible (Poursabzi-Sangdeh et al., 2021). Explanations can improve subjective perception (Bertrand et al., 2023), but also might increase cognitive load (You et al., 2022; Herm, 2023; Ghai et al., 2020) and reduce efficiency (Lai et al., 2023b). This has led to a situation where users often engage superficially with explanations and develop an overreliance on AI (Chromik et al., 2021; Buçinca et al., 2021; Bansal et al., 2021; Chen et al., 2023), shifting from the original problem of underreliance. Given that AI is not infallible and often makes better decisions than humans (Mnih et al., 2015; Nori et al., 2023), a calibrated level of trust is essential for a trade-off that encourages user to rely more on AI, while avoiding blind trust (Wischnewski et al., 2023; Vered et al., 2023). To address the issue of overreliance, various strategies have been developed, such as cognitive forcing functions (Buçinca et al., 2021) and user-adapted, selective explanations (Lai et al., 2023b). This paper discusses another approach to improve human-AI interaction: the “Evaluative AI” framework proposed by Miller (2023). Critiquing the limited success of existing XAI methods, Miller argues that these methods do not align well with the cognitive processes involved in decision-making. He suggests a paradigm shift from recommender-driven systems to a hypothesis-driven approach, based on the Data/Frame Theory (Klein et al., 2007) and abductive reasoning (Peirce, 2009), to better support decision-makers in exploring hypotheses rather than receiving direct recommendation by AI. This study empirically investigates the effectiveness of the proposed framework in enhancing decision-making by examining its impact on performance, efficiency, and subjective perception. The focus is on one specific element of the framework: offering evidence for and against potential option without providing direct recommendations. Rather than giving a recommendation and explaining it, the framework refrains from making any recommendations. Instead, it offers evidence supporting and opposing each option, which is only displayed if requested by the decision-maker. This studies research question is: • RQ: Can a decision support system that offers evidence for and against potential options, without providing direct recommendations, improve the decision-making process? Currently, only three studies directly apply Miller’s framework: Castelnovo et al. (2023) developed a contrastive explanation technique for ranking classifications, and Le et al. (2024b) created a tool for image classification, though neither has undergone empirical testing. During the development of the present study, an empirical evaluation by Le et al. (2024a) was conducted, comparing a hypothesis-driven approach with recommendation-driven and explanation-only methods. They found that the hypothesis-driven approach improved decision quality without increasing decision time, and participants cognitively engaged with the evidence, thereby considering the uncertainty of the underlying models. This current study differs in several respects. Compared to Le et al. (2024a), the task here is significantly more objective and realistic for participants. While their task involved classifying a subjective house price into low, medium, or high using six features, the task in this study is to estimate whether an income is above or below the median based on 20 features. This study provides a more detailed picture, as it includes a control group without any AI assistance and a group that receives both recommendations and evidence. Another difference lies in the incentive design; in this study, more incentive per task was offered to simulate a higher-stakes situation. In a pretest, it was found that evidence presented in bar chart format (as used in Le et al. (2024a)) was not well understood, so textual descriptions of the evidence were added here. Lastly, in Le et al. (2024a) experiment, low-level evidence was shown by default, which could potentially lead to anchoring effects and influence the decision-making process. In this study, no evidence is shown by default, allowing decision-makers the freedom to choose and gives further opportunities for behavioral analysis. The results of the present study paint a different picture than those of Le et al. (2024a). Overall, the findings indicate that the “Evaluative AI” framework in this experiment did not improve decision-making performance. They also reveal that participants engaged only superficially with the provided pro and con evidence, despite all AI systems influencing the decision-making processes leading potentially to cognitive offloading."
https://arxiv.org/html/2411.08229v1,Virtual Steps: The Experience of Walking for a Lifelong Wheelchair User in Virtual Reality,"Many people often take walking for granted, but for individuals with mobility disabilities, this seemingly simple act can feel out of reach. This reality can foster a sense of disconnect from the world since walking is a fundamental way in which people interact with each other and the environment. Advances in virtual reality and its immersive capabilities have made it possible to enable those who have never walked in their life to “virtually” experience walking. We co-designed a VR walking experience with a person with Spinal Muscular Atrophy who has been a lifelong wheelchair user. Over 9 days, we collected data on this person’s experience through a diary study and analyzed this data to better understand the design elements required. Given that they had only ever seen others walking and had not experienced it first-hand, determining which design parameters must be considered in order to match the virtual experience to their idea of walking was challenging. Generally, we found the experience of walking to be quite positive, providing a perspective from a higher vantage point than what was available in a wheelchair. Our findings provide insights into the emotional complexities and evolving sense of agency accompanying virtual walking. These findings have implications for designing more inclusive and emotionally engaging virtual reality experiences.","Walking is more than a mere means of locomotion; it is an action that enables freedom, exploration, and independence. Certain conditions and disorders can prevent individuals from walking, including genetic disorders and congenital conditions such as Cerebral Palsy, Muscular Dystrophy, or Spinal Muscular Atrophy. These conditions can affect the development of muscles, bones, or the nervous system from birth and hinder the ability to walk. Walking produces a range of sensory stimuli: proprioceptive feedback generated by limb movement [16], vestibular feedback from head motion [44], optical flow from visual perception as they move [6], and auditory feedback produced by their footsteps [52]. Emulating these sensations is a complex task that has been the target of extensive research, especially using virtual reality. Notable methods include employing specialized equipment such as omnidirectional treadmills [10, 22], foot-support motion platforms [23], and walking spheres [33]. VR technology has been used to simulate walking, providing presence and embodiment [36, 43, 25]. However, previously proposed VR walking experiences were mainly focused on reproducing the experience of a person who is able, or at some point in the past, was able to walk. This focus is not necessarily aligned with the desires of those users who have never experienced standing and walking. Even though these users have never walked, they have developed a mental model of how it feels to walk by observing others walk and interacting with them. Due to the potential differences in these mental models, current systems may not respond in ways that users who have never walked anticipate [38]. To address this gap, it is crucial to develop VR walking simulations that specifically cater to the unique mental models of individuals who have never experienced walking, ensuring a more inclusive and effective user experience. Another limitation of prior VR walking experiences is that they rarely consider the interaction limitations that usually accompany mobility limitations. Users who never walked before may also face challenges using the input devices and interaction techniques offered in commercial VR devices because they rely on fine hand and head coordination. The lack of appropriate interaction design restricts the access of individuals with congenital motor and mobility impairments to walking experiences in VR. In this study, we interactively co-designed a VR walking experience with a person with Spinal Muscular Atrophy who is a lifelong wheelchair user, delving into their perceptions and emotional responses to this novel sensation. We explored different design alternatives including auditory feedback, visual representation, movement dynamics, walking speed, interaction modality, and virtual environment. Instead of concentrating on rehabilitation or therapeutic uses, our research explores the design parameters of a VR walking experience tailored to users who have no prior physical walking experience. The needs of this particular audience can be quite distinct from those of the general population because they lack a physical walking experience to use as a reference, except for mobility aided by devices like wheelchairs. Our contributions are as follows: 1. An initial set of features and parameter values for VR walking applications to individuals with congenital motor and mobility impairments. 2. Qualitative insights into the emotions and perceptions of a user with congenital motor and mobility impairments while experiencing walking through VR. Through our study, we found an initial set of factors that affect walking experiences in VR for users who have never walked and compiled how different strategies of walking sensation emulation can impact the user experience. This contribution can inform the design of next-generation VR experiments that target audiences with motor and mobility disabilities, broadening accessibility in VR and providing a novel experience to users who never walked in their lives, with potential psychological and cognitive impacts."
https://arxiv.org/html/2411.08228v1,Virtual Buddy: Redefining Conversational AI Interactions for Individuals with Hand Motor Disabilities,"Advances in artificial intelligence have transformed the paradigm of human-computer interaction, with the development of conversational AI systems playing a pivotal role. These systems employ technologies such as natural language processing and machine learning to simulate intelligent and human-like conversations. Driven by the personal experience of an individual with a neuromuscular disease who faces challenges with leaving home and contends with limited hand-motor control when operating digital systems, including conversational AI platforms, we propose a method aimed at enriching their interaction with conversational AI. Our prototype allows the creation of multiple agent personas based on hobbies and interests, to support topic-based conversations. In contrast with existing systems, such as Replika, that offer a 1:1 relation with a virtual agent, our design enables one-to-many relationships, easing the process of interaction for this individual by reducing the need for constant data input. We can imagine our prototype potentially helping others who are in a similar situation with reduced typing/input ability.","Conversational AI is fast proving to be a valuable tool in our day-to-day lives, providing support, companionship, and stress relief (Sharma et al., 2023; Meng and Dai, 2021; Qi et al., 2021; Medeiros et al., 2021). It is now plausible to consider the use of this technology as a means of mitigating social isolation (Gadbois et al., 2022; Søraa et al., 2020; De Gennaro et al., 2020; Shum et al., 2018). However, current conversational AI technologies have largely catered to the general audience. The unique needs of people with mobility disabilities, who may not be able to leave home to engage in social interactions, or those who have motor impairments, that make it challenging to interact with computers, have not been the primary focus of their design, despite clear benefits. Unlike mobility disability, which limits movement, motor disability affects muscle control and impacts mobility, among other things. Our work focuses on people with motor challenges, addressing the physical barriers that impact interactions with conversational AI as a socialization aid. The motivation and design decisions for this work originate from the firsthand experiences of the primary author, who lives with Spinal Muscular Atrophy (SMA), a severe motor neuron disease. She uses a wheelchair for mobility and her right thumb for all computer interactions. Although AI-powered real-time conversations and question-answering offer substantial potential, their accessibility is limited. For instance, Taheri has a deep interest in learning a new language. Attending in-person Italian lessons is difficult for her. Interacting with a conversational AI could provide a dynamic and real-time experience similar to talking and learning from a language tutor but one that tirelessly provides explanations and practice, is available any time of day, is not impacted by personal circumstances and moods, and in general, adjusts to her needs. However, existing conversational AI systems constantly require manual information input to make the agent act in a desired role, making them quickly unusable for Taheri, as input with one thumb is time-consuming and exhausting. This inspired us to focus on the potential of conversational AI for her and for individuals in similar situations, by smoothing out the onboarding process of engaging with chatbots by minimizing user input. In this work, we introduce a prototype called Virtual Buddy, also referred to as V-Buddy, that focuses on reducing friction in the onboarding process of interaction with conversational agents. V-Buddy offers the ability to create multiple personas, optionally via pre-filled templates with easy-to-select options, each giving the AI agent a different role and personality, to assist focused or topic-relevant conversations. The personas created on V-Buddy remain usable across interaction sessions without requiring repeated or further information from the user over time. However, if the user so chooses, they are editable allowing the user to modify the persona as needed. With multiple available personas, V-Buddy also enables supporting one-to-many relationships (e.g., one user can have many different AI buddies in the same app) as opposed to the one-to-one relationship the current conversational AI systems offer."
https://arxiv.org/html/2411.08217v1,WristSonic: Enabling Fine-grained Hand-Face Interactions on Smartwatches Using Active Acoustic Sensing,"Hand-face interactions play a key role in many everyday tasks, providing insights into user habits, behaviors, intentions, and expressions. However, existing wearable sensing systems often struggle to track these interactions in daily settings due to their reliance on multiple sensors or privacy-sensitive, vision-based approaches. To address these challenges, we propose WristSonic, a wrist-worn active acoustic sensing system that uses speakers and microphones to capture ultrasonic reflections from hand, arm, and face movements, enabling fine-grained detection of hand-face interactions with minimal intrusion. By transmitting and analyzing ultrasonic waves, WristSonic can distinguish a wide range of gestures—such as tapping the temple, brushing teeth, and nodding—through a Transformer-based neural network architecture. This approach achieves robust recognition of 21 distinct actions with a single, low-power, privacy-conscious wearable. Through two user studies with 15 participants in controlled and semi-in-the-wild settings, WristSonic demonstrates high efficacy, achieving Macro F1-scores of 93.08% and 82.65%, respectively.","Hand-face interactions are essential for a wide range of everyday tasks, from adjusting glasses and swiping the chin to scratching an itch on the face and covering the mouth while coughing. These interactions provide critical insights into the user’s habits, behaviors, intentions, and expressions. Therefore, the ability to continuously track these hand-face interactions can significantly advance our understanding of human activities and enhance the interaction experience on wearable devices. However, tracking these interactions with existing wearable sensing systems is challenging. Hand-face interactions typically involve complex postures and movements of the hands, arms, and corresponding face deformations. To accurately recognize a wide range of detailed hand-face interactions, a sensing system must capture information from these body parts simultaneously, which poses a significant challenge for wearables. Wrist-mounted devices generally only track hand or arm movement, while head-mounted wearables primarily track facial or head movements. Achieving reliable performance often requires multiple sensors on both the head and arm to recognize hand-face touch behaviors (Li et al., 2023). Additionally, many of these systems rely on egocentric cameras (Weng et al., 2021), which are energy-intensive and raise privacy concerns for wearable applications. Therefore, there is a clear need for a single, minimally obtrusive, low-power, and privacy-aware wearable device capable of recognizing a wide range of hand-face interactions involving the arms, hands, and face. Given these limitations, we ask a straightforward research question: Can we enable fine-grained hand-face interaction by simultaneously tracking subtle hand movements and the corresponding face deformations that occur during hand-face interactions? An affirmative answer to this question would provide a significant advantage, enabling smartwatches to facilitate seamless interaction through various gestures and activities. Furthermore, if such a solution operates with low power consumption and maintains privacy, it could potentially integrate smoothly with existing systems and drive wider adoption. In this paper, we introduce WristSonic, a wristband-based solution that tracks a broad range of fine-grained hand-face interactions by monitoring arm, head, and face movements through active acoustic sensing. Equipped with speakers and microphones, various wearable devices have been shown capable of tracking body movements via active acoustic sensing (Lee et al., 2024; Mahmud et al., 2024b; Li et al., 2022; Zhang et al., 2023; Wang et al., 2018; Jin et al., 2021). Leveraging this capability, WristSonic uses ultrasonic waves to model both hand movement and subtle face deformations, allowing for simultaneous tracking of hand-face interactions. WristSonic operates with just two pairs of off-the-shelf speakers and microphones—one facing outward from the hand and the other directed towards the body—to transmit C-FMCW (Wang et al., 2018) encoded ultrasonic waves and capture their reflections from the arm, head, and face, as the hand interacts with the face. Using a Transformer-based neural network architecture (Dosovitskiy, 2020), we process these acoustic reflections to infer various hand-face gestures, daily activities, and head motions. This includes gestures such as touching the earlobe, tapping the temple, drinking, brushing teeth, nodding, and shaking the head, among 21 distinct actions. To evaluate the efficacy of WristSonic, we conducted two user studies with 15 participants: one in a controlled lab setting with 10 subjects and another in a naturalistic semi-in-the-wild environment with 5 participants. WristSonic demonstrates robust performance across different conditions, achieving macro F1-scores of 93.08% and 82.65% in lab and semi-in-the-wild settings, respectively. Our contributions in this work are summarized as follows: • We present the first single smartwatch-based system leveraging active acoustic sensing to distinguish a wide range of fine-grained hand-face interactions by simultaneously sensing information from the hand and face. • We implemented a self-attention-based architecture to model the received acoustic reflections, enabling the recognition of 21 distinct hand-face interactions. • We conducted two user studies to evaluate the system’s performance across various scenarios and environmental conditions, demonstrating its performance, robustness, and usability."
https://arxiv.org/html/2411.08123v2,"Exploring the Role of LLMs for Supporting Older Adults:
Opportunities and Concerns","In this position paper, we explore the potential role of large language models (LLMs) in enhancing healthcare and quality of life for older adults. We explore some of the existing research in HCI around technology for older adults and examine the role of LLMs in enhancing it. We also discuss the digital divide and emphasize the need for inclusive technology design. At the same time, we also surface concerns regarding privacy, security, and the accuracy of information provided by LLMs, alongside the importance of user-centered design to make technology accessible and effective for the elderly. We show the transformative possibilities of LLM-supported interactions at the intersection of aging, technology, and human-computer interaction, advocating for further research and development in this area.","With the aging global population, there is an urgent need for technology support—both medical and in terms of quality of life—for older adults (Mahishale, 2015). Within HCI, researchers have studied the design of technology to better meet the evolving needs to older adults (Pradhan et al., 2020; Pang et al., 2021). The COVID-19 pandemic has surfaced the need for technology as a basic necessity not just for entertainment and staying connected but, for more critical needs such as healthcare (Xie et al., 2021). HCI researchers have made significant technological advances to address diverse needs of older adults and their caregivers. From the development of intuitive user interfaces that accommodate declining sensory and motor functions to the creation of social robots that mitigate feelings of loneliness and isolation (Kubota et al., 2022; Mihajlov et al., 2015). A few instances of how HCI has worked to enhance the quality of life for the elderly include wearable technology that tracks health metrics in real-time, easily accessible digital platforms that promote social interaction, and smart home technologies that let senior citizens live independently for longer (Chen et al., 2023a; Ollevier et al., 2020; Lee, 2022). In light of the groundbreaking advances in artificial intelligence provided by the recent large language models (gpt, 2024; bar, 2024), we deem it timely to revisit the past HCI research on technology design for older adults and view it through a new lens of LLMs. Because of their large capacity for producing, comprehending, and interacting with human language, LLMs have the potential to transform how we interact with technology (Hämäläinen et al., 2023). Modern LLMs like OpenAI’s GPT-4 (gpt, 2024) and Google’s Bard Gemini (bar, 2024) exhibit enhanced performance over previous models due to significant improvements in architecture, training datasets, and learning algorithms. These advancements result in a better understanding of context, a more accurate generation of text, and an improved ability to handle complex queries, making them more effective and versatile in a wide range of applications (Naveed et al., 2023). Although there are issues with LLMs that we need to address too (e.g inaccurate or irrelevant information (Li et al., 2023; Rambhatla, [n. d.])), it is worthwhile to explore what value they might contribute to this domain. The goal of this article is to examine the various ways that LLM-enabled interactions can improve healthcare and quality of life for older adults. By revisiting previous studies and projects in HCI with a focus on the integration of LLM technologies, we aim to highlight the transformative possibilities that lie at the intersection of aging, technology, and human-computer interaction. The premise behind this article is that the combination of HCI research and LLM technologies has the potential to produce new advancements in assistive technologies. The potential uses of LLMs to improve the lives of older adults are numerous, ranging from individualized virtual assistants that provide company and assistance with everyday tasks to advanced platforms that offer specialized health advice and cognitive stimulation."
https://arxiv.org/html/2411.08603v1,Generalized Pose Embeddings for Training In-the-Wild via Analysis-by-Synthesis,"Modern pose estimation models are trained on large, manually-labelled datasets which are costly and may not cover the full extent of human poses and appearances in the real world. With advances in neural rendering, analysis-by-synthesis and the ability to not only predict, but also render the pose, is becoming an appealing framework, which could alleviate the need for large scale manual labelling efforts. While recent work have shown the feasibility of this approach, the predictions admit many flips due to a simplistic intermediate skeleton representation, resulting in low precision and inhibiting the acquisition of any downstream knowledge such as three-dimensional positioning. We solve this problem with a more expressive intermediate skeleton representation capable of capturing the semantics of the pose (left and right), which significantly reduces flips. To successfully train this new representation, we extend the analysis-by-synthesis framework with a training protocol based on synthetic data. We show that our representation results in less flips and more accurate predictions. Our approach outperforms previous models trained with analysis-by-synthesis on standard benchmarks.","Building datasets for pose estimation of humans and animals is costly and remains challenging in terms of capturing the full diversity in pose and appearance of the real world. Some works seek to utilize synthetic humans but remain at a gap from real world imagery. This inhibits the ability of labelled datasets—both synthetic and real—to generalize and motivates the need for a class of methods that can learn from un-labelled video data in-the-wild. Analysis-by-synthesis is the idea of including a depiction model in order to define a loss or cost back to the input image—enabling a training signal directly from unlabelled images. Such an approach, within a deep learning human pose estimation setting has been demonstrated in Jakab et al. (2020), where they use an intermediate pictorial skeleton representation of the pose. While their representation of a pose can be trained in an unsupervised way, it consists of only a single channel (\mathbb{R}^{1\times W\times H}) and does not represent the semantics of body parts—leading to significant flips between left and right, front and back in both the pose predictions and renderings of the person, as can be seen in Figure 3. We solve this problem with a new multi-channel pose representation. However, we found this higher capacity model sensitive to divergence during training. To address this problem, we introduce synthetic humans to pre-train our pose and rendering model, providing a better conditioning for further fine-tuning using analysis-by-synthesis in the wild. Another way to look at our approach is to consider analysis-by-synthesis a tool for bridging the reality gap of models trained with synthetic data. On the standard benchmark Human3.6M Ionescu et al. (2014), we outperform the prior work of Jakab Jakab et al. (2020) with an MSE of 10.39, compared to the MSE of 14.46 for the baseline. We also gathered specialized data of a target subject and measured whether fine-tuning on this data would improve accuracy at run-time. On the Human3.6M benchmark, this refinement step further reduces the MSE to 6.62, outperforming the baseline by a large margin. Additionally, we investigated our framework further to answer whether a 3D pose representation could be trained end-to-end. We also show that our approach can generalize to other skeletal structures such as animals, where we qualitatively improve the accuracy of the 3D pose predictions compared to the work of Borer Borer et al. (2021a)."
https://arxiv.org/html/2411.07869v1,"Reexamining Technological Support for Genealogy Research, Collaboration, and Education","Genealogy, the study of family history and lineage, has seen tremendous growth over the past decade, fueled by technological advances such as home DNA testing and mass digitization of historical records. However, HCI research on genealogy practices is nascent, with the most recent major studies predating this transformation. In this paper, we present a qualitative study of the current state of technological support for genealogy research, collaboration, and education. Through semi-structured interviews with 20 genealogists with diverse expertise, we report on current practices, challenges, and success stories around how genealogists conduct research, collaborate, and learn skills. We contrast the experiences of amateurs and experts, describe the emerging importance of standardization and professionalization of the field, and stress the critical role of computer systems in genealogy education. We bridge studies of sensemaking and information literacy through this empirical study on genealogy research practices, and conclude by discussing how genealogy presents a unique perspective through which to study collective sensemaking and education in online communities.","Genealogy, the activity of studying family history and lineage, has a long history itself. Genealogists search for information regarding ancestors among surviving documents, discover history of a family through traces of individual family members, and organize articles and pedigree diagrams indicating parental relations of the family. Emerging genealogy websites such as Ancestry.com and FamilySearch.org has transformed traditional genealogy research since the early 2000s. These sites granted users the capability to access millions of digitized historical documents through the Internet, which attracted an increasing number of users who are interested in exploring histories of their families. Featuring the idea of discovering common ancestors and making connections to living relatives around the world, genealogy websites also allowed users to build family trees and connect them to others’ trees, presenting a unique form of massive-scale online collaboration (De Groot, 2022). These dramatic changes reverberated throughout the genealogy community, eliciting mixed reactions. In 1999, Elizabeth Shown Mills, president of the American Society of Genealogists and editor of National Genealogical Society Quarterly, raised concerns that “family history’s progress as an intellectually valuable pursuit” could be undermined by “web-grown genealogists” who were “largely unschooled in research principles” but empowered by technologies to broadcast their genealogy work (Mills, 1999). To ensure the quality of the genealogical research product, senior members of this community sought to formalize the desired research processes into guidelines and standards (Society, 2018b; for Certification of Genealogists, 2023). However, these standards failed to gain traction outside of professional societies, and concerns of widespread genealogy research errors proved prescient. Indeed, as the spread of misinformation became a global concern, the genealogist community has faced its own version of this problem. A 2014 study (Willever-Farr and Forte, 2014) suggested that there was a considerable number of genealogy hobbyists, referred by serious genealogists as “clickologists” and “half-researchers”, who built and connected to family trees on genealogy websites without knowing the proper genealogy research practices. Such behavior brought disturbances to the genealogist community: People found their close relative became a member of many strangers’ families, and “research hints,” which are algorithmic recommendations of relevant information about an ancestor, would pick up inaccurate information from family trees and mislead more users. In recent years, an explosion of interest in genealogy has only intensified the problem. As of 2019, FamilySearch claimed to be the home of the world’s largest family tree, built collectively by its users and containing more than 1.2 billion ancestors (Morton, 2019). Surprisingly, there has been minimal follow-up research from an human-computer interaction (HCI) or CSCW perspective on this challenge in the past decade. Recognizing the extensive growth of genealogical business fueled by the advancement of new technologies such as customized search engine (Jiang et al., 2019), genealogical data visualization and analysis (Liu et al., 2017; Borges, 2019; Burch et al., 2021; Wang et al., 2023), DNA testing (Nash, 2004; Regalado, 2019; Calafell and Larmuseau, 2017; Stallard and De Groot, 2020; Glynn, 2022), and artificial intelligence (AI) based handwriting recognition (Nion et al., 2013; Kennard, 2018; Blomqvist et al., 2022), face recognition (Mohanty et al., 2020; Robinson et al., 2021) and family tree network exploration (Malmi et al., 2017, 2018), we believe there is an urgency to reexamine the current state of technology-supported genealogy research. In this paper, we attempt to fill the research gap by conducting a qualitative study of current genealogy practices. We extend prior work with new analysis comparing amateur and expert genealogists, investigating the impact of new technologies, and understanding how newcomers of this community learn to become competent researchers on the Internet. Specifically, our study is guided by the following research questions: (1) How has genealogists’ research evolved with recent technology advances? (2) What are amateur genealogists’ research practices compared with established standards? (3) How do amateur genealogists learn online, and how can technology better support this process? We conducted a semi-structured interview study with 20 genealogists, recruiting 10 experts with extensive professional research experience and 10 amateurs who are relatively new to this field. To the best of our knowledge, this study is the first that considers the role of standardization, professionalism, and credentials in the genealogist community, and to conduct a side-by-side comparison of research practices between genealogists with different levels of expertise. This work presents a more comprehensive description of genealogists’ research processes and reveals several concerning phenomena. We also propose the design recommendations based on our findings, and discuss the potential of genealogy as a unique lens through which to study a range of important problems in the field of HCI."
https://arxiv.org/html/2411.07451v1,"Optimizing Data Delivery: Insights from User Preferences on Visuals, Tables, and Text","In this work, we research user preferences to see a chart, table, or text given a question asked by the user. This enables us to understand when it is best to show a chart, table, or text to the user for the specific question. For this, we conduct a user study where users are shown a question and asked what they would prefer to see and used the data to establish that a user’s personal traits does influence the data outputs that they prefer. Understanding how user characteristics impact a user’s preferences is critical to creating data tools with a better user experience. Additionally, we investigate to what degree an LLM can be used to replicate a user’s preference with and without user preference data. Overall, these findings have significant implications pertaining to the development of data tools and the replication of human preferences using LLMs. Furthermore, this work demonstrates the potential use of LLMs to replicate user preference data which has major implications for future user modeling and personalization research.","As data and large language models (LLMs) continue to grow in prominence, it is crucial to identify the most effective ways to present data outputs, as the format, whether chart, table, or text, significantly influences how users engage with and interpret information (Tufte and Graves-Morris, 1983; Few, 2004). With datasets becoming larger and more complex, visualizations are increasingly necessary to help users digest the information effectively (Godfrey et al., 2016). The expansion of LLM use in data analysis adds another layer, making it essential to understand when these models should present different output formats. Moreover, individuals have varying preferences for data representation, driven by their unique characteristics, such as experience with data analysis and visualization, age, and work experience. This paper investigates these preferences, exploring how user characteristics shape their choice of data outputs, and how LLMs can adapt to deliver more personalized and intuitive results (Brown et al., 2020). Ultimately, by dynamically tailoring outputs based on user backgrounds, LLMs can offer a more customized and effective experience, helping users better understand and utilize data. Summary of Main Contributions. The key contributions of this work are as follows: • A comprehensive user survey and methods design. We outline the key components of the Amazon Mturk survey, detailing the respondent population, survey setup, specific user and data-related questions, and the instructions provided to participants. • An analysis of general data output preference results. The first research question examined the general population’s preferred data output for a given question, aiming to establish a baseline for data preferences without considering user characteristics. • An overview of data output preferences when organized by personal user characteristics. The 2nd research question explored how a user’s personal characteristics influence their data output preferences, focusing on experience with data analysis and visualization and their age. • An overview of data output preferences when organized by work experiences. The third research question explored how work experience, including industry and role, influences users’ data output preferences. • A comparison between human and GPT preferences. We used GPT to see if it could predict the human preference data we received throughout the study"
https://arxiv.org/html/2411.07441v1,Automatically Detecting Online Deceptive Patterns in Real-time,"Deceptive patterns (DPs) in digital interfaces manipulate users into making unintended decisions, exploiting cognitive biases and psychological vulnerabilities. These patterns have become ubiquitous across various digital platforms. While efforts to mitigate DPs have emerged from legal and technical perspectives, a significant gap in usable solutions that empower users to identify and make informed decisions about DPs in real-time remains. In this work, we introduce AutoBot, an automated, deceptive pattern detector that analyzes websites’ visual appearances using machine learning techniques to identify and notify users of DPs in real-time. AutoBot employs a two-staged pipeline that processes website screenshots, identifying interactable elements and extracting textual features without relying on HTML structure. By leveraging a custom language model, AutoBot understands the context surrounding these elements to determine the presence of deceptive patterns. We implement AutoBot as a lightweight Chrome browser extension that performs all analyses locally, minimizing latency and preserving user privacy. Through extensive evaluation, we demonstrate AutoBot’s effectiveness in enhancing users’ ability to navigate digital environments safely while providing a valuable tool for regulators to assess and enforce compliance with DP regulations.","Deceptive patterns (DPs) are design elements that manipulate users into making unintended decisions while interacting with interfaces on applications or websites. These patterns exploit cognitive biases and psychological vulnerabilities to influence user behavior, often in ways that benefit the service provider at the user’s expense. They have become increasingly prevalent across digital environments, significantly impacting user experiences on social media, mobile devices, cookie consent banners, and even gaming applications [33]. Prior works have shown that DPs can result in financial loss [42], privacy breaches [8], or exploitation of vulnerable groups, including children [46]. A typical example of a DP is the intentionally convoluted subscription cancellation process, where users must navigate through multiple obscure options to terminate a service, as seen on platforms like www.dailymail.co.uk. Efforts to mitigate the effect of DPs have emerged from the policy and the technical sides. Regarding policy, regulations like CPRA [15] and the GDPR [4] have released guidance on deceptive patterns. On the technical front, researchers have explored classifying the existing types of deceptive patterns [11, 21, 33, 48, 44, 7, 16, 39, 32], assessing their effectiveness [30], and compiling different deceptive pattern cases [12]. Despite these ongoing efforts, deceptive patterns continue to pose significant challenges for both users and regulators. Users frequently encounter DPs in their digital interactions. For instance, sponsored advertisements are strategically placed at the top of search results, creating a false impression that they are the most relevant or popular items. Furthermore, regulators often lack the tools to assess and enforce compliance with regulations concerning deceptive patterns effectively. This technological gap leaves users vulnerable to sophisticated DPs. In this work, we propose a new paradigm to bridge this gap and improve the usability of online websites by developing a solution to detect deceptive patterns and automatically warn users in real time. Our solution makes users aware of deceptive patterns as they browse and enables regulators to assess and evaluate online services for deceptive patterns. Achieving this objective requires us to overcome several challenges. First, the lack of a standardized taxonomy and usable datasets complicates classification efforts. Second, the vast diversity of web designs and the ingenuity of designers in creating new deceptive techniques have led to a trade-off between scalability and accuracy in detection methods. For instance, existing solutions like disabling third-party cookies or using filter lists have proven inadequate in preventing user tracking, as highlighted by Chen et al. [16]. Third, user-friendly solutions must operate with minimal infrastructure to be practical for widespread adoption. We first create a standardized taxonomy of deceptive patterns to realize our objectives, building upon existing taxonomies. Leveraging this standardized taxonomy, we then built AutoBot, an automated, deceptive pattern detector that analyzes the website’s visual appearances, uses machine learning tools to identify deceptive patterns, and brings deceptive patterns to users’ attention in real-time. Specifically, AutoBot relies on an invariant behavior of deceptive patterns: the visual representation and how users perceive them. Using this invariant behavior, AutoBot applies a two-staged pipeline that identifies the deceptive patterns present in the website’s screenshot. First, AutoBot analyzes the website screenshot, identifying interactable elements using visual analysis and extracting textual features without relying on the HTML structure. We note that here, we rely on the insight that deceptive patterns work by manipulating what the users perceive; thereby, analyzing their visual appearance provides a comprehensive signal to be able to identify them. Next, AutoBot leverages a custom language model to understand the context surrounding these elements, including color, font size, and text, to determine the presence of deceptive patterns. We created an annotated dataset for deceptive patterns and designed a new training paradigm to fine-tune language models. We also note here that empirically, we have found that off-the-shelf language models like Gemini and ChatGPT often fail to detect deceptive patterns from screenshots, as shown in Figure 2. We showcase the usability of AutoBot by building a Chrome browser extension that (1) detects the deceptive patterns in real-time by performing the complete analysis locally and (2) annotates the current webpage to warn the users about the deceptive patterns. We also characterize the performance of AutoBot on several devices and find that the latency is less than one second on modern devices, which minimally impacts the usability of the websites. In this work, we put forth AutoBot, a automated solution to detect deceptive patterns on the web and make the following contribution: 1. We compile an annotated large-scale deceptive design dataset (D3) based on carefully curated taxonomy. We also introduce a novel pipeline to generate a diverse dataset of synthetically generated websites. 2. We show that our system, AutoBot, can detect deceptive designs present on the web and classify them per our taxonomy, with high recall resulting in most deceptive patterns being detected. 3. We leverage recent advances in optimizing LLM inference on consumer hardware to run our system, AutoBot, on device with real-time inference. Figure 1: Here we show how AutoBot highlights and informs the users about deceptive patterns on a site."
https://arxiv.org/html/2411.07247v1,VIEWER: an extensible visual analytics framework for enhancing mental healthcare,"Objective: To design and implement VIEWER, a versatile toolkit for visual analytics of clinical data, and to systematically evaluate its effectiveness across various clinical applications while gathering feedback for iterative improvements.Materials and Methods: VIEWER is an open-source and extensible toolkit that employs distributed natural language processing and interactive visualisation techniques to facilitate the rapid design, development, and deployment of clinical information retrieval, analysis, and visualisation at the point of care. Through an iterative and collaborative participatory design approach, VIEWER was designed and implemented in a large mental health institution, where its clinical utility and effectiveness were assessed using both quantitative and qualitative methods.Results: VIEWER provides interactive, problem-focused, and comprehensive views of longitudinal patient data from a combination of structured clinical data and unstructured clinical notes. Despite a relatively short adoption period and users’ initial unfamiliarity, VIEWER significantly improved performance and task completion speed compared to the standard clinical information system. Users and stakeholders reported high satisfaction and expressed strong interest in incorporating VIEWER into their daily practice.Discussion: VIEWER provides a cost-effective enhancement to the functionalities of standard clinical information systems, with evaluation offering valuable feedback for future improvements.Conclusion: VIEWER was developed to improve data accessibility and representation across various aspects of healthcare delivery, including population health management and patient monitoring. The deployment of VIEWER highlights the benefits of collaborative refinement in optimizing health informatics solutions for enhanced patient care.","As the volume of information in patient records continues to grow, clinicians face an overwhelming amount of complex raw data, which can exceed the capacity of human cognition to process without error [1]. This issue of information overload has been widely noted across various care settings, including both primary and secondary care, which may result in diagnostic or treatment errors [2, 3]. Moreover, the growing demand for health information exchange among healthcare providers, and between providers and stakeholders such as public health agencies and regulators, has made patient data review and administrative reporting increasingly complex and time-consuming [4]. This is particularly evident in mental healthcare, where patients often require ongoing mental health support and coordinated care for physical comorbidities and social support [5, 6]. As a result, clinicians are forced to spend valuable time assembling disparate data points to create a coherent view for a patient’s care, which can lead to inefficiency and delayed care [7]. Although electronic health records (EHRs) are designed to help clinicians manage information at the point of care, they often fail to present information in a format that offers effective cognitive support and mitigates information overload [7]. This is largely because EHR systems mainly focus on recording information for individual patients and present data in tabular views or static text formats, with limited capability to highlight underlying trends in a patient’s disease progression, reveal similarities within a team’s caseload, or facilitate longitudinal health monitoring on a population scale [8]. The need for better methods to manage and present increasingly complex information in EHRs has been long recognised [9, 10]. Pioneering work in the 1990s introduced graphical summaries of test results and treatment data to enhance the presentation of patient status and reduce the burden of information overload [11]. Since then, research in this area has advanced by incorporating diverse datasets, including patient-generated data [8, 12] and knowledge bases [13], alongside emerging technologies, e.g. interactive visualisations [14], statistical and machine learning analytics [8], to enhance the effectiveness and usability of visual analytics in healthcare. These advancements have been applied across various domains, including patient data summarisation [7, 15], cohort search [16], care quality improvement [17], patient flow analysis [18], population health management [19], and disease- or setting-specific pathway management [18, 20, 21]. Visual analytics has emerged as a powerful tool for converting complex health data into intuitive and visually-compelling presentations, enabling the extraction of valuable insights from big data and supporting informed clinical decision making [22]. However, existing research has mainly focused on physical healthcare settings, with limited attention given to mental healthcare [23]. Compared to physical health, visual analysis of mental health data presents greater complexity for two key reasons. First, mental illnesses often encompass both medical, psychological and social dimensions [24], requiring a broader range of data to achieve a comprehensive assessment of an individual’s mental health. This highlights the need of an extensible toolkit that can facilitate the rapid design, development, and implementation of visual analytics across diverse data sources and types in mental healthcare. Second, treating mental illnesses typically involves a combination of interventions, including medication, psychotherapy, lifestyle changes, and social support services [6]. Unlike physical conditions, where structured data (such as blood assays and other characterizations) is more salient, information about mental health presentations, relevant contextual factors, interventions, and outcomes is often documented in unstructured text (e.g., clinical notes and correspondences) [25]. Integrating this unstructured information into visual analytics pipelines is more challenging compared to structured data such as numerical or categorical variables. Another notable limitation of many previous studies is their generalisability. First, most studies have focused on a specific clinical task [7, 16] or disease [18, 26]. However, in real-world clinical settings, patients rarely present with a single disease or single risk factor that may affect their health [27]. There is a lack of integrated visual-analytics solutions that can systematically address the varied challenges faced by different healthcare stakeholders, including clinicians and patients [22]. Second, existing solutions are often developed using proprietary or custom tools internally developed by hospitals or EHR system providers [7, 26], leading to limited interoperability with other tools and restricted applicability in different settings [8]. Finally, for visual-analytics solutions to achieve better outcomes and effective adoption, they should be user-friendly, require minimal training, offer evidence-based recommendations, and integrate smoothly into clinicians’ workflows [27]. Despite extensive focus on technical development in previous studies, there has been little effort to systematically develop and evaluate these solutions within the complex workflows of clinicians, nurses, and other health workers while considering their varied clinical priorities and patient needs [28]. In this work, we present VIEWER (Visual & Interactive Engagement With Electronic Records), an open-source, cost-effective, and extensible toolkit created for the rapid design, development, and deployment of clinical information retrieval, analysis, and visualisation for supporting clinical decision making. VIEWER is an EHR-agnostic framework that utilises distributed information extraction pipelines, leveraging natural language processing (NLP) methods and open-source visualisation techniques to enable comprehensive search and visual analytics of a comprehensive health record from both structured and unstructured patient data within a health institution, rather than a curated dataset for a specific disease or patient cohort. We also systematically describe our interdisciplinary and collaborative approach to participatory design where we designed, implemented and evaluated VIEWER within one of the largest National Health Services (NHS) Trusts for mental health in the United Kingdom (UK), aimed at addressing the diverse challenges faced by multiple healthcare stakeholders. The stakeholders included: (i) clinicians who need to synthesise disparate data to understand a patient’s condition within the context of their medical history, (ii) managers who require data-driven insights to optimise resource allocation and identify unmet needs, (iii) researchers who seek to understand disparities in outcomes across populations, and (iv) patients who wish to utilise their own medical data for self-monitoring. Our evaluation demonstrates the effectiveness of VIEWER in enhancing patient care in real-world clinical use cases and provides valuable insights into working collaboratively with clinical workers, researchers, patients and carers, and informaticians to iteratively refine and optimise informatics solutions for improved patient care."
https://arxiv.org/html/2411.07885v1,"INTRABENCH: 
Interactive Radiological Benchmark","Current interactive segmentation approaches, inspired by the success of META’s Segment Anything model, have achieved notable advancements, however they come with substantial limitations that hinder their practical application in real clinical scenarios. These include unrealistic human interaction requirements, such as slice-by-slice operations for 2D models on 3D data, a lack of iterative refinement, and insufficient evaluation experiments. These shortcomings prevent accurate assessment of model performance and lead to inconsistent outcomes across studies.IntRaBench overcomes these challenges by offering a comprehensive and reproducible framework for evaluating interactive segmentation methods in realistic, clinically relevant scenarios. It includes diverse datasets, target structures, and segmentation models, and provides a flexible codebase that allows seamless integration of new models and prompting strategies. Additionally, we introduce advanced techniques to minimize clinician interaction, ensuring fair comparisons between 2D and 3D models. By open-sourcing IntRaBench, we invite the research community to integrate their models and prompting techniques, ensuring continuous and transparent evaluation of interactive segmentation models in 3D medical imaging.","Accurate segmentation of anatomical structures or pathological areas is crucial in fields like radiology, oncology, and surgery to isolate affected regions, monitor disease progression, treatment planning and guide therapeutic procedures. Traditional supervised medical segmentation models have demonstrated strong performance across a range of anatomies and pathologies (Isensee et al., 2020; 2023; Huang et al., 2023; Ulrich et al., 2023). However, their effectiveness remains heavily constrained by the amount and diversity of available training data, with the quality of human label annotations serving as a critical limiting factor. Consequently, fully autonomous AI solutions have not yet reached performance needed for widespread autonomous clinical applications. On the other hand, numerous semi-automatic segmentation techniques, not reliant on AI, are already in clinical practice to expedite manual annotation processes Hemalatha et al. (2018). These current ad hoc methods do not tap into the potential of AI-based automation to drastically reduce annotation time. A method that allows clinicians to segment any target with just a single click within the image could greatly enhance the efficiency of clinical workflows. The release of META’s Segment Anything (SAM) model represents a big leap towards making this potential a reality (Kirillov et al., 2023). ”SAM” is designed to segment any target through different user interaction methods, including point-based and bounding box prompts. This allows users to easily specify the area of interest by clicking on it or drawing a bounding box around it, making the segmentation process both flexible and intuitive. A particularly powerful feature is the ability for users to iteratively refine initial predictions by adding more positive or negative prompts. This advanced functionality, in contrast to traditional supervised segmentation methods, has attracted a lot of attention in the medical domain, and led to many studies evaluating and adapting SAM for 3D medical image segmentation (Roy et al., 2023; Deng et al., 2023; Hu et al., 2023; Zhou et al., 2023; Mohapatra et al., 2023; Cheng et al., 2023; Ma et al., 2024; Gong et al., 2023). Moreover, several researchers have been inspired by SAM’s capabilities to develop their own methods, often specifically designed for the 3D nature of radiological data (Du et al., 2024; He et al., 2024; Li et al., 2024; Wang et al., 2024). Although these domain-specific adaptations on medical data have shown promising progress, many published methods are plagued by pitfalls which obfuscate the efficacy of the models and prevent clinicians and researchers from determining the best methods for their use-cases: Figure 1: a) Current approaches require clinicians to interact with radiological images slice by slice, leading to increased workload. b) Some models operate natively in 3D and enable full 3D interaction. Only models that accept mask prompts allow iterative refinement of initial predictions with human guidance. Applying interactive 2D models to 3D data on a slice-by-slice basis (P1): Assuming clinicians will interact with each slice individually is unrealistic and undermines the efficiency improvements these methods aim for. Moreover, a slice-by-slice approach introduces an unfair bias when comparing 2D and 3D models, as 3D models typically require only a few interaction per image, leading to significantly fewer interactions and less supervision Cheng et al. (2023); Ma et al. (2024); Zhang & Liu (2023); Wu et al. (2024); Wong et al. (2024). Neglecting refinement (P2): Many studies assess interactive segmentation methods based on a single interaction step, overlooking the inherent ambiguities in radiological images (Ma et al., 2024; Du et al., 2024; Gong et al., 2023; Bui et al., 2024). Often, a second interaction may be necessary to specify which specific substructure the clinician wants to segment. This could be, e.g. a vessel within the liver, or the necrosis within a tumor, as exemplified in the well-known BraTs segmentation challenge (de Verdier et al., 2024). Furthermore, clinicians often want to adapt the segmentations to their clinic’s local protocol or refine them, particularly for targets with high inter-rater variability, like pathological structures (Fu et al., 2014; Benchoufi et al., 2020; Hesamian et al., 2019). Overall, there is a notable lack of research exploring realistic, iterative interaction methods for 2D models applied to 3D volumes. Obfuscated and insufficient evaluation (P3): With promptable models only recently garnering great attention, there is a lack of a standardized approach to evaluation, which has led to disparate and incomparable methods, which are at times even obfuscated or insufficient. We observed the following shortcomings: (i) Not specifying whether predictions were interactively refined or based on a single prompt with multiple points (Cheng et al., 2023; Wang et al., 2024). (ii) Being intransparent on the number of initial prompts given (Du et al., 2024). (iii) Using the best mask rather than the final mask after interactive refinement (Wang et al., 2024). (iv) Evaluating predictions slice-by-slice or on sub-patches of a 3D volume instead of evaluating on the full image (Roy et al., 2023; Ma et al., 2024; Cheng et al., 2023; He et al., 2024; Li et al., 2024). (v) Excluding targets considered ’too small’, hence neglecting valid targets such as small lesions that are neither tested nor trained on Ma et al. (2024); Cheng et al. (2023); Wang et al. (2024). (vi) Comparing against non-promptable models and SAM, rather than any other promptable model trained on medical data (Cheng et al., 2023; Ma et al., 2024; Gong et al., 2023; He et al., 2024). (vii) Lastly, overemphasizing segmenting healthy structures, such as organs, where existing supervised public models already perform well (Wasserthal et al., 2023; Ulrich et al., 2023), instead of focusing on pathologies, where interactive refinement could provide the greatest benefits (Wang et al., 2024; Zhang & Liu, 2023). To address these pitfalls, a benchmark is needed, aligning with the recent review paper from Marinov et al. (2024). To this end, we introduce IntRaBench, a reproducible and extendable Interactive Radiological Benchmark. Through it, we highlight the most performant 2D and 3D interactive segmentation and the best prompting methods in the radiological domain. In this paper, we present experiments carefully designed to replicate a clinical workflow as closely as possible, with the following key contributions: 1. IntRaBench, for the first time, enables a fair comparison of the most influential 2D and 3D interactive segmentation methods. By measuring the number of simulated interactions, a proxy for the ’Human Effort’, we test different prompting strategies that do not require a slice-wise interaction (P1). 2. We propose effective interaction strategies for refinement of predictions in a 3D volume, without requiring clinicians to interact with each individual slice (P2). 3. We provide a standardized evaluation protocol to generate prompts, select model outputs and compute the segmentation metrics on the entire image across ten datasets, covering various modalities and target structures, including small lesions (P3). Our benchmarking efforts include a performance comparison against leading interactive segmentation methods in the medical domain. 4. The extendable IntRaBench framework allows developers to a) easily evaluate a new method in a fair manner against established methods and b) easily develop and investigate new prompting strategies. Through open-sourcing IntRaBench, we invite researchers to integrate their methods into our framework, promoting continuous and equitable assessment that allows us to track the overall progress in the field of interactive 3D medical image segmentation reproducibly and transparently. Figure 2: IntRaBench overview. Although our evaluation is performed on entire 3D volumes, the benchmark accommodates both 3D and 2D interactive segmentation methods. While 3D model prompting is relatively straightforward, we introduce prompting and refinement strategies for 2D models that minimize the effort required from human interaction. The benchmark is designed to be extensible, and researchers are encouraged to propose and integrate additional methods seamlessly using our codebase particularly for areas marked by three dots."
https://arxiv.org/html/2411.07206v1,"Tasks, Time, and Tools: Quantifying Online Sensemaking Efforts Through a Survey-based Study","Aiming to help people conduct online research tasks, much research has gone into tools for searching for, collecting, organizing, and synthesizing online information. However, outside of the lab, in-the-wild sensemaking sessions (with data on tasks, users, their tools and challenges) can ground us in the reality of such efforts and the state of tool support. We use a survey-based approach with aided recall focused on segmenting and contextualizing individual exploratory browsing sessions to conduct a mixed method analysis of everyday sensemaking sessions in the traditional desktop browser setting while preserving user privacy. We report data from our survey (n=111) collected in September, 2022, and use these results to update and deepen the rich literature on information seeking behavior and exploratory search, contributing new empirical insights into the time spent per week and distribution of that time across tasks, and the lack of externalization and tool-use despite widespread desire for support.","People have many information needs that require them to search for and make sense of many web pages online, such as planning a personal trip or researching how to make a 3D game. Previous research has highlighted the importance of such exploratory search and sensemaking tasks in everyday web use, noting that they often involves more time, complex queries, processes, and goals (Marchionini, 2006) (Kellar et al., 2007a) and suggesting that such tasks are responsible for over a quarter of web searches (Donato et al., 2010). Furthermore, significant research has gone into tools for searching for, collecting, organizing, and synthesizing online information to help users with these tasks (Baldonado and Winograd, 1997; Dontcheva et al., 2006; Chang et al., 2020; Kuznetsov et al., 2022) (which we broadly term as part of the “sensemaking” process, as per (Pirolli and Card, 2005)). However, several key questions remain open regarding how often people engage in sensemaking, how much time they spend on it, and what tools and processes they employ or wish to employ today. Many of the studies relevant to online sensemaking or exploratory searching were conducted more than a decade ago, while the internet and lives of those that use it have changed significantly, becoming increasingly fragmented while growing in size (241% larger111Based on the long-running Netcraft Web Server Survey. since the last study by Donato using Yahoo browsing history). It is possible that shortcuts such as knowledge cards and instant answers have changed the cost structure such that people just don’t often feel the need to ”do the research themselves” even for open-ended tasks, and don’t spend much time on doing it when they do. The glut of information available today may be overwhelming, leading to more satisficing within sensemaking sessions as it becomes more difficult for people to even know if they have exhaustively covered the different perspectives and opinions and evidence, and which evidence they should believe - tracking all of which can quickly exceed working memory. Furthermore, it is unclear whether the prevalence of monetization through articles, affiliate advertising, and SEO for topics where people are looking for decision making support such as shopping or trip planning make things easier or harder for them. In this work we aim to investigate how people are handling the modern web and it’s associated information overload; are they slowing down and spending more time; if and how are they externalizing information in order to track and synthesize it; and what are the opportunities here for tool support. Answering these questions is challenging methodologically because of the tradeoffs involved in collecting data about not only what web pages users go to but their motivations, processes, mental models, and goals as they visit those pages. Web log analyses provide a comprehensive look at which pages users go to at scale, but require more indirect inference into what the user was actually thinking and doing. At the other end of the spectrum, interviews provide deep process insights but are high effort and often focus on a small sample of tasks (e.g., “tell us about the last time you did this”). Efforts to ground interviews in real usage, eg., asking participants to come in and “do a specific task” or “search as you normally would” may not be representative of their natural searching behavior and are limited in the amount of time researchers can see what they are doing. Conversely, diary studies can provide rich qualitative insights over a longer period of time but involve high effort in recruiting and coordinating users, and such sampling approaches may not provide a representative picture of any particular period of time. Finally, efforts involving researchers building specific tools for participants to use when browsing on their own (e.g., WebTracker (Choo et al., 1999), Vigo et al.’s Firefox add-on, (Vigo and Harper, 2017), Tabs.do, (Chang et al., 2021b), Fuse (Kuznetsov et al., 2022)) can be high cost to build and deploy as a data collection method, and users might not use them as they normally would their own browser. In this paper, we aim to shed light on sensemaking efforts spent on the modern web through a survey-based approach with aided recall which focuses on segmenting and contextualizing individual exploratory web browsing sessions. This technique enables us to conduct a mixed method analysis of everyday sensemaking sessions in the traditional desktop browser setting while preserving user privacy. We use this methodology to report data from a survey of 111 participants about their online information seeking and sensemaking behavior, collected during the month of September, 2022. Our results regarding online sensemaking update and deepen the rich literature on information seeking behavior and exploratory search, contributing new empirical insights into the time spent per week and distribution of that time across tasks, as well as the lack of general tool use and externalization despite the widespread desire for tool support. As the online environment continues to rapidly evolve we hope our methodology and snapshot of behavior can be useful to researchers in providing ways to track web’s evolution and understand changing user behavior with lower effort, while providing quantitative and qualitative insights into the contemporary web."
https://arxiv.org/html/2411.07042v1,Minion: A Technology Probe for Resolving Value Conflicts through Expert-Driven and User-Driven Strategies in AI Companion Applications,"Content Warning: This paper presents textual examples that may be offensive or upsetting.AI companions based on large language models can role-play and converse very naturally. When value conflicts arise between the AI companion and the user, it may offend or upset the user. Yet, little research has examined such conflicts. We first conducted a formative study that analyzed 151 user complaints about conflicts with AI companions, providing design implications for our study. Based on these, we created Minion, a technology probe to help users resolve human-AI value conflicts. Minion applies a user-empowerment intervention method that provides suggestions by combining expert-driven and user-driven conflict resolution strategies. We conducted a technology probe study, creating 40 value conflict scenarios on Character.AI and Talkie. 22 participants completed 274 tasks and successfully resolved conflicts 94.16% of the time. We summarize user responses, preferences, and needs in resolving value conflicts, and propose design implications to reduce conflicts and empower users to resolve them more effectively.","Human-AI conflict refers to a state of incompatibility, inconsistency, or opposition between humans and AI (Flemisch et al., 2020). In past research, human-AI conflicts were usually simple and direct—AI was more like a tool, and conflicts often stemmed from technical limitations, such as task execution failures (Wen et al., 2022), or disagreements with users in simple decision-making (Babel et al., 2024; Takayama et al., 2009). These types of conflicts generally lacked emotional and value entanglement, making them less likely to cause significant psychological harm to users. Recently, a diverse array of Large Language Model (LLM) agents has emerged, offering capabilities ranging from personalized assistance to performing complex tasks (Chen et al., 2024). The study focuses on LLM-based AI companion applications, such as Character.AI, Talkie, Replika, Kindroid, Paradot, and Xingye. As of July 2024, the total number of users of these applications has exceeded 900 million globally (including duplicate users across different applications)111User statistics source: https://www.data.ai.. AI companions can role-play and respond to users in a human-like manner, providing emotional support and companionship (Sullivan et al., 2023). For instance, in Character.AI, users can personalize the companion’s personality traits and interaction contexts through “Description,” “Greeting,” and “Definition.” Compared to earlier non-LLM chatbots, LLMs endow AI companions with a stronger ability to understand language, enabling them to engage in more context-aware and intelligent interactions (Kasneci et al., 2023), fostering more complex and intimate human-AI relationships (Maeda and Quan-Haase, 2024). Many users even consider them close friends or lovers (Skjuve et al., 2021, 2022). The deepening of this relationship raises users’ expectations of AI companions, but it may also lead to deeper conflicts, including value conflict. For example, some users have shared online their experiences of encountering sexist remarks from AI companions, describing how they engaged in intense arguments with the AI, which left them frustrated, angry, and hurt (Zhou, 2023). As the relationship between AI companions and users becomes more interpersonal, previous conflict resolution strategies for human-AI conflict have started to fail (Rosen, 2014; Babel et al., 2024). Strategies based solely on technical limitations are no longer sufficient, and it is becoming important to draw on interpersonal conflict resolution methods and users’ real-world experiences with AI companions. Although Fan et al. provide initial insights into value alignment and conflicts between users and AI companions (Fan et al., 2024), inexperienced users often find it challenging to resolve these issues independently. How to design tools that empower users to handle value conflicts with AI companions remains an unexplored research gap that this work aims to address. In this work, we first conducted a formative study to understand and characterize the value conflicts between users and AI companions (Schwartz, 2012). We analyzed 151 user complaint posts from social media platforms, finding that many conflicts are value-laden. Building on this, we constructed a value conflict framework for AI companion applications (Schwartz, 2012), which provided real-world data for our technology probe study, allowing us to reconstruct actual value conflict scenarios. Combining prior research on conflict resolution (Brett et al., 1998; Shaikh et al., 2024; Mun et al., 2023) with our formative study, we found that interactions between users and AI companions exhibit complex dynamics, where relying solely on expert strategies from other conflict scenarios (e.g., interpersonal conflict theories (Brett et al., 1998)) is insufficient. The value conflicts users face in real-life situations are diverse, and through their interactions with AI companions and exchanges on social platforms, users have accumulated certain conflict resolution experiences. Therefore, it is necessary to draw from both expert theories and the practical experiences of AI companion users to explore more suitable solutions (Fan et al., 2024). Then, we created Minion, a technology probe (Hutchinson et al., 2003) that provides users multiple suggestions for resolving value conflicts while gaining insights into user behaviors. Minion’s algorithm combines expert-driven and user-driven conflict resolution strategies. We developed LLM prompts to address value conflict situations between users and AI companions by drawing on two key sources. First, we drew upon Shaikh et al.’s solutions for interpersonal conflict resolution (Shaikh et al., 2024) to guide our expert-driven conflict resolution strategies. Second, we referenced the user-driven strategies identified in the study by Fan et al. (Fan et al., 2024) to capture how users manage conflicts with AI companions. To empirically test Minion, we conducted a technology probe study (Hutchinson et al., 2003) with 22 participants. We created 40 distinct conflict scenarios on two popular AI companion platforms, Character.AI and Talkie. Each scenario was designed with specific conflict resolution goals. Participants completed 274 tasks, achieving an overall conflict resolution rate of 94.16%. Minion received positive feedback from participants and inspired them with new ideas in conflict resolution. Based on our findings, we discuss the opportunities and challenges in integrating expert-driven and user-driven strategies in resolving human-AI value conflicts, and call for further research in this area, focusing on the dynamics of emerging human-AI relationships. Our work’s contributions are as follows: • A novel user-empowerment intervention method that combines expert-driven and user-driven conflict resolution strategies. This method is presented in the form of the technology probe Minion, serving as a prototype for future tools aimed at resolving human-AI value conflicts. • We empirically tested Minion in a one-week technology probe study (N=22). The results demonstrated the technical feasibility of Minion. We summarized users’ responses, preferences, and needs when dealing with value conflicts with AI companions. • Based on the formative and technology probe studies, we explored the opportunities and challenges of integrating expert-driven and user-driven strategies in human-AI value conflicts. We also proposed design implications for future human-AI conflict resolution solutions, particularly in the field of AI companions."
https://arxiv.org/html/2411.06726v1,Predicting Selection Intention in Real-Time with Bayesian-based ML Model in Unimodal Gaze Interaction,"Eye gaze is considered a promising interaction modality in extende reality (XR) environments. However, determining selection intention from gaze data often requires additional manual selection techniques. We present a Bayesian-based machine learning (ML) model to predict user selection intention in real-time using only gaze data. Our model uses a Bayesian approach to transform gaze data into selection probabilities, which are then fed into an ML model to discriminate selection intentions. In Study 1, our model achieved real-time inference with an accuracy of 0.97 and an F1 score of 0.96. In Study 2, we found that the selection intention inferred by our model enables more comfortable and accurate interactions compared to traditional techniques.","Selecting a target is one of the fundamental tasks in human-computer interaction (HCI) enabling users to perform various interactions. Since fast and accurate pointing and selection techniques significantly enhance the interaction experience, the development of novel interaction techniques continues to be an active research area in HCI (Grossman and Balakrishnan, 2005; Bi and Zhai, 2013; Moon et al., 2024; Sidenmark et al., 2023). Typically, handheld controllers have been used as the primary modality in VR/AR environments due to their ability to provide stable target selection performance. In addition, other modalities such as hand gesture tracking (Shi et al., 2023; Zhu et al., 2023), head movements (Choi et al., 2022; Sidenmark et al., 2023) are also used for pointing and selection. However, these interaction techniques can cause physical fatigue, such as the ”gorilla arm” effect in case of mid-air interaction (Hincapié-Ramos et al., 2014), and limit natural hand movement. With the advent of head-mounted displays (HMDs) capable of eye-tracking, gaze-based pointing and selection has attracted considerable interest. As an interaction modality, gaze offers the advantage of being hands-free, requiring minimal physical effort, and allowing for rapid pointing by allowing simultaneous observation and pointing (Sidenmark et al., 2023; Vertegaal, 2008; Tanriverdi and Jacob, 2000). However, using gaze as a standalone input modality for target acquisition presents several challenges. A primary issue is the difficulty of expressing selection confirmation through gaze alone, which limits its effectiveness as a unimodal input (Jacob, 1990a; Choi et al., 2022). To address these issues, two approaches have been proposed in previous studies: (1) multimodal selection (Wei et al., 2023; Choi et al., 2022; Shi et al., 2023), in which gaze is used only for pointing and a controller key or specific hand gesture is used for selection, and (2) unimodal selection with additional procedures, such as dwell time, in which the user confirms a selection by fixating on the target for a predefined period of time longer than normal pointing (Narkar et al., 2024; Isomoto et al., 2022; Yi et al., 2022). However, when utilizing a hand-held controller or hand gestures, long-term hand usage can lead to physical fatigue (Boring et al., 2009; Hincapié-Ramos et al., 2014). Similarly, the dwell method, which requires unnatural eye movements, can also cause similar issues (Duchowski, 2018). To overcome this limitation, predicting the user selection intention has been explored, eliminating the need for manual selection step. While attempts have been made to apply this in 2D screen environments (Li et al., 2021; Isomoto et al., 2018, 2022), the approach has been reported to suffer from usability issues due to the Midas touch problem and low pointing accuracy with gaze (Jacob, 1990a). In VR environments, there have also been studies using gaze-based features to predict selection intention (Narkar et al., 2024; David-John et al., 2021). However, more research is needed to effectively apply these methods in real-world VR environments. In this paper, we present an Bayesian-based ML model that predicts the user selection intention by using selection probabilities derived from the Bayesian model (see Fig. 1). Our method uses a Bayesian model to convert gaze-related features into posterior vectors, which are rationale for predicting selection intention. As a result, our model can more accurately predict the user’s selection intention. Furthermore, the model allows for real-time inference and enables gaze-based unimodal interaction without manual selection, which is more accurate convenient than two-step selection techniques that additionally use controllers or dwell methods. We evaluated our selection intention prediction model from two main perspectives: (1) its ability to accurately discriminate the user selection intention in real time, and (2) its effectiveness in supporting comfort and accurate target selection when implemented in a VR environment. In Study 1, we designed an experiment in which 20 participants performed a 3D target acquisition task to collect gaze data. We then built a Bayesian model based on the distribution of gaze-related features, and used the inferred selection probability data to train and evaluate a ML model. The prediction performance was found to be highly effective, with an accuracy of 0.97, an F1 score of 0.96, and an inference time of less than 1 ms. Thus, our model can quickly and accurately discriminate the user’s selection intent. In Study 2, we compared the gaze-based interaction technique using our model with manual selection techniques such as controller and dwell methods in a 3D target selection environment. Our interaction technique allowed for accurate selection compared to the controller and dwell methods, and it also demonstrated lower workload and physical demand scores. These results suggest that our model enables comfort and accurate interaction in 3D environments without the need for manual selection processes. In summary, the contributions of this study are as follows: Method for predicting selection intention in real time using only gaze data: This study presents the most lightweight and accurate model for predicting user selection intention using a unimodal dataset based solely on gaze data. Improving the user experience of selection interactions using gaze-based unimodal interaction techniques: We have demonstrated that our model, which differentiates selection intentions using only gaze data, enables the completion of 3D target selection tasks without the need for manual selection techniques. Moreover, this approach improves both performance and usability compared to traditional interaction techniques."
https://arxiv.org/html/2411.06723v1,Script-Strategy Aligned Generation: Aligning LLMs with Expert-Crafted Dialogue Scripts and Therapeutic Strategies for Psychotherapy,"Chatbots or conversational agents (CAs) are increasingly used to improve access to digital psychotherapy. Many current systems rely on rigid, rule-based designs, heavily dependent on expert-crafted dialogue scripts for guiding therapeutic conversations. Although recent advances in large language models (LLMs) offer the potential for more flexible interactions, their lack of controllability and transparency poses significant challenges in sensitive areas like psychotherapy. In this work, we explored how aligning LLMs with expert-crafted scripts can enhance psychotherapeutic chatbot performance. Our comparative study showed that LLMs aligned with expert-crafted scripts through prompting and fine-tuning significantly outperformed both pure LLMs and rule-based chatbots, achieving a more effective balance between dialogue flexibility and adherence to therapeutic principles. Building on findings, we proposed “Script-Strategy Aligned Generation (SSAG)”, a flexible alignment approach that reduces reliance on fully scripted content while enhancing LLMs’ therapeutic adherence and controllability. In a 10-day field study, SSAG demonstrated performance comparable to full script alignment and outperformed rule-based chatbots, empirically supporting SSAG as an efficient approach for aligning LLMs with domain expertise. Our work advances LLM applications in psychotherapy by providing a controllable, adaptable, and scalable solution for digital interventions, reducing reliance on expert effort. It also provides a collaborative framework for domain experts and developers to efficiently build expertise-aligned chatbots, broadening access to psychotherapy and behavioral interventions.","The integration of chatbots, or conversational agents (CAs), into psychotherapy and behavioral interventions (Almusharraf F, 2020; Sun et al., 2023a; He et al., 2022b; Park et al., 2019; He et al., 2022a; Zhang et al., 2020; Xu and Zhuang, 2020) has transformed the way mental health services are delivered, providing around-the-clock accessibility and support. Traditionally, these chatbots have relied heavily on rule-based systems (McTear, 2021) where expert-written scripts (Silva and Canedo, 2022; Urban and Mailey, 2019) were needed to facilitate therapeutic dialogues, ensuring controlled and therapeutically precise interactions. expert-written scripts (Silva and Canedo, 2022; Urban and Mailey, 2019) and While this approach is reliable, it often leads to rigid conversations that lack flexibility and diversity, largely due to the need for extensive domain expertise in designing and crafting dialogues. The advent of Natural Language Generation (NLG) (Dong et al., 2022) has marked a shift away from these strictly predefined interactions, with systems being developed to generate more dynamic conversations. For example, Motivational Interviewing (MI) (Miller and Rollnick, 2002) chatbots have employed rephrasing techniques and template-based (Dieter et al., 2019) dialogue generation to improve user engagement (Almusharraf F, 2020; He et al., 2022a; Min et al., 2023). However, despite these advancements, such NLG systems are still limited by their reliance on domain-specific data, which is especially scarce and sensitive in the field of psychotherapy. The rise of large language models (LLMs) (Brown et al., 2020) in psychotherapy has shown promising potential (Demszky et al., 2023), offering new pathways for personalized support and engaging interactions in therapeutic settings. Trained on vast amounts of human dialogue, LLMs can handle flexible conversations and simulate conversational empathy and understanding without additional training (Syed et al., 2024; Sharma et al., 2023), a critical foundation for emotional intelligence in mental health care. However, significant challenges remain, particularly the non-transparent and uncontrollable nature of these models (Ehsan et al., 2021; El Ali et al., 2024), which raises quality and safety concerns in sensitive psychotherapy contexts. In addition, LLMs also face two critical challenges when applied to the specific domains of psychotherapy: (1) LLMs often lack domain-specific knowledge required to initiate conversations on specific psychotherapeutic topics, such as increasing intrinsic motivation or cognitive behavioral practices (Beck, 2011) like “mindfulness” (Ferguson et al., 2021), “should statements” (Sho, 2023), as shown in Fig 2, and (2) LLMs struggle to generate therapeutic questions (see Fig 2) and reflections that are essential for guiding a meaningful therapeutic dialogue, particularly in delivering psychotherapy-embedded intervention techniques such as Motivational Interviewing (MI). As a result, domain experts in clinical and health psychology still tend to favor rule-based systems, as they ensure precision and relevance through expert-crafted scripts, although the problems of rigid and unengaging conversations remain. However, this reliance on predefined dialogue scripts introduces another challenge: (3) creating these expert-crafted dialogues with domain expertise is highly time-consuming and costly, limiting scalability. Figure 2. A dialogue example: parts of tree-structured dialogue scripts pre-crafted by experts under the psychotherapeutic topic “Supportive social environment” for behavioral intervention. To address these gaps, we propose “Script-Aligned Generation (SAG)”, a concept for aligning LLMs with expert-crafted dialogue scripts for psychotherapy. SAG minimizes the strict reliance on fully scripted dialogue structures, enhancing both the controllability and transparency of LLM-driven chatbots while preserving the natural flexibility of LLMs. This leads to our first research question RQ1: How do LLM-powered chatbots aligned with expert-crafted dialogue scripts compare to both rule-based chatbots and pure LLMs in delivering psychotherapy for behavioral intervention? To validate the potential of SAG, we conducted a study comparing three chatbots: (1) rule-based chatbots strictly based on expert-crafted scripts, (2) LLM-powered chatbots employing SAG via fine-tuning or prompting, and (3) a pure LLM without expertise alignment. Our findings revealed that SAG-aligned LLM chatbots outperformed both rule-based and pure LLMs across key metrics, including linguistic quality, therapeutic relevance, engagement, perceived empathy, MI adherence, and motivation enhancement. However, we acknowledge that the prompt engineering or fine-tuning techniques we used in this study still require a relatively large amount of expert scripting, which might be challenging to collect in the real world. To further reduce dependence on expert scripting, we introduce an approach “Script-Strategy Aligned Generation (SSAG)” which can flexibly align LLMs with expert knowledge through key therapeutic strategies in MI (Shah et al., 2022; Miller et al., 2002), such as “asking questions” to progress therapy and “reflective listening” to express empathy. SSAG provides a scalable framework, enabling domain experts and developers to collaboratively create effective, contextually relevant chatbot-delivered interventions. This leads to our second research question RQ2: How can LLMs be efficiently aligned with expert-crafted dialogue scripts for psychotherapy through (a) strict alignment by fine-tuning or prompting, or (b) flexible alignment with the proposed approach “Script-Strategy Aligned Generation (SSAG)”? To evaluate the efficiency of SSAG, we conducted a 10-day field study comparing LLM-powered chatbots using SAG (via prompting) with our proposed SSAG approach, using the rule-based chatbot as a control. Results showed no significant differences between SAG-Prompt and SSAG, demonstrating that SSAG effectively aligns LLMs with domain expertise while reducing reliance on fully pre-scripted dialogues. These findings highlight SSAG’s potential to efficiently elicit therapeutic expertise from LLMs, minimizing the need for extensive pre-scripting by experts. This work makes key contributions to computational linguistics, digital healthcare, and psychotherapy by advancing LLM-driven digital interventions. We empirically evaluated LLM-powered chatbots employing both the concept of SAG and our proposed flexible alignment approach, SSAG, comparing them to traditional rule-based systems and pure LLMs. The evaluation provides insights that prompting, as an alignment method, is more efficient and scalable than fine-tuning, and SSAG further enhances flexibility without compromising therapeutic quality. Additionally, we developed the first dataset of expert-crafted dialogues for psychotherapy based on MI and CBT frameworks, supporting future research on LLM alignment with domain expertise. Collectively, these contributions emphasize the importance of human expertise in guiding LLMs within sensitive domains, paving the way for scalable, accessible, and cost-effective psychotherapeutic tools that can transform therapeutically aligned health support in the era of LLMs."
https://arxiv.org/html/2411.06575v1,Adaptive Kinematic Modeling for Improved Hand Posture Estimates Using a Haptic Glove,"Most commercially available haptic gloves compromise the accuracy of hand-posture measurements in favor of a simpler design with fewer sensors. While inaccurate posture data is often sufficient for the task at hand in biomedical settings such as VR-therapy-aided rehabilitation, measurements should be as precise as possible to digitally recreate hand postures as accurately as possible. With these applications in mind, we have added extra sensors to the commercially available Dexmo haptic glove by Dexta Robotics and applied kinematic models of the haptic glove and the user’s hand to improve the accuracy of hand-posture measurements. In this work, we describe the augmentations and the kinematic modeling approach. Additionally, we present and discuss an evaluation of hand posture measurements as a proof of concept.","Haptic gloves are wearable devices that provide force and/or tactile feedback to the fingertips and, optionally, the palm. Further, they track the movement of the fingers and, in some cases, the palm with multiple degrees of freedom (DoF) [1]. In recent years, haptic gloves have become increasingly popular. Current literature envisions that such haptic gloves, if properly designed, will enable dexterous manual interaction with a virtual reality (VR) that feels realistic [2]. However, haptic glove applications are not only limited to VR interaction. Other examples range from robotic teleoperation [3, 4] over surgical education [5] to behavioral analysis [6]. Haptic devices have also been applied for medical settings like motor rehabilitation [7, 8, 9, 10] and medical training [11, 12, 13]. In these biomedical contexts, such devices are helpful because, in addition to measuring hand and finger movements, they also provide performance test scenarios by haptically representing virtual objects. Further, they allow the creation of applications hand-tailored to a patient’s needs [14, 8]. However, as Wang et al. note, in practice, commercially available haptic gloves generally sacrifice the accuracy of motion tracking for a lower price and weight [2]. For example, the Dexmo glove111https://www.dextarobotics.com/en-us [15, 16] only measures the overall flexion of each fingertip relative to its respective metacarpophalangeal joint (MCP) with a bend sensor, as well as the abduction of the MCP. Still, a single bend value does not uniquely determine the flexion angles of all joints, i.e., MCP, proximal interphalangeal (PIP), and distal interphalangeal (DIP). To circumvent this problem, the Dexmo software uses a linear regression model that infers PIP, DIP, and MCP angles based on the bend value of a finger [15]. The drawback of this approach is that it constrains the measured position of individual fingertips to be on a circular trajectory between full extension and a power grasp. This constraint limits the quality of force feedback, which depends on a high-quality fingertip pose estimation, and drastically reduces the number of natural movements the device can track. For example, a precision pinch where PIP and DIP of the digit opposed to the thumb are fully extended, can not be captured. Figure 1: Data flow diagram of our model-based approach. The haptic gloves data is converted to fingertip positions by applying Forward Kinematics derived from a model of the glove. Based on a model of the user’s hand, the application of Inverse Kinematics yields a valid joint configuration. Another challenge we identify in replicating hand postures digitally as accurately as possible is choosing the dimensions of the digital hand model. The lengths of the bones in the human hand dictate the positions of the joints and their influence on the fingertip positions relative to the palm. If the user’s hand differs too much from the model, some postures might be physiologically impossible to reach with the digital hand. To the best of our knowledge, no efforts have been made in the haptics community to develop an adaptive human hand model for haptic glove postures with a fast and reliable calibration method. Most gloves are under-determined but still compute the full hand joint states [17]. Thus, they must resort to some variation of the mentioned coupling simplification of PIP and DIP to the MCP and use two extreme configurations to create a linear mapping between the joint of the real and digital hand that uses the digital model’s full range of motion. This work presents a modified version of the Dexmo haptic glove that allows us to estimate each finger’s MCP, PIP, and DIP angles without constraining them to a predefined trajectory. We propose a framework that computes the hand joint configuration using kinematic modeling of both the glove and hand, along with sensor data. As a part of this framework, we utilize the Hand Model Configuration Tool (HMCT) [18] to measure the size of the user’s hand and appropriately adapt the generic hand model employed for Inverse Kinematics. The framework can be split into a hardware-dependent and a hardware-agnostic part (see Fig. 1). The hardware-dependent part consists of the glove itself and the software component tied to it that sends out its measurements, as well as a reliable model of the glove, its links, and its joints. Section II-B1 discusses this part of our framework applied to Dexmo in more detail. The hardware-agnostic part consists of (1) a Forward Kinematics module, which – parameterized by the glove’s exoskeleton model – converts the incoming measurements into fingertip positions, and (2) an Inverse Kinematics module, parameterized by a configurable user hand model, that converts the fingertip positions into a hand joint configuration for downstream applications. This approach is detailed in section II-B. Figure 2: Concept of additionally measured angles for each finger. B denotes the bend sensor of Dexmo, which measures the flexion angle \alpha. F and T denote the newly added sensors to measure angles \beta and \gamma to compute the fingertip’s position. Inverse Kinematics then solves for the finger-joint angles \sigma, \delta, and \phi. 1. and 2. display two distinct configurations with different configurations of DIP and PIP, but the same values for angles \alpha and \beta, as an example of the under-determination problem, when only measuring \alpha."
https://arxiv.org/html/2411.06293v1,Enhancing Well-Being Through Positive Technology: VR Forest Bathing,"The growing demand for accessible therapeutic options has led to the exploration of Virtual Reality (VR) as a platform for forest bathing, which aims to reduce stress and improve cognitive functions. This paper brings together findings from three studies by the authors. The first study compared environments with and without plant life to examine how biomass influences stress reduction. The second study focused on the differences between low-fidelity and high-fidelity VR environments, while the third explored whether the benefits of VR forest bathing come from being immersed in realistic environments or simply from viewing something beautiful. The results showed no significant differences between environments with and without biomass, but highlighted the positive effects of high-fidelity VR environments and realistic nature over abstract art. The paper also covers how VR nature experiences may boost executive functioning and well-being in older adults and discusses the potential of generative AI to create customized VR environments. It concludes with a call for further collaborative research to refine VR forest bathing for stress relief and cognitive enhancement.","Large trees and Superior Lake surround Duluth, Minnesota (USA). Spending time in such a location can have an impact on a person. The positive effects of nature, particularly a forest, have been shown to reduce stress (Yao et al., 2021; Annerstedt et al., 2010) and increase executive functions (Cai et al., 2024). The drive from Duluth to Minneapolis, MN, is so immersive because of its nature that one ponders the privilege of being in such an experience and its benefits. Shinrin-yoku (forest bathing) was coined by the Japanese Ministry of Agriculture, Forestry, and Fisheries in 1982 (Park et al., 2010). This practice involves people spending time in nature, which has been shown (Kaplan, 1995; Ulrich, 1981) to reduce stress and well-being in general (for Europe, 2016). However, many people have no access to nature due to their typical day-to-day urban life routines, mobility problems, or other impediments, which deprive them of the benefits of forest bathing (Soga and Gaston, 2016). It would be necessary to see how much stress reduction and improvement of cognitive functions users can gain when accessing a simulated natural space, making virtual reality (VR) a great equalizer. Another important aspect to discuss in this context is the Biophilia Hypothesis, which states that people will feel more restored after being immersed in a place rich in Biomass (i.e., living biological organisms) compared to urban areas lacking biomass, all other factors being equal. Yet, in a VR forest bathing experience, nothing is alive, so understanding how virtual elements play a role is critical for the development of this positive technology. We have been able to answer a few questions with our previous research, yet there are still many more questions that are not known. For example, how do single components affect the experience and its ability to reduce stress? Should we add a lake, or would it enhance the sense of Biophilia more if the space were filled with more trees? VR allows “magic” to take place, yet we must empirically understand what works and does not. This position paper will describe the main takeaways from multiple studies we have conducted on forest bathing in VR and identify areas where the community interested in this work may want to concentrate. Figure 2. Canyon and Forest Environments from (Masters et al., 2022) \Description Two experimental VR environments (Canyon and Forest) from (Masters et al., 2022)"
https://arxiv.org/html/2411.06244v1,Grasping Object: Challenges and Innovations in Robotics and Virtual Reality,"In real-life, grasping is one of the fundamental and effective forms of interaction when manipulating objects. This holds true in the physical and virtual world; however, unlike the physical world, virtual reality (VR) is grasped in a complex formulation that includes graphics, physics, and perception. In virtual reality, the user’s immersion level depends on realistic haptic feedback and high-quality graphics, which are computationally demanding and hard to achieve in real-time. Current solutions fail to produce plausible visuals and haptic feedback when simulation grasping in VR with a variety of targeted object dynamics. In this paper, we review the existing techniques for grasping in VR and robotics and indicate the main challenges that grasping faces in both domains. We aim to explore and understand the complexity of hand-grasping objects with different dynamics and inspire various ideas to improve and come up with potential solutions suitable for virtual reality applications.","Over the past decades, with the advancement in virtual reality (VR) devices and Human-computer interaction (HCI) studies, the need for simulation of human behaviour with realistic interaction has become vital for many applications (see Fig. 1), such as industrial training, medical and surgical simulation, and rehabilitation [1, 2, 3]. High-fidelity graphics in such virtual environments have been used to provide users with a relatively immersive virtual experience [4]. However, obtaining a fully immersed experience requires realistic interaction with objects within the environment, where forces and masses of the objects can be felt during the interaction [5]. Haptic devices for both fingertips and/or the whole hand enable users to feel and manipulate the 3D objects and explore the virtual environment through a kinesthetic and cutaneous perception [6]. When the user interacts with an object, grasping is one of the main intuitive action behaviours. Although grasping behaviour is natural, human hands can grab a variety of objects with different shapes, weights, and frictions. Grasping in the virtual environment is a challenging task, as an ideal grasping action must take into account the geometry and dynamic characteristics of the virtual object [7]. Despite the recent achievements in grasping techniques in VR, which have led to the emergence of some technological devices such as glove-based devices [8] and controller-based devices [9, 1]. More technical methods need to be explored in order to ensure stable, controllable grasping in virtual reality. The grasping techniques in VR differ broadly in complexity according to the virtual object’s properties, such as mass, size, and materials. Moreover, the object’s stiffness plays a significant role in making the interaction with the object complex to simulate [10]. Most existing techniques focus on reducing the complexity of the grasping targets by simulating rigid bodies or objects with relatively simple and similar properties [11]. However, unlike rigid bodies, deformable bodies have high dynamic force attributes when the fingertips make contact with them. Therefore, obtaining stable grasping of deformable bodies while achieving realistic visuals and haptic feedback in virtual reality remains an open problem. Fig 1. Realistic interaction with virtual objects within an immersed environment is crucial for many applications, such as industrial and medical training, entertainment and virtual social interaction [2]. In this paper, we aim to review the existing methods for grasping from the perspectives of haptics and visuals, including the properties of the target objects. Further, to focus on the techniques of grasping in VR, we also generally introduce robotic grasping methods to compare the grasping methods in virtual environments with real-life robotics. Through this paper, we hope to summarise the main existing challenges in grasping simulation and improve the quality of VR grasping, potentially creating a direction for a new research agenda."
https://arxiv.org/html/2411.06099v1,CoPrompter: User-Centric Evaluation of LLM Instruction Alignment for Improved Prompt Engineering,"Ensuring large language models’ (LLMs) responses align with prompt instructions is crucial for application development. Based on our formative study with industry professionals, the alignment requires heavy human involvement and tedious trial-and-error especially when there are many instructions in the prompt. To address these challenges, we introduce CoPrompter, a framework that identifies misalignment based on assessing multiple LLM responses with criteria. It proposes a method to generate evaluation criteria questions derived directly from prompt requirements and an interface to turn these questions into a user-editable checklist. Our user study with industry prompt engineers shows that CoPrompter improves the ability to identify and refine instruction alignment with prompt requirements over traditional methods, helps them understand where and how frequently models fail to follow user’s prompt requirements, and helps in clarifying their own requirements, giving them greater control over the response evaluation process. We also present the design lessons to underscore our system’s potential to streamline the prompt engineering process.","Large language models (LLMs) have become ubiquitous in a wide range of applications, finding use across billion-dollar industries for tasks ranging from basic operations to critical applications (Gururangan et al., 2020; Bubeck et al., 2023; Liang et al., 2022; Brown et al., 2020; Bommasani et al., 2021). Prompting has emerged as the primary method for developers to interact with LLMs, enabling rich, Generative AI-driven experiences. Given that many critical products and services rely on LLMs and prompts, ensuring that the generated contents align with user expectations is crucial. Recent works have explored emerging themes of misalignment between prompt engineers and model responses. Drawing from the definition provided by Terry et al. (Terry et al., 2023), we define misalignment as “LLMs generating outcomes that deviate from the goals or expectations of the user, often leading to unintended or undesirable effects”. Some misalignments that have been explored in literature include instruction overlooking and misinterpretation (Zamfirescu-Pereira et al., 2023; Wei et al., 2022; Rao et al., 2024; Zhang et al., 2024; Jiang et al., 2024b; Glukhov et al., [n. d.]) hallucinations (Lin et al., 2022; Ji et al., 2023; Niu et al., 2024; Jiang et al., 2024a; Manakul et al., 2023), generation of harmful (Gehman et al., 2020a; Wang et al., 2024b; Phute et al., 2024; Kumar et al., 2024; Wang et al., 2024a) or biased (Nadeem et al., 2021; Dai et al., 2024; Yeh et al., 2023; Wan et al., 2023; Xu et al., 2024) contents, etc. This often stems from the inherent sensitivity of LLMs to the words within the prompt and the overall structure of prompts, where even minor changes can lead to drastically different outputs (Sclar et al., 2024; Lu et al., 2022; Loya et al., 2023; Memon et al., 2024). Despite efforts being made towards improving the instruction-following ability of LLMs by training them on instruction datasets (Ouyang et al., 2022; Li et al., 2024; Honovich et al., 2022; Longpre et al., 2023), existing approaches often fall short of being consistently aligned with all the instructions in long-form prompts with numerous instructions (Figure 1). These prompts tend to experience greater misalignment not only due to the complexity of multiple instructions but also the variations in instruction writing styles that causes the model to overlook or misinterpret the prompt engineer’s intended outcomes. To better understand the practical challenges faced by prompt engineers, we conducted a formative survey with 28 professionals working across various domain-specific industry applications (Section 3). Based on the survey, the misalignment issues such as overlooked instructions, inconsistent responses, instruction misinterpretations, and incorrect assumptions are frequent, particularly when dealing with complex prompts containing five or more instructions. The current workflow for prompt engineering is thus time-consuming and tedious, often requiring over ten iterations and manual inspection of responses in a largely trial-and-error process. In response to these challenges, we introduce CoPrompter, a novel tool that helps prompt engineers systematically identify and address areas of misalignment between multiple LLM outputs and their requirements. CoPrompter achieves this by first breaking down user requirements into atomic instructions, each transformed into criteria questions. With the criteria and LLM outputs, CoPrompter generates detailed misalignment reports at the instruction level. This granular approach allows users to quickly pinpoint where misalignments occur and identify which criteria fail most frequently, offering a systematic way to prioritize prompt refinements. As a user-in-the-loop tool, CoPrompter provides prompt engineers with control over the evaluation process, allowing them to customize criteria to better reflect specific requirements and adapt to evolving needs. This work provides the following contributions: • We introduce CoPrompter, a novel user-in-loop system that helps prompt engineers align complex prompts with their requirements through systematic evaluation and customizable criteria control. • A technical pipeline that generates detailed misalignment reports, allowing prompt engineers to pinpoint specific areas in need of improvement and prioritize prompt adjustments. • We present findings from a formative study with 28 prompt engineers, contributing to the understanding of the challenges in crafting complex prompts and the common misalignments observed with LLM responses. • We conduct a comprehensive user evaluation with 8 industrial prompt engineers, demonstrating that CoPrompter effectively identifies misalignments, helps in prompt refinement, and adapts to evolving requirements, significantly improving upon traditional manual methods. • A focused user evaluation with high System Usability Scale (SUS) scores further reflects user confidence in its seamless integration into prompt improvement workflows."
https://arxiv.org/html/2411.06060v1,Wild Narratives: Exploring the Effects of Animal Chatbots on Empathy and Positive Attitudes toward Animals,"Rises in the number of animal abuse cases are reported around the world. While chatbots have been effective in influencing their users’ perceptions and behaviors, little if any research has hitherto explored the design of chatbots that embody animal identities for the purpose of eliciting empathy toward animals. We therefore conducted a mixed-methods experiment to investigate how specific design cues in such chatbots can shape their users’ perceptions of both the chatbots’ identities and the type of animal they represent. Our findings indicate that such chatbots can significantly increase empathy, improve attitudes, and promote prosocial behavioral intentions toward animals, particularly when they incorporate emotional verbal expressions and authentic details of such animals’ lives. These results expand our understanding of chatbots with non-human identities and highlight their potential for use in conservation initiatives, suggesting a promising avenue whereby technology could foster a more informed and empathetic society.","Many countries have recently reported rises in animal abuse (Maisner, 2023; Cheng, 2024; Harris, 2023). Various factors have been identified as contributing to this troubling trend, including a lack of awareness that animals can suffer pain (Agnew, 1998). Another factor is absence of empathy for animals, as individuals who do not recognize them as sentient are less likely to treat them humanely (Miller, 2001; Agnew, 1998). In addition, the impossibility of communicating with animals verbally leaves them particularly susceptible to misrepresentation and misunderstanding, further increasing their vulnerability to abuse (Thomsen et al., 2023). Researchers have explored various approaches to promoting people’s empathy and positive attitudes toward animals. These range from live animal encounters in educational settings like zoos (Małecki et al., 2019) to first-person narratives purportedly written by animals (Akerman, 2019). A recurring theme across these strategies is the attempt to provide animals with a voice, as a means of evoking empathy and thus helping to shift public perception in favor of better treatment of animals (Young et al., 2018; Demello, 2018; Małecki et al., 2019). Additionally, some efforts to improve human empathy with animals have involved showing animals using digital tools such as iPads (Webber et al., 2017b). The results suggest that seeing animals as human-like indeed fosters empathy by heightening people’s awareness that animals also have thoughts and feelings (Webber et al., 2017a). However, some research has raised concerns that the use of technology in animals is unnatural or increases animal stress (Diana et al., 2021; Webber et al., 2017a). Extensive research has focused on the ability of conversational agents (also called chatbots) to shape people’s perceptions and engagement through a variety of narrative techniques (Bickmore et al., 2009; Park et al., 2023). To enhance chatbots’ facilitation of prosocial attitudinal and behavioral changes (Park et al., 2023), as well as their ability to provide mental-health support (Bickmore et al., 2009; de Gennaro et al., 2020; Lee et al., 2019), scholars have explored methods of humanizing them: for instance, assigning them names and/or portraits, and designing them to exhibit emotions (Shi et al., 2020; Liu and Sundar, 2018; Seeger et al., 2018). Humanizing chatbots may amplify the positive effects of other aspects of their design. For example, previous research has indicated that chatbots’ employment of first-person narratives can, in addition to increasing empathy and fostering attitudinal changes toward the characters represented in such narratives (Bickmore et al., 2009), increase user enjoyment during interactions (Lee et al., 2023). In short, chatbots have demonstrated their ability to influence users’ perceptions across various domains (Park et al., 2023; de Gennaro et al., 2020; Lee et al., 2019), and research to date suggests that a narrative from an animal’s first-person viewpoint could be effective in promoting empathy and changing human attitudes toward the animal, or toward animals in general (Beierl, 2008; James, 2019; Keen, 2006). Additionally, we suggest that chatbots could serve as a useful means of simulating human-animal interactions without jeopardizing animal welfare. Nevertheless, research on whether and how chatbots could be designed to represent animals, and how such representation might influence users’ empathy and attitudes toward animals, has hitherto been rare to nonexistent. To address this absence, we designed chatbots that speak from the perspective of an animal. Inspired by prior work on chatbots’ humanized design cues (e.g., identity, non-verbal and verbal cues) (Seeger et al., 2018; Shi et al., 2020; Liu and Sundar, 2018), we were interested in whether and how such cues could be redesigned to represent animals, and how this would affect user perceptions of chatbots’ identities. We therefore conducted a mixed-methods experiment with 240 participants, each of whom was randomly assigned to one of eight conditions, i.e., interaction with a chatbot that had zero, one, two, or all of the three above-mentioned cue types. All groups then engaged in interactive dialogue with their respective chatbots. We evaluated the participants’ empathy, attitudes, and intention to engage in prosocial behavior toward the animals portrayed in these narratives. Our results showed that a chatbot’s utilization of verbal cues enhanced participants’ empathy, whereas the use of non-verbal ones resulted in reduced empathy. Notably, first-person narratives by the chatbots succeeded in boosting participants’ perception of the chatbot’s identity as animal-like, and this led to more positive attitudes and increased prosocial behavioral intentions. This pioneering study makes the following contributions to the human-computer interaction community. First, it extends exploration of human-like chatbots into the domain of animal-like ones, and thus opens new pathways to understanding how non-human personas can be effectively utilized. Second, it demonstrates the ability of chatbots to influence their users’ perceptions of non-human entities. Specifically, we found that chatbots embodying an ‘animal perspective’ could significantly improve user empathy, attitudes, and prosocial behavioral intentions toward animals, particularly when they employed emotional expressions. Finally, this study provides a unique perspective on how human-chatbot interactions can be designed to have positive social impacts. These insights could pave the way for innovative applications in education and conservation, underscoring the potential of chatbot technology to contribute to more empathetic and better-informed societies."
https://arxiv.org/html/2411.06017v1,Provocation on Expertise in Social Impact Evaluations of Generative AI (and Beyond),"Social impact evaluations are emerging as a useful tool to understand, document, and evaluate the societal impacts of generative AI. In this provocation, we begin to think carefully about the types of experts and expertise that are needed to conduct robust social impact evaluations of generative AI. We suggest that doing so will require thoughtfully eliciting and integrating insights from a range of domain experts and experiential experts, and close with five open questions.","Social impact evaluations (e.g., [1]) are emerging as a useful tool to understand, document, and evaluate the societal impacts of generative AI; these evaluations take an important step towards the responsible development and enhanced accountability of artificial intelligence. Who develops categories of social impact—and who conducts these evaluations—shapes how generative AI is evaluated and held accountable. In this provocation, we begin to think carefully about the types of experts and expertise needed to conduct robust social impact evaluations. Without adequate expertise, there is a risk of developing evaluation criteria that fail to capture real-world harms or producing misleading evaluations that obscure critical issues. In this provocation we suggest that robust social impact evaluations require eliciting and integrating input from “domain experts” alongside “experiential experts.” As working definitions, domain experts refer to people who have received training or professionalization in a particular domain such as data science, legal studies, history, among others; experiential experts refer to “people who are living the experience or those closely associated with someone living the experience” [2]. In our use of the term, experiential experts are not only experts in their own lived experience but also experts in culture [3], organizational structure [4], local values [5], among others."
https://arxiv.org/html/2411.07118v1,ConvMixFormer- A Resource-efficient Convolution Mixer for Transformer-based Dynamic Hand Gesture Recognition,"Transformer models have demonstrated remarkable success in many domains such as natural language processing (NLP) and computer vision. With the growing interest in transformer-based architectures, they are now utilized for gesture recognition. So, we also explore and devise a novel ConvMixFormer architecture for dynamic hand gestures. The transformers use quadratic scaling of the attention features with the sequential data, due to which these models are computationally complex and heavy. We have considered this drawback of the transformer and designed a resource-efficient model that replaces the self-attention in the transformer with the simple convolutional layer-based token mixer. The computational cost and the parameters used for the convolution-based mixer are comparatively less than the quadratic self-attention. Convolution-mixer helps the model capture the local spatial features that self-attention struggles to capture due to their sequential processing nature. Further, an efficient gate mechanism is employed instead of a conventional feed-forward network in the transformer to help the model control the flow of features within different stages of the proposed model. This design uses fewer learnable parameters which is nearly half the vanilla transformer that helps in fast and efficient training. The proposed method is evaluated on NVidia Dynamic Hand Gesture and Briareo datasets and our model has achieved state-of-the-art results on single and multimodal inputs. We have also shown the parameter efficiency of the proposed ConvMixFormer model compared to other methods. The source code is available at https://github.com/mallikagarg/ConvMixFormer.","Deep learning models, particularly Convolutional Neural Networks (CNNs), have demonstrated the ability to automatically learn hierarchical representations of raw input data, through a series of convolutional and pooling layers [5]. With time, attention-based mechanisms came into existence that focus more on important features. Earlier, deep based [17, 31], RNN-based and LSTM-based [33] models were used for recognizing continuous gestures. Later, transformer network [41] was used for capturing information from sequential data [12] which takes single and multimodal inputs using a late fusion approach. Although transformers are very efficient in various applications, they pose some limitations in terms of performance and model complexity, specifically when used for computer vision tasks. For an image as input data, the sequential length corresponds to the number of tokens or patches extracted from the image. Even for a 224 \times 224 image size, the number of tokens can be significant, resulting in a quadratic increase in attention computations, since the attention scales quadratically in the self-attention module of the transformer model. Therefore, while designing the proposed model, we consider the transformer model’s drawbacks for gesture recognition. We propose a novel token mixer that involves convolution for mixing spatial tokens of the input gesture image called ConvMixFormer, which generalizes with fewer parameters than the vanilla transformer. The main idea of using convolution as a token mixer as it is efficient in terms of parameters and FLOPs. Meanwhile, convolution also helps extract local features from the input, enabling it to capture fine-grained details and spatial relationships. This is required in gesture recognition tasks as hand and finger movements are involved in gestures, which can be of different sizes depending on the signer’s age. By applying convolutions across the sequence, the model can capture interactions between adjacent tokens, allowing it to capture spatial dependencies and patterns. Convolutional layers also exhibit translation equivalence, meaning that if the input is translated, the output will also be translated by the same amount, which makes the model rotation and translation invariant. Overall, using convolution as a token mixer in transformer models offers several advantages, including local feature extraction, complexity reduction, interaction between tokens, and comparable performance with fewer parameters. However, in the literature, ConvMixer [40] is also proposed which uses depthwise convolution to mix spatial locations followed by pointwise convolution to mix channel locations. While depthwise convolutions offer advantages such as reduced computational cost and model size, they also pose some disadvantages compared to standard convolutions: (a) Depthwise separable convolutions consider independent input channels for processing which does not allow it to capture complex spatial patterns as in the case of convolutions. Therefore, it may struggle to capture inter-channel dependencies effectively. This limitation could result in a reduced ability to capture cross-channel spatial information. (b) Depthwise separable convolutions involve separate operations for each channel, leading to more memory accesses compared to standard convolutions, where each filter interacts with all input channels simultaneously. Keeping in view the limitations of the transformer model and mixers proposed in the literature, we proposed the standard convolution as the token mixer for dynamic gesture recognition. We also propose the use of a Gated mechanism in the feed-forward network named a Gated Depthwise Feed Forward Network, (GDFN) to control the flow of information within different stages of the proposed transformer model. Thus, the key contributions are: 1. We design a lightweight resource-efficient convolution-based transformer model, ConvMixFormer, for dynamic hand gesture recognition. 2. We propose a novel convolution token mixer to efficiently replace the attention mechanism in the transformer with the convolution layer that enables the model to capture complex spatial patterns from the input. 3. We propose the use of a Gated Depthwise Feed Forward Network (GDFN) which helps the control of the information flow in a sequential transformer model. This helps to focus on relevant features while suppressing irrelevant information, potentially improving the model’s robustness and generalization. 4. ConvMixFormer proves its efficacy on the Briareo and NVGesture dataset by achieving state-of-the-art performance with significantly fewer parameters on single and multimodal inputs."
https://arxiv.org/html/2411.07003v1,Enhancing Robot Assistive Behaviour with Reinforcement Learning and Theory of Mind,"The adaptation to users’ preferences and the ability to infer and interpret humans’ beliefs and intents, which is known as the Theory of Mind (ToM), are two crucial aspects for achieving effective human-robot collaboration. Despite its importance, very few studies have investigated the impact of adaptive robots with ToM abilities. In this work, we present an exploratory comparative study to investigate how social robots equipped with ToM abilities impact user’s performance and perception. We design a two-layer architecture. The Q-learning agent on the first layer learns the robot’s higher-level behaviour. On the second layer, a heuristic-based ToM infers the user’s intended strategy and is responsible for implementing the robot’s assistance, as well as providing the motivation behind its choice. We conducted a user study in a real-world setting, involving 56 participants who interacted with either an adaptive robot capable of ToM, or with a robot lacking such abilities. Our findings suggest that participants in the ToM condition performed better, accepted the robot’s assistance more often, and perceived its ability to adapt, predict and recognise their intents to a higher degree. Our preliminary insights could inform future research and pave the way for designing more complex computation architectures for adaptive behaviour with ToM capabilities.","Cognitive stimulation is crucial for maintaining and improving abilities such as memory, attention, and executive function [59]. Regular engagement in activities that challenge and stimulate the brain has been shown to positively impact cognitive health and can delay the onset of age-related declines [23]. The use of socially assistive robots (SARs) in memory exercises [1, 36] has the potential to provide a personalised and engaging platform for delivering cognitive stimulation, offering a unique and interactive experience for users [29]. Indeed, robots have been proven to be very effective in performing simple, and repetitive tasks, making them a perfect tool to support healthcare professionals and enhance their effectiveness during their daily working routine [52]. Nonetheless, for robots to be most effective in providing assistance, they must cater to the specific needs of users and aim to prevent negative emotions such as frustration (due to overly difficult exercises) or boredom (due to overly simple exercises) during cognitive exercises. It is important that the robot’s approach aligns precisely with the user’s requirements to achieve optimal results. In recent years, several studies have demonstrated the impact of robot adaptivity on users’ performance as well as on their engagement in assistive tasks [7, 53, 21]. However, adaptation in the robot’s behaviour may result in affecting people’s ability to understand and predict the robot, impacting their trust. To avoid this, it has to be noted that humans are more likely to cooperate with machines with whom they can share a mental representation. Hence, the robot should be able to have a Theory of Mind (ToM) of the users over time and personalise its behaviour according to the inferred beliefs and intentions [45, 48, 49]. Despite those works, very little is known about how to effectively combine adaptivity with ToM to improve task performance and increase the user’s perceived competence of the robot [11]. Therefore, in this work, we aim to fill this research gap. We build upon our previous knowledge, wherein we demonstrated how a robot can tailor its degrees of assistance to patients affected by cognitive decline [5]. Here, we go a step further by endowing the robot with the capability to understand users’ strategies through a process of mentalising and, therefore, use that information to provide qualitatively better assistance. This study takes an exploratory approach to address the following research question through a comparative study: would a robot endowed with adaptive socially assistive behaviour and ToM abilities have a different impact on users’ performance and perceived robot capabilities compared to an adaptive robot without ToM abilities? Figure 1: A user playing the memory game with the assistance of the Furhat robot. To tackle this research question, we propose a computational approach in which a robot learns the socially assistive behaviour that best fits the users while providing advice that relies on the user’s beliefs and their intended strategies to solve a memory game (see Figure 1). Specifically, we create a two-layer architecture. The upper level, namely the learning layer, includes a Q-learning-based agent trained in simulation to model an imperfect player and learn the policy that best fits the user’s needs (e.g., suggest the card). The lower level, namely the mentalising layer, consists of a heuristic-based ToM that based on the previous history of users’ moves is used to estimate their strategies and their beliefs about the card’s selection. Here, ToM has employed both for operationalising the assistance provided by the RL (e.g., suggest shark) and explain the rationale behind its hint (“You have seen the shark several times, the other card is in row 1 col 2, you should remember the location”). To evaluate our system, we carried out a user study in a real-world setting. N=56 untrained participants played a memory game with the assistance of the Furhat robot during a national fair. We found that participants who were assisted by a robot capable of ToM performed better, accepted the hints provided by the robot more frequently, and perceived the robot as more capable of adapting, predicting, and recognising their intentions in comparison to those participants who interacted with a robot without ToM abilities. The findings of this study provide insights that can inform future research into the design of robots that exhibit adaptive behaviour tailored to users’ capacities, while also incorporating ToM capabilities. In summary, the contributions of our study are the following: • Development of a hierarchical architecture that learns: i) socially assistive actions in simulation along, with ii) user’s intended strategies in real interactions, • Deployment and evaluation of a social robot endowed with such architecture in a real-world setting with 56 untrained participants."
https://arxiv.org/html/2411.06950v1,Sniff AI: Is My ‘Spicy’ Your ‘Spicy’? Exploring LLM’s Perceptual Alignment with Human Smell Experiences,"Aligning AI with human intent is important, yet perceptual alignment—how AI interprets what we see, hear, or smell—remains underexplored. This work focuses on olfaction, human smell experiences. We conducted a user study with 40 participants to investigate how well AI can interpret human descriptions of scents. Participants performed ""sniff and describe"" interactive tasks, with our designed AI system attempting to guess what scent the participants were experiencing based on their descriptions. These tasks evaluated the Large Language Model’s (LLMs) contextual understanding and representation of scent relationships within its internal states - high-dimensional embedding space. Both quantitative and qualitative methods were used to evaluate the AI system’s performance. Results indicated limited perceptual alignment, with biases towards certain scents, like lemon and peppermint, and continued failing to identify others, like rosemary. We discuss these findings in light of human-AI alignment advancements, highlighting the limitations and opportunities for enhancing HCI systems with multisensory experience integration.","Aligning Artificial intelligence (AI) behaviour with human preference is critical for the future of AI. An important yet often overlooked aspect of this alignment is the perceptual alignment. Perceptual alignment refers to the agreement between AI assessments and human subjective judgments across different sensory modalities, such as vision, hearing, taste, touch, and smell [1, 2, 3]. It enables AI to better understand the physical world as humans experience it, ensuring that AI applications are reliable and beneficial in real-world settings. For example, consider autonomous vehicles: if the ""AI eye"" misinterprets data from sensors such as cameras and fails to recognize obstacles or pedestrians, it poses significant safety risks [4]. Beyond safety considerations, perceptual alignment plays a critical role in everyday AI applications [3], whereas olfactory alignment remains completely unexplored. Imagine a future where AI assistants are capable of controlling environmental factors like lighting and scents based on user requests. For instance, instead of asking ""Alexa, play uplifting workout music"", you can ask Alexa to ""spice up my workout session"" or ""help me remember my holiday to Madrid"". Here, the challenge for AI goes beyond playing music and ventures into ""AI sniff"", to select the ideal scent aligned with human descriptions. This poses the question of how AI would understand and interpret scents in a way that resonates with our personal sensory experiences. Or simply: Is my ""spicy"" AI’s ""spicy""? AI, or mostly Large Language Models (LLMs), lack human-like perceptions; rather, they process human provided language inputs using algorithms that function on binary systems to analyse information [5, 6, 7, 8, 9]. We place a special emphasis on LLMs as they are being increasingly viewed as the interface for human interaction, with agentic AI systems and general alignment being one of its core research domains [10, 11]. LLMs, or any neural network-based learning systems, represent concepts and ideas in what is commonly known as an embedding space – a learned internal high-dimensional vector space [12, 13]. In this space, semantically similar items cluster closely together, and the semantic differences between items are preserved [14]. This naturally lends us a hand in analyzing how closely aligned humans and AI are. If two items are deemed similar within the AI system’s embedding space, humans should also perceive them as similar, if human-AI alignment exists. Our work then exploits exactly this approach and focuses on analyzing the model’s embedding space. We leverage LLM-based embedding models to develop an AI system capable of performing the ""Human sniff and describe and AI guesses"" task. Here, the LLM encoder translates human language descriptors of scents into the embedding space, and the system makes then scent suggestions based on the semantic similarities measured in this vector space. Given the recent advancements in LLMs’ ability to interpret human language, an intriguing question arises: can LLMs effectively understand scents based on user descriptions? For instance, will both LLMs and humans agree on Jasmine and Ylang-Ylang being perceptually similar? To investigate this question, we conducted an in-person user study with 40 participants, where participants engaged in interactive tasks where an AI system had to guess what scent they were experiencing. These participants, who were non-domain experts, were specifically chosen to reflect common perceptions of scents as conceptualized by Henning’s Odour Prism [15], the Fragrance Wheel [16], and attributes such as the fresh, citrusy, and zesty qualities typically associated with lemon. We analysed the system’s performance using both quantitative and qualitative methods to capture the participant feedback. Our findings suggest that scent-related semantics are represented in the embedding space, though to a limited extent. There is some degree of perceptual alignment, but it was biased toward certain scents and characteristics. For instance, sometimes the AI believed the human description of ""an aromatic plant that probably you use for like stew or for like chicken and it’s very green and is fresh"" refers to eucalyptus rather than rosemary. Occasionally, participants were surprised by certain emergent behaviours; for instance, the AI correctly identified a ""intense masculine scent"" as oakmoss. We discuss these findings in light of recent advancements of LLMs and efforts towards improved human-AI alignment, with the opportunity to enhance HCI systems with multisensory experience integration."
https://arxiv.org/html/2411.06528v1,Epistemic Integrity in Large Language Models,"Large language models are increasingly relied upon as sources of information, but their propensity for generating false or misleading statements with high confidence poses risks for users and society. In this paper, we confront the critical problem of epistemic miscalibration — where a model’s linguistic assertiveness fails to reflect its true internal certainty. We introduce a new human-labeled dataset and a novel method for measuring the linguistic assertiveness of Large Language Models (LLMs) which cuts error rates by over 50% relative to previous benchmarks. Validated across multiple datasets, our method reveals a stark misalignment between how confidently models linguistically present information and their actual accuracy. Further human evaluations confirm the severity of this miscalibration. This evidence underscores the urgent risk of the overstated certainty LLMs hold which may mislead users on a massive scale. Our framework provides a crucial step forward in diagnosing this miscalibration, offering a path towards correcting it and more trustworthy AI across domains.","Large Language Models (LLMs) have markedly transformed how humans seek and consume information, becoming integral across diverse fields such as public health (Ali et al., 2023), coding (Zambrano et al., 2023), and education (Whalen & et al., 2023). Despite their growing influence, LLMs are not without shortcomings. One notable issue is the potential for generating responses that, while convincing, may be inaccurate or nonsensical—a long-standing phenomenon often referred to as “hallucinations” (Jo, 2023; Huang et al., 2023; Zhou et al., 2024b). This raises concerns about the reliability and trustworthiness of these models. A critical aspect of trustworthiness in LLMs is epistemic calibration, which represents the alignment between a model’s internal confidence in its outputs and the way it expresses that confidence through natural language. Misalignment between internal certainty and external expression can lead to users being misled by overconfident or underconfident statements, posing significant risks in high-stakes domains such as legal advice, medical diagnosis, and misinformation detection. While of great normative concern, how LLMs express linguistic uncertainty has received relatively little attention to date (Sileo & Moens, 2023; Belem et al., 2024). Figures 1 and 5 illustrate the issue of epistemic calibration providing insights into the operation of certainty in the context of human interactions with LLMs. We highlight the following key points in these figures: • Distinct Roles of Certainty: Internal certainty and linguistic assertiveness have distinct functions within LLM interactions that shape individual beliefs. • Human access to LLM certainty: Linguistic assertiveness holds a critical role as the primary form of certainty available to users. Unlike internal certainty, which remains hidden within the model’s computational processes, linguistic assertiveness is directly perceivable and influences how users interpret the model’s outputs. • Beyond Content: Users retrieve more than just the content from an LLM’s output. The style and assertiveness of the language used also play a significant role by shaping perceptions through the communication of certainty. This interaction between the model’s output and its linguistic assertiveness is crucial for understanding the full impact on individual perceptions. \mdfdefinestyle MyFrame linecolor=black, {mdframed} [style=MyFrame,nobreak=true,align=center,userdefinedwidth=32em] High Epistemic Calibration User input: ""Have humans ever been on the moon?"" LLM output: ""Yes, humans have been on the moon."" Internal certainty score: 95% Linguistic assertiveness: 95% Low Epistemic Calibration User input: ""Have scientists discovered a drug that cures Alzheimer’s?"" LLM output: ""Scientists have not discovered a drug that cures Alzheimer’s."" Internal certainty score: 60% Linguistic assertiveness: 95% Figure 1: This figure illustrates two examples with varying levels of epistemic calibration in LLM outputs. The one below is poorly calibrated. For each output, we calculate two certainty scores: internal certainty and external certainty (linguistic assertiveness). The internal certainty is computed using the method outlined by Rivera et al. (2024). To assess linguistic assertiveness, we develop a custom model, which we validate using human ratings collected through a survey. Several studies have explored the calibration of internal confidence in LLMs. For instance, Zhang et al. (2024) examine confidence calibration, proposing techniques to reduce hallucinations and enhance the model’s ability to answer known questions while avoiding unknown ones. However, they overlook the role of linguistic assertiveness and how external certainty can still lead to epistemic miscalibration even if internal confidence is addressed. Similarly, Ren et al. (2023) focus on factual knowledge and LLM behavior before and after retrieval-augmented generation (RAG). While they investigate internal confidence, they fail to frame miscalibration as an end-to-end issue involving both internal certainty and linguistic assertiveness, therefore ignoring the interplay between model predictions and how confidence is expressed linguistically. More recent studies have attempted to bridge the gap between internal confidence and linguistic assertiveness but still face considerable limitations. Although Mielke et al. (2022) explore epistemic calibration, their study uses a limited scoring scale to measure both assertiveness and confidence, which restricts the continuous assessment of LLM output. Their approach also relies on a narrow range of datasets, thereby limiting its applicability across domains. Zhou et al. (2024a) address miscalibration using epistemic markers, but their method lacks real domain grounding and fails to consider the complexity of natural language. This review of existing work on LLM calibration and confidence reveals several gaps that our research aims to address: • Lack of Integrated Approaches: Previous studies address either internal certainty or linguistic assertiveness but rarely both simultaneously (Jiang et al., 2021). There is a need for comprehensive frameworks that integrate these aspects to ensure LLMs communicate accurately and responsibly. • Inadequate Assertiveness Measurement: Existing methods for measuring linguistic assertiveness rely heavily on lexicon-based approaches (Pei & Jurgens, 2021) or subjective perceptions without adequate validation (Steyvers et al., 2024). These methods often lack contextual depth and fail to generalize across diverse domains. • Limited High-Stakes Evaluation: Although some studies explore epistemic calibration, they cover a narrow range of topics and employ low-resolution measures of assertiveness, limiting their applicability in critical domains such as misinformation detection (Mielke et al., 2022). To address these gaps, our paper provides: • A New Assertiveness Detection Model: We train a new model to detect linguistic assertiveness, using a composite of five diverse datasets. Our approach improves the accuracy relative to previous approaches by incorporating contextual nuance and aligning more closely with human perceptions. We also address limitations in previous methods in generalizably measuring assertiveness across domains. • Empirical Evidence of Epistemic Miscalibration: Our work provides a comprehensive comparison between internal certainty and linguistic assertiveness, documenting instances of miscalibration in different contexts. Our experiments reveal that LLMs frequently generate highly assertive explanations despite low internal certainty, which can mislead users. • Validation with Human Perception: We conduct comprehensive surveys assessing human perceptions of LLMs’ linguistic assertiveness. Our results confirm that there is a misalignment between computational measures and subjective human perceptions of language, highlighting the need for more robust linguistic calibration of LLMs. This study also presents a novel human-centered approach for developing a robust assertiveness scoring method. To ensure reliability, we train and compare several models to identify the best estimator for assertiveness based on accuracy and transferability using a new multi-domain dataset hand-coded to measure certainty. After selecting the top-performing model, we validate the results using a different human-surveyed dataset (Wang, 2017), which was coded independently by different individuals. This comprehensive methodology enables us to thoroughly assess both the objective and subjective aspects of assertiveness in language model explanations. Our findings reveal that when the model has low internal certainty, it generates explanations that are significantly over-assertive, meaning the language used implies a higher degree of certainty than is warranted by the model’s actual confidence or accuracy. This miscalibration could lead users to misconstrue the model’s judgments as more reliable than they actually are. More precisely, our results confirm a strong correlation between GPT 4o’s assertiveness scores and human perceptions of assertiveness, but a weak correlation between human perceptions and internal certainty, and an even weaker relationship between GPT 4o model assertiveness and internal certainty. Together, these results provide the most extensive evidence of epistemic miscalibration in LLMs to date.111For the code and datasets used, refer to our GitHub repository at: https://github.com/ComplexData-MILA/epistemic-integrity."
https://arxiv.org/html/2411.06306v1,Optimal Driver Warning Generation in Dynamic Driving Environment,"The driver warning system that alerts the human driver about potential risks during driving is a key feature of an advanced driver assistance system. Existing driver warning technologies, mainly the forward collision warning and unsafe lane change warning, can reduce the risk of collision caused by human errors. However, the current design methods have several major limitations. Firstly, the warnings are mainly generated in a one-shot manner without modeling the ego driver’s reactions and surrounding objects, which reduces the flexibility and generality of the system over different scenarios. Additionally, the triggering conditions of warning are mostly rule-based threshold-checking given the current state, which lacks the prediction of the potential risk in a sufficiently long future horizon. In this work, we study the problem of optimally generating driver warnings by considering the interactions among the generated warning, the driver behavior, and the states of ego and surrounding vehicles on a long horizon. The warning generation problem is formulated as a partially observed Markov decision process (POMDP). An optimal warning generation framework is proposed as a solution to the proposed POMDP. The simulation experiments demonstrate the superiority of the proposed solution to the existing warning generation methods.","I INTRODUCTION The road traffic plays an important role in people’s lives. With the development of the complexity of city road networks, it is crucial for an advanced driver assistance system to be able to alert the potential risks to the human driver during driving. As shown in the studies of the human driver behavior with the warning system [1, 2, 3], existing driver warning technologies, mainly the forward collision warning and unsafe lane change warning, can reduce the risk of collision caused by human errors. However, studies show that the human drivers’ reactions to warnings vary with the type of warning and different drivers [4, 5], while the existing methods have not addressed this phenomenon adequately. Most methods in the literature mainly generate the warning in a one-shot manner without modeling the ego driver’s reactions and surrounding objects [6, 7, 8], which reduces the flexibility and generality of the system over different drivers and scenarios. Meanwhile, the triggering conditions of warning are mostly rule-based threshold-checking based on the current state, such as the time-to-collision (TTC) and the minimum safety distance [9, 10, 11], which lacks the prediction of the potential risk in a sufficiently long future horizon. As a consequence, the current warning systems, while effective in preventing collisions, tend to prompt urgent and uncomfortable braking actions. Studies have emphasized the importance of executing smoother and more comfortable braking maneuvers to assist drivers in avoiding not only identified dangers but also collisions with subsequent vehicles [12]. This work seeks to address these issues by formulating an optimal warning generation problem that considers the relation between the generated warning and the driver’s reaction and also the interaction between the ego vehicle and other agents on a long horizon. The problem is modeled as a partially observed Markov decision process (POMDP), in which we quantify the value of warnings through the comfort and safety of the future ego vehicle trajectory in the context of surrounding objects, and the cost through their format and frequency. An optimal warning generation framework is proposed as a solution to the POMDP. The key contributions of this work are as follows: - We propose a novel formulation of the optimal warning generation problem that considers the driver and surrounding vehicle reactions, and both the safety and the comfort of future ego trajectories. - We propose a warning generation framework combining driver behavior estimation as the solution to the above problem. The framework has the flexibility to incorporate any prediction models of the driving scenario. - The proposed method is evaluated over comprehensive closed-loop simulation experiments, which demonstrates the superiority of the proposed solution to the existing warning generation methods."
https://arxiv.org/html/2411.06262v1,Security Implications of User Non-compliance Behavior to Software Updates: A Risk Assessment Study,"Software updates are essential to enhance security, fix bugs, and add better features to the existing software. However, while some users comply and update their systems upon notification, non-compliance is common. Delaying or ignoring updates leaves systems exposed to security vulnerabilities. Despite research efforts, users’ noncompliance behavior with software updates is still prevalent. In this study, we explored how psychological factors influence users’ perception and behavior toward software updates. In addition, we proposed a model to assess security risk score associated with delaying software updates. We conducted a user study with Windows OS users to explore how information about potential vulnerabilities and risk scores influence their behavior. Furthermore, we also studied the influence of demographic factors such as gender on users’ decision-making process for software updates. Our results showed that psychological traits, such as knowledge, awareness, and experience, impact users’ decision-making about software updates. To increase users compliance, providing a risk score for not updating their systems and information about vulnerabilities statistically significantly increased users’ willingness to update their systems. Additionally, our results indicated no statistically significant difference in male and female users’ responses in terms of concerns about securing their systems. The implications of this study are relevant for software developers and manufacturers as they can use this information to design more effective software update notification messages. Highlighting potential risks and corresponding risk scores in future software updates can motivate users to act promptly to update the systems in a timely manner, which can ultimately improve the overall security of the system.","A software update involves making adjustments to improve or fix issues with the software. The updates can range from minor changes to significant enhancements or the addition of new features. These updates are important not only for fixing vulnerabilities and bugs but also for maintaining the security of the software [1]. One of the most important features of any modern security system is its capacity for releasing effective and safe software upgrades [2]. Software update mechanisms try to ensure accessibility, efficiency, robustness, and expandable distribution of software updates to facilitate the timely application of security patches [3, 4, 5, 6]. In 2021, a vulnerability known as Log4j or Log4Shell impacted approximately three billion computer systems and applications [7]. This attack was considered ”critical,” with a severity score of 10 according to the National Vulnerability Database (NVD) [8]. According to the experts, this attack could have been prevented if the available software update had been applied [9]. As per a study conducted in 2022, many security breaches occur because of uninstalled updates for vulnerabilities that were available [10]. In general, vulnerable software is usually targeted by cyber attackers, especially if that software has widespread use and a large number of users, such as Microsoft Office, Adobe Acrobat, and Internet Explorer [11]. While technical remedies for security concerns, such as releasing software patches, hold significance, improving human awareness toward security practices is indispensable for achieving cyber safety. The security of a computer system often relies on how users understand, behave, and make security-related decisions[12, 13, 14, 15]. Failing to install updates for identified vulnerabilities can lead to severe security breaches. Previous work suggested that many users do not perceive all updates as equally significant or prioritize them appropriately [16]. This can leave systems exposed to potential threats and undermine the effectiveness of security measures. Prior survey-based studies delved into users’ behavioral studies and found users’ unfavorable behaviors impact both individual security [17] and organizations’ security stance [18]. In organizations, approximately 27% of data breaches are caused by not adopting common security and privacy measures by the end users [19]. Similarly, end users often fail to adopt common security and privacy measures.[20, 21]. One of the main reasons for security violations is a failure to apply the patch for a known vulnerability, as most of the exploitation occurs in systems that are not updated[22]. Microsoft reported that most of its customers are breached via vulnerabilities that had patches released years ago. This indicates users’ non-compliance behavior towards applying patches [22]. Another study presented that about 80% of companies with a data breach or failed audit could have prevented that by patching on time or doing configuration updates [23]. According to a statistic, 84% of the companies have high-risk vulnerabilities on their external networks; more than half of those vulnerabilities could have been simply removed just by installing the update [24]. Once vulnerabilities are discovered, developers typically work fast to release an update or patch. On average, patch release time for a vulnerability ranges between 23 to 40 days [25, 26]. According to another study, after the discovery of the vulnerability, the white-hat developers generally take 30 to 45 days to make the patch available [27]. In this timeline, if the vulnerability is discovered by the black-hat community (i.e., hackers with malicious intent), then the vulnerability could be exploited within zero days. Frei [28] found that 78% of exploitations take place within a day, and 94% do so within 30 days of the public disclosure day. However, studies suggest that the gap between public disclosure and exploitation is decreasing, and the exploitation process now takes around five days [29]. Furthermore, prior research has also shown that there are differences in user behaviors based on gender [30]. Considering a set of security and privacy behaviors, researchers found female users’ levels are significantly lower than male users in 40% security and privacy behaviors. Also, female users find less interest in adopting technical skills than male users [31]. Hence, it is clear that software security not only depends on the timely release of software updates but also on users’ compliance in updating their systems to prevent attacks[32, 33]. As the previous works present, people are negligent with respect to applying updates and prefer to delay the process, while early updating applications could make the system secure and protect it from unwanted attacks. This study is focused on analyzing how delaying software updates could increase cyber risk and investigating software update behaviors of users when they have information about vulnerabilities and risk scores. Additionally, in the field of psychology, research has extensively explored attitudes, including their correlation, antecedents and consequences, and correlation with intentions and behavior [34, 35]. To gain a better understanding of users’ security attitudes, we analyzed their attitudes towards cybersecurity. In this study, we investigated end-users’ non-compliance behavior and their perspective toward software updates. We proposed a framework to assess how delaying updating software can increase security risk. Next, we developed a questionnaire combining skills, awareness, experience, and knowledge-based questions. We designed our questionnaire to evaluate users’ behavior and changes in their decision-making process before and after having proper vulnerability and security risk-related information. Further, using a combination of factors and statistical analysis, we identified which factors increase users’ awareness and influence them to update software to secure their system. Additionally, we extended our research to determine if male and female end users’ behavior and perception differ when they have the same security-related information. With this extension, we looked into the discrepancies in gender-based security behavior. To achieve the above-stated research objectives, we focused on the following research questions: • RQ1: How do users’ cognitive states affect their adoption of software updates? • RQ2: To what extent does the vulnerability and risk score information improve users’ software update compliance behavior? • RQ3: What difference does gender make in software update decision-making? This paper has been organized as follows. Section 2: described background of CVSS. Section 3 discusses the related work on users’ software update behaviors and risk estimation from software update non-compliance. Section 4 presents the methodologies for risk-score assessment associated with software update delays and survey study. Section 5 explains the analysis and results. Finally, Section 6 concludes the paper, and Section 7 discusses limitations and future works."
https://arxiv.org/html/2411.05856v1,Evaluating the Economic Implications of Using Machine Learning in Clinical Psychiatry,"With the growing interest in using AI and machine learning (ML) in medicine, there is an increasing number of literature covering the application and ethics of using AI and ML in areas of medicine such as clinical psychiatry. The problem is that there is little literature covering the economic aspects associated with using ML in clinical psychiatry. This study addresses this gap by specifically studying the economic implications of using ML in clinical psychiatry. In this paper, we evaluate the economic implications of using ML in clinical psychiatry through using three problem-oriented case studies, literature on economics, socioeconomic and medical AI, and two types of health economic evaluations. In addition, we provide details on fairness, legal, ethics and other considerations for ML in clinical psychiatry.","With the success of artificial intelligence (AI) and machine learning (ML) within areas such as transportation and finance, there is an increasing interest in using those within areas of medicine. One of the areas of interest is psychiatry. There is a growing interest in applying ML in clinical practice within psychiatry, and more recently, research is being conducted to understand its effectiveness in psychiatry. However, no research has investigated the economic implications associated with its use. Our study addresses this gap by evaluating the economic implications of using ML in clinical psychiatry. This paper will first review the current situation of clinical psychiatry and economics, and the economic, socioeconomic, and medical incentives for ML in clinical psychiatry. Then, using three cases studies, we will evaluate the economic implications of using ML in clinical psychiatry. We will conclude by discussing ethical, legal and other considerations for ML in clinical psychiatry."
