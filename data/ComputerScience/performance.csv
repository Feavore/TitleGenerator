URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10069v1,Layer Importance and Hallucination Analysis in Large Language Models via Enhanced Activation Variance-Sparsity,"Evaluating the importance of different layers in large language models (LLMs) is crucial for optimizing model performance and interpretability. This paper first explores layer importance using the Activation Variance-Sparsity Score (AVSS), which combines normalized activation variance and sparsity to quantify each layer’s contribution to overall model performance. By ranking layers based on AVSS and pruning the least impactful 25%, our experiments on tasks such as question answering, language modeling, and sentiment classification show that over 90% of the original performance is retained, highlighting potential redundancies in LLM architectures. Building on AVSS, we propose an enhanced version tailored to assess hallucination propensity across layers (EAVSS). This improved approach introduces Hallucination-Specific Activation Variance (HSAV) and Hallucination-Specific Sparsity (HSS) metrics, allowing precise identification of hallucination-prone layers. By incorporating contrastive learning on these layers, we effectively mitigate hallucination generation, contributing to more robust and efficient LLMs(The maximum performance improvement is 12%). Our results on the NQ, SciQ, TriviaQA, TruthfulQA, and WikiQA datasets demonstrate the efficacy of this method, offering a comprehensive framework for both layer importance evaluation and hallucination mitigation in LLMs.","Evaluating the importance of different layers in deep learning models is crucial for improving model efficiency, interpretability, and robustness. Identifying key layers allows for effective model compression and a more informed model design. Recently, large language models (LLMs) have shown remarkable capabilities across diverse applications, including question answering, language modeling, and sentiment analysis. However, there is limited research on the functional contributions of individual layers in LLMs, particularly from the perspective of activation variance and sparsity, which could reveal each layer’s unique role in model performance and interpretability Wang et al. (2024); Xiong et al. (2020). Moreover, studies specifically focusing on hallucination propensity based on layer activation patterns in LLMs remain largely unexplored, leaving a critical gap in understanding and mitigating layer-specific hallucination generation. Previous works on layer importance have introduced several sophisticated methodologies. Saarela et al. Saarela and Jauhiainen (2021) proposed Gradient-Based Importance Scores (GBIS), which assess layer importance by calculating the sensitivity of gradients relative to inputs, thereby reflecting model reliance on each layer’s activations. Zopf et al. Bach et al. (2015) introduced Layer-wise Relevance Propagation (LRP), analyzing information flow through the model and helping to understand the role of each layer in the model’s decision process. Additionally, Mencía et al. Zopf et al. (2016) developed Contextual Importance Measures (CIM), dynamically evaluating layer importance based on specific input conditions. While these methods offer valuable insights, they often fall short in capturing complex activation patterns and identifying redundancy in LLMs, particularly as model depth and size increase. Figure 1: Illustration of the Activation Variance-Sparsity Score (AVSS) method for assessing layer importance in large language models. (a) Layer Structure: Overview of model layers (1 to 32) analyzed for activation properties. (b)Activation Variance and Sparsity: Top: High-variance layers capture diverse information. Bottom: Darker cells indicate sparse activations, suggesting redundancy. (c) AVSS Calculation and Ranking: AVSS, normalized AVSS, and cumulative AVSS formulas are used to rank layers, identifying low-scoring layers as pruning candidates. In this work, we propose an enhanced approach, the Activation Variance-Sparsity Score (AVSS), to evaluate layer importance in LLMs. AVSS combines normalized activation variance and sparsity to quantify each layer’s role in model performance. By ranking layers based on AVSS and removing approximately the lowest 25% of layers, we retain over 90% of the original model performance on tasks such as question answering, language modeling, and sentiment analysis, indicating potential redundancy within LLM architectures.Achiam et al. (2023); Azadi et al. (2023); Azaria and Mitchell (2023); Bai et al. (2022); Bradley (1997) To address the unexplored area of hallucination generation across layers, we extend AVSS to introduce the Enhanced Activation Variance-Sparsity Score (EAVSS), a framework designed to quantify hallucination propensity within each layer of LLMs. By incorporating Hallucination-Specific Activation Variance (HSAV) and Hallucination-Specific Sparsity (HSS), EAVSS precisely identifies hallucination-prone layers based on their unique activation patterns during hallucination events. The EAVSS method fills a significant gap in LLM research, providing a comprehensive layer-wise analysis of hallucination potential. Moreover, we apply contrastive learning on layers with high hallucination scores, effectively mitigating hallucination generation and contributing to improved model robustness and reliability. Brier (1950); Burns et al. (2023); Chen et al. (2024a, b); Chiang et al. (2023); Chuang et al. (2024); Cohen et al. (2023); Daheim et al. (2024) The main contributions of our paper are as follows: • We propose the Activation Variance-Sparsity Score (AVSS) as a novel metric for evaluating layer importance in LLMs, combining variance and sparsity to improve interpretability and performance retention. • We introduce an enhanced AVSS framework for assessing hallucination propensity, using Hallucination-Specific Activation Variance (HSAV) and Hallucination-Specific Sparsity (HSS) to identify and target hallucination-prone layers. • We demonstrate that a contrastive learning approach on high-hallucination layers can effectively mitigate hallucination generation, contributing to improved model robustness and efficiency."
https://arxiv.org/html/2411.08069v1,Intelligent Green Efficiency for Intrusion Detection,"Artificial Intelligence (AI) has emerged in popularity recently, recording great progress in various industries. However, the environmental impact of AI is a growing concern, in terms of the energy consumption and carbon footprint of Machine Learning (ML) and Deep Learning (DL) models, making essential investigate Green AI, an attempt to reduce the climate impact of AI systems. This paper presents an assessment of different programming languages and Feature Selection (FS) methods to improve computation performance of AI focusing on Network Intrusion Detection (NID) and cyber-attack classification tasks. Experiments were conducted using five ML models - Random Forest, XGBoost, LightGBM, Multi-Layer Perceptron, and Long Short-Term Memory - implemented in four programming languages - Python, Java, R, and Rust - along with three FS methods - Information Gain, Recursive Feature Elimination, and Chi-Square. The obtained results demonstrated that FS plays an important role enhancing the computational efficiency of AI models without compromising detection accuracy, highlighting languages like Python and R, that benefit from a rich AI libraries environment. These conclusions can be useful to design efficient and sustainable AI systems that still provide a good generalization and a reliable detection.","The rapid growth of Artificial Intelligence (AI) is creating remarkable advancements in many fields, but its impact on the environment is becoming an increasingly important issue [37]. A 2020 paper by Patterson et al. [24] estimated that training a large AI model, such as OpenAI’s GPT-3, could consume about 1,287 MWh of energy, leading to CO2 emissions equivalent to those produced by 125 average homes in the U.S. in one year. According to an estimate from the World Economic Forum (WEF), the information and communications technology (ICT) sector, which includes AI, by 2020 was already accountable for between 1.4% to 5.9% of global greenhouse gas emissions [34]. According to another estimate from the WEF, computing is expected to account for up to 8% of global power demand by 2030 [5]. Considering these concerns and the cruciality of intrusion detection caused by the increasing threat of cyber-attacks [22], this research seeks to optimize security while minimizing environmental impact [9]. Green AI [43] highlights the importance of optimizing AI systems to minimize their energy consumption and environmental impact without compromising their performance [31]. The efficient usage of computational resources can definitely lead to achieving this goal [41], which gets affected by various factors like the choice of a programming language [25], the usage of technologies like Feature Selection (FS) [19], the hardware efficiency, the ML algorithm efficiency and renewable energy integration. FS in ML is the process of identifying and selecting the most relevant features from a dataset for model training [19]. It can be implemented using Filter, Wrapper, and Embedded methods [21]. Filter methods evaluate features based on their statistical properties, making them efficient for high-dimensional data. Wrapper methods assess model performance with different feature sets, considering feature-model interactions, but can be computationally intensive and prone to overfitting. Embedded methods integrate FS into the model-building process, balancing accuracy and efficiency with lower computational complexity than Wrapper methods. At the same time, the impact of FS on computational efficiency varies depending on the programming language choice. Despite being often considered, programming languages also play an essential role in impacting energy usage and computation speed [25]. This occurs because some languages have better optimized themselves to the hardware they are running on. Usually,compiled languages such as C++ take less time to execute than interpreted languages such as Python [17]. Other characteristics, such as memory handling, may also affect the performance and efficiency of the algorithms. This paper addresses the Green AI use to improve computational efficiency in Intrusion Detection Systems (IDS), seeking to optimize security while minimizing environmental impact. It assesses the choice of programming languages and FS and their relation to computational efficiency in the Green AI context and IDS. Several experiments were performed to compare quality metrics, namely accuracy, precision, recall, and F1, and footprint metrics, namely training and prediction time, in different programming languages and FS techniques. It aims to provide useful information to researchers and practitioners to help create environmentally sustainable AI systems. This paper includes five ML models (Random Forest (RF), XGBoost (XGB), LightGBM (LGBM), Multi-Layer Perceptron (MLP), and Long Short-Term Memory (LSTM)), four programming languages (Python, Java, R, Rust), and three FS methods (Chi-Square, Information Gain (IG), Recursive Feature Elimination (RFE)). This paper is organized into multiple sections, including details, that might help researchers to replicate this baseline, and make comparisons with their own results. Section 2 presents related works. Section 3 details data preprocessing techniques, FS, methodology followed, and how model fine-tuning processes were carried out. Section 4 goes into detail regarding the results obtained on all datasets. Finally, Section 5 discusses the main conclusions that have been drawn and proposes some future research directions."
https://arxiv.org/html/2411.07687v1,OSCAR-P and aMLLibrary: Profiling and Predicting the Performance of FaaS-based Applications in Computing Continua,"This paper proposes an automated framework for efficient application profiling and training of Machine Learning (ML) performance models, composed of two parts: OSCAR-P and aMLLibrary. OSCAR-P is an auto-profiling tool designed to automatically test serverless application workflows running on multiple hardware and node combinations in cloud and edge environments. OSCAR-P obtains relevant profiling information on the execution time of the individual application components. These data are later used by aMLLibrary to train ML-based performance models. This makes it possible to predict the performance of applications on unseen configurations. We test our framework on clusters with different architectures (x86 and arm64) and workloads, considering multi-component use-case applications. This extensive experimental campaign proves the efficiency of OSCAR-P and aMLLibrary, significantly reducing the time needed for the application profiling, data collection, and data processing. The preliminary results obtained on the ML performance models accuracy show a Mean Absolute Percentage Error lower than 30% in all the considered scenarios.","Cloud computing is widely adopted today and has been used for years as the standard computing paradigm for enterprise-level distributed systems [1]. Despite its significant advantages in terms of costs and computational power accessibility, it is associated with significant challenges since data offloading leads to potential delays and increased expenses. Therefore, it falls short in meeting the demands of contemporary applications, often Artificial Intelligence (AI)-based and associated with strict or almost real-time processing constraints. Cloud users exhibit high sensitivity to delays and fluctuations, and they benefit from a newly-emerging paradigm called edge computing, striving to relocate applications closer to the point of data generation [2]. This approach offers several advantages: i) lower latency: by moving part of the computation where the data resides, we remove the round-trip-time delays needed to access the remote cloud data centers, resulting in faster response times and better performance; ii) reduced bandwidth usage: local processing at the edge removes the need to send vast amounts of data through the network, saving bandwidth and avoiding bottlenecks; iii) improved privacy: data processed on edge devices can be anonymized on the spot before communication, ensuring that potentially sensitive information is never shared with a central server; iv) better scalability: edge devices are usually cheap, and adding more to help with data processing is easy to implement and economical. However, edge resources cannot be seen as a possible replacement for cloud computing since they are usually characterized by lower computational capacity and consequently become a bottleneck in the computation. An integrated edge-cloud computing continuum enabling application components with different demands to be executed on the most appropriate resources is crucial to supporting complex application workflows effectively. Together with the introduction of the computing continuum paradigm, another significant aspect characterizing the computational landscape in recent years is represented by the quick rise in popularity [3][4] of the Function as a Service (FaaS) model. It breaks down complex applications in workflows of small, usually short-lived components, which run on reusable services consisting of stateless containers activated by suitable events (e.g., a file upload). Using containers instead of virtual machines (VMs) reduces both the development/deployment complexity and the resource usage. Containers can be dynamically created or destroyed in response to workload variations, and their stateless nature allows them to be reused to serve another event as soon as the previous computation completes. This increases the flexibility and makes the FaaS model suitable for scenarios characterized by light average workload interleaved with activity peaks. Finally, the FaaS model in public clouds is characterized by per-millisecond costs bound to the actual resource usage [5]. Despite the benefits associated with distributed computing and FaaS models, evaluating the performance of a complex application whose components may be allocated at different levels of the computing continuum poses significant challenges. Automated tools are needed to support the components profiling, measuring their execution times while exploiting variable resources and hardware configurations. Furthermore, accurately predicting the performance of a given application at a target configuration is the key to proper planning and runtime management of the available resources. As mentioned, edge devices with limited capacity often become the system bottleneck in case of workload spikes, and actions need to be taken to meet the performance objectives (e.g., limiting application execution time with a fixed threshold). This paper proposes an integrated framework for the profiling and performance modeling of FaaS-based applications running in computing continua. In our setting, the application execution is supported by OSCAR [6], a state-of-the-art runtime environment that aims to create a highly parallel, event-driven, pipelined serverless environment to execute general-purpose data-processing computing applications. This includes the usage of AWS Lambda to execute event-driven Docker-based applications. On top of OSCAR, we developed OSCAR-P (OSCAR-Profiler), a novel auto-profiling tool that can automatically test application workflows on different hardware and node combinations, gathering relevant information on the execution time of the individual components. Moreover, OSCAR-P leverages aMLLibrary [7], an open-source Machine Learning (ML) package we designed to automatically develop performance-predicting models for every service/resource pair, which can be combined to forecast the runtime of the entire workflow. Compared to tools such as Kubeflow Katib [8], aMLLibrary does not require extensive configuration and deployment of computational resources, is more straightforward and portable, and supports utilities specific to performance modeling such as feature augmentation and selection. The usage of ML for performance prediction is motivated by the ever-increasing complexity of modern software. Often, the impact of input configurations and settings on software performance is not straightforward, preventing the use of analytical, white-box methods such as Petri nets [9] and queuing networks [10]. Even in the simplest scenarios where accurate modeling is possible, the hypothesis and assumptions underlying analytical formulations prevent them from covering all use cases. Therefore, an approach that does not require any knowledge of the internal details of the system, generally referred to as a black-box technique, is often preferred. In particular, ML is the prominent category of black-box approaches for performance analysis [11]. The models built by aMLLibrary make it possible to predict the performance (e.g., the average execution time) of an application on unseen configurations with high accuracy. This allows to limit the initial application profiling campaign since some configurations do not need to be tested directly. In general, this prediction capability enables efficient design-time decision making [12] and runtime resource management [13, 14, 15], which are essential tasks for numerous cloud systems. This paper extends our initial works in [16] and [7]. Compared to these papers, we i) discuss our contributions with a much larger level of detail, ii) add support for synchronous calls, partitioned applications, and AWS Lambda functions into the framework, and iii) extend our experimental campaign with four new applications representing different real-life use cases. This work is organized as follows. Section 2 presents relevant literature works. Section 3 provides a summary of the OSCAR framework and its architecture. Section 4 thoroughly illustrates the OSCAR-P goals and its architecture, while Section 5 illustrates the capabilities of aMLLibrary and the performance analysis it supports. Section 6 describes the performance models used in our experiments. Section 7 focuses on the experimental scenarios to validate our tools. Conclusions and future works are discussed in Section 8."
https://arxiv.org/html/2411.07622v1,A Performance Analysis of BFT Consensus for Blockchains,"Distributed ledgers are common in the industry. Some of them can use blockchains as their underlying infrastructure. A blockchain requires participants to agree on its contents. This can be achieved via a consensus protocol, and several BFT (Byzantine Fault Tolerant) protocols have been proposed for this purpose. How do these protocols differ in performance? And how is this difference affected by the communication network? Moreover, such a protocol would need a timer to ensure progress, but how should the timer be set?This paper presents an analytical model to address these and related issues in the case of crash faults. Specifically, it focuses on two consensus protocols (Istanbul BFT and HotStuff) and two network topologies (Folded-Clos and Dragonfly). The model provides closed-form expressions for analyzing how the timer value and number of participants, faults and switches affect the consensus time. The formulas and analyses are validated with simulations. The conclusion offers some tips for analytical modeling of such protocols.","There is considerable current interest in the use of blockchains to implement distributed ledgers for finance, health care, energy, logistics, etc. (Fan et al., 2020). This requires continual consensus among interested parties in the system. Research on consensus protocols is mostly split between proof-based consensus (like Bitcoin) and vote-based consensus (Xu et al., 2023). A vote-based protocol for enforcing consensus must guard against errant (possibly malicious) validator behavior. It is said to be Byzantine Fault Tolerant (BFT) if it can achieve consensus despite arbitrary misbehavior. There is a large variety of BFT protocols. They differ in terms of failure models, delay assumptions, cryptographic requirements, etc. Much of the impetus in developing new protocols lies in pushing their performance. The performance of these protocols are measured via complexity analysis (e.g. number of messages or rounds), in empirical or simulation experiments, or with analytical models. Despite decades of such analysis of BFT consensus, much remains unknown regarding their behavior. For example, to ensure the protocol progresses despite failures, a common technique lies in setting a timer; when it expires, the protocol assumes there is a fault, and restarts the consensus process. This timer has some initial value {\tau_{0}}, but the value is usually increased upon timer expiry. How should {\tau_{0}} be set? For guidance, we simulated HotStuff (Yin et al., 2019), a well-known BFT protocol for blockchains, and measured the time T to gather consensus for a block. Fig. 1(a) shows how T varies with {\tau_{0}} and the number of faults. How can one analytically describe this interesting, non-monotonic behavior? (a) non-monotonic behavior (b) crossover for n (c) crossover for switch rate Figure 1. Simulated consensus time for (a) HotStuff with varying number of faults (n=16, clique), (b) HotStuff vs IBFT on Dragonfly ({\nu_{d}}=5,{r^{\mathcal{P}}_{s}}=9) and (c) HotStuff vs IBFT on Dragonfly ({\nu_{d}}=3, n=31). The unit of time on the vertical axis follows that for message processing time at a validator (see Sec. 3.4). (a) IBFT (b) HotStuff Figure 2. IBFT and HotStuff have very different patterns for message exchange. In the simulations for Fig. 1(a), we factor out the impact of the network connecting the nodes by using 0 message delay between any two nodes. What if the network delays are not negligible? How might two topologies differ in their impact on T? A network can affect T through congestion delays. We therefore expect that the same topology can have different impact on two protocols, if their message patterns differ. For example, consider the pattern in Fig. 2(a) for IBFT (Instanbul BFT) protocol (Moniz, 2020), which is very different from the one in Fig. 2(b) for HotStuff. One may expect that, for the same topology, congestion delays can have greater impact on IBFT than HotStuff. However, simulation results in Fig. 1(b) show that, for the Dragonfly (Sensi et al., 2019), IBFT can be faster or slower than HotStuff, depending on the number of participants. Fig. 1(c) shows a similar slower/faster crossover depending on how fast a switch or router can process a message. How do the protocol and topology parameters determine the crossover point in their consensus time? This paper presents three contributions: (1) An analysis of how BFT consensus time depends on the number of validators and faults, timer value and network topology. (2) A comparison of two protocols’ performance, and two topologies’ impact. (3) An approach to analytic modeling of consensus protocols. Below, we begin with a review of related work in Sec. 2. Sec. 3 introduces necessary definitions and notation, and describes two protocols (IBFT and HotStuff) and two topologies (Dragonfly and Folded-Clos) that we use for analyzing consensus time; it also presents the approach to the performance analysis and its validation. A Byzantine validator can cause arbitrary performance degradation (Clement et al., 2009), so we only consider crash faults. The performance model is presented in Sec. 4 for cliques and Sec. 5 for non-clique topologies. Sec. 6 highlights some conclusions about the consensus time for HotStuff and IBFT, and the modeling approach."
https://arxiv.org/html/2411.06348v1,Optimal Algorithm for Multiple-Processor Multitask Scheduling,"The efficient scheduling of multi-task jobs across multiprocessor systems has become increasingly critical with the rapid expansion of computational systems. This challenge, known as Multiprocessor Multitask Scheduling (MPMS), is essential for optimizing the performance and scalability of applications in fields such as cloud computing and deep learning. In this paper, we study the MPMS problem under both deterministic and stochastic models, where each job is composed of multiple tasks and can only be completed when all its tasks are finished. We introduce \mathsf{NP}-\mathsf{SRPT}, a non-preemptive variant of the Shortest Remaining Processing Time (SRPT) algorithm, designed to accommodate scenarios with non-preemptive tasks. Our algorithm achieves a competitive ratio of \ln\alpha+\beta+1 for minimizing response time, where \alpha represents the ratio of the largest to the smallest job workload, and \beta captures the ratio of the largest non-preemptive task workload to the smallest job workload. We further establish that this competitive ratio is order-optimal when the number of processors is fixed. For stochastic systems modeled as M/G/N queues, where job arrivals follow a Poisson process and task workloads are drawn from a general distribution, we prove that \mathsf{NP}-\mathsf{SRPT} achieves asymptotically optimal mean response time as the traffic intensity \rho approaches 1, assuming the task size distribution has finite support. Moreover, the asymptotic optimality extends to cases with infinite task size distributions under mild probabilistic assumptions, including the standard M/M/N model. Experimental results validate the effectiveness of \mathsf{NP}-\mathsf{SRPT}, demonstrating its asymptotic optimality in both theoretical and practical settings.","Scheduling is fundamentally about the optimal allocation of resources over time to perform a collection of jobs. With widespread applications in various fields, scheduling jobs to minimize the total response time (also known as flow time [1], sojourn time [2] and delay [3]) is a fundamental problem in computer science and operation research that has been extensively studied. As an important metric measuring the quality of a scheduler, response time, is formally defined as the difference between job completion time [4, 5] and releasing date, and characterizes the amount of time that the job spends in the system. Optimizing the response time of single-task jobs has been considered both in offline and online scenarios. If preemption is allowed, the Shortest Remaining Processing Time (SRPT) discipline is shown to be optimal in single machine environment. Many generalizations of this basic formulation become NP-hard, for example, minimizing the total response time in non-preemptive single machine model and preemptive model with two machines [1]. When jobs arrive online, no information about jobs is known to the algorithm in advance, several algorithms with logarithmic competitive ratios are proposed in various settings [6, 1]. On the other hand, while SRPT minimizes the mean response time sample-path wise, it requires the knowledge of remaining job service time. Gittins proved that the Gittins index policy minimizes the mean response time in an M/G/1 queue, which only requires the access to the information about job size distribution [7]. However, traditional scheduling problems have evolved significantly from single-task models to complex multitask scenarios. In the contemporary landscape of computational resources, the efficient scheduling of tasks across multiple processors has emerged as a critical challenge, driven by the exponential growth of data and the complexity of applications. To give the simplest example, for the objective of computing matrix vector product, we can divide matrix elements and vector elements into groups of columns and rows respectively, then the tasks correspond to the block-wise multiplication operations. Tasks can also be map, shuffle and reduce procedures in MapReduce framework [8]. Multi-processor Multitask Scheduling (MPMS) [9], where a job is considered to be completed only when all the tasks within the job are finished, addresses the allocation of jobs and tasks, each with potentially varying computational demands, to a set of processors in a manner that optimizes overall system performance metrics. Though much progresses have been made in single-task job scheduling, there is a lack of theoretical understanding regarding MPMS. The extension of traditional scheduling principles to multi-task systems, presents significant challenges. The heterogeneity of task durations and the dynamic nature of job arrivals seems further complicate the analysis. Thus a natural question that arises is, How can we design an efficient scheduling algorithm to minimize the total amount time that the multitask jobs spend in the multi-processor system? The significance of MPMS problem is also underscored by its direct impact on deep learning workflow [10, 11, 12], several different parallel strategies, such as data parallelism, pipeline parallelism, etc, have been proposed to accelerate the training process. Deep learning models benefit from parallel execution during training and inference phases, necessitating effective job and task scheduling to maximize the utilization of available computational resources. Inefficient scheduling can lead to increased training times, higher operational costs, and suboptimal model performance. Therefore, developing scheduling algorithms that can handle the complexities of modern multiprocessor and multi-task systems is paramount for advancing the field of deep learning. 1.1 Contributions. In this paper, we investigate how to minimize the total response time of multitask jobs in a multi-server system and answer the aforementioned question. Our contributions are summarized as follows. • In Section 3, we propose \mathsf{NP}-\mathsf{SRPT} algorithm [13], for minimizing the total response time. \mathsf{NP}-\mathsf{SRPT} algorithm achieves a competitive ratio of \ln\alpha+\beta+1, where \alpha is the maximum-to-minimum job workload ratio, \beta represents the ratio between maximum non-preemptive task workload and minimum job workload. It can be shown that no o(\ln\alpha+\beta)-competitive algorithm exists when the number of machines is constant. In addition, O(\ln\alpha+\beta^{1-\varepsilon}) is the best possible competitive ratio for the class of work-conserving algorithms. • Besides the worst case relative ratio above, we further prove our main result, absolute performance guarantees for \mathsf{NP}-\mathsf{SRPT} algorithm under certain probabilistic structure on the input instances, in which the relevant and remaining workload bound established for the adversarial inputs contributes significantly to the stochastic analysis. Assuming that jobs arrive according to a Poisson process, i.e., in M/G/N system, in Section 4 we prove that the average response time incurred by \mathsf{NP}-\mathsf{SRPT} algorithm is asymptotic optimal when load \rho\rightarrow 1, as long as the task size distribution has finite support. As shown in Section 5, the assumption of finite task workload can be removed for exponentially distributed job size, i.e., M/M/N, together with other job size distributions with certain properties on the tail of the distribution, by establishing an upper bound on the busy period in M/G/N. In addition, we also further validate the optimality of \mathsf{NP}-\mathsf{SRPT} through experiments in Section 6, confirming the efficacy of the algorithm. 1.2 Related Work Single and multiple task scheduling. There has been a large literature on single-task job scheduling, with parallel developments taking place in competitive analysis and queuing theory. For example, recently Hong and Scully [14] showed Gittins’s heavy-traffic optimality in G/G/N. However, little is known about multitask scheduling. Scully et. al [15] presented the first theoretical analysis of single-processor multitask scheduling problem, and gave an optimal policy that is easy to compute for batch arrival, together with the assumption that the processing time of tasks satisfies the aged Pareto distributions. Sun et al. [16] studied the multitask scheduling problem when all the tasks are of unit size, and proved that among causal and non-preemptive policies, fewest unassigned tasks first (FUT) policy, earliest due date first (EDD) policy, and first come first serve (FCFS) are near delay-optimal in distribution (stochastic ordering) for minimizing the metric of average delay, maximum lateness and maximum delay respectively. Wang et. al [3] established results of asymptotic independence of queues in multitask multi-processor system by developing a new technique named Poisson oversampling. To model the scenario when the scheduler has incomplete information about the job size, Scully et. al [17] introduced the multistage job model and proposed an optimal scheduling algorithm for multistage job scheduling in M/G/1 queue. The closed-form expression of the mean response time is also given for the optimal scheduler. In addition to the aforementioned work, there are also studies that further extend the understanding of scheduling by considering parallelizable jobs represented as Directed Acyclic Graphs (DAGs) [18]. Performance and optimality of SRPT and its variants. While SRPT is optimal at minimizing average response time in single-server systems [19], its performance is suboptimal in multi-server environments. However, SRPT remains a highly regarded method in the context of multi-server systems. It has shown that the competitive ratio of SRPT is in logarithmic order and is the best possible [1]. With the stochastic arrivals in M/G/k systems, SRPT is shown to be optimal in heavy traffic scenarios [20]. Another notable contribution by [21] introduces the Intermediate-SRPT algorithm, which addresses jobs with intermediate parallelizability and establishes an optimal O(\log P)-competitive ratio concerning average flow time, where P represents the ratio of maximum to minimum job sizes. Paper organization. The remainder of this paper is organized as following. We introduce the problem definition, notations and necessary background in Section 2. In Section 3 we formally present \mathsf{NP}-\mathsf{SRPT} algorithm, together with the analysis of its competitive ratio and lower bounds. Section 4 is devoted to the proof of the asymptotic optimality of \mathsf{NP}-\mathsf{SRPT} in heavy traffic regime, and the optimality is extended to infinite task size in Section 5. We conduct the experimental validation in Section 6, and conclude our work in Section 7."
https://arxiv.org/html/2411.06224v2,Advancing GPU IPC for Stiff Affine-Deformable Simulation,"Incremental Potential Contact (IPC) is a widely used, robust, and accurate method for simulating complex frictional contact behaviors. However, achieving high efficiency remains a major challenge, particularly as material stiffness increases, which leads to slower Preconditioned Conjugate Gradient (PCG) convergence, even with the state-of-the-art preconditioners. In this paper, we propose a fully GPU-optimized IPC simulation framework capable of handling materials across a wide range of stiffnesses, delivering consistent high performance and scalability with up to 10\times speedup over state-of-the-art GPU IPC methods. Our framework introduces three key innovations: 1) A novel connectivity-enhanced Multilevel Additive Schwarz (MAS) preconditioner on the GPU, designed to efficiently capture both stiff and soft elastodynamics and improve PCG convergence at a reduced preconditioning cost. 2) A C^{2}-continuous cubic energy with an analytic eigensystem for strain limiting, enabling more parallel-friendly simulations of stiff membranes, such as cloth, without membrane locking. 3) For extremely stiff behaviors where elastic waves are barely visible, we employ affine body dynamics (ABD) with a hash-based multi-layer reduction strategy for fast Hessian assembly and efficient affine-deformable coupling. We conduct extensive performance analyses and benchmark studies to compare our framework against state-of-the-art methods and alternative design choices. Our system consistently delivers the fastest performance across soft, stiff, and hybrid simulation scenarios, even in cases with high resolution, large deformations, and high-speed impacts. Our framework will be fully open-sourced upon acceptance.","Incremental Potential Contact (IPC) (Li et al., 2020a) is a cutting-edge elastodynamic contact simulation method widely used in computer graphics, computational mechanics, robotics, etc. Despite its robustness, accuracy, and differentiability in simulating complex frictional contact behaviors, IPC’s efficiency remains a significant bottleneck, limiting its full potential. Several variants have been proposed to address IPC’s efficiency issues, often at the expense of accuracy (Lan et al., 2021, 2023; Li et al., 2023; Lan et al., 2022b). To accelerate IPC without sacrificing accuracy, Huang et al. (2024) introduced GIPC with a GPU-friendly redesign of the numerical algorithms. This included replacing direct factorization with a Preconditioned Conjugate Gradient (PCG) solver and proposing a Gauss-Newton approximation of the barrier Hessian matrices with analytic eigensystems. While GIPC is effective, its efficiency deteriorates significantly as object stiffness increases. This is mainly due to the growing condition number of the global linear system, which requires more PCG iterations to solve. To improve the PCG convergence of stiff material, Wu et al. (2022) proposed the multilevel additive Schwarz (MAS) preconditioner (Wu et al., 2022). Their approach involves sorting the nodes based on Morton codes and building a hierarchy by grouping the nodes at each level. Despite its effectiveness, the lack of consideration for mesh connectivity during reordering leads to suboptimal domain hierarchy construction. This results in high construction costs, additional overhead for deformable simulations and challenges for GPU optimization. When simulating stiff elastic thin shells like cloth, another challenge arises, which is membrane locking. With linear triangle elements, the stiff membrane energy (Young’s modulus around 10MPa for cloth (Penava et al., 2014)) will often result in nonnegligible extra artificial bending resistance, forming sharp creases and plastic appearances in the simulation results (Figure 12 top). Simulating cloth with smaller stiffness can result in more realistic wrinkles, but it will suffer from over-elongation issues. To tackle this challenge, Li et al. (2021) propose to augment soft membrane energy with a barrier-based strain-limiting term to prevent cloth from over-stretching while avoiding membrane locking. This strategy enables realistic cloth simulation within the IPC framework, but the required exact strain limit satisfaction necessitates a backtracking-based line search filtering scheme, as the updated strain has a complicated relation to the step size, making analytic expressions unavailable. Additionally, numerical eigendecomposition is needed for computing a positive semi-definite approximation of the strain-limiting term’s Hessian matrix, which further complicates GPU optimization. For even stiffer problems where elastic waves are barely visible, objects can be treated as rigid (Ferguson et al., 2021) or stiff affine (Lan et al., 2022a) bodies in the IPC framework for a reduced number of degrees of freedom (DOF). But to accurately simulate contact behaviors, surface elements from the original input geometry are used, which also makes simulating rigid-deformable coupled scenarios convenient. Chen et al. (2022) introduced a unified Newton barrier method for stiff affine-deformable simulation, possibly with articulation constraints. However, although some components are GPU-accelerated, the primary simulation processes still execute on the CPU, leading to suboptimal performance. ZeMa (Du et al., 2024) is another GPU IPC framework for stiff affine-deformable simulation, with most processes parallelized on the GPU, except for the linear system, which is solved on the CPU using a direct solver. However, ZeMa lacks a well-optimized contact Hessian assembly algorithm, as it accumulates the 12\times 12 dense contact Hessian matrices to the affine body DOFs atomically, where conflicting operations can significantly impede the performance, especially when there are a large number of contacts. Moreover, direct solvers often fall short in large-scale simulations. In summary, there are still plenty of rooms for optimizing linear solver preconditioners, strain limiting, global Hessian matrix assembly, etc., for realizing a highly GPU-optimized IPC framework that can efficiently simulate large-scale affine-deformable coupled scenarios. In this paper, we propose such a framework, achieving up to 10\times speedup compared to GIPC via the following 3 major innovations: • A novel connectivity-enhanced MAS preconditioner on the GPU that achieves improved PCG convergence at a lower precomputation and per-iteration cost (section 4). Our preconditioner consistently performs effective and well-structured aggregations, which supports smaller blocksizes and further GPU optimizations based on warp reduction. • A C^{2}-continuous cubic strain-limiting energy with an analytic eigensystem, enabling realistic cloth simulation without membrane locking (section 5). As numerical eigendecomposition and line search filtering for the feasibility of the strain limits are not needed, our model supports highly GPU-parallelized computations. • A hash-based multi-layer reduction strategy for fast Hessian matrix assembly (section 6). Our strategy significantly reduces the number of numerical operations, and it enables the development of a memory-efficient symmetric blockwise sparse matrix-vector multiplication method to further boost PCG performance. In section 7, we perform extensive and rigorous performance analyses and benchmark studies to validate our framework and compare it to state-of-the-art GPU IPC systems and alternative design choices that may seem reasonable but suffer from suboptimal performance in practice. Our framework exhibits the fastest performance in soft, stiff, and hybrid simulation scenarios, even with high resolution, extreme deformation, and high-speed impacts. Our system will be fully open-sourced upon acceptance."
