URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10057v1,KuaiFormer: Transformer-Based Retrieval at Kuaishou,"In large-scale content recommendation systems, retrieval serves as the initial stage in the pipeline, responsible for selecting thousands of candidate items from billions of options to pass on to ranking modules. Traditionally, the dominant retrieval method has been Embedding-Based Retrieval (EBR) using a Deep Neural Network (DNN) dual-tower structure. However, applying transformer in retrieval tasks has been the focus of recent research, though real-world industrial deployment still presents significant challenges. In this paper, we introduce KuaiFormer, a novel transformer-based retrieval framework deployed in a large-scale content recommendation system. KuaiFormer fundamentally redefines the retrieval process by shifting from conventional score estimation tasks (such as click-through rate estimate) to a transformer-driven Next Action Prediction paradigm. This shift enables more effective real-time interest acquisition and multi-interest extraction, significantly enhancing retrieval performance. KuaiFormer has been successfully integrated into Kuaishou App’s short-video recommendation system since May 2024, serving over 400 million daily active users and resulting in a marked increase in average daily usage time of Kuaishou users. We provide insights into both the technical and business aspects of deploying transformer in large-scale recommendation systems, addressing practical challenges encountered during industrial implementation. Our findings offer valuable guidance for engineers and researchers aiming to leverage transformer models to optimize large-scale content recommendation systems.","The Transformer (Vaswani, 2017) architecture has demonstrated significant success across multiple domains, with notable models such as BERT (Devlin et al., 2019) and GPT (Brown, 2020; Achiam et al., 2023) in natural language processing (NLP), and Vision Transformers (Alexey, 2020; Liu et al., 2021; Li et al., 2022) in computer vision (CV). These achievements underscore the Transformer’s remarkable capabilities in sequence modeling and parallelization. In the field of recommendation systems, Transformer-based architectures like SASRec (Kang and McAuley, 2018) and Bert4Rec (Sun et al., 2019) have also shown potential. However, these academic efforts often fail to address certain industrial challenges, which has limited their effectiveness in driving business success in large-scale recommendation systems, such as those at Kuaishou. Short-video recommendation poses unique challenges that demand advanced modeling techniques. The diverse nature of short-video content and the rapid evolution of user interests necessitate real-time adaptation to accurately capture these dynamic preferences. Users typically watch hundreds of short-videos each day, expressing preferences across a wide range of interest domains, while the system actively pushes diverse content to mitigate aesthetic fatigue and avoid the ”filter bubble” effect. As a result, models that rely on daily updates, such as PinnerFormer(Pancha et al., 2022), struggle to adapt to users’ evolving content needs. Moreover, traditional approaches like SASRec and Bert4Rec, which compress user behavior into a single interest vector, lack the capacity to accurately capture the full spectrum of user interests reflected in these interactions. To more effectively capture complex user interests, models like MIND (Li et al., 2019) and ComiRec (Cen et al., 2020) employ techniques such as capsule networks to extract multiple interest vectors from user action sequences. However, because these models do not utilize native Transformer-based architectures, they are limited in fully leveraging the advantages offered by Transformers. Furthermore, they do not address the performance overhead associated with processing long sequences, which is a significant concern in industrial applications. To effectively implement the Transformer model within Kuaishou’s large-scale short video recommendation system, this study conducts a detailed analysis of these specific challenges in the recommendation field and proposes a series of concise and effective solutions tailored to address these issues. • How to train with billion-scale item set: Large language models (LLMs) typically leverage next token prediction as the pretraining task. This involves calculating the probabilities of all tokens in the vocabulary being the next token, based on the historical sequence, and selecting the one with the highest probability as the next predicted token. General-purpose language models usually contain fewer than 100,000 tokens in their vocabularies (Tao et al., 2024). In the context of the Kuaishou short video recommendation system, the candidate pool contains billions of short videos, making it computationally prohibitive to compute probabilities for all candidates using a naive softmax approach. Efficient methods are therefore required to address the challenges of large-scale candidate selection while maintaining model performance. • How to capture user’s multi-interests: Different with language which have a clear semantics direction when predicting the next token. In our short-video services, users always have multiple interest points and a higher tolerance for short-videos watching. As a result, there maybe exists multiple short-videos with completely different semantics that are served as ‘positive’ next items at same time, which is unfriendly to vanilla Transformer learning. • How to extend to longer sequences with fewer computation resources: Different with large language model could stack very deeper and wider Transformers to achieve best performance with extremely higher computation resources. As a recommendation model, our model need to response a large amount request (about ¿ 50 billion requests every day), thus our Retrieval model should achieve the balance between efficiency and effectiveness. Particularly, the Transformer time complexity is \mathcal{O}(n^{2}d), where n denotes the input sequence length, d means hidden state dimension (Keles et al., 2022). Thereby Transformer-based model is sensitive with sequences length and we need to devise specific module to accelerate longer sequences training. In this work, we present KuaiFormer, our latest advancements in real-time industrial retrieval, which delivered the most significant improvements in the Retrieval stage at Kuaishou over the past year. Specifically, we introduce several reliable modifications to adapt Transformer to industrial retrieval scenarios: a customized softmax learning objective for stable model training, multiple query tokens to capture users’ diverse interests, and a historical sequence compression mechanism to improve the efficiency of long-sequence modeling. • Smooth In-Batch Softmax Loss with LogQ Correction: To avoid directly training on a billion-scale item set, we first employ in-batch softmax as the learning objective for KuaiFormer. However, in-batch softmax inevitably introduces sampling bias, deviating from uniform item sampling: popular items have more chances to be selected as negative samples, which can lead to performance degradation. We apply the widely-used logQ correction method (Yi et al., 2019) to correct for sampling bias. Additionally, in short-video services, users often have higher tolerance for watching, which reduces the confidence that negative samples in in-batch sampling truly represent items users dislike. Therefore, instead of using strict 0/1 labels for training, we incorporate label smoothing techniques (Müller et al., 2019) to mitigate training noise and enhance model robustness. • Multi-interests Query Tokens: To capture users’ diverse interests, we drew inspiration from the [CLS] token in BERT, which introduces a learnable token to compress the original input information into a holistic sequence representation. We extend this concept in KuaiFormer by introducing multiple learnable tokens, combined with a multi-interest training strategy to extract distinct user interest representations from historical item sequences. Specifically, KuaiFormer’s learnable query tokens leverage a causal masking mechanism, enabling subsequent interest tokens to fully interact with preceding interest token representations, thereby achieving more effective interest disentanglement. • Adaptive Item Compression Mechanism: To address the efficiency challenges of modeling longer sequences, we made an intuitive assumption: compared with the most recently watched short videos, users’ memories of earlier videos are more vague. Therefore, we can apply coarse-grained modeling to earlier items while using fine-grained modeling for the latest ones. Based on this, we devised an adaptive item compression mechanism: first, we divide the earlier item sequences into several groups, compressing each group into a single representation to reduce the input sequence length. This series of compressed representations is then concatenated with the most recent items, forming the token sequence input for the model. The main contributions of our work are as follows: • We propose our next-generation retrieval approach, KuaiFormer, to our knowledge, this work is the first real-time retrieval model by the pure Transformer architecture in industrial-scale RecSys. • We conduct extensive offline and online experiments to show KuaiFormer superiors, which contribute +0.360%/+0.126%/+0.411% online video watch time gains to Kuaishou short-video services. • We offer insights into both model and architectural aspects, addressing practical challenges encountered in industrial RecSys deployment. Our experience offer valuable guidance for engineers and researchers aiming to leverage Transformer to build better industrial RecSys."
https://arxiv.org/html/2411.10055v1,Towards unearthing neglected climate innovations from scientific literature using Large Language Models,"Climate change poses an urgent global threat, needing the rapid identification and deployment of innovative solutions. We hypothesise that many of these solutions already exist within scientific literature but remain underutilised. To address this gap, this study employs a curated dataset sourced from OpenAlex, a comprehensive repository of scientific papers. Utilising Large Language Models (LLMs), such as GPT4-o from OpenAI, we evaluate title-abstract pairs from scientific papers on seven dimensions, covering climate change mitigation potential, stage of technological development, and readiness for deployment. The outputs of the language models are then compared with human evaluations to assess their effectiveness in identifying promising yet overlooked climate innovations. Our findings suggest that these LLM-based models can effectively augment human expertise, uncovering climate solutions that are potentially impactful but with far greater speed, throughput and consistency. Here, we focused on UK-based solutions, but the workflow is region-agnostic. This work contributes to the discovery of neglected innovations in scientific literature and demonstrates the potential of AI in enhancing climate action strategies.","The International Energy Agency (IEA) notes that about half the projected CO2 reductions that will be required to achieve Net Zero by 2050 will depend on technologies that are currently not commercially viable– highlighting the critical need for breakthrough innovations to mitigate the impacts of climate change [1]. Unlike the clear relationship between life sciences and biotech innovation, for example, there is no one academic field that dominates climate innovation. Potential solutions can emerge from disparate fields. Therefore, one likely reason for the neglect of certain climate solutions is the sheer volume and diversity of scientific literature. Traditional methods of knowledge discovery and synthesis may fail to capture innovative approaches buried in vast datasets, leading to missed opportunities for policy and technological advancement [2]. This is especially relevant for countries like the UK, which has a world-leading academic culture and made substantial investments to foster climate innovation but may still have untapped potential in its existing scientific outputs [12]. To address this challenge, we propose the use of machine learning (ML) and Large Language Models (LLMs) to systematically identify climate innovations in scientific literature. We leverage OpenAlex [8], a comprehensive open dataset of scholarly papers and comprehensive meta-data, to provide test data for analysis by state-of-the-art language models, such as GPT4-o from OpenAI. These models are prompted to evaluate paper abstracts across seven dimensions: climate emissions reduction/removal potential, technology level, deployability, market need, potential to enable subsequent innovation, mission focus of research, and neglectedness. Our hypothesis is that the research evidence base for many high-impact climate solutions is already documented in scientific papers from the UK but these have not yet been fully identified or systematically prioritised. Benchmarking the outputs of the LLMs against parallel human evaluations, we aim to assess the effectiveness of these models in finding overlooked innovations and identify any potential advantages over human reasoning. This research contributes to both the discovery of neglected climate solutions and the application of ML in enhancing domain-specific knowledge extraction, potentially accelerating climate action by uncovering actionable insights hidden within the existing literature."
https://arxiv.org/html/2411.09852v1,: Towards Effective Heterogeneous Interaction Learning for Click-Through Rate Prediction,"A clear and well-documented LaTeX document is presented as an article formatted for publication by ACM in a conference proceedings or journal publication. Based on the “acmart” document class, this article presents and explains many of the common variations, as well as many of the formatting elements an author may use in the preparation of the documentation of their work.","Click-through rate (CTR) prediction, which aims to predict the probability of a user clicking an ad or item, is the fundamental task for various applications such as online advertising and recommender systems (Song et al., 2019; Lyu et al., 2020; Zhang et al., 2022, 2024). The quality of CTR prediction significantly influences the company revenue and user experience, drawing extensive attention from both academia and industry (Zhou et al., 2018, 2019; Lyu et al., 2020). For example, in online ad bidding, accurate CTR prediction helps advertisers optimize their bids and target the most receptive audiences. In content recommendation, accurate CTR prediction enables platforms to suggest more relevant content to users. To achieve better CTR prediction, it is crucial to capture the user interests in the evolving environment (Zhou et al., 2018; Lyu et al., 2020; Wang et al., 2019). The abundance of heterogeneous information presents both opportunities and challenges. On the one hand, heterogeneous information depicts user interests from different aspects, providing diverse context (Zhang et al., 2017). For instance, global information, e.g., user profile and context features, offers a static view on general user interests, while user behavior sequences provide substantial information for modeling dynamic user interests (Wang et al., 2019). On the other hand, the heterogeneous nature of the data requires different modeling approaches and careful integration across different information modes (Zhang et al., 2017). For example, while modeling interactions among global information is critical to personalized recommendation (Rendle, 2010; Lian et al., 2018; Wang et al., 2021b), capturing sequential dependencies is the major focus for user behavior modeling (Sun et al., 2019; Chen et al., 2019). Most of the existing CTR prediction models fall into two categories, including non-sequential models and sequential models. Non-sequential models focus on learning informative embeddings through feature interaction via inner-product (Lian et al., 2018; Sun et al., 2021), MLP (Wang et al., 2017, 2021b) and deep structured semantic model (Huang et al., 2013; Elkahky et al., 2015), but ignore the sequential information in user behaviors. Sequential models, in the contrast, employ additional modules, e.g., CNN (Tang and Wang, 2018), RNN (Sun et al., 2019; Zhou et al., 2018) and Attention modules (Lyu et al., 2020; Zhou et al., 2019; Zhai et al., 2024), to capture the sequential dependencies in user behaviors. Promising as it might be, existing sequential methods mostly employ a unidirectional information flow, where global information is used to guide sequence learning, while the reverse information flow from sequence to global information is largely ignored, hence suffering from insufficient inter-mode interaction. For example, global information often captures long-term interests, while sequence information reveals momentary interests, such as a sudden focus on a specific category of products, which can enhance the global context with immediate preference. Besides, due to the computational challenges of performing interaction learning among numerous global features and lengthy sequences, aggressive feature aggregation, e.g., sequence summation (Zhou et al., 2018), pooling (Xiao et al., 2020), and concatenation (Zhou et al., 2019), is often performed at early stages, inevitably leading to excessive information loss. In light of the above limitations, we propose a novel heterogeneous interaction learning module named InterFormer, whose ideas are two-fold. To avoid insufficient inter-mode interaction, we enable bidirectional information flows between different modes, such that global and sequence learning are performed in an interleaving style. Specifically, to learn context-aware sequence embeddings, global sumamrization guides sequence learning via Personalized FFN (pFFN) and Multihead Attention (MHA) (Vaswani et al., 2017). To learn behavior-aware global embeddings, sequence summarization instructs global learning via an interaction module. To mitigate aggressive information aggregation, we adopt MHA for effective information selection, based on which the one-to-one mappings between input and output tokens are retained till the final interaction. Note that our framework is compatible with various interaction learning models like DCNv2 (Wang et al., 2021b), DHEN (Zhang et al., 2022), etc. The main contributions of this paper are summarized as follows: • Challenges. We identify two key bottlenecks of heterogeneous interaction learning, namely insufficient inter-mode interaction and aggressive information aggregation. • Model Design. We propose a novel heterogeneous interaction learning framework named InterFormer for effective feature interaction and selective information aggregation. To our best knowledge, the proposed InterFormer is the first model to address the mutual benefits in heterogeneous interaction learning. • Experiments and Analysis. We carry out extensive experiments on the proposed InterFormer with up to 0.14% AUC improvement on benchmark datasets and 0.15% Normalized Entropy (NE) gain on internal large-scale dataset. Besides, InterFormer exhibits promising scaling results in both feature scaling and model scaling. The rest of the paper is organized as follows. Section 2 briefly reviews the recent works on interaction learning. Section 3 summarizes the preliminaries, and section 4 introduces our proposed InterFormer. Extensive experiments and analyses are carried out in Section 5. We conclude our paper in Section 6."
https://arxiv.org/html/2411.09705v1,Residual Multi-Task Learner for Applied Ranking,"Modern e-commerce platforms rely heavily on modeling diverse user feedback to provide personalized services. Consequently, multi-task learning has become an integral part of their ranking systems. However, existing multi-task learning methods encounter two main challenges: some lack explicit modeling of task relationships, resulting in inferior performance, while others have limited applicability due to being computationally intensive, having scalability issues, or relying on strong assumptions. To address these limitations and better fit our real-world scenario, pre-rank in Shopee Search, we introduce in this paper ResFlow, a lightweight multi-task learning framework that enables efficient cross-task information sharing via residual connections between corresponding layers of task networks. Extensive experiments on datasets from various scenarios and modalities demonstrate its superior performance and adaptability over state-of-the-art methods. The online A/B tests in Shopee Search showcase its practical value in large-scale industrial applications, evidenced by a 1.29% increase in OPU (order-per-user) without additional system latency. ResFlow is now fully deployed in the pre-rank module of Shopee Search. To facilitate efficient online deployment, we propose a novel offline metric Weighted Recall@K, which aligns well with our online metric OPU, addressing the longstanding online-offline metric misalignment issue. Besides, we propose to fuse scores from the multiple tasks additively when ranking items, which outperforms traditional multiplicative fusion.","Modern large-scale recommender systems and search engines heavily rely on modeling diverse user feedback to understand the preferences of users and better provide personalized services. Specifically, for e-commerce platforms like Meituan (Xi et al., 2021), AliExpress (Li et al., 2020), Taobao (Ma et al., 2018a), Walmart (Wu et al., 2022), and Shopee, estimating the click-through rate (CTR) and the click-through & conversion rate (CTCVR) of a user w.r.t. items have become one of their primary tasks. These metrics serve as their main indicators when ranking items. Given that these estimation tasks are closely related and that high-commitment user behaviors exhibit significant sparsity, e.g., CTCVR is typically at the level of 0.1%, multi-task learning (MTL) (Zhang and Yang, 2021; Zheng and Wang, 2022) has become an integral part of such systems to boost cross-task information interchange and mitigate the sample sparsity issue. However, real-world large-scale e-commerce platforms have unique features that make MTL for their ranking systems special and challenging. Firstly, large-scale ranking systems commonly use a multi-stage candidate selection framework (Zhang et al., 2023a; Wang et al., 2020), e.g., match \shortrightarrow pre-rank \shortrightarrow rank, to strike a balance between efficiency and accuracy, as depicted in Figure 1. In the early stages, such as pre-rank, the system must quickly sort through millions of items with limited resources, shortlisting a manageable number of candidates that align well with user interests and search keywords for the next stage. This requirement poses a significant challenge to the efficiency of model inference, forbidding the employment of complex models. Secondly, user actions in e-commerce platforms are typically designed to be carried out progressively, e.g., users can only place an order after clicking the item. Effective leverage of such sequential dependence properties may have critical importance. Figure 1. An illustration of the cascading item filtering process by the multi-stage candidate selection framework in large-scale applied ranking systems and subsequent user feedback. It sketches the quantities of samples at each stage, highlighting the sparsity of conversion feedback. Among the mainstream multi-task learning methods, MMOE (Ma et al., 2018b) and PLE (Qin et al., 2020) foster cross-task information sharing through expert-based network architectures. However, they lack explicit modeling of task relationships, showing relatively inferior performance (Xi et al., 2021). Towards leveraging the sequential dependence among tasks, AITM (Xi et al., 2021) proposed to transfer information sequentially from the former task to the latter via an attention-based module. However, its high computational intensity limits its applicability in resource-constrained scenarios like pre-rank. On the other hand, ESMM (Ma et al., 2018a) proposed to use causal graphs to model sequentially dependent tasks, modeling the conditional probability of the latter step given the former. Thereafter, given the inherent sample selection bias (Chen et al., 2023a) of modeling condition probability, subsequent methods including ESCM2 (Wang et al., 2022) and DCMT (Zhu et al., 2023) further employed counterfactual regularizers (Schnabel et al., 2016) to mitigate this issue. However, they encounter scalability issues when extending beyond two tasks, due to the high variance associated with counterfactual regularizers and their accumulation along the causal dependency chain (Wang et al., 2022). Besides, the applicability of these causal methods is limited to scenarios where clear causal relationships exist. To address the aforementioned limitations and better fit our real-world scenario, we propose ResFlow, a lightweight multi-task learning framework that boosts efficient information transfer from one task to another by introducing a set of residual connections between corresponding layers of their networks. ResFlow is hence generally applicable in situations where the information from the former task is beneficial to the latter (Wu et al., 2022; Xi et al., 2021; Tao et al., 2023; Jin et al., 2022), typically from low-commitment, dense tasks to high-commitment, sparser ones, including sequentially dependent cases, e.g., ”click” \shortrightarrow ”order”, and more general ones where tasks show certain progressiveness, e.g., ”like” and ”forward”. It is worth noting that such residual connections can be straightforwardly extended to longer progressive chains, e.g., ”click” \shortrightarrow ”add-to-cart” \shortrightarrow ”order”, fostering a sufficient and continuous flow of information, as illustrated in Figure 2. With its simplicity and generality, ResFlow can be integrated smoothly into diverse ranking stages and application scenarios. Comprehensive experiments on various offline datasets, as well as online A/B tests in Shopee Search, have validated the superb effectiveness and scalability of ResFlow. In particular, according to our online A/B tests, ResFlow brings a 1.29% increase in OPU (order-per-user) without extra system latency. ResFlow is now fully deployed in the pre-rank module of Shopee Search. Furthermore, to facilitate efficient online deployment, we propose a new offline metric Weighted Recall@K, which aligns well with our online metric OPU, addressing the longstanding issue of online-offline metric misalignment. Besides, we propose to fuse the scores from the multiple tasks additively when ranking items, which according to our experiments consistently outperforms traditional multiplicative score fusion. The contribution of this paper can be summarized as: • We propose ResFlow, a novel, lightweight, and versatile MTL framework, demonstrating its superior performance and adaptability over state-of-the-art methods and fully deploying it in the pre-rank module of Shopee Search. • We propose a new offline metric that addresses the longstanding online-offline metric alignment issues in practical deployment. We propose to additively fuse multi-task scores when ranking items, which outperforms traditional multiplicative score fusion."
https://arxiv.org/html/2411.10227v1,Entropy and type-token ratio in gigaword corpora,"Lexical diversity measures the vocabulary variation in texts. While its utility is evident for analyses in language change and applied linguistics, it is not yet clear how to operationalize this concept in a unique way. We here investigate entropy and text-token ratio, two widely employed metrics for lexical diversities, in six massive linguistic datasets in English, Spanish, and Turkish, consisting of books, news articles, and tweets. These gigaword corpora correspond to languages with distinct morphological features and differ in registers and genres, thus constituting a diverse testbed for a quantitative approach to lexical diversity. Strikingly, we find a functional relation between entropy and text-token ratio that holds across the corpora under consideration. Further, in the limit of large vocabularies we find an analytical expression that sheds light on the origin of this relation and its connection with both Zipf and Heaps laws. Our results then contribute to the theoretical understanding of text structure and offer practical implications for fields like natural language processing.","Entropy and type-token ratio (TTR) are useful metrics for evaluating the richness of lexical variation in writing. The TTR simply measures the proportion of unique words (types) to the total number of words (tokens) in a given text [1, 2], serving as a rough indicator of lexical diversity [3, 4, 5, 6]. The TTR has been originally leveraged in the study of children vocabulary [2, 7] or illnesses which affect linguistic capabilities [8]. However, the notion of TTR is problematic given its dependence on text length. As a consequence, its usefulness reduces when comparing two samples with differing token numbers. Efforts have been made to propose alternatives that show text length independence and are valid for both written and spoken corpora of different registers [9]. In contrast, entropy takes into account the distribution of these words in terms of their occurrence frequencies, capturing both the diversity of lexical items and also how uniformly they are used. As such, understanding the word entropy of texts [10] offers valuable insights into the complexity and unpredictability of language [11]. Originally proposed within information theory [12, 13], in the context of language higher entropy suggests more unpredictability in word choices, which can reveal underlying structures of language use and cognitive processing. Texts with high entropy exhibit greater diversity in word usage, reflecting more complex linguistic structures. Specifically, this measure and related operationalizations have been applied in a broad range of linguistic studies, including differentiation between translated and original documents [14], the study of ambiguity in different legal systems [15], quantification of language change [16], identification of stopwords [17], or observation of change in literary preference [18], among others. Hence, entropy and TTR would address lexical diversity from two different, seemingly independent perspectives. Nonetheless, the production of natural language texts are constrained by statistical laws [19]. These empirical rules highlight patterns that occur in word frequency and vocabulary growth across languages. On the one hand, Zipf law [20] states that a few words are used very frequently, while most are rare. On the other hand, Heaps law [21, 22] dictates that the number of types in a text increases more slowly (sublinearly) than its total number of tokens. These laws, among others [23], help linguists and computational scientists model language, offering insights into communication efficiency and the structure of human language systems. Below, we provide evidence that there exists a functional relation between entropy and type-token ratio. It follows that these diversity measures are not fully independent from each other, and we understand their dependence making use of Zipf and Heaps laws. Quite generally, Zipf and Heaps laws are closely related to the concept of diversity in text analysis. Zipf law, by highlighting the unequal distribution of word frequencies, influences the predictability of words in a text. As mentioned above, this distribution plays a key role in calculating the entropy of natural language texts, where frequent words reduce unpredictability and rarer words increase it. Similarly, Heaps law, which describes vocabulary growth, directly affects the lexical diversity of a text as calculated from the TTR definition. Our work discusses two main findings. First, we uncover an empirical relation between the entropy and the type-token ratio based on our analysis of six gigaword corpora. This contribution is significantly noteworthy since the TTR has typically been computed in texts containing around 10^{5} tokens [4, 6], much smaller than our datasets. While understaning the interplay between the two diversity measures remains a complicated task, the aforementioned relation is consistently found across the studied massive corpora. Second, to gain insight about this correlated behaviour of the entropy and the type-token ratio, we fit an analytical expression for the word entropy to the corpus data. This expression is based on the validity of Zipf law and holds for large vocabularies. Using Heaps law, we express the word entropy in terms of the type-token ratio and fit it to the six massive corpora, finding an excellent agreement with the numerical results. The agreement is observed for languages of different morphological types and sources of distinct genres and registers or styles. This is particularly relevant since it suggests that the entopy-TTR relation is robust against how language users vary their word choices, at least in large corpora. Our findings would then be valuable in optimizing performance and efficiency across various tasks and applications of natural language processing (NLP) and machine learning including quantifying information content, evaluating large language models and enhancing data compression. The remaining of the manuscript is structured as follows. In Section II, we describe the corpora used and analyze their corresponding Zipf and Heaps laws. In Section III we derive an analytical expression for the entropy of large vocabularies and fit this formula to the different datasets, obtaining good results. In Section IV we investigate the type-token ratio in the various corpora using the Heaps law. We combine both diversity metrics in Section V, where we discuss the functional relation that connects H and TTR and its agreement with the empirical results. Finally, Section VI contains our conclusions and suggestions for further work."
https://arxiv.org/html/2411.09973v1,Establishing and Evaluating Trustworthy AI: Overview and Research Challenges,"Artificial intelligence (AI) technologies (re-)shape modern life, driving innovation in a wide range of sectors. However, some AI systems have yielded unexpected or undesirable outcomes or have been used in questionable manners. As a result, there has been a surge in public and academic discussions about aspects that AI systems must fulfill to be considered trustworthy. In this paper, we synthesize existing conceptualizations of trustworthy AI along six requirements: 1) human agency and oversight, 2) fairness and non-discrimination, 3) transparency and explainability, 4) robustness and accuracy, 5) privacy and security, and 6) accountability. For each one, we provide a definition, describe how it can be established and evaluated, and discuss requirement-specific research challenges. Finally, we conclude this analysis by identifying overarching research challenges across the requirements with respect to 1) interdisciplinary research, 2) conceptual clarity, 3) context-dependency, 4) dynamics in evolving systems, and 5) investigations in real-world contexts. Thus, this paper synthesizes and consolidates a wide-ranging and active discussion currently taking place in various academic sub-communities and public forums. It aims to serve as a reference for a broad audience and as a basis for future research directions.trustworthy ai, artificial intelligence, fairness, human agency, robustness, privacy, accountability, transparency","From sophisticated chatbots like Chat-GPT to AI-driven recommender systems enhancing our entertainment experiences on platforms like Netflix and Spotify (Anderson et al., 2020), the impact of AI on our lives is significant. AI-based decision support systems are proving invaluable in critical fields such as life science and healthcare (Rajpurkar et al., 2022). Similarly, AI is reshaping hiring and human resources practice (Van den Broek et al., 2021) and transforming the banking and finance landscape with innovative solutions (Cao, 2022). However, in the past, some AI systems have been used in questionable manners, which has led to unexpected or undesirable results. Examples include biased algorithms perpetuating discrimination in recruitment processes (Chen, 2023) or AI-driven recommender systems favoring popular content and, with this, users interested in popular content (Kowald et al., 2020; Kowald and Lacic, 2022). Alongside biases in algorithms, AI systems rely on training data, including personal and private user information, which raises concerns for potential privacy and security breaches. One example is the Equifax data breach, in which private data records of millions of users were compromised (Zou and Schaub, 2018). Additionally, when thinking of self-driving cars, unreliable AI-based systems could even cause physical harm, as demonstrated by the unfortunate Uber car crash in 2018, in which a malfunctioning algorithm did not detect and, as a consequence, killed a pedestrian on the road (Kohli and Chadha, 2020). As a consequence, there has been an increase in public and academic discussions about the essential requirements AI systems must fulfill to be considered trustworthy. There is also a growing consensus on the necessity of setting up standards and regulations to ensure and validate the trustworthiness of AI. In this respect, the European Commission (EC) has proposed the AI Act (Madiega, 2021), a comprehensive regulatory framework for supporting the responsible development and deployment of AI technologies within the European Union. The AI Act seeks to establish clear rules governing the development and deployment of AI systems while imposing strict requirements for high-risk AI applications. The various interpretations of trustworthy AI add further complexity to this discourse by encompassing not just technical requirements but also human-centered and legal considerations. Another important framework proposed by the European Commission has been the “Assessment List for Trustworthy AI (ALTAI)” (Ala-Pietilä et al., 2020; Radclyffe et al., 2023), which enables organizations to self-assess the trustworthiness of AI solutions based on a checklist. Figure 1: An illustration of the six requirements of trustworthy AI investigated in this paper. This paper contributes insights into this discourse by analyzing the state-of-the-art regarding six aspects of AI systems that are typically understood as requirements for systems to be viewed as trustworthy. These requirements are: 1) human agency and oversight, 2) fairness and non-discrimination, 3) transparency and explainability, 4) robustness and accuracy, 5) privacy and security, and 6) accountability (see Figure 1). We define each of these six requirements, introduce methods to establish and implement these requirements in AI systems, and discuss corresponding validation methods and evaluation metrics. Such validation efforts are crucial from scientific and practical perspectives and might serve as a prerequisite for certifying AI systems and models Winter et al. (2021). Finally, for each of these requirements, we outline ongoing research challenges and future research perspectives. The contributions of our work are two-fold: firstly, we give a comprehensive overview of the requirements of trustworthy AI, in which we cover different viewpoints on trustworthy AI, including technical and also human-centered and legal considerations. Secondly, we discuss open issues and challenges in defining, establishing, and evaluating these requirements of trustworthy AI. Therefore, the guiding research question of this work is defined as follows: What is the current state of research regarding the establishment and evaluation of comprehensive - technical, human-centered, and legal - requirements of trustworthy AI? To address this research question, we follow the methodology described in Section 3.3. Our work complements existing surveys and articles on trustworthy AI in two main ways. Firstly, existing overview articles such as (Chatila et al., 2021; Thiebes et al., 2021; Akbar et al., 2024; Díaz-Rodríguez et al., 2023) tend to focus on definitions of trustworthy AI and neglect evaluation aspects, which is one key aspect of our article. Specifically, related surveys such as (Liang et al., 2022; Wing, 2021; Emaminejad and Akhavian, 2022) focus on specific aspects of trustworthy AI implementation and evaluation, namely data, formal methods, and robotics, respectively. In contrast, our article aims to provide a domain- and method-independent overview of trustworthy AI, which reflects the whole AI-lifecycle, including the evaluation phase. Secondly, concerning validation and evaluation schemes for trustworthy AI, existing technical conceptualizations of trustworthy AI such as (Floridi, 2021; Kaur et al., 2022; Li et al., 2023) have focused on technical and reliability-oriented requirements such as transparency, privacy, and robustness. In contrast, in our paper, we discuss methods and open challenges towards establishing and evaluating trustworthy AI also through the lens of human-centric and legal requirements such as fairness, accountability, and human agency. Therefore, to the best of our knowledge, our paper is the first to investigate all six requirements of trustworthy AI in a unified way by discussing implementation and evaluation aspects across the whole lifecycle of trustworthy AI and outlining open research challenges and issues for all six requirements. Our article shows that while evaluation and validation methodologies for technical requirements like robustness may rely on established metrics and testing procedures (e.g., for model accuracy), the assessment of human-centric considerations often requires more nuanced approaches that consider ethical, legal, and cultural factors. As such, our article emphasizes the need for further research to develop robust evaluation schemes that can be applied in research and practice across a variety of AI systems, particularly in high-risk domains where human values and rights are at stake (e.g., healthcare). Next, in Section 3, we describe the relevant background for this article, including general definitions of AI and its lifecycle, and introduce the six requirements of trustworthy AI covered herein. After discussing each requirement separately in Section 4, the paper closes with a conclusion and an outlook into future research directions in Section 5."
https://arxiv.org/html/2411.09607v1,Initial Nugget Evaluation Results for the TREC 2024 RAG Track with the AutoNuggetizer Framework,"This report provides an initial look at partial results from the TREC 2024 Retrieval-Augmented Generation (RAG) Track. We have identified RAG evaluation as a barrier to continued progress in information access (and more broadly, natural language processing and artificial intelligence), and it is our hope that we can contribute to tackling the many challenges in this space. The central hypothesis we explore in this work is that the nugget evaluation methodology, originally developed for the TREC Question Answering Track in 2003, provides a solid foundation for evaluating RAG systems. As such, our efforts have focused on “refactoring” this methodology, specifically applying large language models to both automatically create nuggets and to automatically assign nuggets to system answers. We call this the AutoNuggetizer framework. Within the TREC setup, we are able to calibrate our fully automatic process against a manual process whereby nuggets are created by human assessors semi-manually and then assigned manually to system answers. Based on initial results across 21 topics from 45 runs, we observe a strong correlation between scores derived from a fully automatic nugget evaluation and a (mostly) manual nugget evaluation by human assessors. This suggests that our fully automatic evaluation process can be used to guide future iterations of RAG systems.","This report examines the partial results from the TREC 2024 Retrieval-Augment Generation (RAG) Track. There is, obviously, tremendous excitement and interest in RAG, but we feel that the evaluation of RAG output remains deficient from many perspectives. Furthermore, the lack of standardized evaluations presents a barrier to continued progress in information access, and more broadly, natural language processing and artificial intelligence. We hope to harness the institutional knowledge and resources provided by the National Institute of Standards and Technology (NIST) via the Text Retrieval Conference (TREC) to tackle these challenges. Now in its 33rd year, TREC has led the way in many aspects of evaluation in information retrieval (IR), natural language processing (NLP), and beyond, producing many innovations that the community (both researchers as well as practitioners) take for granted today. We hope that these efforts will lay similar foundations for standardization of RAG evaluation in the future. Our central hypothesis is that the nugget evaluation methodology (Voorhees, 2003) provides a solid foundation for evaluating RAG systems. This methodology was originally articulated more than two decades ago in the context of the TREC Question Answering (QA) Track for evaluating answers to free-form “definition questions”. This matches our RAG setting, where for a given query, there are claims or facts (i.e., nuggets) from different documents that are combined into a fluent and cohesive natural language response. Given this starting point, our collective efforts have focused on “refactoring” the original nugget evaluation methodology in light of large language models (LLMs). Specifically, we leverage LLMs to both automatically create nuggets and automatically assign nuggets to system-generated answers. We implement our nugget evaluation methodology in the AutoNuggetizer framework. While this is not the first attempt to automate nugget evaluations (Lin and Demner-Fushman, 2005, 2006), the advent of LLMs provides opportunities that did not exist before. The biggest advantage of our approach is minimal reinvention of the wheel, in that we are able to leverage the existing body of work that has gone into exploring the nugget evaluation methodology, e.g., (Lin and Zhang, 2007; Dang and Lin, 2007). For aspects that are not directly impacted by our use of LLMs, we can continue to assert findings from the literature without needing to carefully revalidate those claims again. Furthermore, and unique to the TREC setup, we calibrate our fully automatic process against a (mostly) manual process whereby nuggets are created by NIST assessors semi-manually and assigned manually to system-generated answers by the same assessors. It is important to qualify that this report focuses only on answer content (in terms of nuggets), and does not take into account support, or the requirement that the content be grounded by references into the underlying document collection. Thus, we do not consider possible LLM hallucinations at all. We will examine support in a separate report. To provide the tl;dr — Based on initial results across 21 topics from 45 runs, we observe a strong correlation between scores derived from a fully automatic nugget evaluation and a (mostly) manual nugget evaluation by NIST assessors. This suggests that our fully automatic evaluation process can be used to guide future iterations of RAG systems. This report attempts to provide the experimental results to support the above assertion.111We apologize in advance to our many colleagues who have done excellent work on RAG evaluation. Given the pressures of the strict TREC timeline, we did not have the opportunity to survey the field and acknowledge previous work in this report. Instead, we have decided to focus exclusively on our nugget evaluation methodology. A thorough discussion of the advantages and disadvantages of alternative approaches will be provided in a subsequent follow-up. Query: how did african rulers contribute to the triangle trade Answer: African rulers played a significant role in the triangular trade by capturing and supplying slaves to European traders. They actively participated in the trade by capturing individuals from inland areas and transporting them to the coast, where they were sold to European traders. This involvement was driven by the economic benefits they received, such as gold, guns, and other goods from Europe. Many African rulers and merchants developed new trade routes to avoid those who refused to cooperate, ensuring a steady supply of captives for the transatlantic slave trade. The trade was highly profitable for African rulers, who received a variety of European goods in exchange for slaves. This economic incentive led to the expansion of slave trade networks and the growth of states based on slavery, such as the Kingdom of Dahomey, which became one of the most prosperous nations due to its involvement in the slave trade. The triangular trade system involved three stages: the export of manufactured goods from Europe to Africa, the transportation of enslaved Africans to the Americas, and the return of goods such as sugar, coffee, and tobacco to Europe. African rulers were crucial in the first stage, where they exchanged captives for European goods, which were often luxury items or weapons that helped them gain military and political advantages over their rivals. Despite some African rulers voicing opposition to the practice, the lure of profits ensured their continued participation in the trade. The involvement of African rulers was essential for the success of the transatlantic slave trade, as European traders relied on them to capture and supply slaves from the African interior. This collaboration was fundamental to the operation of the triangular trade, as European traders were generally confined to coastal areas and depended on African intermediaries to procure slaves. In summary, African rulers contributed to the triangular trade by capturing and supplying slaves to European traders, driven by the economic benefits they received in exchange. Their involvement was crucial for the success and expansion of the transatlantic slave trade. Table 1: Sample answer from the TREC 2024 RAG Track for topic 2024-35227. Note that here we have purposely omitted citations in the answer since this report does not cover support."
https://arxiv.org/html/2411.09425v1,MARM: Unlocking the Future of Recommendation Systems through Memory Augmentation and Scalable Complexity,"Scaling-law has guided the language model designing for past years, to estimate expected model performance with respect to the size of learnable parameters and training samples data-scales, e.g. GPTs. It is worth noting that the scaling laws of NLP cannot be directly applied to recommendation systems due to the following reasons: (1) The amount of training samples and model parameters is typically not the bottleneck for the model. Our recommendation system can generate over 50 billion user samples daily, and such a massive amount of training data can easily allow our model parameters to exceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the stability and robustness of the recommendation system, it is essential to control computational complexity FLOPs carefully. In training, we need to process a vast number of recommendation samples every day. During online inference, we usually need to respond within milliseconds (LLMs are usually a few seconds). Considering the above differences with LLM, we can draw a conclusion that: for a RecSys model, compared to model parameters, the computational complexity FLOPs is the more expensive factor that requires careful control.In this paper, we propose our milestone work, MARM (Memory Augmented Recommendation Model), which explores a new cache scaling-laws successfully. By caching part of complex module calculation results, Our MARM extends the single-layer attention-based sequences interests modeling module to a multiple-layer setting with minor inference complexity FLOPs cost (i.e, module time complexity \mathcal{O}(n^{2}*d)\rightarrow\mathcal{O}(n*d)). Equipped with the cache idea, our MARM solution significantly overcomes computational bottlenecks, and could seamlessly empower all user sequences interest extracting modules, and even other models. To support our MARM, we construct a 60TB cache storage center for offline training and online serving. Comprehensive experiment results show that our MARM brings offline 0.43% GAUC improvements and online 2.079% play-time per user gains. Our MARM has been deployed on Kwai short-videos platform,, serving tens of millions of users daily.","Figure 1. (a) A toy example of ranking model. (b) Performance gap in Streaming Data Training. Warming up from the online base model, a 50% sample downsampling leads to a continuous decline in model performance. On the other hand, there is a significant performance gap between a cold-start model trained from scratch and the online base model, but the gap decreases as the training data increases. (c) Parameter distributions. The parameters of an LLM model (blue) mainly are located in ‘dense-activated’ transformer-based DNN module, while ranking models (green) have a parameter distribution primarily composed of ‘sparse-activated’ feature embeddings. Figure 2. Motivation of MARM, using caching idea to reduce higher-FLOPs self-attention to lower-FLOPs target-attention. In recent years, the scaling of model size and data-scale has been a vital point in various fields, e.g., NLP, CV, and RecSys. According to OpenAI Scaling-laws technique report(Kaplan et al., 2020), the volume of data and the breadth and depth of models increase, the performance of the models follows a certain power-law improvement. Following the excited Scaling-laws tenet, many Transformer-based large models are proposed and achieve remarkable performance, e.g., ChatGPT for conversation, Qwen-VL(Bai et al., 2023) for multi-modal understanding, DeepSeek-Coder(Guo et al., 2024) for code generation, and Kling for Video generation. There are also some efforts to valid Scaling-laws in RecSys area (e.g., Wukong(Zhang et al., 2024a), HSTU(Zhai et al., 2024)), but these methods make strong assumptions on features engineering or model architectures, e.g., remove all statics features, replace entire model modules by Transformer. Actually, the common wisdom always formed RecSys ranking model designing as follows: first crafting hundreds elaborated features as model input and then use multi-task prediction module to obtain the click/like/comment/others scores for a user and item candidate pair (in Figure 1(a)). Nevertheless, the recent RecSys Scaling-laws studies are greatly changing such learning paradigms, which is difficult to directly deploy them in real online RecSys ranking models. In this paper, we focus on exploring the Scaling-laws under the above wide-used ranking model architecture. Due to differences in ranking model architecture and usage with pure Transformer-based LLMs, we conclude three elements that the reasons why NLP Scaling-laws do not match to RecSys ranking model: • For the training samples: different from NLP models having a stable data corpus to train an LLM from scratch, the industrial RecSys model always follows a streaming training paradigm. At Kuaishou, our model is streaming trained over 50 billion user logs daily, here we give three model variants training details, e.g., the ‘online base model’, ‘online base model with 50% training data downsampling’, and ‘cold-start’, where the ‘online base model’ is trained over years. From Figure 1(b), we can draw conclusions that: (1) training from scratch will hurt performance significantly; (2) although the model parameters is warm-start from a several years well-trained model, down-sampling the real-time training samples will still have performance degradation. Such phenomenons validate the RecSys model is data-hunger for infinite data to capture real-time users’ preferences. Therefore, the analysis of training data-scale is unnecessary in industry. • For the learning parameters: As shown in Figure 1(c), instead of LLM parameters are mainly located in ‘dense-activated’ Transformer-based DNN module (about 100 Billion) but with a small ‘sparse-activated’ word token parameters (¡0.01B). Ranking models always have contrary parameter distribution that: the major learnable parameters are concentrated on the ‘sparse-activated’ feature parameters (¿200B) while ‘dense-activated’ DNN parameters space (about 0.1B) is much smaller. Where the ‘sparse’ means only small part parameters will activate to involve calculating (i.e., we only need to look up few words tokens for a sentence in LLM; we only need to look up one user/item ID for a user-item training sample in RecSys), and the ‘dense’ means the calculation flow parameters that will fully activate to estimate final results. Indeed, in terms of the amount of learnable parameters, our ranking model (¿ 200B) far exceeds many LLMs (around 100B), which indicates that learnable parameters are not the bottleneck of ranking models. • For the inference complexity, FLOPs: Actually, the RecSys model inference complexity FLOPs is much smaller than LLM and has its upper bound. This is because our model needs to process hundreds of millions of requests efficiently, and that’s why our dense-activate DNN module is much smaller (about 0.1B). Hereby we could not blindly increase the inference complexity, to ensure our service’s stability and robustness of processing each request in milliseconds (LLMs are usually a few seconds). Based on the analysis of the above three aspects, compared to LLM, we can observe that our ranking model has its advantages and disadvantage are: (1) ‘unlimited’ streaming data, (2) massive parameter storage, (3) relatively low-FLOPs DNN module. In other words, for ranking model, the data and storage resources are cheap relatively, but inference computing resources are cautiously expensive. Motivates by such point, we consider that can we use the model the strengths in data and storage bootstrapping its weakness in computing. In other words, can we cache part of complex module calculation results to degenerate its time complexity? To answer the question, we propose our milestone work, MARM (Memory Augmented Recommendation Model), which achieves a new RecSys scaling-laws between the cache size and model performance. At MARM, we extend one of the most important user interests extraction modules in industry ranking model, which aims to calculate users historical item importance with the candidate item, as shown at the bottom of Figure 1(a). To the best of our knowledge, in implementing this module, many outstanding methods (e.g., DIN(Zhou et al., 2018), SIM(Pi et al., 2020), SDIM(Cao et al., 2022), TWIN(Chang et al., 2023)) are utilized a single-layer target-attention mechanism, as shown in Figure 2(a). Intuitively, such module could be further enhanced by stacking multi-layer self-attention before the final target-attention, as shown in Figure 2(b). Unfortunately, the self-attention mechanism time complexity \mathcal{O}(n^{2}*d) is much higher than target-attention mechanism time complexity \mathcal{O}(n*d) (n denotes the sequence length, d denotes representation dimension), which is more sensitive on longer sequences (e.g., n>1000) and will result in a large increasing in FLOPs. In online-serving, our ranking model needs to predict 50\sim8000 user/item-candidate pairs at same time to find the best dozens items for a user, causing the naive multi-layer attention mechanism will produce a lot of repeated calculations, which will bring heavy pressure for our system (in Figure 2(c)). If we cache the frequently used masked self attention module results, the complex masked self attention layer can be replaced by the simple target attention layer to efficiently predict different item candidates well, as shown in Figure 2(d). In this way, our MARM extends the single-layer attention-based sequences interests modeling module to a multiple-layer setting with minor inference complexity FLOPs cost. Specifically, our MARM solution significantly overcomes computational bottlenecks, and could seamlessly empower all user sequences interest extracting modules. Moreover, based on the MARM cache results, we find it can contribute to other models, such as retrieval and cascading models. In our experiments, we fully explore the scaling-laws between cache size and model performance, seeking a perfect balance in terms of cached attention depth, sequence length and embedding dimension. The main contributions of this work are: • We give detailed reasons why the scaling-laws of NLP do not match the ranking scenario, and we are the first work to explore new scaling-laws under the wide-used ranking model architecture from a fresh cache-performance perspective. • We devise a simple-yet-efficient approach, MARM, which utilizes the strengths of ranking model in data and storage to degenerate complex masked-self-attention time complexity that we successfully extend the single-layer attention-based sequences interests modeling module to a multiple-layer setting with minor inference complexity FLOPs cost. • We show our MARM is highly adaptable and scalable, seamlessly integrating into existing high-performance transformer-based models, and facilitating a smooth transition towards GPT-style models. Besides, extensive experiments and comparative ablation studies were conducted offline&online (average improves 0.43% GAUC offline & 2.079% play-time per user online)."
https://arxiv.org/html/2411.09410v2,LLM-assisted Explicit and Implicit Multi-interest Learning Framework for Sequential Recommendation,"Multi-interest modeling in current recommender systems (RS) is mainly based on user behavioral data, capturing user interest preferences from multiple dimensions. However, since behavioral data is implicit and often highly sparse, it is challenging to understand users’ complex and diverse interests. Recent studies have shown that the rich semantic information in the text can effectively supplement the deficiencies of behavioral data and provide a new perspective for building more accurate user portraits. Despite this, it is still difficult for small models to directly extract semantic features associated with users’ deep interests. That is, how to effectively align semantics with behavioral information to form a more comprehensive and accurate understanding of user interests has become a critical research problem.To address this, we propose an LLM-assisted explicit and implicit multi-interest learning framework (named EIMF) to model user interests on two levels: behavior and semantics. The framework consists of two parts: Implicit Behavioral Interest Module (IBIM) and Explicit Semantic Interest Module (ESIM). The traditional multi-interest RS model in IBIM can learn users’ implicit behavioral interests from interactions with items. In ESIM, we first adopt a clustering algorithm to select typical samples and design a prompting strategy on LLM to obtain explicit semantic interests. Furthermore, in the training phase, the semantic interests of typical samples can enhance the representation learning of behavioral interests based on the multi-task learning on semantic prediction and modality alignment. Therefore, in the inference stage, accurate recommendations can be achieved with only the user’s behavioral data. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed EIMF framework, which effectively and efficiently combines small models with LLM to improve the accuracy of multi-interest modeling.","Recommender systems (RS) (Bobadilla et al., 2013; Aggarwal et al., 2016) often rely on user behavioral data to learn user preferences to provide personalized services in multiple applications of World Wide Web. Given that user interests tend to evolve over time, to more accurately capture the short-term and immediate trend of user interest changes, sequential recommendation (SR) (Fang et al., 2020; Xie et al., 2022) has gradually become a key focus in research and applications. Different from traditional content-based (Pazzani and Billsus, 2007; Lops et al., 2011) or collaborative filtering RSs (He et al., 2017, 2020), SR emphasizes the temporal order and contextual information of behaviors, which uses the user’s past behavior sequence to model user interests and predict the items that may be of interest next. Most current SR methods model user interests as a single, comprehensive representation to capture the user’s overall interests. However, users may be interested in multiple types of content over a while, so there is a clear representational bottleneck in modeling users’ varying interests using a single-interest learning model. In the MIND model (Li et al., 2019) proposed in 2019, the concept of multi-interest learning is introduced for the first time. The model employs the dynamic routing mechanism of capsule networks to capture the diverse interests of users. Since then, the research of multi-interest learning (Du et al., 2024; Liu et al., 2024; Yan et al., 2024; Pei et al., 2024) has received increasing attention. Current multi-interest approaches mainly rely on user behavioral data to learn users’ interests. However, most of this behavioral data consists of implicit feedback (Wang et al., 2021), such as clicking, browsing, and purchasing behaviors, which do not always accurately reflect the user’s true preferences. It may contain noisy data (Han et al., 2024; Qiao et al., 2023), such as records generated by mis-touching or non-true expressions of interest due to popularity bias, which may interfere with the model’s understanding of the user’s actual preferences. In addition, user behavioral data usually exhibits a high degree of sparsity, which further limits the model’s ability to capture users’ diverse and dynamically changing interests. With the rapid development of natural language processing (NLP), the era of large language models (LLMs) (OpenAI, 2022; Gao et al., 2023) has arrived. LLMs’ outstanding language understanding, logical inference, and content generation capabilities provide new ideas and methods for solving the above challenges. That is, LLM is not only capable of extracting deep semantic information from text but also contains extensive knowledge of the outside world. With its superior contextual understanding, LLM effectively filters out irrelevant information and accurately captures user interests. Numerous studies (Cui et al., 2022; Geng et al., 2022; Li et al., 2023b; Zhang et al., 2023a; Yang et al., 2024) have shown that applying LLMs to RS demonstrates promising results in solving the cold-start problem and improving the interpretability of recommendation. However, since SR has high real-time requirements and the fine-tuning and inference of LLM requires a lot of time and computing resources, the fundamental problem of combining large models (LLMs) and small models (traditional algorithms) remains unsolved. As mentioned above, multi-interest learning aims to learn the semantic-aware sub-interest, which happens to hold the same view as the LLM-enhanced RS. Therefore, combining the learned semantic information from LLMs with behavioral data is promising yet still very challenging: how to perform the alignment and how to ensure high efficiency? To address the above challenges, we propose an LLM-assisted Explicit and Implicit Multi-interest Learning Framework (shorten as EIMF). Specifically, EIMF can be divided into two modules, the implicit behavioral interest module (IBIM) and the explicit semantic interest module (ESIM). In the training phase, IBIM follows the traditional multi-interest SR model, which can learn the implicit behavioral interests of users from their interaction data with items. ESIM, on the other hand, first analyzes user-generated text sequences using the clustering algorithm to identify representative samples. Next, these typical samples are subjected to deep inference using an LLM to reveal the users’ explicit semantic interests. Finally, we match semantic and behavioral interests based on “id-cluster numbering”, with joint learning on two auxiliary tasks (text classification and modal alignment), which can help fuse semantic information into behavioral interest representations. In the testing phase, accurate recommendations can be achieved based solely on user behavior data. In summary, the contributions of this paper are as follows: • We answer the question of how to combine large\&small models in RS from the perspective of multi-interest learning and semantic alignment. We propose an LLM-assisted multi-interest learning framework, which can fully use the semantic reasoning capabilities of LLM and enrich the user’s behavioral learning, thereby significantly improving the accuracy of recommendation results. • We propose an efficient typical sample strategy to reduce the inference cost of LLM and enrich the user interest representation through multi-task learning in the training phase so that more accurate recommendations can be achieved in the testing phase by only using user behavior data. • We conduct extensive experiments on real-world datasets, and the results show that our EIMF can significantly and stably improve the recommendation performance and exhibit excellent generalization capabilities."
https://arxiv.org/html/2411.09269v1,Harnessing multiple LLMs for Information Retrieval: A case study on Deep Learning methodologies in Biodiversity publications,"Deep Learning (DL) techniques are increasingly applied in scientific studies across various domains to address complex research questions. However, the methodological details of these DL models are often hidden in the unstructured text. As a result, critical information about how these models are designed, trained, and evaluated is challenging to access and comprehend. To address this issue, in this work, we use five different open-source Large Language Models (LLMs): Llama-3 70B, Llama-3.1 70B, Mixtral-8x22B-Instruct-v0.1, Mixtral 8x7B, and Gemma 2 9B in combination with Retrieval-Augmented Generation (RAG) approach to extract and process DL methodological details from scientific publications automatically. We built a voting classifier from the outputs of five LLMs to accurately report DL methodological information. We tested our approach using biodiversity publications, building upon our previous research. To validate our pipeline, we employed two datasets of DL-related biodiversity publications: a curated set of 100 publications from our prior work and an additional set of 364 publications from the Ecological Informatics journal. Our results demonstrate that the multi-LLM, RAG-assisted pipeline enhances the retrieval of DL methodological information, achieving an accuracy of 69.5% (417 out of 600 comparisons) based solely on textual content from publications. This performance was assessed against human annotators who had access to code, figures, tables, and other supplementary information. Although demonstrated in biodiversity, our methodology is not limited to this field; it can be applied across other scientific domains where detailed methodological reporting is essential for advancing knowledge and ensuring reproducibility. This study presents a scalable and reliable approach for automating information extraction, facilitating better reproducibility and knowledge transfer across studies.","Deep Learning (DL) has become a cornerstone in numerous fields, revolutionizing how complex data is analyzed and interpreted. From healthcare and finance to autonomous systems and natural language processing, DL techniques have delivered groundbreaking results. However, as the adoption of DL continues to grow, there is an increasing recognition of a critical shortcoming: the limited availability of detailed methodological information in scientific literature (Waide et al.,, 2017; Stark,, 2018; Samuel et al.,, 2021; Pineau et al.,, 2021; Gundersen et al.,, 2022). This gap presents significant challenges for researchers and practitioners who seek to understand, replicate, and build upon existing studies (Feng et al.,, 2019; GPAI,, 2022). Past research has emphasized the need to make primary data and clear metadata available to support transparency (Michener et al.,, 1997; Whitlock,, 2011). A DL pipeline is a structured process for training and deploying DL models, starting with data collection and preprocessing tasks like cleaning, normalization, and transformation (El-Amir and Hamdy,, 2020). After preparing the data, the pipeline moves to model selection, where an appropriate architecture is chosen based on model complexity and problem type. The selected model is then trained on preprocessed data, fine-tuning through specific optimization algorithms and hyperparameter configurations. Once trained, the model’s performance is evaluated on test data to ensure reliable, unbiased results. The final step involves deploying the model for real-world use or further refinement. For a DL pipeline to be reproducible, detailed documentation at each stage is essential (Pineau et al.,, 2021). This includes logging data collection methods, preprocessing steps, model architecture configurations, hyperparameters, and training details, as well as performance metrics and test datasets. Additionally, maintaining records of software libraries, hardware, frameworks, and versions used is critical for the accurate replication of the study. Without access to such crucial information, stakeholders—including academics, industry professionals, and policymakers—face significant challenges in validating study outcomes or advancing research in meaningful ways. In areas like healthcare, finance, and autonomous systems, where DL applications influence real-world decisions, the absence of methodological transparency can compromise trust in DL models and limit their broader application (Haddaway and Verhoeven,, 2015). We contend that the same holds true for biodiversity research. The advent of DL has significantly transformed various domains, including biodiversity research, by enabling advanced methodologies for data analysis and interpretation (August et al.,, 2020). However, manually extracting relevant deep-learning information from scientific articles remains a labour-intensive and time-consuming process. This challenge affects both the reproducibility of the original studies and the reproducibility of secondary analyses aimed at understanding the methods employed. Traditional manual retrieval methods can be inconsistent, as the perspective of the annotators often varies based on their task interpretation and domain knowledge. These inconsistencies hinder efforts to systematically review or replicate the methodological approaches across studies, highlighting the need for more automated solutions. To address these challenges, we propose a novel approach that leverages the capabilities of Large Language Models (LLMs) for the automated extraction and processing of DL methodological information from scientific publications. LLMs, which are trained on vast amounts of text data, have demonstrated impressive capabilities in natural language understanding and generation. Specifically, we employ five open-source LLMs: Llama-3 70B111https://ai.meta.com/blog/meta-llama-3/, Llama-3.1 70B222https://ai.meta.com/blog/meta-llama-3-1/, Mixtral-8x22B-Instruct-v0.1333https://mistral.ai/news/mixtral-8x22b/, Mixtral 8x7B444https://mistral.ai/news/mixtral-of-experts/, and Gemma 2 9B555https://blog.google/technology/developers/google-gemma-2/ in combination with Retrieval-Augmented Generation (RAG) approach (Lewis et al.,, 2020). By utilizing these advanced models, we aim to extract relevant methodological details with greater accuracy and efficiency than manual methods alone. Our methodology is structured into three critical components: identifying relevant research publications, automatically extracting information through an RAG approach, and converting the extracted textual responses into categorical values for streamlined evaluation. In this work, we take biodiversity publications as a case study due to the growing popularity of DL methods in biodiversity research and the enormous number of publications using DL for various applications in this domain. Given the importance of biodiversity research and the critical need for transparent sharing of DL information in these studies (GPAI,, 2022), we chose this field to demonstrate our approach. However, our methodology is not limited to biodiversity alone; it can be applied to other domains where detailed methodological reporting is essential for advancing scientific knowledge and ensuring reproducibility. To enhance the reliability of our approach, we developed a voting classifier that aggregates the outputs of these LLMs, ensuring that the reported information is consistent and accurate. This methodology was applied to two distinct datasets of biodiversity publications focused on DL: one consisting of 100 publications from our previous work (Ahmed et al., 2024b, ) and another comprising 364 publications from the Ecological Informatics journal666https://www.sciencedirect.com/journal/ecological-informatics. Our approach can help identify gaps in reporting and ensure that critical information about DL methodologies is accessible, thereby enhancing the transparency and reproducibility of research. This paper presents a comprehensive case study on applying multiple LLMs for information retrieval in the context of DL methodologies within biodiversity publications. Through our approach, we aim to contribute to the growing body of research focused on automating information extraction and improving the reproducibility of results in scientific literature. By demonstrating the effectiveness of our pipeline, we hope to pave the way for future research that harnesses advanced AI techniques to further enhance data retrieval and analysis in biodiversity and beyond. Ensuring reproducibility in LLM applications requires a clear, comprehensive methodology that specifies all critical steps, settings, and model configurations. By providing all methodological details transparently, we aim to ensure that our approach can be consistently replicated and applied in future studies, supporting the reliable and reproducible use of LLMs in scientific research. In the following sections, we provide a detailed description of our study. We start with an overview of the state-of-the-art (“Related Work”). We provide the methodology of our study (“Methods”) We describe the results of our work (“Results”) and provide a detailed evaluation of our results (“Evaluation”). We discuss the implications of our study (“Discussion”). Finally, we summarize the key aspects of our study and provide future directions of our research (“Conclusion”)."
https://arxiv.org/html/2411.09181v1,DeBaTeR: Denoising Bipartite Temporal Graph for Recommendation,"Due to the difficulty of acquiring large-scale explicit user feedback, implicit feedback (e.g., clicks or other interactions) is widely applied as an alternative source of data, where user-item interactions can be modeled as a bipartite graph. Due to the noisy and biased nature of implicit real-world user-item interactions, identifying and rectifying noisy interactions are vital to enhance model performance and robustness. Previous works on purifying user-item interactions in collaborative filtering mainly focus on mining the correlation between user/item embeddings and noisy interactions, neglecting the benefit of temporal patterns in determining noisy interactions. Time information, while enhancing the models’ utility, also bears its natural advantage in helping to determine noisy edges, e.g., if someone usually watches horror movies at night and talk shows in the morning, a record of watching a horror movie in the morning is more likely to be noisy interaction. Armed with this observation, we introduce a simple yet effective mechanism for generating time-aware user/item embeddings and propose two strategies for denoising bipartite temporal graph in recommender systems (DeBaTeR): the first is through reweighting the adjacency matrix (DeBaTeR-A), where a reliability score is defined to reweight the edges through both soft assignment and hard assignment; the second is through reweighting the loss function (DeBaTeR-L), where weights are generated to reweight user-item samples in the losses. Extensive experiments have been conducted to demonstrate the efficacy of our methods and illustrate how time information indeed helps identifying noisy edges.","Recommender systems have been widely applied for studying user preferences (lu2015recommender, ). As a powerful method for building recommender systems, collaborative filtering leverages the interaction history of users and items to mine latent information. Lately, with the development of Graph Neural Networks, neural graph collaborative filtering has received extensive attention, and many state-of-the-art methods have been proposed (SimGCL, ; NCL, ; he2020lightgcn, ; SGL, ) with their capability of capturing high-order interaction information. Despite the substantial progress, some challenges still exist, and one of them lies in the noisy nature of collected data (zhang2023robust, ), especially when the data is collected from implicit feedback (e.g., clicks) (10.1145/3474085.3475446, ). For example, users might inadvertently interact with some items (misclicks or merely out of curiosity), or some of the items might be intended for others (e.g., gifts). These noisy interactions are still considered as positive samples for model training, even though they do not align with the interacting user’s preferences. Moreover, many malicious attacks (10.1145/3447548.3467233, ; 10.1145/3274694.3274706, ; 7824865, ) have been designed to craft fake interactions or users to bias recommender systems, resulting in sub-optimal prediction results. In the context of graph neural collaborative filtering, the problem becomes even worse as message passing in graph neural networks (GNNs) will amplify the impact of noisy interactions. Therefore, identifying and rectifying noisy interactions for neural graph collaborative filtering becomes important for building robust recommender systems that align with users’ true interests. Many approaches have been proposed to tackle this issue. Depending on how samples are purified, they can be roughly divided into two categories. The first type of strategies directly remove the identified noisy samples from training data (10.5555/3000375.3000390, ; 10.1145/3474085.3475446, ); while another group of works actively downweight those training samples during model training (BOD, ; RocSE, ; T-CE, ; SGDL, ). To identify noisy interactions before rectifying, these works rely on trainable (e.g., a multi-layer perceptron) or predefined functions (e.g., cosine similarity) to model the correlation between user/item embeddings and reliability. However, none of the existing work has focused on the natural advantage of time information for denoising. The advantages of leveraging interaction time for collaborative filtering are threefold. First, interaction times can be easily collected as every implicit feedback signal has an associated timestamp that we can leverage. Second, user-side temporal patterns can help identify some noisy interactions that cannot be distinguished without time information. For example, if a user usually watches horror movies at night and talk shows in the morning, a record of that user watching a horror movie in the morning is likely to be a noisy interaction, and this noisy interaction cannot be distinguished without the interaction timestamps. However, if the recommender system is aware of this temporal user pattern, it could recommend more talk shows instead of horror movies to the user in the morning. Third, item-side temporal patterns can help identify noisy interactions as well. For example, for some movies, the records of watching these movies mainly occur around a certain time (e.g., Christmas movies are usually viewed around Christmas). Then, a record of a user watching such movies might not align with that user’s usual preferences, but given that the interaction time is close to this particular time, the model will be able to recognize this is less likely to be a noisy interaction. In addition to denoising, time information can also help give a more accurate and timely recommendation for users. To bridge this gap between time-aware recommendation and denoising neural graph collaborative filtering, we aim to design a mechanism that can incorporate interaction time into neural graph collaborative filtering with the following desired properties: (1) be easy to be integrated with most state-of-the-art neural graph collaborative filtering models, (2) be able to learn temporal patterns of users and items, and (3) enhance the model’s performance in both denoising and prediction. Given that most collaborative filtering systems calculate the final prediction score/ranking based on the dot product between user and item embeddings, we propose to generate time-aware user and item embeddings, where their dot products preserve both (1) the global preferences that previous works have been exploiting and (2) the temporal preferences modeling the probability an interaction exists at a particular timestamp, which have been largely ignored by previous works. The encoders for generating time-aware embeddings can be optimized along with the training of the base model. Furthermore, we improve two popular methods for denoising bipartite graph in recommender systems, including reweighting the adjacency matrix (DeBaTeR-A) and reweighting the loss function (DeBaTeR-L), by integrating our proposed time-aware embeddings into the two methods to improve their ability to identify noisy interactions and produce more accurate predictions. DeBaTeR-A calculates the reliability score by the similarity between time-aware user and item embeddings, then reweights and prunes edges in bipartite graph through both soft and hard assignment to remove or downweight noisy interactions. DeBaTeR-L utilizes a weight generator that leverages time-aware emeddings to predict the probability of an interaction being noisy and minimizes the influence of noisy interactions by downweighting noisy samples in the loss function. For both DeBaTeR-A and DeBaTeR-L, time-aware embeddings are also utilized in the base model to enhance the prediction accuracy. We evaluate the effectiveness of proposed methods on four real-world datasets and compare them with state-of-the-art methods. We further inject noisy edges into training datasets to evaluate the robustness of the proposed models against random noises. Ablation studies have been conducted to demonstrate improvements by our proposed time-aware embeddings learning mechanism. Experiments on datasets with different noise sampling strategies demonstrate that our models successfully capture the temporal patterns. Our main contributions can be summarized as follows: • We propose a method (DeBaTeR) that leverages time information in neural graph collaborative filtering to enhance both denoising and prediction performance. To the best of our knowledge, this is the first work to leverage time information in neural graph collaborative filtering recommender systems. • We propose two methods, DeBaTeR-A and DeBaTeR-L, that utilize DeBaTeR for denoising recommender systems by reweighting edges or losses. • Extensive experiments are conducted to demonstrate the efficacy of our proposed methods. Our methods outperform state-of-the-art models for both utility and robustness. The rest of the paper is structured as follows. Section 2 provides relevant preliminary knowledge. Section 3 introduces our proposed methods. Section 4 presents the experimental results and analysis. Section 5 discusses related works. Section 6 draws the conclusion of the paper."
https://arxiv.org/html/2411.09065v1,Language-Model Prior Overcomes Cold-Start Items,"The growth of recommender systems (RecSys) is driven by digitization and the need for personalized content in areas such as e-commerce and video streaming. The content in these systems often changes rapidly and therefore they constantly face the ongoing cold-start problem, where new items lack interaction data and are hard to value. Existing solutions for the cold-start problem, such as content-based recommenders and hybrid methods, leverage item metadata to determine item similarities. The main challenge with these methods is their reliance on structured and informative metadata to capture detailed item similarities, which may not always be available. This paper introduces a novel approach for cold-start item recommendation that utilizes the language model (LM) to estimate item similarities, which are further integrated as a Bayesian prior with classic recommender systems. This approach is generic and able to boost the performance of various recommenders. Specifically, our experiments integrate it with both sequential and collaborative filtering-based recommender and evaluate it on two real-world datasets, demonstrating the enhanced performance of the proposed approach. Code can be found at https://github.com/awslabs/language-model-prior-4-item-cold-start.","Figure 1: Recommender is trained on historical user-item interactions, and then used to recommend new items, including those that previously appeared (i.e., dress) and newly introduced cold-start items (i.e., book). The field of recommender systems (RecSys) has witnessed tremendous growth over the last few years [Kang and McAuley, 2018c, Ma et al., 2020, Ding et al., 2021, Zhang et al., 2021, Lin et al., 2023, Li et al., 2023, Ding et al., 2023], driven by the increasing service digitization and the rising demand for personalized content across diverse platforms such as e-commerce and video streaming. Despite significant advancements, the unresolved item cold start problem remains a critical challenge. It arises with newly introduced items lacking sufficient interaction data, and thus struggling to be accurately recommended to users. For instance, Figure 1 illustrates that items such as laptop, dress, camera and glasses that have appeared in historical data are typically easier to recommend, whereas those never-before-seen items are challenging. This issue is especially severe in dynamic environments, such as news recommendation, where new items are constantly introduced, making it tough to identify similarities among cold items due to insufficient information. To address the item cold start problem, previous works primarily fall into two major categories: content-based recommendation and hybrid methods. Both approaches focus on leveraging additional item metadata to uncover item similarities. Content-based recommendation approaches such as [van den Oord et al., 2013, Volkovs et al., 2017a, b] tackle the issue by utilizing item metadata such as item category. These methods analyze the item content and recommend similar items by aligning them with user preferences, rather than relying heavily on past user-item interactions. For example, a model might recommend a new fantasy novel to a fan of the genre, despite limited interaction data on the novel itself. Another line of works focuses on hybrid methods [Wang and Blei, 2011, Zhang et al., 2016, Pan et al., 2019, Zhou et al., 2020, Liu et al., 2021, Han et al., 2022], which combine the strength of both content-based and collaborative filtering (CF) techniques. These approaches integrate user behavior with item attributes to generate recommendations, aiming to capitalize on both aspects. Hybrid methods are particularly notable for delivering precise recommendations by encompassing a wide spectrum of user preferences and item features. Overall, both strategies aim to tap into the item metadata to provide prior knowledge about item similarities. However, the main problem with these methods is twofold: (1) they require structured metadata, which is not always available, and (2) the structured metadata may be uninformative and unable to capture fine-grained item similarities. For example, consider a scenario with only five unique categories for all items, with a skewed distribution where the largest category comprises 80% of the items. Recent advancements in Language Models (LMs) enable extracting insights from unstructured textual metadata, like product descriptions or movie synopses, using pre-trained models such as BERT [Devlin et al., 2018] and Falcon [Almazrouei et al., 2023].. This approach leverages the inherent prior knowledge of LMs to uncover item similarities, even with limited available interactions or structured item metadata. Existing works [Ding et al., 2021] focus on generating text embedding from the pre-trained LMs based on the item textual metadata. However, directly using LMs embedding of items as the input of recommender may introduce a vast amount of information, not all of which is relevant to the recommendation task at hand. Also, it can significantly increase the dimensionality of data the recommender should process, and limits the flexibility of how the item meta information is integrated into the recommendation system. Therefore, in contrast to previous works, this paper investigates the possibilities and challenges of implicitly harnessing LMs to inject prior knowledge of items. Specifically, we propose a framework that integrates a Bayesian regularizer into the training process of the RecSys. The regularizer takes into account the semantic similarity among items, leveraging LM-encoded prior knowledge to learn fine-grained item embeddings in the continuous latent space. The proposed approach is generic and can be adopted in a plug-and-play fashion in any sequential [Kang and McAuley, 2018c, Ma et al., 2020] or CF-based [Rendle et al., 2012, Wang et al., 2015] RecSys. Our contributions can be summarized as follows: • We introduce a novel Bayesian framework that leverages LMs embeddings of item metadata as the prior to address the item cold start problem in RecSys. This framework leverages the rich semantic information from LMs to improve the ability of the recommender to understand and recommend new items effectively. • The proposed Bayesian prior is integrated into RecSys as a regularizer, and therefore is generic and able to enhance various recommenders by supplementing their learning objectives. • We evaluated the proposed method using both sequential (i.e., SASRec) and CF-based (i.e., BPRMF) recommenders on two real-world datasets from distinct domains. The empirical results demonstrate the enhanced performance of the proposed approach, which improves SASRec by 17.78\% regarding normalized discounted cumulative gain."
https://arxiv.org/html/2411.09053v1,Information Need in Metaverse Recordings - a Field Study,"Metaverse Recordings (MVRs) represent an emerging and underexplored media type within the field of Multimedia Information Retrieval (MMIR). This paper presents findings from a field study aimed at understanding the users information needs and search behaviors specific to MVR retrieval. By conducting and analyzing expert interviews, the study identifies application scenarios and highlights challenges in retrieving multimedia content from the metaverse. The results reveal existing application scenarios of MVRs and confirm the relevance of capturing time-series data from the graphical rendering process and related input-output devices, which are also highly relevant to user needs. Furthermore, the study provides a foundation for developing retrieval systems tailored to MVRs by defining use cases, user stereotypes, and specific requirements for MVR Retrieval systems. The findings contribute to a better understanding of information search behaviors in MVR Retrieval and pave the way for future research and system design in this field.","The growth rate of multimedia creation is high. Digital Cameras are ubiquitous and social media has led to an immense media generation, and, in recent years, boosted short form video content. Furthermore, the COVID crisis has given remote technologies for communication a push, such as increased use of video conferencing, virtual conferences. Another trend re-emerged in the last years, the idea of an everlasting virtual space, where people meet and life together - the metaverse. The growth rate of usage of platforms [1] like Roblox [2] or Minecraft [3] show, that people are heavily using virtual worlds. Trend reports assume an even higher usage in the future [4]. It is likely, that people will create recordings of experiences in the virtual world, like they do in the real world. Early versions of this can be seen as YouTube videos [5] for entertainment purposes. Multimedia Information Retrieval (MMIR) [6] is the field in computer science, which addresses indexing and retrieval of multimedia content. The metaverse is build on virtual worlds, which are basically computer generated multimedia. Therefore, we examine the integration of MVRs in MMIR. In earlier publications [7, 8] we have outlined the differences between metaverse content and other media types, i.e. format, structure and content. The analysis of the differences revealed a lack of support of MMIR for metaverse content. The further integration of MVR in MMIR should be grounded on user demands. There is a noted gap in the existing literature regarding the information needs specifically related to MVR Retrieval. Understanding these information needs is essential for developing effective MVR retrieval systems. MVR as a new multimedia type introduces challenges for integration in MMIR, related to the capture, organization, and retrieval of content generated in virtual environments. One open question is whether such user sessions are recorded, which would be indirectly recorded metaverse content, in the field and for which applications. Another significant challenge concerns the formats of data available in metaverse environments and how they align with user interests. Unlike traditional media recordings, MVRs can capture not only video and audio but also complex data formats such as movement patterns, eye-tracking information, and biosensor data. The potential for data capture in the metaverse is considerable, yet it remains unclear how these rich data formats align with users needs and interests. For example, while systems may be capable of reconstructing virtual scenes with mathematical precision, it is unclear whether users find such detailed data useful or necessary for their tasks. A further challenge lies in understanding users information needs and how they search for and retrieve MVRs. Little is known about the information searching behavior specific to MVRs, and existing search systems are not yet tailored to the unique attributes of virtual worlds. Traditional search filters, such as date ranges, location, and event types, may not fully capture the complexity of user needs in the metaverse. Moreover, it is unclear how users express their information needs when searching for MVRs, as past queries and behaviors have not yet been documented. The lack of understanding of the technical capabilities and user interests shows a critical research gap. Understanding which data types users value and how they search for MVRs is crucial for integrating MVR in MMIR and developing effective MVR Retrieval systems. In this paper, we present a field study conducted with a small expert group. Based on interviews, we describe application scenarios and search behaviors of users, and how MMIR can support them. The following sections present an overview of the metaverse and related technologies, Information Retrieval (IR), and MMIR in Section 2. Section 3, describes the field study design. Section 4 presents the results of the interviews. Finally, Section 5 summarizes the presented work and discusses future work."
https://arxiv.org/html/2411.08891v1,Calibrated Decision-Making through Large Language Model-Assisted Retrieval,"Recently, large language models (LLMs) have been increasingly used to support various decision-making tasks, assisting humans in making informed decisions. However, when LLMs confidently provide incorrect information, it can lead humans to make suboptimal decisions. To prevent LLMs from generating incorrect information on topics they are unsure of and to improve the accuracy of generated content, prior works have proposed Retrieval Augmented Generation (RAG), where external documents are referenced to generate responses. However, traditional RAG methods focus only on retrieving documents most relevant to the input query, without specifically aiming to ensure that the human user’s decisions are well-calibrated. To address this limitation, we propose a novel retrieval method called Calibrated Retrieval-Augmented Generation (CalibRAG), which ensures that decisions informed by the retrieved documents are well-calibrated. Then we empirically validate that CalibRAG improves calibration performance as well as accuracy, compared to other baselines across various datasets.","1 introduction Large language models (LLMs; Jiang et al., 2023; Touvron et al., 2023; Dubey et al., 2024; Achiam et al., 2023) have demonstrated remarkable performance on numerous downstream natural language processing (NLP) tasks, leading to their widespread integration into various decision-making processes (Bommasani et al., 2021; Band et al., 2024; Zhou et al., 2024). However, even with significant increases in model size and the expansion of training datasets, it remains infeasible for LLMs to encode all possible knowledge within their parameters. As a result, the outputs produced by LLMs may not consistently be reliable for important human decision-making processes, potentially overlooking key or hidden details. Additionally, LLMs frequently provide inaccurate or misleading information with a high degree of confidence, a phenomenon referred to as hallucination (Zhuo et al., 2023; Papamarkou et al., 2024), which can lead humans to make flawed decisions. In addition, Zhou et al. (2024) have empirically demonstrated that human users often over-rely on LLM outputs during decision-making processes, and this over-reliance tends to increase in proportion to the model’s confidence. Here, the model’s confidence refers to the verbalized expression of how certain the model is when asked how confident it is in its answer. Specifically, they have found that for answers with high confidence, users show strong over-reliance regardless of whether the answer is correct or not. These findings highlight that utilizing LLMs without proper calibration of their responses and addressing the frequent occurrence of hallucinations can lead to incorrect decisions in high-stakes tasks like medical diagnosis and legal reasoning, potentially resulting in severe consequences (Li et al., 2019; 2022b; Han et al., 2024). Retrieval Augmented Generation (RAG) (Lewis et al., 2020; Li et al., 2022a; Wang et al., 2024) has emerged as a promising method to address hallucinations, which is one of the two key issues when using LLMs in decision-making (Shuster et al., 2021; Li et al., 2024). Instead of generating answers directly, RAG retrieves relevant documents from external databases and uses them as an additional context for response generation. This approach supplements the information that LLMs lack, resulting in more accurate and reliable responses. However, the database cannot encompass all information, and the world knowledge is continuously being updated. In such cases, the retriever may retrieve irrelevant documents, which can distract the LLM and lead to the generation of incorrect answers to the question (Shi et al., 2023). Moreover, as described in Section 2.2, due to the LLM’s overconfidence in the retrieved document, they still tend to assign high confidence to its responses even when they are incorrect. To address the issue of deep neural networks generating overconfident outputs for given inputs and to promote well-calibrated predictions, research on uncertainty calibration has been actively conducted across various fields (Kuleshov et al., 2018; Laves et al., 2020; Kapoor et al., 2024). In particular, for image classification tasks in computer vision, numerous techniques (Lakshminarayanan et al., 2017; Maddox et al., 2019; Thulasidasan et al., 2019) have been developed to improve uncertainty calibration. Especially, post hoc methods like temperature scaling, which simply adjust the output logits, have been shown to be simple yet effective in improving calibration (Kull et al., 2019; Vaicenavicius et al., 2019; Minderer et al., 2021; Widmann et al., 2022). However, in contrast to vision tasks, calibrating LLMs poses a more complex challenge due to their sequential token generation nature (Kapoor et al., 2024). Specifically, LLMs produce sequences of log probabilities for each token, and the number of possible sequences grows exponentially with length, making it impractical to apply traditional calibration methods that consider all output possibilities. This complexity renders straightforward adaptations of calibration techniques like temperature scaling ineffective for long-form sentence generation tasks in LLMs. To address these challenges, recent work by Band et al. (2024) proposed an uncertainty calibration method specifically designed for decision-making scenarios involving LLMs in long-form generation contexts. This method aims that the probabilities associated with user decisions, based on the guidance generated by the LLM, are well-calibrated. However, this method still lacks the ability to calibrate the probabilities associated with user decisions based on the guidance provided by RAG. To address this issue, we propose the Calibrated Retrieval-Augmented Generation (CalibRAG) framework. CalibRAG allows an LLM using RAG to not only select relevant information to support user decision-making but also provide confidence levels associated with that information by utilizing a forecasting function, ensuring well-calibrated decisions based on the retrieved documents. Here, the forecasting function is the surrogate model that predicts the probability of whether the user’s decision based on the guidance provided by RAG will be correct. We empirically validate that our CalibRAG significantly improves calibration performance as well as accuracy, compared to other relevant baselines across several datasets. Our contributions can be summarized as follows: • We propose the CalibRAG framework, which enables well-calibrated decision-making based on the guidance provided by RAG. • We construct a new dataset by creating labels that indicate whether decisions made using retrieved documents correctly answer the questions, essential for training the forecasting function. • We outperform existing uncertainty calibration baselines across various tasks involving RAG context using the Llama-3.1 model in decision-making scenarios."
https://arxiv.org/html/2411.09213v1,Comprehensive and Practical Evaluation of Retrieval-Augmented Generation Systems for Medical Question Answering,"Retrieval-augmented generation (RAG) has emerged as a promising approach to enhance the performance of large language models (LLMs) in knowledge-intensive tasks such as those from medical domain. However, the sensitive nature of the medical domain necessitates a completely accurate and trustworthy system. While existing RAG benchmarks primarily focus on the standard retrieve-answer setting, they overlook many practical scenarios that measure crucial aspects of a reliable medical system. This paper addresses this gap by providing a comprehensive evaluation framework for medical question-answering (QA) systems in a RAG setting for these situations, including sufficiency, integration, and robustness. We introduce Medical Retrieval-Augmented Generation Benchmark (MedRGB) that provides various supplementary elements to four medical QA datasets for testing LLMs’ ability to handle these specific scenarios. Utilizing MedRGB, we conduct extensive evaluations of both state-of-the-art commercial LLMs and open-source models across multiple retrieval conditions. Our experimental results reveals current models’ limited ability to handle noise and misinformation in the retrieved documents. We further analyze the LLMs’ reasoning processes to provides valuable insights and future directions for developing RAG systems in this critical medical domain.","Large language models (LLMs) have demonstrated remarkable capabilities in solving complex medical problems, achieving state-of-the-art performance across various benchmarks. However, ensuring the reliability and truthworthiness of an artificial intelligent (AI) medical system remains a critical challenge, especially in healthcare applications. Retrieval-augmented generation (RAG) has emerged as a promising approach to reduce LLMs’ hallucination problem by integrating external knowledge sources. Figure 1: Blue texts are useful information that should be extract to help determine the answer. Red texts are factual errors that potentially mislead the LLMs. While RAG has potential to improve the factual accuracy of LLMs’ response, incorporating an information retriever also presents new complexities that warrant careful evaluation. Consider the example in Fig. 1. The retrieved documents can contain not only useful knowledge that helps determine the true answer, but also noise information, or more serious, factual errors that can misleads the LLMs. To consciously apply RAG for medical QA, we must consider these practical scenarios and evaluate LLMs ability to interact with retrieved documents reliably. Recent efforts have been made to evaluate AI systems with LLMs in the medical domain (Nori et al. 2023; He et al. 2023; Xiong et al. 2024). For example, MedEval (He et al. 2023) presents a large-scale, expert-annotated benchmark that cover various medical tasks and domains. (Xiong et al. 2024) evaluates RAG extensively based on their MIRAGE benchmark which cover 5 medical QA datasets. However, they only focus on the effect of RAG modules on target accuracy, missing other important aspects of a AI medical system. Several recent works have explore RAG evaluation more comprehensively in general domain (Es et al. 2023; Chen et al. 2024b), RAGAS (Es et al. 2023) assess 3 qualities of RAG’s outputs for QA tasks including: Faithfulness - degree to which responses align with the provided context, Answer Relevance - the extent to which generated responses address the actual question posed, and Context Precision-Recall - the quality of retrieved context. We follow the work from (Chen et al. 2024b) which establishes Retrieval-Augmented Generation Benchmark (RGB) to measure 4 abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. In particular, using questions from 4 medical QA datasets from MIRAGE as basis, we create Medical Retrieval-Augmented Generation Benchmark (MedRGB) to evaluate RAG system in the following 4 test scenarios: • Standard-RAG: evaluates LLMs performance when presented with multiple retrieved signal documents to create a context to answer to question. • Sufficiency: evaluates LLMs reliability when there are noise documents within the retrieved context. By adding ”Insufficient Information” as an additional response option, LLMs should only answer when they are confident to have enough information to determine the correct answer. This requires LLMs to not only be aware of its own internal knowledge, but also be able to filter out noisy information from external documents. • Integration: evaluates LLMs ability to answer multiple supporting questions and integrate the extracted information to help address the main question. • Robustness: evaluates LLMs resiliency to factual errors in the retrieved context. A trustworthy AI medical system should be able detect factually incorrect documents and provide the corrected information. In total, MedRGB consists of 3480 instances for 4 test scenarios, which is over 5 times that of RGB. Using MedRGB, we evaluation 7 LLMs, including both state-of-the art commercial LLMs and open-source models. In summary, our contributions are three-fold: • We establish MedRGB with four test scenarios to evaluate LLMs for medical QA tasks in RAG settings. To best of our knowledge, it is the ﬁrst benchmark comprehensively assess medical RAG systems in these practical setting. • Using MedRGB, we extensively evaluate 7 LLMs, including both state-of-the art commercial LLMs and open-source models, across multiple RAG conditions. Experiment results demonstrate their limitation in addressing the more complex scenarios. • We analyzed the errors of the LLMs and their reasoning process to provide insights and suggest future directions for developing more reliable and trustworthy medical RAG systems. Figure 2: The overall construction process of MedRGB. The green OpenAI symbol implies that the block involves data generation using the GPT-4o model."
https://arxiv.org/html/2411.08700v1,Rethinking negative sampling in content-based news recommendation,"News recommender systems are hindered by the brief lifespan of articles, as they undergo rapid relevance decay. Recent studies have demonstrated the potential of content-based neural techniques in tackling this problem. However, these models often involve complex neural architectures and often lack consideration for negative examples. In this study, we posit that the careful sampling of negative examples has a big impact on the model’s outcome. We devise a negative sampling technique that not only improves the accuracy of the model but also facilitates the decentralization of the recommendation system. The experimental results obtained using the MIND dataset demonstrate that the accuracy of the method under consideration can compete with that of State-of-the-Art models. The utilization of the sampling technique is essential in reducing model complexity and accelerating the training process, while maintaining a high level of accuracy. Finally, we discuss how decentralized models can help improve privacy and scalability.","The large amount of available news sources and articles, coupled with very fast update cycles, creates an atmosphere of information overload that makes it hard for readers to keep track of news that are most relevant to them bib2 . The power of personalized news retrieval can be extremely helpful in improving the users’ overall satisfaction with the service. By carefully selecting items to users – in this case, news articles – Recommender Systems (RS) bring the most relevant items to the attention of users. Recommender systems for news typically rely on user click data that consists of the positive interactions between users and news items. Because machine learning algorithms struggle to learn from positive data without a negative counterpart – i.e. disliked or otherwise irrelevant items –, it is common practice to randomly sample negative examples to balance the learning data. In this paper, a negative sampling technique is proposed, that fuels a Decentralized Neural News Recommendation system (DNNR) by providing better implicit negative examples for the model to train on and learn user patterns. News Recommender Systems (NRS) have certain characteristics related to their business model that are not often, or at all, observed in other domains. The key difference is the speed at which the relevance of the items decay. Unlike item recommendation in music, movies, or the retail market, for example, the relevance of news articles can change very rapidly concomitant with daily happenings and events karimiNews2018 . This leads to a permanent item cold-start problem, since recent news items to recommend have few interactions. Fortunately, news are content-rich, and recent advances in natural language processing (NLP) provide excellent tools to extract rich and compact representations directly from natural text. These content-based representations can compensate for the scarcity of interactions of new items. Another relevant aspect of our work consists of the decentralized nature of our proposed method. We have now arrived in an information-centric age, where computing power is unevenly distributed between provider infrastructure and user devices, where most data is generated DLedge . Centralized computing power, where most computation involving the training of RS is done, need to efficiently manage and process these large quantities of data, produced in a widely distributed system, which raises some issues: • Cost: To train models and do inference on centralized computing power requires the transmission of massive amounts of data; • Latency: the delay to access the provider’s computing infrastructure power and storage is generally not guaranteed, and might restrain some solutions that are more time-critical. • Privacy: training models requires a lot of private information to be carried, raising privacy issues. Organizations with large amounts of user data heightens the risk of illegitimate data use or hazardous private data leaks. Under these circumstances, on-device or edge computing offers advantages by hosting some computation tasks close to the data sources and end users. When combined with centralized computing it can: alleviate backbone network, by handling key computation tasks without exchanging data with the central computing cluster; and allowing for more agile service response, by reducing or removing the delay of data transmissions DLedge . It also has the potential to provide better privacy guarantees, while simultaneously granting users a finer control over processes involving their personal data. Our approach is to train a different model for each user, which makes it trivial to decentralize. This has the potential to offload computation to the user realm, which besides reducing resource requirements from the provider’s side, also improves privacy and user autonomy – e.g. by allowing users to choose and fine-tune models to their needs. Summarizing, we our contributions are the following: • We introduce a personalized negative sampling technique for text-based recommendation, considerably improving model accuracy with respect to the state of the art; • We propose a decentralized training strategy based on the idea of training a separate lightweight neural recommender for each user. • We study the trade-offs between the negative sample size and predictive performance, as well as training and prediction times. We organized the paper as follows. In Section 2, we provide a review of related work."
https://arxiv.org/html/2411.08275v1,"A Large-Scale Study of Relevance Assessments with
Large Language Models: An Initial Look","The application of large language models to provide relevance assessments presents exciting opportunities to advance information retrieval, natural language processing, and beyond, but to date many unknowns remain. This paper reports on the results of a large-scale evaluation (the TREC 2024 RAG Track) where four different relevance assessment approaches were deployed in situ: the “standard” fully manual process that NIST has implemented for decades and three different alternatives that take advantage of LLMs to different extents using the open-source UMBRELA tool. This setup allows us to correlate system rankings induced by the different approaches to characterize tradeoffs between cost and quality. We find that in terms of nDCG@20, nDCG@100, and Recall@100, system rankings induced by automatically generated relevance assessments from UMBRELA correlate highly with those induced by fully manual assessments across a diverse set of 77 runs from 19 teams. Our results suggest that automatically generated UMBRELA judgments can replace fully manual judgments to accurately capture run-level effectiveness. Surprisingly, we find that LLM assistance does not appear to increase correlation with fully manual assessments, suggesting that costs associated with human-in-the-loop processes do not bring obvious tangible benefits. Overall, human assessors appear to be stricter than UMBRELA in applying relevance criteria. Our work validates the use of LLMs in academic TREC-style evaluations and provides the foundation for future studies.","Relevance assessments are critical for evaluating information access systems, providing guidance for model training in information retrieval (IR), natural language processing (NLP), and beyond. Acquiring relevance assessments from humans, of course, is an expensive proposition, not only in terms of compensation for the assessors, but also overhead in articulating clear guidelines, coordinating large-scale annotation efforts, ensuring consistent quality, etc. The advent of large language models (LLMs) provides an opportunity to automate these assessments, potentially accelerating improvements in retrieval methods (Faggioli et al., 2023)—although see Soboroff (2024) for a contrary position. Most recently, Thomas et al. (2024) revealed that LLMs have been used to provide relevance assessments at Bing since late 2022. While the authors quite clearly assert that LLMs can accurately predict searcher preferences in the context of Bing, they began from the industry perspective and retrospectively “backported” their proposed fully automatic technique to an old TREC collection for validation. In this work, we adopt a complementary strategy, exploring different applications of LLMs to generate relevance assessments in situ, directly deployed in a large-scale retrieval-augmented generation (RAG) evaluation organized by the National Institute of Standards and Technology (NIST) as part of the Text Retrieval Conference (TREC) series. Now in its 33rd year, TREC is widely acknowledged as representing the “gold standard” in academic IR evaluations. We examined three different evaluation approaches that vary in levels of LLM involvement, which are compared against fully manual relevance assessments. Our study used the UMBRELA tool (Upadhyay et al., 2024), which has been previously validated as a successful reproduction of Thomas et al. (2024). Given this setup, we explored the following research questions: RQ1 To what extent can automatically generated relevance assessments from LLMs replace NIST assessors? Specifically, we examined three scenarios involving different levels of LLM involvement: fully automatic, manual post-editing, and manual filtering. RQ2 Different levels of LLM involvement lead to different tradeoffs between cost and quality in relevance assessments. Can we characterize these tradeoffs? RQ3 Are there any quantitative or qualitative differences between human versus LLM judgments? Are these differences affected by different levels of LLM involvement? In this work, we analyzed a diverse set of 77 runs from 19 teams that contributed to the TREC 2024 RAG Track. Our findings can be summarized as follows: • For RQ1, we find that system rankings induced by automatically generated relevance assessments using UMBRELA correlate highly with those induced by manual assessments in terms of nDCG@20, nDCG@100, and Recall@100. • For RQ2, we find that, surprisingly, LLM assistance does not appear to increase correlation with fully manual assessments. Thus, the additional costs associated with hybrid human–LLM solutions do not appear to have obvious tangible benefits. • For RQ3, analyses suggest that assessors apply stricter relevance criteria than UMBRELA. We find cases where the LLM draws inferences that humans would consider unwarranted, and vice versa, but overall, the first case is more common. The contribution of this work is, to our knowledge, the first large-scale study of different in situ approaches to using automatically generated LLM assessments, where system rankings induced by these approaches are correlated against reference rankings generated by fully manual assessments. This represents a first step outside the industry setting to validate methods for automatically generating relevance assessments using LLMs. In this context, our work contributes to a long tradition of meta-evaluation in the information retrieval literature, evaluations that evaluate evaluation methodologies."
https://arxiv.org/html/2411.08696v1,Scholarly Wikidata: Population and Exploration of Conference Data in Wikidata using LLMs,"Several initiatives have been undertaken to conceptually model the domain of scholarly data using ontologies and to create respective Knowledge Graphs. Yet, the full potential seems unleashed, as automated means for automatic population of said ontologies are lacking, and respective initiatives from the Semantic Web community are not necessarily connected: we propose to make scholarly data more sustainably accessible by leveraging Wikidata’s infrastructure and automating its population in a sustainable manner through LLMs by tapping into unstructured sources like conference Web sites and proceedings texts as well as already existing structured conference datasets. While an initial analysis shows that Semantic Web conferences are only minimally represented in Wikidata, we argue that our methodology can help to populate, evolve and maintain scholarly data as a community within Wikidata. Our main contributions include (a) an analysis of ontologies for representing scholarly data to identify gaps and relevant entities/properties in Wikidata, (b) semi-automated extraction – requiring (minimal) manual validation – of conference metadata (e.g., acceptance rates, organizer roles, programme committee members, best paper awards, keynotes, and sponsors) from websites and proceedings texts using LLMs. Finally, we discuss (c) extensions to visualization tools in the Wikidata context for data exploration of the generated scholarly data. Our study focuses on data from 105 Semantic Web-related conferences and extends/adds more than 6000 entities in Wikidata. It is important to note that the method can be more generally applicable beyond Semantic Web-related conferences for enhancing Wikidata’s utility as a comprehensive scholarly resource. Source Repository: https://github.com/scholarly-wikidata/ DOI: https://doi.org/10.5281/zenodo.10989709 License: Creative Commons CC0 (Data), MIT (Code)","Scientific conferences are vital for researchers to share their research findings and advancements. It offers an opportunity to discuss research problems or limitations, a platform for networking with peers, and a platform for promoting collaboration, which is essential for learning, innovation, and problem-solving. Because of the importance of scientific conferences, we have seen tremendous growth in the number of conferences over the years [1]. For example, IEEE (Institute of Electrical and Electronics Engineers) sponsors more than 2,000111https://www.ieee.org/about/at-a-glance.html conferences and events annually. Similarly, ACM (Association for Computing Machinery) hosts more than 170222https://www.acm.org/conferences/about-conferences conferences annually worldwide. Therefore, efforts have been made to capture metadata about scientific events [2, 3, 1, 4] in a linked-data format as they provide valuable information. Such data can be used for (i) better understanding the progress of science overall, (ii) the evolution of particular research topics (or fields), (iii) understanding research impact (e.g. by sponsors’ interest) over time, etc. The availability of scholarly metadata enables scientometrics [5], or practical tools such as recommending relevant conferences or papers to readers [2] for navigating through the fastly growing scientific output which is becoming time-consuming and almost impractical. However, as much as the benefits these metadata about scientific events provide, there exist challenges. The primary obstacle is the collection of large-scale metadata, which is nontrivial in nature [2]. Similarly, the sustainability, which is also the focus of this paper, of the accumulated metadata constitutes the second and most significant obstacle. If the data collected is not sustainable, it may be lost over time, resulting in the loss of valuable information and efforts put into data collection. For instance, the Microsoft Academic Graph, which contained over 8 billion triples [2] with information about scientific publications and related data, was retired in December 2021333https://www.microsoft.com/en-us/research/project/microsoft-academic-graph/. While the effort was somewhat continued shortly later in OpenAlex444https://openalex.org/, the case demonstrates sustainability issues in individual or commercial scholarly KG offerings. We argue that collaborative, general purpuse, community-driven platforms, such as Wikipedia, are generally more sustainable than such fragmented efforts: community participation is motivated by intrinsic factors, fostering a sense of belonging to the group [6]. Notably, commercial initiaves seem to recognize this, as shown by Google’s declaration that it will cease operations on Freebase and transfer its content to Wikidata [7]. Wikidata, which focuses on knowledge graphs (KGs), is a sister project of Wikipedia and another example of a community-driven platform [8, 9]. Wikidata currently has more 110M entities and 25K active contributors555https://www.wikidata.org/wiki/Wikidata:Statistics. By bringing Scholarly data about scientific conferences into Wikidata, they can be seamlessly integrated with existing background knowledge through SPARQL queries. Furthermore, Wikidata benefits from a robust tooling ecosystem and widely used libraries, including entity linkers, search tools, SPARQL endpoint with high-availability, easy-to-use query editor, visualization tools, and more [10, 11]. Wikidata also allows non-expert users to directly access the KGs through search and Web UI (user interface). Therefore, the primary objective of our work is to integrate scientific conference metadata into Wikidata, a community-led platform. After conducting an analysis of Wikidata entities related to Semantic Web conferences such as International Semantic Web Conference (ISWC), Extended/European Semantic Web Conference (ESWC), International Conference Knowledge Engineering and Knowledge Management (EKAW), International Conference on Knowledge Capture (K-CAP), SEMANTiCS, and Knowledge Graph and Semantic Web Conference (KGSWC), it was noticed that some conferences were missing and the ones that were present had only minimal information. In this project, we have extended Wikidata to include a more comprehensive set of information (e.g. see ISWC 2023666https://www.wikidata.org/wiki/Q119153957 (Q119153957)). Within the scope of this work, we focused on the Semantic Web conferences but our method is more generally applicable and can be extended to other conference series. We note that 105 conferences we added to, updated in Wikidata is higher than the comparable related work such as Scholarly Data (35 confs)777https://bit.ly/3Vs6XNc, ORKG (5 confs) 888https://orkg.org/organizations/Event as of July, 2024. Large language models (LLMs) have proven their language understanding capabilities with many NLP benchmarks [12]. In recent years, approaches such as in-context learning with a few-shot example have allowed them to perform many tasks such as relation or fact extraction [13, 14]. Such models can be used to easily extract information from sources with natural language text, such as conference proceedings, websites, or call for papers. Nevertheless, their output can be prone to errors. In our work, LLMs are used to extract data, which is then verified by a human-in-the-loop validation to eliminate any noisy extraction and ensure accuracy. In particular, this paper makes the following contributions. • We analysed existing ontologies for representing scholarly data and mapped them to Wikidata to identify relevant Wikidata entities/properties as well as gaps. • We present a method for utilizing large language models to efficiently extract conference metadata from various sources, curating them through a human-in-the-loop validation process using OpenRefine, and populating the data in Wikidata via Wikidata QuickStatements and provide an evaluation for LLM-based extractions. • As a result of this project, we have extended over 1000 existing entities and created more than 5,000 new entities, including conferences, scientific articles, and people. These entities are now available on Wikidata and can be accessed via the Web UI or SPARQL endpoint. • We extend visualization tools Scholia999https://scholia.toolforge.org/ and Synia101010https://synia.toolforge.org to better visualize the information we added to Wikidata."
https://arxiv.org/html/2411.08334v1,Enhancing Multimodal Query Representation via Visual Dialogues for End-to-End Knowledge Retrieval,"Existing multimodal retrieval systems often rely on disjointed models for image comprehension, such as object detectors and caption generators, leading to cumbersome implementations and training processes. To overcome this limitation, we propose an end-to-end retrieval system, Ret-XKnow, to endow a text retriever with the ability to understand multimodal queries via dynamic modality interaction. Ret-XKnow leverages a partial convolution mechanism to focus on visual information relevant to the given textual query, thereby enhancing multimodal query representations. To effectively learn multimodal interaction, we also introduce the Visual Dialogue-to-Retrieval (ViD2R) dataset automatically constructed from visual dialogue datasets. Our dataset construction process ensures that the dialogues are transformed into suitable information retrieval tasks using a text retriever. We demonstrate that our approach not only significantly improves retrieval performance in zero-shot settings but also achieves substantial improvements in fine-tuning scenarios. Our code is publicly available: https://github.com/yeongjoonJu/Ret˙XKnow.","With the growing demand for information retrieval across diverse applications, such as internet search and knowledge-based question answering, precise and efficient retrieval from multimodal queries involving pairs of images and text has emerged as a critical challenge. In such multimodal queries, each modality independently provides insufficient information to retrieve the desired passages within a knowledge base, necessitating the integrated understanding of the visual and textual queries. Existing Vision-Language (VL) retrievers (Qu et al. 2021; Luo et al. 2021; Gao et al. 2022; Lin et al. 2023) often depend on disjointed models for object detection or image captioning to provide visual information. The reliance on disjointed models complicates the training process (e.g., the models should be fine-tuned for separate tasks in domain adaptation) and increases the likelihood of propagating erroneous predictions. The utilization of the captioning model also lacks the fine-grained information embedded within the images. Previous approaches (Lin et al. 2023; Luo et al. 2023) have attempted to address these drawbacks. However, as shown in Fig. 1, they result in lower performance in a zero-shot setting than a text retriever that does not use image information despite their pre-training for the image-text alignment. Lin et al. (2023) introduce token-level embeddings and utilize two types of visual representations: textual description of the image and feature-based visual embeddings with regions of interest by an object detector. They pre-train the retriever to map token-level visual embeddings into the linguistic space of a text retriever and then fine-tune it by adding image captions to the textual queries. Such the retriever captures fine-grained features of the image by employing visual embeddings with captions. They also facilitate modality interaction between the textual query and the image by relying on textual information, but the mechanism also results in complex implementations and inefficient retrieval due to multiple steps. Figure 1: Zero-shot retrieval performance on OK-VQA (Google Search). Ret-XKnow outperforms the text-based retriever, while other multimodal retrievers fall short, relying on the textual query in the pre-training stage. Luo et al. (2023) present an end-to-end approach that projects multimodal features encoded via self-attention into linguistic space with a pre-training task called VL-ICT, to detach the dependency on the disjointed modules. They automatically construct a pre-training dataset by applying the Inverse Cloze Task (ICT) (Lee, Chang, and Toutanova 2019) to a multimodal knowledge base. However, this approach has significant limitations. First, the dataset does not adequately reflect the variety and complexity of real-world queries, as it only removes the title or caption from a sentence extracted as the query without considering the image. Second, in the constructed pairs of a multimodal query and the corresponding passage, the passage can often be matched solely with the textual content of the query. This occurs because the target passage is selected from the content following a sentence with a title or caption, thereby hindering learning rich image representations. To tackle these issues, we propose two approaches: (1) an end-to-end Retriever to eXpand visual Knowledge, Ret-XKnow, and (2) a Visual Dialogue-to-Retrieval (ViD2R) pre-training dataset constructed from visual dialogues containing distinct relevant passages for various queries related to the same image. Ret-XKnow endows a text retriever with the understanding of multimodal queries in the context of efficient information retrieval, inspired by the concept of partial convolutions (Liu et al. 2018), which fill undesired pixels with surrounding pixel information. We compress visual embeddings to focus on the visual information relevant to the textual query by leveraging the relevance scores between visual embeddings and textual query representations as an adaptive mask. We only attach a vision encoder to the text retriever with only a few layers, utilizing output embeddings of the penultimate layer in the vision model for fine-grained visual representations. Our model architecture does not allow the direct intervention of textual query features in the pre-training stage, achieving modality interaction without fusing text features with image features. Through this architecture, we introduce both the late-interaction mechanism (Khattab and Zaharia 2020) for pre-indexing documents and the modality interaction without requiring an additional document encoder and disjointed models. Recent advances in multimodal language models have produced several multimodal dialogue datasets (Zhu et al. 2023; Liu et al. 2023; Wang et al. 2023; Huang et al. 2023) for training models to perform tasks based on visual content. These datasets consist of multi-turn sessions with query-response pairs centered around a single image, providing precise and comprehensive information pertinent to the query and image. The response with detailed information can improve multimodal retrieval tasks by linking image understanding with complex textual queries. Whereas, such datasets are not appropriate for directly training retrievers due to the gap between explicit responses and broader passages. To bridge this gap, we transform the visual dialogue datasets into a format suitable for retrieval tasks through three simple steps: pre-processing, neural filtering, and response-to-passage conversion. Our construction process is applicable in diverse domains and modalities since our approach only requires multimodal dialogue datasets and sets of documents related to the target domain. Our retriever, Ret-XKnow pre-trained with the ViD2R dataset, outperforms various baselines in zero-shot retrieval performance across four multimodal datasets in an end-to-end manner. Furthermore, we demonstrate that the pre-training dataset curated via our construction method effectively mitigates the issue of overlooking visual features during the pre-training stage, leading to remarkable performance in fine-tuning settings. Our contributions are summarized as follows: • We propose Ret-XKnow, an end-to-end multimodal retriever that overcomes the limitations of disjointed models by dynamically focusing on visual features relevant to the textual query. • We introduce the ViD2R dataset, which transforms visual dialogue datasets into a format suitable for training VL retrievers, leading to significant improvements in zero-shot retrieval performance. • We demonstrate the comprehensive adaptability of Ret-XKnow by fine-tuning three downstream tasks. Our end-to-end retriever even shows comparable performance on baseline methods utilizing image captioning."
https://arxiv.org/html/2411.07658v1,"Advancing Sustainability via 
Recommender Systems: A Survey","Human behavioral patterns and consumption para- digms have emerged as pivotal determinants in environmental degradation and climate change, with quotidian decisions pertaining to transportation, energy utilization, and resource consumption collectively precipitating substantial ecological impacts. Recommender systems, which generate personalized suggestions based on user preferences and historical interaction data, exert considerable influence on individual behavioral trajectories. However, conventional recommender systems predominantly optimize for user engagement and economic metrics, inadvertently neglecting the environmental and societal ramifications of their recommendations, potentially catalyzing overconsumption and reinforcing unsustainable behavioral patterns. Given their instrumental role in shaping user decisions, there exists an imperative need for sustainable recommender systems that incorporate sustainability principles to foster eco-conscious and socially responsible choices. This comprehensive survey addresses this critical research gap by presenting a systematic analysis of sustainable recommender systems. As these systems can simultaneously advance multiple sustainability objectives—including resource conservation, sustainable consumer behavior, and social impact enhancement—examining their implementations across distinct application domains provides a more rigorous analytical framework. Through a methodological analysis of domain-specific implementations encompassing transportation, food, buildings, and auxiliary sectors, we can better elucidate how these systems holistically advance sustainability objectives while addressing sector-specific constraints and opportunities. Moreover, we delineate future research directions for evolving recommender systems beyond sustainability advocacy toward fostering environmental resilience and social consciousness in society.","Human activities encompass the consumption of myriad non-renewable resources (e.g., coal, gas, fossil fuels) and natural materials, while concomitantly inflicting environmental degradation through various mechanisms, including atmospheric pollution, carbon dioxide (\textrm{CO}_{\textrm{2}}) emissions, and waste generation [1, 2, 3]. The cumulative global emissions of \textrm{CO}_{\textrm{2}} have exhibited a consistent linear trajectory over time, as illustrated in Fig. 1 (left). According to the U.S. Energy Information Administration (EIA), the United States contributed approximately 5.12 billion metric tons (BMT) of \textrm{CO}_{\textrm{2}} emissions in 2021 [4, 5]. Of this total, a substantial 92 percent, or 4.6 BMT, was directly attributable to the combustion of fossil fuels for energy generation. Moreover, recent years have witnessed a precipitous advancement in generative artificial intelligence (AI), which necessitates the utilization of large-scale datasets for training expansive language models [6]. A comparative analysis of the 2024 environmental sustainability reports from Microsoft and Google reveals a significant increase in their carbon footprints since 2020 (Fig. 1 (right)). Microsoft’s emissions have risen by 29.4%, while Google’s have surged by 66.3%. The primary driver of this growth is the expansion of their data center infrastructure, specifically designed and optimized to accommodate the escalating computational demands of artificial intelligence workloads. It is incontrovertible that the magnitude of carbon emissions continues to escalate, making substantial contributions to anthropogenic climate change [7, 8, 9]. Figure 1: Global \textrm{CO}_{\textrm{2}} emissions trends (1960-2023) and a comparative analysis of Microsoft and Google’s carbon footprints. To align with the United Nations Sustainable Development Goals (SDGs) [10] and adhere to the stipulations of the Paris Agreement aimed at mitigating climate change to below 1.5°C by the mid-2030s, a diverse array of artificial intelligence technologies has been deployed [11]. As the global community becomes increasingly aware of the importance of sustainable practices to address environmental challenges, there is a growing consensus that recommender systems (RSs) can play a crucial role in facilitating sustainable human behaviors [12, 13]. These systems propose alternatives that endorse environmentally sustainable products, encourage eco-conscious travel options, and promote energy-efficient living arrangements within architectural structures. Empirical research has demonstrated that recommender systems tailored for green products not only contribute to the diminution of energy usage and the reduction of greenhouse gas emissions [14] but also foster a paradigm of sustainable consumption among users. RSs are AI models designed to predict user preferences and suggest relevant items or content. These systems have become ubiquitous in various domains, including e-commerce, entertainment, social media, and more [15, 16, 17, 18]. The primary goal of RSs is to enhance user experience by providing personalized recommendations tailored to individual preferences and behaviors [19, 20]. Given the significant impact of RSs on daily human interactions with digital platforms, these systems have the potential to contribute substantially to environmental and social sustainability: i). Waste reduction: By suggesting products or content that align more closely with user preferences, RSs can potentially mitigate waste from unwanted purchases or unused items. ii). Energy efficiency optimization: In sectors such as energy management, RSs can propose optimal energy usage patterns, potentially promoting conservation and reducing carbon footprints. iii). Promotion of sustainable consumption: These systems can be engineered to prioritize environmentally friendly products or services, potentially encouraging more sustainable consumer behavior. iv). Enhancement of social well-being: Through the recommendation of educational content, health-related information, or community activities, these systems may contribute to social development and individual growth. While several reviews have examined sustainable recommendation systems in specific domains, such as energy-efficient building practices [21], eco-friendly travel routes [22], sustainable e-tourism [23], and Sustainable Development Goals (SDGs) perspectives [12], this survey offers a more comprehensive and integrated analysis. The present study provides a holistic view of sustainable recommendation systems across multiple domains, including health-conscious food choices, energy-efficient building management, and environmentally friendly travel solutions, while also examining the underlying computational strategies employed in these systems. By synthesizing a diverse body of research and emphasizing the critical need to incorporate environmental sustainability into system designs, this review aims to enhance the understanding of sustainable recommender systems and stimulate future research that encompasses various aspects of sustainability. Our primary contributions are as follows: • We offer an in-depth examination of the implementation and research trajectories of sustainable recommendation systems in pivotal sectors, including travel, food, and built environment management, coupled with insights into algorithmic optimization. • We present a generic architectural framework for sustainable recommender systems, serving as a foundation for organizing and contextualizing existing research. • We make a substantial contribution to the corpus of research on sustainable recommendation systems by elucidating key challenges within the domain and proposing future research avenues. It establishes a crucial framework for advancing the study and application of sustainability principles in recommendation systems across heterogeneous industries. The subsequent sections of this paper are structured as follows: Section II provides the necessary background for understanding the subsequent review. Section III describes various work in sustainable travel recommendation, covering POI recommendation, route recommendation, and transportation recommendation. Section IV delves into sustainable recommendation practices within the food industry, focusing on health-conscious and environmentally friendly food recommendations. Section V discusses the sustainable building recommendation, ranging from residential to commercial and large-scale buildings. Section VI expands to a wider discussion on the broader applications of sustainable recommendations, which includes environmental and ecological sustainability, behavior and social change, economic and productive sustainability, and user-centric sustainable recommenders. Section VII pivots to the sustainable design of recommendation models, particularly through algorithmic advancements and computational efficiency. Section VIII highlights ongoing challenges and emerging research areas within the field. Finally, Section IX concludes the paper, synthesizing key findings and delineating implications for future research endeavors. (§III)(§III-A)(§III-B)(§III-C)(§IV)(§IV-A)(§IV-B)(§V)(§V-A)(§V-B)(§VI)(§VI-A)(§VI-B)(§VI-C)(§VI-D)(§VII)(§VII-A)(§VII-B)(§VII-C)(§VII-D) Figure 2: Hierarchical taxonomy of recommender system categories and their applications in advancing sustainability initiatives."
https://arxiv.org/html/2411.07589v1,Overhead-free User-side Recommender Systems,"Traditionally, recommendation algorithms have been designed for service developers. But recently, a new paradigm called user-side recommender systems has been proposed. User-side recommender systems are built and used by end users, in sharp contrast to traditional provider-side recommender systems. Even if the official recommender system offered by the provider is not fair, end users can create and enjoy their own user-side recommender systems by themselves. Although the concept of user-side recommender systems is attractive, the problem is they require tremendous communication costs between the user and the official system. Even the most efficient user-side recommender systems require about 5\times more costs than provider-side recommender systems. Such high costs hinder the adoption of user-side recommender systems. In this paper, we propose overhead-free user-side recommender systems, RecCycle, which realizes user-side recommender systems without any communication overhead. The main idea of RecCycle is to recycle past recommendation results offered by the provider’s recommender systems. The ingredients of RecCycle can be retrieved “for free,” and it greatly reduces the cost of user-side recommendations. In the experiments, we confirm that RecCycle performs as well as state-of-the-art user-side recommendation algorithms while RecCycle reduces costs significantly.","Recommender systems have been used in many web services (Linden et al., 2003; Geyik et al., 2019). It was estimated that 35 % of purchases on Amazon and 75 % of watches on Netflix came from recommender systems (MacKenzie et al., 2013). Recommender systems are indispensable both for businesses and users. Although traditional recommender systems aim only at conversion, many fine-grained demands for recommender systems have emerged. Users may want to receive fair recommendations (Kamishima et al., 2012a; Biega et al., 2018; Milano et al., 2020) or serendipitous recommendations (Chen et al., 2021; Anderson et al., 2020; Steck, 2018; Mladenov et al., 2020; Zheng et al., 2018), or users may want recommender systems to be transparent (Sinha and Swearingen, 2002; Balog et al., 2019) and steerable (Green et al., 2009; Balog et al., 2019). For example, on LinkedIn, recruiters may want to receive account recommendations that are fair in terms of gender and race to avoid (implicit) discrimination. A citizen who gathers information for election may want to receive both Republican and Democrat news equitably to avoid filter bubbles (Pariser, 2011). Cinema enthusiasts may want to receive recommendations that involve minor movies instead of popular movies that enthusiasts already know. However, there are too many kinds of demands, and the service provider cannot cope with all of them. Besides, service provider may not implement such functionalities on purpuse. For example, some service providers may intentionally choose to increase short-term conversions instead of caring the fairness of the platform. If the service provider does not implement fair recommender systems, users are forced to use unfair ones or quit the service. It has been considered that users have little ability to change the recommendations. In most cases, the only option available to the user is to wait until the service implements the functionality. Green et al. (2009) also pointed out that “If users are unsatisfied with the recommendations generated by a particular system, often their only way to change how recommendations are generated in the future is to provide thumbs-up or thumbs-down ratings to the system.” User-side recommender systems (Sato, 2022b) offer a proactive solution to this problem. Users can build their own (i.e., private, personal, or user-side) recommender systems to ensure recommendations are made fairly and transparently. Since the system is built by the user, it can be customized to meet specific criteria they want and add the functionalities they want. User-side recommender systems realize ultimate personalization. Table 1. Properties of user-side recommender systems. The definitions of these properties are shown in Section 4.4. Postprocessing (PP) applies postprocessing directly to the official recommender system, which is not sound when the list does not contain some sensitive groups (See also Section 4.1). PP PrivateRank (Sato, 2022b) PrivateWalk (Sato, 2022b) ETP (Sato, 2022d) Consul (Sato, 2022d) RecCycle (ours) Consistent ✓ ✓ ✗ ✓ ✓ ✓ Sound ✗ ✓ ✓ ✓ ✓ ✓ Local ✓ ✗ ✓ ✗ ✓ ✓ Overhead-free ✓ ✗ ✗ ✗ ✗ ✓ The concept of user-side recommender systems looks similar to steerable (Green et al., 2009) (or scrutable (Balog et al., 2019)) recommender systems at first glance. Steerable recommender systems also allow users to control the recommendation results. However, the key difference is that steerable systems are implemented by the service provider, while user-side recommender systems are built by the users themselves. What to steel is chosen by the service provider in traditional steerable recommender systems. If the recommender system in use is not steerable in the way they want, users cannot enjoy steerability and must wait for the service provider to implement it. By contrast, user-side recommender systems allow users to make the system steerable, even if the service provider implemented only a standard non-steerable system. Although user-side recommender systems are attractive, building them is challenging. End users do not have access to the data stored in the service’s database, unlike the developers employed by the service provider. Most modern recommender systems rely on user log data and/or item features to make recommendations. At first glance, it seems impossible to build an effective recommender system without such data. Sato (2022b) addressed this problem by using the official recommender systems provided by the target web service. Although the official recommender systems are black-box and possibly unfair, Sato’s methods turn them into fair and transparent ones on the user’s side by combining multiple outputs. However, existing user-side recommender systems issue multiple queries to the official (possibly unfair) recommender system to build a single (fair) recommendation list. In other words, these methods trade communication costs with fairness. The drawback of this approach is the communication cost. Even the most efficient user-side recommender systems, Consul, require 5 queries to build a recommendation list (Sato, 2022d). This means that Consul loads the service 5 times more. Such a high communication cost causes problems. First, the service provider may disfavor and prohibit such users’ activities to mitigate the load on the service. Second, end users cannot afford to pay the high API cost. Third, such systems are not suitable for real-time applications due to the response time of the multiple queries. We advocate that the communication cost between the end user and the service is crucial for effective user-side recommender systems. An ideal user-side system works as if it were an official system. The recommendation list should be shown to the user at the same time as the official system. However, existing user-side recommender systems require additional queries and thus require more loading time than the official system, which leads to a poor user experience. We propose overhead-free user-side recommender systems, RecCycle (recommendation + recycle), to address this problem. The main idea of RecCycle is to recycle past recommendation results presented by the provider’s recommender systems when the user uses the system as usual. These recommendation results used to be discarded once shown on the page. Sometimes, these recommendations are just shown on the page and do not catch the attention of the user due to the position and/or timing of the presentation. RecCycle “recycles” these information to create new recommendations on the user’s side. These information can be used “for free”, i.e., without any additional communication cost. All of the computation for RecCycle is done locally. RecCycle is so communication efficient that it can realize real-time user-side recommendations, and the user can enjoy the recommendations as if they were shown by the official system. RecCycle can be combined with existing user-side recommender systems. We will elaborate on the premise of RecCycle in the following sections. As a special case, we show that RecCycle can be combined with Consul (Sato, 2022d), which leads to consistent, sound, local, and overhead-free user-side recommender systems (Table 1). In the experiments, we confirm that RecCycle performs as well as state-of-the-art user-side recommendation algorithms while RecCycle reduces costs significantly. The contributions of this study are as follows: • We propose overhead-free user-side recommender systems, RecCycle, for the first time. • We show that RecCycle is consistent, sound, and local, as well as overhead-free. • We empirically validate that RecCycle performs as well as state-of-the-art user-side recommendation algorithms while RecCycle reduces costs significantly. • We deploy RecCycle in a real-world X (Twitter) environment and confirm that users can realize their own recommender system with specified functionalities they call for using RecCycle."
https://arxiv.org/html/2411.07508v2,Feature Interaction Fusion Self-Distillation Network For CTR Prediction,"Click-Through Rate (CTR) prediction plays a vital role in recommender systems, online advertising, and search engines. Most of the current approaches model feature interactions through stacked or parallel structures, with some employing knowledge distillation for model compression. However, we observe some limitations with these approaches: (1) In parallel structure models, the explicit and implicit components are executed independently and simultaneously, which leads to insufficient information sharing within the feature set. (2) The introduction of knowledge distillation technology brings about the problems of complex teacher-student framework design and low knowledge transfer efficiency. (3) The dataset and the process of constructing high-order feature interactions contain significant noise, which limits the model’s effectiveness. To address these limitations, we propose FSDNet, a CTR prediction framework incorporating a plug-and-play fusion self-distillation module. Specifically, FSDNet forms connections between explicit and implicit feature interactions at each layer, enhancing the sharing of information between different features. The deepest fusion layer is then used as the teacher model, utilizing self-distillation to guide the training of shallow layers. Empirical evaluation across four benchmark datasets validates the framework’s efficacy and generalization capabilities. The code is available on https://anonymous.4open.science/r/FSDNet.","Click-Through Rate (CTR) prediction constitutes a fundamental component in recommender systems, online advertising, and search engines (Zhu et al., 2021, 2022; Duan et al., 2024). The primary objective of CTR prediction is to predict the probability of user interaction with presented items by analyzing user profiles, item attributes, and context. The significance of accurate CTR prediction is twofold: it enhances user experience through personalized recommendation alignment while optimizing resource allocation in product rankings and advertisement placements (Chen et al., 2021; Wang et al., 2021c). Central to achieving high prediction accuracy is the effective modeling of feature interactions (Zhu et al., 2021). Early approaches heavily relied on expert knowledge and models based on Logistic Regression (LR) (Richardson et al., 2007), Factorization Machines (FM) (Rendle, 2010; Juan et al., 2016; Blondel et al., 2016), which were limited to modeling low-order or fixed-order feature interactions. The advent of Deep Neural Networks (DNNs) has revolutionized the field by enabling implicit modeling of complex feature interactions (Wang et al., 2017; Covington et al., 2016), thereby reducing dependence on manual feature engineering. However, despite their theoretical status as universal function approximators (Hornik et al., 1989), DNNs exhibit limitations in accurately modeling certain fundamental operations, such as inner products (Rendle et al., 2020). Recent research has shifted focus toward integrating explicit interactions modeled by controlled interaction orders (Qu et al., 2016; Song et al., 2019) to address the limitations of implicit interactions. CTR models are primarily categorized into parallel (Guo et al., 2017; Wang et al., 2017; Lian et al., 2018; Wang et al., 2021c; Chen et al., 2021) and stacked structures (Qu et al., 2018; Li et al., 2019; Yu et al., 2021; Wang et al., 2021b), distinguished by their integration approach. As illustrated in Figure 1, parallel structure models process explicit and implicit components independently before fusion, while stacked structure models cascade components sequentially. Since the DNN part in the parallel structure can be seen as a complementary component, the parallel structure tend to have more stable training compared to the stacked structure (Zhang et al., 2021; Chen et al., 2021). The pursuit of accuracy has led to sophisticated ensemble structure models incorporating Self-Attention (Song et al., 2019; Chen and Shi, 2022), Mask Mechanisms (Wang et al., 2021b), and Contrastive Learning (Wang et al., 2023b). However, these advancements introduce a large number of parameters and complex structures, which significantly restricts their application on resource-constrained devices and in real-time tasks. To address these limitations, knowledge distillation techniques (Hinton, 2015; Deng et al., 2023; Tian et al., 2023) enable efficient model compression by transferring knowledge from complex teacher models to compact student architectures. Notable implementations include a multi-teacher ensemble framework with ”teacher gating” (Zhu et al., 2020), KD-DAGFM’s (Tian et al., 2023) directed acyclic graph approach for feature interaction distillation, and BKD’s (Deng et al., 2023) graph neural network-based methodology for interaction transfer. Despite the success, the aforementioned CTR models still face certain limitations: (a) Parallel Structure (b) Stacked Structure Figure 1. The architecture comparison among parallel and stacked structures. Lack of information sharing between different components. In parallel structure models, each component processes input data independently (Cheng et al., 2016; Guo et al., 2017; Wang et al., 2021c), such as DCNv2 (Wang et al., 2021c) independently performs cross network and deep network learning. Since there is no information sharing between hidden layers of different components, they independently learn latent representations and typically only fuse the results at the final layer. This delayed integration paradigm, where components establish isolated representational spaces, inherently constrains the depth and richness of feature information exchange. Moreover, this architectural isolation may induce gradient bias during backpropagation (Hu et al., 2019), potentially impeding convergence toward global optima. Complex design and low transfer efficiency in knowledge distillation. As depicted in Figure 2 (a), traditional knowledge distillation(Hinton, 2015) employs a smaller model (the student model) to imitate the output distributions of a more sophisticated model (the teacher model). However, studies(Mirzadeh et al., 2020; Gou et al., 2021) indicate that a model achieving excellent results is not always an effective teacher model. When multiple teacher models are needed for joint supervision(Zhu et al., 2020), the teacher-student framework necessitates intricate design considerations and computational overhead. Furthermore, the compression process of knowledge distillation inherently introduces information degradation (Gou et al., 2021), potentially resulting in suboptimal student model performance that may fall short of both the teacher model’s accuracy and that of directly trained counterparts. (a) Knowledge Distillation (b) Self distillation Figure 2. The structural comparison of knowledge distillation and self-distillation. Excessive noise in feature interactions. Users may click on certain items not out of genuine interest, but due to misoperation or other reasons (Wang et al., 2021a; Jurdi et al., 2021). These anomalous patterns propagate through the model, leading to the acquisition of spurious feature relationships. Furthermore, the number of feature interactions exhibits linear growth with increasing layer depth. Empirical analyses through hyperparametric studies (Lian et al., 2018; Qu et al., 2018; Wang et al., 2023a) demonstrate performance degradation beyond certain interaction orders, particularly exceeding third order. This phenomenon suggests the emergence of ineffective interactions, potentially amplifying the noise propagation throughout the learning process. In this paper, we propose a fusion self-distillation module to address the prevalent limitations discussed above. This is a lightweight, plug-and-play module that consists of two parts: feature interaction fusion and self-distillation. It can be broadly applicable to various parallel structured models. In addition, we propose a new CTR parallel structure framework based on DCNv2, called the Feature Interaction Fusion Self-Distillation Network (FSDNet). First, we establish connections between each layer outputs of the cross network and the deep network to resolve the problem of insufficient information sharing in the parallel structure. Then, we introduce self-distillation, which is grounded in knowledge distillation (Gou et al., 2021). As shown in Figure 2 (b), self-distillation does not use an additional teacher model. Instead, the deepest network is used as the teacher model. Specifically, we generate predictions by passing the fused feature information from each layer through a linear activation layer. The deepest fusion layer is regarded as the teacher model, with soft label loss and hint loss used to guide each of the previous shallower layer (i.e., the student models). This process helps stabilize the parallel structure and improve performance. In addition, the smoother soft labels contribute to the model’s generalization ability and robustness to noise. In summary, the key contributions of this paper are as follows: • We analyze three limitations of current parallel structure models: insufficient information sharing, low transfer efficiency in knowledge distillation, and excessive noise in feature interactions. To address these limitations, we propose a novel CTR framework, FSDNet. • We introduce a plug-and-play fusion self-distillation module composed of two parts: information fusion and self-distillation. The former establishes connections between explicit and implicit feature interactions, while the latter improves the framework’s accuracy and robustness via self-hinting. • We conduct extensive experiments across four datasets, demonstrating the effectiveness and robustness of the proposed FSDNet framework. Additionally, we validate the compatibility of the fusion self-distillation module across various parallel structure CTR models."
https://arxiv.org/html/2411.07504v1,"AdaS&S: a One-Shot Supernet Approach for Automatic Embedding Size Search in
Deep Recommender System","Deep Learning Recommendation Model(DLRM)s utilize the embedding layer to represent various categorical features. Traditional DLRMs adopt unified embedding size for all features, leading to suboptimal performance and redundant parameters. Thus, lots of Automatic Embedding size Search (AES) works focus on obtaining mixed embedding sizes with strong model performance. However, previous AES works can hardly address several challenges together: (1) The search results of embedding sizes are unstable; (2) Recommendation effect with AES results is unsatisfactory; (3) Memory cost of embeddings is uncontrollable. To address these challenges, we propose a novel one-shot AES framework called AdaS&S, in which a supernet encompassing various candidate embeddings is built and AES is performed as searching network architectures within it. Our framework contains two main stages: In the first stage, we decouple training parameters from searching embedding sizes, and propose the Adaptive Sampling method to yield a well-trained supernet, which further helps to produce stable AES results. In the second stage, to obtain embedding sizes that benefits the model effect, we design a reinforcement learning search process which utilizes the supernet trained previously. Meanwhile, to adapt searching to specific resource constraint, we introduce the resource competition penalty to balance the model effectiveness and memory cost of embeddings. We conduct extensive experiments on public datasets to show the superiority of AdaS&S. Our method could improve AUC by about 0.3% while saving about 20\% of model parameters. Empirical analysis also shows that the stability of searching results in AdaS&S significantly exceeds other methods.","1. INTRODUCTION Deep Learning Recommender Model(DLRM)s often take a large amount of categorical features as input, and utilize a embedding layer to convert them to low-dimensional embedding vectors (qu2018product, ). Consequently, the embedding layer plays an important role since it dominates the number of parameters as well as the effect of model prediction (autodim, ; yan2021learning, ). Traditional DLRMs generally assign a unified embedding size (abbreviated as “Emb-size” in this work). However, unified Emb-size ignores the heterogeneity of features and suffers from following issues: (1) Inferior performance: setting a unified Emb-size could impair the expressing of feature fields with large cardinality (number of unique feature values), while causing over-fitting for those with low cardinality. (2) Memory inefficiency: Unified Emb-size could cause redundant embedding parameters as well as unnecessary computation overhead, especially for features contribute less to final prediction. Therefore, different Emb-sizes is highly desired now and become a focal point in researches, especially for online recommendation where inference speed and memory cost become the bottleneck. We denote the task of obtaining different Emb-size for features as AES (Automatic Embedding size Search). Existing AES methods can be categorized as heuristic, pruning and NAS-based methods. Heuristic methods tend to allocate Emb-sizes with pre-defined rules. For example, MDE (ginart2021mixed, ) utilizes the popularity of features to determine the Emb-sizes, but its capability is limited by the degeneration of model performance (yan2021learning, ). As an alternative to heuristic methods, pruning methods (liu2021learnable, ; yan2021learning, ; qu2022single, ; kong2022autosrh, ) reduce Emb-sizes from a pre-defined largest dimension according to learnable threshold or mask parameters. For example, AutoSrh (kong2022autosrh, ) use soft selection weights to identify the importance of each dimension. However, the interaction of thresholds (masks) and embeddings undermines the stability of their searching outcomes, and the complex relation between dimensions is not well-studied. Recent NAS-based approaches (autodim, ; zhaok2021autoemb, ; liu2020automated, ; zhao2021ameir, ) view AES as searching candidate network architectures, they utilize NAS techniques to achieve better performance. Typically, AutoDim (autodim, ) introduces additional weights for embeddings of different Emb-sizes into the network, and optimize them with DARTS (ref18, ). However, parameter training and dimension searching in these works are usually coupled, leading to several weaknesses: (1) joint optimization of training and searching could introduce bias during gradient descent and mislead the AES to unstable results. (2) the number of embedding parameters is not guaranteed, raising the risk of resource costs for model deployment. To sum up, though recent pruning and NAS-based methods outperform traditional methods in model effectiveness, they still suffer from the instability of searched results, and it is also challenging to adapt to varying resource constraint (i.e.., memory cost of embedding parameters) while seeking for better Emb-sizes. In this paper, we propose a novel approach called AdaS&S (Adaptive Supernet and RL-Search) to address the aforementioned challenges. A large network containing all candidate embeddings is built (known as “supernet”), and Emb-sizes are searched as “subnet” architecture, i.e., choosing particular embedding tables. To be specific, (1) How to generate stable results? The decoupled training and searching eliminates the bias in joint optimization (guo2020single, ), helps to obtain stable outcomes. Meanwhile, a supernet with robust parameters could alleviate the uncertainty in searching. (2) How to obtain better performance? We propose the Adaptive Sampling to improve the consistency of the trained supernet, so that subnets inheriting weights from it could be quite predictive for stand-alone trained ones. A Reinforcement Learning (RL) based searching strategy is further designed to determine the optimal allocation. (3) How to adapt Emb-sizes to resource constraint during searching? We utilize a regularization of total resource cost to guide the RL agent, so that it produces controllable Emb-sizes. Our approach is experimented on several real-world datasets and shows significant advantages. Besides, the searching can repeat many times given various resource constraints once the supernet is trained, which is especially suitable for the dynamic environment of online recommendation. In summary, the main contribution of this paper are as below: • We firstly summarize three main goals for an ideal AES method: stable searching results, better recommendation performance with searched Emb-sizes, and being adaptive to specific resource constraint. We propose the AdaS&S framework to systematically fulfill all these targets. • Our AdaS&S method models AES by a one-shot two-stage supernet, which successfully decouples Emb-size searching from embedding training. Adaptive Sampling is proposed to perform the supernet training. Also, we design a Reinforcement Learning method to obtain the (near-)optimal Emb-size in the searching space. A regularization for total resource is introduced to guide the search result under resource constraints. • We conduct extensive experiments on several datasets to demonstrate the superiority of our approach over other competitive methods. Empirical results show that AdaS&S could successfully address mentioned challenges of AES."
https://arxiv.org/html/2411.07264v1,Multi-Document Financial Question Answering using LLMs,"We propose two new methods for multi-document financial question answering. First, a method that uses semantic tagging, and then, queries the index to get the context (RAG_SEM). And second, a Knowledge Graph (KG_RAG) based method that uses semantic tagging, and, retrieves knowledge graph triples from a graph database, as context. KG_RAG uses knowledge graphs constructed using a small model that is fine-tuned using knowledge distillation using a large teacher model. The data consists of 18 10K reports of Apple, Microsoft, Alphabet, NVIDIA, Amazon and Tesla for the years 2021, 2022 and 2023. The list of questions in the data consists of 111 complex questions including many esoteric questions that are difficult to answer and the answers are not completely obvious. As evaluation metrics, we use overall scores as well as segmented scores for measurement including the faithfulness, relevance, correctness, similarity, an LLM based overall score and the rouge scores as well as a similarity of embeddings. We find that both methods outperform plain RAG significantly. KG_RAG outperforms RAG_SEM in four out of nine metrics.","LLMs are now increasingly used for various linguistics tasks like information extraction [1] [2] [3], language understanding [4] [5] , named entity recognition [6] [7] [8], text summarization (including query based summarization) [9] [10] [11], translation [12] [13], speech synthesis [14], question answering [15] [16] [17] [18], code generation/understanding [19], and other computational linguistics tasks. In this work, we focus our attention on financial question answering. Specifically multi-document financial question answering. We use 10k reports from six companies for three years. This is different from asking questions about one document specific to a year, domain and organization, which is a significantly easier problem. Our method, which uses semantic tagging and knowledge graphs can scale to a very large corpus of documents including multiple publication dates, multiple domains, and multiple industries. Financial question answering using LLMs has been recently studied [20] [21] and many attempts have been successful and widely used. There are a few benchmark datasets for financial linguistics [22] [23] [16] [17] but most offer single document question answering. In this paper, we propose two new methods, RAG_SEM and KG_RAG for multi-document question answering. We perform experiments using 18 10k reports, spanning three years and 111 questions. Multi-document question answering has been studied recently (including methods based on knowledge graphs) [24] but the exploration of this subject has been limited. Knowledge graphs are very suitable for multi-document question answering because of the fine-grained semantics that they model. Works such as [10] could be explored for this purpose. Specialized models fine-tuned on large financial text corpuses have been shared for the general public to consume [25] [26] [21]. However, their use has been limited due to the risk of hallucinations, that increases for models trained using transfer learning [27]. We do not fine-tune models that generate answers, but we do fine-tune a KG generator, for speed and cost saving. In this work, we introduce two new methods for multi-document financial question answering. Multi-document question answering is difficult because of the fact that the context (RAG) is generated from all documents rather than a document specific to a domain, industry or organization. This makes it challenging as the synthesizer LLM may start giving wrong answers when asked a question about a specific entity. For example, if the question is ""How are companies’ revenues in the finance industry?"" might retrieve information about closely related topics, like economics, business, investing, insurance or lending. This may impact the overall effectiveness of the synthesized response. Figure 1: Segmented Comparison of the Three Methods of Financial Question Answering Figure 2: Comparison of the Three Methods of Financial Question Answering Table 1: Experiment Results (18 10K Reports and 111 Questions) Metric RAG RAG_SEM KG_RAG Relevance 80% 84% 85% Correctness 77% 81% 83% Faithfulness 77% 82% 83% Similarity 73% 78% 80% LLM Score 89% 92% 92% Embeddings 89% 92% 92% rouge1 52.9% 53.8% 53.8% rouge2 19% 19.5% 19.5% rougeL 23.2% 23.8% 23.8%"
https://arxiv.org/html/2411.07820v2,Expanding Horizons in Short Text Analysis: Integrating LLMs and VAEs for Enhanced Topic Modeling,"We introduce the Extract-Refine-Retrieve-Read (ERRR) framework, a novel approach designed to bridge the pre-retrieval information gap in Retrieval-Augmented Generation (RAG) systems through query optimization tailored to meet the specific knowledge requirements of Large Language Models (LLMs). Unlike conventional query optimization techniques used in RAG, the ERRR framework begins by extracting parametric knowledge from LLMs, followed by using a specialized query optimizer for refining these queries. This process ensures the retrieval of only the most pertinent information essential for generating accurate responses. Moreover, to enhance flexibility and reduce computational costs, we propose a trainable scheme for our pipeline that utilizes a smaller, tunable model as the query optimizer, which is refined through knowledge distillation from a larger teacher model. Our evaluations on various question-answering (QA) datasets and with different retrieval systems show that ERRR consistently outperforms existing baselines, proving to be a versatile and cost-effective module for improving the utility and accuracy of RAG systems.","In recent years, the field of natural language processing has witnessed remarkable advancements recently, largely attributable to the proliferation of Large Language Models (LLMs). These LLMs, characterized by their extensive scale training on vast corpora, have demonstrated unparalleled capability of understanding human text and producing high-quality responses kaplan2020scaling; clark2022unified. Moreover, they showcase exceptional practicality and scalability across a spectrum of downstream NLP tasks, such as conversational response generation, text summarization, or content recommendation, under only few-shot or zero-shot settings wu2023survey. However, despite their potential, a major limitation of these LLMs is their struggle to accurately capture the dynamic nature of information updates. Since LLMs are pre-trained on static corpora of data they face difficulties when dealing with the latest or less commonly known information that falls outside their initial training scope. This limitation leads to generating responses that contain outdated or incorrect information, or in some cases, entirely fictitious content— a problem often termed as ""hallucination"" lee2018hallucinations. In response to this pressing challenge, the concept of Retrieval-Augmented Generation (RAG) RAG has emerged as a promising technique to enhance the functionality and reliability of LLMs. Retrieval augmentation addresses the shortcomings of original LLMs by incorporating external knowledge (i.e., non-parametric knowledge) sources through some information retrieval system and leveraging the in-context learning ability of LLM readers to generate more contextually relevant and accurate responses. For instance, in a conversational setting where a user interacts with an LLM like ChatGPT gpt to inquire about the latest news events. ChatGPT, being reliant on static pre-training knowledge, may struggle to offer real-time updates in response to the user query. RAG mitigates this information gap by retrieving pertinent news articles or data related to the query from external databases. By augmenting the original user question with retrieved knowledge, the final LLM reader is more likely to generate a contextually relevant answer. While previous research has underscored the efficacy of retrieval augmentation in mitigating hallucination, it introduces its own set of challenges. One such critical challenge in the RAG systems is the existence of a gap between the information retrievable using the given retriever using original user input queries and the information required for generating optimal responses ragsurvey. Consider a scenario where an external document collection comprises three distinct passages labelled Passage A, B, and C, each containing unique knowledge components denoted as knowledge x, y, and z, respectively. Notably, all three passages include keywords related to Knowledge z, which the user seeks to obtain. However, if the user enters a poorly formulated query, there exists a possibility that Passage A or Passage B may be retrieved instead of the ideal source, Passage C, thereby limiting the LLM reader’s capacity to derive an optimal response. This disparity is referred to as the pre-retrieval gap in RAG and poses a fundamental obstacle to achieving optimal text generation outcomes. In the quest to bridge this pre-retrieval information gap ragsurvey within the existing RAG systems, the Rewrite-Retrieve-Read (RRR) framework rrr introduces a pivotal paradigm shift by incorporating the concept of query rewriting from the realm of traditional information retrieval into the RAG domain. RRR proposes the integration of a specialized Query Rewriter module into the native RAG system to align user queries with both the retriever and the LLM reader, thereby optimizing the retrieval process and narrowing the pre-retrieval gap between user queries and the information sought by the LLM reader, i.e. the LLM that generates the final responses. This query optimization technique has also seen widespread adoption in more recent RAG systems stepback; ragsurvey. However, while RRR demonstrates notable advancements, only the original query is passed to the rewrite module, which focuses more on rephrasing queries or expanding the search scope. This framework falls short in tailoring optimization strategies for queries based on the specific needs of the LLM reader, overlooking the need for retrieving the most relevant knowledge essential for accurate generation. To this end, we propose Extract-Refine-Retrieve-Read (ERRR), a simple but effective framework designed for retrieval augmentation systems. The ERRR framework is crafted to bridge the pre-retrieval information gap through tailored query optimization and aims to resolve the inherent limitations of RRR by enabling retrieval based on the specific information needs of the LLM reader. Specifically, it initiates by extracting parametric knowledge from LLMs and employs a specialized query optimizer that refines user queries. This refinement either complements or validates the extracted parametric knowledge, ensuring that only essential information is retrieved for generating accurate responses, and minimizing the retrieval of extraneous information that could degrade the quality of the output. Additionally, given that many advanced LLMs, such as ChatGPT gpt, function as black-box systems accessible only through inference APIs, our study introduces a trainable ERRR scheme to enhance customizability for different types of queries. This scheme incorporates a smaller, tunable language model as the query optimizer, which not only helps to lower the expenses associated with query optimization for parametric knowledge refinement but also allows for more adaptable customization tailored to each knowledge source. Our methods undergo thorough evaluation on multiple question-answering (QA) datasets (HotpotQA hotpotqa, AmbigNQ ambigqa, PopQA popqa). All frameworks and baselines evaluated are implemented using GPT-3.5-Turbo gpt3 or T5-Large t5 as the query optimizer, with GPT-3.5-Turbo serving as both the final reader and the source of parametric knowledge. Evaluation is conducted on the Brave Web Search Engine 111https://brave.com/search/api/ and a local dense retriever, Dense Passage Retrieval (DPR) DPR, alongside a static open-source Wikipedia corpus. Our evaluations demonstrate that ERRR consistently boosts the performance of retrieval-augmented LLMs across all tested QA datasets and retrieval systems when compared to the RRR framework. Furthermore, the outcomes highlight the adaptability and versatility of ERRR, showcasing its effectiveness across diverse settings and data sources. In summary, our key contributions are as follows: (i) We propose Extract-Refine-Retrieve-Read (ERRR), a retrieval augmentation framework designed to boost the effectiveness of RAG systems by optimizing queries to meet the specific knowledge needs of LLMs. (ii) We reveal that ERRR has a high degree of adaptability across diverse settings and data sources, as well as across various retrieval systems. (iii) We introduce a trainable scheme for ERRR, which not only achieves higher performance but also effectively reduces computational costs."
https://arxiv.org/html/2411.07739v1,"Unlocking Legal Knowledge with Multi-Layered Embedding-Based
Retrieval","This work addresses the challenge of capturing the complexities of legal knowledge by proposing a multi-layered embedding-based retrieval method for legal and legislative texts. Creating embeddings not only for individual articles but also for their components (paragraphs, clauses) and structural groupings (books, titles, chapters, etc), we seek to capture the subtleties of legal information through the use of dense vectors of embeddings, representing it at varying levels of granularity. Our method meets various information needs by allowing the Retrieval Augmented Generation system to provide accurate responses, whether for specific segments or entire sections, tailored to the user’s query. We explore the concepts of aboutness, semantic chunking, and inherent hierarchy within legal texts, arguing that this method enhances the legal information retrieval. Despite the focus being on Brazil’s legislative methods and the Brazilian Constitution, which follow a civil law tradition, our findings should in principle be applicable across different legal systems, including those adhering to common law traditions. Furthermore, the principles of the proposed method extend beyond the legal domain, offering valuable insights for organizing and retrieving information in any field characterized by information encoded in hierarchical text.","The increasing volume and complexity of legal corpora pose significant challenges for legal professionals, including those in legislative consultancy, where the efficient access and analysis of legal texts are critical. Traditional keyword-based search methods often fall short in capturing the nuances of legal language and the intricate relationships within legal documents ([1, 2]). Recent advancements in Generative Artificial Intelligence (GenAI) and Retrieval Augmented Generation (RAG) systems offer promising avenues for more efficient and accurate legal information retrieval. Embeddings, which are dense and compact vector representations of text, effectively capture the meanings of words, phrases, or documents ([3, 4]). Legislative documents, including bills and legal normative statutes, inherently have a hierarchical structure. This intrinsic hierarchy calls for an approach with variable granularity, capable of representing both smaller segments and broader groupings of legal texts through embeddings. This paper proposes a multi-layered embedding-based retrieval method that captures the semantic content of legal texts. By creating embeddings for articles,111In Brazilian law, ‘article’ is the fundamental unit within legislative texts, while in U.S. law, ‘Section’ within codes and statutes are the comparable unit of division that detail specific provisions of the law. In this work we will use the term ‘article’ as the basic unit of articulation, akin to a ‘section’ in U.S. legislation (see Figure 3). We refrain from using the term ‘section’ for the basic units since its literal translation in Portuguese (‘seção’) actually denotes a grouping of the basic units (‘artigos’). their components, and their structural groupings, we aim to provide a more nuanced and comprehensive representation of legal knowledge. Our proposed approach enables RAG models to respond to user queries with varying levels of detail, ranging from small portions to comprehensive sections."
https://arxiv.org/html/2411.07482v1,Enhancing Link Prediction with Fuzzy Graph Attention Networks and Dynamic Negative Sampling,"Link prediction is crucial for understanding complex networks but traditional Graph Neural Networks (GNNs) often rely on random negative sampling, leading to suboptimal performance. This paper introduces Fuzzy Graph Attention Networks (FGAT), a novel approach integrating fuzzy rough sets for dynamic negative sampling and enhanced node feature aggregation. Fuzzy Negative Sampling (FNS) systematically selects high-quality negative edges based on fuzzy similarities, improving training efficiency. FGAT layer incorporates fuzzy rough set principles, enabling robust and discriminative node representations. Experiments on two research collaboration networks demonstrate FGAT’s superior link prediction accuracy, outperforming state-of-the-art baselines by leveraging the power of fuzzy rough sets for effective negative sampling and node feature learning.","Link prediction has emerged as a crucial task in network analysis with extensive applications across diverse domains. In medical sciences, it aids in predicting protein-protein interactions and drug-target associations; in financial systems, it helps detect fraudulent transactions and assess credit risks; and in chemistry, it facilitates the discovery of novel molecular structures and chemical reactions. The ability to accurately predict potential connections in these complex networks has significant implications for scientific advancement and practical applications. Graph Neural Networks (GNNs) have demonstrated remarkable success in link prediction tasks, primarily due to their inherent capability to capture and process structural information in graph-structured data. However, a critical limitation in existing GNN-based approaches lies in their negative sampling methodology. Contemporary methods typically employ random sampling strategies to select negative edges, disregarding the rich semantic and structural information encoded in node representations. This oversight significantly hampers the training process, resulting in slower convergence rates and suboptimal model performance. An ideal negative sampling mechanism should not only leverage node embeddings effectively but also adaptively select high-quality negative samples based on the model’s current state, ensuring both dynamic responsiveness and sampling accuracy. While various methodologies have been explored to enhance link prediction accuracy, the potential of fuzzy rough sets—a mathematical framework for measuring fuzzy relations and handling uncertainty—remains largely unexplored in the context of GNNs and link prediction. This theoretical framework offers unique advantages in capturing imprecise relationships and handling ambiguous data structures, making it particularly suitable for network analysis tasks. To address these limitations and leverage the untapped potential of fuzzy rough sets, we propose a novel fuzzy rough sets-based negative sampling strategy called Fuzzy Negative Sampling (FNS). This approach systematically evaluates candidate negative edges through their fuzzy lower approximation values, selecting the top K candidates as negative training instances. Furthermore, we introduce Fuzzy Graph Attention Network (FGAT), an enhanced graph neural architecture designed to aggregate neighboring node information in a more robust and effective manner. The main contributions of this work can be summarized as follows: • We introduce FNS, a novel negative sampling framework that leverages fuzzy rough sets theory to identify high-quality negative edges, significantly improving the effectiveness of the training process in link prediction tasks. • We propose FGAT, an innovative graph attention network that incorporates fuzzy rough set principles to achieve more robust and discriminative node representations. • We conduct comprehensive experiments across two real-world datasets, demonstrating the effectiveness of our proposed framework."
https://arxiv.org/html/2411.07439v1,"Music Discovery Dialogue Generation Using
Human Intent Analysis and Large Language Models","A conversational music retrieval system can help users discover music that matches their preferences through dialogue. To achieve this, a conversational music retrieval system should seamlessly engage in multi-turn conversation by 1) understanding user queries and 2) responding with natural language and retrieved music. A straightforward solution would be a data-driven approach utilizing such conversation logs. However, few datasets are available for the research and are limited in terms of volume and quality. In this paper, we present a data generation framework for rich music discovery dialogue using a large language model (LLM) and user intents, system actions, and musical attributes. This is done by i) dialogue intent analysis using grounded theory, ii) generating attribute sequences via cascading database filtering, and iii) generating utterances using large language models. By applying this framework to the Million Song dataset, we create – LP-MusicDialog, a Large Language Model based Pseudo Music Dialogue dataset, containing over 288k music conversations using more than 319k music items. Our evaluation shows that the synthetic dataset is competitive with an existing, small human dialogue dataset in terms of dialogue consistency, item relevance, and naturalness. Furthermore, using the dataset, we train a conversational music retrieval model and show promising results.111Our code is available at https://github.com/seungheondoh/lp-music-dialog/","In recent years, conversational systems have emerged as a promising solution to enhance user experience in various domains [1, 2, 3, 4], including conversational music retrieval and recommendation [5, 6]. The goal of a conversational music system is to assist users in finding their desired music through dialogues. Such a system should possess three key capabilities: i) to understand the intents and musical needs of users from their queries expressed in natural language, ii) to generate responses and facilitate human-like interaction, iii) to find music that aligns with the user’s preferences by taking previous dialogues into account. Currently, the primary challenge of developing a conversational music retrieval system is the scarcity of large-scale public datasets. Chaganty et al. [5] introduce the Conversational Playlist Curation Dataset (CPCD). This crowd-sourced dataset comprises human-to-human dialogues that simulate the process of music discovery. However, as it relies on a manual process, the dataset is small and exhibits biases from the music streaming platforms used by the recommenders. To address this problem, Leszczynsk et al. [6] propose a dialogue generation framework through random walks in the music-text joint embedding space and dialogue inpainting [7]. However, this approach requires a high-quality music-text joint embedding and needs to use template-based system responses as input. As a result, the system’s responses are always composed of limited format utterances, leading to low naturalness in human evaluation. In this paper, we introduce a framework for generating human-like music discovery dialogues using intent and a large language model (LLM). The proposed framework is based on the existing method [6], but we address their limitations by employing cascading music database filtering instead of a joint embedding and extensive intent analysis for naturalness. Using the grounded theory approach [8], we analyze a dataset of human music discovery dialogues and develop taxonomies for user intents, system actions, and musical attributes relevant to the task of music discovery. Furthermore, we introduce a model-free attribute sequence generation by applying cascading filtering to a multi-label music annotation database. Finally, we synthesize music discovery dialogues through an LLM using the created attribute sequences and human intents/actions. Our contributions are threefold: First, we analyze music discovery dialogues and propose a taxonomy. Second, we introduce the LP-MusicDialog dataset, a large-scale synthetic dialogue dataset created using human intent and an LLM. Third, we present extensive objective and subjective evaluations to demonstrate the effectiveness of LLM-based pseudo-music dialogues."
https://arxiv.org/html/2411.07021v1,Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation,"Retrieval-augmented generation (RAG) has shown its impressive capability of providing reliable answer predictions and addressing severe hallucination problems. A typical RAG implementation adopts powerful retrieval models to extract external information and leverage large language models (LLMs) to generate corresponding answers. Different with that, recent LLM-based retrieval has raised much attention because it brings substantial improvements in information retrieval (IR) via LLMs’ vigorous semantic understanding capability. However, directly applying LLM to RAG systems remains certain challenges. This may cause feature locality problems since massive parametric knowledge impedes the effective usage of the global information among all corpus, e.g., a LLM-based retriever usually inputs the summary of documents instead of the whole documents. Moreover, various tasks pre-trained in LLMs induce severe variance, which further weakens its performance as the retriever. To address these issues, we propose a novel two-stage fine-tuning architecture called Invar-RAG. In the retrieval stage, a LLM-based retriever is constructed by integrating a LoRA-based representation learning to address the feature locality problem. To justify and consolidate this retrieval’s performance, two patterns (i.e., invariant and variant patterns) and an invariance loss are also developed to alleviate the variance in LLM. Moreover, in the generation stage, a meticulously designed fine-tuning method is devised to improve our LLM for accurate answer generation based on the retrieved information. Experimental results demonstrate that Invar-RAG significantly outperforms existing baselines across three Open-domain Question Answering (ODQA) datasets. The code is available in Supplementary Material to ease reproducibility.","Over the past decade, large language models (LLMs) have demonstrated promising capability in processing natural language Bajaj et al. (2016). Owing to the vast amount of knowledge encoded in their internal parameters, LLMs such as GPT Izacard & Grave (2020) and LLaMa Yao et al. (2014) have demonstrated remarkable performance on various downstream tasks, including Open-domain Question Answering (ODQA) Zhu et al. (2021), Reading Comprehension Cheng et al. (2023). However, the fixed parametric knowledge of LLMs has hindered the further applications of LLMs and made them prone to errors (hallucination Wongsuphasawat et al. (2012) and factual errors Fan et al. (2024)). To overcome the limitations of parametric knowledge, one promising approach is Retrieval-Augmented Generation (RAG) Fan et al. (2024); Lewis et al. (2020). Compared to relying solely on parametric knowledge, RAG enables LLMs to use retrievers to access relevant information from external knowledge sources, enhancing their question-answering abilities. Among the two components of RAG, current methods primarily focus on optimizing the retriever to return more relevant documents due to the high cost of fine-tuning and black-box LLM APIs. Previous retrievers leveraged deep learning technology (e.g., dense retrieval Zhao et al. (2024)) to encode the text representations from the lexical space into the high-dimensional latent space, allowing them to model more complex semantic relationships between queries and corpora. However, the separation between the retriever and generation model has hindered their full integration, limiting their compatibility in downstream applications . Some advanced RAG systems, such as RA-DIT Lin et al. (2023b), have adopted joint training mechanisms that fine-tune both the retriever and the generation model for better alignment. However, this approach is impractical due to the need for frequent fine-tuning and fails to utilize the LLMs’ semantic understanding capabilities during the retrieval stage. Consequently, generative retrieval (GR), also known as LLM-based retrieval, leverages the parametric memory of generative models to directly generate document identifiers (DocIDs) Li et al. (2024), which has aroused much attention. By memorizing the documents as the parametric knowledge of LLM, this kind of method breaks the limitations of traditional IR in terms of document granularity and simple relevance matching Nguyen & Yates (2023), offering more flexibility and creativity, thus better meeting practical needs. However, two severe problems hinder the current LLM-based retrieval. 1)Feature Locality: LLM-based retrieval normally adopt language models to learn the mapping from queries to the relevant document DocIDs. However, these DocIDs actually can not fully represent the global information of the passages. Meanwhile, directly feeding the whole passages into LLM is costly and infeasible, causing a trade-off between effectiveness and efficiency. 2)Retrieval Variance: Due to the inherent generative inconsistency property of large language models , current LLM-based retrieval may generate unforeseeable variances, especially when the input query or the size of context varies, directly results in undesirable and vulnerable performance which may not be preferred. Considering the problems mentioned above and better leveraging the capability of LLMs, we propose a fully LLM-based architecture with a two-stage fine-tuning method called Invar-RAG, as illustrated in Figure 1. In the retrieval stage, our approach initializes the pre-trained LLaMA Yao et al. (2014) as the backbone and follows the bi-encoder architecture in DPR Karpukhin et al. (2020) to construct our retriever. Compared to normal GR methods which need an iterative process of encoding and decoding, we introduce a component called LLM-aligned Retrieval. It first represents the input query and corpora into high-dimension space using a small language model (MiniLM) Wang et al. (2020), then introduces a new loss function constructed by KL-divergence to align the coarse query-documents pairs representation to the LLM’s representation space. This allows the retriever to leverage the rich prior knowledge of LLM, typically addressing the feature locality caused by only feeding DocIDs to LLM. Moreover, based on the initial objective of our LLM-aligned Retrieval, we introduce the invariance loss to overcome the variance in the retrieval stage. By recognizing the invariant pattern that contributes the most to the performance and gradually forcing the model to rely on the invariant pattern, we can avoid the unforeseeable variances in practice and enhance the robustness of our RAG system. Finally, in the generation stage, we freeze the weights we fine-tuned before and optimize the generation function to allow the LLM to give correct answers to the retrieved documents. Our contributions are summarized as follows: • We introduce Invar-RAG, a novel framework featuring a two-stage fine-tuning method on a single shared LLM, including the retrieval stage and generation stage. • We introduce a novel LLM-based retrieval method containing representation learning and invariance loss, respectively addressing the issues of feature locality and retrieval variance. • We validate Invar-RAG’s performance on three public ODQA datasets, no matter for retrieval performance or generation performance, demonstrating its superiority. Figure 1: Overview of proposed Invar-RAG."
https://arxiv.org/html/2411.06877v1,"LLM-Assisted Relevance Assessments:
When Should We Ask LLMs for Help?","Test collections are information retrieval tools that allow researchers to quickly and easily evaluate ranking algorithms. While test collections have become an integral part of IR research, the process of data creation involves significant efforts in manual annotations, which often makes it very expensive and time-consuming. Thus, the test collections could become small when the budget is limited, which may lead to unstable evaluations. As an alternative, recent studies have proposed the use of large language models (LLMs) to completely replace human assessors. However, while LLMs seem to somewhat correlate with human judgments, they are not perfect and often show bias. Moreover, even if a well-performing LLM or prompt is found on one dataset, there is no guarantee that it will perform similarly in practice, due to difference in tasks and data. Thus a complete replacement with LLMs is argued to be too risky and not fully trustable.Thus, in this paper, we propose LLM-Assisted Relevance Assessments (LARA), an effective method to balance manual annotations with LLM annotations, which helps to make a rich and reliable test collection. We use the LLM’s predicted relevance probabilities in order to select the most profitable documents to manually annotate under a budget constraint. While solely relying on LLM’s predicted probabilities to manually annotate performs fairly well, with theoretical reasoning, LARA guides the human annotation process even more effectively via online calibration learning. Then, using the calibration model learned from the limited manual annotations, LARA debiases the LLM predictions to annotate the remaining non-assessed data. Empirical evaluations on TREC-COVID and TREC-8 Ad Hoc datasets show that LARA outperforms the alternative solutions under almost any budget constraint.","Test collections are laboratory tools that enable information retrieval (IR) systems to be evaluated easily. A collection consists of relevance assessments, which are annotations of whether a search result (we refer to as a document) is relevant to the searcher’s need. These ground truth annotations (also known as gold standard labels) in test collections usually originate from relevance assessors who each develop their own question (we refer to as a topic) and annotate whether documents are relevant to the question (Voorhees, 2005). The relevance judgments are then used to evaluate and compare ranking algorithms quickly and easily (Thakur et al., 2021). The Text REtrieval Conference (TREC) is an example of community efforts that develop these large test collections (Voorhees, 2005; Roberts et al., 2021; Craswell et al., 2024). In venues like TREC, the assessments are mostly done by topic developers themselves, so their annotations are always the ground truth. However, a major problem with evaluations that involve annotations is that they are costly and time-consuming. When the annotation budget is limited, test collections could become too small, potentially leading to unstable evaluation results. Various document selection techniques have been developed for more efficient manual assessments (Cormack et al., 1998; Aslam et al., 2006; Aslam and Yilmaz, 2007; Li and Kanoulas, 2017), but the problem of data size has always persisted. To solve the problem of annotation cost, some recent studies have proposed to use Large Language Models (LLMs) to replace the annotation process (Faggioli et al., 2023; Thomas et al., 2024; Qin et al., 2024). By using LLMs, the test collection size could increase drastically since budget is no longer a problem. Moreover, studies have shown that LLM annotations mostly perform better than other cheaper choices like crowd-sourcing (Thomas et al., 2024). However, while these studies have shown some success, we can infer from the results that LLMs do not perfectly match with the ground truth judgments. Moreover, LLMs often show biases (i.e., under or over-generating relevance labels (Thomas et al., 2024)). Even if a well-performing LLM or prompt is found on one dataset, there is no guarantee that it will perform the same in practice, due to difference in tasks, searcher, topics, or documents. Thus, relying solely on LLMs to build a test collection is too risky. In fact, some argue that LLMs should never fully construct a test collection, given that only human assessments can be the ground truth (Soboroff, 2024). Therefore, there is a dilemma between humans and LLMs in building test collections; the former is trustworthy and consists of high quality annotations, but is costly and potentially results in a smaller data size; the latter is cheaper, but produces less accurate assessments and is risky. To address the dilemma between LLMs and humans, our work proposes LLM-Assisted Relevance Assessments (LARA), which effectively balances human annotations with LLM predictions to achieve a trustworthy, yet budget-friendly annotation procedure. Given a limited budget, LARA uses LLMs to choose the most effective documents for manual annotation. Specifically, LARA uses LLMs’ output token probabilities to find their most uncertain predictions that should be manually annotated. Then, the rest of the documents (easy and more confident ones) can be judged by LLMs. While directly using LLM relevance probability as uncertainty is an effective method to find documents for manual annotation, we argue and show that these LLM predictions are often biased and are not optimal. To address this, we propose an algorithm that actively learns to calibrate the LLM relevance probabilities, continuously identifying the most effective documents to annotate. The remaining documents are then annotated using the calibrated LLM predictions, adjusted by the calibration model learned from human assessments. Thus, in our algorithm, manual annotations are guided by calibrated LLM predictions, and LLM predictions are in turn refined by the gathered manual annotations. Finally, we conduct comprehensive experiments on TREC-COVID and TREC-8 Ad Hoc datasets, where we show that the LARA algorithm outperforms all tested alternative methods under any budget constraint."
https://arxiv.org/html/2411.06784v1,Boosting the Targeted Transferability of Adversarial Examples via Salient Region & Weighted Feature Drop,"Deep neural networks can be vulnerable to adversarially crafted examples, presenting significant risks to practical applications. A prevalent approach for adversarial attacks relies on the transferability of adversarial examples, which are generated from a substitute model and leveraged to attack unknown black-box models. Despite various proposals aimed at improving transferability, the success of these attacks in targeted black-box scenarios is often hindered by the tendency for adversarial examples to overfit to the surrogate models. In this paper, we introduce a novel framework based on Salient region & Weighted Feature Drop (SWFD) designed to enhance the targeted transferability of adversarial examples. Drawing from the observation that examples with higher transferability exhibit smoother distributions in the deep-layer outputs, we propose the weighted feature drop mechanism to modulate activation values according to weights scaled by norm distribution, effectively addressing the overfitting issue when generating adversarial examples. Additionally, by leveraging salient region within the image to construct auxiliary images, our method enables the adversarial example’s features to be transferred to the target category in a model-agnostic manner, thereby enhancing the transferability. Comprehensive experiments confirm that our approach outperforms state-of-the-art methods across diverse configurations. On average, the proposed SWFD raises the attack success rate for normally trained models and robust models by 16.31% and 7.06% respectively.","Recent studies [1, 2, 3] have demonstrated that Deep Neural Networks (DNNs) are vulnerable to adversarial examples, which can lead to misclassification by introducing subtle perturbations to the input. This susceptibility poses notable risks for DNN applications in critical domains such as autonomous driving [4, 5] and face recognition [6, 7, 8]. Figure 1: Illustrating the average outputs of the Block-4 in RestNet18. (a) represents the weights of the last linear layer. (b-f) are outputs of clean images and adversarial examples generated by different algorithms. The outputs are plotted according to the indices of the sorted weights. Numerous approaches [9, 10, 11, 12, 13, 14] have been proposed for crafting adversarial examples, categorized as white-box and black-box attacks. White-box attacks assume full access to the model’s architecture and parameters, whereas black-box attacks are constrained to input-output interactions. Most of adversarial attacks [15, 16, 17] perform well in white-box scenarios, but they are often less effective in black-box settings, especially against models with defense mechanisms due to the limited knowledge of the target model. A key challenging in black-box attacks is enhancing the transferability of adversarial examples, i.e., their ability to attack the target black-box model from the surrogate model [18, 19]. Thus, in this paper, we concentrates on enhancing the transferability of adversarial examples in targeted black-box scenarios. Notably, we have observed that when clean images and corresponding adversarial examples are fed into DNNs, targeted adversarial examples crafted by those methods with poor transferability often tend to concentrate on a limited set of features, resulting in overfitting to the surrogate model. As illustrated in Figure 1, from the perspective of deep layer outputs, the perturbation generation process overly focuses on the surrogate model’s target category features, increasing the output variance and reducing the perturbation’s generalization across other models (i.e., transferability). In subplots (c-f) of Figure 1, the mean and variance decrease sequentially (Their mean values are 5.3, 4.5, 3.8, and 2.2, respectively, with variances of 54.5, 37.6, 25.8, and 7.1). Thus, we hypothesize that the output distribution of samples with better transferability are smoother. Motivated by this observation, in this paper, we introduce a novel targeted adversarial example attack framework based on Salient region & Weighted Feature Drop (SWFD) for boosting the transferability. We first propose a weighted feature drop mechanism to prevent the adversarial example generation from becoming overly dependent on a narrow subset of features by diversifying the emphasis across a wider array of features, enhancing the transferability of these adversarial examples. In addition, the underperformance of adversarial examples on different black-box models is likely due to varying decision boundaries. Therefore, we leverage the salient region of the clean image to create auxiliary images, which are employed to further optimize the perturbation, ensuring a robust shift in the feature distribution towards the target category across different models. Our main contributions are summarized as follows: • We propose a novel black-box attack framework aimed at enhancing the transferability of targeted adversarial examples. By smoothing the deep-layer outputs, we improve their adaptability across different models. • We design a weighted feature drop mechanism that selects channel-wise features based on weights scaled by norm distribution. This mechanism mitigates the overfitting of adversarial examples to the surrogate model. • We leverage the salient region to construct auxiliary images that are used to iteratively optimize the perturbation, effectively aligning the perturbed feature distribution with the target class in a model-agnostic way. • The comprehensive experimental results demonstrate that our proposed method has superior transferability, outperforming the state-of-the-art methods in targeted black-box attack scenario. Figure 2: The overview of SWFD framework, which mainly includes two stages: (1) Salient region generation. This stage generates salient regions based on the heatmap; (2) Perturbation optimization. This stage iteratively optimizes the perturbation through the joint classification loss of the original image and auxiliary image based on the weighted feature drop."
https://arxiv.org/html/2411.06256v1,Annotative Indexing,"This paper introduces annotative indexing, a novel framework that unifies and generalizes traditional inverted indexes, column stores, object stores, and graph databases. As a result, annotative indexing can provide the underlying indexing framework for databases that support knowledge graphs, entity retrieval, semi-structured data, and ranked retrieval. While we primarily focus on human language data in the form of text, annotative indexing is sufficiently general to support a range of other data types, and we provide examples of SQL-like queries over a JSON store that includes numbers and dates. Taking advantage of the flexibility of annotative indexing, we also demonstrate a fully dynamic annotative index incorporating support for ACID properties of transactions with hundreds of multiple concurrent readers and writers.","Until recently, and with few exceptions, an inverted index provided the foundational file structure for an information retrieval system. Over the years, research progress on file structures for information retrieval was primarily driven by the need to make traditional first-stage sparse retrieval methods (e.g., BM25) as fast as possible, while minimizing storage and memory requirements, motivating the development of specialized processing methods (e.g., WAND) and compression methods (e.g., vByte). To a large extent, this research views an inverted index as single-purpose file structure, with the sole task of delivering the top-k items from a large collection to a second-stage re-ranker with high throughput and low latency. More recently, vector databases supporting dense retrieval have begun to replace inverted indexes, but the focus remains on the efficiency and effectiveness of first-stage retrieval. Managing large collections of human language data requires more than just a single-minded focus on first-stage ranking. For example, guidelines for the TREC 2024 RAG Track111 https://trec-rag.github.io/ describe the preparation a segmented version of the MS MACRO V2 passage corpus for use by track participants. Processing steps include the identification and elimination of duplicate passages to avoid holes and inconsistencies in evaluation. The original corpus was segmented with “a sliding window size of 10 sentences and a stride of 5 sentences” to make it “more manageable for users and baselines.” The original corpus and its de-duplicated/segmented version are distributed as two independent sets of compressed JSONL files, linked to each other only by a naming convention for document identifiers. In general, collections of human language data employ a variety of text formats, including JSON, JSONL, TSV, CSV, HTML, CBOR, LaTeX, Word, and PDF. Even source code, such as Python and C++, can be considered as a form of human language data. Processing text collections involves transformations such as tokenization, sentence/word splitting, de-duplication, tagging, and entity linking, as well as generating and storing weights for sparse retrieval and vectors for dense retrieval. Tools for these tasks range from one-record-at-a-time processing in notebooks to storage in a variety of database systems, including relational databases, search engines, JSON stores, and knowledge graphs. No single tool allows us to flexibly store, transform, and search multi-format heterogeneous collections of unstructured and semi-structured human language data. This paper introduces annotative indexing, a novel framework that unifies and generalizes traditional inverted indexes, column stores, object stores, and graph databases. As a result, annotative indexing can provide the underlying indexing framework for databases that support knowledge graphs, entity retrieval, semi-structured data, and ranked retrieval. While we primarily focus on human language data in the form of text, annotative indexing is sufficiently general to support a range of other data types. For example, Section 4 presents a JSON store built on our reference implementation222 Code for the reference implementation is available at https://github.com/claclark/Cottontail. Following past practice in the information retrieval community, the reference implementation is named after an animal, in this case the eastern cottontail, which is the most common species of rabbit in North America. The author often encounters them out and about near the University of Waterloo. , which represents numbers and dates as values. An update model is integral to the flexibility of annotative indexing. Section 5 details support for transactions in our reference implementation, including support for ACID properties. As an example, the section presents a dynamically evolving collection that recapitulates the early years of TREC experiments, with dozens of concurrent writers and hundreds of concurrent readers. An annotative index stores human language data as its content plus a set of annotations describing that content. The content is represented by a sequence of tokens, where each token is assigned an integer location in an address space, as illustrated in Figure 1. If content has been deleted, gaps are possible. By convention, our reference implementation appends content at increasing addresses, starting at zero. However, negative addresses are permitted to simplify query processing. As shown in Figure 1, a translation function {\cal T}(p,q) maps an interval in the address space to the associated content. {\cal T}(p,q) is undefined if (p,q) contains a gap. For content addressing purposes, tokenization can be flexibly defined at the word or character level. Separate and distinct tokenization can be also employed for specific applications, e.g, ranking, as discussed later in the paper. {\cal T}(592856130,592856138)\Rightarrow To be or not to be, | that is the {\cal T}(17905274055,17905274393)\Rightarrow { ""docid"": ""msmarco_v2.1_doc_29_677149#3_1637632"" , ""end_char"": 3061 , ""headings"": ""Aeolian Vibration of Transmission Conductors Aeolian Vibration of Transmission Conductors What is Aeolian Vibration? Wind causes a variety of motions on transmission line conductors. Important among them are How Aeolian Vibration Occurs? Theory/Mechanism... ...that creates an alternating pressure imbalance causing the conductor to move up and down at a ninety-degree angle to the flow direction."" , ""start_char"": 1806 , ""title"": ""Aeolian Vibration of Transmission Conductors"", ""url"": ""https://studyelectrical.com/2019/07/aeolian-vibration-..."" } Figure 1: The content of an associative index is situated in an address space, which may contain gaps, where content has been deleted. A translation function {\cal T}(p,q) maps an interval in the address space to its associated content. The figure shows translation examples from an index containing the segmented version of the MS MARCO V2.1 Document Corpus as used by the TREC 2023 RAG Track. In this example, tokenization is word based, but with JSON structural elements internally represented by tokens consisting of Unicode noncharacters. Annotations provide information about intervals over the content. An annotation is a triple \left<f,(p,q),v\right>, where f is a feature, (p,q) is the interval over which the annotation applies, and v is the value of the feature over that interval, which defaults to 0. For convenience we define: \displaystyle\left<f,p,v\right> \displaystyle= \displaystyle\left<f,(p,p),v\right> (1) \displaystyle\left<f,(p,q)\right> \displaystyle= \displaystyle\left<f,(p,q),0\right> (2) \displaystyle\left<f,p\right> \displaystyle= \displaystyle\left<f,(p,p),0\right> (3) For example, the annotation \langle\texttt{start\_char:},(17905274359,17905274362),1806\rangle indicates that over the interval (17905274359,17905274362) the feature start_char: has the value 1806, as shown in Figure 1. The annotation \langle\texttt{tf:porter:aeolian},17905274055,17\rangle indicates that the Porter-stemmed term “aeolian” appears 17 times in the JSON object starting at address 17905274055. The annotation \langle\texttt{:},(17905274055,17905274393)\rangle indicates that interval the interval (17905274055,17905274393) contains a JSON object, as represented by the feature “:”. The annotation, \langle\texttt{aeolian},17905274369\rangle indicates that the word “aeolian” appears at that address. Annotations like these examples together implement BM25 ranking on a JSON store, but the annotative index itself merely stores the content and its associated annotations. The interpretation of the annotations is left to the ranking algorithm. Annotations are indexed by feature, with two access methods (\tau and \rho) that both take an address k in the address space and return the first annotation greater than or equal to k, according to the start or end address of the interval. \displaystyle f.\tau(k) \displaystyle= \displaystyle\left\{\langle(p,q),v\rangle\ |\ \langle f,(p,q),v\rangle\text{\ % is the annotation for $f$ with minimal $p\geq k$}\right\} (4) \displaystyle f.\rho(k) \displaystyle= \displaystyle\left\{\langle(p,q),v\rangle\ |\ \langle f,(p,q),v\rangle\text{\ % is the annotation for $f$ with minimal $q\geq k$}\right\} (5) To simplify index organization and facilitate index processing, the set of annotations for a feature must follow minimal-interval semantics as defined in prior work, including Boldi and Vigna (2016, 2018); Clarke and Cormack (2000) and Clarke et al. (1995a). Minimal interval semantics requires that no annotation for the same feature can be contained in another, but they can overlap. If \langle f,(p,q),v\rangle and \langle f,(p^{\prime},q^{\prime}),v^{\prime}\rangle are annotations for feature f, then either p<p^{\prime} and q<q^{\prime}, or p>p^{\prime} and q>q^{\prime}. The annotations for f are thus totally ordered — in the same order — by their start and end addresses. Boldi and Vigna (2018) describe a set of intervals under minimal-interval semantics as an element of a “Clarke–Cormack–Burkowski lattice”. Clarke, Cormack, and Burkowski themselves call it a “generalized concordance list”. In this paper, the term “annotation list” implies an ordered set of annotations under minimal-interval semantics. Prior work on minimal-interval semantics demonstrated their practical value as a method for expressing queries over heterogeneous collections of semi-structured data, providing efficient support for containment, boolean, merge, proximity, ordering, and other structural operators. This paper extends this prior work in two ways, which together substantially increase expressive power. First, while prior work treated singleton intervals as the only atomic unit for indexing purposes, we index intervals of any length. For example, by indexing intervals we can run a sentence splitter over the content and add annotations to the index indicating sentence boundaries. Second, we associate a value with each interval, which is preserved by containment and merge operations. For example, we can compute terms statistics over the content and add annotations to support ranked retrieval. In this work, the atomic unit for indexing is an annotation, comprising a feature, an interval and a value. Operators combine annotations lists to produce annotation lists. Section 2 places annotative indexing in the context of prior work. Section 3 then presents the overall organization of an annotative index. As a proof of concept, the section also describes the architecture of Cottontail, our reference implementation for annotative indexing. All experimental results in the paper were generated with this reference implementation. The design of the reference implementation reflects the relative simplicity of an annotative index, with a small number of generic components that can be specialized and combined to support different applications. Section 4 then discusses query processing, and Section 5 discusses dynamic update. Annotations and dynamic update complement and support each other. Dynamic update of traditional inverted indexes is generally limited to adding and deleting full documents, with a simple transaction model, if any. Introducing the ability to annotate content after it has been added facilitates more flexible update operations, requiring support for transactions that support multiple concurrent readers and writers."
https://arxiv.org/html/2411.06254v1,KeyB2: Selecting Key Blocks is Also Important for Long Document Ranking with Large Language Models,"The rapid development of large language models (LLMs) like Llama has significantly advanced information retrieval (IR) systems. However, using LLMs for long documents, as in RankLLaMA, remains challenging due to computational complexity, especially concerning input token length. Furthermore, the internal mechanisms of LLMs during ranking are still not fully understood. In this paper, we first explore the internal workings of LLMs during relevance judgement and identify that specific attention heads play a crucial role in aligning relevant tokens. This observation inspires us to revisit the block pre-ranking strategy used in KeyB, which remains state-of-the-art (SOTA) on the TREC 2019 DL document ranking dataset. Building on these insights, we develop KeyB2, an advanced long document IR approach that integrates block pre-ranking with the performance of LLMs. KeyB2 efficiently identifies and processes the most relevant blocks, reducing computational costs and improving ranking effectiveness. Additionally, we introduce a new bi-encoder block matching strategy for KeyB2. Comprehensive experiments on long-document datasets, including TREC 2019 DL, Robust04, and MLDR-zh, show that KeyB2 outperforms baselines like RankLLaMA and KeyB by reducing reranking time and GPU memory usage while enhancing retrieval performance, achieving new SOTA results on TREC 2019 DL with higher NDCG@10 and MAP scores.","The rapid development of large language models (LLMs) such as GPT and Llama has transformed diverse natural language processing (NLP) tasks, including information retrieval (IR), text generation, and comprehension. These models have demonstrated remarkable capabilities in capturing deep semantic relationships within text, leading to significant advancements in IR systems. Cross-encoder models, such as RankLLaMA (Ma et al., 2024), have been particularly effective at evaluating the relevance of queries and passages by encoding entire input sequences jointly. Despite these advancements, processing long documents with LLMs remains challenging, primarily due to the quadratic complexity of self-attention mechanisms, leading to high computational and memory demands. Furthermore, there remains room for improvement in the effectiveness of directly using LLMs. In practical applications, such as legal document retrieval, enterprise search, and academic research, efficiently handling long documents is essential, as relevant information can be spread across lengthy text. Techniques like block selection, which divides long documents into smaller segments and focuses on the most pertinent portions, have emerged as practical solutions to reduce computational demands. The KeyB approach (Li et al., 2023a) has demonstrated that selectively processing key blocks can enhance both retrieval efficiency and accuracy. However, as LLM architectures continue to evolve, revisiting block selection strategies is crucial to fully exploit the potential of these strategies in long document IR. Moreover, the internal mechanisms of LLMs during the ranking process are still not fully understood, particularly how these models weigh and align relevant content between queries and documents. Understanding the role of attention mechanisms in relevance judgement remains a key challenge, and it is essential to investigate how specific attention heads contribute to aligning query terms with document content in the context of IR. In this paper, we propose KeyB2, an enhanced block selection strategy designed to integrate seamlessly with LLMs like Llama, combining efficient block selection with the advanced semantic matching capabilities of modern models. Additionally, compared to KeyB models, we incorporate a bi-encoder-based block selection strategy, enhancing the balance between efficiency and semantic comprehension. The KeyB2 approach aims to address the computational challenges associated with long document processing while leveraging LLMs’ contextual understanding to improve ranking performance. By selectively identifying and processing key blocks, KeyB2 reduces computational overhead and enhances retrieval effectiveness, making it well-suited for large-scale IR tasks. To explore the potential of the KeyB2 approach, we investigate the following research questions: RQ1: What occurs within LLMs, such as RankLLaMA, during relevance judgements in IR tasks, and how do attention mechanisms influence the judgement of query and document content? RQ2: How can the KeyB approach be upgraded using LLMs, and what are the best strategies for designing effective block selection methods? RQ3: Can the integration of block selection strategies with LLMs improve both the efficiency and effectiveness of long document ranking compared to models that process entire documents? Is block selection still crucial in the era of advanced LLMs for long document retrieval, and does it provide an advantage over directly processing entire documents? We validate our approach with extensive experiments across several long-document datasets, including TREC 2019 DL document ranking task, Robust04, and MLDR-zh. Our findings show that KeyB2 significantly outperforms baseline models, including traditional and LLM-based approaches, in terms of retrieval accuracy. Besides, KeyB2 models have higher computational efficiency compared with LLM based IR approach RankLLaMA. The results highlight that combining LLMs with intelligent block selection can optimize long document ranking while maintaining or improving retrieval performance. The structure of this paper is as follows: Section 2 reviews related work in IR, LLMs, and block selection methods. Section 3 provides a detailed analysis of attention mechanisms within LLMs and their relevance to document retrieval. Section 4 introduces the KeyB2 approach, while Section 5 outlines the experimental setup, and presents the results of effectiveness and efficiency with analysis. Lastly Section 6 concludes the paper."
https://arxiv.org/html/2411.06237v1,Leveraging Retrieval-Augmented Generation for University Knowledge Retrieval,"This paper introduces an innovative approach using Retrieval-Augmented Generation (RAG) pipelines with Large Language Models (LLMs) to enhance information retrieval and query response systems for university-related question answering. By systematically extracting data from the university’s official webpage and employing advanced prompt engineering techniques, we generate accurate, contextually relevant responses to user queries.We developed a comprehensive university benchmark, UniversityQuestionBench (UQB), to rigorously evaluate our system’s performance, based on common key metrics in the filed of RAG pipelines, assessing accuracy and reliability through various metrics and real-world scenarios. Our experimental results demonstrate significant improvements in the precision and relevance of generated responses, enhancing user experience and reducing the time required to obtain relevant answers. In summary, this paper presents a novel application of RAG pipelines and LLMs, supported by a meticulously prepared university benchmark, offering valuable insights into advanced AI techniques for academic data retrieval and setting the stage for future research in this domain.","Large Language Models (LLMs), including cutting-edge ones like OpenAI GPTs and Google Gemini models, often face significant challenges when it comes to extracting and utilizing local data, particularly from specialized datasets such as universities archives. These models are typically trained on broad, diverse datasets, which can result in a lack of specificity and accuracy when applied to niche domains. The challenges include the inability to access and process localized data effectively, leading to issues like hallucinations and inaccuracies in generated content. Additionally, the models’ reliance on pre-existing knowledge limits their capability to incorporate newly acquired, domain-specific information without extensive retraining [1, 2]. Figure 1: Our Proposed Pipeline Retrieval-Augmented Generation (RAG) offers a robust solution to the challenges faced by LLMs in processing local documents. By integrating retrieval mechanisms with generation capabilities, RAG pipelines enable models to access and utilize specific, relevant information from extensive datasets. Our proposed pipeline leverages a two-stage RAG approach combined with a Persian Large Language Model (PLM) and advanced prompt engineering techniques. Initially, queries are categorized to identify the most relevant documents, after which the appropriate LLM is engaged to generate accurate and contextually relevant responses. This method significantly enhances the precision and utility of LLMs in handling localized, domain-specific queries [3, 4]. We developed the ”UniversityQuestionBench” dataset, created from the most frequently asked questions by students across various disciplines. This dataset is designed to evaluate the performance of Persian LLMs integrated with RAG using the RAGAS evaluation metrics, which includes three key measures: Faithfulness, Answer Relevance and Context Relevance. By employing these metrics, we ensure that the model provides accurate, relevant, and contextually appropriate responses. The dataset and evaluation processes aim to benchmark the effectiveness of our pipeline in addressing the specific needs of universities students [5, 6, 7]. Our contributions to this paper are as follows: • Development of a two-stage RAG pipeline integrated with Persian LLMs for handling localized queries. • Creation of the UniversityQuestionBench dataset, tailored to the most common queries from university students. • Leveraging the RAGAS evaluation metrics to rigorously assess the performance of our models. • Demonstration of significant improvements in Faithfulness, Answer Relevance and Context Relevance of Responses generated by our pipeline."
https://arxiv.org/html/2411.05936v1,Mitigating Hallucination | ZeroG: An Advanced Knowledge Management Engine,"The growth of digital documents presents significant challenges in efficient management and knowledge extraction. Traditional methods often struggle with complex documents, leading to issues such as hallucinations and high latency in responses from Large Language Models (LLMs). ZeroG, an innovative approach, significantly mitigates these challenges by leveraging knowledge distillation and prompt tuning to enhance model performance.ZeroG utilizes a smaller model that replicates the behavior of a larger teacher model, ensuring contextually relevant and grounded responses, by employing a black-box distillation approach, it creates a distilled dataset without relying on intermediate features, optimizing computational efficiency. This method significantly enhances accuracy and reduces response times, providing a balanced solution for modern document management.Incorporating advanced techniques for document ingestion and metadata utilization, ZeroG improves the accuracy of question-and-answer systems. The integration of graph databases and robust metadata management further streamlines information retrieval, allowing for precise and context-aware responses. By transforming how organizations interact with complex data, ZeroG enhances productivity and user experience, offering a scalable solution for the growing demands of digital document management.","ZeroG significantly improves the quality of responses by a large margin by mitigating hallucinations through the implementation of knowledge distillation and prompt tuning, ensuring responses are accurate and grounded. We differ from [1], which involves fine-tuning the student model, by utilizing a black-box distillation approach without fine-tuning. This approach reduces response latency and enhances overall system reliability. ZeroG leverages LLMs to generate Question and Answer (QnA) pairs from existing documents, storing them in a vector store. When a user query is received, similarity searches using techniques like MMR determine whether it can be addressed by pre-existing QnA pairs or requires a more tailored response using document-specific information. Despite advances in natural language processing, traditional methods often struggle with real-time data updates and accurately handling complex documents, which include sensitive data. This paper explores transforming these presentations into markdown files for easier ingestion into vector stores, enhancing QnA accuracy without frequent reengineering. We also investigate integrating graph databases and utilizing document metadata to refine search and organization capabilities. By pre-generating question sets and caching commonly asked queries, the system streamlines responses, ensuring they are precise and contextually relevant. This paper presents some of the techniques we explored and employed to overcome current limitations in document and knowledge management, significantly improving productivity and effectiveness in handling complex document types."
https://arxiv.org/html/2411.07166v1,The Shapley index for music streaming platforms,"We study an index to measure the popularity of artists in music streaming platforms. This index, which can be used to allocate the amount raised via paid subscriptions among participating artists, is based on the Shapley value, a centerpiece in cooperative game theory. We characterize this Shapley index combining several axioms formalizing principles with normative appeal. This permits to place the index in the literature, as an alternative to the well-known (and widely used in the industry) pro-rata and user-centric indices.","Platform businesses have gained enormous attention in recent years, which has been reflected into the literature on economics research (e.g., Cabral et al., 2019; Belleflamme and Peitz, 2021; Calveras and Ganuza, 2021; Jullien et al., 2021). Among other things, platforms have transformed the ways in which cultural content is produced and consumed (e.g., Aguiar et al., 2024).111Nieborg and Poell (2018) have coined the term platformization of cultural production. This is particularly the case with music. In the old times, consumers typically learned about music from radio stations (or word of mouth) and eventually moved on to buy from record stores. This started to change when digital music emerged and spread universally. After some initial years in which file-sharing platforms, such as Napster, were under scrutiny by the music industry, Apple managed to persuade record companies to sell individual tracks for 99 cents. Gradually, the industry found new profitable paths, eventually embracing streaming, a massive success nowadays. To wit, according to Statista, in the second quarter of 2024, Spotify (the largest music streaming platform) reached an all-time high with 626 million active users worldwide. This marked an increase of 75 million users in just one year. Platforms cash such a massive success of streaming in various ways. But it is estimated that almost 90% of the total revenue that platforms raise comes from premium consumers. That is, consumers that gain access to all the music on the platform after paying a monthly subscription.222Nevertheless, more than half of platform users do not pay any money (instead listening to ads, while using the platform). The hybrid approach, offering both an ad-supported free version and an ad-free subscription version is not exclusive of music platforms. Netflix, Hulu, YouTube, or Pandora, to name a few, have also adopted it to deal with the trade-off between viewership and subscription profits. An interesting question, which we shall not study here, is how those multiple versions should be designed and priced (e.g., Goli et al., 2024). It is estimated that streaming platforms redistribute among artists around 65-70% of the revenue they raise, which thus becomes a major aspect in the management of streaming platforms.333To be more precise, streaming platforms pay “right holders”, who may be the artists themselves if they are independent, or the record labels if the artist is signed to one. The problem of sharing the revenue raised from paid subscriptions to streaming platforms among artists is a new form of revenue sharing problems under bundled pricing (e.g., Adams and Yellen, 1976; Ginsburgh and Zang, 2003; Bergantiños and Moreno-Ternero, 2015). As such, it offers new insights with respect to the classical literature on industrial organization (e.g., Belleflamme and Peitz, 2015). In the early years, platforms used the pro-rata method, in which artists were rewarded in proportion of their total streams. Gradually, they have been moving to a user-centric method, in which, instead, the amount paid by each user is shared among the artists this user streamed, in proportion of the user’s overall streams. The two methods have been recently scrutinized in the scientific literature (e.g., Alaei et al., 2022; Bergantiños and Moreno-Ternero, 2024). In this paper, we study a third method, which is obtained following the tradition of analyzing problems involving agents’ cooperation with a game-theoretical approach.444Classical instances are bankruptcy problems from the Talmud (e.g., Aumann and Maschler, 1985), cost alocation problems (e.g., Tijs and Driessen, 1986), river sharing (e.g., Ambec and Sprumont, 2002), allocating benefits of horizontal cooperation (e.g., Lozano et al., 2013), or the value captured in hierarchical chains (e.g., Henkel and Hoffmann, 2018). More precisely, as in Bergantiños and Moreno-Ternero (2024), we associate to each streaming problem (to be understood as the problem of allocating the overall amount raised from paid subscriptions among artists streamed in the platform) a cooperative (TU) game in which the worth of each coalition of artists is determined by the amount users streaming only those artists pay. We then consider the well-known Shapley value (e.g., Shapley, 1953) of the resulting game as an allocation rule for streaming problems.555Schlicher et al. (2024) associate another cooperative game to a streaming problem. Both Bergantiños and Moreno-Ternero (2024) and Schlicher et al. (2024) are concerned with the core of the resulting games, rather than the Shapley value of such games. Gonçalves-Dosantos et al. (2024a) point out that each of their indicators for streaming (not necessarily music) platforms coincide with the Shapley value of different cooperative games that can suitably be associated to the class of problems they analyze. It turns out that the Shapley value of such a TU game can be easily computed (which, in other settings, is not always the case). In words, it says that the subscription of each user is equally (and fully) allocated among the artists this user streamed. The ensuing allocation rule, which will be the object of our study, is what we dub the Shapley index for streaming problems. The Shapley index is closer to the user-centric index than to the pro-rata index mentioned above, as it also imposes that the amount each user pays is distributed only among artists streamed by such a user. Now, the Shapley index states that it is equally distributed among them, whereas the user-centric says that it is proportionally distributed among them. In that sense, both indices represent the two long-standing (and widely supported) principles of distributive justice: egalitarianism and proportionality (e.g., Young, 1994; Moulin, 2004; Thomson, 2019). Beyond some preliminary game-theoretical results, which yield interesting features for the Shapley index, we concentrate on its normative foundations. More precisely, following the tradition initiated by Nash (1950) and Arrow (1951), we take an axiomatic approach to streaming problems.666Bergantiños and Moreno-Ternero (2024) and Gonçalves-Dosantos et al. (2024a, 2024b) have also applied the axiomatic approach to streaming problems. Other recent instances of this approach, dealing with various problems, are Asheim et al., (2020), Flores-Szwagrzak and Treibich (2020) and Csóka and Herings (2021). To do so, we formalize several principles with normative appeal, referring to operational or ethical aspects of streaming problems, as axioms of indices. We show that several combinations of these axioms characterize the Shapley index. This permits a more thorough comparison between this index and the other two main indices that existed so far. The rest of the paper is organized as follows. In Section 2, we present the preliminaries of the model to analyze streaming problems. In Section 3, we present the Shapley index and some game-theoretical aspects of it. In Section 4, we present our axiomatic analysis. In Section 5, and based on the results from the axiomatic analysis, we properly place the Shapley index in the literature. Finally, Section 6 concludes. Some extra material (mostly referring to the tightness of our characterization results) is gathered in the appendix."
https://arxiv.org/html/2411.06826v1,Adaptive Conditional Expert Selection Network for Multi-domain Recommendation,"Mixture-of-Experts (MOE) has recently become the de facto standard in Multi-domain recommendation (MDR) due to its powerful expressive ability. However, such MOE-based method typically employs all experts for each instance, leading to scalability issue and low-discriminability between domains and experts. Furthermore, the design of commonly used domain-specific networks exacerbates the scalability issues. To tackle the problems, We propose a novel method named CESAA consists of Conditional Expert Selection (CES) Module and Adaptive Expert Aggregation (AEA) Module to tackle these challenges. Specifically, CES first combines a sparse gating strategy with domain-shared experts. Then AEA utilizes mutual information loss to strengthen the correlations between experts and specific domains, and significantly improve the distinction between experts. As a result, only domain-shared experts and selected domain-specific experts are activated for each instance, striking a balance between computational efficiency and model performance. Experimental results on both public ranking and industrial retrieval datasets verify the effectiveness of our method in MDR tasks.","1. INTRODUCTION Click-through rate (CTR) prediction is a critical application of modern recommendation systems. Traditional CTR models have primarily focused on single domains, resulting in under-utilization of data and high maintenance costs. The challenges of MDR models lie in efficiently and effectively utilizing data while capturing the heterogeneity and commonalities across different domains. When simply merging data from different domains, neglecting variations in data distribution and user behavior across contexts can lead to sub-optimal performance. To address this issue, inspired by multi-task learning (MTL)(Ma et al., 2018)(Tang et al., 2020)(Xu et al., 2022), numerous multi tower-based approaches have emerged in recent years, which primarily utilize the Mixture-of-Experts (MoE) (Ma et al., 2018) structure to effectively leverage shared information and alleviate negative transfer issues by combining shared bottom network with multiple domain-specific expert towers. Overall, The objective is to ensure that each expert tower learns scene-specific content in a discriminative manner. Despite their effectiveness, previous MoE-based works(Jiang et al., 2022)(Li et al., 2022)(Tang et al., 2020) still face the following challenges. The first is the low discriminability problem, which is common but non-trivial when employing multiple experts in MDR. Since each domain utilizes all experts, differing only in their gating weights, it struggles to distinguish between the shared characteristics and unique differences among the experts. Some domain-specific works like ADIN (Jiang et al., 2022) could improve experts’ discriminability, but they may encounter the second challenge: Scalability. In most previous MoE-based works, all experts are activated for each input sample. As the number of domains and experts increases, the time consumption for training and inference increases significantly. More importantly, these approaches heavily depend on pre-defined domain partitions that are based on rules or expert knowledge. This limitation implies that the quality of prior experience significantly constrains the expressive capability of the models. For instance, in-feed ads are displayed on over 30 platforms of OPPO smartphones (e.g., browser app and weather app). It is characterized by the exhibition of ads in the form of large images, small images, image galleries, or videos within the browser. These scenes exhibit different user behaviors and data distributions (e.g., daily exposure counts, user click-through rates, and exposure rates). Recently, some works like AdaptDHM(Li et al., 2022) utilize the clustering method to avoid manual domain definition. However, it still faces the problem of low discriminability. When modeling, we must consider the commonalities and heterogeneities between different scenes. However, the boundaries between scenes become increasingly blurred, and relying solely on manually defined domain indicators (e.g., ID) without considering the potential and complex correlations among domains will result in sub-optimal performance. To overcome these challenges simultaneously, we propose CESAA, a novel and flexible method comprising two core modules: a Conditional Expert Selection (CES) Module and an Adaptive Expert Aggregation (AEA) Module. Specifically, CES is designed to tackle differentiation and scalability problems by incorporating a sparsity constraint, which allows each expert to focus only on part of domains or features. Furthermore, the shared expert is shared by all domains and is responsible for learning the commonalities between domains. To strengthen the correlation between experts and specific domains, AEA leverages mutual information loss to enhance the modularity and discriminative power of the experts. This means there is no need for manually pre-defined and fine-grained domain partitioning. Instead, the optimal partitioning can be discovered through end-to-end learning. In summary, our paper makes the following contributions. • We propose a model named CESAA with sparse selection and adaptive aggregation mechanisms to solve low-discriminability and scalability problems simultaneously in multi-domain recommendation. • We design an adaptive mutual information-guided expert aggregation module in CESAA. This mechanism facilitates the effective aggregation of similar domains through end-to-end learning. • We perform extensive experiments on a self-built retrieval dataset and an open-source ranking dataset. The experimental results indicate that CESAA performs better than the state-of-the-art approaches."
https://arxiv.org/html/2411.06823v1,Large Language Model in Medical Informatics: Direct Classification and Enhanced Text Representations for Automatic ICD Coding,"Addressing the complexity of accurately classifying International Classification of Diseases (ICD) codes from medical discharge summaries is challenging due to the intricate nature of medical documentation. This paper explores the use of Large Language Models (LLM), specifically the LLAMA architecture, to enhance ICD code classification through two methodologies: direct application as a classifier and as a generator of enriched text representations within a Multi-Filter Residual Convolutional Neural Network (MultiResCNN) framework. We evaluate these methods by comparing them against state-of-the-art approaches, revealing LLAMA’s potential to significantly improve classification outcomes by providing deep contextual insights into medical texts.","In the past decade, advancements in Deep Learning (DL) and Natural Language Processing (NLP) have revolutionized healthcare research, propelled by the growth in health data [1]. These advancements have successfully improved the interpretation of medical images and the processing of Electronic Health Records (EHR) through Deep Neural Networks [2]. Such technologies are crucial in automatically assigning International Classification of Diseases (ICD) codes, globally vital for healthcare documentation and administration [3]. Research on automating ICD coding, which began over two decades ago, has shifted from manual feature creation to employing advanced machine learning techniques [2]. Despite these advancements, mapping unstructured medical texts to specific ICD codes remains challenging, mainly due to the complexity of medical language. Current models often fail to capture the full contextual and semantic depth required for accurate ICD coding. Recently, Generative Large Language Models (LLMs) like LLAMA and Mixtral have begun to improve standards of accuracy and efficiency in text analysis. These models use advanced linguistic understanding to enhance the correlation between vast documents and entities [4]. This paper explores the use of Large Language Models in automatic ICD coding, particularly employing LLAMA-2. These models are employed for direct classification and for generating enriched text representations for processing by a CNN-based classifier, namely a Multi-Filter Residual Convolutional Neural Network (MultiResCNN). These applications aim to fully exploit the semantic capabilities of LLAMA-2 (7b) to enhance its effectiveness in medical text interpretation and ICD code classification. Our main contributions are: • Adapting LLAMA-2 (7b) for direct ICD code classification and evaluating its effectiveness beyond typical generative uses. • Using LLAMA-2 (7b) to generate enriched text representations, processed by MultiResCNN for enhanced classification. • Rigorously evaluating these methods on the MIMIC-III dataset and comparing their performances with established baselines."
https://arxiv.org/html/2411.06805v1,Boosting the Potential of Large Language Models with an Intelligent Information Assistant,"The emergence of Large Language Models (LLMs) has significantly advanced natural language processing, but these models often generate factually incorrect information, known as ""hallucination"". Initial retrieval-augmented generation (RAG) methods like the ""Retrieve-Read"" framework was inadequate for complex reasoning tasks. Subsequent prompt-based RAG strategies and Supervised Fine-Tuning (SFT) methods improved performance but required frequent retraining and risked altering foundational LLM capabilities. To cope with these challenges, we propose Assistant-based Retrieval-Augmented Generation (AssistRAG), integrating an intelligent information assistant within LLMs. This assistant manages memory and knowledge through tool usage, action execution, memory building, and plan specification. Using a two-phase training approach—Curriculum Assistant Learning and Reinforced Preference Optimization—AssistRAG enhances information retrieval and decision-making. Experiments show AssistRAG significantly outperforms benchmarks, especially benefiting less advanced LLMs, by providing superior reasoning capabilities and accurate responses.","The emergence of Large Language Models (LLMs) has significantly advanced the field of natural language processing, demonstrating an impressive ability to mimic human-like language patterns [1]. However, despite their extensive knowledge acquired during training, LLMs can occasionally generate factually incorrect information, a phenomenon referred to as “hallucination” [2, 3]. To address this, the integration of retrieval systems with LLMs has been suggested, allowing these models to tap into external databases to generate more reliable responses [4]. Initially, retrieval-augmented generation (RAG) relied on a simple ""Retrieve-Read"" framework [5], which was adequate for basic question-answering but insufficient for complex, multi-step reasoning tasks. As language models advanced, various prompt-based RAG strategies emerged [6, 7], incorporating pre-retrieval and post-retrieval prompts to refine the process. However, these strategies heavily relied on the foundational capabilities of the language models. Consequently, the focus shifted to Supervised Fine-Tuning (SFT)-based RAG methods [8], which involve fine-tuning language models specifically for RAG tasks to enhance their performance. Figure 1: Comparisons of Naive, Prompt-based, SFT-based and our Assistant-based RAG frameworks. While SFT-based methods have improved the quality of generated responses, they face two limitations that hinder their practical application. Firstly, these fine-tuned models are not easily adaptable to emerging LLMs, requiring retraining for each new foundational LLM. Secondly, directly fine-tuning a foundational LLM in the RAG scenario may change its innate abilities, potentially leading to negative impacts on the model’s performance on other tasks. To address these challenges, we propose Assistant-based Retrieval-Augmented Generation (AssistRAG), which integrates an intelligent information assistant as a plugin within LLMs. This approach comprises a trainable assistant for information management and a static main LLM dedicated to task execution, as depicted in Figure 1. As an intelligent information assistant, AssistRAG operates in two primary categories to handle complex tasks: memory management and knowledge management. Memory management involves integrating and analyzing content from internal memory, while knowledge management focuses on leveraging external knowledge. These two main functions are supported by four core capabilities of AssistRAG: (1) Tool usage, which involves recalling relevant information from both internal memory and external knowledge bases through a retriever; (2) Action execution, which involves processing, analyzing, and extracting information; (3) Memory building, which involves recording essential knowledge and reasoning patterns from historical interactions; (4) Plan specification, which involves determining the necessity of each step in the process. These four capabilities work together to ensure that AssistRAG can provide accurate and comprehensive support to the main LLM. To implement AssistRAG, we adopt a two-phase training approach. The first phase, Curriculum Assistant Learning, enhances the assistant’s capabilities in note-taking, question decomposition, and knowledge extraction through progressively complex tasks. The second phase, Reinforced Preference Optimization, uses reinforcement learning to tailor the assistant’s feedback to the main LLM’s specific needs, optimizing knowledge extraction based on feedback from the main LLM. During the inference stage, AssistRAG operates through a three-step process: (1) Information Retrieval and Integration: The assistant understands the main LLM’s needs, retrieves relevant knowledge from internal and external sources, and extracts valuable information. (2) Decision Making: The assistant evaluates and decides whether to provide the retrieved memories and knowledge to the main LLM based on their relevance. (3) Answer Generation and Memory Updating: The main LLM generates an answer using its internal knowledge and the assistant’s information, while the assistant updates its memory with crucial reasoning steps. Results from experiments across three complex question-answering datasets reveal that AssistRAG exhibits superior reasoning capabilities and markedly outperforms existing benchmarks. Notably, when applied to different foundational LLMs, AssistRAG appears to confer more pronounced benefits on less advanced LLMs."
https://arxiv.org/html/2411.05930v1,BERTrend: Neural Topic Modeling for Emerging Trends Detection,"Detecting and tracking emerging trends and weak signals in large, evolving text corpora is vital for applications such as monitoring scientific literature, managing brand reputation, surveilling critical infrastructure and more generally to any kind of text-based event detection. Existing solutions often fail to capture the nuanced context or dynamically track evolving patterns over time. BERTrend, a novel method, addresses these limitations using neural topic modeling in an online setting. It introduces a new metric to quantify topic popularity over time by considering both the number of documents and update frequency. This metric classifies topics as noise, weak, or strong signals, flagging emerging, rapidly growing topics for further investigation. Experimentation on two large real-world datasets demonstrates BERTrend’s ability to accurately detect and track meaningful weak signals while filtering out noise, offering a comprehensive solution for monitoring emerging trends in large-scale, evolving text corpora. The method can also be used for retrospective analysis of past events. In addition, the use of Large Language Models together with BERTrend offers efficient means for the interpretability of trends of events.","The concept of weak signals, introduced by Ansoff (1975), refers to early indicators of emerging trends that can have significant implications across various domains. These include events like shifts in public opinion in social trends, early disruptive technologies in innovation, changes in activist groups and public sentiment in politics, and potential disease outbreaks in healthcare. Monitoring and analyzing weak signals offers valuable insights for organizations, researchers, and decision-makers, aiding in informed decision-making. Key data sources for identifying these trends include large text corpora such as news, social media, research and technology journals or reports. The challenges are: distinguishing meaningful weak signals from irrelevant noise, dealing with context ambiguity, and tracking the extended period over which weak signals may gain significance. With advances in NLP and AI, researchers have developed various techniques to detect weak signals across different fields,including statistics-based methods, graph theory, machine learning, semantic-based approaches, and expert knowledge. However, most solutions fall short in fully addressing the challenge of detecting emerging trends Rousseau et al. (2021), either by relying solely on keyword-based analysis, which misses contextual nuances, or by being static and unable to dynamically track evolving weak signals. In this work, we introduce BERTrend, a novel framework for detecting and monitoring emerging trends and weak signals in large, evolving text corpora. BERTrend leverages neural topic modeling, specifically BERTopic, in an online learning setting to identify and track topic evolution over time. Its key contribution lies in dynamically classifying topics as noise, weak signals, or strong signals based on their popularity trends. The proposed metric quantifies topic popularity over time by considering both the number of documents within the topic and its update frequency, incorporating an exponentially growing decay if no updates occur for an extended period. By combining neural topic modeling with a dynamic popularity metric and adaptive classification thresholds, BERTrend provides a comprehensive solution for detecting and monitoring emerging trends in large-scale, evolving text corpora. We discuss the qualitative results on two comprehensive datasets, including the overall evolution of trends and specific case studies. Combined with Large Language Models (LLMs), the method an efficient way of interpreting the detected trends of events through various dimensions indicating how they evolve over time."
