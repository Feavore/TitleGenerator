URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10283v1,"Error analysis of a first-order DoD cut cell method
for 2D unsteady advection","In this work we present an a priori error analysis for solving the unsteady advection equation on cut cell meshes along a straight ramp in two dimensions. The space discretization uses a lowest order upwind-type discontinuous Galerkin scheme involving a Domain of Dependence (DoD) stabilization to correct the update in the neighborhood of small cut cells. Thereby, it is possible to employ explicit time stepping schemes with a time step length that is independent of the size of the very small cut cells.Our error analysis is based on a general framework for error estimates for first-order linear partial differential equations that relies on consistency, boundedness, and discrete dissipation of the discrete bilinear form. We prove these properties for the space discretization involving DoD stabilization. This allows us to prove, for the fully discrete scheme, a quasi-optimal error estimate of order one half in a norm that combines the L^{\infty}-in-time L^{2}-in-space norm and a seminorm that contains velocity weighted jumps. We also provide corresponding numerical results.","So called cut cell or embedded boundary meshes have become popular in recent years due to the easyness of the mesh generation process: a given geometry is simply cut out of a structured, often Cartesian, background mesh. This results in cut cells where the object intersects the background mesh. Cut cells can have various shapes, the sizes of neighboring elements can differ by several orders of magnitudes, and (most difficult to deal with) can become arbitrarily small. For the solution of time-dependent hyperbolic conservation laws this causes the small cell problem: standard explicit time stepping schemes are not stable on small cut cells when the time step length is chosen with respect to the background mesh. One way to address this issue is by using cell merging/cell agglomeration, see, e.g., [BPvL93, Qui94, QK13, MKEKO16]. In this approach small cut cells are merged with larger neighbors until all small cut cells are gone. The resulting mesh still contains unstructured (cut) cells along the embedded boundary but the issue of smallness is gone. Another approach is to keep the small cut cells and to stabilize them. Over the years, different methods have been developed in the context of finite volume and discontinuous Galerkin (DG) schemes to solve the small cell problem as described above. Two well-known approaches are the flux redistribution method, see, e.g., [CC87, CGKM06], and the h-box method [BHL03, BH12]. Newer developments are the dimensionally split approach [KBN09, GNK18], the mixed explicit implicit scheme [MB17], the extension of the active flux method to cut cell meshes [HK20], the state redistribution method [BG21, Giu22], the Domain of Dependence (DoD) stabilization [EMNS20, MS22], and the extension of the ghost penalty stabilization [Bur10] to first-order hyperbolic problems on cut cell meshes [FK21, FFKZ22]. All of these methods are based on finite volume schemes or stabilize a standard DG scheme. Preliminary work [KH21] in the context of so called DG difference schemes indicates that due to the extended support of the underlying basis functions, one might get away without stabilizing the scheme on small cut cells but further investigation is needed. The derivation of error estimates for these schemes is very challenging. Since stabilized schemes on cut cell meshes keep the resulting potentially arbitrarily small and very skewed cut cells, any derivation of an error estimate has to handle these cells, leading to many complications. In this work we will present an error estimate for the time-dependent linear advection equation in two dimensions for the fully discrete scheme based on using the DoD stabilization. Due to complexity, we will only consider advection along a straight ramp, using piecewise constant functions in space combined with explicit Euler in time. We will show that under a CFL constraint that is independent of the size and geometry of the small cut cells, there holds a result of the form \|u(t^{M},\cdot)-u_{h}^{M}\|_{L^{2}(\Omega)}^{2}+\sum_{n=0}^{M-1}\frac{\Delta t% }{4}\left\lvert u(t^{n},\cdot)-u_{h}^{n}\right\rvert_{\beta}^{2}\leq\mathcal{O% }(\Delta t^{2}+h). (1) The seminorm \left\lvert\cdot\right\rvert_{\beta} will be defined below in (20) and consists of face terms. As the numerical results in section 6 will show, the result is sub-optimal in the L^{2} norm (like many DG-error estimates in two dimensions) but optimal in the seminorm. (Note that the factor \Delta t on the left hand side essentially cancels with the sum over the number of time steps.) This result is very close to [DPE12, Thm. 3.7] where an analogous scheme is studied but on a quasi-uniform and shape regular mesh without DoD stabilization. The proof is based on the powerful proof framework that has been developed and is presented in [BEF10] and [DPE12, Ch. 3] and the references cited therein, and has been adjusted appropriately to the situation of the DoD stabilized scheme for cut cells. These changes affect the fundamental properties of the bilinear form, i.e., consistency, discrete dissipation, and boundedness, that we will show in section 4. In particular, dissipation and boundedness are proven with respect to DoD-adjusted seminorms and norms that include an appropriate weighting. This makes their proofs somewhat more involved. In addition, new inverse estimates and new projection error estimates had to be proven. The final proof in section 5 that puts all the pieces together then follows the original proof except for the adjustment of the consistency error. To the best of our knowledge, this is the first error estimate for a fully discrete scheme for solving the unsteady linear advection equation in two dimensions on a cut cell mesh with arbitrarily small cut cells using a stabilized scheme. The situation in one space dimension is very different. For the one-dimensional linear advection equation on a model problem, where typically one small cell is inserted in an otherwise equidistant mesh, error estimates have been shown for several schemes. Many use the following trick introduced in [WW89b, WW89a], which is suitable for finite volume and finite difference schemes: for showing order p, in a truncation error analysis of the one step error, one typically finds that the error is only \mathcal{O}(h^{p}) (instead of \mathcal{O}(h^{p+1})) in the neighborhood of the cut cell. One then defines a new grid solution such that (1) the one step error is \mathcal{O}(h^{p+1}) for this new solution and such that (2) the new solution is at each grid point and at each time step only \mathcal{O}(h^{p}) away from the true solution. In a cut cell context, this approach has first been used to show first- and second-order error estimates for the h-box method [BHL03]. It has then been copied to show a first-order result for the dimensionally split approach [GNK18], a first-order result for the DoD stabilization [EMNS20] (for piecewise constant polynomials, the DG based DoD stabilization and the h-box method are essentially the same), and a second-order result for the mixed explicit implicit scheme [ML24]. Finding such a new grid solution is already challenging in one dimension, and would probably be enormous work in two dimensions. Again for the time-dependent advection equation in one dimension but in a more DG-style approach are the results for the stabilization based on a ghost penalty approach [FK21, FFKZ22]: for the semi-discrete setting, using polynomials of degree p, order p+\tfrac{1}{2} is shown for the L^{2}-norm of the spatial error as well as order p when an interface is present. This contribution is structured as follows: First, in section 2, we describe the problem setup, introduce notation, and define the fully discrete problem based on using the DoD stabilization. We also introduce the seminorm \left\lvert\cdot\right\rvert_{\beta} and the norms \left\lVert\cdot\right\rVert_{\text{uwb}} and \left\lVert\cdot\right\rVert_{\text{uwb},*} that we will use throughout this paper in our proofs. Finally, we collect the major results (concerning consistency, discrete dissipation, and boundedness) that will be necessary to prove the desired result and where the major adjustments were made. The proof of these results will be given later in section 4. Before that, in section 3, we will prove cut cell related estimates such as inverse estimates accounting for the potentially arbitrarily small cut cells and a new result for the projection error. In section 5, we will then put all the pieces together and prove the main error estimate. In the final section 6 we will present numerical results to examine the optimality of our theoretical results."
https://arxiv.org/html/2411.10151v1,Self-Alignment Radio Frequency Resonant Beam System for Information and Power Transfer,"Due to power attenuation, improving transmission efficiency in the radio-frequency (RF) band remains a significant challenge, which hinders advancements in various fields of the Internet of Things (IoT), such as wireless power transfer (WPT) and wireless communication. Array design and retro-directive beamforming (RD-BF) techniques offer simple and effective ways to enhance transmission efficiency. However, when the target is an array or in the near field, the RD-BF system (RD-BFS) cannot radiate more energy to the target due to phase irregularities in the target region, resulting in challenges in achieving higher efficiency. To address this issue, we propose the RF-based resonant beam system (RF-RBS), which adaptively optimizes phase and power distribution between transmitting and receiving arrays by leveraging the resonance mechanism to achieve higher transmission efficiency. We analyze the system structure and develop an analytical model to evaluate power flow and resonance establishment. Numerical analysis demonstrates that the proposed RF-RBS achieves self-alignment without beam control and provides higher transmission efficiency compared to RD-BFS, with improvements of up to 16%. This self-alignment capability allows the system to effectively transfer power and information across varying distances and offsets. The numerical results indicate the capability to transmit watt-level power and achieve 21 bps/Hz of downlink spectral efficiency in indoor settings, highlighting the advantages of RF-RBS in information and power transfer for mobile applications.","Figure 1: A typical application scenario of RF-RBS in IoT (e.g., WPT, wireless communication). The advancement of Internet of Things (IoT) technologies and the widespread application of IoT devices [1] have facilitated pervasive sensing and communication, playing a significant role in intelligent logistics [2], smart homes [3], and smart cities [4]. However, the limited battery life of IoT devices challenges their long-term functionality [5, 6]. Radio-frequency (RF) wireless power transfer (WPT) technology has emerged as a promising solution, offering a continuous and uninterrupted power supply for IoT devices and the potential to enhance their sensing and communication capabilities [7]. Nevertheless, RF WPT faces challenges with low end-to-end transmission efficiency due to significant power attenuation of electromagnetic (EM) waves, resulting in high input power demands and a constrained coverage range [8, 9]. Therefore, improving power transmission efficiency in the RF band remains a critical challenge. Array-based designs have shown promise in achieving efficient transmission but still pose a critical concern, commonly referred to as the beam shaping problem [10, 11]. The objective is to optimize the phase and power distribution within the transmitting array to maximize efficiency. Retro-directive antenna array (RAA) and beamforming (BF) techniques offer a straightforward and efficient approach for rapid alignment and power transmission. Its receiver only needs to send a pilot signal, enabling the transmitting array to determine the optimal phase distribution and focus EM waves on the receiver [12, 13, 14, 15]. To realize this, the Van Atta array [16] and the phase-conjugating array [17] are the most common architectures. These techniques, due to their advantages, have been applied in various contexts such as WPT [18, 19], wireless communications [12], radar [20], RFID systems [21], passive sensors [22], and interference rejection [23, 24]. Given these applications, it is intuitive that a larger receiver may capture more EM power than a small one. However, when using an antenna array at the receiver to further enhance transmission efficiency, the Retro-Directive Beamforming System (RD-BFS) may not yield higher efficiency, as phase irregularities in the target zone degrade power reception [25]. Resonance mechanisms exhibit the characteristics of high-efficiency power and information transfer, where two objects exchange energy, as the resonant field is formed by the superposition of in-phase waves, canceling out other phases [26]. The mechanisms are applicable across various physical systems, such as acoustic, EM, astronomic, and nuclear. Specifically, in the low or medium-frequency bands, the magnetically coupled resonance WPT (MCR-WPT) system [27, 28] establishes resonant coupling between the transmitter and receiver, resulting in improved power transfer and extended transmission distance. In the lightwave band, lasers [29] exploit resonance to improve transmission efficiency over natural light sources. Significantly, the optical resonant beam charging (RBC) system [30] further establishes a long-range and self-alignment WPT system, transferring over 5-W optical power across a 2-m distance with negligible diffraction loss. This optical RBC system also finds applications in communication [31] and positioning [32]. However, due to the large sizes of resonance coils in MCR-WPT [33] and the low electro-optic and photoelectric conversion efficiency in optical systems, the applications in IoT are limited. In the RF band, masers are microwave generators based on stimulated radiation and resonance [34, 35]. However, masers’ integrated resonant metal cavity limits the application as long-range systems. To address these limitations and enhance high-efficiency transmission within the RF band, we propose the RF-based resonant beam system (RF-RBS). This system integrates RAAs and the resonance mechanism to facilitate self-alignment and achieve high-efficiency transmission for wireless power and information transfer. Fig. 1 illustrates a smart home scenario where the RF-RBS is applied, where the base station (BS) is mounted on the ceiling, allowing IoT and mobile devices to be wirelessly charged and communicate simultaneously. The RF-RBS enhances power transfer efficiency and communication reliability while improving radiation safety and reducing EM interference, contributing to the advancement of IoT technologies. The contributions of this work are as follows • We propose an RF-RBS architecture using RAAs and the resonance mechanism, which can achieve watt-level power transmission with higher efficiency and supports high spectral efficiency downlink communication without the need for additional beam control mechanisms. • We developed an analytical model to explain the resonance mechanism based on EM propagation theory and self-reproduction model, enabling the numerical determination of the system’s status and performance. • We analytically characterize the system convergence behavior by taking into account the noise introduced by the RAAs and other components across iterations. Specifically, RF-RBS can converge within a limited number of iterations in the presence of noise. Once the effective operating range is exceeded, the introduced noise significantly reduces the system’s radiated power and transmission efficiency. • Compared to RD-BFS, RF-RBS achieves higher transmission efficiency due to its concentrated power distribution, resulting in up to a 16% improvement. Additionally, increasing the number of antennas in either the BS or the mobile target (MT) can extend the effective operating range of RF-RBS. The remainder of this article is organized as follows. Section II describes the RF-RBS architecture, RAAs, and resonance mechanism. Section III establishes an analytical model to calculate power flow, transmission efficiency, and communication performance. Section IV presents a numerical analysis to demonstrate the superior performance of the RF-RBS compared to the RD-BFS. Finally, Section V concludes the paper. Notation: Boldface lower-case letters are vectors (e.g., \mathbf{s}), whereas boldface upper-case letters are matrices (e.g., \mathbf{H}). \mathbf{I}_{N} is the identity matrix of size N, h_{n,m}=[\mathbf{H}]_{n,m} represents the (n,m)-th element of matrix \mathbf{H}. The notation \mathbf{x}\sim\mathcal{CN}(m,\sigma^{2}) indicates a complex circular symmetric Gaussian random variable (RV) with mean m and variance \sigma^{2}, whereas \mathbf{x}\sim\mathcal{CN}(m,\mathbf{C}) denotes a complex Gaussian random vector with mean m and covariance matrix \mathbf{C}. Constant j=\sqrt{-1} denotes the imaginary unit. The real part of a complex number z is \Re(z). Table I includes the primary system parameters utilized in the subsequent analysis. TABLE I: PRIMARY SYSTEM PARAMETERS Symbol Explanation c Speed of EM wave f_{c}, \lambda Carrier frequency and Wavelength Z_{0} Characteristic impedance \alpha, \beta Path-loss exponent and Scaling factor E, s Signal electric field and Baseband signal p, v, \phi_{0} Signal power, amplitude, and initial phase G_{\text{t}}, G_{\text{r}} Antenna gain L, L_{nm} Distance between transmitter and receiver f_{\text{LO}}, v_{\text{LO}} LO signal frequency and amplitude G_{\text{a}}, \phi_{\text{a}} PA gain and phase-lag k Iterations N, M BS and MT antenna elements \mathbf{H}_{\text{c}} Channel gain matrix \mathbf{H}_{\text{p}} Phase-conjugate transmission matrix \mathbf{n}, F Additive noise and Noise figure v_{\text{m}} Limiter maximum signal amplitude \iota, \gamma Iteration power loss and BS power gain P_{\text{BS}} BS total radiated power P_{\text{MT}} MT total received power W Bandwidth \alpha_{\text{pd}} Feedback ratio \tilde{C} Spectral efficiency"
https://arxiv.org/html/2411.09922v1,Determination and reconstruction of a semilinear term from point measurements,"In this article we study the inverse problem of determining a semilinear term appearing in an elliptic equation from boundary measurements. Our main objective is to develop flexible and general theoretical results that can be used for developing numerical reconstruction algorithm for this inverse problem. For this purpose, we develop a new method, based on different properties of solutions of elliptic equations, for treating the determination of the semilinear term as a source term from a point measurement of the solutions. This approach not only allows us to make important relaxations on the data used so far for solving this class of inverse problems, including general Dirichlet excitation lying in a space of dimension one and measurements located at one point on the boundary of the domain, but it also allows us to derive a novel algorithm for the reconstruction of the semilinear term. The effectiveness of our algorithm is corroborated by extensive numerical experiments. Notably, as demonstrated by the theoretical analysis, we are able to effectively reconstruct the unknown nonlinear source term by utilizing solely the information provided by the measurement data at a single point.Keywords: Inverse Problem, semilinear elliptic equation, uniqueness, stability, numerical analysis Mathematics subject classification 2020 : 35R30, 35J61, 65M32","Let \Omega be a bounded and connected domain of \mathbb{R}^{n}, n\geqslant 2, with C^{2+\alpha} boundary, \alpha\in(0,1), boundary. Let F\in C^{1}({\mathbb{R}}) and let a:=(a_{i,j})_{1\leqslant i,j\leqslant n}\in C^{2}(\overline{\Omega};{\mathbb{R% }}^{n^{2}}) be symmetric, that is a_{i,j}(x)=a_{j,i}(x),\ x\in\Omega,\ i,j=1,\ldots,n, and let a fulfill the ellipticity condition: there exists a constant c>0 such that (1.1) \sum_{i,j=1}^{n}a_{i,j}(x)\xi_{i}\xi_{j}\geqslant c|\xi|^{2},\quad\mbox{for % each $x\in\overline{\Omega},\ \xi=(\xi_{1},\ldots,\xi_{n})\in{\mathbb{R}}^{n}$}. We consider the following boundary value problem (1.2) \left\{\begin{array}[]{ll}-|a|^{\frac{1}{2}}\sum_{i,j=1}^{n}\partial_{x_{i}}% \left(|a|^{-\frac{1}{2}}a_{i,j}(x)\partial_{x_{j}}u_{\delta}(x)\right)=F(u_{% \delta}(x))&\mbox{in}\ \Omega,\\ u_{\delta}=\delta g&\mbox{on}\ \partial\Omega,\end{array}\right. with |a| the absolute of value of the determinant of the matrix a and g\in C^{2+\alpha}(\partial\Omega). In this article, we assume that the data g is chosen in such way that g(\partial\Omega)=[0,R], with R>0, and \delta\in[0,1]. We assume here that F is a non-increasing function and F(0)=0. It is well known that (1.2) admits a unique solution u_{\delta}\in C^{2+\alpha}(\overline{\Omega}) (see [27, Theorem 8.3, pp. 301] for the existence and [11, Theorem 10.7] for the uniqueness), \delta\in[0,1]. Fixing x_{0}\in\partial\Omega, we consider in this article the determination of F on [0,R] from the knowledge of \partial_{\nu_{a}}u_{\delta}(x_{0}), \delta\in[0,1], with \nu(x)=(\nu_{1}(x),\ldots,\nu_{n}(x)) the outward unit normal to \partial\Omega computed at x\in\partial\Omega and \partial_{\nu_{a}}v(x)=\sum_{i,j=1}^{n}a_{i,j}(x)\partial_{x_{j}}v(x)\nu_{i}(x% ),\quad x\in\partial\Omega. More precisely, we study the unique and stable theoretical determination of such class of semilinear terms from this class of data as well as the corresponding numerical reconstruction. Let us recall that semilinear elliptic equations of the form (1.2) can model different physical phenomenon. This includes problems of spreading of biological populations or problems appearing in combustion theory associated with stationary solutions of reaction diffusion equations [37]. This class of equations appear also naturally in many models in Plasma Physics. This includes magnetohydrodynamic equilibrium in a toroidal device (Tokamak) modelled by the so-called Grad-Shafranov equation which can be formulated in terms of a semilinear elliptic equation of the form (1.2) (see e.g. [3, 31, 33]). In all these problems the nonlinear term F plays a fundamental role in the corresponding physical law which explain the necessity of determining such expression. In this article we study the determination of such nonlinear terms from measurements given by measurement at one point at the boundary of the domain and general Dirichlet excitation lying in a space of dimension one. The determination of nonlinear terms appearing in elliptic equations has received a lot of attention this last decades. Most of the results in that direction are uniqueness results based on the linearization method developed by [13] and generalized by [26]. In that direction we can mention the work of [16, 17] that considered the first results of determination of general semilinear terms appearing in an elliptic equation by applying the first order linearization. This approach was then extended by [14, 15] for the determination of semilinear terms depending only on the solution from partial data. More recently, many authors studied the determination of nonlinear terms from partial data or on a manifold by applying the higher order linearization technique and without being exhaustive we can mention the works of [10, 21, 22, 23, 28, 32, 35]. The linearization method has also been used for deriving stable determination of nonlinear terms depending only on the solution by several authors and one can refer for instance to the works [5, 19, 20]. Most of the above mentioned results require an important amount of data that are difficult to compute numerically. In addition, the linearization method used in these theoretical results is not yet well understood in the context of numerical reconstruction and, as far as we know, there has been no numerical reconstruction method for these theoretical results. Some alternative approach to the linearization methods have also been developed for proving the determination of a nonlinear term appearing in an elliptic equation. This includes the work of [2], that strongly relies on the singularities of the domain \Omega and does not work with smooth domains, and the approach of [4, 34] solving this inverse problem with overspecified data. To the best of our knowledge, only the approach of [4, 34] has been exploited for the derivation of a reconstruction algorithm of a quasilinear term in [9, 25]. We are not aware of any article in the mathematical literature studying the numerical reconstruction of a semilinear term appearing in an elliptic equation in accordance with the available theoretical results. The main goal of the present article is to prove theoretical results involving data that can be exploited for numerical reconstruction and to use such data for the derivation of a reconstruction algorithm based on Tikhonov regularization method. Specifically, an iterative thresholding algorithm has been developed and employed to address the nonlinear source inversion problem mentioned above. In recent years, this type of iterative approach has gained considerable attention in solving various inverse problems, including image processing [30], inverse source problems [18], and coefficient identification problems for PDEs [36]. For the convergence analysis of the iterative thresholding algorithm within a general framework, we can refer to [7, 8]. Our analysis is adapted to the determination of semilinear terms depending on the solutions. By employing the iterative thresholding algorithm, extensive numerical examples corroborate our theoretical analysis on the uniqueness and stability results. In particular, it has been demonstrated that a single point of measurement data is sufficient to effectively reconstruct the nonlinear source term."
https://arxiv.org/html/2411.09898v1,A Natural Deep Ritz Method for Essential Boundary Value Problems,"Deep neural network approaches show promise in solving partial differential equations. However, unlike traditional numerical methods, they face challenges in enforcing essential boundary conditions. The widely adopted penalty-type methods, for example, offer a straightforward implementation but introduces additional complexity due to the need for hyper-parameter tuning; moreover, the use of a large penalty parameter can lead to artificial extra stiffness, complicating the optimization process. In this paper, we propose a novel, intrinsic approach to impose essential boundary conditions through a framework inspired by intrinsic structures. We demonstrate the effectiveness of this approach using the deep Ritz method applied to Poisson problems, with the potential for extension to more general equations and other deep learning techniques. Numerical results are provided to substantiate the efficiency and robustness of the proposed method.","In recent years, there has been a rapidly growing interest in using deep neural networks (DNNs) to solve partial differential equations (PDEs). Early attempts to apply neural networks to differential equations date back over three decades, with Hopfield neural networks [11] being employed to represent discretized solutions [17]. Soon after, methodologies were developed to construct closed-form numerical solutions using neural networks [39]. Since then, extensive research has focused on solving differential equations with various types of neural networks, including feedforward neural networks [15, 27, 16, 26], radial basis networks [25], and wavelet networks [20]. With the advancement of deep learning techniques [10, 14, 9], neural networks with substantially more hidden layers have become powerful tools. Innovations such as rectified linear unit (ReLU) functions [6], generative adversarial networks (GANs) [7], and residual networks (ResNets) [9] exemplify these advances, showcasing the strong representational capabilities of DNNs [30, 18, 19, 37, 8, 33]. These developments have spurred the creation of numerous DNN-based methods for PDEs, including the deep Galerkin method (DGM) [35], deep Ritz method (DRM) [5], physics-informed neural networks (PINNs) [31], finite neuron method (FNM) [40], weak adversarial networks (WANs) [42], and mixed residual methods (MIM) [24]. These methods have been widely adopted across various applications, successfully addressing complex problems modeled by differential equations [5, 21, 32, 2, 22, 13, 41, 3]. In the design and implementation of neural network-based methods, the imposition of boundary conditions is a critical challenge. Notably, this issue is also encountered in certain classical numerical methods, such as finite element methods, where handling boundary conditions can be complex enough to require techniques like Nitsche’s method [28], later refined by Stenberg [36]. However, the challenges differ significantly in neural network-based approaches. Unlike classical numerical methods, which leverage basis functions or discretization stencils with compact supports or sparse structures, neural network methods utilize DNNs as trial functions, which are globally defined. Consequently, enforcing boundary conditions, even for problems that are straightforward in classical methods, becomes nontrivial due to the global structure of DNNs. For the natural boundary conditions, the deep Ritz method reformulates the original problem into a variational form, which can reduce the smoothness requirements and potentially lower the training cost by allowing natural boundary conditions to be imposed without additional operations. However, because the trial functions within the approximation sets are generally non-interpolatory, imposing essential boundary conditions remains a challenging task. To date, three primary approaches have been developed for addressing essential boundary conditions in deep learning-based numerical methods. The first approach is the conforming method, which aims to construct neural network functions that exactly satisfy the essential boundary conditions [34, 2, 24]. Generally, the network function u_{NN}(x) is represented as the combination of two parts: \displaystyle u_{NN}(x)=u_{b}(x)+d_{\Gamma}(x)u^{0}_{NN}(x), one reflecting the essential boundary condition, and the other vanishing on the boundary \Gamma by the aid of a “distance function” or a “geometry-aware” function d_{\Gamma}(x). Both test and trial functions can be constructed this way. However, when the domain has a complicated boundary (or even not that complicated), it is not easy to construct a distance function to preserve the asymptotic equivalence. Another one is the penalty method, which is a very general concept and belongs to the so-called nonconforming method [5, 31, 35, 43, 42, 12]. For this method, an additional surface term is introduced into the variational formulation to enforce the boundary conditions. Take the Poisson equation with Dirichlet boundary condition (1.1) as example: \left\{\begin{array}[]{rll}-\Delta u&=f&\mbox{in}\,\Omega,\\ u&=g&\mbox{on}\,\Gamma=\partial\Omega.\end{array}\right. (1.1) The deep Ritz method [5] minimize the following objective \mathcal{L}_{DRM}(u)=\biggl{[}\sum_{x_{j}\in\mathcal{D}}\frac{1}{2}|\nabla u(x% _{j})|^{2}-f(x_{j})u(x_{j})\biggr{]}+\beta\sum_{x_{j}\in\mathcal{D}_{\Gamma}}% \bigl{(}u(x_{j})-u_{b}(x_{j})\bigr{)}^{2}, (1.2) where \mathcal{D} and \mathcal{D}_{\Gamma} define the training data set in the domain and on the boundary, respectively. PINN method is a least square method for the strong form of the PDE, but the the handling of the essential boundary condition is similar to deep Ritz method: \mathcal{L}_{PINN}(u)=\biggl{[}\sum_{x_{j}\in\mathcal{D}}|\Delta u(x_{j})+f(x_% {j})|^{2}\biggr{]}+\beta\sum_{x_{j}\in\mathcal{D}_{\Gamma}}\bigl{(}u(x_{j})-u_% {b}(x_{j})\bigr{)}^{2}. (1.3) Careful balancing of terms within the functional framework is essential to ensure the well-posedness and accuracy of the scheme. Addressing this issue, the deep Nitsche method, as proposed in [21], applies Nitsche’s variational formula to second-order elliptic problems to avoid the use of a large penalty parameter. Nevertheless, some degree of tuning remains necessary for the penalty parameter, and a theoretical basis for determining an optimal penalty value is still absent. In contrast to the penalty method, the Lagrange multiplier method addresses essential boundary conditions by treating them as constraints within the minimization process. This method has been effectively used to impose essential boundary conditions in finite element methods [1] and wavelet methods [4]. When the approximation function spaces are appropriately chosen satisfying the so-called inf-sup condition, this method can achieve optimal convergence rates [1, 4]. While the Lagrange multiplier method can also enforce boundary conditions in neural network-based methods, its effectiveness depends on the stable construction and efficient resolution of the extra constrained optimization problem. In this paper, we introduce a novel neural network-based method for solving essential boundary value problems. Our approach involves transforming the original problem into a sequence of natural boundary value problems, which are then solved sequentially or concurrently using the deep Ritz method. Unlike the previously mentioned approaches, this technique constructs a new framework for imposing essential boundary conditions. We refer to this method as the natural deep Ritz method. This approach simplifies the training process and avoids introducing additional errors associated with boundary condition enforcement. To validate our method, we examine essential boundary and interface value problems for second-order divergence-form equations with constant, variable, or discontinuous coefficients, providing numerical examples that demonstrate the effectiveness. Evidently, a primary ingredient of the proposed method lies in its adjoint approach to handling essential boundary conditions. This approach is grounded in the mathematical framework of the de Rham complex and its dual complex, which serve as foundational structures. By leveraging these complexes, which connect kernel spaces to specific range spaces, we can represent the difference between the solutions of natural and essential boundary value problems as the solution to another natural boundary value problem. This formulation allows us to construct a purely natural approach equivalent to the original problem. While we do not delve extensively into the formal structure of the de Rham and dual complexes, it is important to highlight that our method diverges from the traditional mixed formulations common in classical numerical methods. Notably, we do not introduce the gradient of the unknown function as an auxiliary variable. Moreover, unlike classical mixed formulations, our approach avoids the need for constructing a saddle point problem, which would typically require rigorous continuous and discrete inf-sup conditions for stability and accuracy. In our framework, the solution is reduced to solving three elliptic subproblems using a standard machine learning algorithm. This approach eliminates the need for training an additional network to capture the boundary representation, tuning penalty parameters, or ensuring inf-sup conditions for a boundary Lagrangian multiplier. The conciseness of the present method is among its most significant advantages, both in theory and implementation. The remaining parts of the paper are organized as follows. In Section 2, we present the equivalent natural boundary value problem formulation of the respective essential boundary value problems. In Section 3, the deep Ritz method based on the natural formulation, namely the natural deep Ritz methods, is given. Numerical experiments are presented in Section 4 to verify the proposed method. We end the paper with some concluding remarks in Section 5"
https://arxiv.org/html/2411.09828v1,"Analysis of the Streamline Upwind/Petrov Galerkin Method
Applied to the Solution of Optimal Control Problems","We study the effect of the streamline upwind/Petrov Galerkin (SUPG) stabilized finite element method on the discretization of optimal control problems governed by linear advection-diffusion equations. We compare two approaches for the numerical solution of such optimal control problems. In the discretize-then-optimize approach the optimal control problem is first discretized, using the SUPG method for the discretization of the advection-diffusion equation, and then the resulting finite dimensional optimization problem is solved. In the optimize-then-discretize approach one first computes the infinite dimensional optimality system, involving the advection-diffusion equation as well as the adjoint advection-diffusion equation, and then discretizes this optimality system using the SUPG method for both the original and the adjoint equations. These approaches lead to different results. The main result of this paper are estimates for the error between the solution of the infinite dimensional optimal control problem and their approximations computed using the previous approaches. For a class of problems prove that the optimize-then-discretize approach has better asymptotic convergence properties if finite elements of order greater than one are used. For linear finite elements our theoretical convergence results for both approaches are comparable, except in the zero diffusion limit where again the optimize-then-discretize approach seems favorable. Numerical examples are presented to illustrate some of the theoretical results.","This paper is concerned with the accuracy of numerical solutions of optimal control problems governed by the advection-diffusion equation. Specifically, we are interested in the effect of the streamline upwind/Petrov Galerkin (SUPG) stabilized finite element method on the discretization of the optimal control problem. To be more precise, we consider the linear quadratic optimal control problem \min\frac{1}{2}\int_{\Omega}(y(x)-\widehat{y}(x))^{2}dx+\frac{\omega}{2}\int_{% \Omega}u^{2}(x)dx (1.1) subject to \displaystyle-\epsilon\Delta y(x)+\mathbf{c}(x)\cdot\nabla y(x)+r(x)y(x) \displaystyle=f(x)+u(x), \displaystyle x \displaystyle\in\Omega,\; (1.2a) \displaystyle y(x) \displaystyle=d(x), \displaystyle x \displaystyle\in\Gamma_{d}, (1.2b) \displaystyle\epsilon\frac{\partial}{\partial\mathbf{n}}y(x) \displaystyle=g(x), \displaystyle x \displaystyle\in\Gamma_{n}, (1.2c) where \Gamma_{d}\cap\Gamma_{n}=\emptyset, \Gamma_{d}\cup\Gamma_{n}=\partial\Omega, \mathbf{c},d,f,g,r,\widehat{y} are given functions, \epsilon,\omega>0 are given scalars, and \mathbf{n} denotes the outward unit normal. Assumptions on these data that ensure the well-posedness of the problem will be given in the next section. For advection dominated problems the standard Galerkin finite element method applied to the state equation (1.2) produces strongly oscillatory solutions, unless the mesh size h is chosen sufficiently small relative to \epsilon/\|\mathbf{c}(x)\|, x\in\Omega. To produce better approximations to the solution of (1.2) for modest mesh sizes, various augmentations of the standard Galerkin finite element method have been proposed. For an overview see [11, 12, 13]. In this paper we focus on the streamline upwind/Petrov Galerkin (SUPG) method of Hughes and Brooks [2]. The SUPG method adds to the weak form of the state equation (1.2) a term with the properties that (a) the weak form of the modification has better stability properties than the bilinear form associated with (1.2) and (b) the added term evaluated at the exact solution of (1.2) vanishes. Because of these properties the SUPG method is called a strongly consistent stabilization method [12]. For the numerical solution of the optimal control problem there are at least two approaches. In the first approach, called the optimize-then-discretize approach, one first derives the optimality conditions for (1.1), (1.2). In Section (2.1) we will see that the optimality conditions consist of the state equation (1.2), the adjoint partial differential equation (PDE) \displaystyle-\epsilon\Delta\lambda(x)-\mathbf{c}(x)\cdot\nabla\lambda(x)+(r(x% )-\nabla\cdot\mathbf{c}(x))\lambda(x) \displaystyle=-(y(x)-\hat{y}(x)), \displaystyle x \displaystyle\in\Omega, (1.3a) \displaystyle\lambda(x) \displaystyle=0, \displaystyle x \displaystyle\in\Gamma_{d}, (1.3b) \displaystyle\epsilon\frac{\partial}{\partial\mathbf{n}}\lambda(x)+\mathbf{c}(% x)\cdot\mathbf{n}(x)\;\lambda(x) \displaystyle=0, \displaystyle x \displaystyle\in\Gamma_{n} (1.3c) and the gradient equation \displaystyle\lambda(x)=\omega u(x)\qquad x\in\Omega. (1.4) Then one discretizes each equation (1.2), (1.3) and (1.4), using possibly different discretization schemes for each one. Since the adjoint equation (1.3) is also an advection-diffusion equation, but with advection -\mathbf{c}, we discretize it using the SUPG method. If we proceed this way, the optimize-then-discretize approach leads to a discretization of the optimality system (1.2), (1.3), (1.4) that is strongly consistent. However, this discretization of the optimality system (1.2), (1.3), (1.4) leads to a nonsymmetric linear system, which implies that there is no finite dimensional optimization problem for which this discretization of (1.2), (1.3), (1.4) is the optimality system. The details of the optimize-then-discretize approach will be discussed in Section 2.4. In the other approach for the numerical solution of (1.1), (1.2) called the discretize-then-optimize approach, one first discretizes the state equation using SUPG and the objective function and then solves the resulting finite dimensional optimization problem. The optimality conditions of this finite dimensional optimization problem contain equations, which we call the discrete adjoint equation and the discrete gradient equation, that can be viewed as discretizations of (1.3) and (1.4), respectively. The SUPG stabilization term added to the state equation (1.2) produces a contribution to the discrete adjoint equation and to the discrete gradient equation. This contribution to the discrete adjoint equation has a stabilizing effect, but the discrete adjoint equation is in general not a strongly consistent stabilization method for (1.3). We will give a detailed discussion of the optimize-then-discretize approach in Section 2.3. The main goal of this paper is to derive estimates of the error between the solution y,u,\lambda of the infinite dimensional optimality system (1.2), (1.3), (1.4) and their approximations computed using both, the discretize-then-optimize as well as the optimize-then-discretize approach. Such error estimates will be provided in Section 4. Section 5 contains a few numerical results that illustrate our theoretical findings. We will see that even in our simple model problem (1.1), (1.2) differences can arise between the discretize-then-optimize and the optimize-then-discretize approach. It is important to understand and analyze these to better assess the implication of numerical solution approaches to much more complicated optimal control or optimal design problems that involve nonlinear state equations solved using stabilization techniques. We also note that the general issues described here for the SUPG stabilization also arise when other stabilizations are used, such as the Galerkin/Least-squares (GLS) method of Hughes, Franca and Hulbert [6] and the stabilization method of Franca, Frey and Hughes [4]. Throughout this paper we use the following notation for norms and inner products. We define \langle f,g\rangle_{G}=\int_{G}f(x)g(x)dx, \|v\|_{0,\infty,G}=\mbox{ ess sup}_{x\in G}|v(x)| or \|{\bf v}\|_{0,\infty,G}=\mbox{ ess sup}_{x\in G}\sqrt{\sum_{i}v_{i}(x)^{2}} for vector valued {\bf v}, and \|v\|_{k,G}=\left(\sum_{|\alpha|\leq k}\int_{G}(\partial^{\alpha}v(x))^{2}dx% \right)^{1/2},\qquad|v|_{k,G}=\left(\sum_{|\alpha|=k}\int_{G}(\partial^{\alpha% }v(x))^{2}dx\right)^{1/2}, where G\subset\Omega\subset I\!\!R^{d} or G\subset\partial\Omega and \alpha\in I\!\!N_{0}^{d} is a multi-index, |\alpha|=\sum_{i=1}^{d}\alpha_{i}, and \partial^{\alpha}=\partial^{\alpha_{1}}\ldots\partial^{\alpha_{d}}. If G=\Omega we omit G and simply write \langle f,g\rangle, etc."
https://arxiv.org/html/2411.09769v1,Reduced-order modelling of parameter-dependent systems with invariant manifolds: application to Hopf bifurcations in follower force problems,"The direct parametrisation method for invariant manifolds is adjusted to consider a varying parameter. More specifically, the case of systems experiencing a Hopf bifurcation in the parameter range of interest are investigated, and the ability to predict the amplitudes of the limit cycle oscillations after the bifurcation is demonstrated. The cases of the Ziegler pendulum and Beck’s column, both of which have a follower force, are considered for applications. By comparison with the eigenvalue trajectories in the conservative case, it is advocated that using two master modes to derive the ROM, instead of only considering the unstable one, should give more accurate results. Also, in the specific case where an exceptional bifurcation point is met, a numerical strategy enforcing the presence of Jordan blocks in the Jacobian matrix during the procedure, is devised. The ROMs are constructed for the Ziegler pendulum having two and three degrees of freedom, and then Beck’s column is investigated, where a finite element procedure is used to space discretize the problem. The numerical results show the ability of the ROMs to correctly predict the amplitude of the limit cycles up to a certain range, and it is shown that computing the ROM after the Hopf bifurcation gives the most satisfactory results. This feature is analyzed in terms of phase space representations, and the two proposed adjustments are shown to improve the validity range of the ROMs.","Nonlinear techniques for simulation-free model order reduction (MOR) based on the parametrisation method for invariant manifolds, show several successful applications in recent years, especially for vibrating systems [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], but also for different domains like fluid dynamics with the Navier-Stokes equation [14] or astronomy [15]. In the field of nonlinear vibrations, all these works complement and improve the earlier developments defining and using Nonlinear Normal Modes (NNMs) as invariant manifolds for accurate derivations of reduced order models (ROMs) [16, 17, 18, 19, 20, 21, 22] thanks to the following decisive advances. First, the uniqueness of the invariant manifold has been theoretically proven in [20] thanks to the parametrisation method, yielding spectral submanifolds (SSMs) as the unique smoothest subspace, which is thus the sought and already used NNM of earlier works. Second, arbitrary order expansions have been devised, allowing automated powerful results with a fine convergence as long as the validity limit of the local theory is fulfilled [1, 6, 23]. Finally, methods that are directly applicable to a Finite Element (FE) discretisation, of broad use for engineering structures, have been developed [4, 5, 24]. In the previously cited contributions, most of the developments were concerned with computing the invariant manifolds of a fixed point representing the structure at rest. In this realm, NNMs offer a clear and direct continuation of the idea of linear normal modes. While the majority of the developments for vibrating structures were concerned with geometric nonlinearity, recent applications extend the approach and consider coupled systems or different physics. For instance, friction is considered in [8, 25], a weak piezo-electric coupling is addressed in [11], while a strong electromechanical coupling with application to Micro-Electro-Mechanical Systems (MEMS), is derived in [12]. The treatment of parameter-dependent ROMs using either the center manifold approach or the normal form theory has already been considered in the past. For example, the idea of adding the parameter as an additional state variable with trivial dynamics is already addressed in [26] in the context of center manifold reduction of a simple system experiencing a Hopf bifurcation, and the technique has been applied to friction-induced vibration in braking systems in [27]. In the context of forced systems, the external forcing has been added as an extra oscillator for the computation of NNMs with the center manifold technique in [28]. Considering now normal form theory, parameter-dependent cases in nonlinear vibrations, including bifurcations leading either to divergence or flutter, were already investigated in [29, 30], where the particular case of the Ziegler pendulum was considered. Additionally, other treatments in the realm of parameter-dependent systems can also be found for example in [31, 32], including the case of external forcing. Now focusing on the more general context of the parametrisation method for invariant manifolds, parameter-dependent problems have been less frequently addressed. For example, in the non-autonomous case, where the excitation frequency can be seen as a varying parameter, different treatments have been proposed. In [24], the ROM is computed for a single value and then used to compute bifurcation diagrams with numerical continuation for slight variations in the vicinity of the expansion point. On the other hand, the invariant manifolds are computed for each forcing frequency in [33, 34]. For rotating systems, interpolations between different ROMs computed at selected rotating speeds have proven effective in [35]. On the other hand, MOR techniques using manifolds and embeddings that are especially concerned with large parameter variations have also been exploited in different fields, see e.g. [36, 37, 38] and references therein. The previously cited approaches to deal with parameter-dependent ROMs rarely consider the cases where the fixed point, in the vicinity of which the NNMs are computed, experiences a bifurcation in the considered parameter range. However, bifurcations of the fixed point are commonly encountered in diverse mechanical situations [39], for instance in the case of buckling, where the position of the structure at rest becomes unstable for an increasing value of the load. Another case of interest is that of the flutter, where a Hopf bifurcation occurs with the birth of limit cycle oscillations. This case is typically encountered for a wing subjected to a uniform flow with increasing velocity [40, 41], but also for pipes conveying fluid [42, 43], or structures actuated with follower forces as the Ziegler pendulum [44, 45, 46] or Beck’s column [47, 48] for instance. In such a case, the parametrisation method for invariant manifolds needs to be revisited to account for this major change in the dynamics. For a system encountering a Hopf bifurcation, a first important step has been proposed in [49] with application to a pipe conveying fluid. The parametrisation method has been computed by selecting the unstable mode as the master one, and adjusted by incorporating the bifurcation parameter as an additional variable with trivial dynamics, in a treatment resembling the so-called “suspension trick” largely used in dynamical systems, see e.g. [26, 28, 24]. In particular, the parametrisation method is computed for a given value of the flow velocity, and the ROM is then used to predict the amplitudes of the limit cycles after the Hopf bifurcation. Interestingly, it has been found that the best results are obtained when the parametrisation point is selected after the bifurcation, meaning that the unstable invariant manifold of the unstable fixed point provides a better approximation of the limit cycles than those obtained using the center manifold computed exactly at the Hopf bifurcation parameter value. This contribution aims to elaborate further on the use of MOR techniques for vibrating systems experiencing a Hopf bifurcation, using the direct parametrisation method for invariant manifolds (DPIM). The bifurcation parameter is introduced as an added variable with trivial dynamics, and two distinctive features are addressed as compared to [49]. First, it is shown how considering two master modes can improve the predictions. The two modes are selected from the inspection of the undamped or lightly damped problem since this framework uncovers the coalescence of frequencies, at the heart of the instability, and the presence of a 1:1 resonance between the two bifurcating modes. Even if this information is lost when damping is added, it is shown to improve the results of the ROM, underlining that the near 1:1 resonance affects the quality of the ROM. Second, in the case of a perfect frequency coalescence, degenerate eigenvalues occur and Jordan blocks appear in the linear part of the dynamics. In such a case, a numerical strategy that enforces the presence of Jordan blocks is tested to improve the results. Finally, the findings are numerically assessed in three cases: the Ziegler pendulum with two and three degrees of freedom (DOF), and a cantilever beam with a follower force (Beck’s column). In the latter case, the problem is discretised by the finite element method to illustrate an application to a numerical problem featuring a large number of DOF."
https://arxiv.org/html/2411.10363v1,Scrambled Halton Subsequences and Inverse Star-Discrepancy,"Braaten and Weller discovered that the star-discrepancy of Halton sequences can be strongly reduced by scrambling them. In this paper, we apply a similar approach to those subsequences of Halton sequences which can be identified to have low-discrepancy by results from p-adic discrepancy theory. For given finite N, it turns out that the discrepancy of these sets is surprisingly low. By that known empiric bounds for the inverse star-discrepancy can be improved.","Let (x_{n})\subset[0,1] be a uniformly distributed sequence, i.e. for all 0\leq a<b<1 it holds \lim_{N\to\infty}\frac{\#\left\{1\leq n\leq N\,:\,x_{n}\in[a,b)\right\}}{N}=b-a. It is well-known that for every uniformly distributed sequence (x_{n}) there exists a re-ordering of its elements (y_{n}):=(x_{\sigma(n)}) by some bijection \sigma:\mathbb{N}\to\mathbb{N} such that its star-discrepancy D_{N}^{*}(y_{n}):=\sup_{0<b\leq 1}\left|\frac{\#\left\{1\leq n\leq N\,:\,y_{n}% \in[0,b)\right\}}{N}-b\right| is of the best possible order D_{N}^{*}(y_{n})=O(\log(N)/N), see [Sch72]. Sequences satisfying this asymptotic property are called low-discrepancy sequences. Similarly, the (extreme) discrepancy is defined by D_{N}(y_{n}):=\sup_{0\leq a<b\leq 1}\left|\frac{\#\left\{1\leq n\leq N\,:\,y_{% n}\in[a,b)\right\}}{N}-(b-a)\right|, i.e. the boxes in the supremum are not necessarily anchored at 0. It relates to the star-discrepancy via D_{N}^{*}(y_{n})\leq D_{N}(y_{n})\leq 2D_{N}^{*}(y_{n}). Thus, the two notions of discrepancy necessarily posses the same asymptotic behavior up to a factor. If (x_{n}) is a low-discepancy sequence, one might now ask if the star-discrepancy of a subsequence (y_{n})=(x_{\tau(n)}) for some injective map \tau:\mathbb{N}\to\mathbb{N} remains a low-discrepancy sequence. This question is rather classical for Kronecker sequence (x_{n})=\{n\alpha\} with \alpha\in\mathbb{R}\setminus\mathbb{Q}, where \{\cdot\} denotes the fractional part of a number, and has been extensively treated in the literature for a long time, see e.g. [Bak81, BP94, AL16] to name only a few references. For another class of low-discrepancy sequences, namely van der Corput sequences, answers have been given far more recently, see e.g. [HKLP09, Pil12, Wei24] although the question was implicitly already covered much earlier in [Mei68]. In this paper, we will be mainly interested in the latter examples. Recall that for an integer b\geq 2 the b-ary representation of n\in{\mathbb{N}} is n=\sum_{j=0}^{\infty}e_{j}b^{j} with 0\leq e_{j}=e_{j}(n)<b. Based on the radical-inverse function, the van der Corput sequence in base b is defined by \varphi_{b}(n)=\sum_{j=0}^{\infty}e_{j}b^{-j-1} for all n\in{\mathbb{N}}. It is well-known, see e.g. [Nie92], that D_{N}^{*}(\varphi_{b}(n))\leq\frac{1}{N}+\frac{b+1}{2N}+\frac{b-1}{2\log(b)}% \frac{\log(N)}{N}. This bound should be compared to (and is for small b not too far off from) the record holder for the best known discrepancy as constructed in [Ost09] which satisfies \limsup_{N\to\infty}\frac{ND_{N}^{*}(x_{n})}{\log(N)}\approx 0.222223. In fact, this record holder is a generalized van der Corput sequence. In other words, van der Corput, their subsequences and generalizations may be regarded as prime candidates when looking for sequences with a particularly small star-discrepancy. An easier-to-describe method than in [Ost09] how to (empirically) further reduce the discrepancy of van der Corput was introduced in [BW79]: for fixed b\in\mathbb{N} choose an arbitrary permutation \pi of \{0,\ldots,b-1\} and define the scrambled van der Corput sequence by \displaystyle\varphi_{b,\pi}(n):=\sum_{j=0}^{\infty}\pi(e_{j})b^{-j-1}. (1) It is not difficult to prove the same bound for the star-discrepancy as for the standard van der Corput sequence but the additional parameter allows for (empiric) reduction of the discrepancy. In [BW79], concrete choices for \pi were suggested for all primes p\leq 53. The concept of scrambling may, of course, also be applied to subsequences of van der Corput and we obtain the following result which may be regarded as a generalization of Theorem 1.2 in [Wei24] in combination with Theorem 3 in [Mei68] as will become clear from its proof. Theorem 1. Let f(n) be a permutation polynomial \bmod\ p^{2} for some prime number p, which means that f(n) induces a bijection on \mathbb{Z}/p^{2}\mathbb{Z}, and let \pi be a permutation of \{0,\ldots,p-1\}. Then the discrepancy of the sequence (x_{n}):=(\varphi_{p,\pi}(f(n))) satisfies D_{N}(x_{n})\leq\frac{p-1}{2\log(p)}\frac{\log(N)}{N}+O\left(\frac{1}{N}\right). We call such sequences scrambled van der Corput subsequences. The easiest examples are of the form f(n)=an+b with \gcd(a,p)=1. We call a a shift and denote such a sequence by \varphi_{p,\pi,a}. Other possible choices for f(n) (depending on p) may be found in [Wei24]. Although the theoretical bound does not guarantee a very small discrepancy, allowing the flexibility in choosing the parameters \pi and a in Theorem 1 reduces the star-discrepancy for given N significantly as can be seen from Table 1 where our results are compared to the approach from [BW79]. The difference between the original van der Corput sequence and the scrambled one according to [BW79] is much larger (except for p=2), but moving to subsequences always but once reduced the star-discrepancy as well. In our experiments the gain exceeded 10\% most of the times. p \varphi_{p}(N) \varphi_{p,\pi}(N) \varphi_{p,\pi,a}(N) 2 0.0231 0.0231 0.0143 3 0.0262 0.0199 0.0140 5 0.0160 0.0120 0.0100 7 0.0310 0.0182 0.0107 11 0.0321 0.0198 0.0111 13 0.0563 0.0189 0.0137 17 0.0503 0.0144 0.0121 19 0.0731 0.0153 0.0125 23 0.0846 0.0150 0.0144 29 0.0982 0.0167 0.0162 p \varphi_{p}(N) \varphi_{p,\pi}(N) \varphi_{p,\pi,a}(N) 2 0.0025 0.0025 0,0018 3 0.0031 0.0028 0,0018 5 0.0025 0.0018 0,0010 7 0.0042 0.0023 0,0018 11 0.0049 0.0024 0,0018 13 0.0046 0.0019 0,0017 17 0.0086 0.0022 0,0021 19 0.0095 0.0019 0,0020 23 0.0093 0.0022 0,0020 29 0.0123 0.0025 0,0023 Table 1: Comparison of discrepancies for van der Corput like sequences for N=100 (left) and N=1,000 (right). For \varphi_{p,\pi,a}(N) the smallest found values are listed when using 500 different shifts, 500 random permutations per shift. For a fair comparison with the original and scrambled van der Corput sequence it should however be born in mind that the star-discrepancy was by our method minimized for a given N which might mean that it is particularly small for the chosen N but not for many other n<N. However, we observe in Figures 1 and 2 of the Appendix that both, the scrambled sequence as well as the scrambled subsequence, almost systematically outperform the original van der Corput sequence while the former two methods have the lowest star-discrepancy for approximately the same number of n\leq 1,000. Finding sequences with a particularly small discrepancy is not only a relevant question in dimension d=1 but even of higher importance for d>1, where the star-discrepancy for a sequence (x_{n})\subset[0,1]^{d} and the d-dimensional Lebesgue measure \lambda_{d}(\cdot) is defined by D_{N}^{*}(x_{n}):=\sup_{B\subset[0,1)^{d}}\left|\frac{\#\left\{1\leq n\leq N\,% :\,x_{n}\in B\right\}}{N}-\lambda_{d}(B)\right|, where the supremum is taken over all d-dimensional intervals B=[0,b_{1})\times\ldots\times[0,b_{d}) with 0\leq b_{i}\leq 1 for i=1,\ldots,d. It is widely conjectured that D_{N}^{*}(x_{n})=O(\log(N)^{d}/N) is the best achievable order of convergence for the star-discrepancy but this conjecture has only been proven in the case d=1, see [Sch72]. Furthermore, we remind the reader that the star-discrepancy of sequences (infinitely many points) in dimension d corresponds to that of point sets (finitely many points) in dimension d+1, see [Rot54]. The (extreme) discrepancy D_{N}(x_{n}) again allows arbitrary multi-dimensional half-open intervals in the supremum, i.e. without necessarily anchoring one base point at zero. As a generalization of the one-dimensional case, the inequality D_{N}^{*}(x_{n})\leq D_{N}(x_{n})\leq 2^{d}D_{N}^{*}(x_{n}) holds. Sequences with a particularly small (star-)discrepancy are of great interest for high-dimensional integration tasks. Due to the Koksma-Hlawka inequality, see e.g. [KN74], the worst case error when approximating an integral by the average of some function evaluations depends linearly on the discrepancy of the evaluation points. This motivates why it makes sense to ask, what is the smallest star-discrepancy achievable for a given N\in\mathbb{N} and we set D^{*}(N,d):=\inf\{D_{N}^{*}(P)\,:\,P\subset[0,1]^{d}\#P=N\}. or equivalently define the inverse star-discrepancy by N^{*}(\varepsilon,d):=\inf\{N\in\mathbb{N}\,:\,D^{*}(N,d)\leq\varepsilon\}, which is the minimum number of sample points that guarantees a discrepancy bound of at most \varepsilon>0. Note that even small reductions of N are of practical importance, if an expensive function f is evaluated at these points to (numerically) calculate an integral. Alternatively decreasing the size of N can be motivated by the numerical stability of certain regression problems which depend linearly on the discrepancy, see [WN19]. Therefore, precise theoretical bounds and numerical estimates of N^{*}(\varepsilon,d) are of great interest. As the asymptotic bound \log(N)^{d}/N for the star-discrepancy exponentially depends on the dimension, it does not provide helpful information for moderate size sets. In a seminal paper, [HNWW00], an alternative upper bound for the smallest achievable star-discrepancy of the form \displaystyle D_{N}^{*}(P)\leq c\sqrt{\frac{d}{N}} (2) for some constant c>0 was shown without giving an explicit value for c. This implies N^{*}(\varepsilon,d)\leq\lfloor c^{2}d\varepsilon^{-2}\rfloor for all d,N\in\mathbb{N} and \varepsilon\in[0,1). Currently the best known value for the constant is c=2.4968 according to [GPW21]. This observation can also be used to check whether a given point set or sequence is good in the sense that its empirically observed star-discrepancy is close to or even smaller than the upper bound. As a special example for this idea it was shown in [GGPP21] that the sequence generated by a secure bit generator is good up to dimension at least d=15, because its discrepancy is smaller than \sqrt{\frac{d}{N}} even for relatively big N. Finding Multi-Dimensional Sets with Small Star-Discrepancy. A classical example of multi-dimensional low-discrepancy sequences are the so-called Halton sequences which are the main object of study in this paper: for a given dimension d, let b_{1},\ldots,b_{d} be pairwise relatively prime integers. The Halton sequence (H^{b}_{n}) in the base b=(b_{1},\ldots,b_{d}) is given by x_{n}:=(\varphi_{b_{1}}(n),\ldots,\varphi_{b_{d}}(n)) for all n\geq 1. Scrambled Halton sequences are then defined by choosing a permutation \pi_{b_{i}} for each i\in\{1,\ldots,d\}. Theorem 1 easily generalizes from van der Corput sequences to higher dimensions by the work of Meijer. Indeed, Theorem 5 in [Mei68] can be applied to obtain the following version in several dimensions. Theorem 2. Let p_{1},\ldots,p_{d} be distinct prime numbers and let \pi_{i} be an arbitrary permutation of \{0,\ldots,p_{i}-1\} for each i=1,\ldots,d. Moreover, let f_{i}(n):\mathbb{Z}\to\mathbb{Z} be a permutation polynomial \bmod\ p_{i}^{2}. Then (x_{n}):=(\varphi_{p_{1},\pi}(f_{1}(n)),\ldots,\varphi_{p_{d},\pi}(f_{d}(n))) satisfies D_{N}(x)\leq\frac{\log(N)^{d}}{N}\prod_{j=1}^{d}\frac{2(p_{i}-1)}{\log(p_{i})}% +O\left(\frac{\log(N)^{d-1}}{N}\right). Thus, all scrambled Halton subsequences of the form f(n)=(a_{1}\cdot n,a_{2}\cdot n,\ldots a_{d}\cdot n) with \gcd(a_{i},p_{i})=1 for i=1,\ldots,d are low-discrepancy sequences. We call these a_{i} shifts. Remark 3. Note that for the original Halton sequence, the pre-factor in front of \log(N)^{d}/N is known to be smaller by 2^{2d}, if we look at the star-discrepancy instead of the discrepancy, see e.g. [KN74]. Also in the multi-dimensional setting, the permutations \pi_{i} and the shifts a_{i} may be chosen in order to minimize the star-discrepancy. Our numerical results in this setting are promising and we obtain values for the discrepancy which are at least comparable to other recent methods for finding sets with small discrepancy, see [DR13], [CVJ+23]. As we want to analyze this case extensively but not overload the introduction, we postpone the detailed discussion to Section 2. A mathematical application. Since finding the (empiric) small values for the (inverse) star-discrepancy requires an extensive search, it can only cover some cases N,d\in\mathbb{N}. As a theoretical application of Halton (sub-)sequences, they can be used to improve the value of c in the theoretical bound (2). In fact, numerical calculations for finitely many N can cover those cases for which the application of bounds like in Theorem 2 is not sufficient. Theorem 4. Let d,N\in\mathbb{N}. Then there exists a point set P\subset[0,1]^{d} such that D_{N}^{*}(P)\leq 2.4631837\sqrt{\frac{d}{N}}. The general structure of the proof for Theorem 4 is in parallel to [GPW21]. However, we add some additional ideas to improve the bound: we use Halton sequences and improved bounds on their discrepancy from [Ata04] to address the case d\leq 4 while only the case d=1 had been addressed separately before. Moreover, we were able to slightly sharpen the arguments to derive the bounds. Finally, we use a very recent result on bracketing numbers from [Gne24]. We will discuss the details in Section 3. It is natural to ask why our approach is only applied to the case d\leq d_{0}=4. From a theoretic viewpoint, there is no reason for this choice and we would expect that (scrambled) Halton (sub-)sequences could probably be used for any finite d_{0}: given d_{1}\leq d_{0}, the bound from [Ata04] can be applied to prove an analogue of Theorem 4 for all but finitely many N\in\mathbb{N}. However, these finitely many exceptions need to be checked on a computer. In [GSW09] it was proven that calculating the (star-)discrepancy is an NP-hard problem. Indeed, all known algorithms for calculating the star-discrepancy have exponential run time. The currently best known one was introduced in [DEM96] and is also called DEM algorithm named after its inventors Dobkin, Epstein and Mitchell. Its runtime is of the order O(N^{1+d/2}), where N is the cardinality of the set, compare [CVJ+23]. For our calculations in higher dimensions, see Section 2, we use a more recent implementation of the DEM algorithm based on earlier work of Magnus Wahlström publicly available under [CVdN+23]. We do not describe its details here but refer to the description of the DEM algorithm in [DGW14]. As the runtime of the DEM algorithm is of order O(N^{d/2+1}) our approach seems to be infeasible from some dimension on. If we assume that it is conducted up to e.g. d_{0}=9, then the constant c would go down to approximately 2.4543. For higher dimensions and relatively large N, also the (exact) DEM algorithm becomes infeasible and the star-discrepancy can only be estimated approximately then. In this case, the Threshold Accepting (TA) algorithm is a good solution. It was originally described in [WF97] and later on improved in [GWW12], see also [DGW14]. An implementation of the TA algorithm used to check the calculations of this article is provided under the same link as the DEM algorithm, [CVdN+23]. The remainder of the paper is organized as follows: In Section 2, we discuss numerical results for the application of Theorem 2 in a multi-dimensional setting. We will show for many different combinations of d,N\in\mathbb{N} that we obtain sets with a star-discrepancy which is at most as big as the currently best known ones obtained by alternative methods. Afterwards, we will give proofs of our theoretical results in Section 3. The proofs of Theorems 1 and Theorem 2 rely on p-adic discrepancy theory as discussed in e.g. [Wei24]. Furthermore, Theorem 4 is obtained by using Halton (sub-)sequences in small dimensions and an improved bound on bracketing numbers proven in [Gne24]. Finally, we note the explicit combinations of primes, shifts and permutations by which we obtain our optimal numerical results in the Appendix 4. This puts the reader into the position to reproduce our calculations. Acknowledgments. The author would like to thank Francois Clément for discussions on the content of the paper and for providing the link to the C-code for performing the calculations of the DEM algorithm. Moreover, he is grateful to Michael Gnewuch for his comments on a preliminary version of this paper and especially on the proof of Theorem 4 ."
https://arxiv.org/html/2411.10214v1,Machine Learning Algorithms to Assess Site Closure Time Frames for Soil and Groundwater Contamination,"Monitored Natural Attenuation (MNA) is gaining prominence as an effective method for managing soil and groundwater contamination due to its cost-efficiency and minimal environmental disruption. Despite its benefits, MNA necessitates extensive groundwater monitoring to ensure that contaminant levels decrease to meet safety standards. This study expands the capabilities of PyLEnM, a Python package designed for long-term environmental monitoring, by incorporating new algorithms to enhance its predictive and analytical functionalities. We introduce methods to estimate the timeframe required for contaminants like Sr-90 and I-129 to reach regulatory safety standards using linear regression and to forecast future contaminant levels with the Bidirectional Long Short-Term Memory (Bi-LSTM) networks. Additionally, Random Forest regression is employed to identify factors influencing the time to reach safety standards. Our methods are illustrated using data from the Savannah River Site (SRS) F-Area, where preliminary findings reveal a notable downward trend in contaminant levels, with variability linked to initial concentrations and groundwater flow dynamics. The Bi-LSTM model effectively predicts contaminant concentrations for the next four years, demonstrating the potential of advanced time series analysis to improve MNA strategies and reduce reliance on manual groundwater sampling. The code, along with its usage instructions, validation, and requirements, is available at: https://github.com/csplevuanh/pylenm_extension.","MNA aims to maintain institutional control and monitoring over the timeframe during which concentrations of contaminants, such as radioactive isotopes, naturally decrease through groundwater processes to meet safety standards set by the United States Environmental Protection Agency (EPA) (EPA (1999); EPA (2007); EPA (2015)). MNA is considered advantageous over more invasive remediation techniques due to its cost-effectiveness and minimal environmental disturbance (Pennington et al. (1999)). Historically, MNA has more commonly been applied to the treatment of organic compounds (Rugner et al. (2006); Usher et al. (2008); Declercq et al. (2012); Majone et al. (2015)). Recent research, however, indicates that MNA is becoming increasingly recognized as a viable method for addressing radionuclide contamination (e.g., Blowes et al. (2000); Lee and Hartwig (2005); Fytas (2010); Vazquez et al. (2011); Jurkovic et al. (2019); Sakala et al. (2020); Denham et al. (2020)). Determining the timeframe for monitoring is thus critical to ensure the stability of geochemical conditions, detect changes, and confirm compliance with regulatory standards. One notable site where MNA has been considered is the Savannah River Site (SRS). The SRS is a Department of Energy-owned Superfund site in South Carolina that was originally established in the early 1950s to produce the basic materials used in the fabrication of nuclear weapons, particularly tritium (H-3) and plutonium-239 (Pu-239), as part of the United States’ efforts during the Cold War. The site includes various facilities involved in producing special radioactive isotopes and covers an area of approximately 800 square kilometers (Bea et al. (2013)). Over several decades, low-level radioactive solutions were discharged into unlined basins, leading to significant contamination of groundwater with radionuclides such as uranium (U-238/U-235), tritium (H-3), and strontium-90 (Sr-90) (Bea et al. (2013)). Previous active remediation efforts at SRS focused on installing a subsurface barrier system and periodically injecting alkaline fluids to raise the pH of the acidic plume. However, the remaining acidity of the groundwater still poses challenges for complete neutralization, leaving an extensive contaminant plume covering approximately 600 meters downstream from the basins (Bea et al. (2013)). Recently, the long-term monitoring plan has considered the possibility of Monitored Natural Attenuation (MNA), as the site features natural barriers and diverse hydrostratigraphic units, including the Upper Unconfined Transmissive Aquifer (UUTRA), Lower Unconfined Transmissive Aquifer (LUTRA) (Sassen et al. (2012)), and the Tan Clay Confining Zone (Jean et al. (2004)) (Figure 1). Figure 1: Spatial distribution of the wells in the SRS F-Area with two primary aquifer types: The UUTRA wells are labeled in blue, and the LUTRA wells are labeled in green.. Despite the plan to transition to MNA at the SRS F-Area, significant challenges persist in predicting the decrease in contaminant concentrations. Previous studies (Xu et al. (2011a)) have predicted plume behaviors based on groundwater flow and reactive transport simulations (Appendix A). However, questions remain regarding the models’ ability to accurately predict the time required for contaminant concentrations to decrease to regulatory standards and understand the heterogeneity of factors influencing this prediction process. Temporal variability in contaminant migration is a significant challenge, particularly for radionuclides such as Sr-90 and I-129, which have experienced increases in concentrations over time (Kaplan et al. (2011)). To address these gaps, the primary objective of this study is to assess the time frame required for the site closure of soil and groundwater-contaminated sites. We will extend the PyLEnM library, which is an open-source, modular, and extensible machine learning (ML) framework for long-term groundwater contamination monitoring with an integrated data pipeline, visualization, quality assurance and control (QA/QC), parameter estimation, and other toolboxes (Meray et al. (2022)). Specifically, we will develop a data-driven model to predict the time needed for contaminants such as Sr-90 and I-129 to decrease below the Maximum Contaminant Level (MCL) in groundwater wells. The study will also investigate the impact of the site’s complex hydrostratigraphic units and varying geochemical conditions on the time to MCL. Finally, we will develop a near-term forecasting model of concentration trends based on recurrent neural networks to reduce future field sampling. Additionally, spatial analysis tools will be used to map and visualize contaminant concentration trends, identifying critical areas for focused monitoring and intervention. Validating the developed models against observational data, we will assess performance metrics like R² score and Mean Squared Error (MSE) to ensure reliability and robustness. A detailed analysis of feature importance scores from random forest regression models will identify key environmental factors affecting contaminant decay times (Breiman (2001)). Ultimately, this research aims to propose a comprehensive and adaptive framework for predicting the long-term effectiveness of MNA strategies at SRS and similar sites, ensuring precise identification of critical monitoring and intervention areas for effective site closure and groundwater protection."
https://arxiv.org/html/2411.09734v1,"Modeling AdaGrad, RMSProp, and Adam 
with Integro-Differential Equations","In this paper, we propose a continuous-time formulation for the AdaGrad, RMSProp, and Adam optimization algorithms by modeling them as first-order integro-differential equations. We perform numerical simulations of these equations to demonstrate their validity as accurate approximations of the original algorithms. Our results indicate a strong agreement between the behavior of the continuous-time models and the discrete implementations, thus providing a new perspective on the theoretical understanding of adaptive optimization methods.","Central to numerous machine learning tasks is the challenge of solving the following optimization problem: \arg\min_{\theta\in\mathbb{R}^{n}}f(\theta), where f:\mathbb{R}^{n}\rightarrow\mathbb{R} denotes a typically non-convex and differentiable objective (or loss) function. The pursuit of finding the global minima of such functions presents a significant challenge due to the inherent complexity and non-convexity of the landscape. Gradient Descent (GD) remains one of the most prominent algorithms for minimizing the function f by iteratively finding the optimal parameters \theta Boyd & Vandenberghe (2004). It operates by adjusting the parameters in the direction of the steepest descent of f with a fixed step size \alpha (learning rate). At each iteration, the algorithm computes the gradient of f with respect to \theta, guiding the parameter updates to minimize f progressively Rumelhart et al. (1986): \theta_{t}\leftarrow\theta_{t-1}-\alpha\nabla_{\theta}f_{t}(\theta_{t-1}). (1) While Stochastic Gradient Descent (SGD) Bottou (2010) extends the Gradient Descent method by using mini-batches, or randomly selected subsets of data, to compute gradients, this article focuses on a non-stochastic perspective. The continuous nature of these methods permits a more direct application of differential equation techniques. For readers interested in a continuous description of the stochastic method, we refer to Sirignano & Spiliopoulos (2017). Adaptive optimization methods such as AdaGrad Duchi et al. (2011) and RMSProp Hinton (2012) have been pivotal in advancing gradient-based algorithms. AdaGrad adapts the learning rate for each parameter by accumulating the sum of past squared gradients, which can lead to a significant decrease in the learning rate over time. In contrast, RMSProp mitigates this issue by maintaining an exponentially weighted average of squared gradients, ensuring a more stable learning process. These innovations laid the groundwork for more sophisticated algorithms like Adam. Adam (Adaptive Moment Estimation) Kingma & Ba (2014) refines gradient descent by computing adaptive learning rates for each parameter, leveraging both the first moment and the second moment of the gradients. Variants like AdamW Loshchilov & Hutter (2017) and AdamL2 have been proposed to address specific issues such as weight decay and improved regularization. To analyze these optimization algorithms through a continuous framework, we employ well-established tools from differential equations to examine the convergence and stability of these methods. For example, as the learning rate tends to zero (\alpha\to 0), the gradient descent algorithm, interpreted via the Euler method, can be approximated by the following first-order differential equation: \dot{\theta}=-\nabla_{\theta}f(\theta). Given that the AdaGrad, RMSProp, and Adam algorithms incorporate memory effects through the accumulation of past gradients, they are naturally expected to be represented by integro-differential operators. The primary contributions of this work are summarized as follows: • We propose continuous formulations for the AdaGrad, RMSProp, and Adam algorithms using integro-differential equations, as described in Propositions 1 to 3. We emphasize that the memory effects are encapsulated within the nonlocal terms of these equations. • We provide a numerical comparison between the results obtained from these continuous models and the original discrete algorithms, focusing on the accuracy and dynamics of the continuous approximations: Figures 8 to 14. Additionally, we detail the numerical methods employed to solve the integro-differential equations. This article is organized as follows: Sec. 2. Optimization Algorithms. We review the AdaGrad, RMSProp, and Adam algorithms and present numerical simulations that illustrate their dynamic behavior. Sec. 3. Continuous Representations. We introduce the continuous versions of these algorithms as first-order integro-differential equations. Sec. 4. Numerical Simulations. We compare the numerical results obtained from these continuous models with those from the discrete algorithms, and we provide a detailed explanation of the numerical method used to solve the integro-differential equations. 1.1 Notation In this subsection, we introduce and define the notation used throughout the article. Although the notation may appear “highly” physical, it is selected for its simplicity and to enhance the reader’s comprehension. Our discussion is set within an n-dimensional Euclidean space, and the key elements of the notation are as follows: • Parameter Space: The parameters \theta are represented as vectors in \mathbb{R}^{n}, where \mathbb{R} denotes the set of real numbers. These parameters are assumed to be differentiable functions. • Euclidean Metric: The Euclidean metric \delta_{ij} is employed to raise and lower indices, where \delta_{ij} is the Kronecker delta. • Einstein Summation Convention: We adopt the Einstein summation convention, wherein repeated indices imply summation over the corresponding index range. For example: \theta^{2}:=\|\theta\|^{2}:=\theta_{i}\theta^{i}=\delta_{ij}\theta^{i}\theta^{% j}=\sum_{i=1}^{n}\theta^{i}\theta^{i}, which represents the squared Euclidean norm of the vector \theta. • Derivative Notation: The partial derivative with respect to the i-th component of \theta is denoted by \partial_{i}. Specifically, the gradient operator is expressed as: \nabla_{\theta}=\left(\frac{\partial}{\partial\theta^{1}},\ldots,\frac{% \partial}{\partial\theta^{i}},\ldots,\frac{\partial}{\partial\theta^{n}}\right% ):=\left(\partial_{1},\ldots,\partial_{i},\ldots,\partial_{n}\right). 1.2 Related Work Adaptive optimization algorithms like AdaGrad, RMSProp, and Adam are widely adopted in deep learning due to their robustness and efficiency. However, the inherently discrete nature of deep learning algorithms has led researchers to explore continuous representations to gain deeper theoretical insights and leverage mathematical tools from continuous dynamical systems. Continuous Representations: Researchers have extended discrete optimization algorithms to continuous settings using differential equations. For instance, Su et al. (2016) provide a theoretical framework that connects Nesterov’s accelerated gradient method with a second-order ordinary differential equation (ODE), offering insights into the convergence properties and dynamics of optimization algorithms. Similarly, Wibisono et al. (2016) present a variational perspective on accelerated methods by deriving ODEs using a Lagrangian framework, capturing the essence of momentum in optimization. These continuous formulations allow for the analysis of optimization dynamics and convergence properties, such as asymptotic Lyapunov stability Gantmakher (1970). Stability analysis helps determine whether the solutions of optimization algorithms remain near an equilibrium point or diverge over time, providing deeper insights into their robustness Khalil (2002). Additionally, Romero et al. (2022) focus on the challenge of discretizing continuous-time analogues of optimization algorithms, particularly gradient-based methods, while retaining the optimality properties inherent in the continuous formulation. Extending this continuous perspective to neural networks, Chen et al. (2018) introduce Neural Ordinary Differential Equations (Neural ODEs), which model the evolution of hidden states in continuous time, bridging the gap between discrete neural networks and continuous dynamical systems. Similarly, Ruthotto & Haber (2020) explore how partial differential equations (PDEs) can be used to model and design deep neural networks. By interpreting layers in a neural network as time steps in a PDE solver, they provide a continuous-time perspective that can lead to new architectures and training algorithms, such as parabolic, elliptic, and hyperbolic CNNs. On another front, Li et al. (2017) explore stochastic gradient algorithms through stochastic differential equations (SDEs), providing a deeper understanding of their behavior in the continuous limit. This perspective is valuable for analyzing the convergence and stability properties of stochastic optimization methods. Integro-Differential Equations and Memory Effects: Recent research has examined the use of integro-differential equations (IDEs) to model biological and physical processes, as they can more accurately capture memory and cumulative effects than ODEs. For instance, Zappala et al. (2022) introduce Neural Integro-Differential Equations, combining neural networks with IDEs to model systems where both past and future states influence the current state. This approach is particularly effective in extrapolating temporal data and generalizing to unseen initial conditions, providing a framework for modeling complex dynamics with nonlocal interactions. Physical Perspectives on Optimization: Weinan E (2017) suggests rethinking machine learning algorithms through the lens of dynamical systems and control theory. In this view, optimization algorithms are modeled as continuous-time dynamical systems, treating learning as a control problem where parameters are adjusted for optimal outcomes. He also proposes that this continuous framework makes it possible to apply advanced control techniques, such as adaptive time-stepping, Hamiltonian structures, and constraints, to improve the efficiency and stability of machine learning models. In this context, departing from traditional neural networks that focus on learning mappings between inputs and outputs, Hamiltonian Neural Networks (HNNs) Greydanus et al. (2019) and Lagrangian Neural Networks (LNNs) Cranmer et al. (2020) incorporate fundamental physical principles directly into their architectures. HNNs are rooted in Hamiltonian mechanics and are trained to learn a Hamiltonian function that inherently respects exact conservation laws in an unsupervised manner. This approach makes them particularly effective for modeling systems where energy conservation is critical, such as the two-body problem. Conversely, LNNs employ the Euler-Lagrange equations by parameterizing arbitrary Lagrangian functions using neural networks without requiring canonical coordinates. This capability enables LNNs to perform well in situations where canonical momenta are unknown or difficult to compute. Noether’s theorem, which establishes a fundamental connection between symmetries and conserved quantities in physical systems, has also inspired advancements in machine learning. For example, Noether Networks Alet et al. (2021) leverage this theorem to meta-learn conserved quantities, improving prediction quality in sequential problems. Furthermore, Tanaka & Kunin (2021) develop a theoretical framework to study the geometry of learning dynamics in neural networks, revealing a key mechanism of explicit symmetry breaking behind the efficiency and stability of modern neural networks. While previous works have successfully employed ordinary differential equations (ODEs) and partial differential equations (PDEs) to model optimization algorithms and neural networks, they often focus on methods without explicit memory terms or cumulative effects. Our work distinguishes itself by proposing continuous formulations of AdaGrad, RMSProp, and Adam using integro-differential equations. By explicitly modeling the memory effects inherent in these adaptive algorithms through integral operators, we provide a novel perspective that captures their nonlocal dynamics. This distinction is crucial for achieving a deeper theoretical understanding and for developing new optimization strategies inspired by nonlocal behaviors, setting our work apart from prior studies that model optimization algorithms solely as ODEs. Therefore, our approach allows us to: (a) accurately represent cumulative effects by directly modeling how past gradients influence current updates—a defining characteristic of adaptive optimization algorithms; (b) facilitate theoretical analysis, as the continuous framework enables the application of advanced mathematical tools from the theory of integro-differential equations, such as stability analysis and convergence proofs; and (c) bridge discrete and continuous dynamics, offering insights into the behavior of these algorithms and potential improvements."
https://arxiv.org/html/2411.09728v1,Physics-informed neural networks (PINNs) for numerical model error approximation and superresolution,"Numerical modeling errors are unavoidable in finite element analysis. The presence of model errors inherently reflects both model accuracy and uncertainty. To date there have been few methods for explicitly quantifying errors at points of interest (e.g. at finite element nodes). The lack of explicit model error approximators has been addressed recently with the emergence of machine learning (ML), which closes the loop between numerical model features/solutions and explicit model error approximations. In this paper, we propose physics-informed neural networks (PINNs) for simultaneous numerical model error approximation and superresolution. To test our approach, numerical data was generated using finite element simulations on a two-dimensional elastic plate with a central opening. Four- and eight-node quadrilateral elements were used in the discretization to represent the reduced-order and higher-order models, respectively. It was found that the developed PINNs effectively predict model errors in both x and y displacement fields with small differences between predictions and ground truth. Our findings demonstrate that the integration of physics-informed loss functions enables neural networks (NNs) to surpass a purely data-driven approach for approximating model errors.","1.1 Dealing with Numerical Model Errors Numerical model errors are inevitable when using discretized approximations to represent physical domains. [Surana and Reddy (2016)]. Such errors may result from a number of factors, such as errors in approximating a smooth geometry/boundary [Reddy (2019)], the fidelity of the discretization [Babuška and Guo (1992)], the degree of interpolation [Duarte and Oden (1996), Babuska et al. (1981)], and among others. As a consequence, researchers have aimed to develop tools to measure or approximate numerical model errors. For example, it is common to adopt an absolute error metric that measures the deviation e (i.e., numerical model error) between a solution of a reduced-order numerical model u_{R} and a higher-order numerical model u_{H}111We note that analytical solutions are generally not considered as they often do not permit arbitrary geometries or distributions of causal parameters.: e=u_{H}-u_{R} (1) where these quantities can be scalars, vectors, or matrices – herein, the former quantities will be assumed to be vectors henceforth. To demonstrate concepts related to numerical solutions and their errors, Fig. 1 is provided in the context of the finite element analysis of a two-dimensional plate, which will be introduced later in detail. Here, we can see the differences in the displacement fields generated using higher- and lower-order finite elements models (i.e., Fig. 1b) and their respective relative error fields (i.e., Fig. 1c). The relatively small errors, compared to displacement magnitudes, are illustrated in the heatmaps and further confirmed by the line graphs in Fig. 1d. The non-Gaussian error distributions are shown in Fig. 1e. Figure 1: Illustration depicting (a) a classic symmetric elastic plate stretching problem solved using 1,800 finite element 4- and 16-node quadrilateral discretizations, denoted with ’4’ and ’16’ superscripts, respectively; (b) finite element displacement solutions separated into x and y fields for coincidental nodes; (c) x- and y-displacement error heat maps; (d) the same displacement errors depicted via line graphs; and (e) histograms plotted on the same axis demonstrating typical non-Gaussian model error distributions. Units in (b-e) are in meters. Results in this figure are only for demonstration purposes. Commonly, frameworks for dealing with e have included (a) implicit models: broadly, the integration of a corrected operator or an operator performing a correction on numerical model quantities of interest and (b) explicit models: a model which directly approximates or corrects numerical model solutions, for example, the direct approximation or correction of nodal finite element solutions. This work focuses on solutions to forward problems (cause \to effect), yet explicit and implicit corrections are also applicable in the context of inverse problems (effect \to cause) [Arridge et al. (2023)]. Classical approaches to error correction have included (a) scalar quantification or error indication for measuring the total error via a norm bound, and (b) probabilistic estimates such as the Bayesian approximation error (BAE) approach [Nissinen et al. (2007)]. In particular, BAE has been successful in applications with high sensitivity to model errors, especially for medical imaging, and is based on statistical sampling used in integrating mean and covariance quantities into imaging frameworks [Kaipio and Somersalo (2006)]. The BAE approach is often considered as an early approach to data-driven numerical model correction and continues to be used in modern imaging [Hauptmann and Tarvainen (2023), Arridge et al. (2023)]. However, there are a number of challenges in implementing classical approaches to deal with numerical model error e: • Scalar error quantities do not provide insight on spatial error distributions; • Error indicators may provide only rough bounds and may be biased [Freno and Carlberg (2019)]; • Statistical approaches may require a prior assumption on the error distribution, which is rarely, if ever, known in the context of numerical model solutions [Smyl et al. (2021)]. In all classical cases mentioned, a key realization is that explicit model errors are not directly quantified and therefore cannot be used to approximate explicit model errors and model compensation. This poses significant challenges to end users wishing to visualize nodal error distributions, quantify global or local errors when a prior is unknown, and/or quickly assess potential error at a region of interest. Why haven’t we built robust analytical or semi-analytical explicit model error approximators yet? The challenge in developing a model for linking numerical model solutions and/or features (e.g., boundary conditions, spatial distributions of model inputs, source terms, discretization, etc.) to approximations of model errors lies in the complexity of these relations. Not only would such a model be nonlinear, it may be highly ill-conditioned leading to high sensitivity or inaccuracy given small changes to one of many approximator inputs. Thus, it is challenging to construct a sufficiently smooth model error approximator in a well-posedness sense. A well-posed model capable of consolidating a broad set of complex model inputs into a generalizable framework would be highly valuable for numerical model implementation. It is therefore no surprise that, in at least a general sense, researchers have tested machine learning (ML) models to handle the complex model error approximation task [Koponen et al. (2021), Smyl et al. (2021), Freno and Carlberg (2019)]. In doing this, many of the underlying numerical complexities are abstracted to a learning process, which are currently data-driven or black-box implementations. To give context to ML, as it is key to the present work, the following subsection will briefly detail contemporary relevant ML applications. 1.2 Relevant Applications of ML In general, data-driven ML has been successful across the spectrum of science and engineering [Brunton and Kutz (2022)]. Research has focused on developing and studying ML approaches for structural design that incorporates a multitude of design variables, response, topology and structural form information to train neural networks (NNs) and later predict structural form [He and Ding (2021), Shin and Kim (2020), Shi (2020)] and structural topology [Hoang et al. (2022), Liu et al. (2020), Xue et al. (2021), Lei et al. (2019), Liu et al. (2019)]. More broadly, structural engineering has benefited from ML in the areas of structural health monitoring (SHM) [Farrar and Worden (2012)], performance assessment [Sun et al. (2020)] and analysis of various structural causalities/effects [Thai (2022), Lee et al. (2018)]. Considering the numerous successes of data-driven ML approaches in structural engineering and mechanics, data-driven only networks often exhibit undesirable properties. It is known that such networks can be sensitive to discrepancies in training data resulting from modeling errors, noise and architecture [Gudivada et al. (2017), Sessions and Valtorta (2006)]. The use of regularization and ad hoc techniques, such as dropout [Srivastava et al. (2014)], offers a means of alleviating the noted sensitivities, since they incorporate limited prior knowledge into the network. As a result, networks may suffer from a lack of generalizability and result in physically unrealistic predictions. Quite recently, the adoption of a graph neural network (GNN)-based approach for handling physical modeling has been promising. GNNs capture complex relationships and dependencies within data by modeling them as interconnected nodes and edges. This framework allows for a dynamic and flexible representation of both spatial and temporal dependencies. Unlike many NN approaches, GNNs can adapt to evolving structures in the data, facilitating continuous learning and adjustment to changing conditions [Zhou et al. (2020)]. Physics-informed neural networks (PINNs) were previously introduced by Raissi et al. [Raissi et al. (2019)] for the purpose of solving physics-governed partial differential equations (PDEs). Designed within supervised ML, PINNs are capable of incorporating specific physics-based prior knowledge into learned models by penalizing unrealistic physical attributes. This is accomplished by applying additional constraints into the learning problem, thereby ensuring that the physics internal to the network are compatible with an accurate physical model. By incorporating simultaneous physics-based prior information and constraints, PINNs have shown promise in enhancing generalizability compared to purely data-driven NNs for supervised emulation of PDE solutions to physical processes. [Cai et al. (2022), Karniadakis et al. (2021), Mishra and Molinaro (2021)]. These prior works, and the potentiality for using PINNs in solving inverse problems provides the central motivation for this work. 1.3 Paper Structure and Contributions This paper is structured as follows with a focus on modeling errors resulting from discretized civil and mechanical problems. First, we define the formulation of model error approximation and input for the explicit model error approximator using PINNs. Next, the details of the numerical simulation and dataset generation are introduced. Following, the architecture of the developed PINNs and loss functions are presented and discussed. Then, analysis and discussion of the results will be presented. Conclusions and limitations will be addressed in the end. Our contributions in this paper are: 1. Propose and test explicit PINN model error approximators; 2. Provide an in-depth understanding on the sensitivities of explicit machine learned model error approximators considering superresolution, prediction uncertainty, and different loss functions; 3. Provide pathways forward for future research in ML for explicit numerical modeling errors."
https://arxiv.org/html/2411.09605v1,Title of Your Manuscript,"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.","1 Sample Section Title Lorem ipsum dolor sit amet, consectetur adipiscing (Fabioetal2013) elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud Blondeletal2008 exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit (Blondeletal2008; FabricioLiang2013) anim id est laborum. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum see appendix A."
https://arxiv.org/html/2411.09584v1,A Sylvester equation approach for the computation of zero-group-velocity points in waveguides,"Eigenvalues of parameter-dependent quadratic eigenvalue problems form eigencurves. The critical points on these curves, where the derivative vanishes, are of practical interest. A particular example is found in the dispersion curves of elastic waveguides, where such points are called zero-group-velocity (ZGV) points. Recently, it was revealed that the problem of computing ZGV points can be modeled as a multiparameter eigenvalue problem (MEP), and several numerical methods were devised. Due to their complexity, these methods are feasible only for problems involving small matrices. In this paper, we improve the efficiency of these methods by exploiting the link to the Sylvester equation. This approach enables the computation of ZGV points for problems with much larger matrices, such as multi-layered plates and three-dimensional structures of complex cross-sections.","In many physics and engineering applications, we encounter parameter-dependent quadratic eigenvalue problems (QEP) of the form W(k,\omega)u:=\big{(}(\mathup{i}\mkern 1.0muk)^{2}L_{2}+\mathup{i}\mkern 1.0% mukL_{1}+L_{0}+\omega^{2}M\big{)}\,u=0, (1) where L_{0}, L_{1}, L_{2}, M are real n\times n matrices, which are usually obtained by a (semi-)discretization of a boundary value problem. The solutions (k,\omega) form eigencurves \omega(k), and we are interested in locating the critical points on these curves, where \omega^{\prime}(k)=\frac{\partial\omega}{\partial k}=0. Although solutions of (1) can be complex, we consider the important case where \omega and k are both real. This work is motivated by the study of (anisotropic) elastic waveguides (see, e.g., [23, 35]), where \omega denotes the angular frequency and k the wavenumber. In this context, the eigencurves are referred to as dispersion curves. The slope c{g}=\omega^{\prime} is called group velocity, which is of practical relevance, as it describes the propagation of energy. Points (k_{*},\omega_{*}) on the dispersion curves where the group velocity vanishes are called zero-group-velocity (ZGV) points. Often, the term is used exclusively for solutions at finite wavenumber k_{*}, as this is the non-trivial case, but here we use the designation for solutions at any k_{*}. In the light of this motivating practical application, we will generally refer to points on the curves formed by eigenvalues of parameter-dependent eigenvalue problems that satisfy \omega^{\prime}(k)=0 as ZGV points, irrespective of their physical interpretation. Recently, a numerical algorithm for the computation of ZGV points in anisotropic elastic waveguides was introduced [23] that can be applied to a general problem of the form (1). The method is based on a generalization of the method of fixed relative distance (MFRD) from [19], which provides good initial approximations that can be refined by a locally convergent Newton-type method. Inspired by the Sylvester-Arnoldi method from [28], we show in this contribution that sophisticated tools from linear algebra substantially speed up the algorithm and reduce its memory requirements. This enables us to solve problems with larger matrices and tackle more complex problems such as multi-layered plates as well as waveguides of arbitrary two-dimensional cross-sections. In the following, we first discuss properties of ZGV points in Section 2. In Section 3, we introduce several tools we will use in the following section; the presentation is intertwined with their application to the computation of ZGV points: the Sylvester equation, multiparameter eigenvalue problems, and the MFRD. Our main contributions are included in Section 4, where we show how we can exploit the structure of the Sylvester equation to apply the MFRD more efficiently, and in Section 5, where we present a scanning algorithm for the computation of ZGV points that combines the MFRD and a locally convergent Gauss-Newton method. In Section 6, we introduce a waveguide model that is used in the numerical experiments in the following section, where we demonstrate the strength of the proposed method. Finally, we discuss possible generalizations and give a conclusion in Sections 8 and 9."
https://arxiv.org/html/2411.09525v1,Data-driven parameterization refinement for the structural optimization of cruise ship hulls,"In this work, we focus on the early design phase of cruise ship hulls, where the designers are tasked with ensuring the structural resilience of the ship against extreme waves while reducing steel usage and respecting safety and manufacturing constraints. The ship’s geometry is already finalized and the designer can choose the thickness of the primary structural elements, such as decks, bulkheads, and the shell. Reduced order modeling and black-box optimization techniques reduce the use of expensive finite element analysis to only validate the most promising configurations, thanks to the efficient exploration of the domain of decision variables. However, the quality of the results heavily relies on the problem formulation, and on how the structural elements are assigned to the decision variables. A parameterization that does not capture well the stress configuration of the model prevents the optimization procedure from achieving the most efficient allocation of the steel.To address this issue, we extended an existing pipeline for the structural optimization of cruise ships developed in collaboration with Fincantieri S.p.A. with a novel data-driven reparameterization procedure, based on the optimization of a series of sub-problems. Moreover, we implemented a multi-objective optimization module to provide the designers with insights into the efficient trade-offs between competing quantities of interest and enhanced the single-objective Bayesian optimization module.The new pipeline is tested on a simplified midship section and a full ship hull, comparing the automated reparameterization to a baseline model provided by the designers. The tests show that the iterative refinement outperforms the baseline on the more complex hull, proving that the pipeline streamlines the initial design phase, and helps the designers tackle more innovative projects.","The shipbuilding industry faces continuously evolving requirements, as the rise in environmental consciousness prescribes reducing operational costs and adopting new engine technologies [43]. The hull design should reflect a lower resource usage during the manufacturing and operational phase, while at the same time accommodating new machinery such as liquid hydrogen tanks or batteries. All of these innovations must then observe the structural stability constraints imposed by the classification societies, which guarantee the safety and durability of the ship. The implementation of flexible and efficient optimization procedures during the initial design phase thus represents a crucial factor for innovation and competitiveness. However, the formulation of the optimization problem is a delicate task. When facing a project with novel characteristics, the designers might not anticipate the emergence of fringe behaviors and corner cases for which their initial formulation produces underwhelming results. In this work, we minimize the total mass of a cruise ship during the initial design phase, combining surrogates-assisted optimization and an automatic refining strategy for the optimization problem. The validation of the optimized designs is carried out through Finite Elements Analysis (FEA), integrating the tools and pipelines familiar to the designers. FEA is widespread in industry [34], and commercial solvers are available for the certification of industrial designs. However, the computational cost of FEA is incompatible with rapid iterations of optimization steps in the initial design phase of large-scale projects. Advanced FEA codes provide adjoint-based optimizers [20], but the implementation of complex and sometimes intractable rules from the classification societies hinder their adoption into the design workflow. Recent works on the structural optimization of marine artifacts have been focused on the use of either simplified analytical formulations, or FEA of small, incomplete models. In [4], a single hull ring was optimized to reduce mass and structural instability with a multi-objective genetic algorithm, with objectives and constraints computed by FEA. Finite differences of the simplified expressions for yielding and buckling were employed to find search directions for the optimization of a semi-submersible floater in [26]. FEA of a coarse mesh, combined with a simplified analytical model for the stiffeners structures, was optimized with particle swarm optimization in [32]. A collection of recent publications on design methods for the marine industry, including structural and topology optimization, can be found in the report of the International Ship and Offshore Structures Congress [25]. Reduced order models (ROMs) [6, 23, 9] provide an alternative to FEA in the initial design phase, enabling the designers to quickly iterate changes to parameter values and shape configurations, with the expensive validations being performed only on the most promising configurations. Non-intrusive approaches [7, 28] are able to separate the optimization procedure from the underlying physical model, a crucial requirement for workflows relying on closed-source commercial codes. Still, the optimization results are heavily dependent on the problem formulation, which when kept simple might be hindered by the designers’ bias or inexperience with novel technologies, or on the other end could scale poorly when too complex. In this work, we present an automated reparameterization procedure in the context of structural optimization, where the problem formulation is refined hierarchically through an integer linear problem (ILP) [37] based on the structural responses observed near the best-known configuration. The procedure extends an existing automated optimization pipeline for the optimization of passenger ship hulls [44], developed in collaboration with Fincantieri S.p.A., where data-driven surrogates based on proper orthogonal decomposition (POD) and Gaussian process regression (GPR) [33, 21] are optimized using Bayesian optimization (BO) [38]. Moreover, the framework is extended with the addition of multi-objective optimization through genetic algorithm (GA) [39, 17, 47, 14], and the integration of constraints for the vertical center of gravity (VCG) in the BO procedure. Single-objective optimization is further refined with a specialization for the discrete parametric domain, and a greedy heuristic that we call principal dimension search (PDS). Figure 1 shows the end-to-end pipeline, from the initial parameterization to the optimal hull. The process begins with an initial formulation of the optimization problem and the high-fidelity evaluation of a randomized sampling of the parametric space. The high-fidelity evaluations are used to build surrogates for the cheap evaluation of the quantities of interest (QoIs). The following optimization steps, multi and single-objective, employ the surrogates to efficiently select promising parameter configurations to expand the high-fidelity evaluations and refine the surrogates. Then an automatic refinement of the problem formulation is performed, increasing the capability of reaching better values of the QoIs at the cost of introducing more decision variables. After the problem refinement, the sequence of surrogates construction and optimization is started again, until the users are satisfied with the QoIs of the high-fidelity optimum. Figure 1: Optimization pipeline implementing the NAND approach. The operations start with an initial problem formulation (1) and a corresponding sampling of the parametric space. The configurations are processed during the analysis phase (2) to build a database of high-fidelity snapshots. The design phase consists of the construction of surrogates (3), and their optimization both multi-objective (4) and single-objective (5). The optimization results are then validated by (1), updating the snapshots database, and the design phase starts again. If the optimization doesn’t find a new optimum candidate, the user can either accept the current optimum, or refine the parameterization (6) and re-enter the pipeline with a more expressive problem. The paper is organized as follows. The problem formulation for the initial design phase is described in Section 2, along with the proposed solution for an automatic optimization pipeline. In Section 3 we discuss the numerical methods for the implementation of the optimization pipeline. The pipeline is applied to two test cases, a simplified midship section and a full ship model, with the results being analyzed in Section 4. Finally, we draw conclusions in Section 5."
https://arxiv.org/html/2411.09504v1,Asymptotic Analysis of IMEX-RK Methods for ES-BGK Model at Navier-Stokes level,"Implicit-explicit Runge-Kutta (IMEX-RK) time discretization methods are very popular when solving stiff kinetic equations. In [21], an asymptotic analysis shows that a specific class of high-order IMEX-RK schemes can accurately capture the Navier-Stokes limit without needing to resolve the small scales dictated by the Knudsen number. In this work, we extend the asymptotic analysis to general IMEX-RK schemes, known in literature as Type I and Type II. We further suggest some IMEX-RK methods developed in the literature to attain uniform accuracy in the wide range of Knudsen numbers. Several numerical examples are presented to verify the validity of the obtained theoretical results and the effectiveness of the methods.","One of the most well-known kinetic models for rarefied gas dynamics is the Boltzmann transport equation (BTE). Its dimensionless form is written as \partial_{t}f+v\cdot\nabla_{x}f=\frac{1}{\varepsilon}Q(f,f), (1.1) where f(t,x,v) is the distribution function which depends on time t>0, on the position of particles x\in\mathbb{R}^{d_{x}} and on their velocity v\in\mathbb{R}^{d_{v}}. Here \varepsilon is the so-called Knudsen number, defined as the ratio of the mean free path of molecules and the characteristic length scale of the physical problem. The dimension of space and velocity domains are denoted by d_{x} and d_{v}, respectively. The Boltzmann collision operator Q(f,f) is a non-linear operator that describes the binary collisions between molecules. It acts only on the velocity dependence of the distribution function f, and has the following fundamental properties • of conserving mass momentum and energy: \left\langle Q(f,f)\phi(v)\right\rangle=\textbf{0}\in\mathbb{R}^{d_{v}+2}, (1.2) for \phi(v)=\left(1,v,\frac{1}{2}|v|^{2}\right)^{\top}, and \left\langle g\right\rangle:=\int_{\mathbb{R}^{d_{v}}}g(v)dv, • to satisfy H-theorem: \int_{\mathbb{R}^{d_{v}}}Q(f,f)\ln fdv\leq 0, (1.3) • to vanish, i.e., Q(f,f)=0 when f is the local Maxwellian: \mathcal{M}[f](t,x,v)=\frac{\rho(t,x)}{(2\pi T(t,x))^{d_{v}/2}}\exp\left(-% \frac{|v-u(t,x)|^{2}}{2T(t,x)}\right) (1.4) where \rho, u, and E are density, mean velocity and energy associated to f: \left\langle f\phi(v)\right\rangle=\left(\rho,\rho u,E\right)^{\top}=U\in% \mathbb{R}^{d_{v}+2}, (1.5) and temperature T is given by \displaystyle\frac{d_{v}\rho T}{2}=E-\frac{1}{2}\rho|u|^{2}. We introduce the vector U for later use. When the Knudsen number is small, it is well known that BTE is closely related to fluid models such as compressible Euler or compressible Navier-Stokes (CNS) equations. The form of these fluid models associated to BTE are traditionally derived using the perturbation techniques like the Hilbert or Chapman’ÄìEnskog expansions [19, 13, 14]. Thus, BTE can be used for various Knudsen number, i.e., from rarefied to continuum gas dynamics. In spite of its good predictability and close relationship with fluid models, computation of the Boltzmann collision operator is very expensive due to its high dimensionality. Furthermore, the problem becomes more severe as the Knudsen number gets closer to zero (fluid regime). In this case, solving BTE by a standard explicit numerical scheme requires the use of a time step of the order of \varepsilon, which leads to very expensive numerical computations. Even if one adopts an implicit or semi-implicit time discretization for the collision part, it is still numerically challenging to construct an efficient implicit solver due to the complicated structure of the Boltzmann collision operator. To circumvent the issue on computational cost of the Boltzmann collision operator Q(f,f) in (1.1), simpler kinetic models have been proposed to mimic the main properties of the full integral operator Q(f,f). One such model is the BGK model [4]: \frac{\partial f}{\partial t}+v\cdot\nabla_{x}f={\frac{\tau}{\varepsilon}}% \left(\mathcal{M}[f]-f\right). (1.6) where \tau is the collision frequency that depends on \rho and T. The BGK model replaces the Boltzmann collision operator with a simple relaxation toward the local Maxwellian. Note that the BGK model still maintains the conservation of mass, momentum, and energy, as well as the entropy inequality [13]. In addition, the BGK model describes the correct fluid limit as \varepsilon\to 0, i.e., at the leading order term \varepsilon=0, it yields the compressible Euler equations (see [13, 19]). Unfortunately, at the first-order correction in \varepsilon, the transport coefficients obtained at the Navier–Stokes level are not satisfactory. In particular, the Prandtl number defined by \displaystyle\textrm{Pr}=\frac{\gamma}{\gamma-1}\frac{\mu}{\kappa}, which relates the viscosity \mu to the heat conductivity \kappa of gases is fixed by 1. Here, the polytropic constant \gamma for monatomic molecule with transional motions is given by \displaystyle\gamma=\frac{d_{v}+2}{d_{v}}. Note that for most realistic gases, we have Pr<1. In addition, in the hard-sphere model for monoatomic gases (\gamma=5/3) in Boltzmann equation, its Prandtl number is very close to 2/3. Many variants of the BGK model have been proposed in order to give the correct transport coefficeints at the Navier–Stokes level. In [20], Holway proposed a model that possesses several desirable properties. It not only satisfies the correct conservation laws and the entropy condition, but also yields the Navier-Stokes equations with a Prandtl number less than one through the Chapman-Enskog expansion. This model is known as the ellipsoidal statistical model (ES-BGK) and reads: \displaystyle\begin{split}&\frac{\partial f}{\partial t}+v\cdot\nabla_{x}f=% \frac{\tau}{\varepsilon}(\mathcal{G}[f]-f),\end{split} (1.7) where the collision frequency \tau=\frac{\rho T}{\mu(1-\nu)} depends on the free parameter -\frac{1}{2}\leq\nu<1. The anisotropic Gaussian distribution \mathcal{G}[f] is defined by \mathcal{G}[f](t,x,v)=\frac{\rho(t,x)}{\sqrt{\textrm{det}(2\pi\mathcal{T}(t,x)% )}}\exp\left(-\frac{(v-u(t,x))^{\top}\mathcal{T}(t,x)^{-1}(v-u(t,x))}{2}\right). (1.8) The temperature tensor \mathcal{T}(t,x) and stress tensor \Theta(t,x) are defined as \mathcal{T}(t,x)=(1-\nu)T(t,x)Id+\nu\Theta(t,x), (1.9) \Theta(t,x)=\frac{1}{\rho}\int_{R^{d_{v}}}(v-u)\otimes(v-u)f(t,x,v)dv=\frac{1}% {\rho}\left\langle(v-u)\otimes(v-u)f(v)\right\rangle, (1.10) respectively. As emphasized before, ES-BGK model also has a close relationship with fluid models. The Chapman-Enskog expansion applied to ES-BGK model gives the compressible Euler equations [16] at the leading order term (f\approx f_{0}): \partial_{t}\left(\begin{array}[]{l}\rho\\ \rho u\\ E\end{array}\right)+\nabla_{x}\cdot\left(\begin{array}[]{l}\rho u\\ \rho u\otimes u+pId\\ (E+p)u\end{array}\right)=0, (1.11) where p=\rho T and Id is the d_{x}\times d_{x} identity matrix. While at the first-order correction in \varepsilon (f\approx f_{0}+\varepsilon f_{1}), we obtain the CNS equations [13] (for details see Appendix 7): \displaystyle\partial_{t}\left(\begin{array}[]{l}\rho\\ \rho u\\ E\end{array}\right)+\nabla_{x}\left(\begin{array}[]{l}\rho u\\ \rho u\otimes u+pId\\ (E+p)u\end{array}\right)=\left(\begin{array}[]{l}0\\ \varepsilon\nabla_{x}\cdot(\mu\sigma(u))\\ \varepsilon\nabla_{x}\cdot(\mu\sigma(u)u+q)\end{array}\right) (1.21) where the stress tensor \sigma(u) and heat flux q are given by \sigma(u)=\nabla_{x}u+(\nabla_{x}u)^{\top}-\frac{2}{d_{v}}\nabla_{x}\cdot uId,% \quad q=\kappa\nabla_{x}T, with viscosity \mu=\frac{p}{(1-\nu)\tau} and thermal conductivity \kappa=\frac{d_{v}+2}{2}\frac{p}{\tau}. Thus, the Prandtl number is given by \frac{2}{3}\leq Pr=\frac{5}{2}\frac{\mu}{\kappa}=\frac{1}{1-\nu}<\infty, and this implies that desired such number can be recovered by choosing \nu appropriately. Note that the equation for the total energy can be replaced by the equation for the temperature T, [2] \frac{d_{v}}{2}\rho\left(\partial_{t}T+u\cdot\nabla_{x}T\right)+\rho T\nabla_{% x}\cdot u=\mathcal{O}(\varepsilon). (1.22) Considering the relationship with fluid models, when developing numerical methods for ES-BGK model, such methods should have a correct asymptotic behavior, i.e., for small parameter \varepsilon, the schemes should degenerate into a good approximation of the fluid asymptotic (compressible Euler or CNS equations). This property is often called asymptotic preserving (AP) property, [22]. Additionally, methods should ensure asymptotic accuracy (AA). In other words, the scheme exhibits robustness and high accuracy in the fluid dynamic regime, allowing it to capture fluid dynamic behavior without resolving the small Knudsen number. On the construction of AP and AA methods for kinetic models, IMEX time discretization [1, 23, 24, 11, 5, 6] have been successfully applied and studied. We restrict our literature reviews to the works on the NS limit based on IMEX time discretization applied to BGK or ES-BGK model. For instance, in [3, 26], the authors considered a micro-macro decomposition of the BGK equation and then applied IMEX-RK schemes to the resulting coupled system. In [18], authors introduced a first order IMEX method for ES-BGK model. The collision term is treated implicitly, allowing \Delta t to be chosen independently of \varepsilon, while the convection part is treated explicitly. Authors prove that the method is AP and consistent to first order time discretization of CNS equations. High order IMEX-RK methods [21] and IMEX multistep methods [17] are also similarly applied to ES-BGK model, and the methods are shown to be able to capture the NS limit under suitable conditions. In this paper, we analyze the asymptotic behaviors of different types of IMEX-RK schemes applied to ES-BGK model. Specifically, we will extend the results in [21] to general IMEX-RK schemes, known in literature as Type I and Type II, and prove that they can capture the CNS limit without resolving \varepsilon. Another novelty of the present work is to suggest IMEX-RK methods developed in [7] to attain uniform accuracy in the wide range of Knudsen numbers. These schemes have been developed to solve hyperbolic relaxation systems. The authors carried out an asymptotic expansion of the numerical method up to \mathcal{O}(\varepsilon) and imposed additional order conditions on the IMEX-RK schemes to achieve a consistent and accurate discretization at the diffusion limit. These conditions are sufficient but not necessary. However, the price to pay is that the IMEX-RK schemes proposed in [7] require more stages than those commonly used. The outline of this paper is the following. In Section 2, we explain IMEX-RK methods for ES-BGK model. In Section 3, we perform asymptotic analysis at the Navier-Stokes level. Then in Section 4, we give several numerical tests in order to validate our theoretical findings. Finally, conclusion will be provided."
https://arxiv.org/html/2411.09498v1,Analysis and discretization of the Ohta–Kawasaki equation with forcing and degenerate mobility,"The Ohta–Kawasaki equation models the mesoscopic phase separation of immiscible polymer chains that form diblock copolymers, with applications in directed self-assembly for lithography. We perform a mathematical analysis of this model under degenerate mobility and an external force, proving the existence of weak solutions via an approximation scheme for the mobility function. Additionally, we propose a fully discrete scheme for the system and demonstrate the existence and uniqueness of its discrete solution, showing that it inherits essential structural-preserving properties. Finally, we conduct numerical experiments to compare the Ohta–Kawasaki system with the classical Cahn–Hilliard model, highlighting the impact of the repulsion parameter on the phase separation dynamics.","Diblock copolymers are linear polymer chains made up of two bonded diblocks of mutually repellent polymer species. At lower temperatures, a combination of such diblock copolymers tends to separate into discrete phases because of their repulsion. However, due to the connection between the two polymer diblocks, phase separation occurs on the polymer length scales, resulting in a process known as microphase separation [36]. Depending on the composition of the diblock copolymer, this results in the development of periodic nanoscale patterns in the form of lamellae, spheres, cylinders, and gyroids; see [2, 5, 34]. Directed self-assembly of diblock copolymers is one of the most promising advancements in the fabrication of nanoscale devices at an economical price. Such self-assembly procedures are researched in the semiconductor industry as a promising means of expanding the capabilities of standard lithographic techniques to build more fine-scale patterned structures for electronic circuits; see [16]. In addition, it is scalable and cost-effective for mass production and has been used in the production of photonic metamaterials [43], membranes for viral filtering [48], and functional materials [35]. In this study, the Ohta–Kawasaki model for the microphase separation of diblock copolymers is investigated. First presented in [38], the Ohta–Kawasaki model is part of the density functional theory derived from the self-consistent field theory model. Moreover, it belongs to a class of nonlocal Cahn–Hilliard models and is well accepted in the scientific literature, see [20, 19, 18, 39, 27, 14, 37, 38, 21, 31, 50]. While the Ohta–Kawasaki model introduces a nonlocal term in the free energy to capture repulsive interactions, other nonlocal Cahn–Hilliard models frequently incorporate nonlocal diffusion for long-range interactions in place of standard diffusion, a feature particularly relevant in applications such as cell biology; see, e.g., [30, 28]. Furthermore, there are also variants of the Cahn–Hilliard model that are nonlocal-in-time in contrast to the nonlocal-in-space Ohta–Kawasaki model, see, e.g., [29]. Numerous investigations, see [45, 18, 20, 19], have shown that the Ohta–Kawasaki model successfully portrays periodic patterns of the diblock copolymer microphase separation that correspond to those found in experiments. In the previous works, it has been exploited that in the case of a constant mobility, the system can be transformed into the Cahn–Hilliard equation with an additional term on the right-hand side. However, it was stated [46, 32] that degenerate mobility in phase-field models can more accurately describe the physics of phase separation, as pure phases must have vanishing mobility. In this work, we study the mathematical analysis of the Ohta–Kawasaki model with degenerating mobilities. In addition, external forces are allowed our analysis, which has also not been done for the Ohta–Kawasaki model, see [22] for the Cahn-Hilliard case with one-sided mobility. As the external force disrupts the mass conservation of the model, the non-locality becomes more difficult, especially for the numerical discretization of the model. In Section 2, we introduce the Ohta–Kawasaki model and derive it from an underlying energy formulation. In addition, we introduce the functionals and parameters involved. In Section 3, we present the complete analysis of the model. We begin in Subsection 3.1 with various well-known analytical results that we require in the following subsections to prove the existence of weak solutions. In Subsection 3.2, we prove the existence of solutions in the non-degenerate case by using the Galerkin method and discretizing the PDE in space. We derive an energy result and pass to the limit in the formulation. In Subsection 3.4, we consider the case of degenerate mobility and introduce a further regularization of the PDE. In Section 4, we present a fully discrete scheme for the Ohta–Kawasaki system. We establish the existence and uniqueness of a discrete solution that preserves key structural quantities of the model, such as mass balance and energy dissipation. We show several simulations to highlight the microphase separation in Section 5 when comparing different values of the crucial repulsion parameter including the case of the standard Cahn–Hilliard equation. Finally, we mention some conclusions to the work in Section 6."
https://arxiv.org/html/2411.09444v1,Learning efficient and provably convergent splitting methods,"Splitting methods are widely used for solving initial value problems (IVPs) due to their ability to simplify complicated evolutions into more manageable subproblems. These subproblems can be solved efficiently and accurately, leveraging properties like linearity, sparsity and reduced stiffness. Traditionally, these methods are derived using analytic and algebraic techniques from numerical analysis, including truncated Taylor series and their Lie algebraic analogue, the Baker–Campbell–Hausdorff formula. These tools enable the development of high-order numerical methods that provide exceptional accuracy for small timesteps. Moreover, these methods often (nearly) conserve important physical invariants, such as mass, unitarity, and energy. However, in many practical applications the computational resources are limited. Thus, it is crucial to identify methods that achieve the best accuracy within a fixed computational budget, which might require taking relatively large timesteps. In this regime, high-order methods derived with traditional methods often exhibit large errors since they are only designed to be asymptotically optimal. Machine Learning techniques offer a potential solution since they can be trained to efficiently solve a given IVP with less computational resources. However, they are often purely data-driven, come with limited convergence guarantees in the small-timestep regime and do not necessarily conserve physical invariants. In this work, we propose a framework for finding machine learned splitting methods that are computationally efficient for large timesteps and have provable convergence and conservation guarantees in the small-timestep limit. We demonstrate numerically that the learned methods, which by construction converge quadratically in the timestep size, can be significantly more efficient than established methods for the Schrödinger equation if the computational budget is limited.","In this paper we consider first order initial value problems (IVPs) which arise in a wide range of physical applications. They encompass systems of ordinary differential equations (ODEs) with a finite dimensional state space, as well as the more general case of partial differential equations (PDEs) expressed as ODEs on an infinite dimensional Hilbert space \mathfrak{H}. Mathematically, first order IVPs can be written as (1) \dot{u}(t)=f(u,t),\quad u({0})=u_{0},\quad u({t})\in\mathfrak{H},\quad t\in[0,% T], for some T>0, where u is some state that evolves in time t from a given initial state u_{0} under the action of a vector field f and \dot{u} denotes the derivative of u with respect to time t. In general, it is not possible to solve IVPs like (1) analytically; even where closed form solutions do exist, their evaluation is often prohibitively expensive computationally. Thus we require numerical methods that are stable, accurate and computationally efficient. These are typically realised in terms of time-stepping methods where, for the sake of simplicity, we consider the evolution to the final time T=Nh as being split into N equal steps of size h\ll T. A one-step numerical method is then uniquely defined by the forward map from the approximate solution at a given time t to the approximate solution at time t+h. Splitting and composition methods [6] allow the separation of the evolution in (1) into simpler IVPs which can be solved efficiently and accurately; as a consequence they have been used successfully in many areas, see e.g. [32]. In particular, we assume that the vector field f=f^{[{1}]}+f^{[{2}]} can be split into two components f^{[{1}]} and f^{[{2}]}, each of which defines an IVP that is simpler to solve numerically; as shown in [18] this is indeed the case for a wide range of applications. Traditionally, splitting and composition methods methods are derived using analytic and algebraic conditions, including truncated Taylor series and their Lie algebraic analogue, the Baker–Campbell–Hausdorff (BCH) formula [23, 9] to guarantee consistency (or local error) of high order. Once stability is ensured, this leads to high-order convergence (of the global error) in the asymptotic limit of small timesteps h\rightarrow 0, see e.g. [37, 28, 32, 5]. However, the asymptotic convergence of traditional methods implies that they are most efficient for small timestep h, which correspond to significant computational cost. It is also not obvious that a method which is designed to be fast in the limit h\rightarrow 0 will be the best choice for larger h. To make these points explicit we define two criteria to characterise an optimal numerical method: C1: The method has the fastest decrease in error in the asymptotic regime h\to 0. C2: The method has the smallest error for a given limited computational budget. Traditionally, the focus has been on constructing methods that satisfy criterion C1. This, however, is not helpful if computational resources are limited and simulations have to be carried out at relatively large timestep sizes. This scenario, which is of significant practical interest, is the case we consider in this work. Our aim is to construct numerical methods with machine learning techniques such that they satisfy criterion C2, while being provably convergent in the limit h\rightarrow 0. The exact solutions of IVPs often conserve a range of quantities that are known as invariants or first integrals. For example, for the Schrödinger equation the norm of the complex-valued solution does not change with time, which physically translates to the conservation of total probability, see [35]. More generally, for the Schrödinger equation the evolution of the IVP preserves the inner product of the underlying Hilbert space, a property known as unitarity, as well as the total energy, see [35]. Similarly, Hamiltonian systems conserve a symplectic two-norm in phase space, see [20]. Such invariants often have a physical interpretation and are typically related to conservation laws. It is highly desirable to construct numerical methods which preserve these invariants at least approximately. This usually also improves the stability of the method since it limits the trajectory to a sub-manifold of the Hilbert space. An important example is the class of symplectic integrators, see e.g. [20]. Machine Learning (ML) methods offer a promising alternative to traditional numerical methods for determining a numerical solution to an IVP, see e.g. [24] for a recent review. The key idea is to learn a forward solution map by training on a large set of initial and final values for the IVP. Provided the examples used for training are sufficiently representative of the (distribution of) initial conditions we are interested in solving, these methods can achieve high accuracy on unseen initial conditions from the same distribution and potentially also generalise to a wider class of IVPs. In their simplest form, ML methods are purely data-driven. They learn a fixed-cost forward solution map which is typically represented by a neural network. Unless measures are taken to specifically enforce inductive priors (see e.g. [33, 31, müller2023a]), ML techniques do not guarantee the conservation of invariants such as unitarity and symplecticity which can be enforced with traditional splitting methods. With few exceptions such as [11], ML methods suffer from their detachment from the differential equation framework: since they do not parametrise the forward map as a function of the timestep size h, there is no sense in which convergence in the limit h\to 0 can be quantified. In contrast to traditional numerical methods which can be made more accurate by reducing the timestep size, it is not possible to achieve higher accuracy for these ML methods in a controlled way. Even ML methods that learn the forward map with a variable timestep size h do not necessarily generalise to different timestep sizes. In particular they may fail to converge in the limit h\rightarrow 0. Moreover, the stability of the forward map – a crucial component for establishing the convergence – is exceptionally hard to guarantee. However, provided sufficient training data is available, the model is sufficiently expressive, and the loss function weights large time step performance sufficiently, ML based methods have the potential to result in smaller overall errors, and thus superior performance for specific computational budgets and larger timesteps (in the sense of criterion C2) than traditional methods which aim to be efficient in the asymptotic regime (in the sense of criterion C1), see [24, 19]. 1.1 Contributions In this paper we combine techniques from numerical analysis and ML to find efficient splitting schemes in the sense of criterion C2 while being provably convergent in the limit h\rightarrow 0. Specifically we use machine learning to find splitting schemes that are tailored for a specific distribution of initial conditions, yet maintain many of the advantages of classical numerical methods, such as interpretability, generalisability, convergence, and conservation. As our numerical results show, the learned methods also generalise to scenarios not contained in the training set. More specifically, we use ML methods to learn coefficients of splitting methods, while algebraically enforcing conditions that guarantee desirable properties such as consistency, stability and reversibility. We demonstrate the utility of this framework by learning splittings of medium to long length which result in lower errors than classical methods for large step sizes h. Restricting our search to physically plausible methods allows us to search a significantly lower dimensional submanifold. This reduces the training time compared to naive black-box ML approaches while also guaranteeing convergence. We numerically verify that the learned methods are efficient for the Schrödinger equation with a double well potential and also demonstrate that they have the desirable conservation properties. While by construction the learned methods are formally only quadratically convergent in h, we also show that we can learn near fourth order methods."
https://arxiv.org/html/2411.09329v1,Improving hp-Variational Physics-Informed Neural Networks for Steady-State Convection-Dominated Problems,"This paper proposes and studies two extensions of applying hp-variational physics-informed neural networks, more precisely the FastVPINNs framework, to convection-dominated convection-diffusion-reaction problems. First, a term in the spirit of a SUPG stabilization is included in the loss functional and a network architecture is proposed that predicts spatially varying stabilization parameters. Having observed that the selection of the indicator function in hard-constrained Dirichlet boundary conditions has a big impact on the accuracy of the computed solutions, the second novelty is the proposal of a network architecture that learns good parameters for a class of indicator functions. Numerical studies show that both proposals lead to noticeably more accurate results than approaches that can be found in the literature.","Convection-diffusion-reaction (CDR) problems are fundamental models for simulating transport events. CDR problems capture the interaction between a fluid’s bulk motion, or convection, its progressive spreading of properties by random molecular motion, or diffusion, and the impact from other quantities in coupled systems, which might be modelled as the reaction term. They constitute a framework for modelling the transport of variables like temperature or concentration. Let \Omega\subset\mathbb{R}^{2} be a bounded domain with polygonal Lipschitz-continuous boundary \partial\Omega. The Lebesgue and Sobolev spaces on this domain are denoted by L^{p}(\Omega) and W^{k,p}(\Omega), respectively, where, 1\leq p\leq\infty, k\geq 0. The Hilbert space, equivalent to W^{k,2}(\Omega), is denoted by H^{k}(\Omega). Then, a linear CDR boundary value problem, already in nondimensionalized form, is given by \displaystyle\begin{split}-\varepsilon\Delta u({\boldsymbol{x}})+{\boldsymbol{% b}}({\boldsymbol{x}})\cdot\nabla u({\boldsymbol{x}})+c({\boldsymbol{x}})u({% \boldsymbol{x}})&=f({\boldsymbol{x}}),\quad\text{in}\ \Omega,\\ u({\boldsymbol{x}})&=g({\boldsymbol{x}}),\quad\text{on}\ \partial\Omega.\end{split} (1) Here, {\boldsymbol{x}}=(x,y)\in\overline{\Omega}, u({\boldsymbol{x}}) is the unknown scalar solution and f({\boldsymbol{x}})\in L^{2}(\Omega) is a known source function. In addition, \varepsilon\in\mathbb{R}^{+} is the diffusion coefficient, {\boldsymbol{b}}\in(W^{1,\infty}(\Omega))^{2} is the convection field, and c\in L^{\infty}(\Omega) is the reaction field. The Dirichlet boundary"
https://arxiv.org/html/2411.09244v1,Parallel in time partially explicit splitting scheme for high contrast multiscale problems,"Solving multiscale diffusion problems is often computationally expensive due to the spatial and temporal discretization challenges arising from high-contrast coefficients. To address this issue, a partially explicit temporal splitting scheme is proposed in [8]. By appropriately constructing multiscale spaces, the spatial multiscale property is effectively managed, and it has been demonstrated that the temporal step size is independent of the contrast. To enhance simulation speed, we propose a parallel algorithm for the multiscale flow problem that leverages the partially explicit temporal splitting scheme. The idea is first to evolve the partially explicit system using a coarse time step size, then correct the solution on each coarse time interval with a fine propagator, for which we consider both the sequential solver and all-at-once solver. This procedure is then performed iteratively till convergence. We analyze the stability and convergence of the proposed algorithm. The numerical experiments demonstrate that the proposed algorithm achieves high numerical accuracy for high-contrast problems and converges in a relatively small number of iterations. The number of iterations stays stable as the number of coarse intervals increases, thus significantly improving computational efficiency through parallel processing.","Numerous scientific problems and models exhibit multiscale properties, such as flow in heterogeneous porous media, the diffusion of pollutants in the atmosphere, turbulent transport in high Reynolds number flows, and so on. These models often involve significant variations in media properties, commonly referred to as high contrast. The presence of high contrast introduces stiffness to the system, which makes spatial and temporal discretization challenging for traditional numerical methods due to their high computational demands. There have been many existing approaches in the literature to handle spatial multiscale problems, such as numerical homogenization (NH) [12, 1], multiscale finite element methods (MsFEM) [15, 28, 4], generalized multiscale finite element methods (GMsFEM) [5, 10, 11], constraint energy minimizing GMsFEM (CEM-GMsFEM) [21, 6, 26], localized orthogonal decomposition (LOD) [33, 25] and nonlocal multi-continua method (NLMC) [9, 19, 29]. Among which, CEM-GMsFEM is a multiscale finite element method used to effectively address multiscale problems with high-contrast parameters. It constructs multiscale basis functions by minimizing energy constraints, which can achieve contrast independent convergence rates. Based on CEM-GMsFEM, NLMC is proposed to construct local basis functions that automatically identify physical properties in each local region and provides non-local transmissibilities in the global formulation. For non-stationary multiscale problems, the high-contrast will lead to very small time steps when treating explicitly. The partially explicit temporal splitting scheme [8] originates from the solution decomposition [14] and splitting algorithms [13]. The main idea of the method is to divide the solution space into two parts, the dominant part and the complementary part, such that the time step size is independent of the high-contrast. The method is successfully employed in solving wave equation, nonlinear diffusion, and time-fractional diffusion equations[7, 16, 22, 29] and extended to an adaptive algorithm[29]. In this paper, we follow the concept developed in [8] for linear equations. All the existing literatures are based on the sequential method for solving the partially explicit temporal splitting scheme, which is easy to implement but might be inefficient when the temporal mesh partition is small enough and a long time simulation is needed. For this reason, we introduce a parareal algorithm to enhance computational efficiency. The parareal algorithm was proposed by Lions, Maday et al in [24]. Its core idea is to divide the entire time interval into subintervals and compute simultaneously and independently on each subinterval. There has been many research work on the analysis and applications of the parareal method, see [20, 2, 31, 27, 32]. However, the existing literature is based on Euler or Runge-Kutta method as fine solver in each subinterval, which could be time consuming if one takes small time steps. To address this issue, we further introduce the waveform relaxation (WR) method via the diagonalization technique based on all-at-once algorithm[23, 18, 3, 30] as the fine solver in the parareal framework. All-at-once algorithm is a global method that solves the problem over the entire time interval simultaneously instead of solving it step by step. It naturally fits for parallel computation and can significantly improve computational efficiency. The main contributions of this paper are as follows. • The parareal algorithm for solving the partially explicit temporal splitting scheme is presented. The convergence of the proposed algorithm is proved. • The WR method via the diagonalization technique based on the all-at-once method is introduced into the parareal algorithm to speed up the computation for the fine propagator. • The proposed algorithm can achieve high accuracy, and convergence can be reached with a small number of iterations. As the number of coarse interval (processors) increases, the number of iteration needed to achieve the error tolerance is quite stable, thus significantly saving the computational cost with parallel computation. The rest of this paper is organized as follows. In section 2, we present preliminaries. In section 3, we give in detail the construction of multiscale spaces. The parareal all-at-once algorithm and the error estimate of full discretization are given in section 4. Numerical experiments and conclusion are given in section 5 and section 6, respectively."
https://arxiv.org/html/2411.09162v1,An Asymptotic-Preserving Scheme for Isentropic Flow in Pipe Networks,"We consider the simulation of isentropic flow in pipelines and pipe networks. Standard operating conditions in pipe networks suggest an emphasis to simulate low Mach and high friction regimes – however, the system is stiff in these regimes and conventional explicit approximation techniques prove quite costly and often impractical. To combat these inefficiencies, we develop a novel asymptotic-preserving scheme that is uniformly consistent and stable for all Mach regimes. The proposed method for a single pipeline follows the flux splitting suggested in [Haack et al., Commun. Comput. Phys., 12 (2012), pp. 955–980], in which the flux is separated into stiff and non-stiff portions then discretized in time using an implicit-explicit approach. The non-stiff part is advanced in time by an explicit hyperbolic solver; we opt for the second-order central-upwind finite volume scheme. The stiff portion is advanced in time implicitly using an approach based on Rosenbrock-type Runge-Kutta methods, which ultimately reduces this implicit stage to a discretization of a linear elliptic equation.To extend to full pipe networks, the scheme on a single pipeline is paired with coupling conditions defined at pipe-to-pipe intersections to ensure a mathematically well-posed problem. We show that the coupling conditions remain well-posed in the low Mach/high friction limit – which, when used to define the ghost cells of each pipeline, results in a method that is accurate across these intersections in all regimes. The proposed method is tested on several numerical examples and produces accurate, non-oscillatory results with run times independent of the Mach number.","This paper focuses on the development of a novel numerical method for gas flow in pipelines and pipe networks. To describe the gas transport, we use the isothermal/isentropic Euler equations with a source to account for the friction along the pipe walls: \displaystyle\rho_{t}+{\left(\rho u\right)}_{x}=0, (1.1) \displaystyle{\left(\rho u\right)}_{t}+{\left(\rho u^{2}+p\right)}_{x}=-\frac{% \kappa}{2D}\rho u{\left|u\right|}, where x is the (one-dimensional) spatial variable, t denotes time, \rho is the fluid density, u denotes the fluid velocity, p=\rho^{\gamma} is the pressure under the isentropic assumption, in which \gamma is the ratio of specific heats, and \kappa denotes the Fanning friction coefficient. In the one-dimensional approach, which is quite common when modeling gas flows in pipes due to the ratio of pipe length L and cross section D; see e.g. [67, 14]; the unknowns \rho and u are averaged over the (assumed constant) cross section. The standard operating conditions hold the gas flow at moderate velocities; see e.g. [16] where they note a reference Mach number around 0.001; and thus we must account for the low Mach number or high friction regimes of (1.1). However, the limiting solution as this parameter goes to zero brings about a number of difficulties – the most notable of which is that the underlying system becomes very stiff. This makes designing efficient and accurate methods to approximate the solution quite challenging. For example, conventional explicit schemes applied to system (1.1) would greatly over-resolve the solution in time, as the wave speeds of the system are proportional to the inverse of the reference Mach number – further implying that the CFL stability restriction is proportional to this small parameter; see, e.g., [36, 35, 69]. Hence, explicit schemes may prove quite computationally expensive, especially in the low Mach/high friction limit. One alternative to avoid this over-resolution issue are asymptotic-preserving (AP) schemes. By the definition in [48, 49], a scheme is AP if the discretization of the continuous system remains consistent and stable regardless of the value taken for the underlying singular parameter (denoted by \varepsilon in this paper). Furthermore, AP schemes allow for the space and time discretization to be independent of \varepsilon; i.e., the CFL condition for stability does not depend on the small parameter in the system. Due to the potential speed up when discretizing with AP methods, they have attracted a lot of attention in the numerical and engineering community. AP schemes have been extensively studied for the kinetic equations; see, e.g., [45, 46, 54, 50, 51, 68, 78] and references therein. More recently, AP methods for the kinetic equations have been further extended to use AP Monte Carlo methods to reduce numerical diffusion effects [30, 31], and to use AP neural networks to solve the underlying PDE system [52, 53]. There has also been extensive development of AP methods in the low Mach limit of the isentropic Euler, compressible Euler, and Navier-Stokes systems in [2, 3, 4, 9, 12, 20, 21, 22, 37, 55, 56, 58, 66, 70, 77], and in the low Froude limit of the shallow water equations in [8, 13, 17, 24, 47, 59, 63, 72, 75, 76]. The number of studies significantly shrinks, though, when it comes to the consideration of nonlinear friction terms that remain in the limiting solution. When this nonlinear friction term, such as that in (1.1), remains in the low Mach/Froude limit, it adds the difficulty that this source must be discretized implicitly in some manner. If discretized naively, this requires some a nonlinear solve, which would preferably be avoided. To our knowledge, only the work in [17, 27] consider some nonlinear friction term, in which only [17] avoids a nonlinear solve via their proposed semi-implicit hybrid finite volume/finite element method. Furthermore, the number of studies of AP schemes in the extension to pipe networks is also quite limited. To our knowledge, there are only two such developments. In [28], they present an AP hybrid-DG method on networks for the linear convection-diffusion equation. The work in [27] proposes a finite element AP method applied to the barotropic Euler equations with a nonlinear friction term on pipe networks. However, the aformentioned method does opt for an implicit time discretization reliant on a nonlinear solver. In this paper, we propose an AP scheme for the isentropic Euler equations with a nonlinear friction source on pipe networks. The method on a single pipeline uses hyperbolic flux splitting proposed in [37, 63] to split the stiff and non-stiff parts of the system, which are then treated through an implicit-explicit approach. The non-stiff portion of the system is advanced in time explicitly and discretized in space using the second-order central-upwind (CU) finite volume scheme developed in [60, 61]. The stiff part of the system is advanced in time implicitly using an approach related to Rosenbrock-type Runge-Kutta methods seen in, e.g., [73, 79], which ultimately reduces this implicit stage to an elliptic equation that can then be solved linearly after discretizing in space via standard central-difference. The proposed scheme for a single pipeline is extended to pipe networks by defining coupling conditions, such as those in [7, 6], at pipe-to-pipe intersections to ensure a mathematically well-posed problem. We show that the coupling conditions remain well-posed in the low Mach/high friction regimes, and use them to define the ghost cell values of each pipeline. The resulting method is tested on several numerical examples of pipe networks and produces accurate and non-oscillatory results, in addition to running significantly faster than the analogous fully explicit scheme in the low Mach/high friction limit. This paper is organized as follows. In §2, we discuss the asymptotics of the isentropic Euler equations (1.1) on pipe networks and associated numerical difficulties. We then develop an AP scheme for the underlying system in §3. We present the performance of the proposed scheme on three examples in §4, and make some concluding remarks in §5."
https://arxiv.org/html/2411.09113v1,Convergence rates of Landweber-type methods for inverse problems in Banach spaces,"Landweber-type methods are prominent for solving ill-posed inverse problems in Banach spaces and their convergence has been well-understood. However, how to derive their convergence rates remains a challenging open question. In this paper, we tackle the challenge of deriving convergence rates for Landweber-type methods applied to ill-posed inverse problems, where forward operators map from a Banach space to a Hilbert space. Under a benchmark source condition, we introduce a novel strategy to derive convergence rates when the method is terminated by either an a priori stopping rule or the discrepancy principle. Our results offer substantial flexibility regarding step sizes, by allowing the use of variable step sizes. By extending the strategy to deal with the stochastic mirror descent method for solving nonlinear ill-posed systems with exact data, under a benchmark source condition we also obtain an almost sure convergence rate in terms of the number of iterations.","Due to their simplicity of implementation and low computational complexity per iteration, Landweber-type methods are well-recognized for solving ill-posed inverse problems in Banach spaces. Although their convergence has been well-understood, deriving their convergence rates remains a challenging open question. In this paper, we will consider Landweber-type methods for solving ill-posed inverse problems, where the forward operators map from a Banach space to a Hilbert space, and develop novel strategies to tackle the challenge of deriving convergence rates under a benchmark source condition on sought solutions. We consider ill-posed inverse problems governed by the operator equation \displaystyle F(x)=y, (1) where F:\mbox{dom}(F)\subset X\to Y is a Fréchet differentiable operator mapping from a Banach spaces X to a Hilbert space Y, with domain \mbox{dom}(F). The Fréchet derivative of F at x\in\mbox{dom}(F) is denoted by F^{\prime}(x). For numerous examples of inverse problems that take the form of equation (1), such as integral equations of the first kind, tomography problems, and parameter estimation in partial differential equations, see [9, 11, 20, 28]. We assume (1) has a solution, i.e. y\in\mbox{Ran}(F), the range of F. In practical applications, prior feature information about the sought solution is often available, and it is important to incorporate this information into the reconstruction process. Let {\mathcal{R}}:X\to(-\infty,\infty] be a proper, lower semi-continuous, strong convex function that takes into account the available feature information. To reconstruct a solution of (1) that aligns with the features described by {\mathcal{R}}, we start with an initial guess x_{0}\in X with \partial{\mathcal{R}}(x_{0})\neq\emptyset and take \xi_{0}\in\partial{\mathcal{R}}(x_{0}), where \partial{\mathcal{R}}(x_{0}) denotes the subdifferential of {\mathcal{R}} at x_{0}. We want to determine a solution x^{\dagger} of (1) such that \displaystyle D_{\mathcal{R}}^{\xi_{0}}(x^{\dagger},x_{0})=\min\left\{D_{% \mathcal{R}}^{\xi_{0}}(x,x_{0}):F(x)=y\right\}, (2) where D_{\mathcal{R}}^{\xi_{0}}(x,x_{0}) denotes the Bregman distance induced by {\mathcal{R}} at x_{0} in the direction \xi_{0}; for the definition of subdifferential, Bregman distance and other relevant concepts from convex analysis, see [31]. A brief review of these concepts is provided in Section 2. We are mainly interested in the situation where the problem (2) is ill-posed in the sense that its solution does not depend continuously on the data. In applications, data acquired through experiments inevitably contains noise. Assume, instead of the exact data y, we only have a noisy data y^{\delta} satisfying \displaystyle\|y^{\delta}-y\|\leq\delta with a small noise level \delta>0. Directly substituting y with y^{\delta} in the minimization problem (2) may render the problem ill-defined, even if well-defined, the solution obtained from y^{\delta} may not depend continuously on the data. To effectively utilize noisy data y^{\delta} to approximate the solution of (2), regularization techniques must be employed to mitigate this instability. Although many regularization methods have been developed for solving (2) with noisy data, in this paper we will focus on the following Landweber-type method \displaystyle\begin{split}\xi_{k+1}^{\delta}&=\xi_{k}^{\delta}-\gamma_{k}^{% \delta}F^{\prime}(x_{k}^{\delta})^{*}(F(x_{k}^{\delta})-y^{\delta}),\\ x_{k+1}^{\delta}&=\arg\min_{x\in X}\left\{{\mathcal{R}}(x)-\langle\xi_{k+1}^{% \delta},x\rangle\right\},\end{split} (3) where x_{0}^{\delta}:=x_{0} and \xi_{0}^{\delta}:=\xi_{0}. This method builds on various efforts to extend the classical Landweber iteration ([5, 16, 19, 22, 23, 24, 26, 29]) and can be interpreted as a mirror descent method ([18, 21]). Each iteration involves two steps: the first performs a gradient step in the dual space X^{*} of X, followed by the second step, which uses the mirror map defined by {\mathcal{R}} to pull the element back to the original primal space X. Under the tangential cone condition \|F(\bar{x})-F(x)-F^{\prime}(x)(\bar{x}-x)\|\leq\eta\|F(\bar{x})-F(x)\| around a sought solution with \eta\in[0,1), the convergence has been proved when the iteration is terminated by a priori or a posteriori stopping rules, see [13, 16, 19, 26] for instance. To assess how fast the iteration converges towards the desired solution, understanding the convergence rates is crucial and has significant theoretical interest. For ill-posed inverse problems, deriving such rates typically relies on assuming appropriate source conditions are satisfied by the sought solution. In Hilbert spaces, the linear Landweber iteration is well-studied, where it is known to be an order-optimal regularization method ([9]). For the more general method (3) in Hilbert spaces with a nonlinear forward operator F and a regularization functional {\mathcal{R}}(x)=\frac{1}{2}\|x\|^{2}, convergence rate analyses have been undertaken in [13, 22, 23]. Specifically, in the seminal work by [13], the nonlinear Landweber iteration \displaystyle x_{k+1}^{\delta}=x_{k}^{\delta}-\gamma F^{\prime}(x_{k}^{\delta}% )^{*}(F(x_{k}^{\delta})-y^{\delta}) (4) in Hilbert spaces with a constant step size has been considered. Under the range invariance condition \displaystyle F^{\prime}(x)=Q_{x}F^{\prime}(x^{\dagger})\quad\mbox{ and }\quad% \|I-Q_{x}\|\leq\kappa_{0}\|x-x^{\dagger}\| (5) in a neighborhood of the sought solution x^{\dagger}, it has been shown that, if the method is terminated by the discrepancy principle \displaystyle\|F(x_{k_{\delta}}^{\delta})-y^{\delta}\|\leq\tau\delta<\|F(x_{k}% ^{\delta})-y^{\delta}\|,\quad 0\leq k<k_{\delta} (6) with suitably large \tau>1, then the convergence rate \|x_{k_{\delta}}^{\delta}-x^{\dagger}\|=O(\delta^{2\mu/(1+2\mu)}) holds, provided the source condition x^{\dagger}-x_{0}=(F^{\prime}(x^{\dagger})^{*}F^{\prime}(x^{\dagger}))^{\mu}\omega is satisfied for some 0<\mu\leq 1/2 with sufficiently small \|\omega\|. Deriving convergence rates for the method (3) in its full generality is a very challenging task. The main difficulties arise from several aspects: the possible nonlinearity of the forward operator F, the non-quadratic nature of the regularization functional {\mathcal{R}}, the non-Hilbertian structure of the underlying space X, and the possible use of variable step-sizes \gamma_{k}^{\delta}. When F is a bounded linear operator and the step-size is constant, the convergence rate analysis on the corresponding method has been rigorously studied in [17]. In that work, through the interpretation of the method as a dual gradient method, the convergence rates have been derived successfully under varational source conditions ([14]) when the iteration is terminated by either an a priori stopping rule or the discrepancy principle. The approach developed in [17] heavily relies on the linearity of the forward operator and the constancy of the step-size. However, it is unclear how to extend the strategy presented in [17] to address the equation (1) when the forward operator is nonlinear or when variable step sizes are employed. In this paper, we will develop entirely novel ideas to tackle the challenge of deriving convergence rate of the method (3) under the benchmark source condition \displaystyle\xi^{\dagger}:=\xi_{0}+F^{\prime}(x^{\dagger})^{*}\lambda^{% \dagger}\in\partial{\mathcal{R}}(x^{\dagger})\mbox{ for some }\lambda^{\dagger% }\in Y (7) on the sought solution x^{\dagger}, where \partial{\mathcal{R}} denotes the subdifferential of {\mathcal{R}}. Note that, under the tangential cone condition, (2) can be equivalently stated as \displaystyle\min\left\{D_{\mathcal{R}}^{\xi_{0}}(x,x_{0}):F^{\prime}(x^{% \dagger})(x-x^{\dagger})=0\right\}. The Karush-Kuhn-Tucker (KKT) condition for this problem is exactly (7). In the context of ill-posed problems in infinite-dimensional spaces, the mere existence of a solution does not guarantee the satisfaction of the KKT condition. Therefore, (7) is commonly referred to as a source condition. This source condition has been extensively utilized in deriving convergence rates for regularization methods of ill-posed inverse problems ([6, 14, 17]). Under the range invariance condition (5) on F and the source condition (7) on x^{\dagger}, we will show that the convergence rate \|x_{k_{\delta}}^{\delta}-x^{\dagger}\|=O(\delta^{1/2}) holds if k_{\delta} is chosen by a suitable a priori stopping rule or the discrepancy principle (6). Our results hold for any strongly convex regularization functional {\mathcal{R}} with great flexibility on step-sizes by allowing the use of variable ones. Thus constant and adaptive choices of step-sizes are covered by our results. The key point for deriving our convergence rate is the observation that, under the condition (5), \xi_{k}^{\delta}-\xi_{0}\in\mbox{Ran}(F^{\prime}(x^{\dagger})^{*}) for every k. This motivates us to reformulate the method (3) in an equivalent manner which defines an auxiliary sequence \{\lambda_{k}^{\delta}\} such that \xi_{k}^{\delta}=\xi_{0}+F^{\prime}(x^{\dagger})^{*}\lambda_{k}^{\delta} for all k. Based on this sequence \{\lambda_{k}^{\delta}\}, the source condition (7), and the sequence \{x_{k}^{\delta}\}, we can construct a sequence of quantities which obey a nice recursive relation. Through the investigation of these quantities leads us to obtain the desired convergence rate under an a priori stopping rule. Based on this a priori result, after a careful comparison we obtain the convergence rate when the method is terminated by the discrepancy principle. Furthermore, we extend the idea to deal with the stochastic mirror descent method, which is a stochastic version of the method (3) for solving nonlinear ill-posed systems; we consider the exact data case and obtain an almost sure convergence rate result under a benchmark source condition. This paper is organized as follows. In Section 2 we collect some basic facts of convex analysis in Banach spaces. In Section 3 we devote to deriving convergence rates of the Landweber-type method (3) when terminated by either a priori stopping rules or the discrepancy principle. In Section 4 we extend the idea to derive convergence rate for the stochastic mirror descent method for nonlinear ill-posed systems with exact data. Finally, in Section 5 we provide some numerical results to validate the theoretical results on convergence rates."
https://arxiv.org/html/2411.09090v1,"A Vectorial Envelope Maxwell Formulation for Electromagnetic Waveguides 
with Application to Nonlinear Fiber Optics","This article presents an ultraweak discontinuous Petrov–Galerkin (DPG) formulation of the time-harmonic Maxwell equations for the vectorial envelope of the electromagnetic field in a weakly-guiding multi-mode fiber waveguide. This formulation is derived using an envelope ansatz for the vector-valued electric and magnetic field components, factoring out an oscillatory term of \exp(-i\mathsf{k}z) with a user-defined wavenumber \mathsf{k}, where z is the longitudinal fiber axis and field propagation direction. The resulting formulation is a modified system of the time-harmonic Maxwell equations for the vectorial envelope of the propagating field. This envelope is less oscillatory in the z-direction than the original field, so that it can be more efficiently discretized and computed, enabling solution of the vectorial DPG Maxwell system for 1000\times longer fibers than previously possible. Different approaches for incorporating a perfectly matched layer for absorbing the outgoing wave modes at the fiber end are derived and compared numerically. The resulting formulation is used to solve a 3D Maxwell model of an ytterbium-doped active gain fiber amplifier, coupled with the heat equation for including thermal effects. The nonlinear model is then used to simulate thermally-induced transverse mode instability (TMI). The numerical experiments demonstrate that it is computationally feasible to perform simulations and analysis of real-length optical fiber laser amplifiers using discretizations of the full vectorial time-harmonic Maxwell equations. The approach promises a new high-fidelity methodology for analyzing TMI in high-power fiber laser systems and is extendable to including other nonlinearities.","Motivation. Optical fibers are electromagnetic waveguides that transmit light very efficiently (i.e. with low losses) over long distances and are useful for a vast number of applications. Fiber laser amplifiers are optical fiber devices designed for achieving highly coherent light sources with high power outputs. The combination of high average powers and extremely high beam qualities make fiber lasers an important technology in many industrial, defense, and scientific applications [jauregui2013fiber]. However, the efforts of power-scaling beam combinable fiber amplifiers have encountered roadblocks in the form of nonlinear effects [agrawal, jauregui2013fiber] including the thermally-induced transverse mode instability (TMI). TMI is characterized by a sudden reduction of quality and stability of the beam emitted by a fiber laser system once a certain power threshold has been reached; this nonlinearity has revealed itself as the one of the strongest limitations for the average power scaling of current fiber laser systems [jauregui2020tmi]. Modeling and simulation play a decisive role in understanding the TMI and other detrimental nonlinear effects, finding mitigation strategies, and informing fiber architectures. Literature. This paper is an extension of the work done in [nagaraj2018raman, henneking2021fiber, henneking2021phd, henneking2022parallel]. Nagaraj et al. [nagaraj2018raman] developed a 3D vectorial Maxwell model to simulate passive Raman gain amplification in an optical fiber. Building on that framework, we added the ability to model the more common active gain amplification through a rare-earth, lanthanide metal dopant in the fiber core region [henneking2021fiber]; the implementation of this fiber amplifier model was then extended to support large-scale numerical simulations [henneking2021phd, henneking2022parallel]. The model is discretized with the discontinuous Petrov–Galerkin (DPG) finite element method with optimal test functions [demkowicz2017dpg]. As an ultraweak DPG formulation, the model can make use of robust automatic hp-adaptive algorithms [chakraborty2024hp] and advanced solvers for wave propagation [petrides2021adaptive, badger2023scalable]. The computational expense for solving the vectorial Maxwell fiber amplifier model increases slightly more than linearly with the number of wavelengths (which is proportional to the length of the waveguide) due to the effect of numerical pollution [henneking2021pollution, melenk2023waveguide1, melenk2020maxwell, babuska1997pollution]. An efficient parallel implementation of the model, using high-order discretization with fast integration techniques [mora2019fast, badger2020fast], is able to simulate thousands of wavelengths in an optical fiber amplifier [henneking2022parallel]. Because of the short wavelength of the light (\mathcal{O}(\mu\text{m})), however, these computations are limited to fiber lengths of only a few centimeters when real-length fiber amplifiers are several meters long. Given the exceedingly large number of wavelengths within a real-length fiber amplifier, it is computationally intractable to resolve the wavelength scale with a full vectorial Maxwell model. A common approach for fiber amplifier modeling is thus to reduce the complexity of the Maxwell equations by introducing additional assumptions or approximations of the physics involved, leading to simplified but computational efficient models such as coupled-mode-theory [naderi2013tmi, goswami2021fiber] or beam propagation models [gonthier1991bpm, saitoh2001bpm, ward2013bpm]. However, the variety of assumptions made in their derivations may limit their ability to accurately capture some of the nonlinear optical phenomena in fiber amplifiers, which motivates our work on developing Maxwell formulations for optical waveguides that enable higher-fidelity simulations of fiber laser systems. Contributions. The main contributions of this paper are: (1) derivation of a vectorial envelope DPG formulation of the time-harmonic Maxwell equations that can be efficiently discretized and solved inside a weakly-guiding optical fiber waveguide; (2) derivation and analysis of different approaches for implementing absorbing boundaries with a stretched coordinate perfectly matched layer (PML) at the waveguide output; (3) application of the vectorial envelope DPG Maxwell formulation to a fiber laser amplifier model that includes nonlinear gain and thermal effects via coupling to the heat equation; and (4) numerical results of the fiber amplifier model showing the three regimes, stable, transition, and chaotic, of thermally-induced TMI nonlinearity. The contributions made in this paper enable efficient computations of the propagating electromagnetic fields inside weakly-guiding multi-mode optical waveguides. To the best of our knowledge, this is the first Maxwell model capable of solving for the full vectorial electromagnetic field in a real-length fiber laser amplifier and the first vectorial Maxwell model capable of capturing the onset of the TMI phenomenon."
https://arxiv.org/html/2411.09029v1,Newton’s Method Applied to Nonlinear Boundary Value Problems: A Numerical Approach,"This work investigates the application of the Newton’s method for the numerical solution of a nonlinear boundary value problem formulated through an ordinary differential equation (ODE). Nonlinear ODEs arise in various mathematical modeling contexts, where an exact solution is often unfeasible due to the intrinsic complexity of these equations. Thus, a numerical approach is employed, using Newton’s method to solve the system resulting from the discretization of the original problem. The procedure involves the iterative formulation of the method, which enables the approximation of solutions and the evaluation of convergence with respect to the problem parameters. The results demonstrate that Newton’s method provides a robust and efficient solution, highlighting its applicability to complex boundary value problems and reinforcing its relevance for the numerical analysis of nonlinear systems. It is concluded that the methodology discussed is suitable for solving a wide range of boundary value problems, ensuring precision and stability in the results.","Ordinary differential equations (ODEs) play a central role in mathematical modeling, being used in fields ranging from physics and engineering to biology and economics [1, 2]. In particular, boundary value problems for ODEs appear in various applications, such as the description of heat transfer phenomena, fluid mechanics, and population growth [3]. However, the exact solution of these equations is not always possible, especially when the equation is nonlinear, which requires numerical methods to obtain approximate solutions [4]. Among numerical methods, Newton’s method has been widely used due to its efficiency and rapid convergence, especially in nonlinear systems [5]. In the context of nonlinear ODEs, it becomes a powerful tool for solving boundary value problems by transforming the initial problem into a system of nonlinear equations, whose solution is iteratively approximated [6]. This approach is particularly relevant in situations where the behavior of the solution is complex and requires refinement of successive approximations. The present work aims to apply Newton’s method to solve a nonlinear boundary value problem, based on the formulation proposed by [4]. The differential equation addressed represents a typical example of a nonlinear problem, and the boundary conditions impose additional challenges that require robust numerical strategies. Using an interval discretization and iterative formulations, this study presents an approximate solution to the proposed problem, along with a detailed analysis of the convergence and accuracy of the results. Thus, the objective of this work is to demonstrate the effectiveness of Newton’s method in solving nonlinear boundary value problems, providing a practical approach and a theoretical basis that can be applied to a wide range of similar problems. The analysis of the results obtained illustrates the potential of this method in providing stable and reliable numerical solutions for complex ODEs, reinforcing its applicability and importance in the field of numerical analysis."
https://arxiv.org/html/2411.08986v1,"Error Bounds and Parameter Scalings for
the 
Time Relaxation Reduced Order Model","The a priori error analysis of reduced order models (ROMs) for fluids is relatively scarce. In this paper, we take a step in this direction and conduct numerical analysis of the recently introduced time relaxation ROM (TR-ROM), which uses spatial filtering to stabilize ROMs for convection-dominated flows. Specifically, we prove stability, an a priori error bound, and parameter scalings for the TR-ROM. Our numerical investigation shows that the theoretical convergence rate and the parameter scalings with respect to ROM dimension and filter radius are recovered numerically. In addition, the parameter scaling can be used to extrapolate the time relaxation parameter to other ROM dimensions and filter radii. Moreover, the parameter scaling with respect to filter radius is also observed in the predictive regime.","The incompressible Navier-Stokes equations (NSE) are \displaystyle\mathbf{u}_{t}+(\mathbf{u}\cdot\nabla)\mathbf{u}-\nu\Delta\mathbf% {u}+\nabla p \displaystyle=\mathbf{f}, (1) \displaystyle\nabla\cdot\mathbf{u} \displaystyle=0, (2) where \mathbf{u} and p are the velocity and pressure fields, respectively, defined on the spatial domain, \Omega, and the time interval, [0,T]. \mathbf{f} is an external force, and \nu is the inverse of the Reynolds number. Appropriate boundary and initial conditions are needed to close the system. Fluid flows at high Reynolds numbers exhibit a wide range of spatial and temporal scales that make their direct numerical simulation (DNS) often impractical [1, 2]. This leads to the need of alternative computational approaches, such as large eddy simulations (LES), Reynolds-averaged Navier–Stokes equations (RANS), and numerical regularizations. One type of regularization is the time relaxation model (TRM) [3, 4], which leverages spatial filtering to increase the numerical stability. The TRM for a domain \Omega\subset\mathbb{R}^{d}, d=2\text{ or }3, and for t>0 is given as \displaystyle\mathbf{u}_{t}+(\mathbf{u}\cdot\nabla)\mathbf{u}-\nu\Delta\mathbf% {u}+\chi\mathbf{u}^{*}+\nabla p \displaystyle=\mathbf{f}, (3) \displaystyle\nabla\cdot\mathbf{u} \displaystyle=0, (4) where the dimensionless parameter \chi is called the time relaxation parameter, which is often manually tuned to adjust the numerical stabilization, and \mathbf{u}^{*} is a regularization term defined in Section 2. The goal of \mathbf{u}^{*} is to drive the unresolved fluctuations of \mathbf{u} down to 0. TRM has been investigated in [5, 6, 7, 8, 9, 10, 11] and has been used in various applications [12, 13]. A TRM review can be found in [14]. Although the DNS computational cost is significantly reduced by LES, RANS, and numerical stabilization, these approaches remain computationally prohibitive in decision-making applications where multiple forward simulations are needed. In those cases, reduced order models (ROMs) represent efficient alternatives. ROMs are computational models whose dimension is orders of magnitude lower than the dimension of full order models (FOMs), i.e., models obtained from classical numerical discretizations. In the numerical simulation of fluid flows, Galerkin ROMs (G-ROMs), which use data-driven basis functions in a Galerkin framework, have provided efficient and accurate approximations of laminar flows, such as the two-dimensional flow past a circular cylinder at low Reynolds numbers [15, 16]. However, turbulent flows are notoriously hard for the standard G-ROM. Indeed, to capture the complex dynamics, a large number [17] of ROM basis functions is required, which yields high-dimensional ROMs that cannot be used in realistic applications. Thus, computationally efficient, low-dimensional ROMs are used instead. Unfortunately, these ROMs are inaccurate since the ROM basis functions that were not used to build the G-ROM have an important role in dissipating the energy from the system [18]. Indeed, without enough dissipation, the low-dimensional G-ROM generally yields spurious numerical oscillations. Thus, closures and stabilization strategies are required for the low-dimensional G-ROMs to be stable and accurate [18, 19, 20, 21, 22, 23]. FOM stabilizations and closures are supported by thorough numerical analysis, particularly when applied alongside traditional methods like the finite element method (FEM) or SEM [1, 24, 25, 26]. These references address both fundamental numerical analysis issues, such as stability and convergence, and practical challenges, like determining appropriate parameter scalings for stabilization coefficients. These two aspects are closely linked, as insights from numerical analysis guide the selection of parameter scalings, which inform practical decisions. We emphasize, however, that despite growing interest in ROM closures and stabilizations, their comprehensive mathematical and numerical analysis remains an open challenge. Indeed, while some strides have been made in analyzing ROM closures and stabilizations [27, 28, 29, 30, 31, 32, 33], much work is needed to reach the rigor of FOM analysis. In this paper, we take a step in this direction by establishing the first rigorous numerical analysis results, including stability and a priori bounds, for the time relaxation reduced order model (TR-ROM), which was successfully used in [34] in numerical simulations of turbulent channel flow. Crucially, we also derive parameter scalings that ensure ROM parameters automatically adjust with changes in the corresponding FOM and ROM parameters, eliminating the need for manual tuning often required in existing data-driven ROMs. This article is organized as follows: In Section 2, we give preliminaries about the SEM, G-ROM, and ROM filtering. In Section 3, we present the TR-ROM, prove its unconditional stability and an a priori error bound, and derive novel scalings for the time relaxation parameter. In Section 4, we show that the theoretical convergence rates and parameter scalings with respect to ROM dimension and filter radius are numerically recovered for the 2D flow past a cylinder and 2D lid-driven cavity. In Section 5, we present the conclusions of our theoretical and numerical investigations."
https://arxiv.org/html/2411.09555v1,Image Processing for Motion Magnification,"Motion Magnification (MM) is a collection of relative recent techniques within the realm of Image Processing. The human visual field can not capture all possible displacement of an object of interest, in particular the smallest one. This is the main motivation of introducing these techniques, in fact, the goal is to opportunely process a video sequence to obtain as output a new video in which motions are magnified and visible to the viewer. These techniques can amplify very subtle motions, imperceptible to the human eye, on proper video sequences, made by the aid of high resolution and high speed cameras. In this work, we propose some preliminary results on MM, developing a technique called Phase-Based Motion Magnification which is performed in the Fourier Domain and rely on the Fourier Shifting Property. In particular, we show the mathematical motivation at the foundation of this method, focusing on some basic test made up using synthetic images.","Image Processing, which usually abbreviates Digital Image Processing (DIP), consists on an ensemble of methods which enable to manipulate or altering an image to obtain a desired result, typically for improving its visual quality or extracting useful information from it. Image processing requires the use of a variety of techniques and algorithms to modify or analyze images and it is a fundamental component of artificial intelligence, computer vision and many other fields, [12]. In general, an image, can be seen as a two-dimensional function I(x,y), where x and y are the spatial (or plane) coordinates and the amplitude I at any pair of coordinates (x,y) is called intensity or gray level of the image at that point. The manipulation of the image is realized through calculators, for this reason both x and y has to be discrete quantities, and so we refer to the image as a Digital Image. Moreover, since it carries information through the two axes, we can intend it as a two dimensional signal, and for this cause DIP can be seen as a specialized form of Digital Signal Processing, where the signal is a two dimensional array rather than a one-dimensional time series. Many of the same mathematical tools are used in both fields, such as the Fourier Transform. The discussion we have done until now can be naturally extended to videos, since they are composed by a sequence of images, also known as frames. So, a video, is a three dimensional signal I(x,y,t) since the information varies no solely over the spatial coordinates, but also over time. In the context of DIP, we introduce a technique called Motion Magnification (MM). This is a relatively recent technique, proposed by a research group of the Massachusetts Institute of Technology [15], [20], [21]. More specifically, this technique establish its roots in DIP. In fact, given a input video I(x,y,t) it is possible, through appropriate manipulation - by working on the individual frames that make up the video - to produce an output \tilde{I}(x,y,t). This output video can reveals subtle color changes and, more importantly, movements that would be invisible to the naked eye. The primary goal of MM is the detection of those “invisible” movements that are indeed present, and that, when analyzed, can provide significant information about the object, structure, or even person captured in the video. In general, the input video, which may seem static to the observer, can actually contain extremely subtle movements. Objects in the scene could shift by as little as \frac{1}{50} or \frac{1}{100} of a pixel. Through a opportune procedure, these sub-pixel motions can be amplified, transforming them into more pronounced displacements that stretch across multiple pixels, making them visible. Essentially, we can think of MM as a kind of “microscope” that, instead of zooming in to reveal more detailed visuals, actually “zoom in” on movements or color changes. Imperceptible movements are often much faster than what the human eye can detect, which explains why it is necessary to use appropriate video cameras. These cameras offer the possibility of collecting high-density spatial data (high pixel resolution) at high-speed (high frame rate) from a distant scene of interest. MM has a various field of applications. For instance, there are numerous examples of its application in the medical field. These methodologies can be applied for monitoring respiration, specifically aiming to estimate breath rate using non-contact methods, [2], [17], or to introduce non invasive method based on MM for the study of heart failure, [1], [14]. Another particular branch of application of these techniques is vibration analysis, [6]. This has become a widely and powerful tool that allows engineers to study how structures respond to vibrations. This has become a widely and powerful tool that allows engineers to study how structures respond to vibrations. The estimation of the vibration, is usually performed using the classical contact devices, such as accelerometers. Recently, to perform this task, non-contact devices such as video cameras, are used for monitoring vibrating systems, [3], [4], [5], [8]. The scientific literature about this subject is extensive. For example, [9] applied motion magnification and image processing techniques to extract the frequency content of vibrations in several ancient structures in Rome and Istanbul. Additionally, [7] conducted a study on Structural Health Monitoring, aiming to identify the exact instant of occurrence for damage or sudden structural changes. Moreover, [22] for rotating fault diagnosis can be mentioned. In this paper, we propose a preliminary study of the so called Phase-based MM, [20]. This approach to MM is performed in the frequency domain, and exploits the Fourier Shift Theorem. This algorithm is effective for global movements due to the Fourier basis, which is defined in the entire domain. The magnification procedure has been implemented in MATLAB and the “Code metadata” are: Current code version v. 1.0 Permanent link to repository https://github.com/eTrebo98/MotMagArt1 Code versioning system used git Software code languages MATLAB This paper is organized as follows. In section 2, we explore the mathematical background under the proposed procedure, describing the steps needed to perform the magnification of a shift between two consecutive frame of a video. In section 3, since the algorithm needs to be applied on a calculator, we focus our attention on the discretized version of it, introducing the Discrete Fourier Transform (DFT). Next, in section 4, we show some numerical results made up with MATLAB using synthetic gray-scale images. We put much emphasis on presenting the amplification of the movement between two consecutive frames, furthermore we consider also an example of an output video sequence in which the shift are exaggerated respect to the input one. In section 5, we provide conclusions and future developments."
https://arxiv.org/html/2411.09530v1,Discrete Dirac structures and discrete Lagrange–Dirac dynamical systems in mechanics,"In this paper, we propose the concept of (\pm)-discrete Dirac structures over a manifold, where we define (\pm)-discrete two-forms on the manifold and incorporate discrete constraints using (\pm)-finite difference maps. Specifically, we develop (\pm)-discrete induced Dirac structures as discrete analogues of the induced Dirac structure on the cotangent bundle over a configuration manifold, as described by Yoshimura and Marsden [41]. We demonstrate that (\pm)-discrete Lagrange–Dirac systems can be naturally formulated in conjunction with the (\pm)-induced Dirac structure on the cotangent bundle. Furthermore, we show that the resulting equations of motion are equivalent to the (\pm)-discrete Lagrange–d’Alembert equations proposed in [7, 32]. We also clarify the variational structures of the discrete Lagrange–Dirac dynamical systems within the framework of the (\pm)-discrete Lagrange–d’Alembert–Pontryagin principle. Finally, we validate the proposed discrete Lagrange–Dirac systems with some illustrative examples of nonholonomic systems through numerical tests.","Variational integrators and nonholonomic integrators. It is well known that discrete Hamilton’s variational principle is one of the structure-preserving numerical integrators, providing a long-time numerically stable scheme for conservative Lagrangian systems. This principle is based on the fact that the discrete Euler–Lagrange equations preserve a discrete symplectic structure, called the discrete Lagrangian two-form, along the discrete Lagrangian map, which is the discrete analogue of the flow of the Lagrangian vector field in the continuous setting. On the other hand, when nonholonomic constraints are present, as in nonholonomic mechanical systems, the associated equations of motion can be derived using the Lagrange–d’Alembert principle. In the discrete setting, discrete Hamilton’s principle is replaced by the discrete Lagrange–d’Alembert principle, from which the discrete Lagrange–d’Alembert equations can be obtained, as discussed in [7]. These algorithms, which generalize variational integrators for unconstrained Lagrangian systems, exhibit geometric properties similar to those of continuous nonholonomic systems. From a slightly different perspective, [32] considered nonholonomic systems that admit reversing symmetries and developed integrators for such systems that preserve an analogous reversing symmetry. In these cases, the numerical integrator, referred to as a ""nonholonomic integrator,"" no longer preserves the discrete symplectic structure. In fact, it is not yet fully understood what structure, if any, is preserved by such a nonholonomic integrator. Nevertheless, it remains a long-term numerically stable scheme for conservative nonholonomic mechanical systems. Dirac structures in mechanics. The notion of Dirac structures generalizes presymplectic and almost Poisson structures and was developed by [14, 11, 9, 10]. Dirac structures can directly incorporate constraint distributions into the framework of Hamiltonian systems. Initially, they were applied to constrained Hamiltonian systems, sometimes referred to as implicit Hamiltonian systems [9, 36, 2, 6, 35]. Later, [41] explored Dirac dynamical system on the Lagrangian side, focusing on Lagrangian systems with nonholonomic constraints. Specifically, they examined bundle structures over a configuration manifold Q, including Tulczyjew’s triple TT^{\ast}Q,T^{\ast}TQ,T^{\ast}T^{\ast}Q, and introduced the Dirac differential \mathbf{d}_{D}L:TQ\to T^{\ast}T^{\ast}Q for a given Lagrangian on TQ, possibly degenerate. This led to the definition of Lagrange–Dirac dynamical systems, which serves as the Lagrangian version of Dirac dynamical systems. The work included some illustrative examples of nonholonomic mechanical systems and electric circuits. Over the past years, various physical systems—such as continuum systems, and nonequilibrium thermodynamic systems as well as nonholonomic mechanical systems and electric circuits—have been formulated within the context of Dirac dynamical systems [41, 16, 17, 19]. Variational structures of Dirac dynamical systems. The variational structure of Lagrange–Dirac dynamical systems was further clarified by [42] in the context of the Lagrange–d’Alembert–Pontryagin principle. As previously mentioned, for nonholonomic Lagrangian systems, the equations of motion can be derived from the Lagrange–d’Alembert principle. Specifically, this involves taking variations of the action integral for the Lagrangian, where one must impose variational constraints by choosing variations of curves in the configuration manifold so that the variations lie within the distribution and also satisfy the nonholonomic constraints on the motion of the system. The Lagrange–d’Alembert–Pontryagin principle, an extension of the Lagrange–d’Alembert principle to the Pontryagin bundle TQ\oplus T^{\ast}Q, recovers the equations of motion for Lagrange–Dirac dynamical systems. Additionally, the Hamiltonian analogue of Lagrange–Dirac dynamical systems was developed through the induced Dirac structures [42]. Discretization of Dirac structures. Several approaches to discretizing Dirac structures have been proposed, such as those by [25] and [5]. The former approach considered the discretization of induced Dirac structures on vector spaces together with their associated Lagrange–Dirac systems. This work aimed to develop the discrete analogue of the induced Dirac structures and the associated Lagrange–Dirac dynamical systems proposed in [41]. By comparing with the continuous Tulczyjew’s triple (see, e.g., [34]), [25] defined a discrete triple, which was then used to introduce their notion of discrete Dirac structures. However, [5] later noted that the discrete structures proposed by [25] do not constitute true Dirac structures. In contrast, [5] proposed an alternative approach, constructing discrete Dirac structures on a discrete Pontryagin bundle over a configuration manifold, rather than on the cotangent bundle. This alternative approach aims to develop a general Dirac system instead of constructing the implicit Lagrangian and Hamiltonian systems. Our goals and the organization of this Paper. The main goal of this paper is to propose an alternative discrete version of the induced Dirac structure on the cotangent bundle over a configuration manifold, enabling the construction of a discrete version of the Lagrange–Dirac dynamical system, consistent with the approach outlined in [41]. We begin by presenting a discretization of continuous Dirac structures on a manifold M, defined by a two-form \Theta_{M} and a distribution \Delta_{M}. This is achieved by introducing finite forward difference and backward difference maps. Next, we extend the concept of continuous Dirac structures to the induced Dirac structure on the cotangent bundle M=T^{\ast}Q. In this context, we introduce (\pm)-discrete symplectic forms and (\pm)-discrete constraint spaces, which are discrete analogues of the canonical symplectic forms and distributions on Q. To construct the discrete Lagrange–Dirac systems (or the discrete implicit Lagrangian systems) within the framework of the proposed discrete Dirac structures, we consider the bundle structures between T^{\ast}T^{\ast}Q, T^{\ast}Q\times T^{\ast}Q and T^{\ast}(Q\times Q). These structures serve as the discrete analogues of Tulczyjew’s triple between T^{\ast}T^{\ast}Q, TT^{\ast}Q and T^{\ast}TQ in the continuous setting. We define the discrete flat maps (\Omega^{d\pm}_{T^{\ast}Q})^{\flat}:T^{\ast}Q\times T^{\ast}Q\to T^{\ast}T^{% \ast}Q, which are skew-symmetric bundle maps over T^{\ast}Q naturally induced by the discrete canonical symplectic structures \Omega^{d\pm}_{T^{\ast}Q}. We also introduce the discrete analogues of the Dirac differential of the Lagrangian, defined as the maps d^{\pm}_{D}L:Q\times Q\to T^{\ast}T^{\ast}Q. Furthermore, we present the (\pm)-associated discrete Lagrange–d’Alembert–Pontryagin principles, which yield the (\pm)-discrete equations of motion for the discrete Lagrange–Dirac dynamical systems. Finally, we show that the resulting (\pm)-discrete system equations can recover the discrete Lagrange–d’Alembert equations given in [7, 32], clarifying that the resulting nonholonomic integrators preserve the discrete induced Dirac structures. We then validate the proposed approach with numerical tests on two illustrative examples of nonholonomic systems. This paper is organized as follows: • §2 and §3: We provide a brief review of variational formulations in mechanics, covering both continuous and discrete settings. • §4: We describe the continuous setting of Dirac structures and the associated Lagrange–Dirac systems, in which a Dirac structure is defined by a given distribution on a configuration manifold and the canonical symplectic structure on the cotangent bundle. • §5: We introduce the concept of (\pm)-discrete Dirac structures on a manifold. This includes defining discrete two-forms and discrete constraint spaces over the manifold in conjunction with (\pm)-finite difference maps. In particular, we illustrate the discrete theory by presenting the discrete induced Dirac structures on the cotangent bundle of a configuration manifold. • §6: We develop the notion of (\pm)-discrete Lagrange–Dirac dynamical systems within the framework of the (\pm)-discrete induced Dirac structures on the cotangent bundle. These formulations are also derived from the (\pm)-discrete Lagrange–d’Alembert–Pontryagin principles. We show that the resultant (\pm)-discrete Lagrange–d’Alembert–Pontryagin equations are equivalent to the system equations obtained from the (\pm)-discrete Lagrange–Dirac dynamical systems. • §7: We demonstrate that the discrete Lagrange–d’Alembert equations developed by [7, 32] can be recovered from the (\pm)-discrete Lagrange–d’Alembert–Pontryagin equations. This implies that the resulting (\pm)-nonholonomic integrators preserve the (\pm)-induced discrete Dirac structures. • §8: We show the validity of our discrete theory by illustrative examples of a vertical rolling disk on a plane and a classical Heisenberg system through numerical tests."
https://arxiv.org/html/2411.09511v1,Structure-informed operator learning for parabolic Partial Differential Equations,"In this paper, we present a framework for learning the solution map of a backward parabolic Cauchy problem. The solution depends continuously but nonlinearly on the final data, source, and force terms, all residing in Banach spaces of functions. We utilize Fréchet space neural networks (Benth et al. (2023)) to address this operator learning problem. Our approach provides an alternative to Deep Operator Networks (DeepONets), using basis functions to span the relevant function spaces rather than relying on finite-dimensional approximations through censoring. With this method, structural information encoded in the basis coefficients is leveraged in the learning process. This results in a neural network designed to learn the mapping between infinite-dimensional function spaces. Our numerical proof-of-concept demonstrates the effectiveness of our method, highlighting some advantages over DeepONets.","In this paper we provide a framework for learning the solution map of a backward parabolic Cauchy problem incorporating structural information about the final datum, source, and the force term. The solution of the Cauchy problems depends in a nonlinear, but continuous way on the final datum, the source and force terms, which are all functions living in appropriate Banach spaces. We propose to use the recently introduced neural networks in Fréchet spaces (see Benth et al. (2023)), an infinite dimensional neural network structure, to solve this operator learning problem. Our approach provides an alternative to the operator learning method Deep Operator Networks (DeepONets), studied by Lu et al. (2019) and Lu et al. (2021). DeepONet extends the shallow network for operator learning proposed and analysed in Chen & Chen (1995). The validity of these operator learning approaches is resting on the universal approximation theorem (see Chen & Chen (1995), Lu et al. (2021) and more recently a generalisation by Lanthaler et al. (2022) to measurable operators). Instead of using finite-dimensional neural networks approximating sampled (also called censored) expressions of the input functions in the operator in question, as done in DeepONets, we make use of the information contained in the basis functions spanning the relevant function spaces. We build a neural network which is learning the map between these function spaces expressed by their basis functions. An infinite dimensional activation function allows us to set up a deep neural network that preserves the structural information encoded in the basis coefficients when processing through the layers. Our approach rests on a truly infinite dimensional neural network, reflecting that we are approximating continuous nonlinear operators between infinite dimensional spaces. Implemented on a computer, we sample a finite set of basis functions in the training rather than censoring the input functions to have finite dimensional approximations. We refer also to (Kovachki et al., 2022, Sect. 2, p. 9) where a similar idea was mentioned but not further explored. Often linear operators between function spaces can be expressed as integral operators, for example as convolutions. An approximation of such integral operators using graph kernels is proposed in Anandkumar et al. (2019) for operator learning of partial differential equations. The authors take an infinite dimensional perspective in learning operator maps from various parameters into the solution, viewed as continuous mappings between function spaces of Sobolev type. The affine transform-part of the neural network is viewed as an integral kernel operator, and a discrete version of this is mapped to the next layer by a finite dimensional activation function. These ideas are further expanded into Fourier neural operators (see Li et al. (2021); Kovachki et al. (2022), where the kernel is represented by the Fourier transform and its inverse to obtain computationally attractive representations for estimating the kernel operator. Cao et al. (2024) propose to use the Laplace transform instead of the Fourier transform, taking advantage of the pole-residue relationship between the input and output space of the operator to be approximated. In Kovachki et al. (2022) a universal approximation theorem is shown for networks learning nonlinear operators between certain Banach spaces of functions with infinite dimensional (integral operator) affine transforms, but finite dimensional activation functions. In a recent paper Li et al. (2024) the Fourier neural operator methodology is applied in conjunction with physics informed learning. We refer the reader to the extensive literature review on operator and physics informed learning learning in Li et al. (2024) (see also more references in Cao et al. (2024)). In Benth et al. (2023) a universal approximation theorem was shown which ensures that continuous operators from a Fréchet space into a Banach space can be approximated on compacts with Fréchet neural networks. In our analysis of the parabolic Cauchy problem, using the classical theory of e.g. Friedman (1975), the solution map can be shown to be Lipschitz continuous from a product of Sobolev spaces into the continuous functions on compacts. Thus, the universal approximation theorem ensures that we can approximate the operator arbitrary well by the Fréchet neural network. In Lanthaler et al. (2022) the operators they are interested in training using DeepONet are shown to be Lipschitz continuous. For the Fourier neural operators studied in Kovachki et al. (2022), proofs of the integrability properties of the operators seem to be missing for the operators they aim to train that enables them to use of their universal approximation theorem. Cao et al. (2024) do not address the regularity properties for the operators they study numerically. In a numerical proof-of-concept, we demonstrate that our proposed approach indeed works out. We compare with the DeepONets, and point out some advantages with our approach. We once again would like to emphasise also that we make use of infinite dimensional activation functions, and not finite dimensional ones as in DeepONet and the Fourier neural operator approaches, because for deep learning this allows us to progress structural information from the basis functions throughout the layers. Our approach and analysis extends the neural network approach to approximate numerically high dimensional partial differential equations by training using synthetic data generated by stochastic differential equations along with the Feynman-Kac formula, as advocated by E et al. (2017) and Han et al. (2018) (see also Beck et al. (2023) for an overview and further references). We can naturally make use of the Feynman-Kac formula also for operator learning problems related to Cauchy problems, as we show in this paper. In passing we remark that Benth et al. (2024) have made use of similar ideas to price options in energy markets which require an infinite dimensional framework. Learning the operator mapping certain parameter functions into the solution of partial differential equations is a forward problem, often referred to as “many query”. A neural operator method allows for fast and efficient computations of the solution for various specifications of the input parameter functions (see Lanthaler et al. (2022) for a discussion and analysis of the curse of dimensionality in such problems). The inverse problem, where observations of the dynamical system in question is available and one wants to back out parameters, is also of interest, and has been empirically investigated in a Bayesian framework in Li et al. (2021). Rather than learning the operator map for a specific dynamical system, Yang et al. (2023) propose a framework to learn operators connected to a family of differential equations. Empirical evidence demonstrates that commonalities across differential equations reduce the training burden in such an approach. Our results are presented as follows. In the next section we provide a review of infinite dimensional neural network introduced in Benth et al. (2023), collecting some useful material. Section 3 defines the Cauchy problem relying on the classical analysis of Friedman (1975), and identifies the operator maps that will be the core object of analysis in this paper. To use infinite dimensional neural networks to learn the operator maps, we need continuity properties to hold according to the universal approximation theorem. Continuity of the nonlinear operator maps are analysed and shown for Sobolev spaces in this Section. A numerical example is given in Section 4, providing a proof-of-concept. Here we are benchmarking our proposed method with the DeepONets-approach, and provide further extensions and perspectives of our proposed structure-informed operator learning approach."
https://arxiv.org/html/2411.09038v1,An Implementation of the Finite Element Method in Hybrid Classical/Quantum Computers,"This manuscript presents the Quantum Finite Element Method (Q-FEM) developed for use in noisy intermediate-scale quantum (NISQ) computers, and employs the variational quantum linear solver (VQLS) algorithm. The proposed method leverages the classical FEM procedure to perform the unitary decomposition of the stiffness matrix and employs generator functions to design explicit quantum circuits corresponding to the unitaries. Q-FEM keeps the structure of the finite element discretization intact allowing for the use of variable element lengths and material coefficients in FEM discretization. The proposed method is tested on a steady-state heat equation discretized using linear and quadratic shape functions. Numerical verification studies demonstrate that Q-FEM is effective in converging to the correct solution for a variety of problems and model discretizations, including with different element lengths, variable coefficients, and different boundary conditions. The formalism developed herein is general and can be extended to problems with higher dimensions. However, numerical examples also demonstrate that the number of parameters for the variational ansatz scale exponentially with the number of qubits to increase the odds of convergence, and deterioration of system conditioning with problem size results in barren plateaus, and hence convergence difficulties.Keywords: Quantum Computing; Quantum-Finite Element Method; Variational Quantum Linear Solver; Hybrid Quantum-Classical Algorithms.","Quantum computing is nearing an inflection point in its evolution, as larger and distributed quantum computing devices are becoming available. These advances are beginning to allow for the development of new methodologies and algorithms for solving computational mechanics problems using quantum computers with the hope of achieving problem scales that are not accessible with classical computing. Despite significant progress in development of efficient quantum error correction algorithms, the available quantum computers remain noisy and are typically referred to as noisy intermediate-scale quantum, or NISQ computers [36]. This manuscript explores a finite element method framework for hybrid classical-NISQ computers using the variational quantum linear solver (VQLS) algorithm. Evaluation of linearized systems of equations is a key component in most of the computational methods used for solving linear and nonlinear mechanics problems. Many of the approaches that leverage quantum computing so far consist of constructing a linear system of equations using a classical computer and evaluating this linear system by use of a quantum computer or a simulator. In their seminal work, Harrow, Hassidim, and Lloyd developed a quantum linear solver (HHL) algorithm that scales logarithmically with the linear system size and theoretically provides exponential speedup as compared to classical algorithms for systems of equations with certain properties [18]. Several improvements have been made to the original HHL algorithm since its inception (see e.g., [3, 11, 45, 48, 12])–allowing the solution of relatively large systems (e.g., 2^{17} equations evaluated in Ref. [33]). The HHL algorithm is suitable for fault tolerant systems and requires a level of quantum error correction that is not yet available in current NISQ devices. Quantum algorithms for iterative linear solvers offer a potential pathway to solving systems with high condition numbers [38, 39]. Bravo-Prieto et al. [7] proposed the Variational Quantum Linear Solver (VQLS) algorithm for solving linear systems in NISQ computers. VQLS is a variational quantum algorithm where the problem is evaluated via classical optimization with the associated cost function (and its derivatives) computed in a quantum computer. In VQLS, the matrix of the linear system is represented as a linear combination of unitary matrices. Quantum circuits are then prepared for each unitary matrix and used in cost function evaluations. A key difficulty in using VQLS is finding an efficient way to decompose the matrix [1]. A general methodology for symmetric real matrices as a sum of various combinations of Pauli’s spin matrices was developed in [34]. However, this approach requires solving for unknown coefficients corresponding to combinations of Pauli’s spin matrices, which are typically not readily available for general linear systems. Trahan et al. [47] recently employed this approach to solve one-dimensional time-independent Poisson’s equation and time-dependent heat and wave equations with finite element method using VQLS. The examples considered in their work consist of homogeneous material domains with constant element lengths in FEM discretization. It was concluded that the circuit depth and the number of unitaries required to decompose the matrix directly affect the time complexity of the VQLS algorithm, resulting in scalability issues for applications in practical linear systems [47]. Variational algorithms were shown to exhibit optimization convergence difficulties with increasing system size due to barren plateaus and poor representation of the solution space by the variational ansatzes [2, 9, 25, 49], subsequently attempts have been made to partially address these difficulties (see e.g., [6, 17, 24]). Variational quantum algorithms have recently been deployed to solve solid mechanics and structural mechanics problems. Liu et al. [27] integrated variational quantum eigensolver with the finite element method to compute the fundamental natural frequency of a structure. Lu et al. [28] solved the 4th order PDE Euler-Bernoulli beam equation for general boundary conditions with a variational quantum eigensolver method. The best practices for the variational quantum eigensolver have been reviewed in Ref. [46]. The performance of discrete finite elements with VQLS was recently investigated in the context of solving one-dimensional finite element problems in Trahan et al. [47]. An alternative type of quantum computer to gate-based systems, a quantum annealing machine, has been used to solve a truss problem discretized with finite elements in a box algorithm [43]. Additionally, it has been shown that quantum annealing can achieve high solution accuracy for nonlinear elasto-plastic problems in 1D and 2D [32]. Raisuddin and De [37] developed an iterative quantum annealing algorithm for finite element problems. This manuscript presents a new formulation of the finite element method (Q-FEM) based on the VQLS algorithm for its implementation in NISQ computers. The primary novel contribution of the proposed formulation is in its use of the finite element construction procedure to achieve a unitary representation of the global stiffness matrix. The proposed formulation allows for the automated development of efficient circuits needed for computing the VQLS cost function. The proposed formulation has been implemented in the context of steady state heat equation for elements with linear and quadratic shape functions, spatially varying properties and lengths, and subjected to natural and essential boundary conditions. The performance of the proposed approach was assessed as a function of problem characteristics (e.g., element type, problem size), as well as the variational ansatz used in problem parameterization. The proposed approach achieves significant efficiency advantage compared with generic matrix decomposition algorithms. The remainder of this manuscript is organized as follows: Section 2 provides an overview of the VQLS algorithm, as well as the state preparation method, variational ansatzes, and cost function used in this study. Section 3 discusses the Q-FEM implementation approach, specifically the treatment of the global stiffness matrix. In Section 4, we discuss unitary and quantum circuit construction in the context of one-dimensional steady-state heat equation discretized using linear and quadratic elements. Section 5 presents the numerical verification of Q-FEM relative to ansatz selection, discretization characteristics, and boundary conditions. Section 6 provides conclusions and directions for future work. Notation: Arbitrary vectors and matrices are represented by boldface lower and upper-case letters respectively, following the classical computational mechanics literature. The matrix-vector product of matrix \mathbf{A} with a vector \mathbf{b} is represented as \mathbf{A}\mathbf{b}. We use upper-case letters without boldface for unitary matrices (that represent circuit operations), for instance, the second order identity matrix is denoted as I. The quantum state vector, inner and outer products are denoted following a ‘bra-ket’ notation as described: a normalized quantum state vector is denoted as |\cdot\rangle, an un-normalized quantum state vector is represented with a boldface letter such as \ket{\mathbf{a}}, while the inner product between two quantum state vectors |a\rangle and |b\rangle is denoted with \langle a|b\rangle, and the outer product between the same states is written as |a\rangle\langle b|."
https://arxiv.org/html/2411.08905v1,Synthesis Method for Obtaining Characteristic Modes of Multi-Structure Systems,"This paper introduces an efficient method of characteristic mode decomposition for multi-structure systems. Our approach leverages the translation and rotation matrices associated with vector spherical wavefunctions, enabling the synthesis of a total system’s characteristic modes through independent simulation of each constituent structure. We simplify the computationally demanding translation problem by dividing it into three manageable sub-tasks: rotation, z-axis translation, and inverse rotation, which collectively enhance computational efficiency. Furthermore, this method facilitates the exploration of structural orientation effects without incurring additional computational overhead. To demonstrate the effectiveness of our approach, we present a series of compelling numerical examples that not only validate the accuracy of the method but also highlight its significant advantages.","The theory of characteristic modes is pivotal in antenna analysis and design [1, 2, 3]. Its derived form—substructure characteristic modes theory—has gained prominence for revealing the intrinsic electromagnetic properties of structures within complex environments [4, 5, 6]. This approach is increasingly employed in designing antennas for handheld devices and platform-mounted systems [7]. Recent extensions have unified various characteristic mode formulations, which were traditionally based on the method of moments (MoM), into a more robust scattering-based framework, significantly enhancing numerical efficiency [8, 9]. Despite these advancements, the computational burdens for systems with multiple structures, such as antenna arrays, remain substantial. In the unified framework [8, 9], characteristic modes are obtained by eigenvalue decomposition of the transition matrix (T-matrix) or scattering matrix. This matrix serves as an operator that connects the incident and structural scattering responses, solely determined by the properties of the structures within a designated enclosing sphere. This characteristic indicates the possibility to construct the T-matrix for combined structures from independent structural data, thus facilitating the rapid computation of characteristic modes for complex systems. This paper primarily focuses on optimizing and exploring this potential. Although extensive research in fields such as optical scattering has effectively elucidated and summarized the technique of deriving the total system’s T-matrix from the individual structures’ T-matrices [10, 11, 12, 13], particularly through employing the two translation properties of spherical wavefunctions [14, 15], we revisit this established method from a unique perspective. We have developed a fully matrixized representation that is not only concise and efficient but also facilitates easier understanding compared to conventional series representations. This is particularly advantageous in scenarios that require specifying the radiation background for characteristic mode decomposition, where our approach minimizes repetitive computations. Following the method outlined in [16], we decompose the general translation problem into three sub-steps: rotation, z-axis translation, and inverse rotation, eschewing the direct solutions typically employed in conventional studies. This significantly curtails computation time, as z-axis translations are inherently simpler. Moreover, by adjusting the translation direction using the rotation matrix, we can correlate problems that have identical translation distances but different directions—common in uniformly arranged structures—thereby enhancing the reuse of the translation matrix and further boosting computational efficiency. The integration of the rotation matrix within our method introduces additional degrees of freedom; notably, it enables alterations to the structure’s orientation during post-processing, obviating the need to recompute the T-matrix. This enhancement is crucial for investigating the effects of the structure’s posture or polarization. To substantiate our approach, we present a series of illustrative numerical examples that demonstrate the substantial advantages of our synthesis technique in deriving characteristic modes. These examples highlight the potential applications of our method in complex structures and antenna arrays."
https://arxiv.org/html/2411.08760v1,Energy Dissipation Preserving Physics Informed Neural Network for Allen-Cahn Equations,"This paper investigates a numerical solution of Allen–Cahn equation with constant and degenerate mobility, with polynomial and logarithmic energy functionals, with deterministic and random initial functions, and with advective term in one, two, and three spatial dimensions, based on the physics-informed neural network (PINN). To improve the learning capacity of the PINN, we incorporate the energy dissipation property of the Allen-Cahn equation as a penalty term into the loss function of the network. To facilitate the learning process of random initials, we employ a continuous analogue of the initial random condition by utilizing the Fourier series expansion. Adaptive methods from traditional numerical analysis are also integrated to enhance the effectiveness of the proposed PINN. Numerical results indicate a consistent decrease in the discrete energy, while also revealing phenomena such as phase separation and metastability.","Phase field models have a central role to understand the behaviour of the many complicated moving interface problems in material science [1], fluid dynamics [2], fracture mechanics [3, 4], image analysis [5], and mean curvature flow [6]. Among these models, Allen-Cahn introduced in [7] to describe the motion of anti-phase boundaries in crystalline solids at a fixed temperature is a particular case of gradient flows in the form of u_{t}=-\mu(u)\ \frac{\delta\mathcal{E}(u)}{\delta u}, (1) where \frac{\delta\mathcal{E}(u)}{\delta u} represents the variational derivative of the free energy taken in the L^{2}(\Omega)-norm with \Omega\subset\mathbb{R}^{d} (d=1,2,3) as follows \mathcal{E}(u)=\int_{\Omega}\left(\frac{\epsilon^{2}}{2}|\nabla u|^{2}+F(u)% \right)d{\bm{x}}. (2) Specifically, the Allen-Cahn equation corresponds to the following nonlinear partial differential equation (PDE) u_{t}=\mu(u)(\epsilon^{2}\Delta u-f(u)),\quad({\bm{x}},t)\in\Omega\times(0,T], (3) where u denotes the concentration of one the species of the alloy, known as the phase state between materials, \epsilon represents the small interfacial length during the phase separation process, and \mu(u) is the non–negative mobility function. As \epsilon tends towards zero, it forms boundaries between the two stable phrases u=\pm 1, which is a process known as phase separation. Moreover, the nonlinear f(u) is the derivative of a free energy functional F(u), which is characterized by logarithmic or polynomial functions. The logarithmic free energy potential (see, e.g., [8]) can be formulated as F(u)=\frac{\theta}{2}[(1+u)\ln(1+u)+(1-u)\ln(1-u)]-\frac{\theta_{c}}{2}u^{2},% \,\,\,\,\,0<\theta\leq\theta_{c}, (4) where \theta and \theta_{c} are absolute and transition temperatures, respectively. The derivative of the logarithmic free energy potential is then equivalent to f(u)=F^{\prime}(u)=\frac{\theta}{2}\ln[\frac{1+u}{1-u}\Bigg{]}-\theta_{c}u. On the other hand, the polynomial free energy potential, also called as quartic double–well, is an approximation of the logarithmic ones when the temperature \theta closes to \theta_{c}, formulated as follows F(u)=\frac{1}{4}(1-u^{2})^{2}, (5) and its derivative is f(u)=u^{3}-u. It is well-known that the solution u({\bm{x}},t) of the Allen–Cahn equation has two crucial properties; the decay of the total free energy for \mu(u)>0, that is, \frac{d\mathcal{E}(u(t))}{dt}\leq 0, (6) which is a typical property of the gradient flows, and the maximum bound principle. In the double-well potential (5), the solution always stays inside the interval [-1,1], while the solution of the logarithmic potential (4) is bounded in the interval [-s,s] for all the time, where 1>s>0 is the constant satisfying f(s)=0. Due to the existence of a nonlinear term and of the small interfacial length parameter \epsilon, designing of an accurate, efficient, and stable numerical approximation is not always easy task. A numerical investigation of the Allen-Cahn equations has been studied by using different numerical techniques in the spatial domain, such as finite element methods in [9], discontinuous Galerkin methods in [10, 11], and in the temporal domain, such as the implicit-explicit (IMEX) techniques [12, 13], the average vector field (AVF) method [11, 14], and the splitting methods [15]. The aforementioned numerical methods can be accurate up to the given threshold but they have some restrictions such as mesh dependence, high computational burden for the nonlinear partial differential equations (PDEs). On the other hand, in recent years, the usage of deep neural networks (DNNs) has lead to a significant achievement in the several areas, such as visual recognition [16], cognitive science [17] as well as solving differential equations [18, 19, 20, 21, 22, 23, 24]. Among them, Physics-Informed Neural Network (PINN) introduced by Raissi et al. in [22] has received great attention thanks to its flexibility in tackling a wide range of forward and inverse problems involving PDEs. In this structure, weights and biases of the neural network model are optimized according to the loss function, containing the physics of the underlying problem, which are basically governing equations, boundary conditions, and initial condition. Up to now, the PINN has been used to solve different types of problems in computational science and engineering, such as inverse problems [25], fluid dynamics [26, 27], parameter estimation [28], topology optimization [29], fractional PDEs [30], and stochastic PDEs [31, 32]. In addition, different variants of the standard PINN have been proposed to increase the performance of PINN, such as meta-learning [33], gradient-enhanced [34], balance of weights [35], decomposition of spatial domain [36], adaptive sampling [37], adaptivity in temporal domain [38, 39], adaptive activation function [40], and enforcing boundary conditions [41]. Compared to the conventional methods, such as finite difference, finite element, deep learning approaches like PINN is mesh-free thanks to the automatic differentiation and can avoid the curse of dimensionality; see, e.g., [42]. However, for low dimensional PDEs, it is still not straightforward to claim that the computational accuracy obtained from DNNs is better than the ones obtained by the conventional methods; see, e.g., [43] for more discussion. Although the standard PINN (std-PINN) has been widely accepted and has yielded remarkable results across a range of problems in the computational science and engineering, it is not always capable of solving the Allen-Cahn equation, a typical example of phase field models, due to the sharp transition layers, evolution of the layers in time, and high sensitivity to the initial conditions. To develop the performance of PINN, the authors in [44] use two specially designed convolutional neural networks (CNNs) and the loss functions correspond to the full-discrete systems obtained from finite difference methods in both space and time. Similarly, the multi-step discrete time models with adaptive collocation strategy are considered in [45]. The authors in [38] involve the idea of adaptivity in both time and space by sampling the data points, while in [46], the same neural network is retrained over successive time segments, while satisfying the obtained solution for all previous time segments. In addition, the system of the Allen-Cahn equation (3) first reduced into a first-order problem and then the converted minimization problem is approximated by using a deep learning approach in [47]. A theoretical perspective for the propagation of PINN errors is also given in [48] for the Allen-Cahn equations. Unlike the aforementioned studies, we here propose a novel methodology based on preserving of the energy dissipation to predict the dynamics of the Allen-Cahn equation. The proposed network approach guarantees the decay of energy and also plays a key role in accurately learning the dynamics of the Allen-Cahn equation. Embedding of different conservation constraints, such as mass and momentum conservation, into the PINN architecture are also considered in [49, 50, 51] for different types of PDEs. Our specific contributions can be summarized as: • We propose a novel PINN approach based on preserving of the energy dissipation to learn the dynamics of the Allen-Cahn equation more accurately. • We offer a detailed set of benchmark examples to evaluate the performance of the proposed approach. These examples include logarithmic and polynomial free energy potentials, constant and degenerate mobility functions, deterministic and random initial functions, and advective term in one, two, and three spatial dimensions. The rest of this manuscript is outlined as follows: In next section, we briefly review the standard physics-informed neural network (std-PINN). In Section 3, we introduce our main contribution, which is the addition of energy dissipation constraint (6) into the loss function. Section 4 presents numerical strategies utilizing adaptive approaches to enhance the performance of the PINN and the numerical simulations of various benchmark problems. Last, we give some concluding remarks in Section 5 based on the findings in the paper."
https://arxiv.org/html/2411.08750v1,Optimal Transport-Based Displacement Interpolation with Data Augmentation for Reduced Order Modeling of Nonlinear Dynamical Systems,"We present a novel reduced-order Model (ROM) that leverages optimal transport (OT) theory and displacement interpolation to enhance the representation of nonlinear dynamics in complex systems. While traditional ROM techniques face challenges in this scenario, especially when data (i.e., observational snapshots) is limited, our method addresses these issues by introducing a data augmentation strategy based on OT principles. The proposed framework generates interpolated solutions tracing geodesic paths in the space of probability distributions, enriching the training dataset for the ROM. A key feature of our approach is its ability to provide a continuous representation of the solution’s dynamics by exploiting a virtual-to-real time mapping. This enables the reconstruction of solutions at finer temporal scales than those provided by the original data. To further improve prediction accuracy, we employ Gaussian Process Regression to learn the residual and correct the representation between the interpolated snapshots and the physical solution.We demonstrate the effectiveness of our methodology with atmospheric mesoscale benchmarks characterized by highly nonlinear, advection-dominated dynamics. Our results show improved accuracy and efficiency in predicting complex system behaviors, indicating the potential of this approach for a wide range of applications in computational physics and engineering.Keywords: Reduced-Order Models, Data-Driven Modeling, Optimal Transport, Displacement Interpolation, Wasserstein Distance, Computational Fluid Dynamics, Parametric PDEs, Synthetic Data Generation, Nonlinear Dynamics, Atmospheric Flow Simulation.","The modeling and simulation of complex dynamical systems, which are often characterized by nonlinear dynamics and high-dimensional state spaces, pose significant challenges. Traditional full-order models (FOMs), such as Finite Element and Finite Volume (FV) methods, offer high accuracy but come with significant computational costs. While high-performance computing facilities can alleviate some of these computational demands [1], computing power alone is often not sufficient for real-time simulation, large-scale parametric studies, and efficient uncertainty quantification. Since complex dynamical systems are ubiquitous in Science and Engineering, coupled with tight deadlines and the need to quantify uncertainties in the results, there is an urgent need for more efficient modeling approaches. Reduced-order models (ROMs) have emerged as a powerful surrogate strategy to alleviate computational bottlenecks, providing rapid evaluations while maintaining accuracy [2, 3, 4, 5, 6, 7, 8, 9]. ROMs have been successfully applied across various domains [10], including fluid dynamics [11], structural mechanics [12], and electromagnetic systems [13]. Despite their widespread success, standard ROM techniques face significant challenges when dealing with highly nonlinear, advection-dominated phenomena [14]. These challenges are particularly evident in systems characterized by moving discontinuities, steep gradients, and traveling waves. Such features often result in a slow decay of the Kolmogorov n-width, which describes the error arising from a projection onto the best-possible linear subspace of a given dimension [15]. This slow decay limits the efficacy of linear ROMs and has spurred several research directions, including shifted Proper Orthogonal Decomposition (POD) [16, 17], calibration methodologies [18], and registration methods [19, 20]. Additionally, the development of nonlinear ROMs has marked a significant advancement, particularly through the use of deep learning frameworks that efficiently extract relevant features from high-dimensional data [21]. Novel architectures, such as recurrent neural networks [22], convolutional autoencoders [23, 24, 25] and graph-based approaches [26, 27, 28], have further refined the potential of neural networks for regression tasks, enabling more flexible and efficient ROMs. In this paper, we introduce a novel data-driven ROM for highly nonlinear, advection-dominated systems by incorporating concepts from Optimal Transport (OT) theory [29], specifically leveraging displacement interpolation. The ROM is designed to handle both nonlinear dynamics and limited data scenarios, which are common challenges in many practical applications. The key elements of our methodology include an OT-based data augmentation strategy, a time-interpolation parameter mapping, and a correction step to de-bias the barycentric representation. The resulting framework generates physically consistent synthetic data used for a non-intrusive ROM approach able to capture complex nonlinear dynamics efficiently across continuous time scales. The first step in our methodology is the selection of checkpoints from the original solution snapshots and the computation of OT plans between consecutive checkpoints. These OT plans are crucial for leveraging displacement interpolation to generate synthetic snapshots, allowing for continuous and geometrically coherent transitions between computed solutions. This process enriches the model space with synthetic solutions that are not restricted to the support of the original data snapshots. Moreover, this strategy significantly increases the amount of data available for training, enriching the POD basis, enhancing its expressivity, and improving accuracy at finer temporal scales than those of the original snapshots. Similarly, our approach could be exploited for deep learning-based approaches in model order reduction, where large datasets are crucial but only a limited amount of full-order solutions can be computed. To construct a ROM capable of continuous inference, we need to establish mappings between the interpolation parameter and physical time. We propose both a linear mapping and an optimal regression approach, balancing flexibility and accuracy. This allows for efficient online prediction at arbitrary time points. As a correction step, we perform POD with regression (POD-R) of the residuals. A Gaussian Process Regression (GPR) model [30] is trained to predict the POD coefficients of these residuals, compensating for systematic biases in the OT-based interpolation. The integration of optimal transport with ROMs builds upon recent works in this direction, including interpolation in the general L2-Wasserstein space [31, 32], solution approximation using Wasserstein Barycenters [33, 34], alignment of features using OT mappings [35], and the use of the Wasserstein kernel within kPOD reduction [36]. Notably relevant to the design of our methodology is work on convex displacement interpolation [37], which has been extended in [38] where nonlinear interpolation is used for data augmentation, focusing on OT maps between Gaussian models. In our approach, we consider the OT problem in its entropic regularized form, popularized through the use of the Sinkhorn algorithm [39]. Specifically, we use the multiscale formulation proposed in [40], which is particularly useful for problems involving vanishing entropy since it overcomes numerical instabilities, slow convergence, and excessive blurring typical of the standard implementations [41]. While the methodology is general and applicable to a wide range of nonlinear dynamical systems, we choose to demonstrate its effectiveness for the simulation of mesoscale atmospheric flow, modeled by the weakly compressible compressible Euler equations. We focus on two classical benchmarks: the smooth rising thermal bubble and the density current. Both benchmarks are challenging as the dynamics is highly nonlinear and there is strong advection dominance, and hence are ideal to test our methodology. The rest of the paper is organized as follows: Section 2 introduces the general ROM framework of time-dependent problems. Section 3 provides background on optimal transport and displacement interpolation. Section 4 details our OT-based ROM framework, including the data augmentation strategy and the non-intrusive nature of the approach. Section 5 describes the FOM for the above-mentioned atmospheric benchmarks. Section 6 presents the numerical results obtained with our ROM and discusses the effectiveness of the methodology. Finally, Section 7 offers conclusions and perspectives for future work."
https://arxiv.org/html/2411.08702v2,"A Deep Uzawa-Lagrange Multiplier Approach for Boundary
Conditions in PINNs and Deep Ritz Methods","We introduce a deep learning-based framework for weakly enforcing boundary conditions in the numerical approximation of partial differential equations. Building on existing physics-informed neural network and deep Ritz methods, we propose the Deep Uzawa algorithm, which incorporates Lagrange multipliers to handle boundary conditions effectively. This modification requires only a minor computational adjustment but ensures enhanced convergence properties and provably accurate enforcement of boundary conditions, even for singularly perturbed problems.We provide a comprehensive mathematical analysis demonstrating the convergence of the scheme and validate the effectiveness of the Deep Uzawa algorithm through numerical experiments, including high-dimensional, singularly perturbed problems and those posed over non-convex domains.","The numerical approximation of partial differential equations (PDEs) using artificial neural networks (ANNs) has gained significant attention in recent years [4, 20, 18, 14, 9]. This surge is largely attributed to the success of deep learning in various complex tasks [13, 10]. In the context of solving PDEs, neural network-based methods such as the deep Ritz approach [4], which approximates solutions by minimising the Dirichlet energy, and the physics-informed neural networks (PINNs) [18], which minimise the {\operatorname{L}}^{2}-norm of the residuals are prominent examples. Despite their success, a challenge in these methods lies in the enforcement of boundary conditions. While classical numerical methods also face difficulties in this regard, the non-standard nature of neural network approximation spaces makes this issue particularly pronounced. Standard penalty approaches often require large penalty weights to enforce boundary conditions accurately, resulting in ill-conditioned optimisation problems that are difficult to tune and can lead to suboptimal solutions. This issue is particularly severe in problems involving singular perturbations or complex domains. Additionally, the practical implementation of boundary conditions in ANN-based methods poses challenges, as accurately capturing boundary data within the neural network’s architecture often proves difficult. To address these challenges, we propose extending the Lagrange multiplier framework from finite elements, as introduced by Babuška [1], to neural network-based PDE solvers. Our work develops a class of algorithms termed Deep Uzawa algorithms, which iteratively solve the resulting saddle point problems to weakly impose boundary conditions. The key innovation lies in adapting Uzawa’s algorithm [22] to this context, allowing for efficient iterative approximation of PDEs where boundary conditions are enforced using an augmented Lagrangian formulation. This approach ensures that the algorithmic framework remains stable and accurate due to the coercivity of the energies involved. The Deep Uzawa methods, RitUz and PINNUz, extend existing deep Ritz and PINN frameworks with minimal modifications, making them highly practical for integration into current computational workflows. The theoretical analysis provided includes convergence proofs that demonstrate the iterative schemes’ stability at the PDE level. These theoretical guarantees offer a robust foundation for the implementation of the Deep Uzawa algorithms and provide insight into their convergence behaviour. We compare the behaviour of these approaches to the vanilla methods and show that the Deep Uzawa approach achieves comparable or superior performance without relying on tuning penalty parameters. Numerous methods have been explored for weakly imposing boundary conditions within ANN-based PDE solvers. The deep Ritz method [4] and PINNs [18] form the foundational basis for many current approaches, but both rely on penalty terms for boundary enforcement, which can make optimisation challenging, particularly when large penalties are needed. To address these shortcomings, [14] proposed an adaptation using Nitsche’s method [17] from finite element analysis to weakly impose boundary conditions, mitigating conformity issues highlighted in [4]. Similarly, [23] compared traditional Ritz-Galerkin methods with ANN-based approaches, noting the implicit regularisation properties provided by neural networks. Other advancements, such as the penalty-free neural network strategy in [19], have targeted second-order boundary value problems in complex geometries. In high-dimensional settings, [11] explored deep learning approaches for elliptic PDEs with non-trivial boundary conditions, showcasing the flexibility of neural networks in handling such cases. Our Deep Uzawa method builds on these developments by leveraging a consistent saddle point framework to address the boundary enforcement problem, offering a minor tweak computationally that provably enhances stability and accuracy. The application of Uzawa-type iterations in neural network contexts, as presented in [16], serves as a foundation for our iterative scheme. Our approach provides a structured way to balance the competing objectives of PDE accuracy and boundary condition enforcement, demonstrating improved stability in various numerical experiments, including problems on non-convex domains and high-dimensional geometries. The rest of the paper is organized as follows: In §2, we introduce the notation and fundamental concepts related to Sobolev spaces, which form the basis for the functional framework of our analysis. §3 presents the development of a Deep Ritz-Uzawa (RitzUz) method, including a proof of convergence in suitable Sobolev spaces. In §4, we extend this approach to the PINNs-Uzawa (PINNUz) scheme, demonstrating convergence within an appropriate space for least-squares minimisation. The construction of neural network approximations and their integration within the Deep Uzawa framework are discussed in §5. Numerical results showcasing the effectiveness of our methods for boundary layer problems, those in complex geometries and high dimension are provided in §6."
https://arxiv.org/html/2411.08497v1,General Order Virtual Element Methods for Neumann Boundary Optimal Control Problems in Saddle Point Formulation,"In this work, we explore the application of the Virtual Element Methods for Neumann boundary Optimal Control Problems in saddle point formulation. The method is proposed for arbitrarily polynomial order of accuracy and general polygonal meshes. Our contribution includes a rigorous a priori error estimate that holds for general polynomial degree. On the numerical side, we present (i) an initial convergence test that reflects our theoretical findings, and (ii) a second test case based on a more application-oriented experiment. For the latter test, we focus on the role of VEM stabilization, conducting a detailed experimental analysis, and proposing an alternative structure-preserving strategy to circumvent issues related to the choice of the stabilization parameter.","In this paper, we consider Virtual Element Methods (VEM) approximation for optimal Neumann boundary control. Optimal Control Problems (OCPs) constrained to partial differential equations (PDEs) are widespread in many practical applications in scientific and engineering contexts, see e.g. [2, 30, 32, 33] and the references therein. The main goal of OCPs is to reach a beneficial behavior of the PDE solution by employing an external variable, the control. The role of the control consists in making the state solution similar to a desired configuration using constrained minimization of a functional. The control is distributed when its action is defined all over the domain, while we talk about boundary control if the control affects the boundary conditions. Due to its coupled nature, the numerical analysis for OCPs constrained to PDEs is a fascinating mathematical challenge. Several works have been proposed with Finite Element Methods, Discontinuous Galerkin Methods, Finite Volumes and hybrid methods: we refer the reader to this far from exhaustive list and the reference therein [18, 21, 24, 27, 42, 43, 44] Lately, great attention has been paid to VEM, a family of polygonal methods for simulating PDEs. VEM can be applied on arbitrarily polygonal meshes, guaranteeing huge flexibility in terms of geometrical complexity and they might play a crucial role in many real-life scenarios. Indeed, after being conceived in the seminal paper [3], they have been applied to various contexts of interest [5, 7, 13, 19, 20, 23, 41]. The investigation of boundary OCPs through VEM is still limited yet of utmost importance. Indeed, the versatility of arbitrary polygonal meshes, combined with the action of control strategies on complex geometries, might be not only a challenging theoretical topic but also relevant from a practical viewpoint. To our knowledge, C^{1}-VEM is presented in [16] for distributed OCPs constrained to elliptic equations and with a constrained state variable. The VEM discretization for distributed control is the topic of several investigations, see e.g. [34, 36, 37, 38, 40]. Some VEM works related to boundary OCPs are [39] for the Dirichlet case with control constraints and [25] for the Neumann case with control constraints and approximation of the first order. In this contribution, we focus on the unconstrained Neumann boundary control problem in a saddle point structure. This naturally derives from the optimality conditions and considers the coupled nature of the system leading to more robust and stable systems solving for all the variables at once. This formulation is generally more effective than iterative algorithms. To the best of our knowledge, we differentiate from previous contributions in the following terms: • the presented method has arbitrary polynomial accuracy; • we analyze the problem in saddle point formulation as presented in [28]; • we prove well-posedness and a priori error estimates for a general polynomial degree and variable coefficients; • numerical tests cover high-order schemes; • we numerically investigate how the stabilization parameter affects the method’s accuracy. Moreover, we propose a structure-preserving strategy to circumvent issues related to VEM stabilizations, based on recent developments in VEM theory [8, 9, 10, 12]. The paper is structured as follows: in Section 2 we introduce the linear-quadratic Neumann boundary OCP at the continuous level in a saddle point framework. Section 3 focuses on the VEM approximation of such optimization problem and proves the well-posedness of the saddle point structure. In Section 4 we derive optimal a priori error estimates. Section 5 presents two test cases: a convergence analysis test and a more application-oriented test for which we discuss the role of the stabilization parameter, proposing a workaround strategy using structure-preserving VEM. Finally, conclusions follow in Section 6. Throughout the work, (\cdot,\cdot)_{\omega} denotes the standard \mathrm{L}^{2} scalar product defined on a generic \omega\subset\mathbb{R}^{2} and inside the proofs, the symbol C denotes any constant independent of the mesh size."
https://arxiv.org/html/2411.08377v1,Dual-Valued Functions of Dual Matrices with Applications in Causal Emergence,"Dual continuation, an innovative insight into extending the real-valued functions of real matrices to the dual-valued functions of dual matrices with a foundation of the Gâteaux derivative, is proposed. Theoretically, the general forms of dual-valued vector and matrix norms, remaining properties in the real field, are provided. In particular, we focus on the dual-valued vector p-norm (1\!\leq\!p\!\leq\!\infty) and the unitarily invariant dual-valued Ky Fan p-k-norm (1\!\leq\!p\!\leq\!\infty). The equivalence between the dual-valued Ky Fan p-k-norm and the dual-valued vector p-norm of the first k singular values of the dual matrix is then demonstrated. Practically, we define the dual transitional probability matrix (DTPM), as well as its dual-valued effective information ({\rm{EI_{d}}}). Additionally, we elucidate the correlation between the {\rm{EI_{d}}}, the dual-valued Schatten p-norm, and the dynamical reversibility of a DTPM. Through numerical experiments on a dumbbell Markov chain, our findings indicate that the value of k, corresponding to the maximum value of the infinitesimal part of the dual-valued Ky Fan p-k-norm by adjusting p in the interval [1,2), characterizes the optimal classification number of the system for the occurrence of the causal emergence.","The dual number, initially proposed by Clifford [2] in 1873, was then built upon by Study [23] in 1903 to represent the dual angle. In light of the powerful role that dual algebra plays in the kinematic analysis of spatial mechanisms, an increasing number of researchers have concentrated on the distinctive attributes of dual algebra, with a particular focus on the dual-valued functions. The preceding work generalized the differentiable functions in the real field and holomorphic functions in the complex field to dual-real-valued and dual-complex-valued functions, respectively. Specifically, Kramer (1930) [15] defined the polygenic functions of dual real numbers. Gu and Luh (1987) [8] employed the Taylor series expansion to derive the general form for functions of dual numbers. Pennestrì and Stefanelli (2007) [19] outlined Gaussian, polar, and exponential representations of dual real numbers, as well as the trigonometric functions for dual angles. Furthermore, Messelmi (2015) [16] developed the holomorphic dual-complex-valued functions and presented the exponential, logarithmic, trigonometric, and hyperbolic functions of dual complex numbers. Güngör and Tetik (2019) [9] advanced the De-Moivre and Euler formulae for dual-complex numbers. Recently, research into the norms of dual vectors and dual matrices has yielded some promising results. Qi et al. (2022) [20, 22] defined the magnitude of dual complex numbers, and formulated the 1-norm, 2-norm, and \infty-norm of dual complex vectors, as well as the Frobenius norm of dual complex matrices. Additionally, Miao and Huang (2023) [17] introduced the concept of the p-norm of a dual complex vector with p being an arbitrary positive integer, along with the operator p-norm of a dual complex matrix induced by this p-norm. Besides, the expressions of the operator 1-norm, 2-norm, and \infty-norm for dual complex matrices were derived. Despite the graceful approach [8] for constructing the dual continuation of real-valued functions, it was limited to differentiable real-valued functions of real numbers. Hence, we aim to investigate how to extend a common real-valued function to a dual-valued one in a “continuous” manner, while maintaining the properties established on the real space. An ingenious idea is to employ the Gâteaux derivative, proposed by Gâteaux [6] in 1913, as the infinitesimal part. Thus, the expressions of dual-valued vector and matrix norms are derived and their rationality is demonstrated. The highlight of the applications of the dual-valued functions lies in the interconnection between the dual-valued Ky Fan p-k-norm and causal emergence (CE) [11, 12] in dynamical systems. For the conventional methodologies utilized to quantify the extent of CE, it is necessary to traverse all the coarse-graining methods [27, 28] or to identify a clear cut-off in the spectrum of singular values of the transitional probability matrix (TPM) [29]. Nevertheless, our method addresses these two issues. Particularly, the infinitesimal part of the dual-valued Ky Fan p-k-norm plays an essential role in identifying the optimal classification number of a system that occurs CE. The remainder of this paper is structured as follows. Section 2 begins by laying out some preliminary properties of dual matrices and useful notations. Section 3 proposes the definition and characteristics of the dual continuation of a real-valued function, derived from the Gâteaux derivative. Section 4 focuses on one of the core contributions to the general form of dual-valued vector norms and illustrates concrete expressions of dual-valued vector p-norms (1\!\leq\!p\!\leq\!\infty). The other principal development, the general form of dual-valued matrix norms, is developed in Section 5. Besides, the specific expressions of unitarily invariant dual-valued matrix norms, dual-valued operator norms, and other dual-valued functions are discussed. In Section 6, the dual transitional probability matrix (DTPM) is defined, as well as its dual-valued effective information ({\rm{EI_{d}}}) and the concept of dynamical reversibility. Theoretical consistency between the dynamical reversibility of the DTPM, the maximum point of {\rm{EI_{d}}}, and the maximum point of the dual-valued Schatten p-norm (1\!\leq\!p<2) enables the capture of the optimal classification number occurring CE in the classical dumbbell Markov chain. In conclusion, Section 7 presents an overview of our findings and outlines potential avenues for future research."
https://arxiv.org/html/2411.08342v1,"Recursive reduction quadrature for the evaluation of Laplace layer
potentials in three dimensions","A high-order quadrature rule is constructed for the evaluation of Laplace single and double layer potentials and their normal derivatives on smooth surfaces in three dimensions. The construction begins with a harmonic approximation of the density on each patch, which allows for a natural harmonic polynomial extension in a volumetric neighborhood of the patch. Then by the general Stokes theorem, singular and nearly singular surface integrals are reduced to line integrals preserving the singularity of the kernel, instead of the standard origin-centered 1-forms that often require expensive adaptive integration. These singularity-preserving line integrals can be semi-analytically evaluated using singularity-swap quadrature. In other words, the evaluation of singular and nearly singular surface integrals is reduced to function evaluations on the vertices on the boundary of each patch. The recursive reduction quadrature largely removes adaptive integration that is needed in most existing high-order quadratures for singular and nearly singular surface integrals, leading to improved efficiency and robustness. The algorithmic steps are discussed in detail. The accuracy and efficiency of the recursive reduction quadrature are illustrated via several numerical examples.","We consider the evaluation of Laplace layer potentials on smooth surfaces in three dimensions. The Laplace single and double layer potentials and their normal derivatives are defined by the formulas (1) \displaystyle{{\mathcal{S}}}[\sigma]({\boldsymbol{r}}^{\prime}) \displaystyle=\int_{S}G({\boldsymbol{r}}^{\prime},{\boldsymbol{r}})\sigma({% \boldsymbol{r}})da({\boldsymbol{r}}), (2) \displaystyle{{\mathcal{D}}}[\mu]({\boldsymbol{r}}^{\prime}) \displaystyle=\int_{S}\frac{\partial G({\boldsymbol{r}}^{\prime},{\boldsymbol{% r}})}{\partial\boldsymbol{\nu}}\mu({\boldsymbol{r}})da({\boldsymbol{r}}), (3) \displaystyle{{\mathcal{S}^{\prime}}}[\sigma]({\boldsymbol{r}}^{\prime}) \displaystyle=\int_{S}\frac{\partial G({\boldsymbol{r}}^{\prime},{\boldsymbol{% r}})}{\partial\boldsymbol{\nu}^{\prime}}\sigma({\boldsymbol{r}})da({% \boldsymbol{r}}), (4) \displaystyle{{\mathcal{D}^{\prime}}}[\mu]({\boldsymbol{r}}^{\prime}) \displaystyle=\int_{S}\frac{\partial^{2}G({\boldsymbol{r}}^{\prime},{% \boldsymbol{r}})}{\partial\boldsymbol{\nu}^{\prime}\partial\boldsymbol{\nu}}% \mu({\boldsymbol{r}})da({\boldsymbol{r}}), where G is the Laplace Green’s function (5) G({\boldsymbol{r}}^{\prime},{\boldsymbol{r}})=\frac{1}{4\pi|{\boldsymbol{r}}^{% \prime}-{\boldsymbol{r}}|},\quad{\boldsymbol{r}}^{\prime},{\boldsymbol{r}}\in% \mathbb{R}^{3}. Here, da({\boldsymbol{r}}) is the surface differential at {\boldsymbol{r}}. The point {\boldsymbol{r}}^{\prime} is referred to as the target point, while {\boldsymbol{r}} is the source point. The symbols \boldsymbol{\nu}^{\prime} and \boldsymbol{\nu} represent the unit outward normal vectors at {\boldsymbol{r}}^{\prime} and {\boldsymbol{r}}, respectively. We assume that the densities \sigma and \mu are sufficiently smooth, and that the surface S is piecewise smooth and divided into a collection of patches, where each patch is sufficiently smooth and admits a high-order parameterization. Thus, the only singularity in the above integrals is induced by the Green’s function or its derivatives. It is clear that one only needs to consider the evaluation of Laplace layer potentials Eq. 1–Eq. 4 when S is replaced with one of the patches, denoted by P. We will mostly focus on triangular patches so that the readers will not be deviated from the main ideas by some technical details for non-triangular patches, even though our quadrature scheme essentially works for patches of arbitrary shape. Broadly speaking, quadrature schemes can be classified into two categories: global quadratures and local quadratures. A global quadrature treats the underlying integration domain as a whole and constructs quadrature nodes and weights for the whole domain. Most global quadratures in two dimensions are modifications of the trapezoidal rule for periodic smooth functions, including [54, 42, 39, 3, 1, 6], due to the topological fact that any Jordan curve in two dimensions is homeomorphic to the unit circle. Some global quadratures for singular and near-singular surface integrals in three dimensions are modifications of the trapezoidal rule for periodic smooth functions in two dimensions (see, for example, [19, 61, 62, 63]). For surfaces that are homeomorphic to the unit sphere, a spectrally accurate quadrature has been developed in [27, 26]. Since the topology of surfaces in three dimensions is more complex than that of curves in two dimensions, global quadratures for surface integrals in three dimensions are more specialized than those for line integrals in two dimensions. Finally, there is a compatibility issue (see, for example, [31]) of whether a global quadrature is amenable with the application of fast algorithms such as the fast multipole method (FMM) and its descendants [29, 14, 15, 23, 25, 30, 66, 68]. A local quadrature first divides the whole integration domain into a collection of elements of finite number of geometries (also referred to as chunks, panels, or patches in the literature), then constructs quadrature nodes and weights for each element. Depending on the distance of the target to the source patch, there are altogether four cases: (a) Smooth interaction: the target {\boldsymbol{r}}^{\prime} is sufficiently far from the source element. (b) Self interaction: the target {\boldsymbol{r}}^{\prime} is on the source element. (c) Near interaction: the target {\boldsymbol{r}}^{\prime} is not on the source element, but on S and close to the source element. (d) Close evaluation: the target {\boldsymbol{r}}^{\prime} is off-surface and close to the source element. In the case of smooth interaction, the integrals are smooth, and high-order quadrature can be used to evaluate layer potentials in both two and three dimensions. For case (b), the integrals are weakly singular (i.e., integrable in the Riemann sense, mostly with logarithmic singularity in two dimensions or 1/r singularity in three dimensions for layer potentials), singular (i.e., the so-called principal value integrals), or hypersingular (i.e., the so-called Hadamard finite-part integrals). With a slight abuse of terminology, we will simply refer to case (b) as singular integrals. For cases (c) and (d), the integrals are nearly weakly singular, nearly singular, or nearly hypersingular. With a slight abuse of terminology, we will refer to these two cases as near-singular integrals. The difference between case (c) and case (d) is that the target point is associated with a parameter value in the parameter space for S in case (c), while the target point only has coordinates in the underlying physical space in case (d). In general, case (d) is harder than case (c) as there is less information on the target point. In two dimensions, singular and nearly singular integrals related to layer potentials are relatively easy to evaluate since they can be reduced to standard one-dimensional integrals in the parameter space with \ln|t-s|, 1/(t-s), or 1/(t-s)^{2} singularities, where t and s are the parameter values for the target {\boldsymbol{r}}^{\prime} and the source {\boldsymbol{r}}, respectively. And the problem is largely viewed as solved by the research communities. In three dimensions, the problem becomes much harder. For self interaction, one may use a polar coordinates centered at the given target point to eliminate the 1/r singularity, as in [20, 12, 13]. Additionally, the generalized Gaussian quadrature (GGQ) method [11, 43, 65] can be applied to construct efficient high-order quadrature nodes and weights [9, 10], which can be precomputed and stored. For nearly singular integrals, a popular approach is to evaluate layer potentials for targets sufficiently away from the surface using oversampled smooth quadrature rule, then apply interpolation/expolation to evaluate the layer potential at the given target point. This includes the QBX (quadrature by expansions) [22, 40, 49, 55, 59, 60], its descendants [56, 45, 17], and methods in [7, 8]. Recently, there are also quadratures based on the harmonic density interpolation [47], planewave density interpolation [48], and adaptive integration [28] that combines the FMM and careful precomputed tables. Our treatment closely follows [69], where a novel quadrature scheme has been presented for the evaluation of Laplace double layer potential. The scheme in [69] approximates the density \sigma on P in quaternion algebra such that the resulting approximation has a natural harmonic polynomial extension. The harmonic polynomial extension is valid in the whole space, but only needed in a volumetric neighborhood of P. Such density approximation enables the application of the general Stokes theorem without any further approximation in exact arithmetics and reduces singular and nearly singular integrals on the surface patch P to line integrals on the boundary of P. In other words, the scheme approximates the density in such a way that one may subsequently apply the integration by parts formula to analytically reduce surface integrals to line integrals on the boundary of each surface patch. The scheme is very accurate, leading to ten digits of accuracy or higher for targets arbitrarily close to the source patch. As observed in [69], the scheme can be viewed as a natural extension of the kernel-split quadrature developed by Johan Helsing and his collaborators [32, 33, 34, 35, 36, 37, 38] for the evaluation of singular and nearly singular integrals on curves in two dimensions, where the line integrals are converted to complex contour integrals and Cauchy’s integral formula is applied subsequently to analytically evaluate those complex contour integrals. The only approximation invovled is the polynomial approximation for the smooth part. In this paper, we make further improvements on the scheme in [69]. We observe that the kernels of line integrals constructed in [69] have weaker but more complicated singularity. Indeed, the construction of line integrals in [69] depends on the choice of the origin, and as a result the singularity of the kernels depends not only on the difference vector of the target and source points, but also on the absolute coordinates of the target and source points. In other words, the translation invariance of the kernel is lost. This leads to expensive adaptive integration to resolve the near-singularity in the kernel when the target point is close to the boundary curves. In order to remove the adaptive integration from the scheme, we switch to the target-centered 1-forms (i.e., line integrals) from the origin-centered 1-forms in [69]. These target-centered 1-forms preserve the singularity of the kernel. That is, the singularity of these 1-forms are of the form 1/r^{n} for a positive integer n, the same as the original kernel in layer potentials. A nice trick in the singularity-swap quadrature by af Klinteberg and Barnett [2] enables us to evaluate these nearly singular line integrals analytically by recurrence, after approximating the density in line integrals via standard polynomial interpolation and finding a complex-valued parameter for the given target point. To be more precise, the 1-forms are further reduced to 0-forms by applying analytic integration by parts once again. Hence the recursive reduction quadrature. Some other contributions of this paper are summarized as follows. First, we show that a set of complete and linearly independent basis functions can be chosen for the quaternion harmonic polynomial approximation of the density for flat triangular patches for any given order. Since all curvilinear patches are locally flat in the limit of patch size going to zero, this provides a good guideline for the choice of basis functions in the quaternion approximation. Second, in [69] the Laplace single layer potential is converted to the Laplace double layer potential with a target-dependent density. This certainly works but is slightly ad hoc and inefficient. We develop a new integration-by-parts formula for the Laplace single layer potential in three dimensions, which is a natural extension of treatment of the Laplace single layer potential in two dimensions. Third, we carefully examine the algorithmic steps of the whole scheme to improve its efficiency. We would like to remark here that even though it is possible to write the entire paper using vector calculus, we choose to use quaternion algebra and differential forms as in [69] to keep its mathematical elegance. Nevertherless, the paper is mostly self-contained and the use of quaternion algebra and differential forms is minimized so that it is easily accessible to the readers. The outline of the paper is as follows. In Section 2, we collect and review the analytical apparatus used in the rest of the paper. In Section 3, we present the main theoretical results. In Section 4, we discuss the algorithmic steps in the scheme and analyse the cost of each step. Numerical examples are shown in Section 5. Finally, Section 6 contains some further remarks on future extensions of the work."
https://arxiv.org/html/2411.08271v1,High-order and Mass-conservative Regularized Implicit-explicit relaxation Runge-Kutta methods for the logarithmic Schrödinger equation,"The non-differentiability of the singular nonlinearity (such as f=\ln|u|^{2}) at u=0 presents significant challenges in devising accurate and efficient numerical schemes for the logarithmic Schrödinger equation (LogSE). To address this singularity, we propose an energy regularization technique for the LogSE. For the regularized model, we utilize Implicit-Explicit Relaxation Runge-Kutta methods, which are linearly implicit, high-order, and mass-conserving for temporal discretization, in conjunction with the Fourier pseudo-spectral method in space. Ultimately, numerical results are presented to validate the efficiency of the proposed methods.","In this paper, we focus on the numerical solution for the logarithmic Schrödinger equation (LogSE), which take the form: \begin{cases}{\rm i}\partial_{t}u(\boldsymbol{x},t)+\Delta u(\boldsymbol{x},t)% =\lambda uf(u),\quad\boldsymbol{x}\in\Omega,\;\;t>0,\\[2.0pt] u(\boldsymbol{x},0)=u_{0}(\boldsymbol{x}),\quad\boldsymbol{x}\in\bar{\Omega},% \end{cases} (1.1) where {\rm i}=\sqrt{-1}, f(u)=\ln(|u(\boldsymbol{x},t)|^{2}), \Omega\subset{\mathbb{R}}^{d}, with d\geq 1, represents a bounded domain with a smooth boundary. The function u_{0} is a given initial condition with a regularity that will be specified later. The constant \lambda\neq 0 is a real number, where for \lambda>0, the solution exhibits repulsive or defocusing behavior, and for \lambda<0, the solutions demonstrate attractive or focusing interactions. The LogSE (1.1) proposed as a model for nonlinear wave mechanics [4]. LogSE (1.1) conserves both mass and energy as follows, \begin{split}M(t):&=\int_{\Omega}|u(\boldsymbol{x},t)|^{2}d\boldsymbol{x}% \equiv\int_{\Omega}|u_{0}(\boldsymbol{x})|^{2}d\boldsymbol{x}=M(0),\\ E(t):&=\int_{\Omega}[|\nabla u(\boldsymbol{x},t)|^{2}d\boldsymbol{x}+\lambda F% (|u(\boldsymbol{x},t)|^{2})]d\boldsymbol{x}\\ &\equiv\int_{\Omega}[|\nabla u_{0}(\boldsymbol{x})|^{2}+\lambda F(|u_{0}(% \boldsymbol{x})|^{2})]d\boldsymbol{x}=E(0),\end{split} (1.2) where F(\rho)=\int_{0}^{\rho}\ln(s)ds=\rho\ln\rho-\rho,\quad\rho=|u|^{2}. (1.3) The nonlinearity term f(u)=\ln(|u(\boldsymbol{x},t)|^{2}) is non-differentiable at u=0. This characteristic introduces significant practical challenges and theoretical complexities, particularly when it comes to analyzing and solving the numerical solutions of the LogSE (1.1). Consequently, the existing literature on this topic is somewhat limited. For the Cauchy problem associated with the LogSE, Cazenave [6] established a suitable functional framework. Extensive literature exists on numerical solutions for the Schrödinger equation with smooth nonlinearity, employing methods such as the finite difference method [2], time-splitting method [3, 1], and among others. However, for the LogSE (1.1), the available numerical schemes are more limited. This scarcity arises because these methods cannot be directly applied to the LogSE due to the non-differentiability of the nonlinearity at u=0. To circumvent this issue, Bao [2, 1] introduced a regularized logarithmic Schrödinger equation by substituting f(u) with f_{\varepsilon}(u^{\varepsilon}), where 0<\varepsilon\ll 1. They developed a semi-implicit finite difference method [2], which, although not conserving energy, provided a viable approach. Later, Bao [3] introduced a different regularization strategy at the energy density level, substituting the energy density locally in the region 0<\rho<\varepsilon^{2} with a sequence of polynomials while keeping it unchanged in the region \rho\geq\varepsilon^{2}. In [3, 1], Bao constructed regularized Lie-Trotter splitting and Strang splitting methods, which preserve mass conservation but are only first and second order, respectively, for solving the LogSE. Recently, Wang [11] developed a nonregularized first-order implicit-explicit scheme for the LogSE, with the nonlinearity being explicit but only first order in time. Over the past decades, a variety of relaxation Runge-Kutta (RRK) methods [9] have been developed, offering explicit and mass conservative solutions. However, these explicit methods are plagued by stringent step size restrictions that can limit their practical applicability. Recently, Li [10] investigated implicit-explicit relaxation Runge-Kutta (IMEX RRK) methods for nonlinear stiff ordinary differential equations. The proposed methods are linearly implicit, can achieve arbitrarily high order accuracy, and are designed to preserve monotonicity. The structure of the rest of this paper is as follows: Section 2 introduces the energy regularized logarithmic Schrödinger equation (ERLogSE). In Section 3, IMEX RRK methods are applied to discretize the ERLogSE in time, combined with the Fourier pseudo-spectral method for spatial discretization. Section 4 presents several numerical experiments to demonstrate the efficiency and accuracy of the proposed numerical methods. Finally, Section 6 concludes the paper."
https://arxiv.org/html/2411.08822v1,A probabilistic reduced-order modeling framework for patient-specific cardio-mechanical analysis,"Cardio-mechanical models can be used to support clinical decision-making. Unfortunately, the substantial computational effort involved in many cardiac models hinders their application in the clinic, despite the fact that they may provide valuable information. In this work, we present a probabilistic reduced-order modeling (ROM) framework to dramatically reduce the computational effort of such models while providing a credibility interval. In the online stage, a fast-to-evaluate generalized one-fiber model is considered. This generalized one-fiber model incorporates correction factors to emulate patient-specific attributes, such as local geometry variations. In the offline stage, Bayesian inference is used to calibrate these correction factors on training data generated using a full-order isogeometric cardiac model (FOM). A Gaussian process is used in the online stage to predict the correction factors for geometries that are not in the training data. The proposed framework is demonstrated using two examples. The first example considers idealized left-ventricle geometries, for which the behavior of the ROM framework can be studied in detail. In the second example, the ROM framework is applied to scan-based geometries, based on which the application of the ROM framework in the clinical setting is discussed. The results for the two examples convey that the ROM framework can provide accurate online predictions, provided that adequate FOM training data is available. The uncertainty bands provided by the ROM framework give insight into the trustworthiness of its results. Large uncertainty bands can be considered as an indicator for the further population of the training data set.","Over the past decades, the use of patient-specific computational models in biomedical research and clinical practice has become abundant [1, 2, 3, 4, 5]. In cardiology, a large variety of patient-specific computational models has been developed in the context of specific diseases and phenomena [6, 7, 8, 3, 4]. In our previous work [9, 10, 11] we have, for example, developed a computational model to study cardiac mechanics in the context of Ventricular Tachycardias (VTs), taking into consideration echocardiogram data. The objective of patient-specific models varies from gaining a fundamental understanding to supporting clinical decision making. The requirements of a computational model strongly depend on its intended purpose. A computational model used in biomedical research to enhance fundamental understanding typically requires detailed anatomical and physiological representations. The evaluation of such a model often involves substantial computational effort. Although this is generally not problematic in view of the modeling purpose, the computational demand can make calibration procedures impractical, warranting the accurate determination/measurement of the model parameters. In contrast, a computational model used for clinical decision support is typically required to have limited computational effort, as to not delay the decision-making process. Limiting the computational effort is typically achieved by reducing the level of detail of the anatomical and physiological representations to what is needed to support the clinician. In this setting, it is very important that the model can be tailored easily to a patient through model calibration and that it can accommodate the large uncertainties inherent to the clinical setting. The results obtained from a biomedical research model are often highly informative for clinical decision making, but its application in clinical practice can be hindered by the substantial computational effort. This creates a need for reduced-order modeling (ROM) approaches [12], which involve an offline stage in which a computationally demanding full-order model (FOM) is simplified to a computationally affordable reduced-order model. This reduced-order model – which should balance model details and accuracy with computational effort – is then used to support the clinical decision-making process in the online stage. In this work we intend to develop a ROM approach for a patient-specific cardiac mechanics model. We specifically consider the echocardiogram-based isogeometric analysis model developed by Willems et al. [9] (Figure 1) as the FOM. This model represents the anatomy of the ventricles by means of non-uniform rational B-splines (NURBS), which can be fitted to echocardiogram data [10]. The time-dependent mechanical behavior is modeled by a tissue-scale nonlinear continuum mechanics description, which is coupled to a lumped parameter model for the arterial system and a phenomenological activation potential relation. The model parameters are selected based on expertise from clinical research. This model computes the time-dependent mechanical response in the form of displacement fields, from which many other properties, such as stress fields and pressure-volume curves, are derived. When solving the model for multiple cardiac cycles, as required to reach cyclic steady-state conditions, typical simulation times surmount to multiple hours. Although the model settings can be optimized for computational efficiency, usage of this model for clinical decision making is impractical, especially because calibration procedures in that setting would require many evaluations of the model. The model would, however, be very suitable for application in the offline stage of a ROM approach, where it is used to generate training data. Figure 1: Schematic of the considered full-order cardiac model. Patient-specific input (circles) is mapped on patient-specific output (diamonds) by combining a NURBS-based fitting algorithm with an isogeometric cardiac solver. Besides the requirement that the ROM should be computationally affordable, it should also be able to incorporate as much of the information about a patient as possible. We therefore require the reduced-order model to use the same input as the FOM, i.e., the echocardiograms, tissue stiffness parameters and parameters for the circulatory system and activation potential law. We assume that for clinical decision making the pressure-volume curves are most important, and that displacement and stress fields as obtained from the full-order model are not essential in this setting. Optionally, such detailed output should, however, be obtainable in a subsequent offline evaluation of the full-order model. The choice for a specific ROM approach depends on the characteristics of the FOM on which it is based, as well as on functionality, accuracy and computational complexity requirements of the ROM. A myriad of methods to construct a ROM is available. Inspired by Peherstorfer et al.111The authors present their classification in a multi-fidelity modeling context. [13], we classify the ROM models resulting from these methods as: 1. A simplified reduced-order model is constructed from a FOM by making additional modeling assumptions, for example regarding the geometry or physics, to allow for mathematical simplification of the problem formulation. In essence, this follows a traditional modeling approach, in which domain-specific expertise is used to obtain a simplified ROM that adequately balances model functionality and accuracy on the one hand with computational complexity on the other. Consequently, in many research domains there is a wealth of literature on simplified ROMs. Due to the additional modeling assumptions, the simplified reduced-order model parameters and results in general do not coincide with those of the FOM on which it is based. For the model parameters, this means that available data for the FOM needs to be cast into a form suitable for the ROM, a process that typically involves loss of information. The results of simplified ROMs are typically less detailed compared to their FOM counterparts, restricting the objectives for which they can be used. 2. A projection-based reduced-order model retains the formulation of the FOM but reduces its complexity by identifying an adequate solution subspace in which this formulation is solved. Projection-based methods, of which proper orthogonal decomposition [14] and the reduced basis method [15] are prominent examples, exploit the mathematical structure of the FOM in a generic way and hence do not require (extensive) domain-specific expertise. A projection-based ROM typically has the same parameters and outcomes as the full-order model. The accuracy and computational complexity of the ROM can be balanced through the selection of the subspace on which the FOM is projected. For complex problems, for example incorporating time-dependent and nonlinear phenomena, finding an adequate balance can be complicated [16, 17]. 3. A data-fit reduced-order model – also commonly referred to as a surrogate model – provides an abstract mapping between the input and output data of the FOM based on a set of FOM results. Such methods, of which neural networks [18] and Gaussian processes [19] are prominent examples, in principle consider the FOM as a black box. Although standard data-fit ROMs are physics-agnostic, they need to be tailored to the specific input and output under consideration. The amount of training data needed to attain a data-fit ROM with sufficient accuracy depends on the input and output spaces, which can make the generation of a FOM data set computationally demanding in the case of large parameter spaces. We note that, as with any classification, not all ROM approaches fit perfectly, and that methods have been developed that combine ingredients of these classes. A typical example of such hybrid methods are physics-informed neural networks [20]. The ROM framework developed in this work, which is schematically illustrated in Figure 2, leverages functionality from different classes of ROMs. The core of the framework is the consideration of a simplified reduced-order cardiac model in the form of an organ-scale one-fiber cardiac model, similar to that of Arts et al. [21]. This simplified reduced-order modeling approach is motivated by the fact that the one-fiber reduced-order model is established in clinical practice as part of the CircAdapt model [22]. Projection-based methods are expected to be computationally demanding on account of the nonlinear and time-dependent character of the considered FOM, and data-fit ROMs are impractical on account of the number of FOM simulations that would be required for training. While the physiological model parameters of the FOM can, to a large extent, be used directly in the employed ROM, the scan data cannot be incorporated directly. In order to introduce this input into the reduced-order model, we consider a projection-based reduced-order model for the anatomy, in which we use a proper orthogonal decomposition to represent patient-specific geometries by a relatively small number of modal coefficients. A data-fit reduced-order model in the form of a Gaussian process is then used to map this geometric input onto the effective parameters of the cardiac ROM. Figure 2: Schematic of the developed ROM framework. The framework considers the same input (circles) as the FOM (Figure 1) on which it is based. Since the ROM framework incorporates the scan-fitting algorithm in the online stage, it generates the same spline anatomy output as the FOM (light blue filled diamond). In the online stage, it also generates hemodynamical and mechanical output in the same form as that of the FOM, albeit based on a simplified ROM (open diamonds). If required, these output quantities can also be evaluated using the FOM in a subsequent offline stage, after which they can be used as additional training data. We acknowledge that the choices made in our ROM framework are strongly driven by our own research experience and that alternative choices are possible and may even be better depending on the considered setting and goals. In light of this, our goal is to develop a versatile ROM framework, in the sense that it is not specific to our choice of models. Alternative cardiac models should be usable without making fundamental changes to the framework, and ideally different types of problems should also be possible to consider, provided that their structure is similar. This paper is organized as follows. In Section 2 we commence with the introduction of the two cardiac models that serve as the FOM and the ROM in our framework. In Section 3 we then introduce the Gaussian process that maps geometric inputs onto the ROM parameter space. Subsequently, Section 4 introduces the Bayesian calibration framework that is used to generate training data for this Gaussian process. With all elements of the ROM framework in place, in Section 5 we then first extensively test it in the context of an idealized NURBS ventricle parametrized by two geometric quantities. We then extend the application of the framework to a scan-based setting in Section 6, in which the geometric input quantities are replaced by a modal decomposition of the NURBS ventricle. Finally, conclusions and recommendations are presented in Section 7."
https://arxiv.org/html/2411.08284v1,Dynamic Thresholding Algorithm with Memory for Linear Inverse Problems,"The relaxed optimal k-thresholding pursuit (ROTP) is a recent algorithm for linear inverse problems. This algorithm is based on the optimal k-thresholding technique which performs vector thresholding and error metric reduction simultaneously. Although ROTP can be used to solve small to medium-sized linear inverse problems, the computational cost of this algorithm is high when solving large-scale problems. By merging the optimal k-thresholding technique and iterative method with memory as well as optimization with sparse search directions, we propose the so-called dynamic thresholding algorithm with memory (DTAM), which iteratively and dynamically selects vector bases to construct the problem solution. At every step, the algorithm uses more than one or all iterates generated so far to construct a new search direction, and solves only the small-sized quadratic subproblems at every iteration. Thus the computational complexity of DTAM is remarkably lower than that of ROTP-type methods. It turns out that DTAM can locate the solution of linear inverse problems if the matrix involved satisfies the restricted isometry property. Experiments on synthetic data, audio signal reconstruction and image denoising demonstrate that the proposed algorithm performs comparably to several mainstream thresholding and greedy algorithms, and it works much faster than the ROTP-type algorithms especially when the sparsity level of signal is relatively low.","A typical linear inverse problem is to reconstruct unknown data d\in\mathbb{R}^{n} via some linear measurements y\in\mathbb{R}^{m} subject to noise effects: \displaystyle y=Bd+\nu, (1.1) where B\in\mathbb{R}^{m\times n} is a given measurement matrix with m\ll n, and \nu\in\mathbb{R}^{m} is a noise vector. This problem arises in many scenarios, where the number of measurements m is much smaller than the length of the target vector d. For instance, when using CT for medical diagnosis, it is expected to use as little radiation dose as possible in order to reduce the impact of radiation on the patient. Also, in the same and many other application scenarios, the target signal often admits certain special structure that makes it possible to reconstruct the signal from the underdetermined system (1.1). In fact, many natural signals and images can be sparsely represented under some orthogonal linear transforms (e.g., discrete wavelet transforms). As a result, we may assume that the target data d can be represented as d=\Phi^{T}x, where \Phi\in\mathbb{R}^{n\times n} is a transform matrix and the vector x\in\mathbb{R}^{n} is sparse (or compressible in the sense that it can be approximated by a sparse vector). In such cases, reconstructing d via solving the linear inverse problem (1.1) amounts to recovering a sparse (or compressible) vector x through the following system: \displaystyle y=Ax+\nu, (1.2) where A=B\Phi^{T}\in\mathbb{R}^{m\times n} is still called the measurement matrix. As the solution x of this problem is sparse, the problem above can be referred to as a sparse linear inverse problem. This problem has a wide range of applications in such areas as image processing [25, 42], wireless communication [6, 11, 26], sensor networks [9, 10], to name a few. The system (1.2) can be reformulated as the sparse optimization problem \displaystyle\underset{x\in\mathbb{R}^{n}}{\min}\{{\left\|y-Ax\right\|}_{2}^{2% }:\left\|x\right\|_{0}\leq k\}, (1.3) where k is a given integer number reflecting the sparsity level of x, and \left\|\cdot\right\|_{0} denotes the number of nonzero entries of a vector. For the convenience of discussion, we list the main abbreviations used in the paper in Table 1. Table 1: List of Abbreviations Abbreviation Full Name DTAM Dynamic thresholding algorithm with memory DWT Discrete wavelet transform EDOMP Enhanced dynamic orthogonal matching pursuit [49] gOMP Generalized orthogonal matching pursuit [40] NTP Natural thresholding pursuit [48] OMP Orthogonal matching pursuit [18, 38] PGROTP Partial gradient relaxed optimal k-thresholding pursuit [32] PSNR Peak signal-to-noise ratio RIC Restricted isometry constant RIP Restricted isometry property ROTP Relaxed optimal k-thresholding pursuit [45] ROTP\omega ROTP with \omega times of data compression at each iteration [45] SNR Signal-to-noise ratio SP Subspace pursuit [12] StOMP Stagewise orthogonal matching pursuit [16] Thresholding is a large class of widely used algorithms for sparse optimization problems (1.3). This class of algorithms includes the hard thresholding [4, 5, 19, 24, 30, 36, 41], optimal k-thresholding [31, 32, 37, 45, 47], soft thresholding [3, 6, 13, 15, 17, 28, 44], and the recent natural thresholding pursuit [48]. Although the hard thresholding selecting indices of a few largest magnitudes of a vector can guarantee the iterates generated by the algorithm are feasible to (1.3), it is generally not an optimal thresholding approach from the viewpoint of minimizing the error metric \|y-Ax\|_{2}^{2}, as pointed out in [45]. Thus a more sophisticated data compression method called the optimal k-thresholding was first introduced in [45], based on which the family of optimal k-thresholding algorithms, termed ROTP\omega, were proposed in [45], where \omega reflects the times of data compression in every iteration. Although ROTP\omega is generally more stable and robust for solving linear inverse problems than hard thresholding and greedy algorithms [45, 47], its computational cost remains high since the algorithm needs to solve quadratic optimization subproblems in the course of iteration. To reduce the cost, some modifications of ROTP\omega using acceleration or linearization techniques have been proposed recently [21, 32, 37, 48]. For instance, PGROTP [32] and the heavy-ball-based ROTP [37] were developed by incorporating the partial gradient and heavy-ball acceleration into ROTP (ROTP\omega with \omega=1), respectively. Numerical results indicate that PGROTP can be faster than ROTP2 [32]. However, PGROTP is still time-consuming when solving large-scale problems. It is worth mentioning that the natural thresholding pursuit (NTP) in [48], using linearization of quadratic subproblem, remarkably reduces the complexity of ROTP-type algorithms. In addition, the stochastic counterpart of NTP was recently developed in [21] for sparse optimization problems. Except for thresholding algorithms, the greedy methods are also a popular class of algorithms for solving sparse linear inverse problems. OMP is one of such greedy algorithms [18, 38] which gradually identifies the support of solution to the problem by selecting only one index in each iteration. The index selected by OMP corresponds to the largest absolute component of the gradient of error metric, i.e., the objective function in (1.3). The OMP and its modified versions were analyzed in such references as [8, 14, 34]. However, theoretical and numerical results indicate that OMP tends to be inefficient as the sparsity level k becomes large. The main reason for this might be that when k is relatively large and when the large magnitudes are close to each other, there is no guarantee for a correct index being selected by the OMP procedure, and many significant indices corresponding to large magnitudes in gradient are completely discarded at every iteration. This means most useful information conveyed by the gradient of the current iterate is ignored in OMP procedure. Motivated by this observation, several modifications of OMP with different index selection criteria were introduced, including gOMP [40], StOMP [16], EDOMP [49] and SP [12]. For instance, at every iteration, gOMP picks a fixed number, K, of the largest magnitudes of gradient. However, such a selection rule might result in a wrong index set especially when the gradient is s-sparse with s<K since in such a case the algorithm have to pick more indices than necessary. On the contrary, StOMP and EDOMP adopt certain dynamic index selection criteria whose purpose is to efficiently use the information of significant gradient components. EDOMP is generally stable, robust and efficient for sparse signal recovery, although the convergence of EDOMP has not yet established at present [49]. Inspired by the dynamic index selection strategies in StOMP [16] and EDOMP [49] and iterative methods with memory [1, 27, 35], we propose a new algorithm called dynamic thresholding algorithm with memory (DTAM) in this paper. The algorithm is different from existing ones in three aspects: (i) The iterative search direction in this method is a combination of the gradients of more than one or all iterates generated so far by the algorithm instead of the only gradient for the current iterate. (ii) The index selection in this algorithm is dynamic according to a rule defined by a generalized mean function [46] evaluated at the current search direction with memory. It should be pointed out that the generalized mean function is used for the first time to serve such a purpose. (iii) The algorithm adopts a novel dimensionality reduction strategy based on the sparsity of iterative point and search direction. The key idea here is to reduce a high-dimensional quadratic optimization problem to a low-dimensional one whose dimension is at most twice of the sparsity level of the solution to the linear inverse problem. We also carry out a rigorous analysis of DTAM to establish an error bound which measures the distance between the solution of the problem and iterates generated by the algorithm. The error bound is established under the restricted isometry property (RIP). It implies that DTAM is guaranteed to locate the k-sparse solution of linear inverse problem if the matrix satisfies the RIP of order 3k. Moreover, as a byproduct of our analysis, the convergence of PGROTP with \bar{q}=k is also obtained in this paper for the first time, which is given in Corollary 3.9. The numerical performances of DTAM and several existing algorithms including PGROTP [32], NTP [48], StOMP [16], SP [12] and OMP [18, 38] are compared through experiments on threes types of sparse linear inverse problems: The problems with synthetic data, practical audio signal reconstruction and image denoising. Numerical results indicate that the proposed algorithm does perform very well for solving linear inverse problems compared with several existing algorithms, and it works faster than PGROTP. The paper is organized as follows. In Section 2, we introduce some useful inequalities, generalized mean functions, the PGROTP algorithm, and the new algorithm DTAM. The analysis of DTAM is performed in Section 3. Numerical results are reported in Section 4, and the conclusions are given in last section."
https://arxiv.org/html/2411.08239v1,Exact Eigenvalues and Eigenvectors for Some n-Dimensional Matrices,"Building on previous work that provided analytical solutions to generalised matrix eigenvalue problems arising from numerical discretisations, this paper develops exact eigenvalues and eigenvectors for a broader class of n-dimensional matrices, focusing on non-symmetric and non-persymmetric matrices. These matrices arise in one-dimensional Laplacian eigenvalue problems with mixed boundary conditions and in a few quantum mechanics applications where standard Toeplitz-plus-Hankel matrix forms do not suffice. By extending analytical methodologies to these broader matrix categories, the study not only widens the scope of applicable matrices but also enhances computational methodologies, leading to potentially more accurate and efficient solutions in physics and engineering simulations.","The matrix eigenvalue problem (MEVP) is to find the eigenpairs (\lambda\in\mathbb{C},\bm{x}\in\mathbb{C}^{n}) such that A\bm{x}=\lambda\bm{x} (1.1) while the generalised MEVP (GMEVP) is to find the eigenpairs (\lambda\in\mathbb{C},\bm{x}\in\mathbb{C}^{n}) such that A\bm{x}=\lambda B\bm{x}, (1.2) where A,B\in\mathbb{C}^{n\times n}. While various numerical methods are available in the literature, this paper focuses on finding the analytical (interchangeably, exact) eigenvalues and eigenvectors for certain structured MEVP and GMEVP with arbitrarily large n. We provide a very brief background for the problem. MEVPs and GMEVPs are pivotal in various scientific and engineering disciplines [18, 15]. Recent developments have achieved analytical solutions for eigenpairs of certain structured matrices–such as Toeplitz-plus-Hankel and block-diagonal matrices—that typically arise from the discretisation of differential equations [12, 10, 11, 2, 7, 8]. While these solutions offer robust frameworks for symmetric or persymmetric matrices, they fall short of addressing the complexities presented by non-persymmetric and non-symmetric matrices frequently encountered in more general settings. This paper builds upon the classic and more recent results obtained for special matrices and their generalisations to structured forms (typically symmetric and persymmetric) [16, 1, 5, 4, 20, 19, 13, 21, 9, 14, 3]. We refer to [6] for a relatively more detailed literature study of matrices where exact eigenpairs are available. The work [6] provides exact eigenpairs for five sets of symmetric and persymmetric matrices as well as their generalisations. In this paper, we extend the results and provide the exact eigenvalue and eigenvectors for several new sets of n-dimensional non-symmetric and non-persymmetric matrices. To the author’s best knowledge, exact eigenpairs were not available in the literature for these new sets of matrices. The generalised approach retains the rigour of analytical methods while broadening their applicability to matrices encountered in advanced applications, including those that arise from the discretisations of PDEs with non-standard boundary conditions and other fields of science (e.g, the matrices in [17] arising from Quantum Physics). The results presented herein not only demonstrate the analytical derivability of eigenpairs for more complex matrix structures but also propose novel methodologies for their computation. The rest of the article is organized as follows. Section 2 presents the main results for matrices that are both symmetric and non-persymmetric, while Section 3 presents the main results for matrices that are both non-symmetric and non-persymmetric. Concluding remarks are given in Section 4."
https://arxiv.org/html/2411.08175v1,Well-posedness of a Variable-Exponent Telegraph Equation Applied to Image Despeckling,"In this paper, we present a telegraph diffusion model with variable exponents for image despeckling. Moving beyond the traditional assumption of a constant exponent in the telegraph diffusion framework, we explore three distinct variable exponents for edge detection. All of these depend on the gray level of the image or its gradient. We rigorously prove the existence and uniqueness of weak solutions of our model in a functional setting and perform numerical experiments to assess how well it can despeckle noisy gray-level images. We consider both a range of natural images contaminated by varying degrees of artificial speckle noise and synthetic aperture radar (SAR) images. We finally compare our method with the nonlocal speckle removal technique and find that our model outperforms the latter at speckle elimination and edge preservation.","In practice, due to various factors, images are often degraded by different types of noise, resulting in loss of pixel information in images. Therefore, image restoration is an essential step before beginning high-level image analysis. Hence a crucial challenge in digital image processing is eliminating noise from the acquired images by finding the best possible approximation of the unknown true image from a noisy image. One commonly employed strategy is to smooth out noise from the image while preserving essential attributes such as edges and textures. In this study, we focus on the removal of speckle noise. This kind of noise has a granular appearance and emerges through the interference of wavefronts in coherent imaging systems such as active radar, synthetic aperture radar, medical ultrasounds, laser, and optical coherence tomography images. Mathematically a signal-dependent image degradation model can be described as I_{0}=I\eta\, where I_{0}, I, and \eta indicate the speckled image, clean image, and the speckle-noise, respectively. It is commonly assumed that the speckle noise \eta is Gamma(L,L) distributed, where L\in{\rm I\!N} is the number of “looks” concerned to the number of spatial observations [6]. Numerous studies report the fundamentals and the statistical characteristics of the speckle noise [1, 28, 39, 40]. Well-known despeckling approaches possess Bayesian approaches in the spatial domain [28, 39, 40], Bayesian approaches in the transformed domain [4, 47], order-statistics and morphological filters [5, 18], simulated annealing despeckling [60], nonlocal filtering [50, 67], probabilistic patch-based algorithm [19], homomorphic approach [7, 34], wavelet-based approaches [2, 12, 29, 31, 56], nonlinear diffusion in the Laplacian pyramid domain [63], nonlinear diffusion-based methods [35, 36, 45, 54, 59, 61, 68], variational methods [8, 21, 27, 37, 44, 53, 55], telegraph equation [44, 45, 46] and, very recently, deep learning based approaches [17, 48, 49, 58, 66]. A thorough study of these despeckling techniques is beyond the scope of this work. For a detailed explanation of these methods, we refer the readers to [6, 20] and the references therein. This study focuses on developing a variable-exponent telegraph diffusion model for image despeckling and provides rigorous proof of existence and uniqueness. From the beginning of the Perona-Malik (PM) [51] model, nonlinear partial differential equations (PDE) have been extensively used to develop noise reduction models. Due to the availability of well-established numerical schemes and theoretical properties, PDE-based image processing is an exciting research area for real-life applications. The PDE-based approach is well-known in the image processing community and aims to remove image noise without destroying meaningful details of the image content, typically edges, lines, or other information that is crucial for understanding an image [9, 59]. The total variational (TV) based algorithms achieved remarkable results among various PDE-based models. The first variational-based approach to deal with the multiplicative noise removal problem was proposed by Rudin et al. [53], which is known as the RLO model. For an image variable I, their model takes the form I^{*}:=\text{arg}\min_{I\in BV\left(\Omega\right)}\left\{\int_{\Omega}|DI|+% \lambda_{1}\int_{\Omega}\frac{I_{0}}{I}dx+\lambda_{2}\int_{\Omega}\bigg{(}% \frac{I_{0}}{I}-1\bigg{)}^{2}dx\right\}\,, (1.1) where \lambda_{1} and \lambda_{2} are two Lagrange multipliers, dynamically updated as explained in [53] and I_{0}>0 is the initial data. Here BV(\Omega) denotes the space of functions with bounded variation [26] defined as a space of L^{1} valued functions on an open, bounded Lipschitz domain \Omega\subset\mathbb{R}^{n} such that the following quantity \displaystyle\int_{\Omega}|DI|:=\sup_{\varphi}\left\{\int_{\Omega}I\text{div}(% \varphi)dx\big{|}\varphi\in C^{1}_{0}(\Omega;\mathbb{R}^{n}),|\varphi|\leq 1% \right\}\,, is finite. The problem associated with the evolution equation of (1.1), along with the initial and boundary conditions, can be written as \left.\begin{aligned} &{I_{t}}=\text{div}\bigg{(}\frac{\nabla I}{|\nabla I|}% \bigg{)}+\lambda_{1}\frac{I_{0}}{I^{2}}+\lambda_{2}\frac{I_{0}^{2}}{I^{3}}\,% \hskip 14.22636pt\text{in}\hskip 5.69046pt\Omega_{T}:=\Omega\times(0,T)\,,\\ &I(x,0)=I_{0}(x)\hskip 95.3169pt\text{in}\hskip 5.69046pt\Omega\,,\\ &\partial_{n}I=0\hskip 125.19194pt\text{in}\hskip 5.69046pt\partial\Omega_{T}:% =\partial\Omega\times(0,T)\,.\end{aligned}\right\} Here and in the following, \Omega is the domain of the image variable I, T>0 is a specified time, and I_{t} denotes the first-time derivative of I. div and \nabla represent the divergence and gradient operator, respectively, and \partial_{n} denotes the derivative at the boundary surface \partial\Omega in the outward normal direction n. In 2008, Aubert and Aujol [8] employed the concept of speckle noise in the total variation framework. By using the maximum a-posteriori (MAP) estimator, they developed a TV model (the AA model) for image despeckling. Their model takes the form I^{*}:=\text{arg}\min_{I\in S\left(\Omega\right)}\left\{\int_{\Omega}|DI|+% \lambda\int_{\Omega}\bigg{(}\log I+\frac{I_{0}}{I}\bigg{)}dx\right\}\,, (1.2) where S(\Omega)=\{I\in BV(\Omega),I>0\} and \lambda is a regularization parameter. Later, Qiang et al. [43] proposed a modified version of the AA model for image despeckling, which uses the p-Laplace operator with a lower order term and a maximum a posteriori estimator. The model can be expressed as I^{*}:=\text{arg}\min_{I\in S\left(\Omega\right)}\left\{\int_{\Omega}|DI|^{p}+% \lambda\int_{\Omega}\bigg{(}\log I+\frac{I_{0}}{I}\bigg{)}dx\right\}\,. (1.3) where 1<p<2. In 2013, Dong et al. [24] suggest a convex total variation model for multiplicative speckle-noise reduction with the following form I^{*}:=\text{arg}\min_{I\in BV\left(\Omega\right)}\left\{\int_{\Omega}\alpha(x% )|DI|+\lambda\int_{\Omega}\left(I+I_{0}\log\frac{1}{I}\right)dx\right\}. (1.4) They choose the gray level indicator function \alpha, as \left(1-\frac{1}{1+k|G_{\xi}\ast I_{0}|^{2}}\right)\frac{1+kM^{2}}{kM^{2}},\,% \,\,\,\text{or}\,\,\,\,\,\dfrac{G_{\xi}\ast I_{0}}{M}, with M=\sup_{x\in\Omega}(G_{\xi}\ast I_{0})(x), where \xi>0, k>0, “\ast” is the convolution operator, G_{\xi} is the two dimensional Gaussian kernel and \lambda is a given parameter. In addition to TV approaches, diffusion-based filters can also remove multiplicative noise from degraded images. One of the earliest diffusion-based models for multiplicative speckle noise removal was proposed by Yu and Acton [61], integrating a spatially adaptive filter with the PM model [51]. The model takes the form \left.\begin{aligned} &{I_{t}}=\text{div}(g(q_{0},q)\nabla I)\hskip 24.18501pt% \text{in}\hskip 5.69046pt\Omega_{T}\,,\\ &I(x,0)=I_{0}(x)\hskip 51.21504pt\text{in}\hskip 5.69046pt\Omega\,,\\ &\partial_{n}I=0\hskip 79.6678pt\text{in}\hskip 5.69046pt\partial\Omega_{T}\,.% \end{aligned}\right\} Here g(\cdot) is the diffusion coefficient, which can be defined as g(q,q_{0})=\left(1+\frac{q^{2}-{q_{0}}^{2}}{{q_{0}}^{2}(1+{q_{0}}^{2})}\right)% ^{-1}, where q is the instantaneous coefficient of variation (ICOV), serves as the edge detector function, and determined by the formula q(I,\nabla I,\nabla^{2}I)=\sqrt{\dfrac{(1/2)(\nabla I/I)^{2}-(1/16)(\nabla^{2}% I/I)^{2}}{[1+(1/4)(\nabla^{2}I/I)]^{2}}}\,, and q_{0} is the speckle scale function, serves as the diffusion threshold value determined by the ratio of the local standard deviation to mean q_{0}(I)=\frac{std(I)}{mean(I)}\,. This filter provides significant enhancement in edge preservation and speckle suppression when compared with conventional filters. Later, based on a gray level indicator function, Zhou et al. proposed a diffusion model called “doubly degenerate diffusion (DDD) [68]” for the multiplicative noise removal problem. Their model takes the form \left.\begin{aligned} &I_{t}=\text{div}(g(I,|\nabla I|)\nabla I),\hskip 14.226% 36pt\text{in}\hskip 5.69046pt\Omega_{T},\\ &I(x,0)=I_{0}(x),\hskip 51.21504pt\text{in}\hskip 5.69046pt\Omega\,,\\ &\partial_{n}I=0,\hskip 76.82234pt\text{in}\hskip 5.69046pt\partial\Omega_{T}% \,.\end{aligned}\right\} They choose the diffusion coefficient as \displaystyle g\left(I,|\nabla I|\right)=\dfrac{2|I|^{\nu}}{M^{\nu}+|I|^{\nu}}% \cdot\dfrac{1}{\left(1+|\nabla I|^{2}\right)^{(1-\beta)/2}}, where \nu>0, 0<\beta<1, and M=\sup_{x\in\Omega}I. In this case, the gray level indicator and edge detector functions are a(I):=\dfrac{2|I|^{\nu}}{M^{\nu}+|I|^{\nu}} and b(I):=\dfrac{1}{\left(1+|\nabla I|^{2}\right)^{(1-\beta)/2}} respectively. In 2017, Zhou et al. [69] proposed a variable exponent diffusion model for image despeckling. The model takes the form \left.\begin{aligned} &I_{t}=\text{div}\left(\dfrac{\nabla I}{1+\left(|\nabla I% _{\rho}|/K\right)^{\beta(I)}}\right),\hskip 14.22636pt\text{in}\hskip 5.69046% pt\Omega_{T},\\ &I(x,0)=I_{0}(x),\hskip 85.35826pt\text{in}\hskip 5.69046pt\Omega\,,\\ &\partial_{n}I=0,\hskip 113.81102pt\text{in}\hskip 5.69046pt\partial\Omega_{T}% \,.\end{aligned}\right\} (1.5) Here \beta(I) is a region indicator, and the authors chose it as follows \displaystyle\beta\left(I\right)=2-\dfrac{2|I|^{\alpha}}{M^{\alpha}+|I|^{% \alpha}}, where K,\alpha>0, I_{\rho}=G_{\rho}\ast I, and M=\sup_{x\in\Omega}I. Also, the authors in [69] establish the existence and uniqueness results of their model. The present paper aims to propose a variable exponent telegraph diffusion model for image despeckling, and we establish the existence and uniqueness of our model. Over the last few years, many researchers investigated the variable exponent-based diffusion models [10, 13, 16, 32, 41, 57, 69] for image denoising. Since the variable exponent models utilize the benefits of isotropic diffusion, TV diffusion, and anisotropic diffusion depending on the values of the exponent, they have some advantages over fixed exponent-based models for image restoration processes. Furthermore, telegraph diffusion-based methods [14, 11, 44, 45, 46, 52, 64, 65] have been widely explored for removing both additive and multiplicative noise. Despite the promising applications of variable-exponent diffusion models in noise reduction and theoretical studies, the use of variable-exponent telegraph diffusion models remains unexplored. To the best of our knowledge, no previous research has proposed variable exponents in a telegraph equation framework for speckle noise elimination. Furthermore, the theoretical results are unexplored in this setting. Based on existing results in the literature, it seems that the variable exponent telegraph equation could be a potentially effective technique for speckle noise removal. In this study, we first analyze existing variable-exponent diffusion models for image despeckling and then extend these models to a telegraph diffusion framework. Additionally, we establish the existence and uniqueness of weak solutions for both diffusion and telegraph diffusion models with variable exponents by considering a general form for the diffusion coefficient. These results can be directly applied to similar PDE models to check their wellposedness. The remainder of the paper is organized as follows. In Section 2, we review existing diffusion-based models with variable exponents and introduce some modifications to the exponents. We then discuss existing telegraph diffusion models and propose new hybrid telegraph diffusion models for image despeckling. Section 3 presents the existence and uniqueness results for both the diffusion and telegraph diffusion models. In Section 4, we describe the numerical implementation of our model, and Section 5 evaluates the despeckling performance of the proposed approach. Finally, Section 6 provides the conclusion."
https://arxiv.org/html/2411.07855v1,Filtered finite difference methods for nonlinear Schrödinger equations in semiclassical scaling,"This paper introduces filtered finite difference methods for numerically solving a dispersive evolution equation with solutions that are highly oscillatory in both space and time. We consider a semiclassically scaled nonlinear Schrödinger equation with highly oscillatory initial data in the form of a modulated plane wave. The proposed methods do not need to resolve high-frequency oscillations in both space and time by prohibitively fine grids as would be required by standard finite difference methods. The approach taken here modifies traditional finite difference methods by incorporating appropriate filters. Specifically, we propose the filtered leapfrog and filtered Crank–Nicolson methods, both of which achieve second-order accuracy with time steps and mesh sizes that are not restricted in magnitude by the small semiclassical parameter. Furthermore, the filtered Crank–Nicolson method conserves both the discrete mass and a discrete energy. Numerical experiments illustrate the theoretical results. Keywords. finite difference method, filter, nonlinear Schrödinger equation, semiclassical, highly oscillatory, asymptotic-preserving, uniformly accurateMathematics Subject Classification (2020): 65M06, 65M12, 65M15","As a basic model problem of a dispersive evolution equation with solutions that are highly oscillatory in both space and time, we consider the time-dependent weakly nonlinear Schrödinger equation in semiclassical scaling carles2008semi ; carles2010multiphase , \mathrm{i}\varepsilon\,\partial_{t}u+\frac{\varepsilon^{2}}{2}\Delta u=\lambda% \varepsilon\,|u|^{2}u, (1) which is to be solved for the complex-valued function u=u(t,x) under periodic boundary conditions with x\in\mathbb{T}^{d}=(\mathbb{R}/2\pi\mathbb{Z})^{d} over a bounded time interval {0<t\leq T} and with highly oscillatory initial data at t=0: u(0,x)={\mathrm{e}}^{\mathrm{i}\kappa\cdot x/\varepsilon}a_{0}(x). (2) Here, 0<\varepsilon\ll 1 represents the semiclassical small parameter, and \lambda is a fixed nonzero real number. In the initial condition, \kappa\in\mathbb{R}^{d}\backslash\{0\} is a fixed wave vector, and a_{0}:\mathbb{T}^{d}\rightarrow\mathbb{C} is a given smooth profile function with derivatives bounded independently of \varepsilon. The final time T is chosen independently of \varepsilon. On this time scale, the nonlinearity has an O(1) effect on the solution. The initial function u(0,\cdot) in (2) is required to be a 2\pi-periodic continuous function. This is satisfied if the small parameter \varepsilon is assumed to take only values for which \kappa/\varepsilon\in\mathbb{Z}^{d} and a_{0} is 2\pi-periodic. This assumption on \varepsilon is not a restriction, since it can always be achieved with an O(\varepsilon) modification of \kappa and a corresponding smooth modification of a_{0}. It is readily checked that there are two conserved quantities for (1) in any dimension d\geq 1: the mass \int_{\mathbb{T}^{d}}|u|^{2}dx=\text{const}. and the energy \int_{\mathbb{T}^{d}}\left(\frac{\varepsilon^{2}}{2}|\nabla u|^{2}+\frac{% \lambda\varepsilon}{2}|u|^{4}\right)dx=\text{const}. It is known that the solution u(t,x) is highly oscillatory in both time and space at a scale proportional to the small semiclassical parameter \varepsilon. This poses significant challenges in the development of efficient numerical methods and their error analysis. Traditional finite difference methods like the leapfrog and Crank–Nicolson schemes have been studied for Schrödinger-type equations in the semiclassical scaling markowich1999numerical , where stringent constraints on the time step \tau\ll\varepsilon and mesh size h\ll\varepsilon are required to guarantee accurate approximations of observables. Time-splitting spectral discretizations bao2002time ; bao2003numerical ; jin2011mathematical ; lasser2020computing ease these restrictions, allowing for \tau=O(\varepsilon),h=o(\varepsilon) while still providing accurate results. Asymptotic-preserving methods have been proposed in arnold2011wkb ; degond2007asymptotic ; besse2013asymptotic by reformulating the Schrödinger equation using the WKB expansion grenier1998semiclassical ; carles2008semi or the Madelung transform madelung1927quantum . It is commonly believed that finite difference methods applied directly to (1) require very restrictive meshing conditions and are thus not suitable for solving the semiclassical Schrödinger equation. In this work, we aim to demonstrate the effectiveness of finite difference methods when enhanced with appropriate filtering. Specifically, we present two finite difference methods, constructed by applying appropriate filters to the leapfrog and Crank–Nicolson methods. These filtered schemes enable us to approximate the solution of (1)–(2) with second-order accuracy even when using comparatively large time steps \tau and mesh sizes h that are not restricted by \varepsilon. The proposed filtered methods tend to the standard leapfrog and Crank–Nicolson schemes as the ratios of the time step and mesh size to the semiclassical parameter \varepsilon approach zero, which is, however, not the regime of principal interest in this paper. The methods are not only asymptotic-preserving as \varepsilon\to 0 but are also uniformly accurate (of order 1/2) for 0<\varepsilon\leq 1. Moreover, the filtered Crank–Nicolson scheme turns out to preserve the discrete mass and a modified discrete energy exactly. For various classes of highly oscillatory ordinary differential equations, filtered time-stepping methods have already been used successfully, e.g., in garcia1998long ; hochbruck99gautschi ; hairer00long ; hairer02gni ; grimm06error ; hairer20filtered ; hairer22large . Modulated Fourier expansions hairer02gni are a powerful tool for deriving and analyzing numerical methods for highly oscillatory problems. They represent both the exact and the numerical solution as sums of products of slowly varying modulation functions and highly oscillatory exponentials. Comparing the modulated Fourier expansions of the numerical and the exact solution then yields error bounds. We shall pursue a related approach also here, for the first time combined in both time and space, which becomes possible under a consistency relation between the time step and the spatial mesh size. We will formulate the filtered finite difference methods and prove results for them only in the spatially one-dimensional case (d=1). This apparent limitation is introduced only for ease of presentation. The methods and the theoretical results can be extended to higher dimensions without additional difficulties. The extension to the full space \mathbb{R}^{d} instead of the torus \mathbb{T}^{d} is straightforward for the formulation of the methods and can be done analogously in the theory. The condition \kappa/\varepsilon\in\mathbb{Z}^{d} imposed for 2\pi-periodicity is then no longer needed. In Section 2, we introduce the filtered finite difference methods and state the main results of this paper. We give the dominant term of the modulated Fourier expansion of the numerical solution and prove second-order error bounds over a fixed time interval (independent of \varepsilon), with step sizes \tau and meshwidths h that can be arbitrarily large compared to \varepsilon. For h\gg\varepsilon, there is a mild stepsize restriction \tau\leq ch for the filtered leapfrog method and no such restriction for the filtered Crank-Nicolson method. However, the step size \tau and the mesh width h cannot be chosen independently but are related by a consistency condition. The results of Section 2 are proved in Sections 3 and 4. In Section 3 we study the consistency error, i.e., the defect obtained on inserting a function with controlled small distance to the exact solution into the numerical scheme. Section 4 first presents the linear Fourier stability analysis and then gives a nonlinear stability analysis that bounds the error of the numerical solution in terms of the defect. In Section 5, numerical experiments are conducted to illustrate the theoretical results. We also present numerical experiments on the long-time behaviour of mass and energy with the filtered finite difference discretizations, which go beyond the analysis in this paper."
https://arxiv.org/html/2411.07720v1,Explicit symmetric low-regularity integrators for the nonlinear Schrödinger equation,"The numerical approximation of low-regularity solutions to the nonlinear Schrödinger equation is notoriously difficult and even more so if structure-preserving schemes are sought. Recent works have been successful in establishing symmetric low-regularity integrators for this equation. However, so far, all prior symmetric low-regularity algorithms are fully implicit, and therefore require the solution of a nonlinear equation at each time step, leading to significant numerical cost in the iteration. In this work, we introduce the first fully explicit (multi-step) symmetric low-regularity integrators for the nonlinear Schrödinger equation. We demonstrate the construction of an entire class of such schemes which notably can be used to symmetrise (in explicit form) a large amount of existing low-regularity integrators. We provide rigorous convergence analysis of our schemes and numerical examples demonstrating both the favourable structure preservation properties obtained with our novel schemes, and the significant reduction in computational cost over implicit methods.","We consider the numerical approximation of low-regularity solutions to the nonlinear Schrödinger equation (NLSE) with cubic nonlinearity on the torus \mathbb{T}^{d}\ (d=1,2,3) in the following form: (1) \begin{cases}i\partial_{t}u=-\Delta u+\mu|u|^{2}u,&\quad t>0,{\quad\textbf{x}}% \in\mathbb{T}^{d},\\ u(0,\textbf{x})=u_{0}(\textbf{x}),&\quad\textbf{x}\in\mathbb{T}^{d},\end{cases} where u=u(t,\textbf{x}) and \mu is a positive/negative constant corresponding to the defocusing/focusing NLSE. The NLSE with cubic nonlinearity, also called Gross-Pitaevskii equation (GPE), is derived from the mean-field approximation of many-body problems in quantum physics and chemistry, especially in the modeling and simulation of Bose-Einstein condensation [8, 29]. We are interested in the design of numerical schemes for the NLSE (1) with low-regularity initial data (which also results in the low regularity of solutions), meaning that u_{0}\in H^{\alpha}, where H^{\alpha}=H^{\alpha}(\mathbb{T}^{d}) is the periodic Sobolev space of order \alpha, for some \alpha>0 of small magnitude. The specific regularity assumptions (and thus the minimum value of \alpha required for the convergence of our schemes) will become apparent throughout Sections 2 & 3. For the NLSE with sufficiently smooth data, various accurate and efficient numerical methods have been proposed and analyzed in the past few years, including the finite difference methods [1, 5, 9], exponential integrators [10, 17, 19], time splitting methods [11, 12, 14, 24]. However, these classical numerical methods generally require much higher regularity of the solution to converge at desired optimal order. In the past decade, numerical approximation to nonsmooth solutions of the NLSE and other dispersive equations with low-regularity solutions has received significant attention to relax the regularity requirements in the numerical analysis community. The first low-regularity integrator (LRI) treating this approximation for the NLSE was introduced by Ostermann & Schratz [26], and this was followed by higher order methods constructed by Ostermann et al. [27] and Bruned & Schratz [15], and integrators with lower regularity assumptions by Cao et al. [16]. These initial constructions on a torus have also been extended to non-periodic boundary conditions by Alama Bronsard [3] and Bai et al. [6], and a fully discrete error analysis was provided by Li & Wu [23] and Ostermann & Yao [28]. Recent work noticed that, while these novel integrators perform very well in terms of convergence properties for low-regularity regimes, their structure preservation properties and consequently long-time behaviour were lacking. This resulted in a sequence of works constructing structure preserving low-regularity integrators for the NLSE and other dispersive systems by Alama Bronsard [2], Alama Bronsard et al. [4], Banica et al. [7], Feng et al. [18], and Maierhofer & Schratz [25]. These works have demonstrated that it is possible to develop integrators that maintain favourable convergence behaviour for the low-regularity data, while introducing structure preservation properties, in particular constructing symmetric low-regularity integrators. However, in all the aforementioned works, the introduction of symmetry into the integrators required the methods to become implicit, thus incurring the additional computational cost of the solution of a nonlinear equation at each time step. Recent work in the construction of exponential integrators has successfully overcome this issue by designing (and analysing) multi-step methods that are fully explicit, which maintain similar favourable features as their single step analogues but are also symmetric. This construction was performed in particular for the NLSE by Bao & Wang [13], for the nonlinear Dirac equation by Jahnke & Kirn [20], and for the dispersion-managed nonlinear Schrödinger equation by Jahnke & Mikl [21, 22]. In this work, we seek to take on board the approach taken in these recent constructions and we design what are, to the best of our knowledge, the first fully explicit symmetric low-regularity integrators for the nonlinear Schrödinger equation. We study the convergence properties of these new schemes and demonstrate that efficient computations are possible while maintaining both guaranteed low-regularity convergence and symmetry of the method, the latter of which leads to favourable long-time behaviour of the methods observed in numerical experiments. The remainder of this paper is structured as follows. In Section 2, we introduce the construction of our novel explicit two-step low-regularity schemes based on the availability of single-step non-symmetric low-regularity integrators of specific form. In particular, we provide several examples of existing non-symmetric low-regularity integrators available in the literature which fit this framework in Section 2.3. The convergence analysis of these methods is presented in Section 3, where a general framework is introduced in Section 3.1 and the detailed error estimates for specific examples are carried out in Sections 3.2 & 3.3. We provide detailed numerical experiments highlighting the advantages of our new methodology in Section 4. Finally, some concluding remarks and directions for future research are provided in Section 5. Throughout the paper, we denote by C a generic positive constant independent of the mesh size h and time step size \tau, and by C(M) a generic positive constant depending on the parameter M. The notation A\lesssim B is used to represent that there exists a generic constant C>0, such that |A|\leq CB."
https://arxiv.org/html/2411.07712v1,Rate of convergence for numerical \alpha-dissipative solutions of the Hunter–Saxton equation,"We prove that \alpha-dissipative solutions to the Cauchy problem of the Hunter–Saxton equation, where \alpha\in W^{1,\infty}(\mathbb{R},[0,1)), can be computed numerically with order \mathcal{O}({\Delta x}^{\nicefrac{{1}}{{8}}}+{\Delta x}^{\nicefrac{{\beta}}{{4% }}}) in L^{\infty}(\mathbb{R}), provided there exist constants C>0 and \beta\in(0,1] such that the initial spatial derivative \bar{u}_{x} satisfies \|\bar{u}_{x}(\cdot+h)-\bar{u}_{x}(\cdot)\|_{2}\leq Ch^{\beta} for all h\in(0,2]. The derived convergence rate is exemplified by a number of numerical experiments.","This paper is concerned with the Cauchy problem for the Hunter–Saxton (HS) equation, which takes the form (1.1) u_{t}(t,x)+uu_{x}(t,x)=\frac{1}{4}\int_{-\infty}^{x}u_{x}^{2}(t,z)dz-\frac{1}{% 4}\int_{x}^{\infty}u_{x}^{2}(t,z)dz,\hskip 11.38092ptu|_{t=0}=\bar{u}. The above equation was derived in [18] via a first-order asymptotic expansion around constant equilibrium states of the nonlinear variational wave equation, \psi_{tt}+c(\psi)(c(\psi)\psi_{x})_{x}=0. One can therefore view solutions of (1.1) as describing the long-time behavior of these perturbed equilibrium states. The HS equation has many intriguing properties, but of particular importance and of main concern from a numerical perspective is the fact that solutions experience wave breaking – a phenomenon characterized by pointwise blow-ups of the spatial derivative u_{x} within finite time. Classical solutions therefore cease to exist, and one has to study weak solutions. Despite u_{x} developing singularities at certain points in space-time, u(t,\cdot) remains continuous for all t\geq 0, in fact Hölder continuous, while u_{x}(t,\cdot)\in L^{2}(\mathbb{R}). In addition, wave breaking also gives rise to energy concentrations on sets of measure zero, and in order to extend weak solutions beyond wave breaking, one has to ambiguously choose how to manipulate this concentrated energy. Two natural choices immediately come to mind: i) one can choose to reinsert all the concentrated energy at every wave breaking occurrence, leading to conservative solutions, or ii) all the concentrated energy can be removed from the system, yielding dissipative solutions. Well-posedness has been established for both kinds of solutions, see [1, 2, 8, 10]. Yet a more general and flexible concept is that of \alpha-dissipative solutions, proposed in [11] for the related Camassa–Holm equation and in [13] for the HS equation. As the name suggests, the idea is to remove an \alpha-fraction of the concentrated energy, but in addition, one may allow \alpha to belong to W^{1,\infty}(\mathbb{R},[0,1))\cup\{1\}, such that the amount of energy removed can depend on the spatial location at which wave breaking takes place. This is the kind of solution we will focus on in this work. Let us emphasize that the existence of such solutions has been shown in [13], but uniqueness is still an open question. To keep track of the corresponding energy, which distinguishes the various continuations, it is common to augment the wave profile u by a nonnegative, finite Radon measure \mu that represents the energy density, see e.g., [2, 13]. This measure encodes all the information about wave breaking, and its absolutely continuous part satisfies d\mu_{\mathrm{ac}}=u_{x}^{2}dx. Moreover, \mu_{\mathrm{sing}}, its singular part, tells us where energy has concentrated. However, let us also point out that wave breaking can occur in the form of infinitesimal energy concentrations, which are described by \mu_{\mathrm{ac}}, see for instance the cusped wave profile in Example 4.3, which is discussed in [5, Ex. 5.2] and [14, Ex. 3] as well. All the aforementioned continuations require the energy to be nonincreasing in time and are therefore governed by the following system (1.2a) \displaystyle u_{t}+uu_{x} \displaystyle=\frac{1}{2}F-\frac{1}{4}F_{\infty}(t), (1.2b) \displaystyle\mu_{t}+(u\mu)_{x} \displaystyle\leq 0, where F(t,x)=\mu(t,(-\infty,x)) denotes the cumulative energy associated with the measure \mu(t). This system does not provide us with enough information to distinguish different types of weak solutions, due to the measure-valued transport inequality. In other words, except from the particular case of equality in (1.2b), which leads to conservative solutions, the above system contains no information about the exact rate of energy dissipation. To resolve this issue, one introduces a coordinate transformation, namely a mapping L, which transforms the Eulerian initial data (\bar{u},\bar{\mu},\bar{\nu}) into a quadruplet \bar{X}=(\bar{y},\bar{U},\bar{V},\bar{H}) in Lagrangian coordinates. Here \bar{\nu} is a measure added purely for technical reasons in the sense that the same solution (u,\mu)(t) is recovered for any t\geq 0, irregardless of which \bar{\nu} we pick initially, see [15, Lem. 2.13] for details. The crux is that, under this transformation, (1.2) rewrites into a system of ODEs that attains unique and global solutions and it also gives us control of the exact loss of energy upon wave breaking. More precisely, see [13, Sec. 2], the time evolution of the \alpha-dissipative solution X=(y,U,V,H) with initial data \bar{X}=(\bar{y},\bar{U},\bar{V},\bar{H}) is governed by (1.3a) \displaystyle y_{t}(t,\xi) \displaystyle=U(t,\xi), (1.3b) \displaystyle U_{t}(t,\xi) \displaystyle=\frac{1}{2}V(t,\xi)-\frac{1}{4}V_{\infty}(t), (1.3c) \displaystyle V(t,\xi) \displaystyle=\int_{-\infty}^{\xi}\big{(}1-\alpha(y(\tau(\eta),\eta))\chi_{\{% \omega\mid t\geq\tau(\omega)>0\}}(\eta)\big{)}\bar{V}_{\xi}(\eta)d\eta, (1.3d) \displaystyle H_{t}(t,\xi) \displaystyle=0, where V_{\infty}(t)=\displaystyle\lim_{\xi\rightarrow\infty}V(t,\xi) represents the total Lagrangian energy and \tau:\mathbb{R}\rightarrow[0,\infty] is the wave breaking function given by \displaystyle\tau(\xi) \displaystyle=\begin{cases}0,&\bar{y}_{\xi}(\xi)=0=\bar{U}_{\xi}(\xi),\\ -2\frac{\bar{y}_{\xi}(\xi)}{\bar{U}_{\xi}(\xi)},&\bar{U}_{\xi}(\xi)<0,\\ \infty,&\text{otherwise}.\end{cases} Solving (1.3) amounts to following the sought solution (u,F) along generalized characteristics, and to recover (u,F), one introduces yet another nonlinear mapping M. The nonlinear nature of the mappings L and M severely complicates the upcoming convergence rate analysis. The pointwise blow-up of u_{x} at wave breaking introduces difficulties with numerical stability, especially for finite difference schemes, but also for standard continuous Galerkin methods. By relaxing the continuity hypothesis and allowing for discontinuities across element interfaces, the authors in [24] introduce a local discontinuous Galerkin (LDG) method for (1.1). This scheme is based on a central flux formulation and its spatial derivative has a nonincreasing L^{2}(\mathbb{R})-norm, i.e., the method is energy stable. However, none of the presented numerical results experiences wave breaking, and as no rigorous convergence analysis is conducted, it is unclear whether this method is able to handle wave breaking. Moreover, the same authors revisit the LDG method in [25], and replace the central flux with an upwind flux, which again leads to an energy stable method. They also propose a new DG method, which turns out to coincide with the finite difference scheme developed in [16, Sec. 6] when choosing piecewise constant functions. Since the upwind scheme in [16, Sec. 6] converges towards dissipative solutions, even when wave breaking occurs, so does the DG method in this case. There is an intrinsic numerical diffusion in traditional finite difference schemes and they therefore naturally give rise to dissipative solutions as shown in [16]. Moreover, such solutions are characterized by the following Oleinik-type condition, see [8, 16], u_{x}(t,x)\leq\frac{2}{t}\hskip 34.14322pt\text{for all }t>0\text{ and a.e. }x% \in\mathbb{R}, which provides additional stability and makes them more amendable for numerics. In spite of that, a scheme based on the method of characteristics, which equivalently can be expressed as a finite difference scheme, was shown to converge to conservative solutions in [14]. Furthermore, several geometrically oriented finite difference methods were proposed in [21], albeit without any convergence analysis. In addition, a convergent numerical algorithm for \alpha-dissipative solutions was recently proposed in [5] for \alpha\in[0,1] and subsequently extended to \alpha\in W^{1,\infty}(\mathbb{R},[0,1)) in [4]. The latter method, to which we derive a convergence rate, relies on the observation that if the initial data \bar{u} in (1.1) is piecewise linear, then so is the associated \alpha-dissipative solution u(t,\cdot) for all t\geq 0. One combines this property with a piecewise linear projection operator P_{{\Delta x}} that preserves the relation d\bar{\mu}_{\mathrm{ac}}=\bar{u}_{x}^{2}dx, which is essential to ensure that the numerical approximation, for each fixed {\Delta x}>0, dissipates energy when it is supposed to. The numerical solution is thereafter evolved by an iteration scheme that is based on computing successive approximations of the solution to (1.3) with initial data L\circ P_{{\Delta x}}((\bar{u},\bar{\mu},\bar{\nu})). This is explained throughly in Section 2.2 and [4, Sec. 3.2]. Despite the existence of several numerical methods for (1.1), the convergence rate derived in [3] for \alpha\in[0,1] and here for \alpha\in W^{1,\infty}(\mathbb{R},[0,1)) are the first robust convergence rates, i.e., error rates that persist even if wave breaking takes place. The few results that exist elsewhere either break down at wave breaking or prevent it from taking place, see [3, Sec. 1] for a brief discussion. To be more precise, we prove that the numerical method from [4] satisfies (1.4) \sup_{t\in[0,T]}\!\|u(t)-u_{{\Delta x}}(t)\|_{\infty}\leq\mathcal{O}({\Delta x% }^{\gamma}), for some \gamma>0, where T>0 is a fixed final simulation time, {\Delta x} denotes the spatial discretization parameter, and \{u_{{\Delta x}}\}_{{\Delta x}>0} is the family of numerical approximations. It suffices to this end, to derive a convergence rate in [L^{\infty}(\mathbb{R})]^{2} for the numerical Lagrangian pair (y_{{\Delta x}}\!-\!\text{{id}},U_{{\Delta x}})(t), since by [5, Lem. 4.11], we have \sup_{t\in[0,T]}\!\|u(t)-u_{{\Delta x}}(t)\|_{\infty}\!\leq\!\sup_{t\in[0,T]}% \!\|U(t)-U_{{\Delta x}}(t)\|_{\infty}+\sqrt{\bar{\mu}(\mathbb{R})}\!\!\sup_{t% \in[0,T]}\!\|y(t)-y_{{\Delta x}}(t)\|_{\infty}^{\nicefrac{{1}}{{2}}}. This is however challenging, due to the highly nonlinear relation between the Eulerian and Lagrangian variables. In particular, the singular part \bar{\mu}_{\mathrm{sing}}, which is supported on a set of measure zero in Eulerian coordinates, leads to a set \mathcal{A} of positive measure in Lagrangian coordinates, on which we are unable to connect the differentiated Eulerian and Lagrangian variables. As a consequence, we only have a convergence rate for the initial Lagrangian energy density, \bar{V}_{{\Delta x},\xi}, on \mathbb{R}\setminus\mathcal{A}. However, as the norms \|y(t)-y_{{\Delta x}}(t)\|_{\infty} and \|U(t)-U_{{\Delta x}}(t)\|_{\infty} heavily depend on the difference V_{\xi}(t)-V_{{\Delta x},\xi}(t) on all of \mathbb{R}, cf. (1.3), we have to prove that the set \mathcal{A} contributes at most with a certain order. To achieve this, one needs to postulate additional regularity on the Eulerian initial data. In particular, we require that there exist constants C>0 and \beta\in(0,1] such that (1.5) \|\bar{u}_{x}(\cdot+h)-\bar{u}_{x}(\cdot)\|_{2}\leq Ch^{\beta}\hskip 11.38092% pt\text{for all }h\in(0,2]. The remaining argument then hinges on the fact that, by construction, the aforementioned projection operator, P_{{\Delta x}}, preserves the mass of the singular part \bar{\mu}_{\mathrm{sing}} locally. This property is combined with a novel pair of transformations, inspired by [17, Def. 6], that allows us to prove that the contribution from the set \mathcal{A} is at most of order \mathcal{O}({\Delta x}^{\nicefrac{{\beta}}{{2}}}). The way in which we proceed to prove the convergence rate shares a lot of similarities with the one proposed in [3] for \alpha\in[0,1] – the main difference is the transformation used to deal with the problematic set \mathcal{A}. The mapping in [3], which can be constructed explicitly, maps the points where the exact solution breaks initially to those points where the numerical solution breaks initially, and, when combined with the local preservation of \bar{\mu}_{\mathrm{sing}}, this enables one to prove that the set \mathcal{A} contributes at most with order \mathcal{O}({\Delta x}^{\nicefrac{{\beta}}{{2}}}). However, the author of [3] has to assume that the initial energy measure \bar{\mu} has no singular continuous part. In this work, on the other hand, we introduce a pair of transformations which rescales both the projected and the exact initial data. In this way we are able to handle any finite, positive Radon measure \bar{\mu}, but this comes at the cost of only having implicit expressions for the transformations. In spite of that, we also obtain an improved convergence rate; instead of the \mathcal{O}({\Delta x}^{\nicefrac{{\beta}}{{8}}})-rate shown in [3, Thm. 4.11] for \alpha\in[0,1], we deduce that (1.4) holds with \gamma=\frac{1}{4}\min\{\beta,\frac{1}{2}\} for \alpha-dissipative solutions where \alpha\in W^{1,\infty}(\mathbb{R},[0,1))\cup\{1\}, which reduces to \mathcal{O}({\Delta x}^{\nicefrac{{\beta}}{{4}}}) when \alpha is a constant. The transformations used in [3] and herein are compared in Section 3.6. Note that even if one starts with a purely absolutely continuous energy measure, a singular continuous part may form at some later time. Thus, in order to be able to analyze numerical methods where one maps back and forth between Eulerian and Lagrangian coordinates, like in [14], one has to be able to treat singular continuous measures. We hope that our approach also turns out valuable in such cases. This paper is organized in the following way. In Section 2 we outline how to construct \alpha-dissipative solutions via the generalized method of characteristics from [2] and [13] and we thereafter recall the numerical method from [4]. Then, in Section 3, we proceed by proving the aforementioned convergence rate. Our rigorous analysis of the pair of transformations and the subsequent change of variables play a key role. Finally, in Section 4 we conduct several numerical experiments which support our theoretical result. In particular, the cusped wave profile analyzed in [5, Ex. 5.2] and [14, Ex. 3] satisfies (1.5) with \beta=\nicefrac{{1}}{{6}}, and hence converges with order \mathcal{O}({\Delta x}^{\nicefrac{{1}}{{24}}})."
https://arxiv.org/html/2411.07532v1,"Goal oriented optimal design of infinite-dimensional
Bayesian inverse problems using quadratic approximations","We consider goal-oriented optimal design of experiments for infinite-dimensional Bayesian linear inverse problems governed by partial differential equations (PDEs). Specifically, we seek sensor placements that minimize the posterior variance of a prediction or goal quantity of interest. The goal quantity is assumed to be a nonlinear functional of the inversion parameter. We propose a goal-oriented optimal experimental design (OED) approach that uses a quadratic approximation of the goal-functional to define a goal-oriented design criterion. The proposed criterion, which we call the G_{q}-optimality criterion, is obtained by integrating the posterior variance of the quadratic approximation over the set of likely data. Under the assumption of Gaussian prior and noise models, we derive a closed-form expression for this criterion. To guide development of discretization invariant computational methods, the derivations are performed in an infinite-dimensional Hilbert space setting. Subsequently, we propose efficient and accurate computational methods for computing the G_{q}-optimality criterion. A greedy approach is used to obtain G_{q}-optimal sensor placements. We illustrate the proposed approach for two model inverse problems governed by PDEs. Our numerical results demonstrate the effectiveness of the proposed strategy. In particular, the proposed approach outperforms non-goal-oriented (A-optimal) and linearization-based (c-optimal) approaches.","Inverse problems are common in science and engineering applications. In such problems, we use a model and data to infer uncertain parameters, henceforth called inversion parameters, that are not directly observable. We consider the case where measurement data are collected at a set of sensors. In practice, often only a few sensors can be deployed. Thus, optimal placement of the sensors is critical. Addressing this requires solving an optimal experimental design (OED) problem AtkinsonDonev92 ; Ucinski05 ; Pukelsheim06 . In some applications, the estimation of the inversion parameter is merely an intermediate step. For example, consider a source inversion problem in a heat transfer application. In such problems, one is often interested in prediction quantities such as the magnitude of the temperature within a region of interest or heat flux through an interface. A more complex example is a wildfire simulation problem, where one may seek to estimate the source of the fire, but the emphasis is on prediction quantities summarizing future states of the system. In such problems, design of experiments should take the prediction/goal quantities of interest into account. Failing to do so might result in sensor placements that do not result in optimal uncertainty reduction in the prediction/goal quantities. This points to the need for a goal-oriented OED approach. This is the subject of this article. We focus on Bayesian linear inverse problems governed by PDEs with infinite-dimensional parameters. To make matters concrete, we consider the observation model, \boldsymbol{y}=\mathcal{F}m+\boldsymbol{\eta}. (1.1) Here, \boldsymbol{y}\in\mathbb{R}^{d} is a vector of measurement data, \mathcal{F} is a linear parameter-to-observable map, m is the inversion parameter, and \boldsymbol{\eta} is a random variable that models measurement noise. We consider the case where m belongs to an infinite-dimensional real separable Hilbert space \mathscr{M} and \mathcal{F}:\mathscr{M}\to\mathbb{R}^{d} is a continuous linear transformation. The inverse problem seeks to estimate m using the observation model (1.1). Examples of such problems include source inversion or initial state estimation in linear PDEs. See Section 2, for a brief summary of the requisite background regarding infinite-dimensional Bayesian linear inverse problems and OED for such problems. We consider the case where solving the inverse problem is an intermediate step and the primary focus is accurate estimation of a scalar-valued prediction quantity characterized by a nonlinear goal-functional, \mathcal{Z}:\mathscr{M}\to\mathbb{R}. (1.2) In the present work, we propose a goal-oriented OED approach that seeks to find sensor placements minimizing the posterior uncertainty in such goal-functionals. Related work. The literature devoted to OED is extensive. Here, we discuss articles that are closely related to the present work. OED for infinite-dimensional Bayesian linear inverse problems has been addressed in several works in the past decade; see e.g., AlexanderianPetraStadlerEtAl14 ; AlexanderianSaibaba18 ; HermanAlexanderianSaibaba20 . Goal-oriented approaches for OED in inverse problems governed by differential equations have appeared in HerzogRiedelUcinski18 ; Li19 ; ButlerJakemanWildey20 . The article HerzogRiedelUcinski18 considers nonlinear problems with nonlinear goal operators. In that article, a goal-oriented OED criterion is obtained using linearization of the goal operator and an approximate (linearization-based) covariance matrix for the inversion parameter. The thesis Li19 considers linear inverse problems with Gaussian prior and noise models, where the goal operator itself is a linear transformation of the inversion parameters. A major focus of that thesis is the study of methods for the combinatorial optimization problem corresponding to optimal sensor placement. The work ButlerJakemanWildey20 considers a stochastic inverse problem formulation, known as data-consistent framework ButlerJakemanWildey18 . This approach, while related, is different from traditional Bayesian inversion. Goal-oriented OED for infinite-dimensional linear inverse problems was studied in AttiaAlexanderianSaibaba18 ; WuChenGhattas23a . These articles consider goal-oriented OED for the case of linear parameter-to-goal mappings. For the specific class of problems considered in the present work, a traditional approach is to consider a linearization of the goal-functional \mathcal{Z} around a nominal parameter \bar{m}. Considering the posterior variance of this linearized functional leads to a specific form of the well-known c-optimality criterion ChalonerVerdinelli95 . However, a linear approximation does not always provide sufficient accuracy in characterizing the uncertainty in the goal-functional. In such cases, a more accurate approximation to \mathcal{Z} is desirable. Our approach and contributions. We consider a quadratic approximation of the goal-functional. Thus, \mathcal{Z} is approximated by \mathcal{Z}(m)\approx\mathcal{Z}_{\text{quad}}(m)\vcentcolon=\mathcal{Z}(\bar{% m})+\left\langle\nabla\mathcal{Z}(\bar{m}),m-\bar{m}\right\rangle+\frac{1}{2}% \left\langle\nabla^{2}\mathcal{Z}(\bar{m})(m-\bar{m}),m-\bar{m}\right\rangle. (1.3) Following an A-optimal design approach, we consider the posterior variance of the quadratic approximation, \mathbb{V}_{\mu_{\text{post}}^{\boldsymbol{y}}}\{\mathcal{Z}_{\text{quad}}\}. We derive an analytic expression for this variance in the infinite-dimensional setting, in Section 3. Note, however, that this variance expression depends on data \boldsymbol{y}, which is not available a priori. To overcome this, we compute the expectation of this variance expression with respect to data. This results in a data-averaged design criterion, which we call the G_{q}-optimality criterion. Here, G indicates the goal-oriented nature of the criterion and q indicates the use of a quadratic approximation. The closed-form analytic expression for this criterion is derived in Theorem 3.2. Subsequently, in Section 4, we present three computational approaches for fast estimation of \Psi, relying on Monte Carlo trace estimators, low-rank spectral decompositions, or a low-rank singular value decomposition (SVD) of \mathcal{F}, respectively. Focusing on problems where the goal functional \mathcal{Z} is defined in terms of PDEs, our methods rely on adjoint-based expressions for the gradient and Hessian of \mathcal{Z}. We demonstrate effectiveness of the proposed goal-oriented approach in a series of computational experiments in Section 5.1 and Section 5.2. The example in Section 5.1 involves inversion of a volume source term in an elliptic PDE with the goal defined as a quadratic functional of the state variable. The example in Section 5.2 concerns a porous medium flow problem with a nonlinear goal functional. The key contributions of this article are as follows: \bullet derivation of a novel goal-oriented design criterion, the G_{q}-optimality criterion, based on a quadratic approximation of the goal-functional, in an infinite-dimensional Hilbert space setting (see Section 3); \bullet efficient computational methods for estimation of the G_{q}-optimality criterion (see Section 4); \bullet extensive computational experiments, demonstrating the importance of goal-oriented OED and effectiveness of the proposed approach (see Section 5)."
https://arxiv.org/html/2411.07460v1,Application of MUSIC algorithm in real-world microwave imaging of unknown anomalies from scattering matrix,"In this contribution, we consider MUltiple SIgnal Classification (MUSIC)-type algorithm for a non-iterative microwave imaging of small and arbitrary shaped extended anomalies located in a homogeneous media from scattering matrix whose elements are scattering parameters measured at dipole antennas. In order to explain the feasibility of MUSIC in microwave imaging, we investigate mathematical structure of MUSIC by establishing a relationship with an infinite series of Bessel function of integer order and antennas setting. This is based on the representation formula of scattering parameters in the presence of small anomalies and the application of Born approximation. Simulation results using real-data at f=925MHz of angular frequency are exhibited to show the feasibility of designed algorithm and to support investigated structure of imaging function.","Microwave imaging for identification of the shape or location of unknown anomaly from measured scattered fields or scattering parameters is an interesting and important inverse scattering problem. Throughout various researches, it turned out that this is inherently a non-linear and ill-posed problem so that it cannot be successfully resolved. In order to solve this interesting and important problem, various inversion algorithms have been investigated for example, Newton-type scheme [1] for brain imaging, conjugate gradient [2] and Gauss-Newton methods [3] for breast imaging, back-projection method [4] for heart imaging, Newton-Kantorovich method [5] for arm imaging, Levenberg-Marquadt method [6] for in-body imaging, least-mean square approach [7] for ground penetrating radar (GPR), linear inversion technique [8] for thermal therapy monitoring, focusing method [9] for damage detection of civil structures, and level-set method [10] for crack reconstruction. We also refer to [11, 12, 13, 14, 15, 16, 17, 18] for various application and inversion techniques in microwave imaging. As we seen, most of algorithms for solving inverse scattering problem related to microwave imaging are based on the Newton-type iteration scheme, i.e., obtaining the shape of the target (minimizer), which minimizes the discrete norm (usually, \ell^{2}-norm) between the measured scattered field or scattering parameter data in the presence of true and man-made targets such that \mbox{find }\mathbf{x}\mbox{ to minimize }\mathcal{E}(\mathbf{r}):=\frac{1}{2}% ||\mathbb{F}\mathbf{x}-\mathbb{M}||^{2}+\mbox{regularization}, where \mathbb{F} is the discretized linear forward operator, \mathbb{M} contains measurements data, and \mathbf{x} is a complex vector of pixels of the differential object. Iteration-based techniques have been applied to identify the shape, location, and topological properties of unknown target. Nevertheless, obtaining a good initial guess, estimating a priori information, selecting appropriate regularization term, and evaluating complex Fréchet derivative at each iteration steps must be considered beforehand. If one of these conditions is not fulfilled, one may encounter various problems such as the non-convergence, the occurrence of local minimizer, and the requirement of large computational costs due to the large number of iteration procedures. Due to this reason, application of Newton-type method is very hard to the real-world inverse problems. Correspondingly, for an alternative, various non-iterative inversion techniques have been developed and successfully applied to various inverse scattering problems for example a variational algorithm based on Fourier inversion [19, 20], direct sampling method [21, 22], orthogonality sampling method [23, 24], linear sampling method [25, 26], factorization method [27, 28], Kirchhoff and subspace migrations [29, 30], topological derivatives [31, 32], truncated singular value decomposition (TSVD) inversion scheme [33, 34]. Among them MUltiple SIgnal Classification (MUSIC) algorithms operated at single and multiple frequency have been successfully applied to the various inverse scattering problems for example, detection of inhomogeneities in half-space problem [35, 36], multi-layered medium [37], and inhomogeneous media [27], imaging of crack-like defects [38, 39], detecting internal corrosion [40], eddy-current nondestructive evaluation [41], electrical breast biopsy [42], imaging of extended targets [43], and microwave imaging [44]. We also refer to [45, 46] for theoretical and experimental validation of MUSIC in breast cancer detection in monostatic measurement configuration and [47, 48, 49, 50, 51, 52, 53, 54, 55, 56] for applications of MUSIC in various inverse scattering problems. Following the traditional results, most of non-iterative imaging algorithms were established under some strong assumptions that every elements of so-called multi-static response (MSR) or scattering matrices can be handled, the total number of dipole antennas (in general, transducers and receivers) are sufficiently large enough, and measurement data is not affected by the existence of neighboring antennas. Unfortunately, in many cases, manufacturing microwave systems that can measure scattered field data with the transducer and receiver at the same location (diagonal elements) is inconvenient. Hence, designing an alternative imaging algorithm without consideration of diagonal elements of MSR or scattering matrices is an important and interesting direction for real-world application. In this research, we apply MUSIC as a non-iterative microwave imaging algorithm when the diagonal elements of the scattering matrix are unknown. In order to show the feasibility of MUSIC in real-world microwave imaging, we carefully explore mathematical structure of imaging function by establishing a relationship with an infinite series of Bessel function of integer order, the total number and location of dipole antennas, and the applied frequency. This is based on the Born approximation in the existence of a small anomaly [48] and the physical factorization of the scattering matrix in the presence of an extended anomaly [43]. Based on the explored structure of the imaging function, we can discover various intrinsic properties and the fundamental limitations of MUSIC in real-world microwave imaging. In order to support our theoretical results, simulation results for small and various extended anomalies through the real-data generated by commercial tool and manufactured microwave machine are exhibited. This research is organized as follows: In Section 2, we briefly introduce the forward problem including scattering parameter and MUSIC algorithm without diagonal elements of scattering matrix. In Section 3, we analyze mathematical structure of imaging function by establishing infinite series of Bessel functions of integer order and discover intrinsic properties of MUSIC in real-world application. In Sections 4 and 5, experimental results respectively using synthetic and real data are exhibited to show the feasibility and limitation of the MUSIC. A conclusion including an outline of future work is given in Section 6. Finally, let us emphasize that obtained imaging results via MUSIC seems good but does not guarantee the complete shape of anomalies. Thus, one must apply iteration scheme to retrieve more accurate shape of anomaly (if there remains a discrepancy between the measurement and computed data) once an appropriate cost functional is chosen."
https://arxiv.org/html/2411.07450v1,"On properties and numerical computation of critical points of eigencurves of
bivariate matrix pencils","We investigate critical points of eigencurves of bivariate matrix pencils A+\lambda B+\mu C. Points (\lambda,\mu) for which \det(A+\lambda B+\mu C)=0 form algebraic curves in \mathbb{C}^{2} and we focus on points where \mu^{\prime}(\lambda)=0. Such points are referred to as zero-group-velocity (ZGV) points, following terminology from engineering applications. We provide a general theory for the ZGV points and show that they form a subset (with equality in the generic case) of the 2D points (\lambda_{0},\mu_{0}), where \lambda_{0} is a multiple eigenvalue of the pencil (A+\mu_{0}C)+\lambda B, or, equivalently, there exist nonzero x and y such that (A+\lambda_{0}B+\mu_{0}C)x=0, y^{H}(A+\lambda_{0}B+\mu_{0}C)=0, and y^{H}Bx=0.We introduce three numerical methods for computing 2D and ZGV points. The first method calculates all 2D (ZGV) points from the eigenvalues of a related singular two-parameter eigenvalue problem. The second method employs a projected regular two-parameter eigenvalue problem to compute either all eigenvalues or only a subset of eigenvalues close to a given target. The third approach is a locally convergent Gauss–Newton-type method that computes a single 2D point from an inital approximation, the later can be provided for all 2D points via the method of fixed relative distance by Jarlebring, Kvaal, and Michiels. In our numerical examples we use these methods to compute 2D-eigenvalues, solve double eigenvalue problems, determine ZGV points of a parameter-dependent quadratic eigenvalue problem, evaluate the distance to instability of a stable matrix, and find critical points of eigencurves of a two-parameter Sturm-Liouville problem.","We study parameter-dependent linear eigenvalue problems of the form (A+\lambda B+\mu C)x=0, (1) where A,B,C\in\mathbb{C}^{n\times n}, \lambda,\mu\in\mathbb{C} and x\in\mathbb{C}^{n} is nonzero. We consider \lambda as a parameter that the eigenvalue \mu and the eigenvector x depend on, and assume that the problem is biregular according to the following definition. Definition 1.1. A bivariate matrix pencil A+\lambda B+\mu C is biregular if for each (\lambda_{0},\mu_{0})\in\mathbb{C}^{2} the generalized eigenvalue problems (GEPs) \big{(}(A+\lambda_{0}B)+\mu C\big{)}x=0 (2) and \big{(}(A+\mu_{0}C)+\lambda B\big{)}x=0 (3) are both regular, i.e., \det(A+\lambda_{0}B+\mu C)\not\equiv 0 and \det(A+\lambda B+\mu_{0}C)\not\equiv 0. It is easy to see that a bivariate pencil A+\lambda B+\mu C is biregular if and only if the characteristic polynomial p(\lambda,\mu):=\det(A+\lambda B+\mu C) does not have a divisor of the form \lambda-\lambda_{0} or \mu-\mu_{0}. A sufficient condition for biregularity is that matrices B and C are both nonsingular. If A+\lambda B+\mu C is biregular then for each \lambda_{0}\in\mathbb{C} there are n eigenvalues \mu\in\mathbb{C}\cup\{\infty\} of (2). As a result, the set of points (\lambda,\mu)\in\mathbb{C}^{2} such that \det(A+\lambda B+\mu C)=0 is composed of curves in \mathbb{C}^{2} that are called eigencurves. If (\lambda_{0},\mu_{0})\in\mathbb{C}^{2} is a point on an eigencurve such that \mu_{0} is a simple eigenvalue of the GEP (2), then we can locally parameterize \mu as an analytic function of \lambda such that \mu(\lambda_{0})=\mu_{0} and define \mu^{\prime}(\lambda_{0}). Theorem 1.2. Let \mu_{0}\in\mathbb{C} be a simple eigenvalue of a regular GEP ((A+\lambda_{0}B)+\mu C)x=0. Then there exist analytic functions \mu(\lambda) and x(\lambda)\neq 0 in a neighbourhood of \lambda_{0} such that \big{(}A+\lambda B+\mu(\lambda)C\big{)}x(\lambda)=0 and \mu(\lambda_{0})=\mu_{0}. Proof. We know that the theorem holds for the standard eigenvalue problem where C=I, see, e.g., [10, Thm. 2]. Since (A+\lambda_{0}B)+\mu C is a regular GEP, it follows that the matrix M(\alpha):=\alpha(A+\lambda_{0}B)+C is nonsingular for almost all \alpha\in\mathbb{C}. It is easy to see that \big{(}(A+\lambda_{0}B)+\mu C\big{)}x=0 if and only if ((A+\lambda_{0}B)+\eta M(\alpha))x=0 for \eta=\mu/(1-\alpha\mu). As the theorem holds for the standard eigenvalue problem (M(\alpha)^{-1}(A+\lambda_{0}B)+\eta I)x=0, it thus holds for the GEP (2) as well. ∎ Definition 1.3. Let (\lambda_{0},\mu_{0})\in\mathbb{C}^{2} be such that \mu_{0} is a simple eigenvalue of a regular GEP (A+\lambda_{0}B)+\mu C)x=0 and let \mu(\lambda) be an analytic function in a neighbourhood of \lambda_{0} such that \det(A+\lambda B+\mu(\lambda)C)=0 and \mu(\lambda_{0})=\mu_{0}. We say that (\lambda_{0},\mu_{0}) is a ZGV (zero-ground-velocity) point of the bivariate pencil A+\lambda B+\mu C if \mu^{\prime}(\lambda_{0})=0. The expression ZGV point comes from the study of waves in optics and acoustics, where the angular frequency \omega of a guided wave is related to the wavenumber k via a dispersion relation \omega(k). Of special interest are zero-group-velocity (ZGV) points (k_{*},\omega_{*}) on the dispersion curves, where the group velocity c_{g}=\frac{\partial\omega}{\partial k} of a wave vanishes whereas the wave number k_{*} remains finite, see, e.g., [27]. The rest of this work is structured as follows. In Section 2 we discuss the properties of ZGV points, introduce 2D points, and show that each ZGV point (\lambda_{0},\mu_{0}) is also a 2D point with \lambda_{0} being a multiple eigenvalue of the GEP (3). In Section 3 we review two-parameter eigenvalue problems (2EPs) and singular GEPs, both will be essential tools in later sections. Section 4 contains our main result that 2D points are eigenvalues of a singular 2EP, which we exploit to derive a numerical method for computing all 2D and ZGV points. In Subsection 4.1 we show that the related singular 2EP can be solved more efficiently by projecting it into a nonsingular 2EP and applying the numerical method from [13]. In Section 5 we provide a Gauss–Newton-type method that computes a 2D point from a good initial approximation and prove its quadratic convergence in the generic case. Approximate solutions for 2D points can be obtained by the method of fixed relative distance [16] that is presented in Section 6. Finally, Section 7 explores several applications of 2D and ZGV points, illustrated with numerical examples."
https://arxiv.org/html/2411.07422v2,Impact of Numerical Fluxes on High Order Semidiscrete WENO–DeC Finite Volume Schemes,"The numerical flux determines the performance of numerical methods for solving hyperbolic partial differential equations (PDEs). In this work, we compare a selection of 8 numerical fluxes in the framework of nonlinear semidiscrete finite volume (FV) schemes, based on Weighted Essentially Non–Oscillatory (WENO) spatial reconstruction and Deferred Correction (DeC) time discretization. The methodology is implemented and systematically assessed for order of accuracy in space and time up to seven. The numerical fluxes selected in the present study represent the two existing classes of fluxes, namely centred and upwind. Centred fluxes do not explicitly use wave propagation information, while, upwind fluxes do so from the solution of the Riemann problem via a wave model containing A waves. Upwind fluxes include two subclasses: complete and incomplete fluxes. For complete upwind fluxes, A=E, where E is the number of characteristic fields in the exact problem. For incomplete upwind ones, A<E. Our study is conducted for the one– and two–dimensional Euler equations, for which we consider the following numerical fluxes: Lax–Friedrichs (LxF), First–Order Centred (FORCE), Rusanov (Rus), Harten–Lax–van Leer (HLL), Central–Upwind (CU), Low–Dissipation Central–Upwind (LDCU), HLLC, and the flux computed through the exact Riemann solver (Ex.RS).We find that the numerical flux has an effect on the performance of the methods. The magnitude of the effect depends on the type of numerical flux and on the order of accuracy of the scheme. It also depends on the type of problem; that is, whether the solution is smooth or discontinuous, whether discontinuities are linear or nonlinear, whether linear discontinuities are fast– or slowly–moving, and whether the solution is evolved for short or long time. For the special case of smooth solutions, the expected convergence rates are attained for all fluxes and all orders. However, errors are still larger for the simpler fluxes, though differences diminish as the order of accuracy increases. For all selected cases involving discontinuities, differences among fluxes arise for all orders of accuracy considered. Moreover, there are flow situations for which the differences are huge, independently of the order of accuracy of the scheme. The best fluxes are the complete upwind ones. The difference between the best centred flux, FORCE, and incomplete upwind ones is not dramatic, which constitutes and advantage for good centred methods due to their simplicity and generality.","The vast majority of numerical schemes for solving hyperbolic partial differential equations (PDEs) is based on a discrete representation of their underlying principle: the rate of change in time of some quantities inside a given spatial region is given by what crosses the surface of the same region, expressed by the (normal) flux function, plus what is generated/dissipated within the region, expressed by the source function (if present). Therefore, a numerical method will require corresponding expressions for the numerical flux and the numerical source. Historically, Godunov [Godunov] is credited for having proposed a numerical flux as an integral average of the physical flux evaluated at the solution of the Riemann problem at the interface between the elements of a tessellation of the spatial domain. The resulting Godunov upwind method is a conservative generalization of the CIR scheme, first presented in [courant1952solution] by Courant, Isaacson and Rees. Even after six decades, the design of numerical fluxes remains a fundamental task in the construction of finite volume (FV) [hirsch2007numerical, ToroBook, leveque2002finite, godlewski2021numerical, toro2024computational], finite difference [leveque2007finite] and Discontinuous Galerkin (DG) finite element methods [reed1973triangular, cockburn2000development, cockburn2001runge]. This is so, both in the frameworks of semidiscrete and fully–discrete schemes. Apart from the basic properties of consistency and Lipschitz–continuity, the scheme designer aims for generous stability properties, monotonicity (for the scalar case), minimal numerical diffusion and efficiency [ToroBook]. It is known that for the scalar case the Godunov upwind method is the scheme with the smallest local truncation error, within the class of monotone schemes. The largely pending challenge is to design numerical fluxes with desirable properties for solving nonlinear systems in multiple space dimensions. Over the last few decades, many numerical fluxes with different properties have been put forward, which then prompts a relevant and natural question: among the available fluxes in the current literature, which ones achieve the optimal balance between accuracy and computational cost? At the first order level the current literature provides useful, even if not exhaustive, information on the performance of various numerical fluxes [hirsch2007numerical, ToroBook, leveque2002finite, godlewski2021numerical, toro2024computational]. Much less is known in the setting of higher order (in space and time) numerical methods. A reasonable expectation is that the adoption of a high order discretization could compensate for the deficiency of less accurate numerical fluxes. Despite being the numerical flux a crucial element, the design of numerical schemes for hyperbolic PDEs requires the definition of other important components which depend on the specific discretization framework adopted. Broadly speaking, there are two major discretization frameworks, namely the semidiscrete (or method of lines) and the fully–discrete approaches. In the former setting, the discretization in space is separated from the one in time. Given a discretization in space, the problem remains continuous in time as a system of ordinary differential equations (ODEs). In principle, any ODEs solver can then be deployed to complete the scheme. Instead, in the fully–discrete setting, the discretizations in space and time are intertwined, fully coupled and simultaneously designed in a single step. In the high order semidiscrete FV framework, one needs a suitable spatial reconstruction and a suitable time–stepping strategy. The spatial reconstruction must be nonlinear, so as to prevent or reduce spurious oscillations in the vicinity of discontinuities or large gradients (even in smooth problems). The need for the nonlinear character of the spatial reconstruction emerges from Godunov’s theorem [Godunov] as a necessary condition for a monotone scheme; see [ToroBook] for statement and proof. In this paper, we study the performance of 8 existing numerical fluxes in the setting of an arbitrary high order semidiscrete FV framework, comprising Weighted Essentially Non–Oscillatory (WENO) [liu1994weighted, shu1998essentially, shu1989efficient] spatial reconstruction and Deferred Correction (DeC) [micalizzi2023new, ciallella2022arbitrary, Decremi, minion2003semi, Decoriginal] time discretization. More in detail, we carry out a systematic comparison of the following numerical fluxes: Lax–Friedrichs (LxF) [lax1954weak], First–Order Centred (FORCE) [Toro1996, toro2000centred, chen2003centred], Rusanov [Rusanov1961] (Rus), Harten–Lax–van Leer (HLL) [harten1983upstream], Central–Upwind (CU) [kurganov2001semidiscrete, kurganov2000new], Low–Dissipation Central–Upwind (LDCU) [kurganov2023new], HLLC [toro1992restoration, toro1994restoration], the Godunov flux from the exact Riemann solver [Godunov] (Ex.RS). Most of these are thoroughly described in [ToroBook]. Both the spatial and time discretizations adopted here are well established. As a matter of fact, the WENO–DeC approach has already been investigated up to order 5 in space and time; the performance of such a framework has been shown to be very satisfactory in tackling challenging and realistic problems [ciallella2022arbitrary, ciallella2023arbitrary, ciallella2024high]. In the present work, we investigate the performance of WENO–DeC schemes up to order of accuracy 7 in space and time. Moreover, in addition to the very high order extension, we compare the performance of many numerical fluxes available in the literature within such a framework. The assessment of the resulting methods is through the time–dependent, one– and two–dimensional compressible Euler equations. A judicious choice of suitable problems is performed with the aim of understanding the strengths and limitations of the numerical fluxes under investigation in combination with the aforementioned semidiscrete approach. Key features of the chosen problems include: robustness in the presence of very strong shocks; accuracy in resolving waves associated with intermediate, linear, characteristic fields; and long–time evolution. From the systematic assessment of the methods following the above criteria, we anticipate the following conclusions: • There are several physical situations in which significant differences are seen in the performance of the numerical fluxes under investigation, with HLLC and Ex.RS outperforming by far the other competitors. • There are tests for which the choice of the numerical flux has less impact on the methods performance. But even in such cases, it can consistently be observed that the performance of LxF, FORCE and Rus is inferior to that of the remaining numerical fluxes, which give similar results amongst themselves. In particular, the performance of LxF is always the worst, while the relative performance of FORCE and Rus depends on the specific problem. • For all investigated numerical fluxes, there is an advantage in increasing the space–time order of accuracy of the discretization. The advantage is much more evident in the more diffusive numerical fluxes, that is the centred fluxes LxF and FORCE, and the incomplete upwind numerical flux Rus. The benefits of the enhanced higher order accuracy are less evident as the sophistication of the numerical flux increases. As a matter of fact, to a certain extent and depending on the test problem, increasing the space–time order of accuracy compensates for the deficiencies of a more diffusive numerical flux. However, enhanced higher order accuracy per se, within the range of considered orders, is not sufficient to attain the accuracy delivered by sophisticated upwind fluxes derived from complete Riemann solvers, namely HLLC and Ex.RS. Conversely, depending on the physical situation, a suitable numerical flux, even for a first order method, may be equivalent to implementing higher order space–time discretizations. This is typically the case for slowly–moving linear waves associated with intermediate characteristic fields and for very long–time evolutions of traveling waves. • A surprising outcome has emerged from the implementation of the centred (essentially one-dimensional) numerical fluxes LxF and FORCE in a two–dimensional setting via a simultaneous updating formula. A von Neumann stability analysis of these numerical fluxes in a first order setting shows them to be linearly unstable in two and three space dimensions [toro2000centred, ToroBook]. Curiously, we found that increasing the order of accuracy has a stabilizing effect, as shown in some of our numerical experiments, though an explanation remains illusive. There are other works in the literature concerned with the influence of numerical fluxes on high order methods. Investigations in the semidiscrete framework include [leidi2024performance, qiu2007numerical, qiu2008development, qiu2006numerical, hongxia2020numerical, san2015evaluation]. In [leidi2024performance], the performances of Rus, HLL and a low–dissipation version of HLLC (referred to as “LHCLL” in the reference) are compared for low Mach number flows, in a FV setting with various spatial reconstructions up to order 7. DG schemes with 8 numerical fluxes and spatial accuracy up to order 3 are studied in [qiu2007numerical, qiu2008development], and with 9 numerical fluxes and spatial accuracy up to order 4 in [qiu2006numerical]. In [hongxia2020numerical], in a FV setting with fifth order HWENO space reconstruction the performance of 8 numerical fluxes is assessed. In [san2015evaluation], results from 6 numerical fluxes in a FV framework with WENO space reconstruction up to order 7 are compared on the Kelvin–Helmholtz instability problem. In all previously alluded comparative analyses, a semidiscrete approach with third order time integration was adopted. Two further related works [titarev2005weno, toro2005tvd] are worth mentioning, in which the basic monotone flux utilized in the high order methods is replaced by a total variation diminishing (TVD) flux: in [titarev2005weno] the approach is implemented in a semidiscrete framework, while in [toro2005tvd] this is done in a fully–discrete framework. In the context of the present work concerned with semidiscrete methods, it is important to remark that a broadly adopted practice consists in employing high order space discretizations along with lower order time discretizations. See for example [evstigneev2016construction, Evstigneev2016OnTC, gerolymos2009very, balsara2000monotonicity, shi2003resolution, hermes2012linear, gao2020seventh], in which very high order spatial reconstructions are considered but the order of accuracy of the selected time discretizations never exceeds 4. Many published works consider very high order space discretizations in combination with strong stability preserving (SSP) [shu1988total, shu1988efficient] or linearly strong stability preserving (\ell\text{SSP}) [gottlieb2001strong] Runge–Kutta (RK) in time. The accuracy barrier for such ODEs solver, if non-negative RK coefficients are to be preserved [shu1988efficient], is order 4 [ruuth2002two] on nonlinear problems. Actually, \ell\text{SSP} RK methods can be arbitrarily high order accurate but only on linear problems. With the main goal of preventing loss in accuracy due to the mismatch between temporal and spatial order, in some works, a well–tuned reduction of the time step is performed. The main benefit of this strategy is to formally make the accuracy of the scheme equal to the one of the space discretization, however, the severely reduced time step causes excessive numerical diffusion and a huge increase in computational cost, making the scheme unsuitable for practical applications. In many other works, no adaptation of the time step is considered and the formal order of the scheme is limited by the one of the lower order time discretization. Such a practice is based on the questionable assumption that the spatial error always dominates the time error. Preliminary investigations performed by the authors seem to contradict this expectation, though a thorough study of this issue is left for future works. In fact, as already stated, the WENO–DeC framework adopted in this paper allows for the construction of arbitrarily high order schemes, which are distinguished by the fact that the temporal order of accuracy matches the one of the WENO spatial reconstruction, that is to say 2r-1, where r-1 is the degree of component ENO polynomials making up the WENO polynomial. Related works on arbitrarily high order frameworks are available in the literature; see for example [veiga2024improving, velasco2023spectral, abgrall2023extensions, Decremi, micalizzi2024novel, abgrall2019high, bacigaluppi2023posteriori, abgrall2020high]. Some of them [veiga2024improving, velasco2023spectral, abgrall2023extensions] are obtained through a simple method of lines approach, adopting a spatial discretization of the PDE and solving in time the resulting ODEs system with a sufficiently accurate time integration method. In the other mentioned references, more involved space–time discretizations are considered, such as the continuous Galerkin–DeC framework described in [Decremi, micalizzi2024novel, abgrall2019high, bacigaluppi2023posteriori, abgrall2020high]. An alternative approach to construct schemes of arbitrary space–time accuracy is the fully–discrete ADER methodology, first communicated in the early works [toro2001towards, grptoro, titarev2002ader, schwartzkopff2002ader]. In the fully–discrete ADER approach, the discretizations in space and time are inextricably coupled via the solution of the Generalized Riemann problem [grptoro], GRP_{m}, at cell interfaces, in which the initial conditions consist of nonlinear reconstructed polynomials of arbitrary degree m, leading to a method of order of accuracy equal to m+1 in space and time. The ADER method is a one–step scheme in which the nonlinear reconstruction is performed only once per time step. The ADER methodology has been developed in both the FV and DG frameworks. Further developments of the ADER methodology can be found for example in [dumbser2005ader, dumbser2006arbitrary, dumbser2006building, dumbser2008unified, ADERNSE, dumbser2009very, boscheri2019high, popov2024space, toro2024ader, micalizzi2023efficient]. Elementary introductions to ADER can be found in [ToroBook, Chapters 19 and 20] and in [toro2024computational, Chapter 14]. The rest of the paper is structured as follows. In Section 2 we recall the governing equations and the FV method in the semidiscrete setting. In Section 3 we describe the space discretization, namely, the WENO reconstruction in Section 3.1 and the numerical fluxes under investigation in Section 3.2, while, an outline of the DeC time discretization is given in Section 4. Numerical results are reported in Section 5. Conclusions and future perspectives are found in Section 6."
https://arxiv.org/html/2411.07365v1,Parallelisation of partial differential equations via representation theory,"A little utilised but fundamental fact is that if one discretises a partial differential equation using a symmetry-adapted basis corresponding to so-called irreducible representations, the basic building blocks in representational theory, then the resulting linear system can be completely decoupled into smaller independent linear systems. That is, representation theory can be used to trivially parallelise the numerical solution of partial differential equations. This beautiful theory is introduced via a crash course in representation theory aimed at its practical utilisation, its connection with decomposing expansions in polynomials into different symmetry classes, and give examples of solving Schrödinger’s equation on simple symmetric geometries like squares and cubes where there is as much as four-fold increase in the number of independent linear systems, each of a significantly smaller dimension than results from standard bases.","1 intro Introduction. A function in a symmetric geometry like a square or cube can be decomposed into different symmetry classes by understanding the ways in which symmetry groups may present themselves as linear operators, which is precisely the topic of representation theory. A simple example of this fact is that univariate functions can be decomposed into even and odd parts, which falls out naturally from a Taylor series expansion provided the expansion converges: {\sum_{k=0}^{\infty}f_{k}x^{k}}\qquad=\qquad\underbrace{\sum_{k=0}^{\infty}f_{% 2k}x^{2k}}_{\hbox{Even}}\qquad+\qquad\underbrace{\sum_{k=0}^{\infty}f_{2k+1}x^% {2k+1}}_{\hbox{Odd}} An even-odd decomposition can be viewed as a specific example of a symmetric decomposition corresponding to the symmetry action of reflection x\mapsto-x. More precisely, we view reflection as the cyclic group of order 2, which for concreteness we write as a multiplicative group {\mathbb{Z}}_{2}:=\left\{{-1,1}\right\}. The basis of monomials x^{k} are symmetry-adapted with respect to {\mathbb{Z}}_{2}: they translate applying the symmetry action of reflection to multiplication by a representation, the basic object of study in the field of representation theory which correspond to maps from the group to linear operators. Even and odd monomials translate reflection into multiplication by the trivial representation \rho_{\rm t}(g)=1 and sign representation \rho_{\rm s}(g)=g, respectively: for all g\in{\mathbb{Z}}_{2} we have (gx)^{2k}=\rho_{\rm t}(g)x^{2k},\qquad(gx)^{2k+1}=\rho_{\rm s}(g)x^{2k+1}. Splitting a function into even-odd parts can be reinterpreted as splitting the expansion according to bases associated with these two irreducible representations of {\mathbb{Z}}_{2}. Even-odd decompositions generalise naturally to squares and cubes. On a square, this can be viewed is a consequence of the basis x^{k}y^{j} respecting the symmetry action of reflection in each coordinate, i.e., reflection corresponds to representations of the group {\mathbb{Z}}_{2}^{2}={\mathbb{Z}}_{2}\times{\mathbb{Z}}_{2}:=\left\{{\begin{% pmatrix}1\\ &1\end{pmatrix},\begin{pmatrix}1\\ &-1\end{pmatrix},\begin{pmatrix}-1\\ &1\end{pmatrix},\begin{pmatrix}-1\\ &-1\end{pmatrix}}\right\}, which has four irreducible representations: \displaystyle\rho_{\rm t}(g)\; \displaystyle=1,\quad\rho_{\rm h}\begin{pmatrix}g_{1}\\ &g_{2}\end{pmatrix}=g_{1},\quad\rho_{\rm v}\begin{pmatrix}g_{1}\\ &g_{2}\end{pmatrix}=g_{2},\quad\rho_{\rm s}(g)=\det g. We can therefore decompose a function into four different types of symmetry classes: trivial (even-even), horizontal (odd-even), vertical (even-odd), and sign (odd-odd) representations, as depicted in Figure 1. Figure 1: A function on a square can be decomposed into four different symmetry classes according to their even-odd symmetry, which can be viewed in terms of the irreducible representations of {\mathbb{Z}}_{2}^{2}. On the left we plot an arbitrary function f(x,y)=\cos(6y(y-1)(x-1/5)^{2}+\sin(4y-1/10){\rm e}^{x}) and on the right we plot the four components of its symmetric decomposition corresponding to {\mathbb{Z}}_{2}^{2}. Applying a reflection across the x- or y-axis leaves each term either invariant or swaps the sign. Certain PDEs can be parallelised across four independent solves according to this decomposition. Symmetric decompositions generalise to other symmetry groups such as the dihedral group D_{4} associated with a square or the octohedral group O_{h} associated with a cube. For example, if we utilise the full D_{4} symmetry group of a square instead of only reflection along the x- and y-axes we can decompose a function into six components, see Figure 2. Such a decomposition falls out of the construction of symmetry-adapted bases where applying the symmetry action is equivalent to multiplication by a representation. The irreducible representations enumerate the basic ways symmetry may present itself and hence we wish to find bases which correspond to irreducible representations. A major difference is that in general the irreducible representations are matrix-valued, which makes the construction more involved than the scalar representations associated with {\mathbb{Z}}_{2} and {\mathbb{Z}}_{2}^{2}. Therefore, we need to consider the symmetry group acting on a basis with more than one element in tandem. Moreover, while in the case of the square we can write a basis corresponding to irreducible representations explicitly, in the case of the cube we can only compute such a basis numerically. But the approach to decomposition does generalise and we outline a systematic procedure to do so. Figure 2: A symmetric decomposition associated with the full symmetry group of the square (the dihedral group D_{4}) into six symmetry classes for the same function from Figure 1. The first two columns correspond to scalar irreducible representations where the functions are either invariant or flip sign whenever we apply a symmetry action. The last column corresponds to a 2-dimensional irreducible representation, thus whilst they remain in an invariant subspace the symmetry is not as visually obvious, but the numerical solution of certain classes of PDEs still decouples across these terms. Our motivation for utilising symmetry-adapted bases is that they naturally decouple the discretisation of certain important classes of partial differential equations (PDEs) into smaller independent systems, the number equal to the total of the dimension of all irreducible representations, which can in principle be solved in parallel. For simplicity we focus on the Schrödinger equation \Delta u+a({\mbox{\boldmath$x$\unboldmath}})u=\lambda u where a is invariant under the corresponding symmetry action with natural (Neumann) boundary conditions, that is, where the normal derivative of u vanishes at the boundary. For example, on a square where the potential is invariant under D_{4} we have four scalar irreducible representations and one 2-dimensional representation and a discretisation built using an associated symmetry-adapted basis will decouple the Schrödinger equation into 6 independent eigenvalue problems which can be solved in parallel. On the cube where the potential is invariant under O_{h} this increases substantially to 20 independent eigenvalue problems. Note this feature is not limited to scalar-valued PDEs, and extends naturally to vector-valued equations as discussed briefly in the conclusion. In the discussion we focus on monomial bases for pedagogical reasons but essentially everything we discuss is applicable to multivariate orthogonal polynomials, which have a natural link to symmetry groups in that the degree-n orthogonal polynomials are closed under symmetry actions [13] and hence generate representations. It should be emphasised for the reader inexperienced in numerical methods: do not use monomials in numerical methods! They are prone to ill-conditioning and orthogonal polynomials have many beautiful properties such as sparse discretisations (as discussed in detail in our recent review and references therein [27]). In fact many of the figures in this paper are generated using multivariate orthogonal polynomials for numerical stability. The paper is structured as follows: Section LABEL:Section:background: We discuss prior work in incorporating discrete symmetries into the discretisation of partial differential equations via symmetry-adapted bases and the practical computation of decompositions of representations. Section LABEL:Section:crash: We give a quick overview of the essentials of representation theory with an emphasis on practical applied mathematics: we state the standard results in terms of concrete matrices and vectors as opposed to abstract vector spaces. This includes Schur’s lemma (Lemma 3.8) which is a fundamental result in representation theory that states, essentially, that any linear map that intertwines irreducible representations is by necessity either zero or a a scalar multiple of the identity. It also includes a discussion on how any representation can be decomposed into irreducible representations via an orthogonal matrix that can be computed numerically. Section LABEL:Section:symdecompos: Symmetry-adapted bases are constructed on a square where this is accomplished in closed form and on a cube where we utilise the numerical algorithm for decomposing representations. Section LABEL:Section:PDEs: Symmetry-adapted bases can decouple PDEs into distinct systems, whose number is the sum of the dimensions of the irreducible representations. The workhorse for proving this result is Schur’s lemma: by showing that portions of discretisations of PDEs are either zero or a scalar multiple of the identity implies that they are sparse. We demonstrate examples on the square using the dihedral group D_{4} (which decouples across 6 systems) and a cube using the octohedral group O_{h} (which decouples across 20 systems). We conclude with an example of 3 one-dimensional particles (so also living in a cube) using only permutation and negation symmetry (which decouples across 8 systems, a four-fold increase over existing bases). Section LABEL:Section:conc: We conclude by discussing extensions to vector-valued PDEs and other groups, including the high-dimensional setting of the Schrödinger equation with multiple particles."
https://arxiv.org/html/2411.07900v2,Hybrid finite element implementation of two-potential constitutive modeling of dielectric elastomers,"Dielectric elastomers are increasingly studied for their potential in soft robotics, actuators, and haptic devices. Under time-dependent loading, they dissipate energy via viscous deformation and friction in electric polarization. However, most constitutive models and finite element (FE) implementations consider only mechanical dissipation because mechanical relaxation times are much larger than electric ones. Accounting for electric dissipation is crucial when dealing with alternating electric fields. Ghosh and Lopez-Pamies [17] proposed a fully coupled three-dimensional constitutive model for isotropic, incompressible dielectric elastomers. We critically investigate their numerical scheme for solving the initial boundary value problem (IBVP) describing the time-dependent behavior. We find that their fifth-order explicit Runge-Kutta time discretization may require excessively small or unphysical time steps for realistic simulations due to the stark contrast in mechanical and electric relaxation times. To address this, we present a stable implicit time-integration algorithm that overcomes these constraints. We implement this algorithm with a conforming FE discretization to solve the IBVP and present the mixed-FE formulation implemented in FEniCSx. We demonstrate that the scheme is robust, accurate, and capable of handling finite deformations, incompressibility, and general time-dependent loading. Finally, we validate our code against experimental data for VHB 4910 under complex time-dependent electromechanical loading, as studied by Hossain et al. [25].","Dielectric elastomers are soft materials that deform significantly when subjected to electric fields. These materials were first reported by Pelrine et al. [39] and have since garnered interest as electromechanical transducers for a wide variety of applications, such as robotics, biomedical engineering, and energy harvesting (check, for example, [26, 2, 44, 5, 10, 28, 7, 38, 56, 29]). To harness their full potential in engineering applications, robust constitutive models and computational schemes that can predict their complex electromechanical behavior under real-life loadings and stimuli are needed. Thus, work on electromechanical constitutive modeling has motivated research for decades [53, 34, 35, 16, 12]. However, most of these models are limited to elastic dielectrics, which are dielectric elastomers that deform and polarize without dissipating energy. This idealization is not true in general for real-life loadings and applications, as dielectric elastomers are inherently dissipative solids that exhibit energy loss through viscous deformation and friction in their electric polarization process. Recognizing the dissipative nature of dielectric elastomers, many models focusing on the mechanical dissipation have also been proposed, but the majority of such models account only for mechanical dissipation and save for a few works most assume ideal dielectric behavior [23, 55, 58]. This is because the electric relaxation time is much smaller than the mechanical relaxation time for most dielectric elastomers. However, accounting for electric dissipation in addition to mechanical dissipation becomes critical for several loading conditions such as in the presence of alternating electric potentials (check, for example, dielectric spectroscopy experiments on prestretched VHB 4910 specimen in [40]). In fact, the measured permittivity in such experiments becomes stretch dependent and was coined as apparent permittivity in [17]. Furthermore, an important use of dielectric elastomers is in composite material discovery in which dielectric elastomers, when filled with solid or liquid inclusions, lead to remarkable macroscopic or effective material properties [57, 18]. The overall dissipative nature of these composites could be far more complex or pronounced than the electric and mechanical dissipation of the constituent elastomer [19, 20, 47], thus highlighting the need for a constitutive model for the coupled electric and mechanical dissipative behavior of dielectric elastomers. A first attempt to propose such a comprehensive constitutive model was made in [17]. The proposed model works for a prominent class of isotropic and incompressible dielectric elastomers that exhibit : a) non-Gaussian elasticity, b) deformation enhanced shear thinning viscosity, c) electrostriction and d) time- and deformation-dependent polarization. At this point, some work that has gone into addressing some of the challenges in FE implementation of constitutive models of dielectric elastomers needs to be acknowledged. These implementations have addressed challenges such as large deformation, electromechanical coupling, and the incorporation of viscoelastic effects ([41], [22], see also [48] for an open-source implementation in FEniCSx) as well as efficient implementations of coupled-physics such as thermo-electro-viscoelasticity ([36]), magnetorheological elastomers ([15, 42]) and notably an efficient FE framework for coupled electromechanics [27]. To this end, the purpose of this paper is five-fold. Firstly, the ease of a robust FE implementation of the constitutive model proposed in [17] is demonstrated. Second, various time-integration schemes are examined along with a re-examination of the time-integration scheme proposed in [17]. Third, the FE framework is employed to describe the electromechanical behavior of the acrylate elastomer VHB 4910 and the results are compared with the experiments of Hossain et al. [25]. Fourth, it is demonstrated that such a FE framework is needed for finding material properties of the proposed model as boundary effects become important in such material characterization experiments invalidating the often made assumptions of biaxility/uniaxiality. This is because the sample geometry and the boundary conditions of the electro-mechanical experiments is such that the resulting electric and deformation fields are no longer homogeneous. This is also acknowledged in [37]. Finally, in a future work the FE solver will be used as a plug-in tool, within a larger framework that could involve using Machine Learning (ML), for data generation for automated constitutive model identification such as in [33] or coupling with Deep Learning tools such as Neural ODEs [50] for data-driven constitutive modeling modeling. The organization of the paper is as follows. In Section 2, the governing equations that take the form of an initial boundary value problem (IBVP) that explain the time-dependent dielectric response of a dielectric elastomer are outlined. The section begins with discussions on kinematics and the balance equations. Following this, the specific model for the prominent class of isotropic and incompressible dielectric elastomers as proposed in [17] is explained. In order to deal with nearly or fully incompressible elastomers, it is more convenient to reformulate the governing equations into a hybrid set of governing equations in which a pressure field (in addition to the deformation field and the electric potential) is an additional unknown field. This hybrid form of the governing equations are described next, followed by the weak form of the equations. Finally, Section 2 concludes with the time- and space- discretized forms of the IBVP that is to be implemented in a FE framework. Section 3 is devoted to the study of explicit and implicit time integration schemes for the solution of the IBVP in the 1D setting. Specifically, the overall accuracy, stable-time-increment, and total-time-to-solution (TTS) for each of the schemes are studied. We note that implicit schemes are much better at handling the disparate time-scales exhibited by the electric and mechanical dissipation processes in elastomers. Hence these are better suited for solving the time-dependent response of dielectric elastomers exhibiting both mechanical and electric dissipation. Section 4 is devoted to the hybrid FE element formulation developed in Section 2 along with an implicit time-stepping algorithm based on Backward Euler discretization of time to solve an initial boundary value problem that mimics the corresponding experiments in [25]. Finally, Section 5 summarizes the findings, and presents concluding remarks along with possible extensions of the work in the future."
https://arxiv.org/html/2411.07661v1,A preconditioned second-order convex splitting algorithm with a difference of varying convex functions and line search,"This paper introduces a preconditioned convex splitting algorithm enhanced with line search techniques for nonconvex optimization problems. The algorithm utilizes second-order backward differentiation formulas (BDF) for the implicit and linear components and the Adams-Bashforth scheme for the nonlinear and explicit parts of the gradient flow in variational functions. The proposed algorithm, resembling a generalized difference-of-convex-function approach, involves a changing set of convex functions in each iteration. It integrates the Armijo line search strategy to improve performance. The study also discusses classical preconditioners such as symmetric Gauss-Seidel, Jacobi, and Richardson within this context. The global convergence of the algorithm is established through the Kurdyka-Łojasiewicz properties, ensuring convergence within a finite number of preconditioned iterations. Numerical experiments demonstrate the superiority of the proposed second-order convex splitting with line search over conventional difference-of-convex-function algorithms.","Our focus is the following nonconvex optimization problem \min_{u\in X}H(u)+F(u) (1.1) where H(u) is a convex function, \nabla F(u) is a Lipschitz continuous function with constant L, and X is a finite-dimensional Hilbert space. Nonconvex optimizations have wide applications and the difference of convex functions algorithm (shortened as DCA henceforth) is one of the most efficient algorithms [19, 17, 20]. We refer to [1, 11] for detailed difference of convex functions (DC) structure and analysis for many widely used nonconvex optimization problems. The widely employed second-order implicit-explicit (IMEX) convex splitting technique finds extensive application in phase-field simulations [28, 21, 14] and solving systems of ordinary differential equations [5]. The method can be written as: \frac{2}{3\delta t}(3u^{n+1}-4u^{n}+u^{n-1})+h(u^{n+1})+2f(u^{n})-f(u^{n-1})=0, (1.2) where h(u)=\nabla H(u), f(u)=\nabla F(u) and we assume h(u) is continuous henceforth. It is well-known that (1.2) is based on the second-order backward differentiation formulas (BDF) for the implicit and possible linear part h(u) and Adams-Bashforth scheme for the explicit and possible nonlinear part f(u) [5, 28]. Motivated by the recent advancements in the preconditioned framework for DC algorithms [13, 29], as well as the enhanced DC approach with line search [3, 4], we propose a preconditioned second-order convex splitting framework, also can be called as varying DC, with line search to solve (1.1). Initially, we introduce the following energies with a positive step size \delta t: \displaystyle{E}^{n}(u)={H}^{n}(u)-{F}^{n}(u), (1.3) \displaystyle{H}^{n}(u)=H(u)+\frac{1}{\delta t}\|u-u^{n}\|_{2}^{2}, \displaystyle{F}^{n}(u)=\frac{1}{3\delta t}\|u-u^{n-1}\|_{2}^{2}-F(u)-\langle f% (u^{n})-f(u^{n-1}),u-u^{n-1}\rangle. We then solve the subproblem involving the difference of convex functions by incorporating a proximal term to update y^{n}. The optimization task for determining y^{n} is expressed as follows: y^{n}=\operatorname*{arg\,min}_{u}\left\{{H}^{n}(u)-\langle\nabla{F}^{n}(u^{n}% ),u\rangle+\frac{1}{2}\|u-\hat{u}^{n}\|_{M}^{2}\right\}. (1.4) Here, the positive semidefinite weight M is utilized to create efficient preconditioners. The starting point for preconditioned iteration is denoted by \hat{u}^{n}, which can take the form of either \hat{u}^{n}=u^{n} or \hat{u}^{n}=\frac{4}{3}u^{n}-\frac{1}{3}u^{n-1}. By applying the first-order optimality condition, the minimization problem (1.4) for determining y^{n} leads to the equation: \frac{2}{3\delta t}(3y^{n}-4u^{n}+u^{n-1})+M(y^{n}-\hat{u}^{n})+h(y^{n})+(2f(u% ^{n})-f(u^{n-1}))=0. (1.5) It is crucial to highlight that the inclusion of the extrapolation term on f, specifically 2f(u^{n})-f(u^{n-1}), distinguishes the scheme (1.2) significantly from DCA-like algorithms [18]. For the Allen-Cahn model with specific H and F functions, where S represents a positive constant and I is the identity operator, the choices discussed in [28] involve M=SI and \hat{u}^{n}=2u^{n}-u^{n-1}, while those in [21] consider M=\delta tSI and \hat{u}^{n}=u^{n}. The careful selection of S(u^{n+1}-2u^{n}+u^{n-1}) in [28] or S\delta t(u^{n+1}-u^{n}) in [21] preserves second-order time discretization and enhances stabilization. We will reformulate (1.5) as classical preconditioned iterations for solving y^{n}, offering greater convenience. By defining d^{n}=y^{n}-u^{n}, we proceed with an aggressive Armijo-type line search involving parameters \beta^{k}\lambda_{0}>0, k=0,1,2,\ldots, \beta\in(0,1), and \lambda_{0},\alpha>0. The objective is to find the smallest integer k satisfying: {E}^{n}(y^{n}+\beta^{k}\lambda_{0}d^{n})\leq{E}^{n}(y^{n})-\alpha\beta^{k}% \lambda_{0}\|d^{n}\|_{2}^{2},\quad d^{n}=y^{n}-u^{n}. (1.6) The final step size \lambda_{n}:=\beta^{k}\lambda_{0} obtained from the line search in (1.6) dictates the update of u^{n+1} as follows: u^{n+1}=y^{n}+{\lambda}_{n}d^{n}. (1.7) In cases where the line search is viable, the relationship u^{n+1}=u^{n}+(1+\lambda_{n})d^{n} holds, with \lambda_{n}>0. This signifies that the update u^{n+1} involves a full step of d^{n} alongside an additional step of \lambda_{n}d^{n}. This method stands in contrast to the Fukushima-Mine line search [23], traditional line search methods for Newton techniques [25, Algorithm 2.1], and the Armijo line search for generalized conditional gradient methods [16], where typically (1+\lambda_{n})\in[0,1]. Our research contributions can be summarized as follows. Initially, we are, to the best of our knowledge, the first to establish the global convergence of the iteration sequence of the second-order convex splitting method. While second-order convex splitting methods have found widespread application in phase-field simulations involving equations like Allen-Cahn and Cahn-Hilliard, as well as gradient flow problems [5, 14, 21, 28], existing convergence analyses have mainly concentrated on stability of the energy or perturbed energy [14, Theorem 3.2], [28, Lemma 2.3], or [21, Theorem 1.2]. Recent advancements in KL analysis [7, 6, 22] have enabled us to demonstrate global convergence of the iteration sequence under mild conditions. Additionally, we have introduced line search acceleration [3, 4, 2] to enhance the second-order convex splitting method and incorporated a preconditioning technique [13, 29] to handle large-scale linear subproblems efficiently during each iteration. By conducting a finite number of preconditioned iterations without error control, we can guarantee global convergence along with line search accelerations. Numerical experiments have validated the superior efficiency of the proposed preconditioned second-order convex splitting with line search. The subsequent sections outline the structure of the remaining content. Section 2 introduces essential analytical tools, presents the proposed second-order convex splitting with line search and preconditioning, and provides a comprehensive global convergence analysis with KL properties. Section 3 presents detailed numerical tests to showcase the effectiveness of the proposed algorithms. We also give an in-depth exploration of preconditioning and line search techniques. We also give a conclusion in Section 4."
https://arxiv.org/html/2411.07562v2,Thermodynamic consistency and structure-preservation in summation by parts methods for the moist compressible Euler equations,"Moist thermodynamics is a fundamental driver of atmospheric dynamics across all scales, making accurate modeling of these processes essential for reliable weather forecasts and climate change projections. However, atmospheric models often make a variety of inconsistent approximations in representing moist thermodynamics. These inconsistencies can introduce spurious sources and sinks of energy, potentially compromising the integrity of the models.Here, we present a thermodynamically consistent and structure preserving formulation of the moist compressible Euler equations. When discretised with a summation by parts method, our spatial discretisation conserves: mass, water, entropy, and energy. These properties are achieved by discretising a skew symmetric form of the moist compressible Euler equations, using entropy as a prognostic variable, and the summation-by-parts property of discrete derivative operators. Additionally, we derive a discontinuous Galerkin spectral element method with energy and tracer variance stable numerical fluxes, and experimentally verify our theoretical results through numerical simulations.","Atmospheric models typically utilise multiple different, and often inconsistent, thermodynamic approximations throughout a single code. These thermodynamic inconsistencies introduce resolution independent spurious sources and sinks of energy and hence violate the first law of thermodynamics. For climate models these energy errors can be significant, and are believed to adversely affect the integrity of long-time climate simulations Lauritzen et al. (2018); Eldred et al. (2022). To improve the energy budgets of global climate models, recent studies have explored the application of thermodynamic potentials to ensure thermodynamic consistency Thuburn (2017); Staniforth and White (2019); Eldred et al. (2022). This approach focuses on approximating a single thermodynamic quantity—known as the thermodynamic potential—and deriving all other relevant quantities from it. The most commonly employed potentials in this context are internal energy and the Gibbs function. For a comprehensive overview of thermodynamic potentials we refer the reader to Staniforth and White (2019) and Eldred et al. (2022). The thermodynamic potential approach was first proposed in Thuburn (2017), which introduces a thermodynamically consistent semi-Lagrangian model for a two-phase liquid-vapor system utilizing the Gibbs potential as a function of its natural variables: pressure and temperature. However, extending this method to include a third ice phase poses challenges, as the Gibbs potential does not uniquely define an equilibrium state at the triple point. To address this issue, Bowen and Thuburn (2022a, b) explore an alternative approach that employs internal energy as the potential in a semi-Lagrangian discretization of the three-phase moist compressible Euler equations, effectively sidestepping the triple point ambiguity associated with the Gibbs potential. Similarly, Guba et al. (2024) presents a thermodynamically consistent method that also uses internal energy as the potential, implemented within a horizontally semi-Lagrangian and vertically spectral element discretization. While these methods ensure thermodynamic consistency and therefore discretize an energy-conserving set of equations, it is important to note that the discretizations themselves may not guarantee energy conservation or stability. To the best of their knowledge, the authors are not aware of any mimetic or structure-preserving spatial discretizations for the moist compressible Euler equations, however it is noteworthy that significant progress has been made in developing such methods for the dry compressible Euler equations, as well as for related shallow water and thermal shallow water equations McRae and Cotter (2014); Lee et al. (2018); Lee and Palha (2020); Taylor and Fournier (2010); Eldred et al. (2019); Natale et al. (2016); Cotter (2023). These advancements encompass a variety of approaches, including mixed finite element, continuous Galerkin, and discontinuous Galerkin methods. Although the specifics differ among these techniques and equations, they all maintain structure preservation by discretising the equations in a vector-invariant skew-symmetric form and utilizing summation-by-parts (SBP) operators. Structure preservation may reduce model biases and more accurately represent physical processes, but it does not guarantee numerical stability for compressible flows with active tracers, such as the dry and moist compressible Euler equations and the thermal shallow water equations. We hypothesize that this limitation arises because energy is not a mathematical entropy for these equations. In this context, mathematical entropy refers to any conserved quantity that is a convex function of the prognostic variables; for instance, energy in the shallow water equations and physical entropy in the conservative dry Euler equations. Numerous studies Waruszewski et al. (2022); Ranocha (2020); Ducros et al. (2000); Morinishi (2010); Sjögreen and Yee (2019); Gassner et al. (2016); Hennemann et al. (2021); Pirozzoli (2010) have demonstrated that discrete mathematical entropy stability is both necessary and often sufficient for ensuring numerical stability. These methods often utilize the conservative form of the equations and aim to conserve the physical entropy by employing techniques such as splitting derivative operators, SBP, and entropy stable numerical fluxes. In Ricardo et al. (2024a, b), the authors introduce numerical methods for the thermal shallow water and dry compressible Euler equations that are both mimetic and mathematical entropy stable. These studies identify tracer variance as a form of mathematical entropy and propose a split-form, skew-symmetric formulation of the equations. The resulting formulations are discretized using SBP operators, ensuring both entropy and energy stability. In this work, we extend this structure-preserving and mathematical entropy-stable approach to the moist compressible Euler equations. The result is a thermodynamically consistent skew-symmetric formulation that, when discretized using a carefully designed SBP scheme, maintains both structure preservation and mathematical entropy stability. We then develop a novel energy and mathematical entropy stable discontinuous Galerkin spectral element method (DG-SEM) for these equations. The rest of the paper is as follows. In section 2 we derive an operator-split skew-symmetric formulation of the moist compressible Euler equations. We demonstrate that energy and mathematical entropy conservation can be proved using only integration by parts, and therefore any SBP discretisation of these equations will inherit discrete analogues of these conservation properties. In section 3 we outline the thermodynamic approximation we use in our numerical method. The discrete conservation and stability results are independent of the particular thermodyanmic approximation used but we include this for completeness. In section 4 we introduce DG-SEM and detail our novel DG-SEM discretisation. Following this, in section 5 we prove that our discrete method: conserves mass, water, and entropy; and is semi-discretely energy and (mathematical) entropy stable. These theoretical results are then verified via numerical experiments in section 6. Finally, our conclusions from this study are presented in section 7."
https://arxiv.org/html/2411.07390v1,Counting the number of stationary solutions of Partial Differential Equations via infinite dimensional sampling,"This paper is concerned with the problem of counting solutions of stationary nonlinear Partial Differential Equations (PDEs) when the PDE is known to admit more than one solution. We suggest tackling the problem via a sampling-based approach. We test our proposed methodology on the McKean-Vlasov PDE, more precisely on the problem of determining the number of stationary solutions of the McKean-Vlasov (or porous medium) equation.Keywords. McKean Vlasov PDE, Stochastic partial differential equations, Stationary solutions of PDEs, Infinite dimensional sampling.AMS Subject Classification. 35Q83, 35Q70, 60H15, 65M22, 65K99, 82M60.","Within the field of Partial Differential Equations (PDEs), a large body of literature is concerned with investigating well-posedness of PDEs. That is, with studying existence and uniqueness of solutions, in appropriate spaces. It is however the case that many nonlinear PDEs of pivotal importance in applications admit more than one solution. This paper is devoted to the problem of determining the number of solutions of stationary nonlinear PDEs (and then possibly finding properties of such solutions), i.e. solving problems of the form \mathcal{L}u=0\,, (1) where \mathcal{L} is a suitable (nonlinear) operator acting on a given function space. Studying the set of solutions of (1) is central in at least three broad contexts. First, solutions of (1) are candidate equilibria of the associated time-dependent PDE, \partial_{t}u=\mathcal{L}u\,. (2) This is relevant for both deterministic and stochastic problems. Indeed, when \mathcal{L} is the Fokker-Planck operator associated with a given Stochastic (ordinary) Differential Equation (SDE), the solutions of (1) are the invariant measures of the SDE. Second, at least when the solutions of (1) are stable – in the sense that they are stable equilibria of (2) – one expects them to correspond to metastable states of the associated SPDE, i.e. of the evolution \partial_{t}u=\mathcal{L}u+\epsilon\,\eta(t,x)\,, (3) where \epsilon>0 is a small parameter and \eta is spatiotemporal noise. Finally, if the problem has a variational structure, solutions of (1) are related to extrema of associated functionals, often referred to as energy functionals or potentials. Hence the connection with the many applications of interest in calculus of variations and with energy landscape exploration problems. Many problems in both natural and applied sciences can be recast as one of (1), (2), and (3). Let us give some examples. Stochastic processes with multiple invariant measures naturally arise in the study of interacting particle systems and associated mean field limits. Such systems are paradigmatic models in statistical mechanics and kinetic theory, in connection with the study of phase transitions, as well as in the study of collective navigation and consensus formation Talay (1996); Dawson (1983). The importance of understanding processes with multiple invariant measures cannot be understated. Indeed, while statistical sampling has been one of the main motivations for the study of processes with one invariant measure - ergodic processes - many processes in natural and engineered systems are likely to exhibit multiple invariant measures. This includes the tendency of fireflies to synchronise their flashing or not, the microstructure of nematic crystals aligning to several distinct equilibrium configurations (giving rise to different material properties), the opinion formation process in social media, which can converge towards various possible outcomes, etc Goddard et al. (2023); Freidlin and Koralov (2021); Carrillo et al. (2020); Pareschi and Toscani (2013); Gorbonos et al. (2024); Yin (2001). Related to problems of the form (3), metastability theory and landscape exploration are central to molecular dynamics, computational chemistry and condensed matter physics, particularly for the calculation of reaction rates. Indeed, a large number of methods have been proposed to understand transitions between metastable states, including the string method in finite Weinan et al. (2002) and infinite dimensions Weinan et al. (2004), the nudged elastic band method Henkelman et al. (2000), surface walking approaches such as the dimer method, the gentlest ascent dynamics Weinan and Zhou (2011), the activation-relaxation technique, and more. All such methods require some prior knowledge of the metastable states (so that one can e.g. ‘initialise the string’), which are a subset of the solutions of stationary problems like (1). Despite the pervasiveness of PDEs with multiple solutions, the methods available to find all such solutions is limited. To the best of our knowledge, aside from the brute force approach of starting the Newton algorithm from a large number of different initial points, there are few systematic strategies that work in full generality. The available tools include deflation Farrell et al. (2015) (which only requires pre-knowledge of one solution), continuation and homotopy-continuation Mehta (2011) (which have similar requirements). These approaches are all numerical or computational as, except for rare cases, one does not expect to be able to find solutions analytically. 111We note in passing that in differential geometry the Atiyah-Singer index theorem Atiyah (1970) serves precisely the purpose of counting solutions of differential equations associated to linear elliptic operators. It might be worth exporting this theorem to computational stochastic analysis and reconciling that framework with what is known in more applied fields. However, such a theorem does not apply to considered in this paper. We mention also so-called ‘landscape exploration methods’ such as the eigenvector-following method Doye and Wales (1997) and minimax approaches Li and Zhou (2001) to find saddle points, which are also relevant to the study of reaction rates. However, these approaches mostly apply to finite dimensional landscapes. We emphasize that the complexity of the problems that arise in this context is high; examples are illustrated in the works Wang et al. (2019); Han et al. (2021) concerning the study of nematic crystals (different solutions of the associated stationary PDE problem here correspond to distinct equilibrium configurations of the crystal), in the setting of de Gennes theory. In Wang et al. (2019), twenty eight solutions were found for the problem considered there, with no guarantee that all had been found. For semilinear Schrödinger equations, there are (at least) countably many soliton type solutions, even after accounting for symmetries, Sulem and Sulem (1999); Fibich (2014). There are two challenges to determining the set of solutions to (1): determining the number of solutions and then finding the solutions themselves (or at least some properties of such solutions). These challenges should be regarded as distinct. Deflation is a very powerful method, proven useful in many settings. But even by deflation one cannot guarantee all solutions have been found. This limitation is true of all the other methods we have mentioned, and it will certainly be true, also, of the method we propose in this paper. Thus, it is pragmatic to have one’s disposal and array of methods and exploit synergies between them, to make progress on the specific problem at hand. As pointed out in Farrell et al. (2015), string methods are powerful, but they rely on pre-knowledge of metastable states. Deflation can assist with that. Similarly, and more pertinent to the context at hand, if by deflation one has found n solutions but some ‘solution counting method’ reveals that we should expect at least say n+2 then one can redouble the search. This paper attempts to put forward an approach to address the problem of counting the number of (stable) solutions of a given stationary PDE of the form (1). The purpose of this short paper is to present the main idea of this approach though it is undoubtedly the case that further investigation and (non-trivial) analysis will be needed in order to understand its strengths and limitations. We comment on this in Section 1.1 below, Section 2 and at various points throughout. The paper is organised as follows: in Subsection 1.1 we present our method and then, in subsequent sections, we apply this method to find the number of stable stationary solutions of the McKean-Vlasov (or porous media) equation. These correspond to invariant measures of stochastic processes which are non-linear in the sense of McKean. We do this both for parameter regimes where the number of solutions is, a priori, known and also in settings where the number of solutions is yet to be settled. In Section 2 we provide background material on the McKean-Vlasov PDE and on the associated McKean-Vlasov SPDE; the latter will be the central tool we employ to find the number of stable solutions of the stationary McKean-Vlasov PDE. Section 3 and Section 4 are devoted to numerical and simulation aspects, respectively (more detail below). 1.1. Our Approach. We propose that in order to count the number of stable solutions of the stationary problem (1) one can sample from the invariant measure (assuming it exists and is unique) of the Stochastic Partial Differential Equation (SPDE) obtained by perturbing the time-dependent problem (2) by additive noise; that is, one can sample from the invariant measure of the SPDE \partial_{t}u=\mathcal{L}u+\eta(t,x)\,, (4) where \eta is a suitable space-time noise. At least in the case in which the solutions of (1) are isolated, we expect the number of modes of the invariant measure of (4) to coincide with the number of stable solutions of (1) and the number of stable equilibria of (2). Further, we expect a correspondence between the metastable states of (4), the modes, and the stable stationary solutions of (1) and (2). The noise \eta must be appropriately chosen not only to make sure that (4) is well posed and has a unique invariant measure, but also to, potentially, satisfy other constraints specific to the problem under consideration, see Section 2 for more detail on this point.222We consider additive noise because it is straightforward from both an analytical and computational perspective, but the use of other types of noise merits exploration. Intuitively, if the noise \eta is strong enough we would expect (4) to have at most one invariant measure. Let us explain the rationale of the approach in a simpler, finite dimensional setting. Consider one-dimensional Langevin dynamics in multi-well potential: dY_{t}=-U^{\prime}(Y_{t})dt+\sqrt{\alpha}d\beta_{t} (5) where Y_{t}\in\mathbb{R},U:\mathbb{R}\rightarrow\mathbb{R} is a confining potential, \alpha>0, and \beta_{t} is one dimensional standard Brownian motion. Under very general assumptions on U, as soon as \alpha>0, the SDE (5) has only one invariant measure, the Maxwellian e^{-2U/\alpha} (after choosing U to include the correct normalization constant, which can always be done). In contrast, when \alpha=0 this equation is a simple ODE with as many stable stationary states as the number of minima (wells) of the potential U (assuming such minima are isolated). Dirac measures centered around such stationary states can be viewed as the (multiple) stable invariant measures of the ODE. For a double well potential, one has three stationary states, two stable and one unstable. When U has many wells, a naive way to count them is to simulate (5) for long enough and count the number of modes of the measure e^{-2U/\alpha}, as this measure will concentrate around the minima of U. This is shown in Figure 1. Figure 1. A simple scalar multiwell potential, U, and the associated trajectory of (5). As expected, the trajectory tends to persist near one of the two minima. This approach comes with limitations. First and foremost, one never knows if the whole state space has been adequately sampled; hence there are no guarantees that all the minima of U have been found. This problem is unavoidable, whether in finite or infinite dimensions, and irrespective of the method used to sample. The potential landscape might also be more complex with non-isolated minima, elusive saddle points, etc. All these problems persist in the infinite dimensional setting. This is why we propose this method to be used in conjunction with other methods, as an aid for exploration. We also emphasize, again, that the method we propose will only help count the number of stable solutions of (1), in the sense that they are stable in (2). It will be far less informative about unstable solutions. Analogously with the finite dimensional case, one might see the idea of using the SPDE (4) as a landscape exploration method, especially if the deterministic part of the equation has some gradient structure. Some comments on this - and words of caution - can be found in Note 2.1. While infinite dimensional Markov Chain Monte Carlo (MCMC) sampling algorithms are now well-developed for many problems of interest, one might not know the invariant measure of (4) explicitly, making the use of MCMC algorithms impractical. The option - which we take in the example of this paper - is to simulate the SPDE for sufficiently long time. Despite all the caveats that we have highlighted, this approach is general and simple to understand. In the next section we provide an illustration of its effectiveness in the context of the search of stationary solutions of porous media equations."
https://arxiv.org/html/2411.07194v1,Re-anchoring Quantum Monte Carlo with Tensor-Train Sketching,"We propose a novel algorithm for calculating the ground-state energy of quantum many-body systems by combining auxiliary-field quantum Monte Carlo (AFQMC) with tensor-train sketching. In AFQMC, having a good trial wavefunction to guide the random walk is crucial for avoiding sign problems. Typically, this trial wavefunction is fixed throughout the simulation. Our proposed method iterates between determining a new trial wavefunction in the form of a tensor train, derived from the current walkers, and using this updated trial wavefunction to anchor the next phase of AFQMC. Numerical results demonstrate that our algorithm is highly accurate for large spin systems, achieving a relative error of 10^{-5} in estimating ground-state energies. Additionally, the overlap between our estimated trial wavefunction and the ground-state wavefunction achieves a high-fidelity. We provide a convergence proof, highlighting how an effective trial wavefunction can reduce the variance in the AFQMC energy estimate.","The quantum many-body problem appears in a wide range of fields, including condensed matter physics, high energy and nuclear physics, quantum chemistry, and material science. One of the most challenging parts of this problem is that the computational cost grows exponentially with the size of the system. Quantum Monte Carlo (QMC) [1, 2, 3, 4] is a class of algorithms that can efficiently deal with high-dimensional problems by reducing the computational cost to a polynomial scale with the system size. It guarantees that in expectation, one can obtain the exact ground-state energy. However, with a finite sample size, QMC generally suffers from sign problem [5, 6], meaning the variance grows exponentially in time. To fix this issue, the constrained-path auxiliary-field quantum Monte Carlo (cp-AFQMC) [7] and phaseless auxiliary-field quantum Monte Carlo (ph-AFQMC) [8] were developed to avoid the sign problem, and have been successfully applied to both lattice models [9, 10, 11, 12, 13, 14, 15, 16, 17, 18] and realistic materials [19, 20, 21, 22, 23, 24, 25, 26, 27, 28]. In cp-AFQMC, the wavefunction is represented as a sum of an ensemble of statistical independent random walkers. The sign problem is avoided by introducing a trial wavefunction to bias the random walkers such that they point in the same direction as the trial wavefunction, with a price of a potential systematic bias. This bias depends on the quality of the trial wavefunction [29]. Ideally, the bias will be removed if the trial wavefunction is exactly the ground-state wavefunction of the system [30]. Based on this result, self-consistent approaches for successively improving the trial wavefunction have been developed [31, 32, 33, 34] to reduce the systematic error. On the other hand, a different type of approach towards the quantum many-body problem is to directly solve for the wavefunction through approximating it by some low-complexity ansatz. One of the most popular ansatz is the matrix product state (MPS) [35, 36], also known as the tensor train (TT) [37]. A huge advantage of TT representation is that its storage complexity and computational complexity can be made near linear (instead of exponential) in the dimension d. However, for complicated systems, the approximation error can be large, which leads to inaccurate energy estimates. Recently, an algorithm that directly combines AFQMC and TT representation has been proposed by one of the authors [38]. In this method, one projects the random walkers into a TT by a TT-sketching technique [39] at every step. It presents a potentially cheaper strategy to update a TT, using AFQMC to simulate a matrix-vector multiplication. While this method avoids the expensive compression of the Hamiltonian operator, it still suffers from the limitation that sometimes, one cannot approximate the ground-state wavefunction as a low-rank TT. In this paper, we provide improvements to AFQMC and tensor methods by combining the best of both worlds. A TT representation of the wavefunction is estimated periodically using TT-sketching. Then the estimated tensor-train wavefunction is used as the trial wavefunction to “anchor” the next episode of AFQMC. We then alternate between estimating a new trial wavefunction, and running the next episode of AFQMC. While this can potentially remove the energy bias by successively improving the trial wavefunction, we show that this strategy also significantly reduces the energy variance as well. On the other hand, our approach in estimating the tensor-train wavefunction is made cheap by the fact that it does not have to be highly accurate, but just have to be sufficiently accurate to guide the walkers. This is unlike other tensor-train approaches, including [38], where it is imperative to obtain an accurate approximation to the ground state since the tensor-train is used to obtain to final energy estimate. We also motivate our procedure by theoretically showing that an improved trial wavefunction can significantly lower the variance of AFQMC’s energy. The rest of this paper is organized as follows. In Section 2, we introduce the procedure of cp-AFQMC for calculating the ground state of spin models, and also some preliminaries for TT and TT-sketching. Our proposed algorithm is then presented in Section 3. In Section 4, we provide a theoretical analysis for the convergence of cp-AFQMC, showing that the procedure can give an accurate energy estimate, despite the fact that the wavefunction itself has a large variance. The results of numerical experiments are shown in Section 5."
https://arxiv.org/html/2411.07151v1,Slice sampling tensor completion for model order reduction of parametric dynamical systems,"The paper addresses the problem of finding a low-rank approximation of a multi-dimensional tensor, \boldsymbol{\Phi}, using a subset of its entries. A distinctive aspect of the tensor completion problem explored here is that entries of the d-dimensional tensor \boldsymbol{\Phi} are reconstructed via C-dimensional slices, where C<d-1. This setup is motivated by, and applied to, the reduced-order modeling of parametric dynamical systems. In such applications, parametric solutions are often reconstructed from space-time slices through sparse sampling over the parameter domain. To address this non-standard completion problem, we introduce a novel low-rank tensor format called the hybrid tensor train. Completion in this format is then incorporated into a Galerkin reduced order model (ROM), specifically an interpolatory tensor-based ROM. We demonstrate the performance of both the completion method and the ROM on several examples of dynamical systems derived from finite element discretizations of parabolic partial differential equations with parameter-dependent coefficients or boundary conditions.","In this paper, we focus on reduced-order modeling for a multiparameter dynamical system using sparse sampling of the parameter domain. Our dimension reduction technique is based on a tensor completion method in a hybrid tensor train (HTT) low-rank format. This new format is particularly well-suited for slice sampling, where the observed entries of a tensor are available as slices. Such slice sampling is relevant in the context of dynamical systems that depend on multiple parameters, where the parametric solution manifold can be effectively learned through space-time slices. Building on the success of solving low-rank matrix completion problems [11, 8, 9, 37, 21, 30], there has been significant progress in developing numerical algorithms for completing multi-dimensional arrays (tensors) in various low-rank tensor formats; see, e.g., [41, 23, 6, 26, 7]. A typical low rank tensor completion problem can be formulated as finding a minimal rank tensor \widetilde{\boldsymbol{\Phi}}\in\mathbb{R}^{N_{1}\times\dots\times N_{d}} that fits an unknown tensor \boldsymbol{\Phi}\in\mathbb{R}^{N_{1}\times\dots\times N_{d}} for a subset of its observed entries: \widetilde{\boldsymbol{\Phi}}=\underset{\boldsymbol{\Psi}\in\mathbb{R}^{N_{1}% \times\dots\times N_{d}}}{\operatorname{argmin}}\mbox{rank}(\boldsymbol{\Psi})% ,\quad\text{s.t.}~{}\boldsymbol{\Psi}|_{\Omega}=\boldsymbol{\Phi}|_{\Omega}, (1) where \boldsymbol{\Psi}|_{\Omega} is a restriction of the tensor on the set of indexes \Omega of observed entries. The exact fitting can be relaxed to approximate one, yielding the inexact completion problem, \widetilde{\boldsymbol{\Phi}}=\underset{\boldsymbol{\Psi}\in\mathbb{R}^{N_{1}% \times\dots\times N_{d}}}{\operatorname{argmin}}\mbox{rank}(\boldsymbol{\Psi})% ,\quad\text{s.t.}~{}\|\boldsymbol{\Psi}|_{\Omega}-\boldsymbol{\Phi}|_{\Omega}% \|\leq\varepsilon, (2) with a given \varepsilon\geq 0. The inexact completion (2) is a common problem setup for the case of noisy data [10] and this is a formulation we are interested here. The definition of the tensor rank in (1)–(2) is not unique and depends on the choice of a rank revealing tensor decomposition, with CANDECOMP/PARAFAC (CP), Tuker or Tensor Train (TT) among most popular choices. For CP, Tucker, and TT ranks, the completion problems (1) and (2) are NP-hard. One popular approach is to relax it into a convex optimization problem [35, 17, 6], following the ideas from [11, 9]. Other approaches to tensor completion include ALS methods [38, 20], Riemannian optimization [23, 36], Bayesian methods [39, 40], and projection methods [33], with applications ranging from video recovery to seismic data reconstruction [6, 25]. The application of tensor completion methods for reduced-order modeling of parametric ODEs or PDEs is rare, and this study aims to explore this direction. In general, the use of tensor methods for solving parametric PDEs is not new. Several studies have developed sparse tensorized solvers for certain high-dimensional and stochastic PDEs [34, 22, 13, 18, 31, 15, 14]. The reconstruction of scalar output quantities of parametric solutions in tensor form from incomplete observations was addressed in [4, 19]. In [4], the authors employed a tensor cross approximation, while [19] applied TT-completion via Riemannian optimization to recover an option pricing statistic from solutions of parametrized Heston and multi-dimensional Black-Scholes models. Additionally, a comparison of TT-cross interpolation and TT-completion for a parameterized diffusion equation in [36] demonstrated that TT-completion requires fewer PDE solver executions to find a low-rank approximation of a particular solution functional. While the works [4, 36, 19] focused on recovering scalar solution statistics in tensor format, here we aim to approximate a tensor of solution snapshots for subsequent use in building Galerkin ROMs for the parametrized dynamical system of interest. Similar to the previous studies, we seek to find a low-rank approximation of the tensor from a small subset of its entries. However, due to the relatively high separation ranks and large sizes of the space and time modes in the snapshot tensor, applying existing completion algorithms in standard low-rank formats is computationally prohibitive. Moreover, the sampling of the unknown tensor occurs not elementwise, but slicewise. Thst is, for a given (e.g., randomly chosen) parameter value, an entire slice of the unknown tensor is obtained by solving the dynamical system for all space and time degrees of freedom. To leverage slice sampling and improve the computational efficiency of the completion, we introduce a new low-rank format, which we call HTT. Completion in the HTT format involves the projection of a sparsely sampled tensor onto reduced orthogonal bases along the spatial and temporal modes (similar to HOSVD), followed by multiple completions of smaller elementwise sampled tensors in TT format, which can be processed in parallel using available completion algorithms. This new format is particularly well-suited for the subsequent construction of a Galerkin ROM known in the literature as the low rank tensor decomposition (LRTD) ROM [27, 28, 29]. The LRTD–ROM is a natural extension of the POD ROM for parametric dynamical systems. The remainder of the paper is organized as follows: Section 2 provides a more detailed explanation of slice sampling and introduces the completion method. We deviate from the traditional approach of defining tensor rank before formulating the completion problem, as we find it more instructive to first explain the method of obtaining a fitting tensor. The resulting rank-revealing format becomes more intuitive afterward. Section 3 describes the Galerkin ROM for the dynamical system. This ROM utilizes HTT as the dimension reduction technique (in place of the standard POD), and we refer to it as HTT-ROM. Section 4 presents the results of numerical experiments."
https://arxiv.org/html/2411.07077v1,A stable one-synchronization variant of reorthogonalized block classical Gram–Schmidt,"The block classical Gram–Schmidt (BCGS) algorithm and its reorthogonalized variant are widely-used methods for computing the economic QR factorization of block columns \bm{\mathcal{X}} due to their lower communication cost compared to other approaches such as modified Gram–Schmidt and Householder QR. To further reduce communication, i.e., synchronization, there has been a long ongoing search for a variant of reorthogonalized BCGS variant that achieves O(\bm{u}) loss of orthogonality while requiring only one synchronization point per block column, where \bm{u} represents the unit roundoff. Utilizing Pythagorean inner products and delayed normalization techniques, we propose the first provably stable one-synchronization reorthogonalized BCGS variant, demonstrating that it has O(\bm{u}) loss of orthogonality under the condition O(\bm{u})\kappa^{2}(\bm{\mathcal{X}})\leq 1/2, where \kappa(\cdot) represents the condition number.By incorporating one additional synchronization point, we develop a two-synchronization reorthogonalized BCGS variant which maintains O(\bm{u}) loss of orthogonality under the improved condition O(\bm{u})\kappa(\bm{\mathcal{X}})\leq 1/2. An adaptive strategy is then proposed to combine these two variants, ensuring O(\bm{u}) loss of orthogonality while using as few synchronization points as possible under the less restrictive condition O(\bm{u})\kappa(\bm{\mathcal{X}})\leq 1/2. As an example of where this adaptive approach is beneficial, we show that using the adaptive orthogonalization variant, s-step GMRES achieves a backward error comparable to s-step GMRES with BCGSI+, also known as BCGS2, both theoretically and numerically, but requires fewer synchronization points.Keywords: Gram–Schmidt algorithm, low-synchronization, communication-avoiding, s-step GMRES, loss of orthogonalityMSC codes: 65F10, 65F25, 65G50, 65Y20","Given a matrix \bm{\mathcal{X}}\in\mathbb{R}^{m\times n}, this work considers computing the economic QR factorization for \bm{\mathcal{X}}, that is, \bm{\mathcal{X}}=\bm{\mathcal{Q}}\mathcal{R}, where \bm{\mathcal{Q}}\in\mathbb{R}^{m\times n} has orthogonal columns and \mathcal{R}\in\mathbb{R}^{n\times n} is upper triangular. The classical Gram–Schmidt algorithm is a widely-used method for this problem. Reducing communication, i.e., data movement and synchronization, has become increasingly crucial for the performance of traditional solvers. Thus blocking, which reduces synchronization points and leverages BLAS-3 operations, is an appealing approach; see the survey [6]. Consequently, the block classical Gram–Schmidt (BCGS) algorithm and its variants have attracted significant interest. To simplify, we consider n=ps and let \bm{\mathcal{X}}\in\mathbb{R}^{m\times ps} consist of p block vectors, such that \bm{\mathcal{X}}=\begin{bmatrix}\bm{X}_{1}&\dotsi&\bm{X}_{p}\end{bmatrix} where \bm{X}_{k}\in\mathbb{R}^{m\times s} for any k=1,2,\dotsc,p. BCGS is employed, for example, in communication-avoiding Krylov methods [8, 10, 15], which have been demonstrated to outperform non-block methods in many practical scenarios [10, 14]. In BCGS, particularly when integrated within communication-avoiding Krylov methods, we typically focus on the backward error \frac{\|\bm{\mathcal{X}}-\widehat{\bm{\mathcal{Q}}}\widehat{\mathcal{R}}\rVert% _{2}}{\|\bm{\mathcal{X}}\rVert_{2}}, and the loss of orthogonality (LOO) \|I-\widehat{\bm{\mathcal{Q}}}^{\top}\widehat{\bm{\mathcal{Q}}}\rVert_{2}, where \hat{\cdot} denotes computed quantities. These two aspects will directly impact the stability of communication-avoiding Krylov subspace methods and other downstream applications. It should be noted that the backward error is O(\bm{u}) for all commonly used BCGS methods, where \bm{u} represents the unit round-off, making the LOO typically the primary concern. Considering a distributed memory parallel setting, we define a synchronization point as a global communication involving all processors, such as MPI_Allreduce. The traditional BCGS method involves two synchronization points (i.e., block inner products and intraorthogonalization which orthogonalize vectors within a block column) per iteration (i.e., per block column) but suffers from instability, as its LOO is not O(\bm{u}) [4]. To improve its LOO, one can use a reorthogonalization technique, essentially running the BCGS iteration twice in the for-loop. This reorthogonalized variant BCGSI+, also known as BCGS2, achieves O(\bm{u}) LOO under certain conditions regarding the condition number \kappa(\bm{\mathcal{X}}), as analyzed in [1, 2, 4]. However, BCGSI+’s main drawback is that it requires four synchronization points per iteration. Due to the expensive nature of synchronization, there has been significant interest in minimizing the number of synchronization points per iteration. Synchronization is typically needed for computing inner products or norms of large matrices. One possible strategy to reduce the number of synchronization points is to organize these inner product and norm calculations as collectively as possible. As proposed in [5], BCGS-PIPI+ is a variant of reorthogonalized BCGS that utilizes Pythagorean inner products to reduce synchronization points from four to two, achieving O(\bm{u}) LOO under the assumption O(\bm{u})\kappa^{2}(\bm{\mathcal{X}})\leq 1. Derived in [4], BCGSI+A-1s, which resembles BCGS+LS proposed in [13], is a one-sync reorthogonalized BCGS variant created by removing one intraorthogonalization and delaying another. Additionally, it is the block version of DCGS2 [3] or CGS-2 [12], which is demonstrated to have O(\bm{u}) LOO in [4]. Unfortunately, BCGSI+A-1s can only achieve O(\bm{u})\kappa^{2}(\bm{\mathcal{X}}) LOO given the assumption O(\bm{u})\kappa^{3}(\bm{\mathcal{X}})\leq 1 [4]. In this paper, our aim is first to formulate a stable one-sync reorthogonalized BCGS in which the backward error and LOO remain at the level O(\bm{u}). In [4], a two-sync variant reorthogonalized BCGS, called BCGSI+A-2s, stems from removing the first intraorthogonalization and using Pythagorean-based Cholesky QR as the second intraorthogonalization. Unlike BCGS-PIPI+, which also features two synchronization points per iteration, this version only reaches O(\bm{u})\kappa^{2}(\bm{\mathcal{X}}) LOO. According to the analysis in [4], comparing these two variants shows that the first intraorthogonalization plays a significant role in the LOO. Consequently, we reintegrate the omitted intraorthogonalization into BCGSI+A-1s, which can be seen either as utilizing Pythagorean inner products to boost the LOO of BCGSI+A-1s or applying the delayed intraorthogonalization concept to BCGS-PIPI+, to eliminate an additional synchronization point. Both approaches lead to the one-sync reorthogonalized BCGS method, denoted as BCGSI+P-1S in this paper, which achieves O(\bm{u}) LOO, provided O(\bm{u})\kappa^{2}(\bm{\mathcal{X}})\leq 1. Moreover, observing that the assumption O(\bm{u})\kappa^{2}(\bm{\mathcal{X}})\leq 1 in LOO arises from employing Pythagorean-based Cholesky QR for the first intraorthogonalization, we allow for the possibility of a more stable intraorthogonalization method, which requires at least one extra synchronization. Tall-Skinny QR (TSQR) is a common choice for intraorthogonalization in communication-avoiding methods like low-synchronization BCGS and s-step GMRES, since it requires only one synchronization [11]. This results in BCGSI+P-2S with O(\bm{u}) LOO, provided O(\bm{u})\kappa(\bm{\mathcal{X}})\leq 1. This new variant of BCGS maintains the same LOO property as BCGSI+ while reducing the synchronization needs from four to two points per iteration. For the one-sync variant BCGSI+P-1S, the requirement on the condition number of the input matrix \bm{\mathcal{X}} makes this approach generally unsuitable for use within communication-avoiding Krylov subspace methods. For example, communication-avoiding GMRES, also known as s-step GMRES, analyzed in [7], requires an orthogonalization method yielding a well-conditioned Q-factor under the assumption O(\bm{u})\kappa^{\alpha}(\bm{\mathcal{X}})\leq 1, with \alpha=0 or 1 in order to guarantee a backward error at the O(\bm{u}) level. Thus, we suggest a new adaptive approach to orthogonalization within communication-avoiding GMRES: primarily adopting BCGSI+P-1S and switching to BCGSI+P-2S if O(\bm{u})\kappa^{2}(\bm{\mathcal{X}}_{k})>1, where \bm{\mathcal{X}}_{k}=\begin{bmatrix}\bm{X}_{1}&\bm{X}_{2}&\dotsi&\bm{X}_{k}% \end{bmatrix}. This switching condition, i.e., O(\bm{u})\kappa^{2}(\bm{\mathcal{X}}_{k})>1, can be checked using O(ns^{2}) operations without necessitating extra synchronization, according to our analysis. The remainder of this paper is organized as follows. We propose the one-sync reorthogonalized BCGS, and analyze its backward error and LOO in Section 2. Then in Section 3, we propose a two-sync reorthogonalized BCGS and show that its LOO property is as same as BCGSI+. In Section 4, we demonstrate how to employ these low-sync reorthogonalized BCGS in s-step GMRES. Numerical experiments are presented in Section 5 to compare the new variants introduced in Sections 2, 3, and 4 with BCGSI+A, BCGSI+A-1s, and BCGS-PIPI+, and to further compare these variants employed in s-step GMRES. We introduce some notation used throughout the paper before proceeding. Uppercase Roman scripts, for example, \bm{\mathcal{X}} and \bm{\mathcal{Q}}, denote the matrices containing p block vectors. Each block vector is denoted by uppercase Roman letters \bm{X}_{k} and \bm{Q}_{k}, i.e., \bm{\mathcal{X}}_{k}=\begin{bmatrix}\bm{X}_{1}&\bm{X}_{2}&\dotsi&\bm{X}_{k}% \end{bmatrix} and \bm{\mathcal{Q}}_{k}=\begin{bmatrix}\bm{Q}_{1}&\bm{Q}_{2}&\dotsi&\bm{Q}_{k}% \end{bmatrix}. For square matrices, we use uppercase Roman scripts, for example, \mathcal{R}, \SS, and \mathcal{Y}, to represent ps\times ps matrices. We use MATLAB indexing to denote submatrices. For example, \mathcal{R}_{1:k-1,k}=\begin{bmatrix}R_{1,k}\\ R_{2,k}\\ \vdots\\ R_{k-1,k}\end{bmatrix},\quad\SS_{1:k-1,k}=\begin{bmatrix}S_{1,k}\\ S_{2,k}\\ \vdots\\ S_{k-1,k}\end{bmatrix},\quad\text{and}\quad\mathcal{Y}_{1:k-1,k}=\begin{% bmatrix}Y_{1,k}\\ Y_{2,k}\\ \vdots\\ Y_{k-1,k}\end{bmatrix}. In addition, \mathcal{R}_{kk}=\mathcal{R}_{1:k,1:k}, \SS_{kk}=\SS_{1:k,1:k}, and \mathcal{Y}_{kk}=\mathcal{Y}_{1:k,1:k}. We also use \|\cdot\rVert to denote the 2-norm, and further use \kappa(\bm{\mathcal{X}})=\|\bm{\mathcal{X}}\rVert/\sigma_{\min}(\bm{\mathcal{X% }}) to represent the 2-norm condition number, where \sigma_{\min}(\bm{\mathcal{X}}) is the smallest singular value of \bm{\mathcal{X}}. The functions [\bm{Q},R]=\texttt{IO}_{\mathrm{A}}\left(\bm{X}\right), [\bm{Q},R]=\texttt{IO}_{1}\left(\bm{X}\right), and [\bm{Q},R]=\texttt{IO}_{2}\left(\bm{X}\right) represent intraorthogonalization, which are methods to perform the economic QR factorization of a block vector \bm{X}. Various algorithms such as Householder QR (HouseQR), TSQR (TSQR), classical Gram–Schmidt, modified Gram–Schmidt (MGS), or Cholesky QR (CholQR), described in [6], can be applied as intraorthogonalization routines."
https://arxiv.org/html/2411.06997v1,An Integro-differential Model of Cadmium Yellow Photodegradation,"Many paintings from the 19^{\text{th}} century have exhibited signs of fading and discoloration, often linked to cadmium yellow, a pigment widely used by artists during that time. In this work, we develop a mathematical model of the cadmium sulfide photocatalytic reaction responsible for these damages. By employing non-local integral operators, we capture the interplay between chemical processes and environmental factors, offering a detailed representation of the degradation mechanisms. Furthermore, we present a second order positivity-preserving numerical method designed to accurately simulate the phenomenon and ensure reliable predictions across different scenarios, along with a comprehensive sensitivity analysis of the model.","Cadmium yellow [1, 16, 17], whose synthesis was originally described by Gay-Lussac in 1818, was extensively employed by artists throughout the 19^{\text{th}} and 20^{\text{th}} centuries, including Pablo Picasso [11, 19], Joan Miró [24], Edvard Munch [27, 34], Henri Matisse [28, 29, 40], Claude Monet [41], James Ensor [44] and Vincent van Gogh [45]. When exposed to light, this synthetic pigment, primarily composed of cadmium sulfide (CdS), undergoes a photocatalytic reaction that results in color degradation, posing a significant challenge to the long-term preservation of pictorial matrices and cultural heritage. The first systematic study of the CdS deterioration mechanism dates back to 2005 with the work [26], which investigated paintings from 1887 to 1923 using X-ray diffraction and scanning electron microscopy. Since then, a range of significant multidisciplinary contributions has emerged in the scientific literature, furthering the knowledge on the topic [20, 21, 22, 35, 38, 39]. From a chemical perspective, CdS reacts with environmental humidity and oxygen to form cadmium sulfate (CdSO_{4}), with light acting as an activator. Specifically, since the cadmium sulfide behaves as a semiconductor with a definite band-gap energy [15] E_{bg}=2.42 eV, the reaction is initiated by supra-band-gap light with photon energy E_{M}=\dfrac{\texttt{h}\texttt{c}}{\lambda_{M}}>E_{bg} and therefore by light within a spectral range corresponding to wavelengths shorter than \lambda_{M}=512.331 nm (here, c=2.99792\cdot 10^{17} \text{nm}\cdot\text{s}^{-1} denotes the speed of light and h=4.13567\cdot 10^{-15} \text{s}\cdot\text{eV} is the Planck constant). Moreover, observations indicate that temperature has a minimal impact on the deterioration effect, which is more pronounced with increased relative humidity. The overall process, described in [35] as follows CdS+4h^{+}_{surf}+2H_{2}O+O_{2}\longrightarrow CdSO_{4}+4H^{+}, (1.1) results in a noticeable color change and the formation of a thin (5-8 µm) layer of cadmium sulfate on the paint surface. The objective of this work is twofold. Firstly, due to the absence of similar approaches in the literature, we develop a novel integro-differential mathematical model that represents the photochemical degradation process of cadmium yellow, with particular emphasis on the non-local effects of light exposure. Secondly, to ensure efficient and realistic simulations of the phenomenon, we design an accurate numerical method that unconditionally preserves the essential properties of the continuous model, namely the positivity and monotonicity of its solutions. Therefore, we aim to construct a comprehensive mathematical framework that enhances the understanding of the phenomenon and supports the development of effective preservation strategies for cultural heritage. The manuscript is structured as follows. Section 2 presents the derivation of our model and establishes the foundation for the subsequent analysis. In Section 3 we introduce a dynamically consistent numerical method and provide evidence of its quadratic convergence and advantageous performances. Some simulations under various scenarios are discussed in Section 4 and a detailed local sensitivity analysis of the model is conducted in Section 5. Finally, closing remarks and perspectives for future investigation in Section 6 conclude the paper."
https://arxiv.org/html/2411.06943v1,Asymptotic stability of many numerical schemes for phase-field modeling,"The numerical stability of nonlinear equations has been a long-standing concern and there is no standard framework for analyzing long-term qualitative behavior. In the recent breakthrough work [27], a rigorous numerical analysis was conducted on the numerical solution of a scalar ODE containing a cubic polynomial derived from the Allen-Cahn equation. It was found that only the implicit Euler method converge to the correct steady state for any given initial value u_{0} under the unique solvability and energy stability. But all the other commonly used second-order numerical schemes exhibit sensitivity to initial conditions and may converge to an incorrect equilibrium state as t_{n}\to\infty. This indicates that energy stability may not be decisive for the long-term qualitative correctness of numerical solutions.We found that using another fundamental property of the solution, namely monotonicity instead of energy stability, is sufficient to ensure that many common numerical schemes converge to the correct equilibrium state. This leads us to introduce the critical step size constant h^{*}=h^{*}(u_{0},\epsilon) that ensures the monotonicity and unique solvability of the numerical solutions, where the scaling parameter \epsilon\in(0,1). For a given numerical method, if the initial value u_{0} is given, no matter how large it is, we prove that h^{*}>0. As long as the actual simulation step 0<h<h^{*}, the numerical solution preserves monotonicity and converges to the correct equilibrium state. On the other hand, we prove that the implicit Euler scheme h^{*}=h^{*}(\epsilon), which is independent of u_{0} and only depends on \epsilon. Hence regardless of the initial value taken, the simulation can be guaranteed to be correct when h<h^{*}. But for various other numerical methods, no mater how small the step size h is in advance, there will always be initial values that cause simulation errors. In fact, for these numerical methods, we prove that \inf_{u_{0}\in\mathbb{R}}h^{*}(u_{0},\epsilon)=0. Various numerical experiments are used to confirm the theoretical analysis.","1 introduction The phase-field model is an essential tool for describing phase transitions and interface dynamics in materials science. The origins of this model can be traced back to the pioneering work of van der Waals [23]. Among the various phase-field models, the Allen-Cahn equation, Cahn-Hilliard equation and the Molecular Beam Epitaxy (MBE) equation are some of the most commonly used. In this work, we will focus specifically on one of the most widely studied phase-field models: the Allen-Cahn equation [1] \displaystyle u_{t}-\Delta u+\frac{1}{\epsilon^{2}}f(u) \displaystyle=0\quad\text{ in }\Omega_{T}=\Omega\times(0,T), (1) \displaystyle\frac{\partial u}{\partial n} \displaystyle=0\quad\text{ on }\partial\Omega_{T}=\partial\Omega\times(0,T), \displaystyle\left.u\right|_{t=0} \displaystyle=u_{0}. It is well-known that this model can be viewed as the L^{2} gradient flow of the free energy functional E(u)=\int_{\Omega}\frac{1}{2}|\nabla u|^{2}+\frac{1}{\epsilon^{2}}F(u)dx (2) and satisfies the energy dissipation law: \frac{d}{dt}E(u(t))=-\int_{\Omega}|-\Delta u+\frac{1}{\epsilon^{2}}f(u)|^{2}dx% =-\int_{\Omega}|u_{t}|^{2}dx\leq 0, (3) where f(u)=u^{3}-u=F^{\prime}(u) and F(u)=\frac{1}{4}(u^{2}-1)^{2}, the scaling coefficient \epsilon\in(0,1) represents the interfacial width. The numerical simulation of phase field equations has a wide range of practical applications. However, designing efficient and stable numerical schemes has always been a significant challenge, primarily due to the following reasons: (1) Numerical algorithms need to accurately capture the dynamic information of phase transitions while ensuring system stability during long-term simulations; (2) Phase field models should satisfy specific physical properties, such as mass conservation and energy dissipation; (3) The strong nonlinearity present in phase field problems makes constructing efficient high-order methods particularly challenging. In the early studies of numerical simulations of phase field equations, fully explicit and implicit methods dominated. Fully explicit methods include, for example, the Explicit Euler scheme [17, 8] and higher-order explicit Runge-Kutta methods [31]. Fully implicit methods include, for example, the Implicit Euler scheme [17], Crank-Nicolson [5, 16, 29], the modified Crank-Nicolson scheme [4, 6, 22], and Diagonally Implicit Runge-Kutta methods [30]. However, both fully explicit and fully implicit methods are inefficient when solving practical problems, especially for long-term evolution. To address these challenges, partially implicit schemes were developed based on the principle of energy stability. That means similar to continuous energy dissipation (3), the discrete energy does not increase, i.e., E(u_{n+1})\leq E(u_{n}) for the numerical solutions \{u_{n}\}_{n=0}^{\infty}. These include, for example, the convex splitting scheme [7, 11, 12, 10], stabilized methods [9, 14, 26], the invariant energy quadratization method [28] and the scalar auxiliary variable method [21, 20]. These are all methods that ensure either energy stability or modified energy stability \widetilde{E}(u_{n+1})\leq\widetilde{E}(u_{n}). However, recent literature [25] indicates that these partially implicit schemes may achieve energy stability at the cost of sacrificing accuracy. Since the numerical stability of nonlinear equations has been a long-standing concern and lack of a general theoretical analysis framework, especially for long-term qualitative behavior. The recent literature [27] has discovered some novel stability phenomena while examining the follow scalar nonlinear ODE associated with the Allen-Cahn equation u_{t}+\frac{1}{\epsilon^{2}}f(u)=0\text{ with initial value }u(0)=u_{0}. (4) The ODE (\ref{1.1}) can be solved exactly [19] u(t)=\frac{u_{0}}{\sqrt{e^{-\frac{2}{\epsilon^{2}}t}+u_{0}^{2}\left(1-e^{-% \frac{2}{\epsilon^{2}}t}\right)}}. (5) (a) Figure 1: Exact solution u(t) for different initial values As t\rightarrow+\infty, the solution exponentially converges to sign(u_{0}), which corresponds to three steady state solutions \pm 1 and 0, depending on the sign of u_{0}, as shown in the Figure 1. Following [27], we say a numerical solution u_{n} generated by some given numerical schemes approximating u(t_{n}) at t_{n}=nh with step size h>0, converges to the correct steady state solution of (4) if \lim_{n\to\infty}u_{n}=\mathrm{sign}(u_{0}). (6) Similar to (3), the following energy law holds for the ODE (4) \frac{d}{dt}E(u(t))=-|u_{t}|^{2}\leq 0, (7) where E(u)=\frac{1}{\epsilon^{2}}F(u). From now on, we will always use this definition for the energy E(\cdot) when discussing numerical schemes for this ODE model (3). As pointed out in [27], any solution of the ODE (\ref{1.1}) is also a solution of the Allan-Chan equation (\ref{A1}). As a result, any dynamical issues of a numerical scheme for the ODE (\ref{1.1}), such as stability, accuracy and convergence to the correct steady state solution, should also exist in general for the PDE model (\ref{A1}). In this paper, we consider two class of the most commonly used numerical schemes for phase-field models, namely the first-order and second-order schemes. The first-order schemes include the explicit and implicit Euler schemes while the second-order schemes include the Crank-Nicolson scheme, the modified Crank-Nicolson scheme, the convex splitting scheme based on the modified Crank-Nicolson scheme and the implicit midpoint scheme [24, 2]. It is well-known that the explicit Euler method is conditionally stable and often necessitates very small time steps, particularly for stiff problems. On the other hand, the implicit Euler, Crank-Nicolson, and implicit midpoint schemes are unconditionally stable for linear test equation and are well-suited for dealing with stiff problems [Hairer]. However, they encounter the challenge of having to solve a nonlinear system at each time step, where the existence and uniqueness of the solution are concerns. Recent literature [13] indicates that when employing the explicit Euler method to solve the Allen-Cahn equation (1), under certain time step constraints, it satisfies the maximum principle and energy stability. Furthermore, the work [27] shows that for equation (4), the Implicit Euler, Crank-Nicolson, and modified Crank-Nicolson schemes are conditionally uniquely solvable, and exhibit conditional energy stability or modified energy stability. It also states that the convex splitting scheme based on the modified Crank-Nicolson scheme is unconditionally uniquely solvable and unconditionally energy stable. However, regarding the standard implicit midpoint scheme, the energy stability for equations (1) or (4) remains unknown. Energy stability is often achieved by second-order approximation of the nonlinear term, effectively transforming it into the modified Crank-Nicolson scheme [18, 6]. The study in [25, 27] indicate that several second-order implicit schemes mentioned above all exhibit a common issue: even if the numerical method is uniquely solvable and energy stable, it is still possible to converge to the wrong equilibrium state or experience oscillations. Specifically, for any h>0, there exists an initial condition u_{0} with |u_{0}|>1 such that the numerical solution converges to wrong equilibrium state. Moreover, for |u_{0}|\leq 1, all second-order schemes studied in [27] converge to the correct steady state solution but may experience oscillations if the time step size is not sufficiently small. However, the implicit Euler method is quite different from all other methods and it can converges to correct equilibrium states regardless of initial conditions. Therefore, the energy stability of numerical solutions may not be a decisive factor for long-term computation of phase field models. Following [27], we hope to find another property of the numerical schemes to replace energy stability that enables their numerical solution can preserve the qualitative properties of the original equation, that is, to converge to the correct equilibrium state without numerical oscillations. According to the classical theory of ordinary differential equations, the solutions of the scalar ODE u^{\prime}(t)=f(u) keep their monotonicity, due to the facts that the solution curves never cross the zeros of f and hence f(u) has a definite sign. This observation inspires us to use the monotonicity of numerical solutions instead of energy stability, leading to the critical step h^{*} defined below. h^{*}=\sup\Big{\{}h=h(u_{0},\epsilon)\mid\text{Keep the unique solvability and% monotonicity of numerical solutions}\Big{\}}. (8) For a given numerical method, if the initial value u_{0} is given, no matter how large it is, we prove that h^{*}>0. As long as the actual simulation step h\in(0,h^{*}], the numerical solution preserves monotonicity and converges to the correct equilibrium state. On the other hand, we prove that for implicit Euler scheme h^{*}=h(\epsilon), which is independent of u_{0} and only depends on \epsilon coming from the solvability condition. Hence regardless of the initial value taken, the simulation can be guaranteed to be correct when h<h^{*}. But for various other numerical methods, when the initial value |u_{0}|>1, h^{*} depends not only on \epsilon but also on u_{0}. Moreover, we find that h^{*} is always less than step size limitation imposed by the unique solvability condition, so no mater how small the step size h is in advance, there will always be initial values that cause simulation errors. In fact, for these numerical methods, we prove that \inf_{u_{0}\in\mathbb{R}}h(u_{0},\epsilon)=0. The rest of the paper is organized as follows. In Section 2, we engage in a detailed discussion of the explicit and implicit Euler schemes. In Section 3, we discuss four second-order schemes. We first discussed the Crank-Nicoslon scheme in detail, and the other three numerical schemes can be discussed similarly, so we only provide the general ideas and main results. Numerical results are presented and discussed in Section 4. Finally, some concluding remarks are given in Section 5"
https://arxiv.org/html/2411.06844v1,A stable multiplicative dynamical low-rank discretization for the linear Boltzmann-BGK equation,"The numerical method of dynamical low-rank approximation (DLRA) has recently been applied to various kinetic equations showing a significant reduction of the computational effort. In this paper, we apply this concept to the linear Boltzmann-Bhatnagar-Gross-Krook (Boltzmann-BGK) equation which due its high dimensionality is challenging to solve. Inspired by the special structure of the non-linear Boltzmann-BGK problem, we consider a multiplicative splitting of the distribution function. We propose a rank-adaptive DLRA scheme making use of the basis update & Galerkin integrator and combine it with an additional basis augmentation to ensure numerical stability, for which an analytical proof is given and a classical hyperbolic Courant–Friedrichs–Lewy (CFL) condition is derived. This allows for a further acceleration of computational times and a better understanding of the underlying problem in finding a suitable discretization of the system. Numerical results of a series of different test examples confirm the accuracy and efficiency of the proposed method compared to the numerical solution of the full system.","Numerically solving kinetic equations usually requires immense computational and memory efforts due to the high-dimensional phase space containing all possible states of the system. The state of a kinetic system is described by a distribution function f which can be interpreted as the corresponding particle density in phase space. Instead of solving one high-dimensional equation, the concept of dynamical low-rank approximation (DLRA) [23] allows us to split the problem into three lower dimensional subequations leading to an appropriate approximation of the solution. In particular, in a one-dimensional setting we approximate the distribution function f(t,x,v), with t\in\mathbb{R}^{+} denoting the time, x\in D\subset\mathbb{R} the spatial, and v\in\mathbb{R} the velocity variable, by \displaystyle f(t,x,v)\approx\sum_{i,j=1}^{r}X_{i}(t,x)S_{ij}(t)V_{j}(t,v), and evolve the corresponding low-rank factors in three substeps further in time. The sets \{X_{i}:i=1,..,r\} and \{V_{j}:j=1,..,r\} contain the orthonormal basis functions in space and in velocity, respectively, and r is called the rank of this approximation. DLRA has recently gained increasing interest and has been studied in various fields including radiation transport [2, 30, 13, 31, 34], radiation therapy [27], plasma physics [16, 18, 15, 19], chemical kinetics [32, 17] and Boltzmann type transport problems [12, 14, 11, 22]. The core idea of this method is to project the solution to a manifold of low-rank functions of the above form and constrain the solution dynamics there. Different time integrators which are able to ensure this behaviour and are robust to the presence of small singular values are available. Frequently used integrators for kinetic problems are the projector-splitting [29] as well as the (rank-adaptive) basis update & Galerkin (BUG) [10, 8] and the parallel integrator [9]. For the rank-adaptive BUG and the parallel integrator extensions to schemes with proven second-order robust error bounds have been derived in [7, 25]. For a large number of collisions, the solution f of the Boltzmann-BGK equation stays close to the Maxwellian equilibrium distribution M which in general is not a low-rank function. Inspired by [14, 24], we use the multiplicative splitting f=Mg, for which in [14] it has been shown that g is a low-rank function even if this if not the case for f. Hence, we derive an evolution equation for g and apply the low-rank approach to this part of the distribution function. Difficulties may arise in the discretization as it is per se not clear how to treat spatial derivatives. In this paper we propose a stable dynamical low-rank discretization for the linear Boltzmann-BGK equation. The main features of this paper are: 1. A multiplicative splitting of the distribution function: As the Maxwellian equilibrium distribution M is generally not a low-rank function, we consider the multiplicative splitting f=Mg and apply the low-rank ansatz to the remaining function g. It can be considered as a deviation from the equilibrium and is shown to be of low rank [14]. 2. A stable numerical scheme for linear Boltzmann-BGK with rigorous mathematical proofs: We show that a stable discretization has to be derived carefully and compare it with an intuitive discretization that fails to guarantee numerical stability. We give a rigorous analytical proof of stability and derive a classic hyperbolic CFL condition. This enables us to choose an optimal time step size of \Delta t=\text{CFL}\cdot\Delta x with CFL denoting the CFL number, leading to a reduction of the computational effort. 3. A rank-adaptive integrator: For the low-rank scheme we use the rank-adaptive BUG integrator from [8], leading to a basis augmentation in both the K- and L-step of the low-rank algorithm. Compared to the projector-splitting integrator used in [12, 14, 11], this allows us to determine the rank adaptively in each step avoiding the a priori choice of a certain fixed rank. 4. A series of numerical experiments validating the derived properties: We give a number of numerical examples that validate the derived stability while showing a significant reduction of computational and memory requirements of the low-rank scheme compared to the full order method. The paper is structured as follows: After the introduction in Section 1, we provide background information on the linear Boltzmann-BGK equation, explain the considered multiplicative structure, and derive two possible systems of equations in Section 2. Both systems are equivalent in the continuous setting. In Section 3, we discretize in velocity and in space, before subsequently time is discretized giving two different fully discretized schemes. It is then shown in Section 4 that a naive discretization can lead to a numerical scheme that is not von Neumann stable whereas a more careful treatment guarantees numerical stability. Section 5 gives a brief introduction to the concept of DLRA and applies this method such that a numerically stable low-rank scheme is obtained. Numerical experiments in both 1D and 2D in Section 6 confirm the derived results. Section 7 gives a brief conclusion and outlook."
https://arxiv.org/html/2411.06641v1,Convergence analysis of time-splitting projection method for nonlinear quasiperiodic Schrödinger equation,"This work proposes and analyzes an efficient numerical method for solving the nonlinear Schrödinger equation with quasiperiodic potential, where the projection method is applied in space to account for the quasiperiodic structure and the Strang splitting method is used in time. While the transfer between spaces of low-dimensional quasiperiodic and high-dimensional periodic functions and its coupling with the nonlinearity of the operator splitting scheme make the analysis more challenging. Meanwhile, compared to conventional numerical analysis of periodic Schrödinger systems, many of the tools and theories are not applicable to the quasiperiodic case. We address these issues to prove the spectral accuracy in space and the second-order accuracy in time. Numerical experiments are performed to substantiate the theoretical findings.","The nonlinear Schrödinger equation with the cubic nonlinearity is a universal mathematical model describing many physical phenomena, and there exist extensive investigations for this model with periodic potentials or given boundary conditions, see e.g. [1, 2, 3, 4, 5, 6, 7, 8, 9]. In recent years, the quasiperiodic potential V is increasingly considered in various fields. For instance, the Schrödinger system with a potential function V=V_{1}+V_{2} is typically considered in Moiré lattices [10, 11, 12], where V_{1} represents a periodic lattice and V_{2} is a rotated V_{1} by a twisted angle. Varying this twisted angle could result in a transformation between periodic and quasiperiodic structures, as well as a state transition between localization and delocalization in these systems. In fact, the potential function V often appears incommensurate, which is crucial to the construction of quasiperiodic potentials. Even more surprisingly, the nonlinear quasiperiodic Schrödinger systems also exhibit more intriguing phenomena of significant scientific value, including metal-insulator transitions, defects, Anderson localization, dislocations and aperiodic lattice solitons [13, 14, 15, 16]. Motivated by the above discussions, we consider the d-dimensional nonlinear quasiperiodic Schrödinger equation (NQSE) with the real-valued smooth quasiperiodic potential function V(\bm{x}) \displaystyle\begin{cases}i\dfrac{\partial\psi(\bm{x},t)}{\partial t}=-\Delta% \psi(\bm{x},t)+V(\bm{x})\psi(\bm{x},t)+\theta|\psi(\bm{x},t)|^{2}\psi(\bm{x},t% ),~{}~{}(\bm{x},t)\in\mathbb{R}^{d}\times[0,T],\\ \psi(0)=\psi(\bm{x},0),\end{cases} (1) where \Delta=\sum_{j=1}^{d}\partial_{x_{j}x_{j}} is the Laplace operator and the cubic nonlinear term originates from the nonlinear (Kerr) change of the refractive index with |\psi(\bm{x},t)|^{2}=\psi(\bm{x},t)\bar{\psi}(\bm{x},t). The parameter \theta\in\mathbb{R} describes the strength of the nonlinearity. There has been some improvement in the research on the mathematical theory of time- or space-quasiperiodic Schrödinger systems. For instance, Avila et al. made significant contributions to the spectral theory of quasiperiodic Schrödinger operators (QSOs) from the perspective of dynamical systems [17, 18, 19, 20, 21]. Wang et al. investigated Anderson localization of QSOs and the existence of solutions to quasiperiodic Schrödinger equations (QSEs) [22, 23, 24]. Although research on various aspects of quasiperiodic Schrödinger systems has attracted significant attention in recent years, many issues remain unresolved in the study of these systems, including the spectral theory of high-dimensional QSOs, the well-posedness of quasiperiodic solutions for various Schrödinger equations, and the development of associated numerical algorithms. Numerically, quasiperiodic systems lack translation symmetry, have no boundary and are not decay, which introduce significant challenges. Therefore, our goal is not only to provide an effective numerical algorithm but also to establish rigorous theoretical analysis that ensures the reliability of our method, thereby advancing the development of quasiperiodic Schrödinger systems in mathematics and related application fields. There are several numerical methods to solve quasiperiodic systems, such as periodic approximation method (PAM), quasiperiodic spectral method (QSM) and the projection method (PM). PAM is a commonly used approach for solving quasiperiodic systems [10, 12, 14]. Its core idea is to approximate a quasiperiodic system by a periodic function system within a finite domain. While relatively mature research mechanisms exist for these approximation systems, this algorithm still faces unavoidable rational approximation errors and is constrained by a limited approximation domain [25, 26]. To resolve this issue, QSM and PM have been developed [25] and applied to solve the linear quasiperiodic Schrödinger equation [27], which corresponds to model (1) with \theta=0. For the case \theta\neq 0, efficient numerical method and rigorous numerical analysis remain untreated. This work proposes and analyzes an efficient numerical method for solving the nonlinear Schrödinger equation with quasiperiodic potential, where PM is applied in space to account for the quasiperiodic structure, and the Strang splitting method is used in time, see e.g. [28, 29, 30, 31, 32]. Compared to the numerical analysis of periodic Schrödinger equations, the analysis of algorithms for solving NQSEs presents greater challenges. One major challenge arises from the relatively dense Fourier frequencies of quasiperiodic functions, for which some conventional analytical tools and theories, such as the Sobolev embedding theorems, are no longer valid in the numerical analysis of NQSEs. To address this, we utilize the regularity of the corresponding parent functions to control the regularity of quasiperiodic functions. Meanwhile, novel strategies have been introduced in the analytical process. For example, we devise an auxiliary high-dimensional periodic function system (See (13) for details) to manage the upper bound of the numerical quasiperiodic solutions at each iteration step. The rest of the paper is organized as follows: In Section 2, several preliminary results are presented or proved. Section 3 introduces the numerical scheme and its error analysis results. Section 4 provides miscellaneous auxiliary estimates that support the proof of the main theorem. Section 5 presents numerical results that validate the theoretical analysis. Finally, a conclusion is presented in Section 6."
https://arxiv.org/html/2411.06629v2,A dual-pairing summation-by-parts finite difference framework for nonlinear conservation laws,"Robust and stable high order numerical methods for solving partial differential equations are attractive because they are efficient on modern and next generation hardware architectures. However, the design of provably stable numerical methods for nonlinear hyperbolic conservation laws pose a significant challenge. We present the dual-pairing (DP) and upwind summation-by-parts (SBP) finite difference (FD) framework for accurate and robust numerical approximations of nonlinear conservation laws. The framework has an inbuilt ""limiter"" whose goal is to detect and effectively resolve regions where the solution is poorly resolved and/or discontinuities are found. The DP SBP FD operators are a dual-pair of backward and forward FD stencils, which together preserve the SBP property. In addition, the DP SBP FD operators are designed to be upwind, that is they come with some innate dissipation everywhere, as opposed to traditional SBP and collocated discontinuous Galerkin spectral element methods which can only induce dissipation through numerical fluxes acting at element interfaces. We combine the DP SBP operators together with skew-symmetric and upwind flux splitting of nonlinear hyperbolic conservation laws. Our semi-discrete approximation is provably entropy-stable for arbitrary nonlinear hyperbolic conservation laws. The framework is high order accurate, provably entropy-stable, convergent, and avoids several pitfalls of current state-of-the-art high order methods. We give specific examples using the in-viscid Burger’s equation, nonlinear shallow water equations and compressible Euler equations of gas dynamics. Numerical experiments are presented to verify accuracy and demonstrate the robustness of our numerical framework.","Nonlinear hyperbolic conservation laws, defined by systems of nonlinear partial differential equations (PDEs), are widespread in modelling several important physical phenomena and industrial problems such as fluid flow, combustion, black holes, magneto-hydrodynamics, astrophysical and atmospheric fluid dynamics, to name but only a few. Analytical solutions are intractable for most realistic situations. Therefore, accurate and efficient numerical simulations of nonlinear conservation laws are important to academia and industry, and are critical for developing modern technologies, deepening scientific knowledge and paving the way for new inventions and discoveries. The derivation of effective numerical methods for computing approximate solutions of nonlinear conservation laws that are fast, accurate and robust has been a hot research topic beginning from the seminal and early works of von Neumann and Richtmyer [1], Lax and Wendroff [2, 3], Godunov [4], Harten [5], and in recent times has continued to attract substantial interests from the computational and applied mathematics community, see e.g. [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]. Historically, routine numerical approaches to solving nonlinear conservation laws were dominated by low order (first and second order) finite volume and finite difference methods [4, 21, 22, 3] which rely on Godunov-type approximate Riemann solvers. Low order accurate methods can be robust but, because of their inherent excessive numerical dissipation, are not expected to effectively and accurately resolve fine scale and nonlinear features present in the solutions, for example shocks, turbulence, vortices, and highly dispersive wave-dominated phenomena such as gravitational and Rossby wave propagation. High order numerical methods are less dissipative and are known to be efficient and well-suited for wave dominated problems, see e.g., the pioneering work by Kreiss and Oliger [23]. Not all high order methods are acceptable, however, for nonlinear conservation laws. High order accurate numerical simulations of nonlinear conservation laws are challenging as initial attempts often result in crashes due to compounding numerical errors or the presence of undesirable numerical artefacts which can pollute numerical simulations everywhere. Desirable high order accurate methods for nonlinear hyperbolic conservation laws must be robust (provably stable) and preserve several important invariants present in the system. While discrete structure preservation of conservation laws and model symmetries of nonlinear conservation laws can improve accuracy, mitigate against numerical instabilities and enhance the robustness of high order numerical approximations, this also introduces additional theoretical and computational demands, such as proof of nonlinear stability, mimetic reformulations and discretizations. In particular, the proof of nonlinear stability for high order numerical methods for systems of nonlinear conservation laws can be technically challenging. Many previous studies have advocated for numerical methods with provably linear stability, where the assumption of smoothness of solutions is then used to simulate nonlinear problems [24, 25, 26, 27]. However, linearly stable numerical methods are generally not robust for many nonlinear problems and will not yield desirable results in many situations. For instance, turbulence and shocks are highly nonlinear phenomena where linearly stable numerical methods are not sufficient to deliver desirable results. It has been shown and observed in many numerical simulations published in the literature [28, 29, 8, 20], that nonlinear numerical stability is critical to ensuring robustness and accuracy of high order numerical methods for several nonlinear conservation laws. However, before we proceed, first at the continuous level, we must define what it means for a system of nonlinear conservation laws to be nonlinearly stable. We can then mimic the stability properties of the continuous model at the discrete level, leading to a provably nonlinearly stable numerical method. There are two parallel (and sometimes intersecting) definitions of nonlinear stability, namely, energy stability and entropy stability. On the one hand, energy stability, which follows from a linear stability theory, aims to bound/conserve a mathematical energy of the solution, a nonnegative functional of the solution (typically a weighted L_{2}-norm of the solution for first order systems) at a future time by the energy of the initial (and boundary) data [24, 25, 26, 27, 30, 31, 32]. The energy estimate may sometime coincide with the estimate of the physical energy in the system. However, it is also noteworthy that for certain systems (e.g. Einstein’s equations of general relativity) the physical energy is not always nonnegative and may not be useful in deriving nonlinearly stable numerical approximations. On the other hand, an entropy stable numerical method is designed to bound/conserve a mathematical entropy [28, 33, 32, 6, 22, 34, 35, 29, 36, 5], a convex scalar functional that is conserved by the PDE system, for smooth solutions. The notion of entropy stability also has a physical interpretation. For example, a solution of the compressible Euler equations should not only satisfy the nonlinear conservation laws, but also the second law of thermodynamics. A direct implication of this is that the total specific entropy should only increase in a closed system. Consequently, this translates into the solution satisfying an extra partial differential inequality, also known as entropy inequality. A weak solution that satisfies the entropy inequality is considered as the physically correct weak solution [28, 21, 22, 37]. For some systems, such as the Burgers equation and the nonlinear shallow water equations where the energy defines a mathematical entropy, energy stability and entropy stability can be used interchangeably. However, in this paper we will define nonlinear stability in terms of entropy stability. That is, a system of nonlinear conservation laws is entropy stable if there is a mathematical entropy which is conserved by the system for smooth solutions. For an entropy stable system of PDEs, a provably nonlinearly stable numerical method can be derived by replicating the continuous analysis at the discrete level. An essential ingredient for the development of provably stable numerical methods for PDEs is the summation-by-parts (SBP) principle [38, 39, 40, 41, 42, 43], with its mimetic structure, which allows provably stable methods to be constructed at the semi-discrete level, as long as careful numerical treatments of the boundary conditions are provided. At the continuous level, however, the key ingredients that are necessary in proving stability results are integration by parts, and possibly the chain rule and the product rule. So, while SBP numerical derivative operators are designed to mimic integration by parts property at the discrete level, it is challenging for SBP operators to mimic the chain rule or product rule at the discrete level. To be able to replicate the continuous analysis at the discrete level, the system of nonlinear conservation laws at the continuous level must be rewritten into the so-called skew-symmetric or entropy-conserving split-form so that we can negate the use of the chain rule and product rule at the discrete level. For standard systems of nonlinear conservation laws there are several artful ways of doing this, see e.g. [6, 7, 8, 9, 44, 45, 20, 15, 14, 30]. We note, however, for several systems of PDEs finding a mathematical entropy and the skew-symmetric formulation that can be targeted by SBP discretizations can be cumbersome and nontrivial. Once an entropy functional is identified and an entropy conserving or mimetic reformulation of the PDE system is given, then the system can be approximated by SBP operators leading to an entropy conserving numerical method. While, theoretically, this may yield a provably nonlinearly stable numerical method, however, traditional SBP operator for the first derivative based central FD stencils [38, 39, 40, 41] or collocated discontinuous Galerkin spectral element method (DG-SEM) [42, 43, 14] can suffer from spurious unresolved wave-modes, which can destroy the accuracy of numerical solutions and crash the simulation of a nonlinear conservation law. It was recently shown in [29] that the local linear stability property of high order methods is necessary for convergent numerical simulations of nonlinear conservation laws. Sadly, state-of-the-art high order accurate SBP FD and DG-SEM entropy stable schemes for nonlinear conservation in skew-symmetric split form are not locally energy stable, and do not satisfy this necessary criterion. Therefore, a sufficient amount of dissipation must be added, by filtering, limiting or artificial numerical dissipation, to stabilize the simulation and obtain acceptable results. Numerical dissipation helps in many ways, but it can lower numerical accuracy, compromise the conservative properties of the system and introduce interesting and undesirable numerical artefacts. In this study we propose a new high order accurate and provably nonlinearly stable numerical framework for nonlinear conservation laws that significantly minimizes oscillations from shocks and avoids several pitfalls of current state-of-the-art high order methods. We are currently investigating the local energy-stability properties of the proposed DP SBP framework and comparing with other high-order schemes. The results of these will be reported in our forthcoming paper. The dual-pairing (DP) SBP framework [46, 47, 48] was recently introduced to improve the accuracy of FD methods for wave problems and nonlinear conservation laws, in complex geometries. The DP-SBP operators are a pair of high order (backward and forward) difference stencils which together preserve the SBP principle. The DP-SBP operators also have additional degrees of freedom which can be tuned to diminish numerical dispersion errors, yielding the so-called dispersion-relation-preserving (DRP) DP-SBP FD operators [48]. However, the DP-SBP operators are asymmetric and dissipative, can potentially destroy symmetries that exist in the continuum problem. To obtain acceptable results, the DP-SBP operators must be combined in a careful manner so that we can prove numerical stability and preserve model symmetries. Recent applications of DP-SBP operators to nonlinear conservation laws [49, 50] combines the operators with classical finite volume flux splitting [21, 4, 22] so that the upwind features of DP-SBP operators induces some dissipation. The resulting numerical methods [49, 50] are provably linearly stable. As we have recounted linear stability is not sufficient to simulate highly nonlinear phenomena such as turbulence. This is corroborated by the numerical experiments of turbulent flows performed later in this paper and in [49] for linearly stable methods using the DP-SBP operators, where the simulations of the classical Kelvin-Helmholtz instability crashed when the flow became turbulent. In this paper we present the DP upwind SBP FD framework for efficient, accurate and robust numerical approximations of nonlinear conservation laws. The DP SBP FD operators are designed to be upwind, that is they come with some built-in dissipation everywhere, as opposed to DG-SEM which can only induce dissipation through numerical fluxes acting at element interfaces. We combine the DP SBP operators together with skew-symmetric and upwind flux splitting of nonlinear hyperbolic conservation laws. The semi-discrete approximation is conservative and provably entropy-stable for arbitrary nonlinear hyperbolic conservation laws. That is, for smooth solutions it conserves entropy and dissipates entropy when solutions are non-smooth. We give specific examples using the in-viscid Burger’s equation, nonlinear shallow water equations and compressible Euler equations of gas dynamics. We focus primarily on initial value problems (IVP) driven by initial conditions or internal forcing with periodic boundary conditions. Extensive and detailed numerical experiments are presented verifying accuracy, stability and the conservative properties of the method. Long time numerical simulations of merging vortices, barotropic shear instability and the Kelvin-Helmholtz instability, with fully developed turbulence, demonstrate the robustness and accuracy of the framework when compared with the state-of-the-art methods. The rest of the paper is organised as follows. In Section 2 we perform the continuous analysis, give a quick review of skew-symmetric and entropy conserving flux splitting of nonlinear conservation laws. Next, in section 3, we introduce the SBP FD framework for first derivative d/dx and detail the necessary assumptions that define the traditional SBP framework [38, 39] and the DP upwind SBP framework [46, 47, 48]. In section 4 we derive the semi-discrete approximations for an arbitrary conservation and prove the conservative and stability properties of the numerical framework. In section 5 we present detailed numerical experiments verifying accuracy, discrete conservative and stability properties, and the robustness of the proposed method. Section 6 summarises the results of the study and speculates on the direction of future research."
https://arxiv.org/html/2411.06614v1,Kaczmarz Kac Walk,"The Kaczmarz method is a way to iteratively solve a linear system of equations Ax=b. One interprets the solution x as the point where hyperplanes intersect and then iteratively projects an approximate solution onto these hyperplanes to get better and better approximations. We note a somewhat related idea: one could take two random hyperplanes and project one into the orthogonal complement of the other. This leads to a sequence of linear systems A^{(k)}x=b^{(k)} which is fast to compute, preserves the original solution and whose small singular values grow like \sigma_{\ell}(A^{(k)})\sim\exp(k/n^{2})\cdot\sigma_{\ell}(A).","1.1. The Kaczmarz method. The Kaczmarz method [11] is an iterative method for solving linear systems of equations proposed in 1937. Let A\in\mathbb{R}^{m\times n}, m\geq n, be a given matrix, either square or overdetermined. We assume that A:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m} is injective and that Ax=b, where x\in\mathbb{R}^{n} is unknown and b\in\mathbb{R}^{m} is a known right-hand side. Using A_{1},\dots,A_{m}\in\mathbb{R}^{n} to denote the rows of A, Kaczmarz proposes to interpret the linear system Ax=b geometrically and to think of the solution x as the unique point in the intersection of the hyperplanes H_{1},\dots,H_{m}\subset\mathbb{R}^{m} where H_{i}=\left\{w\in\mathbb{R}^{n}:\left\langle A_{i},w\right\rangle=b_{i}\right% \}\qquad\mbox{for}~{}1\leq i\leq m. xz\pi_{i}zH_{i} Figure 1. Projection \pi_{i}y onto H_{i} given by \left\langle a_{i},w\right\rangle=b_{i}. The Pythagorean Theorem implies that, if z\in\mathbb{R}^{n} is any point and \pi_{i}:\mathbb{R}^{n}\rightarrow H_{i} is the orthogonal projection onto any hyperplane, then \|x-\pi_{i}z\|\leq\|x-z\| with equality if and only if z\in H_{i} (see Fig. 1). By iteratively applying projections onto the m hyperplanes, we iteratively decrease the distance to the solution x. Each individual projection is fast to compute since \pi_{i}y=y+\frac{b_{i}-\left\langle A_{i},y\right\rangle}{\|A_{i}\|^{2}}A_{i}. This has made the method popular when dealing with very large matrices: it is a row-based approach that does not require to load the entire matrix into memory. The main remaining question is in which order one should choose the projections/rows. Here, an elegant solution was given by Strohmer–Vershynin [26] who proposed to pick the projections randomly with likelihood proportional to the size of the row when interpreted as a vector in \ell^{2}(\mathbb{R}^{n}). Theorem (Strohmer–Vershynin). If the projection onto hyperplane H_{i} is chosen with likelihood \|A_{i}\|^{2}/\|A\|_{F}^{2}, then \displaystyle\mathbb{E}~{}\|x_{k}-x\|^{2}\leq\left(1-\frac{\sigma_{\min}(A)^{2% }}{\|A\|_{F}^{2}}\right)^{k}\|x_{0}-x\|^{2}, where \sigma_{\min} is the smallest singular value of A. This rate is known to be optimal [20]. The Kaczmarz method has been used in a wide variety of settings, we mention early related work of Agmon [1], Motzkin [17] and more recent work [2, 3, 4, 5, 6, 11, 12, 13, 14, 15, 18, 21, 22, 23, 24, 25, 27]. 1.2. Kaczmarz Kac Walk A basic idea when trying to solve a linear system of equations Ax=b is to replace it by another system A^{*}x=b^{*} which has the same solution but a ‘better’ matrix A^{*}. Returning to the main idea behind the Kaczmarz method, suppose we have two hyperplanes \left\langle A_{i},x\right\rangle=b_{i}\qquad\mbox{and}\qquad\left\langle A_{j% },x\right\rangle=b_{j} with rows normalized to length 1, i.e. \|A_{i}\|=1=\|A_{j}\|. One could take a linear combination of the two and replace any of the two equations by that linear combination. A particularly natural choice is to project one of the hyperplanes onto the orthogonal complement of the other and then renormalizing, that is \left\langle\frac{A_{j}-\left\langle A_{i},A_{j}\right\rangle A_{i}}{\|A_{j}-% \left\langle A_{i},A_{j}\right\rangle A_{i}\|},x\right\rangle=\frac{b_{j}-% \left\langle A_{i},A_{j}\right\rangle b_{i}}{\|A_{j}-\left\langle A_{i},A_{j}% \right\rangle A_{i}\|}. (1) This obviously requires A_{i} and A_{j} to be linearly independent or, since both are normalized to \|A_{i}\|=1=\|A_{j}\|, we only need A_{i}\neq\pm A_{j}. Equation (1) requires the computation of a single inner product: A_{i} and A_{j}-\left\langle A_{i},A_{j}\right\rangle A_{i} are orthogonal, the Pythagorean Theorem implies \|A_{j}-\left\langle A_{i},A_{j}\right\rangle A_{i}\|=\sqrt{1-\left\langle A_{% i},A_{j}\right\rangle^{2}}. This suggests the following procedure. Kaczmarz Kac Walk. For A\in\mathbb{R}^{m\times n} with rows normalized to length 1, pick two rows i\neq j uniformly at random and if A_{i}\neq\pm A_{j}, A_{j}\leftarrow\frac{A_{j}-\left\langle A_{i},A_{j}\right\rangle A_{i}}{\sqrt{% 1-\left\langle A_{i},A_{j}\right\rangle^{2}}} (2) as well as b_{j}\leftarrow(b_{j}-\left\langle A_{i},A_{j}\right\rangle b_{i})/\sqrt{1-% \left\langle A_{i},A_{j}\right\rangle^{2}}. This process should make the rows ‘more orthogonal’ and the new linear system should be better conditioned and easier to solve. Some quick numerical experiments show that this indeed happens (see Fig. 1). The question is now whether and in what sense the improvement can be quantified. 1.3. Main Result We now discuss the main result: the procedure is beneficial insofar as it regularizes the matrix along degenerate subspaces where the singular values are <1 (in expectation) while keeping the Frobenius norm invariant. Theorem. Let A\in\mathbb{R}^{m\times n} be invertible with normalized rows, \|A_{i}\|_{\ell^{2}}=1, that are different (A_{i}\neq\pm A_{j} when i\neq j). Use \phi(A)\in\mathbb{R}^{m\times n} to denote the matrix obtained by selecting i\neq j uniformly at random from \left\{1,2,\dots,m\right\} and applying (2). Then, \forall x\in\mathbb{R}^{n}\qquad\mathbb{E}~{}\|\phi(A)x\|^{2}\geq\|Ax\|^{2}+%"
https://arxiv.org/html/2411.06609v1,"On the optimal choice of the illumination function 
 in photoacoustic tomography","This work studies the inverse problem of photoacoustic tomography (more precisely, the acoustic subproblem) as the identification of a space-dependent source parameter. The model consists of a wave equation involving a time-fractional damping term to account for power law frequency dependence of the attenuation, as relevant in ultrasonics. We solve the inverse problem in a Bayesian framework using a Maximum A Posteriori (MAP) estimate, and for this purpose derive an explicit expression for the adjoint operator. On top of this, we consider optimization of the choice of the laser excitation function, which is the time-dependent part of the source in this model, to enhance the reconstruction result. The method employs the A-optimality criterion for Bayesian optimal experimental design with Gaussian prior and Gaussian noise. To efficiently approximate the cost functional, we introduce an approximation scheme based on projection onto finite-dimensional subspaces. Finally, we present numerical results that illustrate the theory.Keywords: optimal experimental design, Bayesian inverse problem, photoacoustic tomography, time-fractional derivative, acoustic attenuation2020 Mathematics Subject Classification: 35R30, 35R11, 35L20, 62F15","Photoacoustic tomography (PAT) is a novel non-invasive imaging technique that combines optical and ultrasound methods to generate high-resolution images of biological tissue, and has recently attracted much interest. In PAT, short pulses of laser light are emitted into the tissue. This causes a rapid thermal expansion and generates ultrasonic waves through the photoacoustic effect. The laser-induced ultrasonic waves propagate through the tissue and are captured by an array of detectors surrounding the tissue’s surface. These measurements are then processed to reconstruct the optical absorption properties of the tissue, providing detailed information about its internal structure [66, 49, 72]. The propagation of acoustic pressure in PAT is typically described by acoustic wave equations of the form (1.1) c_{0}^{-2}\partial^{2}_{t}p-\Delta p=\partial_{t}J, on a space-time domain (0,T)\times\Omega, where \Omega is a bounded domain in \mathbb{R}^{d} or \Omega=\mathbb{R}^{d}, d\in\{2,3\}. The medium is assumed to be at rest initially and suitable boundary conditions are imposed in case \Omega\not=\mathbb{R}^{d}. Here, p=p(t,x) is the acoustic pressure at location x and time t, c_{0}=c_{0}(x) is the space-dependent sound speed, assumed to be positive, and J=J(t,x) is the heating function defined as the thermal energy converted per unit volume and per unit time [70]. Up to a positive factor, the heating function J is of the form J(t,x)=i(t)a(x), where i represents the temporal profile of the illumination used in the experiment and a is the absorption density function to be identified. The inverse problems of photoacoustic tomography is to determine the absorption density function a, using measurements of the acoustic pressure on an array of transducers represented by a surface \Sigma immersed in the acoustic domain \Omega (1.2) p=p_{\text{obs}}\text{ on }(0,T)\times\Sigma. Usually, a very short laser pulse is used for illumination, i.e. i(t)\approx\delta_{0}(t) where \delta_{0} is the Dirac function concentrated at t=0. With this approximation, the model for PAT becomes (1.3) \left\{\begin{array}[]{r@{\text{ }}ll}c_{0}^{-2}\partial^{2}_{t}p-\Delta p&=0&% \text{ in }(0,T)\times\Omega,\\ p(0,\cdot)=a(x),\partial_{t}p(0,\cdot)&=0&\text{ on }\Omega.\end{array}\right. with boundary conditions. From an inversion perspective, the unknown source term is treated as an initial condition, for which various reconstruction techniques have been explored in, e.g., [7, 20, 29, 66, 61]. Nevertheless, over the past decade, intensity-modulated continuous-wave (IM-CW) lasers have gained popularity as a possibly preferred choice for optical excitation in PAT. While short pulses are acknowledged for yielding better results with the same light energy, IM-CW lasers demonstrate their advantages through their compactness and cost-efficiency [50, 56]. Considering a source identification formulation in time domain allows to capture a wide range of illumination functions, including delta pulse like and CW-like choices. In addition, from a practical standpoint, attenuation effects are crucial, as neglecting them can lead to distortions and artifacts in the reconstructed images [61, 31]. To address this issue, we adopt an attenuated model for photoacoustic tomography in the time domain, employing a fractionally damped wave equation (1.4) \left\{\begin{array}[]{r@{\text{ }}ll}c_{0}^{-2}\partial^{2}_{t}p-\Delta p-{b_% {0}\partial_{t}^{\alpha}\Delta p}&=a(x)i^{\prime}(t)&\text{ in }(0,T)\times% \Omega,\\ p&=0&\text{ on }(0,T)\times\partial\Omega,\\ p(0,\cdot)=\partial_{t}p(0,\cdot)&=0&\text{ in }\Omega.\end{array}\right. Here \partial_{t}^{\alpha} is a time-fractional derivative of order \alpha\in[0,1], cf. Section 2, b_{0} derives from the diffusivity of sound and we assume {c_{0}>0,\ b_{0}\geq 0\text{ a.e. on }\Omega,\quad c_{0},\ c_{0}^{-1},\ b_{0},% \ b_{0}^{-1}\ \in L^{\infty}(\Omega),\quad c_{0}\equiv c,\ b_{0}\equiv b\text{% a.e. on }\Omega\setminus\Omega_{0},} that is, b_{0} and c_{0} take constant values outside the imaging domain \Omega_{0}\subseteq\Omega and are bounded away from zero and infinity on \Omega. Our aim in this work is to study the identification problem of the absorption density a=a(x) in the fractionally damped model (1.4). In addition, we investigate the influence of the intensity function i=i(t) on the reconstruction result. More precisely in the context of optimal experimental design, we seek to determine an optimal laser intensity setup so that the quality of the reconstructed parameter is maximized in a certain sense. Related works The problem of PAT has been attracting much interest during the last decade, both from a mathematical perspective and in its practical applications. Various approaches have been employed to solve PAT, for instance time reversal methods [35, 66], and direct methods involving the adjoint problem [11, 30]. Studies on the latter class of methods employ both discretize-then-adjoin [65] and adjoin-then-discretize approaches [7, 11], where the adjoint operator is explicitly described in its continuous form and remains independent of the discretization process. Attenuation models have been extensively studied to account for the loss of acoustic energy as photoacoustic waves travel through tissues or other media. These models are often formulated in the frequency domain, where attenuation causes high-frequency components to dissipate more quickly over distance, see [61, 20]. To derive the corresponding lossy wave equation in the time domain, fractional derivatives in time, which also capture memory effects, are involved [34, 64]. Inverse problems with fractional wave models have been investigated [41], while efficient numerical methods for solving fractionally damped wave equations are explored in studies such as [45, 10]; we also refer to the recent book [38]. These operators effectively capture the frequency-dependent attenuation by modeling the dispersive and dissipative behavior of wave propagation over time, see also [43] for details. Tuning the choice of the intensity function in the model falls within the scope of optimal experimental design (OED) for parameter identification. There exists an extensive amount of literature focused on OED for various types of parameter identification problems. OED for infinite-dimensional Bayesian inverse problems has been introduced in [3, 4] with a focus on the two most common criteria, namely A- and D-optimality criteria. In the particular setting with Gaussian noise and Gaussian prior distribution for the parameter, the A-optimality criterion minimizes the trace of the posterior covariance matrix, while the D-optimality criterion maximizes the determinant of the posterior precision matrix (inverse covariance). Detailed interpretation of these criteria can be found in [2]. It is known that OED for infinite-dimensional inverse problems is challenging from both mathematical and computational points of view. Computing the cost functional in (4.1) requires the trace of the inverse of an an operator whose dimension is typically very large [4, 48]. Hence, efficient methods to solve these problems have been addressed by approaches typically relying on low-rank approximation of the forward operator. Another option is to use a random estimator for the trace in (4.1), see [4], or trace on the observation space [48]. We refer to [2] for a detailed review. From a practical perspective, application of OED to various problems has been explored, for instance inference of the permeability in porous medium flow [4], source identification in contaminant transport [5], iron loss in electrical machines [32], magnetorelaxometry imaging [33], stellarator coil design [26], and electrical impedance tomography [36]. Contributions The main contributions of this work are twofold. (1) We study the problem of photoacoustic tomography in lossy media in the time domain. The parameter to be identified in this setting, namely the absorption density, is treated as a space-dependent factor within a PDE source term. This model can be considered as an extension of the one in [45], with the added capability of incorporating the laser intensity function used for excitation. The inverse problem is approached within a Bayesian framework, where we compute the MAP estimate. We follow the adjoin-then-discretize approach and derive an explicit expression for the adjoint state by introducing a suitable adjoint problem. Additionally, in the discretized setting, we compare different choices of priors, some of which are edge-promoting. (2) We formulate the optimal experimental design problem in a Bayesian setting, where both parameter and observation space are considered to be infinite-dimensional. Specifically, we consider the A-optimality criterion in a setting with Gaussian noise and a Gaussian prior, which results in minimizing the trace of the covariance operator in the posterior distribution. Since directly computing the trace is known to be computationally challenging due to the need to invert a large dense matrix, we introduce an approximation scheme that projects the data misfit Hessian onto finite-dimensional subspaces. We analyze the convergence properties of this projection scheme, establishing conditions under which the solutions of the approximated OED problems converge to the solution of the original infinite-dimensional problem, thus demonstrating the stability and consistency of our approach. Finally, in the discretized setting, we illustrate performance of the projection scheme with a compatible choice of finite-dimensional subspaces. Organization of the paper The paper is organized as follows. In Section 2 we introduce some basic definitions and concepts which will be used throughout the paper. Section 3 is devoted to studying the forward and inverse problems of PAT in a fractional setting, also introducing an infinite-dimensional Bayesian framework for the inverse problem. Section 4 focuses on optimizing the laser intensity function for PAT. In Section 5, we discuss discretization of the infinite-dimensional problem and provide a suitable computational framework. Finally, in Section 6 we present some numerical results in order to illustrate our theory."
https://arxiv.org/html/2411.06488v1,Numerical analysis of the Cahn–Hilliard cross-diffusion model in lymphangiogenesis,"In this paper, a fully discrete finite element numerical scheme with a stabilizer for the Cahn–Hilliard cross-diffusion model arising in modeling the pre-pattern in lymphangiogenesis is proposed and analysed. The discrete energy dissipation stability and existence of the numerical solution for the scheme are proven. The rigorous error estimate analysis is carried out based on establishing one new L^{\frac{4}{3}}(0,T;L^{\frac{6}{5}}(\Omega)) norm estimate for nonlinear cross-diffusion term in the error system uniformly in time and spacial step sizes. The convergence of the numerical solution to the solution of the continuous problem is also proven by establishing one new L^{\frac{4}{3}}(0,T;W^{1,\frac{6}{5}}(\Omega)) norm estimate for the approximating chemical potential sequence, which overcomes the difficulty that here we can not obtain l^{2}(0,T;H^{1}(\Omega)) estimate for the numerical chemical potential uniformly in time and spacial step sizes because of the nonlinear cross-diffusion characterization of the Cahn–Hilliard cross-diffusion model. Our investigation reveals the connection between this cross-diffusion model and the Cahn–Hilliard equation and verifies the effectiveness of the numerical method. In addition, numerical results are presented to illustrate our theoretical analysis.","The modeling of lymphangiogenesis [25, 21], arising in different backgrounds, is relatively recent. For example, an ODE system is presented for describing the wound healing lymphangiogenesis [2, 4]; a diffusion system with haptotaxis and chemotaxis is analyzed for the tumor lymphangiogenesis [15] ; a reaction-diffusion-convection system models embryo lymphangiogenesis of zebrafish [26]; and the development of the lymphatic network has been modeled in [23]. Recently, a Cahn–Hilliard(CH) cross-diffusion model with energy structure is proposed for lymphangiogenesis [22] using the idea in [17], and structure-preserving numerical schemes have been introduced [18]. Like the phase-field models [11], an energy dissipation law can be shown for the Cahn–Hilliard cross-diffusion model. While various numerical analysis results and error estimates have been derived for the phase-field models [1, 7, 8, 9, 19, 20], there are few about the cross-diffusion model [10]. It is a significant task to research the efficiency of the numerical scheme and study how to design accurate numerical schemes for the Cahn–Hilliard cross-diffusion models. In this paper, we present error estimates and study the convergence of a numerical scheme for the following Cahn–Hilliard cross-diffusion model arising in modeling the pre-pattern in lymphangiogenesis (1) \displaystyle\partial_{t}\phi \displaystyle=\operatorname{div}(\nabla\mu-c\nabla h_{c}(\phi,c)),\quad x\in% \Omega,\ 0<t\leq T, (2) \displaystyle\partial_{t}c \displaystyle=-\operatorname{div}(c(\nabla\mu-c\nabla h_{c}(\phi,c)))+% \operatorname{div}(\nabla h_{c}(\phi,c)),\quad x\in\Omega,\ 0<t\leq T, (3) \displaystyle\mu \displaystyle=-\Delta\phi+\varepsilon^{-2}f(\phi)+h_{\phi}(\phi,c)\quad\mbox{% in }{\Omega},\quad x\in\Omega,\ 0<t\leq T with periodic or homogenous Neumann boundary conditions and the initial data (4) \displaystyle\phi(\cdot,0)=\phi_{0},\quad c(\cdot,0)=c_{0},\quad x\in\Omega. Here, \Omega is the bounded domain of \mathbb{R}^{d} (d=1,2,3) with the smooth boundary \partial\Omega and T>0 is the final time. The cross-diffusion system (1)-(3) in [18], based on the model of [22], is proposed to model lymphangiogenesis, which denotes the expansion and formation of new lymphatic networks by lymphatic endothelial cells sprouting from existing networks and by migration of lymphatic endothelial cells via the interstitial flow [24, 5]. In [18], a quasi-convex splitting, motivated by [12, 13], and fully discrete finite-element scheme has been proposed and analyzed for the model (1)-(3) with the fluid-collagen interaction energy of one logarithmic potential’s type [14] and the nutrient energy density h(\phi,c) given by [16] (5) \displaystyle h(\phi,c)=c^{2}/2-c\phi,\,g(c)=1, In this case, since h_{c} is linear with respect to \phi and c, we may simply use the following homogenous bounday condition (6) \displaystyle\frac{\partial\phi}{\partial n}=\frac{\partial c}{\partial n}=% \frac{\partial\mu}{\partial n}=0,\quad x\in\partial\Omega,\ 0\leq t\leq T. Notice that the second-order derivative of the energy density f is generally unbounded. However, in this paper we assume f^{\prime} is bounded with f(\phi)=F^{\prime}(\phi) for some regular potential F and analysis an approximate model by applying a truncation technique to the potential function F. In addition, we assume that there exists the positive constants K_{i}>\varepsilon^{2},\,i=1,2 and L\geq 1 such that (7) \displaystyle F(\phi)\geq K_{1}\phi^{2}-K_{2},\,\forall\phi\in\mathbb{R^{d}}, (8) \displaystyle|f^{\prime}(\phi)|\leq L,\,\forall\phi\in\mathbb{R^{d}}. For example, we may use the double-well potential with truncation in [16] \displaystyle F(\phi) \displaystyle=\begin{cases}(1-\phi)\ln(1-\phi)+\frac{(\phi-\delta)^{2}}{2% \delta}+(\ln\delta+1)(\phi-\delta)+\delta\ln\delta&\mbox{if }\phi\leq\delta,\\ \phi\ln\phi+(1-\phi)\ln(1-\phi)&\mbox{if }\delta<\phi<1-\delta,\\ \phi\ln\phi+\frac{(\phi-1+\delta)^{2}}{2\delta}-(\ln\delta+1)(\phi-1+\delta)+% \delta\ln\delta&\mbox{if }1-\delta\leq\phi,\end{cases} or (9) F(s)=\left\{\begin{array}[]{lr}\begin{aligned} &\frac{3M^{2}-1}{2}(s-M)^{2}+(M% ^{3}-M)(s-M)+\frac{1}{4}(M^{2}-1)^{2},&&s\geq M,\\ &\frac{1}{4}(s^{2}-1)^{2},&&s\in[-M,M],\\ &\frac{3M^{2}-1}{2}(s+M)^{2}-(M^{3}-M)(s+M)+\frac{1}{4}(M^{2}-1)^{2},&&s\leq-M% .\end{aligned}\end{array}\right. The rest of this paper is organized as follows. One fully discrete finite-element numerical scheme is proposed and the energy dissipation stability is proven in Section 2. The existence of the solution to the numerical scheme is proven in Section 3. The rigorous error analysis with error estimates is carried out in Section 4 and the convergence of the numerical solution to the solution of the continuous problem is proven in Section 5. Some numeric results are given in Section 6, and conclusion is stated in Section 7."
https://arxiv.org/html/2411.06476v1,Eigen-componentwise convergence of SGD for quadratic programming,"Stochastic gradient descent (SGD) is a workhorse algorithm for solving large-scale optimization problems in data science and machine learning. Understanding the convergence of SGD is hence of fundamental importance. In this work we examine the SGD convergence (with various step sizes) when applied to unconstrained convex quadratic programming (essentially least-squares (LS) problems), and in particular analyze the error components respect to the eigenvectors of the Hessian. The main message is that the convergence depends largely on the corresponding eigenvalues (singular values of the coefficient matrix in the LS context), namely the components for the large singular values converge faster in the initial phase. We then show there is a phase transition in the convergence where the convergence speed of the components, especially those corresponding to the larger singular values, will decrease. Finally, we show that the convergence of the overall error (in the solution) tends to decay as more iterations are run, that is, the initial convergence is faster than the asymptote.","SGD is an optimization algorithm originally proposed by Robbins and Monroe [19] and is widely used in current machine learning practices [2, 5, 23]. In SGD, if we want to minimize the objective function taking the form F(x)=\frac{1}{N}\sum_{i=1}^{N}f_{i}(x), each time we choose i with a certain probability from 1\ldots N and then update x_{k} by setting x_{k+1}=x_{k}-\alpha_{k}\nabla f_{i}(x_{k}), where \alpha_{k} is the step size, or learning rate, we use at the k-th step. Compared with the classical steepest descent, or gradient descent (GD) algorithm, SGD is more efficient per iteration. However, such advantage comes at a cost [6]: when the objective function is strongly convex and smooth, using GD with fixed step size offers a linear convergence rate, while using SGD with fixed step size usually cannot guarantee convergence (in expectation) to the minimum. To do so, we need to set our step size to decay to 0, but that results in sub-linear convergence. Still, when the objective function is overly complicated, as happens for example in training neural networks, SGD is still useful because calculating individual \nabla f_{i}(x) is significantly easier than \nabla F(x). SGD has been studied extensively over the last decade [1, 12, 13, 21, 24]. In this paper, to understand the behavior of SGD in terms of eigen-componentwise convergence (see below for a precise definition), we focus on a simple but nontrivial class of problems, namely linear least-squares (LS) problems \min_{x}\frac{1}{2}\|Ax-b\|^{2} where A\in\mathbb{R}^{M\times N} and M,N are large. One approach for solving consistent least-squares problems is the randomized Kaczmarz algorithm [9, 15, 20], initially proposed by Kaczmarz [11] in 1937. The basic idea is that for a least-squares problem \min_{x}\frac{1}{2}\|Ax-b\|^{2}, we project x_{k} to the hyperplane \langle a_{i},x\rangle=b_{i} each time, where a_{i} represents the i-th row of A and i cycles from 1 to M, the number of rows A has. In subsequent work, Strohmer and Vershynin [26] propose a variant of randomized Kaczmarz algorithm and proved to have a linear convergence rate. Their idea is instead of choosing i cyclically, we choose row i with probability \|a_{i}\|^{2}/\|A\|_{F}^{2} at each step. Until now, Randomized Kaczmarz algorithm has been thoroughly investigated and improved [14, 16, 18]. The motivation of our paper comes from Steinerberger [25], in which he shows (among other results) that when randomized Kaczmarz is applied to an LS problem \min_{x}\frac{1}{2}\|Ax-b\|^{2}, the eigencomponentwise error \langle x_{k}-x_{*},v_{\ell}\rangle will converge significantly faster to 0 for smaller \ell, where v_{\ell} denotes the (right) singular vector of A with corresponding singular value \sigma_{\ell}. Equivalently, v_{\ell} is the \ell-th eigenvector of the Hessian A^{\top}A. The goal of this paper is to extend Steinerberger’s result to SGD applied to general LS problems. More specifically, our analysis applies to general step sizes (and not speficially the one chosen by Randomized Kaczmarz), and LS problems that are not consistent; thereby getting the setting closer to the practical SGD usage. First, we introduce the eigen-componentwise convergence of (deterministic) GD on LS problems by giving an equality for \langle x_{k}-x_{*},v_{\ell}\rangle. Then we explore the eigen-componentwise convergence of SGD on LS problems with three different step sizes: fixed step size, 1/k decaying step size, and 1/k^{\gamma} decaying step size. In each of these cases, we first give an upper bound of \mathbb{E}[\langle x_{k}-x_{*},v_{\ell}\rangle] to show that the components for large singular values converge faster in the initial phase, and we show that our predictions of \langle x_{k}-x_{*},v_{\ell}\rangle is generally accurate in the initial phase. Then we give an upper bound of \mathbb{E}[\langle x_{k}-x_{*},v_{\ell}\rangle^{2}] to illustrate the existence of phase transition after which the difference of the convergence speed of different components is no longer obvious. Finally, we use the two bounds to show that the error in the solution tends to decay as more iterations are run. Our results are important in several ways. First, both LS problems and SGD are common in a great variety of contexts and have been heavily explored for a long time [4, 7, 8, 10, 22]. Traditional randomized Kaczmarz algorithm does a good job at solving LS problems, but only when the problem is consistent [3]. In our work, we give a detailed analysis applying SGD on LS problems with decaying step size, which can guarantee convergence to the optimal solution by bounding \mathbb{E}[\langle x_{k}-x_{*},v_{\ell}\rangle]. Also, by giving a bound of \mathbb{E}[\langle x_{k}-x_{*},v_{\ell}\rangle^{2}], we show the existence of phase transition, which in turn explains the reason why the convergence rate of the overall error in the solution decreases as the number of iterations increases. That actually gives us another favorable property of SGD. In machine learning practices, we are not actually aiming for the minimal solution to the objective function as that could lead to various problems, like overfitting. So a common practice is choosing an optimization algorithm and iterates until the value of the objective function falls into a particular threshold. Our results show that the convergence speed of SGD is usually faster in the initial phase, therefore useful to practical purposes. Throughout the paper, we use \|\cdot\| and \|\cdot\|_{F} to denote the 2-norm and the Frobenius norm respectively."
https://arxiv.org/html/2411.06289v1,Systematic design of compliant morphing structures: a phase-field approach,"We investigate the systematic design of compliant morphing structures composed of materials reacting to an external stimulus. We add a perimeter penalty term to ensure existence of solutions. We propose a phase-field approximation of this sharp interface problem, prove its convergence as the regularization length approaches 0 and present an efficient numerical implementation. We illustrate the strengths of our approach through a series of numerical examples.","Advances in additive manufacturing and synthesis of complex “active” materials whose properties can be altered through external stimuli are opening the door to a new generation of integrated devices and materials. While manufacturing such structures or materials has received a considerable attention (see for instance [1, 2]), their actual design remains challenging. Starting from the pioneering work of [3, 4, 5], topology optimization has established itself as a powerful tool for systematic design of micro-devices, MEMS, or materials microstructures. Here, the goal is to algorithmically find the distribution of materials in a ground domain that optimizes an objective function [6]. It is well-known that such problems generally do not admit a “classical” solution (see [7] for instance) resulting in optimal designs consisting of an infinitely fine mixture of multiple materials. Homogenization approaches [8, 9, 7] tackle this problem directly by extending admissible designs to such mixtures. This type of approach is mathematically well grounded and leads to well-posed problems that can be implemented efficiently. However, it is often criticized for leading to designs that cannot be manufactured. Several other classes of techniques aim at restricting the class of admissible designs in such a way that avoids fine mixtures. The combination of material interpolation (SIMP) and filters [10, 11] is a commonly employed approach. Shape parameterization by level set functions [12, 13] also limits the complexity of designs. Finally, by penalizing the length (or surface) of interfaces between materials, perimeter penalization [14, 15, 16] also produces designs with limited complexity. Additionally, perimeter penalization can be efficiently implemented using a phase-field approach [17, 18, 19]. In this article, we propose a phase-field algorithm for the systematic design of active structures achieving prescribed deformations under some unknown distributions of a stimulus. Our focus is on linear elastic materials in which an external stimulus can generate an isotropic inelastic strain, similar to linear thermo-elastic materials. Section 2 is devoted to the mathematical analysis of the problem and its phase-field approximation. A numerical scheme is proposed in Section 3 and illustrated by a series of numerical simulations in Section 4."
https://arxiv.org/html/2411.06250v1,Improved Convergence and Approximation properties of Baskakov-Durrmeyer Operators,"In this paper, we describe two novel changes to the Baskakov-Durrmeyer operators that improve their approximation performance. These improvements are especially designed to produce higher rates of convergence, with orders of one or two. This is a major improvement above the linear rate of convergence commonly associated with conventional Baskakov-Durrmeyer operators. Our research goes thoroughly into the approximation features of these modified operators, providing a thorough examination of their convergence behavior. We concentrate on calculating precise convergence rates, providing thorough error estimates that demonstrate the new operators’ efficiency as compared to the classical version. In addition, we construct Voronovskaja-type formulae for these operators, which provide insights into the asymptotic behavior of the approximation process as the operator’s degree grows. By exploring these aspects, we demonstrate that the proposed modifications not only surpass the classical operators in terms of convergence speed but also offer a more refined approach to error estimation, making them a powerful tool for approximation theory.Keywords: Baskakov-Durrmeyer operators, linear positive operators, Voronovskaja type asymptotic result.Mathematics Subject Classification(2010): 40A05, 41A10, 41A25, 41A35, 41A60.","V. A. Baskakov [Baskakov1957] defined a basic family of linear positive operators in approximation theory, called Baskakov operators, for f\in C[0,\infty) in following form: \displaystyle V_{n}(f;x)= \displaystyle\sum_{k=0}^{\infty}p_{n,k}(x)\,f\left(\dfrac{k}{n}\right), (1.1) \displaystyle\text{where}\,p_{n,k}(x)= \displaystyle\binom{n+k-1}{k}\dfrac{x^{k}}{(1+x)^{n+k}}, (1.2) and C[0,\infty) is the class of all continuous functions on [0,\infty). These operators belong to a larger family of positive linear operators that includes well-known operators such as Bernstein [Bernstein1912] and Szász-Mirakjan [szasz1950general]. This makes them especially helpful in circumstances when precise function representations are unknown or too difficult to work with directly. Baskakov operators employ probabilistic approaches and binomial-type weights to perform function approximation, making them very efficient in certain applications. Baskakov operators are used in practical applications which require smooth approximations or interpolations. One significant use is numerical analysis, where they contribute to approximate complicated functions for faster calculation, particularly in tasks like as numerical integration or differentiation. These operators are also employed in signal processing to recreate smooth signals from noisy or partial data, hence reducing distortions. Although classical Baskakov operators are effective in many approximation applications, but still they have several limitations, such as a sluggish linear rate of convergence and issues approximating functions with greater levels of smoothness. To address these restrictions, researchers created a variety of modifications and generalizations in order to increase their efficiency, widen their applicability, and overcome inherent limits, each adapted to individual purposes. The Baskakov-Durrmeyer operators [sahai1985simu] are well-known generalizations that combine the qualities of both Baskakov and Durrmeyer operators and contain an integral form, making it especially good for approximating smoother functions with better precision. Sahai and Prasad [sahai1985simu] defined these operators in the following way: \displaystyle D_{n}(f;x) \displaystyle=(n-1)\sum_{k=0}^{\infty}p_{n,k}(x)\int_{0}^{\infty}p_{n,k}(t)\,f% (t)\,dt, (1.3) and studied their approximation properties. Due to its well known properties, researchers defined other Durrmeyer type operators as one see [finta2008dur, gupta2014different] and the references therein. Another notable generalization is the introduction of q-Baskakov operators, which generalizes the conventional form via q-calculus, giving additional freedom in controlling convergence behavior and approximation quality. One can see the literature on these generalizations by [agrawal2014appro, aral2010durr, aral2011gene, gupta2011some] Similarly, Stancu-Baskakov operators incorporate an additional parameter, allowing for better control and adaptability, particularly when dealing with functions of varying smoothness. Also, Boehme and Brucner defined the Baskakov-Kantorovich operators [boeh1964], which incorporate Kantorovich variant characteristics, resulting in improved handling of non-smooth functions and more precise error estimates. A lot of research is done on these operators [abel2003esti, totik1985satu] and their citations. Extensions of bivariate and multivariate Baskakov operators have been created to approximate functions of several variables, which is beneficial in complicated applications like image processing and computer-aided geometric design. The order of approximation of the operators must be improved to satisfy the increasing need for higher precision, faster convergence, and enhanced computing efficiency across a wide range of domains. Classical operators, such as the Baskakov or Bernstein operators, often have a slow linear rate of convergence, limiting their ability to approximate complex or smooth functions with acceptable accuracy. Improving the order of these operators not only produces more accurate approximations, but it also speeds up the convergence process, lowering computing costs and increasing time efficiency. This is especially crucial in applications like numerical simulations, solving differential and integral equations, and data interpolation, where little inaccuracies can result in considerable variances. Higher-order operators are also more appropriate for approximating smooth functions with high regularity, as well as handling functions with irregularities such as discontinuities or sharp gradients. Furthermore, they provide better error estimates, tighter control over approximation accuracy, and higher flexibility, making them more dependable in applications including signal processing, picture reconstruction, and machine learning. Overall, enhancing the order of approximation operators solves the constraints of traditional approaches, allowing for more exact, economical, and adaptive solutions to current mathematical and computing issues. Recently, in [khosravian2018new], Khosravian-Arab et al. modified the well known Bernstein operators by using a new technique to improve their degree of approximation. Following this, Acu et al. [Acu2019] have applied this approach on the Bernstein-Durrmeyer operators. In another paper [gupta2019modi], same authors have put it on the Bernstein-Kantorovich operators too. Similarly, Kajla and Acar [kajla2019modified] have modified the \alpha-Bernstein summation operators. Following this, Jabbar and Hassan [jabbar2024bet] modified the Baskakov operators with two modifications and studied their properties. This current research has been devoted to improve their convergence rates, leading to modifications and enhancements of the Baskakov-Durrmeyer operators. These improvements enable faster and more accurate approximations, making Baskakov type operators an even more powerful tool in modern approximation theory and its applications. The present article is organized in the following form: In Section 2, we present the first order modification of classical Baskakov-Durrmeyer operators and study their convergence and asymptotic formula. In the next section, we define another modification whose order of convergence is 2. We also present its Voronovskaja type result."
https://arxiv.org/html/2411.06152v1,On the convection boundedness of numerical schemes across discontinuities,"This short note introduces a novel diagnostic tool for evaluating the convection boundedness properties of numerical schemes across discontinuities. The proposed method is based on the convection boundedness criterion and the normalised variable diagram. By utilising this tool, we can determine the CFL conditions for numerical schemes to satisfy the convection boundedness criterion, identify the locations of over- and under-shoots, optimize the free parameters in the schemes, and develop strategies to prevent numerical oscillations across the discontinuity. We apply the diagnostic tool to assess representative discontinuity-capturing schemes, including THINC, fifth-order WENO, and fifth-order TENO, and validate the conclusions drawn through numerical tests. We further demonstrate the application of the proposed method by formulating a new THINC scheme with less stringent CFL conditions.","Numerical simulations of complex flow systems present significant challenges due to the presence of discontinuities, such as material interfaces in multiphase flows, reaction fronts in combustion, and shock waves in supersonic flows. Numerical methods for resolving discontinuities are generally categorized into tracking and capturing schemes. Discontinuity-capturing schemes are widely used due to their flexibility and ability to extend to higher-order accuracy. However, designing high-resolution discontinuity-capturing schemes is challenging, as Godunov’s theorem states that no linear scheme of higher than second order can maintain monotonicity. Over the decades, significant efforts have been made to overcome Godunov’s barrier and develop non-linear, high-resolution discontinuity-capturing schemes. High-order shock-capturing schemes, such as WENO (Weighted Essentially Non-Oscillatory) [1, 2, 3], CWENO (Central WENO) [4, 5], and TENO (Targeted Essentially Non-Oscillatory) [6, 7], have been successfully developed. Recent advancements [8, 9, 10, 11] have further improved the resolution and robustness of these schemes. Interface-capturing schemes, such as THINC (Tangent Hyperbola for Interface Capturing) [12], have been applied to compressible flows to enhance the resolution of discontinuous flow structures, such as contact discontinuities, through the BVD (Boundary Variation Diminishing) algorithm [13, 14, 15] or discontinuity-detecting criterion [16]. Recent work [17, 18] has demonstrated that existing three-cell-based non-linear schemes can be unified into a single framework, from which a new high-resolution scheme, named ROUND (Reconstruction Operator on Unified Normalized-variable Diagram), has been proposed. Significant efforts have also been made to understand and optimize the numerical properties of non-linear discontinuity-capturing schemes, such as their spectral properties [19] and stability [20]. A general framework based on a quantitative error metric for evaluating shock-capturing schemes has also been developed [21]. To quantify the overshoot error as a function of the CFL number, recent work [22] introduced error metrics for non-linear shock-capturing schemes. However, limited research has addressed the convection boundedness properties of non-linear schemes across discontinuities. Therefore, this study proposes a diagnostic tool to evaluate and improve the convection boundedness of numerical schemes across discontinuities. This work is organised as follows. In Section 2, the convection boundedness criterion across discontinuities and the proposed diagnostic method based on the normalised variable diagram are given. In Section 3, we apply the proposed method to evaluate the representative schemes and validate the conclusions drawn through numerical tests. In Section 4, we demonstrate the application of the proposed method by formulating a new THINC scheme with less stringent CFL conditions. Finally, a brief concluding remark is given in Section 5."
https://arxiv.org/html/2411.06063v1,Predicting band structures for 2D Photonic Crystals via Deep Learning,"Photonic crystals (PhCs) are periodic dielectric structures that exhibit unique electromagnetic properties, such as the creation of band gaps where electromagnetic wave propagation is inhibited. Accurately predicting dispersion relations, which describe the frequency and direction of wave propagation, is vital for designing innovative photonic devices. However, traditional numerical methods, like the Finite Element Method (FEM), can encounter significant computational challenges due to the multiple scales present in photonic crystals, especially when calculating band structures across the entire Brillouin zone. To address this, we propose a supervised learning approach utilizing U-Net, along with transfer learning and Super-Resolution techniques, to forecast dispersion relations for 2D PhCs. Our model reduces computational expenses by producing high-resolution band structures from low-resolution data, eliminating the necessity for fine meshes throughout the Brillouin zone. The U-Net architecture enables the simultaneous prediction of multiple band functions, enhancing efficiency and accuracy compared to existing methods that handle each band function independently. Our findings demonstrate that the proposed model achieves high accuracy in predicting the initial band functions of 2D PhCs, while also significantly enhancing computational efficiency. This amalgamation of data-driven and traditional numerical techniques provides a robust framework for expediting the design and optimization of photonic crystals. The approach underscores the potential of integrating deep learning with established computational physics methods to tackle intricate multiscale problems, establishing a new benchmark for future PhC research and applications.","Photonic crystals (PhCs) are dielectric materials that are constructed from a unit cell periodically repeated with a period size comparable to the wavelength [joannopoulos2008molding]. These materials exhibit the band gap phenomenon, where specific frequency intervals prevent electromagnetic wave propagation. Complete band gaps occur when all polarizations and directions of wave propagation are prohibited, making PhCs promising for innovative photonic devices like optical transistors, photonic fibers, and low-loss optical mirrors [yanik2003all, russell2003photonic, wang2023analytical, labilloy1997demonstration]. The band structure of PhCs describes how electromagnetic wave propagation depends on frequency, polarization, and direction, representing the dispersion relation mathematically. In this paper, we focus on 2-dimensional (2D) periodic PhCs, which are homogeneous along the z axis and have high-contrast dielectric materials embedded in dielectric materials within the xy plane. We propose a novel supervised learning based scheme to achieve an accurate prediction of the dispersion relation for a given unit cell structure of 2D PhCs. Various numerical methods have been employed to calculate photonic band structures, including plane wave expansion methods [ho1990existence, leung1990full], the transfer matrix method [pendry1992calculation, pendry1996calculating], finite difference time domain method [chan1995order], layer Korringa-Kohn-Rostoker method [stefanou1992scattering] and multipole methods [nicorovici1995photonic, botten2001photonic]. In particular, there is wide application of finite element methods in recent years [axmann1999efficient, boffi2006modified, dobson1999efficient, engstrom2010complex, schmidt2009computation, schmidt2010efficient]. However, those numerical methods have to meet the issue of simulating the band structures with potentially high computational costs. The permittivity can vary widely, and the ratio between these values, the so-called contrast, should be large to generate the band gap. This leads to Helmholtz eigenvalue problems with both high-contrast and piecewise constant coefficients, which is numerically challenging. Thus, in the process of designing PhCs structures, it is desirable to establish a bidirectional relationship between the structure and the band gap property efficiently. In the realm of computational mathematics, deep learning models offer a promising alternative to traditional approaches for modeling complex input-output mappings. These models have been successfully applied in various fields, including fluid dynamics [beigzadeh2012prediction, butz2002modelling, mi2001flow] and materials science [cang2018improving, koker2007neural, kondo2017microstructure], showcasing their potential in capturing elusive relationships between material structures and properties. Unsurprisingly, deep learning models have been already utilized in the dispersion relation prediction and the inverse design of PhCs [liu2018training, jiang2022dispersion, yao2019intelligent, tahersima2019deep, 10.1093/jcde/qwad013]. In particular, Jiang et al. [jiang2022dispersion] propose the use of convolutional neural networks (CNNs) and conditional generative adversarial networks (cGANs) to bridge the structure and properties from the forward and inverse directions, respectively. Recent work by Ma et al. [PhysRevX.11.021052] also demonstrates the potential of neural networks in predicting photonic band structures, showcasing how data-driven methods can achieve remarkable efficiency in the exploration of PhC designs. Unlike those existing approaches, we propose a novel supervised learning scheme which leverages the U-Net architecture with transfer learning and Super-Resolution techniques to predict dispersion relations efficiently. This integration of data-driven and traditional computational methods holds the potential to accelerate progress in designing and optimizing PhCs structures. The main features of our scheme are fourfold. (a) It is capable of predicting the entire dispersion relation, or the first several band functions, using one supervised learning scheme. Current deep learning based schemes either use separate neural networks to predict each band function or use one neural network with all the band functions of interest as output [jiang2022dispersion, Christensen2020]. (b) It predicts dispersion relation over the entire first Brillouin zone instead of only on its boundary, and hence is more accurate. (c) By hybridizing the U-Net architecture with transfer learning, our supervised learning scheme can learn effectively from relatively small datasets and reduce the overall training cost significantly [chollet2021deep]. (d) By incorporating a Super-Resolution (SR) model, our scheme has the potential to reduce the cost of generating training data or improve the accuracy of existing noisy data. This represents a huge reduction of computational complexity since traditional numerical methods suffer from high computational cost, which makes the generation of the training data extremely expensive. The remainder of the paper is organized as follows. In Section 2, we revisit the mathematical models used for calculating band functions and outline the primary supervised learning tasks. Then we delve into the neural network architectures employed in our approach in Section 3. The training and testing procedures as well as the performance and accuracy of the proposed model are demonstrated in Section 4. Finally, we consolidate our findings and conclusions in Section 5."
https://arxiv.org/html/2411.06049v1,Geometric Ergodicity and Strong Error Estimates for Tamed Schemes of Super-linear SODEs,"We construct a family of explicit tamed Euler–Maruyama (TEM) schemes, which can preserve the same Lyapunov structure for super-linear stochastic ordinary differential equations (SODEs) driven by multiplicative noise. These TEM schemes are shown to inherit the geometric ergodicity of the considered SODEs and converge with optimal strong convergence orders. Numerical experiments verify our theoretical results.","The long-time behavior of the Wiener process-driven SODE (SDE) \displaystyle\mathrm{d}X(t)=b(X(t))\mathrm{d}t+\sigma(X(t))\mathrm{d}W(t),~{}t% \geq 0, plays a vital role in many scientific areas. As a significant long-time behavior, the ergodicity characterizes the identity of the temporal average and spatial average for a Markov process or chain generated by Eq. (SDE) and its numerical discretization, respectively, which has a lot of applications in quantum mechanics, fluid dynamics, financial mathematics, and many other fields [4, 6]. It is known that the coefficients of most nonlinear SODEs from applications violate the traditional but restrictive Lipschitz continuity assumptions. This paper analyzes integrators for the super-linear Eq. (SDE) whose solution is uniquely ergodic with respect to an equilibrium distribution that (1) are ergodic with respect to the exact equilibrium distribution of Eq. (SDE) on infinite time intervals, (2) strongly converge with optimal convergence rate to the solutions of Eq. (SDE) on any finite time intervals, and (3) require a small amount of computational costs. It is known, even for the particular Langevin system, that pure sampling methods can accomplish (1), but they typically do not approximate the solution to Eq. (SDE); see, e.g., [1]. Integrators for Eq. (SDE) certainly satisfy (2), but they are often divergent on infinite-time intervals or ergodic concerning a different equilibrium distribution [15, 16]. The backward Euler method and stochastic theta method in, e.g., [10, 13, 14, 15] satisfy (1) and (2), but they usually require a significant amount of computational cost. We will show that a family of TEM methods constructed in this paper, as explicit schemes, can simultaneously accomplish these three goals. To construct explicit methods that can inherit the unique ergodicity of Eq. (SDE), including the tamed and truncated methods studied in, e.g., [2, 8, 9, 17], the numerical Lyapunov structure plays a key role. However, it was shown in [7] that the classical Euler–Maruyama (EM) scheme applied to Eq. (SDE) with super-linear growth coefficients would blow up in p-th moment for all p\geq 2. In particular, the square function is not an appropriate Lyapunov function of the EM scheme, even though it is a natural Lyapunov function of the considered monotone Eq. (SDE). Our main aim in this paper is to construct a family of explicit TEM schemes to preserve the same Lyapunov structure of the super-linear Eq. (SDE). In combination with the equivalence of the transition probabilities, these TEM schemes are shown to inherit the geometric ergodicity of Eq. (SDE) under certain non-degenerate conditions. To our knowledge, this is the first time constructing explicit schemes that preserve the unique ergodicity of super-linear SODEs driven by multiplicative noise. To establish the second property, it is essential to estimate the effect on the dynamics of the TEM methods through strong error estimates between the TEM methods and Eq. (SDE). By examining the relationship between these error estimates and the step size, we prove that the expected optimal strong convergence order is 1/2 in the multiplicative noise case and 1 in the additive noise case. The outline of the paper is as follows. In Section 2, we propose the TEM schemes and develop their probabilistic regularity properties. The strong error estimates between the TEM schemes and the SODEs are explored in Section 3. In Section 4, we validate the theoretical results by numerical experiments."
https://arxiv.org/html/2411.06030v1,A novel study on the MUSIC-type imaging of small electromagnetic inhomogeneities in the limited-aperture inverse scattering problem,"We apply MUltiple SIgnal Classification (MUSIC) algorithm for the location reconstruction of a set of two-dimensional circle-like small inhomogeneities in the limited-aperture inverse scattering problem. Compared with the full- or limited-view inverse scattering problem, the collected multi-static response (MSR) matrix is no more symmetric (thus not Hermitian), and therefore, it is difficult to define the projection operator onto the noise subspace through the traditional approach. With the help of an asymptotic expansion formula in the presence of small inhomogeneities and the structure of the MSR-matrix singular vector associated with nonzero singular values, we define an alternative projection operator onto the noise subspace and the corresponding MUSIC imaging function. To demonstrate the feasibility of the designed MUSIC, we show that the imaging function can be expressed by an infinite series of integer-order Bessel functions of the first kind and the range of incident and observation directions. Furthermore, we identify that the main factors of the imaging function for the permittivity and permeability contrast cases are the Bessel function of order zero and one, respectively. This further implies that the imaging performance significantly depends on the range of incident and observation directions; peaks of large magnitudes appear at the location of inhomogeneities for permittivity contrast case, and for the permeability contrast case, peaks of large magnitudes appear at the location of inhomogeneities when the range of such directions are narrow, while two peaks of large magnitudes appear in the neighborhood of the location of inhomogeneities when the range is wide enough. The numerical simulation results via noise-corrupted synthetic data also show that the designed MUSIC algorithm can address both permittivity and permeability contrast cases.","In this paper, we consider the fast identification of a set of two-dimensional small inhomogeneities, the dielectric permittivities (or magnetic permeabilities) of which differ from those of homogeneous space in the limited-aperture inverse scattering problem. This is an old but very interesting problem for scientists and engineers because it has a potentially wide application range in physics, geophysics, nondestructive evaluations, material engineering, and medical sciences, e.g., biomedical imaging [1, 2], stroke detection [3, 4], ground-penetrating radar [5, 6], synthetic aperture radar (SAR) imaging [7, 8], breast cancer detection [9, 10], damage detection of civil structures [11, 12], and landmine detection [13, 14]. In essence, the main aim of the limited-aperture inverse scattering problem is the identification of unknown shape, location, or physical properties, which involves estimating the electric conductivity, dielectric permittivity, or magnetic permeability. To solve the problem, various iterative-based schemes have been investigated, e.g., Newton-type methods [15, 16], Gauss-Newton methods [17, 18], level-set techniques [19, 20], the Born iterative method [21, 22], conjugate gradient method [23, 24], Levenberg-Marquardt algorithm [25, 26], and optimization schemes [27, 28]. As we have already seen [29, 30], the iteration process must begin with a good initial guess that is close to the unknown target. If not, serious situations will arise, e.g., non-convergence issues, local minimizing problems, and high computational costs, which means that the success of iterative-based schemes is significantly dependent on the generation of a good initial guess. Accordingly, it is natural to develop both a mathematical theory and a numerical technique for generating a good initial guess and, correspondingly, various non-iterative techniques for identifying the location or imaging the shape of unknown inhomogeneities. The multiple signal classification (MUSIC) algorithm is a well-known non-iterative technique in the inverse scattering problem. Traditionally, MUSIC is used in signal processing problems to estimate the individual frequencies of multiple time-harmonic signals [31], and in pioneering research [32], it has been used to identify the locations of a number of point-like scatterers. MUSIC has also been applied in various inverse scattering problems, e.g., the identification of three-dimensional small inhomogeneities [33], localization of small inhomogeneities hidden in three-dimensional half-space [34], detection of internal corrosion [35], imaging of thin curve-like inhomogeneities [36], perfectly conducting cracks [37], extended targets [38], and anisotropic inhomogeneities [39]. Based on this utility, it has been applied to various real-world problems, such as damage diagnosis on complex aircraft structures [40], eddy-current nondestructive evaluation [41], impedance tomography [42], remote sensing in safety/security applications [43], rebar detection [44], damage imaging of aircraft structures [45], indoor localization problems [46], through-wall imaging [47], super-resolution fluorescence microscopy for single-molecule localization [48], multi-frequency imaging [49], time-reversal MUSIC for the imaging of extended targets [50], imaging of anisotropic scatterers in a multi-layered medium [51], magnetoencephalography (MEG) from human cortical neural activities [52], microwave imaging [53], and breast cancer detection [10]. We also refer to other studies [54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66] for more applications of MUSIC. Let us emphasize that most studies have addressed full- and limited-view inverse problems. There also exists a considerable number of interesting limited-aperture inverse scattering problems; refer to [67, 68, 69, 70, 71, 72, 73, 74, 75, 76] and the references therein. However, to the best of our knowledge, MUSIC has not been applied to the limited-aperture problem. Notice that MUSIC is based on the characterization of the range of the so-called multi-static response (MSR) matrix, which is symmetric but not Hermitian in full- and limited-view inverse problems. The difficulties that arise in the application of MUSIC in the limited-aperture problem come from the non-symmetric property of the MSR matrix. Accordingly, the imaging function of MUSIC is yet to be designed because an appropriate method to generate the projection operator onto the noise subspace does not exist. In this study, we apply the MUSIC algorithm to the limited-aperture inverse scattering problem to identify or image small electromagnetic inhomogeneities, the dielectric permittivities (or magnetic permeabilities) of which differ compared with a homogeneous background. The first goal of this study is to design a MUSIC imaging function. This is based on the fact that the far-field pattern, which is the element of the MSR matrix, can be represented by an asymptotic expansion formula in the existence of small inhomogeneities, and the structures of left and right singular vectors of the MSR matrix are associated with nonzero singular values. The next goal is to analyze the mathematical structure of the imaging function to certify its feasibility and explore any fundamental limitations. For this, we prove that the imaging function is represented by an infinite series of first-kind integer-order Bessel functions and the configuration of incident and observation directions. On the basis of the analyzed structure, we identify that the main factors of the imaging function for the permittivity and permeability contrast cases are Bessel functions of orders zero and one, respectively. This further implies that the imaging performance is significantly dependent on the range of incident and observation directions; peaks of large magnitudes appear at the location of inhomogeneities for permittivity contrast case, and for the permeability contrast case, peaks of large magnitudes appear at the location of inhomogeneities when the range of such directions is narrow, while two peaks of large magnitudes appear in the neighborhood of the location of inhomogeneities when the range is wide enough. In light of this, a least condition for the proper imaging performance of MUSIC in the limited-aperture inverse scattering problem can be explored. The final goal is to exhibit numerical simulation results with synthetic data corrupted by random noise to demonstrate the feasibility and limitations of the designed imaging function and to support theoretical results. This paper is organized as follows. In Section 2, we briefly survey the two-dimensional direct scattering problem in the presence of well-separated small electromagnetic inhomogeneities and introduce the asymptotic expansion formula of the far-field pattern. In Section 3, the imaging function of MUSIC in a limited-aperture inverse scattering problem is designed, the mathematical structure of an imaging function is established, and some properties (including pros and cons) of the imaging function are discussed. In Section 4, corresponding numerical simulation results with noise-corrupted synthetic data generated by the Foldy-Lax framework [77] are exhibited. In Section 5, a short conclusion is provided, including an outline of future research."
https://arxiv.org/html/2411.07064v1,Rigorous enclosure of Lyapunov exponents of stochastic flows,"We develop a powerful and general method to provide arbitrarily accurate rigorous upper and lower bounds for Lyapunov exponents of stochastic flows. Our approach is based on computer-assisted tools, the adjoint method and established results on the ergodicity of diffusion processes. We do not require any structural assumptions on the stochastic system and work under mild hypoellipticity conditions outside of perturbative regimes. Therefore, our method allows for the treatment of systems that were so far inaccessible from existing mathematical tools. We demonstrate our method to exhibit the chaotic nature of three non-Hamiltonian systems. Finally, we show that our approach is robust to continuation methods to produce bounds on Lyapunov exponents for large parameter regions.","Introduction We study stochastic flows (\varphi_{t})_{t\geq 0} generated by nonlinear stochastic differential equations of the form \mathrm{d}\varphi_{t}(\omega,x)=X_{0}(\varphi_{t}(\omega,x))\mathrm{d}t+\sum_{% i=1}^{\ell}X_{i}(\varphi_{t}(\omega,x))\circ\mathrm{d}B^{i}_{t}(\omega),\qquad% \varphi_{0}(\omega,x)=x, (1) where X_{0},X_{1},\ldots,X_{\ell} denote analytic and complete vector fields on a Riemannian manifold (\mathcal{M},\langle\cdot,\cdot\rangle), the B^{i}’s are independent standard Brownian motions over a filtered probability space (\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\geq 0},\mathbb{P}) and \circ denotes Stratonovich noise (See [Pavliotis2014StochasticApplications] for an introduction to stochastic differential equations). A central tool to the study of the dynamics of the stochastic flow (\varphi_{t})_{t\geq 0} is the (top) Lyapunov exponent \lambda(\omega,x,v)=\lim_{t\to\infty}\frac{1}{t}\log\|D\varphi_{t}(\omega,x)v\|. (2) In this work, we propose a simple computer-assisted method to rigorously enclose \lambda under very mild conditions. Indeed, we essentially require the statistics of (\varphi_{t})_{t\geq 0} to be ergodic, i.e. there exists a unique smoooth probability measure \mu on \mathcal{M} such that for all A\in\mathcal{B}(\mathcal{M}) and t\geq 0, \int_{\mathcal{M}}\mathbb{P}(\varphi_{t}\in A\mid\varphi_{0}=x)\mu(\mathrm{d}x% )=\mu(A). Under the existence and uniqueness of such ergodic measure (and integrability conditions), it is well-established by the celebrated Osdelets multiplicative ergodic theorem [Arnold1998RandomSystems, Chapters 3 & 4] that for all (x,v)\in T\mathcal{M}, the Lyapunov exponent \lambda is well-defined and constant for (\mathbb{Pb}\times\mu)-almost every (\omega,x)\in\Omega\times\mathcal{M}, Lebesgue-almost every v\in\mathbf{P}(T_{x}\mathcal{M}), the projective space of the tangent space T_{x}\mathcal{M}. Furthermore, the above limit (2) holds in expectation. The Lyapunov exponent \lambda is an essential tool to characterise stochastic dynamics and typically plays an important role in the proof of synchronisation when \lambda is negative [Baxendale1991StatisticalDiffeomorphisms, Flandoli2017SynchronizationNoise] or in the characterisation of various forms of chaotic dynamics when \lambda is positive [Lamb2023HorsehoesCircle, Ledrappier1988EntropyTransformations]. In particular, from (2), it can be seen that it is directly linked to sensitivity to initial conditions, which is a quintessential feature of chaos. Thus, obtaining rigorous bounds on the Lyapunov exponent is crucial to determine the dynamics of (\varphi_{t})_{t\geq 0}. In such endeavour, the limit (2) motivates the formulation of the top Lyapunov exponent \lambda as an ergodic average, the so-called Furstenberg–Khasminskii formula \lambda=\int_{\mathbf{P}\mathcal{M}}Q\mathrm{d}\tilde{\mu}, (3) where the integrand Q depends on the vector fields X_{0},X_{1},\cdots X_{\ell} and \tilde{\mu}(\mathrm{d}\xi)=\tilde{w}(\xi)\mathrm{d}\xi is the ergodic probability measure of the Markov process (\xi_{t})_{t\geq 0}=(\varphi_{t},s_{t})_{t\geq 0} on \mathbf{P}\mathcal{M}=\cup_{x\in\mathcal{M}}\mathbf{P}_{x}\mathcal{M} where s_{t}(\omega,x,v)=\frac{D\varphi_{t}(\omega,x)v}{\|D\varphi_{t}(\omega,x)v\|}% \in\mathbf{P}_{\varphi_{t}(\omega,x)}\mathcal{M}. Note that in general, the existence and uniqueness of such invariant probability measure \tilde{\mu} is non-trivial but will not be the focus of this work. It is a control-theoretic problem associated to the stochastic differential equation solved by (\xi_{t})_{t\geq 0} \mathrm{d}\xi_{t}=\tilde{X}_{0}(\xi_{t})\mathrm{d}t+\sum_{i=1}^{\ell}\tilde{X}% _{i}(\xi_{t})\circ\mathrm{d}B^{i}_{t},\qquad\xi_{0}=(x,v), (4) with vector fields \tilde{X}_{0},\tilde{X_{1}},\ldots,\tilde{X}_{\ell} on \mathbf{P}\mathcal{M}. In the overwhelming majority of cases, obtaining bounds or even only the sign for \lambda via the Furstenberg–Khashminskii formula (3) has so far proven extremely difficult. One could for instance try to enclose \tilde{w} rigorously using computer-assisted proof methods [Gom19, Nakao2019NumericalEquations, Rum10, BerLes15] by solving the stationary Fokker-Planck equation \tilde{\mathcal{L}}^{*}\tilde{w}=0, (5) where \tilde{\mathcal{L}}^{*} denotes the adjoint in L^{2}(\mathbf{P}\mathcal{M}) of \tilde{\mathcal{L}}=\tilde{X}_{0}+\frac{1}{2}\sum_{i=1}^{\ell}\tilde{X}_{i}^{2}, which is the generator of the process (\xi_{t})_{t\geq 0} written in Hörmander form. However, the treatment of such partial differential equations (PDEs) by computer assistance is for now out of reach due to the typical non-elliptic nature of \tilde{\mathcal{L}}^{*} (and \tilde{\mathcal{L}}). The gap to bridge is even larger when considering problems on an unbounded state space \mathcal{M} [Breden2023Computer-AssistedSystems]. In some special cases, some estimates for (3) be achieved analytically in an asymptotic parameter regime e.g. as X_{i}\to 0,\,i\neq 0 [Bedrossian2022AEquations, Bedrossian2023LowerEquations, Chemnitz2023PositiveNoise]. In such regime, one can, for instance, make use of the so-called adjoint method proposed by Arnold, Papanicolaou and Wihstutz [Arnold1986AsymptoticApplications] (See [Baxendale2024LyapunovNoise] for a recent example, see also [Glynn2008BoundingProcesses] in the case of more general ergodic averages). In this work, we propose to combine the adjoint method with computer-assisted techniques to obtain rigorous and tight bounds on Lyapunov exponents for general low dimensional systems: we do not assume any asymptotic or perturbative regime, work under mild hypoellipticity conditions and our method applies to unbounded domains, without requiring any particular structure on (1). Suppose that we find a function u and a constant \Lambda satisfying the Poisson equation \tilde{\mathcal{L}}u=Q-\Lambda. (6) Then, (at least formally), we have \int_{\mathbf{P}\mathcal{M}}(Q-\Lambda)\tilde{w}\mathrm{d}\xi=\int_{\mathbf{P}% \mathcal{M}}(\tilde{\mathcal{L}}u)\tilde{w}\mathrm{d}\xi=\int_{\mathbf{P}% \mathcal{M}}u(\tilde{\mathcal{L}}^{*}\tilde{w})\mathrm{d}\xi=0, (7) therefore \Lambda=\int_{\mathbf{P}\mathcal{M}}Q\tilde{w}\mathrm{d}\xi=\lambda, as \tilde{w} integrates to one. Remark 1. The solution u to the Poisson problem (6) is linked to the fluctuations of the time average of Q(\xi_{t})-\lambda and for \tilde{\mu}-almost every \xi u(\xi)=-\lim_{t\to\infty}\int_{0}^{t}\mathbb{E}\left[Q(\xi_{s})-\lambda\mid\xi% _{0}=\xi\right]\mathrm{d}s. (8) Evidently, giving an analytic expression for u is not in general possible, however, in many cases, it is sufficient to find a good enough approximation \bar{u} for it. As mentioned already, this can sometimes be achieved in some asymptotic regime by pen and paper (See [Baxendale2024LyapunovNoise] for instance). However, using numerical tools, one can produce a very precise ansatz for \bar{u}. Suppose that we have found an approximation \bar{\lambda} of \lambda (say via a Monte-Carlo method [Kloeden1992NumericalEquations]), we can compute a numerical solution \bar{u} to the Poisson problem (6) (e.g. using (8)) such that \tilde{\mathcal{L}}\bar{u}\approx Q-\bar{\lambda}. Then, posing \bar{Q}\overset{\mathrm{def}}{=}\tilde{\mathcal{L}}\bar{u}+\bar{\lambda}, we should have \bar{Q}\approx Q. Furthermore, as in (7) \int_{\mathbf{P}\mathcal{M}}\bar{Q}\tilde{w}\mathrm{d}\xi=\bar{\lambda}, and thus we can control the difference between the approximate Lyapunov exponent \bar{\lambda} (which we know explicitly) and the exact one \lambda (unknown), via the difference Q-\bar{Q} (where both terms are known): |\lambda-\bar{\lambda}|=\left|\int_{\mathbf{P}\mathcal{M}}(Q-\bar{Q})\tilde{w}% \mathrm{d}\xi\right|. The problem has thus been reduced to bounding the right-hand side integral. While \tilde{w} is still not known, various strategies can then be applied to show this integral is small. The derivation of such bound mostly depends on the nature of Q (and \bar{Q}) and \mathcal{M} and a priori estimates on \tilde{\mu}. In this work, our strategy will be based on directly applying L^{\infty} and L^{1} bounds globally. For instance, on a bounded state space \mathcal{M}, Q and \bar{Q} are typically L^{\infty} and |\lambda-\bar{\lambda}|=\left|\int_{\mathbf{P}\mathcal{M}}(Q-\bar{Q})\tilde{w}% \mathrm{d}\xi\right|\leq\|Q-\bar{Q}\|_{\infty}\int_{\mathbf{P}\mathcal{M}}% \tilde{w}\mathrm{d}\xi=\|Q-\bar{Q}\|_{\infty}. (9) Note that, even if one could have rigorously enclosed \lambda by using a computer-assisted proof for solving the stationary Fokker-Planck equation (5) or directly the Poisson equation (6), this would be a computer-assisted proof to solve a PDE, whereas the adjoint method only requires the rigorous application of \tilde{\mathcal{L}} to \bar{u}, which is much easier and computationally cheaper (See Remark 8 for more details). We also point out that this method does not only apply to the computation of Lyapunov exponents, but to any ergodic average of the form (3) where Q is given and explicit. Already, from the simple estimate (9), one can obtain results such as the one below. Theorem 2. Consider the cellular flow with sinks (\varphi_{t})_{t\geq 0} on \mathcal{M}=\mathbb{T}^{2} generated by the stochastic differential equation \begin{cases}\mathrm{d}x_{t}&=(\cos(x_{t})/2-\cos y_{t})\sin x_{t}\mathrm{d}t+% \sigma\mathrm{d}B^{1}_{t}\\ \mathrm{d}y_{t}&=(\cos(y_{t})/2+\cos x_{t})\sin y_{t}\mathrm{d}t+\sigma\mathrm% {d}B^{2}_{t}.\end{cases} (10) Then, for \sigma=\sqrt{2}, we have the following bounds for the Lyapunov exponent \lambda defined by (2) \lambda=0.0558453099857\pm 10^{-13}>0. (11) Here and everywhere else in the paper, expressions like \lambda=x\pm r mean that \lambda\in[x-r,x+r]. (a) Phase portrait for \sigma=0. (b) Chaotic random attractor for \sigma=\sqrt{2}. Figure 1: Deterministic and random dynamics of (10). The above estimate (9) can also be generalised to treat systems on an unbounded state space \mathcal{M} and one instead makes use of Foster–Lyapunov inequalities [Canizo2023Harris-typeSemigroups, Hairer2008ErgodicPDEs, Meyn1993StabilityProcesses] to recover the necessary estimates. Below are two examples where \mathcal{M} is unbounded. Theorem 3. Consider the stochastic flow (\varphi_{t})_{t\geq 0} on \mathcal{M}=\mathbb{T}\times\mathbb{R} generated by the noisy pendulum equation \begin{cases}\mathrm{d}x_{t}&=y_{t}\mathrm{d}t\\ \mathrm{d}y_{t}&=-(\kappa\sin x_{t}+\gamma y_{t})\mathrm{d}t+\sigma\mathrm{d}B% _{t}\end{cases} (12) Then, for \kappa=2/3,\gamma=1/4 and \sigma=4, we have the following bounds for the Lyapunov exponent \lambda \lambda=0.0271763\pm 4.29\times 10^{-3}>0. (a) Phase portrait for \sigma=0. (b) Chaotic random attractor for \sigma=4. Figure 2: Deterministic and random dynamics of (12) for \kappa=2/3 and \gamma=1/4. Remark 4. For system (12), the operators \tilde{\mathcal{L}} and \tilde{\mathcal{L}}^{*} are very much non-elliptic, since there is no noise on the x variable, and also on the projective variable s at all points of \mathbf{P}\mathcal{M} (See Section 5 for the explicit formula). Usual computer-assisted proof techniques would therefore not be applicable to this system, but the one proposed in this paper is. For our approach, the main difficulty often lies in finding an accurate enough approximate solution \bar{u}, which is especially challenging here due to the lack of ellipticity. Finally, we show that our method is robust and can be combined with continuation methods recently introduced in [Breden2023AExpansions] (see also [AriGazKoc21]) to enclose Lyapunov exponents for a whole parameter range. Theorem 5. Consider the stochastic flow (\varphi_{t})_{t\geq 0} on \mathcal{M}=\mathbb{R}^{2} generated by the Hopf normal form with additive noise \mathrm{d}\left(\begin{matrix}x_{t}\\ y_{t}\\ \end{matrix}\right)=\left[\left(\begin{matrix}\alpha&-\beta\\ \beta&\alpha\\ \end{matrix}\right)\left(\begin{matrix}x_{t}\\ y_{t}\\ \end{matrix}\right)-\left(\begin{matrix}a&b\\ -b&a\\ \end{matrix}\right)\left(\begin{matrix}x_{t}\\ y_{t}\\ \end{matrix}\right)(x_{t}^{2}+y_{t}^{2})\right]\mathrm{d}t+\sigma\mathrm{d}% \left(\begin{matrix}B^{1}_{t}\\ B^{2}_{t}\\ \end{matrix}\right),\qquad a>0. (13) Let \beta\in\mathbb{R} and fix a=\alpha=4, \sigma=\sqrt{2} and let \lambda_{b} denote the Lyapunov exponent of this system for a shear parameter b and b\mapsto\bar{\lambda}_{b} be the function represented in Figure 3(a) (and whose precise description can be found that at [Huggzz/Enclosure-of-Lyapunov-exponents]). Then, for all b\in[0,30] |\lambda_{b}-\bar{\lambda}_{b}|\leq 3.41\times 10^{-4}. Corollary 6. Consider \lambda_{b} as in Theorem 5, there exists b^{*}\in(21.5322,21.5381) such that \lambda_{b^{*}}=0. Furthermore, \lambda_{b}<0 for b\in[0,21.5322] and \lambda_{b}>0 for b\in[21.5381,30]. (a) Graph of the function b\mapsto\bar{\lambda}_{b}; the orange points are sampled at Chebyshev nodes (b) Chaotic random attractor for a=\alpha=4, \sigma=\sqrt{2} and b=21.5381. Figure 3: Quantitative and qualitative random dynamics induced by (13). Theorem 5 and Corollary 6 complete the study of the Lyapunov exponent for this system initiated in [DeVille2011StabilitySystem], pursued in [Doan2018HopfNoise] and with recent and complementary results on the positivity of the Lyapunov exponent \lambda_{b} in the asymptotic regime b\to\infty in [Baxendale2024LyapunovNoise, Chemnitz2023PositiveNoise]. Note that, all our examples do not belong in the volume-preserving/incompressible/Hamiltonian class [Arnold2001TheSystems, Baxendale2002LyapunovSystems] of stochastic flows, which means establishing the positivity of the (top) Lyapunov exponent would be particularly difficult with more classical pen-and-paper techniques, as tools such as the Furstenberg criterion [Furstenberg1963NoncommutingProducts] (See [CotiZelati2024Three-dimensionalFlows] for a recent example) are not available. The examples of Theorems 2 and 3 can actually be seen as noise-induced transitions from negative to positive Lyapunov exponent. The remainder of the paper is organized as follows. The principle of the adjoint method is recalled in Section 2, where we also introduce a general framework allowing us to turn this idea into a computer-assisted proof, together with a first simple example. We then recall how the Furstenberg–Khasminskii formula can be derived and how the ergodicity of the projective process can be shown in Section 3. In Sections 4, 5 and LABEL:sec:hopf, we provide more background on each of the systems studied in Theorems 2, 3 and 5, together with the proofs of each Theorem. Finally, Section LABEL:sec:outlook discusses potential further applications of our computer-assisted adjoint method. The computer-assisted parts of the proofs can be reproduced using the code available at [Huggzz/Enclosure-of-Lyapunov-exponents]."
https://arxiv.org/html/2411.07057v1,"Randomized Forward Mode Gradient
for Spiking Neural Networks in Scientific Machine Learning","Spiking neural networks (SNNs) represent a promising approach in machine learning, combining the hierarchical learning capabilities of deep neural networks with the energy efficiency of spike-based computations. Traditional end-to-end training of SNNs is often based on back-propagation, where weight updates are derived from gradients computed through the chain rule. However, this method encounters challenges due to its limited biological plausibility and inefficiencies on neuromorphic hardware. In this study, we introduce an alternative training approach for SNNs. Instead of using back-propagation, we leverage weight perturbation methods within a forward-mode gradient framework. Specifically, we perturb the weight matrix with a small noise term and estimate gradients by observing the changes in the network output. Experimental results on regression tasks, including solving various PDEs, show that our approach achieves competitive accuracy, suggesting its suitability for neuromorphic systems and potential hardware compatibility.","Recent advances in machine learning have greatly expanded the capabilities of artificial intelligence (AI) for applications in solving differential equations, function approximation, and various other fields [1, 2, 3, 4, 5]. As demand grows for energy-efficient and computationally feasible solutions, spiking neural networks (SNNs) have attracted significant attention within the machine learning community. Unlike traditional artificial neural networks (ANNs), which rely on continuous activation functions, SNNs operate with sparse, binary spiking events, making them more efficient in terms of energy consumption [6, 7, 8, 9, 10]. Furthermore, SNNs benefit from recent advancements in neuromorphic hardware (e.g., Intel’s Loihi 2 chip [11, 12]), which enables SNNs to approximate brain-like computations, hence facilitating lightweight faster models. Despite these benefits, most deep neural networks still rely on back-propagation for training. This method, however, is considered “biologically implausible” as it lacks symmetry with the way biological neural systems learn, does not engage massive parallelism, and is often incompatible with neuromorphic hardware. This incongruity underscores the need for novel learning algorithms better suited to SNNs. Weight perturbation offers a promising alternative, where small perturbations are applied to the synaptic connections during the forward pass, and weight updates are adjusted in response to changes in the loss function. Instead of perturbing weights directly, forward-mode automatic differentiation (AD) can be employed to compute a directional gradient along the perturbation direction [13]. Forward-mode AD has seen a resurgence in deep learning applications [14]. In this work, we leverage weight perturbation to train SNNs, aiming for greater biological plausibility. We explore two different methods for determining surrogate gradients and implementing perturbations. Evaluating these methods on regression tasks that are more changeling [15, 16, 17], particularly relevant to scientific machine learning (SciML), our results indicate the viability of this approach, positioning it as a step towards realizing SNNs on neuromorphic hardware [18] without reliance on back-propagation. The paper is organized as follows. In section 2, we review related works, and in section 3 we describe the methodology. In section 4 we present our results, and we conclude in section 5 with a summary."
https://arxiv.org/html/2411.06848v1,Generative Feature Training of Thin 2-Layer Networks,"We consider the approximation of functions by 2-layer neural networks with a small number of hidden weights based on the squared loss and small datasets. Due to the highly non-convex energy landscape, gradient-based training often suffers from local minima. As a remedy, we initialize the hidden weights with samples from a learned proposal distribution, which we parameterize as a deep generative model. To train this model, we exploit the fact that with fixed hidden weights, the optimal output weights solve a linear equation. After learning the generative model, we refine the sampled weights with a gradient-based post-processing in the latent space. Here, we also include a regularization scheme to counteract potential noise. Finally, we demonstrate the effectiveness of our approach by numerical examples.","We investigate the approximation of real-valued functions f\colon[0,1]^{d}\to\mathbb{R}. To this end, assume that we are given samples (x_{k},y_{k})_{k=1}^{M}, where x_{k}\in[0,1]^{d} are independently drawn from some distribution \nu_{\text{data}} and y_{k}\approx f(x_{k}) are possibly noisy observations of f(x_{k}). For approximating f based on (x_{k},y_{k})_{k=1}^{M}, we study parametric architectures f_{w,b}\colon[0,1]^{d}\to\mathbb{R} of the form f_{w,b}(x)=\mathfrak{Re}\biggl{(}\sum_{l=1}^{N}b_{l}\Phi(\langle w_{l},x% \rangle)\biggr{)}, (1) where \mathfrak{Re} denotes the real part, \Phi\colon\mathbb{R}\to\mathbb{C} is a nonlinear function, and w_{1},...,w_{N}\in\mathbb{R}^{d} are the features with corresponding weights b_{1},...,b_{N}\in\mathbb{C}. If the function \Phi is real-valued, the model (1) simplifies to a standard 2-layer neural network architecture without \mathfrak{Re} and with b_{1},...,b_{N}\in\mathbb{R}. The more general model (1) also covers other frameworks such as random Fourier features (Rahimi & Recht, 2007). For a fixed activation function \Phi and width N, we aim to find parameters (w,b)\in\mathbb{R}^{d,N}\times\mathbb{C}^{N} such that the f_{w,b} from (1) approximates f well. From a theoretical perspective, we can minimize the mean squared error (MSE), namely \bigl{(}\hat{w},\hat{b}\bigr{)}\in\operatorname*{arg\,min}_{w,b}\|f-f_{w,b}\|^% {2}_{L^{2}(\nu_{\text{data}})}, (2) to obtain the parameters (\hat{w},\hat{b}). In practice, however, we do not have direct access to \nu_{\text{data}} and f, but only to data points (x_{k},y_{k})_{k=1}^{M}, where x_{k} are iid samples from \nu_{\text{data}} and y_{k} are noisy versions of f(x_{k}). Hence, we replace (2) by the empirical risk minimization \bigl{(}\hat{w},\hat{b}\bigr{)}\in\operatorname*{arg\,min}_{w,b}\sum_{k=1}^{M}% |y_{k}-f_{w,b}(x_{k})|^{2}. (3) However, if M is small, minimizing (3) can lead to significant overfitting towards the training samples (x_{k},y_{k})_{k=1}^{M} and poor generalization. To circumvent this problem, we investigate the following principles. • We use architectures of the form (1) with small N. This amounts to the implicit assumption that f can be sparsely represented with this model. Unfortunately, such under-parameterized networks (N\ll M) are difficult to train with conventional gradient-based algorithms (Boob et al., 2022; Holzmüller & Steinwart, 2022), see also Table 1. Hence, we require an alternative training strategy. • Often, we have prior information about the regularity of f, i.e., that f is in some Banach space \mathcal{B} with a norm of the form \|f\|_{\mathcal{B}}^{p}=\int_{[0,1]^{d}}\|Lf(x)\|_{q}^{p}\mathrm{d}x, (4) where L is some differential operator and p,q\geq 1. A common example within this framework is the space of bounded variation (Ambrosio et al., 2000), which informally corresponds to the choice L=\nabla, q=2 and p=1. In practice, the integral in (4) is often approximated using Monte Carlo methods with uniformly distributed samples (\tilde{x}_{m})_{m=1}^{S}\subset[0,1]^{d}. If we use (4) as regularizer for f_{w,b}, the generalization error can be analyzed in Barron spaces (Li et al., 2022). Contribution We propose a generative modeling approach to solve (3). To this end, we first observe that the minimization with respect to b is a linear least squares problem. Based on this, we analytically express the optimal \hat{b} in terms of w, which leads to a reduced problem. Using the implicit function theorem, we can compute \nabla_{w}\hat{b}(w) and hence the gradient of the reduced objective. To facilitate its optimization, we replace the deterministic features w with stochastic ones, and optimize over their underlying distribution p_{w} instead. We parameterize this distribution as p_{w}={G_{\theta}}_{\#}\mathcal{N}(0,I_{d}) with a deep network G_{\theta}\colon\mathbb{R}^{d}\to\mathbb{R}^{d}. Hence, we coin our approach as generative feature training. Further, we propose to add a Monte Carlo approximation of the norm (4) to the reduced objective. With this regularization, we aim to prevent overfitting."
https://arxiv.org/html/2411.06286v1,SPIKANs: Separable Physics-Informed Kolmogorov-Arnold Networks,"Physics-Informed Neural Networks (PINNs) have emerged as a promising method for solving partial differential equations (PDEs) in scientific computing. While PINNs typically use multilayer perceptrons (MLPs) as their underlying architecture, recent advancements have explored alternative neural network structures. One such innovation is the Kolmogorov-Arnold Network (KAN), which has demonstrated benefits over traditional MLPs, including faster neural scaling and better interpretability. The application of KANs to physics-informed learning has led to the development of Physics-Informed KANs (PIKANs), enabling the use of KANs to solve PDEs. However, despite their advantages, KANs often suffer from slower training speeds, particularly in higher-dimensional problems where the number of collocation points grows exponentially with the dimensionality of the system. To address this challenge, we introduce Separable Physics-Informed Kolmogorov-Arnold Networks (SPIKANs). This novel architecture applies the principle of separation of variables to PIKANs, decomposing the problem such that each dimension is handled by an individual KAN. This approach drastically reduces the computational complexity of training without sacrificing accuracy, facilitating their application to higher-dimensional PDEs. Through a series of benchmark problems, we demonstrate the effectiveness of SPIKANs, showcasing their superior scalability and performance compared to PIKANs and highlighting their potential for solving complex, high-dimensional PDEs in scientific computing.","Physics-Informed Neural Networks (PINNs) have gained widespread attention for their ability to incorporate physical laws into machine learning models, allowing solutions to forward and inverse problems involving partial differential equations (PDEs) [1]. However, traditional PINNs struggle with computational costs when solving multi-dimensional, highly complex PDEs. This challenge is exacerbated by the need for a large number of collocation points, leading to high memory overhead and inefficient scaling as the problem’s dimensionality increases [2]. As an alternative to the traditional multi-layer perceptron (MLP) architecture, Kolmogorov-Arnold networks (KANs) introduced by [3] and extended in [4] have prompted further exploration in the context of physics-informed learning. These networks, featuring trainable activation functions, have demonstrated promising results and superior performance compared to MLPs in terms of interpretability, robustness against catastrophic forgetting [5, 6], and resilience to noisy data [7]. To leverage these benefits within the PINNs framework, [3, 8] have proposed physics-informed KANs (PIKANs) and deep operator networks (DeepOKANs [9]). PIKANs have demonstrated success on benchmark problems for physics-informed machine learning and applications [10, 11, 12, 13, 14, 15, 16]. Despite the advantages of KANs in physics-informed learning, the use of B-splines as basis functions for the activations in the original formulation leads to a significant increase in computational cost. Although previous works have proposed alternative basis functions [17, 18, 19, 20] that reduce computational time per iteration, the fundamental issue of exponential growth in the number of training points and network evaluations remains. Consequently, the curse of dimensionality associated with this architecture, combined with slower performance in KANs, limits the application of these networks for solving high-dimensional initial-boundary value problems. To address these limitations, we present Separable Physics-Informed Kolmogorov-Arnold Networks (SPIKANs). Inspired by Separable PINNs [21], we propose a network architecture that decomposes the solution of multi-dimensional PDEs into separable components, enabling more efficient training and inference. For a d-dimensional PDE with N^{d} sampled collocation points, instead of training one KAN with a cloud of O(N^{d}) training points, SPIKANs decompose the problem into d KANs, each receiving O(N) points as inputs. This approach substantially reduces computational costs in terms of time and memory, at the cost of requiring a factorizable mesh of collocation points rather than the traditional unstructured point-cloud used in PINNs. This paper is organized as follows: in Sec. 2, we briefly introduce KANs and PIKANs, along with the description of the proposed method, SPIKANs. In Sec. 3, we demonstrate the use of SPIKANs in four benchmark problems, in increasing order of dimensionality, comparing gains in accuracy and speedup. Finally, we summarize the findings and discuss the limitations in Sec. 4."
https://arxiv.org/html/2411.06225v1,RandNet-Parareal: a time-parallel PDE solver using Random Neural Networks,"Parallel-in-time (PinT) techniques have been proposed to solve systems of time-dependent differential equations by parallelizing the temporal domain. Among them, Parareal computes the solution sequentially using an inaccurate (fast) solver, and then “corrects” it using an accurate (slow) integrator that runs in parallel across temporal subintervals. This work introduces RandNet-Parareal, a novel method to learn the discrepancy between the coarse and fine solutions using random neural networks (RandNets). RandNet-Parareal achieves speed gains up to x125 and x22 compared to the fine solver run serially and Parareal, respectively. Beyond theoretical guarantees of RandNets as universal approximators, these models are quick to train, allowing the PinT solution of partial differential equations on a spatial mesh of up to 10^{5} points with minimal overhead, dramatically increasing the scalability of existing PinT approaches. RandNet-Parareal’s numerical performance is illustrated on systems of real-world significance, such as the viscous Burgers’ equation, the Diffusion-Reaction equation, the two- and three-dimensional Brusselator, and the shallow water equation.","Parallel-in-time (PinT) methods have been used to overcome the saturation of well-established spatial parallelism approaches for solving (prohibitively expensive) initial value problems (IVPs) for ordinary and partial differential equations (ODEs and PDEs), described by systems of d\in\mathbb{N} ODEs (and similarly for PDEs) \frac{d\boldsymbol{u}}{dt}=h(\boldsymbol{u}(t),t)\enspace\text{ on }t\in\left[% t_{0},t_{N}\right],\enspace\text{with }\boldsymbol{u}\left(t_{0}\right)=% \boldsymbol{u}^{0},\enspace N\in\mathbb{N}, (1) where h:\mathbb{R}^{d}\times\left[t_{0},t_{N}\right]\rightarrow\mathbb{R}^{d} is a smooth multivariate function, \boldsymbol{u}:\left[t_{0},t_{N}\right]\rightarrow\mathbb{R}^{d} is the time dependent column vector solution, and \boldsymbol{u}^{0}\in\mathbb{R}^{d} is the initial value at t_{0}. PinT schemes are particularly important when the sequential application of an accurate numerical integrator \mathscr{F} over \left[t_{0},t_{N}\right] is infeasible in a reasonable wallclock time. There are three general approaches for PinT computation: parallel across-the-problem, parallel-across-the-step, and parallel-across-the-method. In [17, 55], another classification is provided: multiple shooting, methods based on waveform relaxation and domain decomposition, multigrid approaches, and direct time-parallel methods. Parallel-across-the-step methods, in which solutions at multiple time-grid points are computed simultaneously, include Parareal (approximation of the derivative in the shooting method) [45], Parallel Full Approximation Scheme in Space and Time (PFASST) (multigrid method) [13, 50], and Multigrid Reduction in Time (MGRIT) [14, 16] methods (see [19] for details). Among them, Parareal [45] has garnered popularity, with extensive theoretical analyses, improved versions, and empirical applications [17, 55]. This is due to its non-intrusive nature which allows seamless integration with arbitrary temporal and spatial discretizations, and to its successful performance across diverse fields, such as plasma physics [64, 66, 67], finance [4, 56], and weather modeling [59, 60]. Limited theoretical results are available for MGRIT and PFASST, with a few extensions and empirical applications. Interestingly, combined analyses have shown equivalences between Parareal and MGRIT, and connections between MGRIT and PFASST. In Parareal, a coarse and fast solver \mathscr{G} is run sequentially to obtain a first approximation of the solution, which is then corrected by running a fine (accurate) but slow integrator \mathscr{F} in parallel across N temporal subintervals. This procedure is then iterated until a convergence criterion is met after k\leq N iterations, leading to a speed-up compared to running \mathscr{F} sequentially over the entire time interval. A recent advancement, GParareal [57], improves Parareal convergence rates (measured as k/N) by learning the discrepancy \mathscr{F}-\mathscr{G} using Gaussian Processes (GPs). This method outperforms Parareal for low-dimensional ODEs and a moderate number of computer cores N. However, the cubic cost (in the number of data points, roughly kN at iteration k) of inverting the GP covariance matrix hinders its broader application. Subsequent research introduced nearest neighbors (nns) GParareal (nnGParareal) [21], enhancing GParareal’s scalability properties in both N and d through data reduction. Significant computational gains were achieved by training the GP on a small subset of nns, resulting in an algorithm loglinear in the sample size. This allowed scaling its effectiveness up to systems with a few thousand ODEs, beyond which it loses its potential. Indeed, being based on the original GP framework, it uses a costly hyperparameter optimization procedure that requires fitting one GP per ODE dimension. This study introduces RandNet-Parareal, a new approach using random neural networks (RandNets) to learn the discrepancy \mathscr{F}-\mathscr{G}. RandNets are a family of single-hidden-layer feed-forward neural networks (NNs), where hidden layer weights are randomly sampled and fixed, and only the output (or readout) layer is subject to training. Compared to standard artificial NNs, RandNets are hence much simpler to train: the input data are fed through the network, the predictions observed, and the weights of the linear output (or readout) layer are obtained as minimizers of a penalized squared loss between the NN outputs and the training targets. Since this optimization problem admits a closed-form solution, no backpropagation is required, and the issues of vanishing and exploding gradients persisting for standard fully trainable NNs are therefore avoided. The literature on the topic is rich and somewhat fragmented, and different names are used for essentially the same model. RandNets are related to Random Feature Networks [6, 49, 62, 63, 65] and Reservoir Computing [24, 26, 25, 27, 28], Random Fourier Features (RFFs) and kernel methods [41, 61, 70, 74]. Some authors use the name Extreme Learning Machines (ELMs) [34, 35, 36, 37, 44] to refer to RandNets, while others use the term randomized or random NNs [5, 32, 39, 46, 78, 82] for the same paradigm. RandNets show excellent empirical performance, and have been used in the context of mathematical finance [22, 33, 38], mathematical physics [52], electronic circuits [69], photonic [47] and quantum systems [23, 48], random deep splitting schemes [53], scientific computing [10, 11, 79, 81], and have shown excellent empirical performance in numerous further applications. Moreover, recent work [22, 25] proves that RandNets are universal approximators within spaces of sufficiently regular functions, and provides explicit approximation error bounds, with these results generalized to a large class of Bochner spaces in [52]. These contributions show that RandNets are a reliable machine learning paradigm with provable theoretical guarantees. In this paper, we show that endowing Parareal with RandNets-based learning of \mathscr{F}-\mathscr{G}, the new proposed RandNet-Parareal algorithm, leads to significantly improved scalability, convergence speed, and parallel performance with respect to nnGParareal, GParareal, and Parareal. This allows us to solve PDE systems on a fine mesh of up to 10^{5} discretization points with negligible overhead, outperforming nnGParareal by two orders of magnitude and reducing its model cost by several orders. Here, we compare the performance of Parareal, nnGParareal, and RandNet-Parareal on five increasingly complex systems, some of which are drawn from an extensive benchmark study of time-dependent PDEs [75]. These include the one-dimensional viscous Burgers’ equation, the two-dimensional Diffusion-Reaction equation, a challenging benchmark used to model biological pattern formation [76], the two- and three-dimensional Brusselator, known for its complex behavior, including oscillations, spatial patterns, and chaos, and the shallow water equations (SWEs). Derived from the compressible Navier-Stokes equations, the SWEs are a system of hyperbolic PDEs exhibiting several types of real-world significance behaviors known to challenge numerical integrators, such as sharp shock formation dynamics, sensitive dependence on initial conditions, diverse boundary conditions, and spatial heterogeneity. Example applications include of tsunamis or flooding simulations. We intentionally chose two hyperbolic equations (Burgers’ and SWE) to challenge RandNet-Parareal on systems for which Parareal is known to struggle, with slow or non-convergent behavior [2, 3, 9, 18, 72]. Previous works have developed ad-hoc coarse solvers to address Parareal’s slow convergence for Burgers’ [7, 40, 68, 71], and for SWE [1, 31, 54, 73]. Here, we adopt a different strategy: by leveraging the generalization capabilities of RandNets within the Parareal algorithm, we enhance the performance of standard, off-the-shelf integration methods such as Runge-Kutta, obtaining speed gains up to x125 and x22 compared to the accurate integrator \mathscr{F} and Parareal, respectively. All experiments have been executed on Dell PowerEdge C6420 compute nodes each with 2 x Intel Xeon Platinum 826 (Cascade Lake) 2.9 GHz 24-core processors, 48 cores and 192 GB DDR4-2933 RAM per node. To illustrate our proposed algorithm and facilitate code adoption, we provide a step-by-step Jupyter notebook outlining RandNet-Parareal. Moreover, all simulation outcomes, including tables and figures, are fully reproducible and accompanied by the necessary Python code at https://github.com/Parallel-in-Time-Differential-Equations/RandNet-Parareal. It is well acknowledged that comparing PinT methods based on different working principles is extremely hard, with [55] representing a recent survey article with some comparisons. Quoting [55],“caution should be taken when directly comparing speedup numbers across methods and implementations. In particular, some of the speedup and efficiency numbers are only theoretical in nature, and many of the parallel time methods do not address the storage or communication overhead of the parallel time integrator”. [19] is one of very few recent attempts to systematically compare different PinT classes. However, it is limited exclusively to the Dahlquist problem. Thus, it has become conventional to compare new techniques to the existing state-of-the-art methods within the same group of solvers. This is why, in this work, we compare RandNet-Parareal with the original Parareal and its recently improved versions, GParareal [57], and nnGParareal [21]. The rest of the paper is organized as follows. In Section 2, we describe the Parareal algorithm. Section 3 briefly explains GParareal and nnGParareal, focusing on the latter. RandNet-Parareal is introduced in Section 4, while Sections 5 and 6 present our numerical results, and a final discussion. A computational complexity analysis of RandNet-Parareal, a robustness evaluation of the proposed algorithm, complementary simulation studies, and other additional results are available in the Supplementary Material. Notation. We denote by \boldsymbol{v}\in\mathbb{R}^{n} a column vector with entries {v}_{i}, i\in\{1,\ldots,n\}, and by \|\boldsymbol{v}\| and \|\boldsymbol{v}\|_{\infty} its Euclidean and infinity norms, respectively. We use A\in\mathbb{R}^{n\times m} to denote a real-valued n\times m matrix, n,m\in\mathbb{N}, with elements A_{ij}, jth column A_{(\cdot,j)}, j\in\{1,\dots m\}, and ith row A_{(i,\cdot)}, i\in\{1,\dots,n\}. We write A^{\top}, A^{\dagger}, and \|A\|_{\rm F} for the A matrix transpose, Moore-Penrose pseudoinverse, and Frobenius norm, respectively. \mathbb{I}_{n} denotes the identity matrix of dimension n."
