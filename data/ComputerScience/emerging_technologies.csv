URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.09787v1,ART-Rx: A Proportional-Integral-Derivative (PID) Controlled Adaptive Real-Time Threshold Receiver for Molecular Communication,"Molecular communication (MC) in microfluidic channels faces significant challenges in signal detection due to the stochastic nature of molecule propagation and dynamic, noisy environments. Conventional detection methods often struggle under varying channel conditions, leading to high bit error rates (BER) and reduced communication efficiency. This paper introduces ART-Rx, a novel Adaptive Real-Time Threshold Receiver for MC that addresses these challenges. Implemented within a conceptual system-on-chip (SoC), ART-Rx employs a Proportional-Integral-Derivative (PID) controller to dynamically adjust the detection threshold based on observed errors in real time. Comprehensive simulations using MATLAB and Smoldyn compare ART-Rx’s performance against a statistically optimal detection threshold across various scenarios, including different levels of interference, concentration shift keying (CSK) levels, flow velocities, transmitter-receiver distances, diffusion coefficients, and binding rates. The results demonstrate that ART-Rx significantly outperforms conventional methods, maintaining consistently low BER and bit error probabilities (BEP) even in high-noise conditions and extreme channel environments. The system exhibits exceptional robustness to interference and shows the potential to enable higher data rates in CSK modulation. Furthermore, because ART-Rx is effectively adaptable to varying environmental conditions in microfluidic channels, it offers a computationally efficient and straightforward approach to enhance signal detection in nanoscale communication systems. This approach presents a promising control theory-based solution to improve the reliability of data transmission in practical MC systems, with potential applications in healthcare, brain-machine interfaces (BMI), and the Internet of Bio-Nano Things (IoBNT).","Molecular communication (MC) or particle-based communication, a paradigm inspired by nature, has emerged as a promising solution to communicating with biological organisms where traditional communication methods have been shown to be ineffective [1, 2, 3, 4]. By leveraging biochemical mechanisms for the transmission of information, MC is also believed to play a vital role in the realization of the Internet of Everything (IoE) [5], particularly the Internet of Bio-Nano Things (IoBNT) [6, 7], and the concept of digital twins through the extension of connectivity to nanoscale and biological environments [8]. This unconventional bio-inspired technique encodes information with one or more types of information molecules (IM) at the transmitter end, which are then propagated to a receiver through various mechanisms such as channel diffusion [9, 10, 11, 12, 13], mimicking methods of communication commonly found in the natural world. As research in bioengineering and nanotechnology continues to advance, there are vast opportunities [14] in the realm of MC for the development of bio-nanoscale communication systems that could potentially revolutionize fields such as healthcare [11, 15, 16], nanomachines [3, 12, 16], and brain machine interfaces (BMI) [3, 14]. Microfluidic channel-based MC systems [11, 12, 17] have been the center of attention due to their controllable environment [18, 19], their ability to simulate biochemical intra-body communications [20], and their ability to test lab-on-a-chip technologies [21]. The system is also a suitable platform for testing various modulation, diffusion, and demodulation techniques of IM [13], making the microfluidic channel ideal for both fundamental research and exploration of potential practical applications. However, the microfluidic MC system still faces a wide variety of challenges that need to be addressed, particularly in detection and noise mitigation [12, 22], before it can be deployed for more practical use cases in the real world. The stochastic nature of diffusion in MC means that it is also susceptible to many of the same issues that affect traditional electromagnetic (EM) communications, particularly noise interference. This includes different types of noise, such as intersymbol interference (ISI) [23], environmental noise, binding noise, and Brownian noise [13], which can lead to high bit error rates (BER) and, as a result, affect the reliability of information transmission [24]. In addition to the complexity of the microfluidic environment in the MC channel, detection schemes such as maximum likelihood (ML) [25, 26] or optimal detection thresholds [27] alone may not always provide the most optimal performance under rapidly changing channel conditions and, as demonstrated in [28, 29, 30, 31], adaptive thresholding methods perform promisingly well. Although more recent and complex schemes such as machine learning (ML)-based adaptive thresholding could potentially provide better performance under varying channel conditions [32, 33, 34, 35, 36, 37, 38], computational costs, prior knowledge requirements of channel models, and large amounts of learning data requirement may outweigh the benefits in some scenarios [39], such as deployment on an intra-body nanomachine. Such a device would ideally require accurate, efficient, and simple detection schemes. To address these challenges, we propose a novel adaptive thresholding technique based on a Proportional-Integral-Derivative (PID) controller [40]. In this paper, it will be called an adaptive real-time threshold receiver (ART-Rx). PID controllers are already ubiquitous in various industrial settings and are known to be reliable, fast, and efficient. Unlike ML, it does not require any training or learning of channel models [40, 41]. Our proposed approach aims to utilize the benefits of PID controllers to adaptively adjust receiver detection thresholds in response to dynamical channel noise conditions. The closed-loop system will allow the receiver to adjust the detection threshold in discrete real-time based on various feedback parameters such as the BER of previously transmitted bits and channel environmental parameters. The implementation of ART-Rx tested in this paper with the Smoldyn simulator [42] employs a PID controller that adjusts the detection threshold by calculating the error between the observed peak receptor binding levels and a desired setpoint at each symbol interval. The suggested implementation can potentially improve the overall reliability and robustness of the molecular communication receiver in noisy microfluidic environments such as within the human body while remaining computationally efficient. The remainder of this paper is organized as follows. Section II provides a detailed overview of our ART-Rx implementation and the underlying assumptions. In Section III, we describe the simulation setup, including evaluation parameters and specifications, and present simulation results comparing ART-Rx performance to the statistically optimal detection threshold compared to as a benchmark under various noise levels and channel conditions. This section also includes a comprehensive performance analysis and discusses limitations of our approach, such as the impact of PID parameter tuning and challenges associated with real-world implementation. Finally, Section IV concludes the paper by summarizing key findings and outlines directions for future research on the ART-Rx system."
https://arxiv.org/html/2411.10435v1,The Spatial Complexity of Optical Computing and How to Reduce It,"Similar to algorithms, which consume time and memory to run, hardware requires resources to function. For devices processing physical waves, implementing operations needs sufficient “space,” as dictated by wave physics. How much space is needed to perform a certain function is a fundamental question in optics, with recent research addressing it for given mathematical operations, but not for more general computing tasks, e.g., classification. Inspired by computational complexity theory, we study the “spatial complexity” of optical computing systems in terms of scaling laws—specifically, how their physical dimensions must scale as the dimension of the mathematical operation increases—and propose a new paradigm for designing optical computing systems: space-efficient neuromorphic optics, based on structural sparsity constraints and neural pruning methods motivated by wave physics (notably, the concept of “overlapping nonlocality”). On two mainstream platforms, free-space optics and on-chip integrated photonics, our methods demonstrate substantial size reductions (to 1\%-10\% the size of conventional designs) with minimal compromise on performance. Our theoretical and computational results reveal a trend of diminishing returns on accuracy as structure dimensions increase, providing a new perspective for interpreting and approaching the ultimate limits of optical computing—a balanced trade-off between device size and accuracy.","I Results Figure 2: Standard optical nonlocality and overlapping nonlocality. (a) Schematic of a 1D free-space optical system. The communication cone of an output port, defined as the set of all couplings to it, is characterized by two parameters, the horizontal shift d_{\text{shift}} and the spanning range w_{\text{cone}}. An ideally local optical device would have both d_{\text{shift}} and w_{\text{cone}} equal to zero. (b) The overlapping nonlocality (ONL) C associated with a transverse aperture (or “cut”) is the number of communication cones intersecting it. For example, C=5 for the green cut and C=6 for the purple cut. (c) Demonstration of calculating C for an arbitrary cut for a 2D free-space optical system. For a given cut, an output contributes a one to C only if its communication cone intersects the cut. (d) Definition of the in-plane distance d_{\parallel} traversed by an optical coupling. For a pair of coupled ports (i,j), d_{\parallel} is the distance between them, projected onto the output (or input) plane. This distance measures the nonlocality of each individual coupling. Figure 3: Scaling laws of three types of optical device kernels. Scaling laws of the maximum ONL, \max(C), with respect to the mathematical operation dimension, N, for the three considered types of device kernels: (a) trivial sparse, (b) row sparse, and (c) local sparse kernels. In (a), lines and shaded regions (inset) represent the average and one standard deviation of numerical simulation results. The standard deviation is negligibly small. In (b,c), darker lines represent theoretical values derived from Eq. 2, and lighter lines represent numerical simulation results (see Methods and Supplementary Note 1). I.1 The Spatial Complexity of Free-Space Optics To implement a nontrivial mathematical operation, an optical device has to be “nonlocal” — its output at any position should depend on the input across a range of positions, which ultimately implies the need for sufficient thickness to incorporate enough “channels” to communicate sideways within the device [17, 27]. The relation between the concept of nonlocality and thickness (and, more broadly, spatial complexity) is, however, a subtle one. Consider the configuration shown in Fig. 2a: In a 1D optical structure, each output port (a sampling point for the output field) is coupled to a set of input ports (sampling points for the input field), spanning a range over the input plane. These couplings between pairs of points (input j and output i) encode the function of the optical system, namely, they are the elements D_{ij} of its kernel operator (in matrix form) \mathbf{D}. Two parameters can be used to describe how spatially extended, or “nonlocal”, the range of these couplings is: d_{\text{shift}}, the horizontal shift of the center of this range relative to the output, and w_{\text{cone}}, the width of the range. We then refer to the cone-shaped region (highlighted in orange in Fig. 2a) that encloses all the couplings for an arbitrary output i as the communication cone of i. The parameters d_{\text{shift}} and w_{\text{cone}} represent different aspects of the conventional notion of nonlocality in optics and wave physics. Importantly, however, very large values for these parameters do not imply that the optical system require multiple “sideways channels” and, thus, a large thickness. As argued in [17], a single-mode optical fiber with multiple taps, for example, can have arbitrarily large nonlocality, as quantified here by d_{\text{shift}} and w_{\text{cone}}, but would only require a finite (small) thickness to accommodate that single mode. It is a different aspect of “nonlocality” that is directly related to the size requirements of optics, the so-called overlapping nonlocality (ONL), which arises if the input position range for one output point overlaps with that for another output point, requiring multiple sideways channels to implement the desired function [17]. From a geometrical perspective (Fig. 2b), the ONL associated with a transverse aperture S is the number C of communication cones that must cross from one side of S to the other to implement the desired kernel. We refer to the transverse aperture as a “cut,” as it divides the output (and input) plane into two parts. For a device operating at an effective wavelength of \lambda_{0}/n (where n is the maximum refractive index in the device), the cut must be large enough to support C sideways channels to realize those crossing communication cones—one sideways channel for each cone. Using diffraction arguments (assuming the system only contains transparent, non-absorbing materials), one can then determine the lower bound on the device thickness t from the maximum value of C throughout the device [17]: t\geq\begin{cases}\max(C)\frac{\lambda_{0}}{2(1-\cos\theta)n}&\text{for 1D},\\ \max(C)\frac{1}{l_{\text{cut}}}\left[\frac{\lambda_{0}}{2(1-\cos\theta)n}% \right]^{2}&\text{for 2D},\end{cases} (1) where the parameter \theta represents the maximum allowed ray angle inside the device. Eq. 1, also distinguishes between 1D and 2D cases, since in 1D, the size of the aperture is the thickness t, whereas in 2D, t\equiv A/l_{\text{cut}}, where A is the area of the cut and l_{\text{cut}} is the length of its projection to the output (or input) plane (Fig. 2c). These theoretical results imply that the thickness required to perform an exact mathematical operation, described by a kernel \mathbf{D}, can only be reduced by decreasing the wavelength or increasing the refractive index, neither of which may be desirable or possible [28]. This is valid for any free-space optical (or wave-based) analog computing system, as the ONL C is purely determined by the mathematical form of the desired functionality and not by the details of the optical implementation. Here, instead, we approach the problem of minimizing the thickness, and therefore the size, of the system from a different perspective—by modifying the form of the mathematical operation so that the thickness scales slowly as the operation dimension increases, while maintaining high accuracy for a given task. To achieve this, we first analyze the scaling laws of the ONL, C, for operations under various constraints, and clarify the relationship between the ONL, sparsity, and standard optical locality. Consider a device kernel \mathbf{D} of dimension N\times N, with input and output ports arranged in grids on two parallel planes (see Supplementary Note 1). For the same \mathbf{D}, the number of required sideways channels, C, may vary across different choices of cuts. The most important quantity is therefore \max(C), which is associated with the cut that is crossed by the largest number of sideways channels, and the entire device must be sufficiently thick to accommodate all the associated channels (see Supplementary Note 1 for the definitions of valid and invalid cuts). To analyze how the form of \mathbf{D} affects \max(C), we first study the worst-case scenario, where every output is coupled to all input points, resulting in a completely filled matrix \mathbf{D}. In this case, C=N, since the communication cone of every output intersects every cut. Next, we examine whether increasing sparsity in \mathbf{D} (by introducing zeros, i.e., removing couplings) reduces \max(C). Our analysis and numerical experiments (see Supplementary Note 1 for the proof and simulation) show that trivial sparsity does not help, except when \mathbf{D} becomes “extremely sparse,” as shown in Fig. 3a. However, an overly sparse kernel cannot encode enough free parameters, largely limiting its ability to realize useful optical functions. Therefore, trivially increasing sparsity is not a feasible way to reduce \max(C). One seemingly promising approach is to impose more drastic structural sparsity by removing a fraction of output ports, as this completely prevents those outputs from contributing to C. In matrix terms, removing outputs corresponds to setting rows of \mathbf{D} to zero. With a fraction \rho_{\text{row}} of rows remaining activated, \max(C) scales as \rho_{\text{row}}N, still depending linearly on N (Fig. 3b). Aiming for a better scaling law, we then encourage locality when imposing sparsity by constructing matrices whose entries D_{ij} are nonzero only when the in-plane distance d_{\parallel}(i,j) (Fig. 2d) is below a predefined threshold \max(d_{\parallel}). This, in turn, constrains all outputs’ communication cones, d_{\text{shift}}+w_{\text{cone}}/2\leq\max(d_{\parallel}). Matrices constructed in this way display a distinct feature: When viewed in a graph layout, all couplings deviate only slightly from vertical (Fig. 1c). We refer to these matrices as local sparse matrices. While reducing the in-plane distance crossed by individual couplings does not directly reduce the ONL, if this distance is, on average, small for all couplings, then the number of independent communication cones crossing any transverse aperture should be reduced, hence reducing the ONL. Our further analysis and simulation indeed reveal that, with local sparse matrices, \max(C) scales sublinearly as \mathcal{O}(N^{1/2}) (Fig. 3c). Eq. 2 summarizes the scaling laws for the expected values of the maximum ONL for the three considered types of matrices, corresponding to different forms of device kernel (see Supplementary Note 1 for details and proofs), \mathbb{E}[\max(C)]=\begin{cases}N&=\mathcal{O}\left(N\right)\text{ for % trivial sparse matrices},\\ \rho_{\text{row}}N&=\mathcal{O}\left(N\right)\text{ for row sparse matrices},% \\ 2\max(d_{\parallel})\cdot\left[\sqrt{2N}-\max(d_{\parallel})\right]&=\mathcal{% O}\left(N^{1/2}\right)\text{ for local sparse matrices with small $\max(d_{% \parallel})$}.\end{cases} (2) Figure 4: Space-efficient computing with free-space optics. Schematic of (a) conventional, (b) row sparse, and (c) local sparse ONNs performing a classification task on the fashion-MNIST dataset. (d-f) Thicknesses of the three interlayer regions in conventional (top row), row sparse (middle row), and local sparse (bottom row) ONNs. To demonstrate the trade-off between model thickness and accuracy, each model type is pruned using five different pruning thresholds (see Methods). Error bars represent one standard deviation of thickness (along y-axis) and accuracy (along x-axis) across eight different random seeds. A local sparse ONN with a pruning threshold of \tau=0.01, compared to a conventional ONN with \tau=0.05, drastically reduces the thickness while only compromising accuracy by 3.6\% on the fashion-MNIST dataset (vertical orange arrows; see Supplementary Note 6). Our findings imply that, while enforcing sparsity alone is not helpful, reducing the degree of standard optical nonlocality, quantified by d_{\text{shift}} and w_{\text{cone}} and bounded by \max(d_{\parallel}), also reduces the scaling of the maximum ONL, \max(C), with the operation dimension. Therefore, reducing the spatial complexity of a free-space optical computing structure requires a pruning method motivated by wave physics and fundamentally rooted in the concept of optical nonlocality. These results promise compact, scalable optical realizations of large-scale computations, provided that the target task can still be accomplished with high accuracy, as discussed next. I.2 Space-Efficient Computing with Free-Space Optics: Local Sparse ONNs Unlike artificial neural networks stored and manipulated in computer memory, optical (or, any wave-based) neural networks exist in the physical world. Many properties of these networks, including their optical locality, stem from the Euclidean distance defined in the position space, thus demanding physics-motivated pruning techniques. To translate the structural sparsity constraints identified above into an optics-specific pruning method, we leverage the recently proposed brain inspired modular training (BIMT) [29], which offers a perfect routine for pruning physical neural networks targeting for local sparse patterns. BIMT focuses on spatial networks [30], where each neuron in the network is assigned a position, enabling the definition of physical distances. To enhance locality, BIMT penalizes nonlocal channels using an additional loss term, \mathcal{L}_{\text{nonlocal}} (see Methods), and strategically swaps neurons and the associated weights across layers (except for the input). These features make BIMT an ideal method for designing local sparse optical neural networks (LSONNs), suitable for ultrathin free-space optical devices. To quantitatively examine the trade-off between the optical device’s minimum thickness and the model’s inference accuracy, we train LSONNs and their conventional counterparts on three datasets, MNIST [31], fashion-MNIST [32], and Kuzushiji-MNIST [33]. Then, we apply weight pruning to the resulting networks by removing weights with absolute values below a threshold \tau. (Here, weight pruning is also applied to conventional networks because, otherwise, the network would be fully connected, corresponding to an extremely thick device.) Each network has 4 bias-free layers with \left[784,100,100,10\right] neurons. The neurons in the input and hidden layers are arranged in square grids in position space. The 10 neurons in the output layer are arranged in a ring. Each layer, except the last one, is followed by a SiLU (Swish) activation function [34]. Once the network is trained, we calculate the minimum possible device thickness (i.e., the minimum distance between each two successive layers) for a free-space optical implementation, by calculating the maximum ONL and applying Eq. 1. Fig. 4 summarizes the thickness-accuracy trade-offs of conventional, row sparse, and local sparse ONNs. Remarkably, across all three datasets, LSONNs achieve a thickness reduction of more than one order of magnitude in t_{1,2} and reduce t_{3} by approximately a factor of 3 compared to their conventional and row sparse counterparts, with a slight or negligible drop in accuracy (e.g., only 3.6\% on the fashion-MNIST dataset). This thickness-accuracy trade-off can be interpreted as diminishing returns on accuracy: beyond a certain level of optical complexity and accuracy, additional spatial resources lead to only marginal improvements in accuracy. Additionally, we attribute this substantial reduction in thickness to a strategic allocation of the ONL across all transverse apertures: In LSONNs, large values of C always pass through apertures with large in-plane length l_{\text{cut}}, while in conventional and row sparse ONNs, large values of C often need to pass through narrow apertures (with small l_{\text{cut}}), causing “information bottlenecks” that require greater thickness to accommodate the necessary C (see Supplementary Note 2). In summary, we have demonstrated training LSONNs as an example of the proposed space-efficient paradigm for designing free-space optical computing structures that use an available volume optimally to perform an intended computation task: First, we have analyzed the scaling laws for the ONL, the key physical quantity associated with the spatial complexity of optical systems, and discovered that mathematical operations in the local sparse form can result in thin free-space optical structures. Then, by applying a physics-motivated pruning method that targets the required local sparse form, we have trained specialized ONNs that meet the desired criteria. This procedure is directly applicable to the designs of diffractive-surface- and metalens-based ONNs [35, 36], potentially enabling a new generation of scalable ultrathin free-space optical devices. I.3 Space-Efficient Computing on Photonic Chips: Analysis and Design Figure 5: Space-efficient computing on photonic chips. (a) Trade-off between model accuracy and the degree of block-diagonalization for different block-diagonal models. Shaded regions represent one standard deviation of accuracy across eight different random seeds. (b-e) Weights of block-diagonal and unpruned models. For each model, the number of free parameters, the involved unitary transformations (including both left and right unitary matrices, U and V^{T}, from the singular value decomposition of all weight matrices), and the total number of required MZIs are listed on the right. Optical networks on integrated photonic chips are typically based on arrays of photonic waveguides and modulators and meshes of Mach-Zehnder Interferometers (MZIs) performing arbitrary linear operations on the waveguide inputs [37, 38]. Optimizing spatial resources in this context, therefore, demands reducing the number of required MZIs, n_{\text{MZI}}. When realizing a device kernel matrix \mathbf{D} of dimension M\times N, an MZI-based architecture requires n_{\text{MZI}}=\left[M(M-1)+N(N-1)\right]/2 components to implement two unitary transformations, U(M) and U(N) [37]. However, if the kernel matrix could be reduced to a “block-diagonal” form, the scaling of n_{\text{MZI}} with both M and N would be reduced from quadratic to quasi-linear, as a block-diagonal matrix only comprises a linear number of small blocks with each block only requiring a few MZIs. This simplification of the photonic chip architecture becomes more apparent from a graph perspective: Block-diagonalization disassembles the huge bipartite connectivity graph into small, decoupled (disconnected) components, resulting in a significant loss of parameters and, more importantly, inevitably sacrificing inter-block connectivity (coupling), as shown in the connectivity graphs in Fig. 1d-f. Similar to free-space optics, the question is then whether a given operation can still be performed using a space-efficient architecture of this type, and what level of performance degradation can be expected. While this block-diagonalization approach is not expected to be applicable to all computational tasks, we find that relevant tasks solvable by neural networks are amenable to block-diagonalization, leading to a substantial reduction in spatial complexity. To optimize the trade-off between n_{\text{MZI}} and the ONNs’ performance, we propose a two-step pruning method. First, we include in the loss function an extra term, \mathcal{L}_{\text{off-bloc-diag}}, that penalizes all off-block-diagonal entries. No parameter is removed at this stage. Once the weights are trained, they are loaded into specially designed block-diagonal-structured (BDS) linear layers, where only entries in the diagonal blocks are registered as learnable parameters, while all other entries remain zero and are not updatable. The model with BDS linear layers is further trained on the same dataset to fully adapt to the block diagonal structure (see Methods). To evaluate the trade-off between n_{\text{MZI}} and the model’s inference accuracy, we train networks with several degrees of block-diagonalization on the three MNIST-like datasets used in the previous section [31, 32, 33]. Each network has 5 bias-free layers with \left[784,100,100,10,10\right] neurons. Each layer, except the last one, is followed by a SiLU activation function [34]. We consider six different block-diagonalized architectures, named from “b-d1” to “b-d6”, where n_{\text{MZI}} varies from approximately 3,200 to approximately 22,600. Fig. 5 summarizes the trade-off between n_{\text{MZI}} and accuracy. These six models share one important feature, the weight (size 10\times 10) connecting the last two layers remains unpruned, which we find is crucial for maintaining satisfactory accuracy. (Results on models with a pruned last weight are included in Supplementary Note 6.) Remarkably, model “b-d4”, with its largest block being only 10\times 10 in size, achieves accuracies of 93.24\pm 0.40\%, 84.51\pm 0.16\%, and 66.76\pm 0.25\% on the MNIST, fashion-MNIST, Kuzushiji-MNIST datasets, respectively. Implementing “b-d4” using the MZI-grid architecture already requires fewer than 4,200 MZIs, compared to the unpruned implementation’s approximately 0.3 million MZIs—a reduction of about 98.7\%. We also expect that emergent photonic components, for example, inverse-designed, programmable multiport couplers [39, 40, 41], could enable an even more compact and flexibly tunable on-chip implementation of similar models based on small, decoupled blocks. Finally, we extend this block-diagonalization approach to 1. develop block-circulant ONNs, whose weight matrices consist of rows of cyclically shifting blocks, leveraging light diffraction for space-efficient computing (e.g.,[42]); and 2. prune models optimized for edge AI applications, e.g., MobileNetV2 [43] for CIFAR-10 [44] classification. We block-diagonalize the fully connected (FC) layer of MobileNetV2 and, with a largest block size of 10\times 10, reduce n_{\text{MZI}} by \sim 99\% (considering the photonic implementation of FC layers only), while the accuracy decreases only slightly from 92.97\pm 0.24\% to 92.41\pm 0.25\%. (See Supplementary Note 4 for both models’ configurations and performance.) With these extensions, we envision that a hybrid architecture—employing digital implementations for convolution layers, nonlinear activations, shortcut connections, etc., and photonic analog components for the classifier based on block-diagonal (or block-circulant) FC layers—could facilitate FLOP-, memory-, and space-efficient computation acceleration in real-world edge applications."
https://arxiv.org/html/2411.10015v1,"MicroCrackAttentionNeXt: 
Advancing Microcrack Detection in Wave Field Analysis Using Deep Neural Networks through Feature Visualization","Micro Crack detection using deep neural networks(DNNs) through an automated pipeline using wave fields interacting with the damaged areas is highly sought after. However, these high dimensional spatio-temporal crack data are limited, moreover these dataset have large dimension in the temporal domain. The dataset presents a substantial class imbalance, with crack pixels constituting an average of only 5% of the total pixels per sample. This extreme class imbalance poses a challenge for deep learning models with the different micro scale cracks, as the network can be biased toward predicting the majority class, generally leading to poor detection accuracy. This study builds upon the previous benchmark SpAsE-Net, an asymmetric encoder–decoder network for micro-crack detection. The impact of various activation and loss functions were examined through feature space visualisation using manifold discovery and analysis (MDA) algorithm. The optimized architecture and training methodology achieved an accuracy of 86.85%.","Micro crack detection in materials is of significant importance due to the potential for catastrophic failures, which can lead to substantial financial losses and safety hazards in industries (Malekloo et al., 2022; Golewski, 2023). Detecting cracks in complex structures, like aircraft bodies or intricate machinery components, poses a substantial challenge using conventional methods like visual inspection or standard cameras, especially when dealing with complex geometries. The use of wave-based approaches for crack detection offers a powerful solution, as these methods allow for the analysis of structures that are not easily accessible or too complex to inspect manually. Convolutional Neural Networks (CNNs) are especially good at processing spatial data due to their ability to capture local spatial correlations within an image (LeCun et al., 2015). Nevertheless, standard segmentation methods, such as vanilla architectures, demonstrate limited performance on this particular dataset, due to the complex spatio-temporal nature of the crack patterns. This becomes even more significant when the cracks represent a tiny minority in the dataset, leading to poor detection accuracy. This issue is enhanced when dealing with very small cracks, as they not only lead to data imbalance but may also cause minimal disruption in wave behaviour. In such cases, the waves may exhibit minimal changes, making it difficult for the model to detect the cracks accurately. This challenge necessitates the development of a more tailored custom model. Our proposed MicroCrackAttentionNeXt is designed to overcome the limitations of vanilla models like UNet by incorporating enhanced spatial and temporal feature extraction. Unlike UNet Ronneberger et al. (2015), where the input and target share the same modality (image-to-image translation). Our model processes spatio-temporal input data and outputs spatial crack predictions, enabling it to handle more complex data while improving micro-scale detection accuracy. The asymmetric encoder-decoder structure, with attention layers is particularly effective as it focuses on capturing critical crack patterns rather than relying heavily on skip connections. The attention mechanism ensures that the model prioritizes the time steps when the waves interact with the cracks, improving detection precision. The DNNs capacity to recognise minute details and complex patterns in high dimensional data is impacted by the activation functions used, which becomes crucial in the micro-scale setting where accuracy is much needed. Activation functions enhance the network’s expressive power, enabling it to capture diverse features and representations. Rectified Linear Unit (ReLU) Nair and Hinton (2010) and its variants are commonly used activation functions. ReLU introduces non-linearity by setting negative values to zero, allowing positive ones to pass unchanged, which aids in deep network training. The ""dying ReLU"" issue, where neurons become inactive, hampers learning Xu et al. (2015); He et al. (2015a). Variants like Leaky ReLU mitigate this by allowing small negative slopes. SELU (Scaled Exponential Linear Unit) Klambauer et al. (2017) scales outputs to maintain self-normalizing properties, keeping activations near zero mean and unit variance. GeLU (Gaussian Error Linear Unit) Hendrycks and Gimpel (2023) enhances representation learning by incorporating probabilistic elements, though it has higher computational complexity. ELU (Exponential Linear Unit) Clevert et al. (2016) improves learning dynamics but is computationally expensive. Various Loss functions have been proposed in the literature to combat class imbalance issues in the DNN model. The loss functions tested are: 1)Dice LossLin et al. (2018), 2)Focal LossLin et al. (2018), 3)Weighted Dice LossYeung et al. (2021) and, 4)Combined Weighted Dice LossJadon (2020). These activation functions aim to strike a delicate balance between adaptability and computational efficiency, essential considerations in the micro-material domain, where capturing fine details is crucial for accurate crack detection. Empirical exploration and meticulous fine-tuning of these activation functions is imperative to identify the optimal choice that aligns with the distinctive characteristics of micro-material images. Ultimately, a nuanced and effective approach to crack detection in micro-materials relies on the thoughtful selection and optimization of activation functions within the CNN architecture. The extent of the influence of different activations is difficult to determine against conventional metrics such as accuracy and F1 score. Hence, it is imperative to analyse the internal dynamics of the model. Methods like Principal Component Analysis, t-SNE van der Maaten and Hinton (2008) and UMAP McInnes et al. (2020) are used to analyse the higher dimensional feature maps of these blackbox models against the target. However, these methods provide little to no insight when used on segmentation problems. In this study, we use the recently proposed Manifold Discovery Analysis (MDA) Islam et al. (2023) to qualitatively assess the impacts of various activation functions. Moreover, through this, we were able to analyse the effects activations had on the feature maps of the model, allowing us to choose the best activation function for the given problem. The primary contributions of this paper are: • Introducing MicroCrackAttentionNeXt – an incremental improvement over (Moreh et al., 2024). • Qualitative Investigation of the impact of activations MicroCrackAttentionNeXt through Manifold Discovery and Analysis. The paper’s structure is outlined in the following manner: Section 2 encompasses a concise yet informative overview of relevant studies. Section 3 deals with the dataset used and the proposed methodology. The assessment of the performance of the proposed system and the results obtained are included in Section 4. Ultimately, concluding remarks and future works are presented in Section 5."
https://arxiv.org/html/2411.09760v1,SpecPCM: A Low-power PCM-based In-Memory Computing Accelerator for Full-stack Mass Spectrometry Analysis,"Mass spectrometry (MS) is essential for proteomics and metabolomics but faces impending challenges in efficiently processing the vast volumes of data. This paper introduces SpecPCM, an in-memory computing (IMC) accelerator designed to achieve substantial improvements in energy and delay efficiency for both MS spectral clustering and database (DB) search. SpecPCM employs analog processing with low-voltage swing and utilizes recently introduced phase change memory (PCM) devices based on superlattice materials, optimized for low-voltage and low-power programming. Our approach integrates contributions across multiple levels: application, algorithm, circuit, device, and instruction sets. We leverage a robust hyperdimensional computing (HD) algorithm with a novel dimension-packing method and develop specialized hardware for the end-to-end MS pipeline to overcome the non-ideal behavior of PCM devices. We further optimize multi-level PCM devices for different tasks by using different materials. We also perform a comprehensive design exploration to improve energy and delay efficiency while maintaining accuracy, exploring various combinations of hardware and software parameters controlled by the instruction set architecture (ISA). SpecPCM, with up to three bits per cell, achieves speedups of up to 82× and 143× for MS clustering and DB search tasks, respectively, along with a four-orders-of-magnitude improvement in energy efficiency compared with state-of-the-art CPU/GPU tools.","\IEEEPARstart Mass spectrometry (MS) is a key analytical tool used by proteomics and metabolomics, aiding in drug discovery and chemical analysis by identifying and quantifying molecules based on their mass-to-charge ratios[1]. Its high sensitivity and precision have made it one of the most widely used techniques for detecting even the smallest molecular variations. However, one of the key challenges in MS processing is handling the vast and continually growing data volumes. For example, the MassIVE database, a publicly accessible repository for proteomics MS data [2], now exceeds 600 TB (as of September 2024) and continues to grow at an accelerating pace, with hundreds of terabytes of new spectral data added annually. The MS analysis process involves comparing spectra generated from MS experiments against an extensive reference library to identify proteins, a procedure known as database (DB) search [3]. To accelerate the search process, spectral clustering is done by grouping similar reference spectra together. During search, the query is first compared to the cluster centroids, quickly focusing the search on an appropriate cluster and thereby speeding up the overall process [4]. Ideally, such a database should be clustered on a daily basis as new samples are continually added, but this is currently done only once per year due to the excessive time required, resulting in lower accuracy. Traditional systems with separate memory and processor units are hindered by limited data movement bandwidth and computing efficiency in processing such a sheer amount of data. Modern MS analysis tools [5, 6, 7], whether based on conventional CPU or GPU architectures, often spend more than 60% of their time on large matrix operations with significant memory footprint. These tools struggle to manage large datasets efficiently, mainly due to the high energy consumption and latency involved in data transfer. To overcome these challenges, in-memory computing (IMC) has emerged as an alternative paradigm that processes data directly within the memory where it is stored, substantially reducing the latency and energy overhead caused by data movement. To capitalize on this opportunity, various memory topologies have been explored, including DRAM, SRAM, RRAM, PCM, NAND Flash, and other emerging memory technologies [8]. While RRAM has been widely adopted [9, 10, 11] for its well-recognized high-density and efficient read operations, particularly due to its support for multi-level cells (MLC), it faces limitations, such as high energy consumption and high voltage requirements during write operations. This drawback will significantly degrade the energy efficiency of the clustering process, where frequent data updates are required to adapt to the newly collected MS data. While NAND flash memories provide superior memory density and fabrication maturity, they suffer from relatively high latency, e.g., several \mu s[12], due to the high resistance in the read-path, which is caused by the nature of reading data from serially connected cells. This work adopts a recently developed multi-level phase change memory (PCM) [13] based on superlattice materials, which features lower error rates, reduced voltage requirements, faster and more energy-efficient programming. In particular, we aim to explore the analog IMC, which offers dramatic efficiency improvements, achieving more than two orders of magnitude benefit [8] compared to conventional digital counterparts by utilizing low-voltage swing analog operations. Additionally, the analog IMC performs both reading and computation across the entire bitcell array simultaneously, enabling high parallelism and significant compute density improvement. Consequently, the analog IMC on PCM facilitates efficient processing for classification while also enabling the effective updating of stored weights to adapt to newly collected MS data. From an algorithmic perspective, we employ hyperdimensional computing (HD), a brain-inspired computing paradigm that leverages lightweight and highly parallel operations by encoding input features into high-dimensional (long) binary vectors. HD replaces costly and complex floating-point arithmetic with simpler binary or integer operations, which can be executed in parallel, leading to dramatic throughput improvements as demonstrated in [14, 15, 16]. Furthermore, HD’s data representation in hyperspace offers significant error resilience, with data points being well separated by large geometric distances. This property has been demonstrated in previous work [10], where HD tolerated up to a 10% bit error rate for MS DB search tasks. This resilience creates a strong synergy with analog IMC on emerging devices, helping to overcome their computing and storage non-idealities while achieving greater storage density and computing efficiency. The use of MLC PCM involves complex trade-offs between efficiency and accuracy, which require careful optimization across both hardware and algorithms. These interrelated challenges cannot be addressed at a single abstraction level. Therefore, we introduce SpecPCM, an IMC accelerator designed for the efficient processing of MS workloads. This framework integrates design efforts across the entire vertical stack, spanning application, algorithm, circuit, device, and instruction set levels, to enhance performance throughout the end-to-end MS pipeline. The detailed contributions of this work are summarized as follows: 1. We propose an analog IMC system with architecture and circuits specifically tailored for MS algorithms. While prior works have applied IMC to MS database (DB) search tasks in the HD domain, our work is the first to apply IMC for both clustering and DB search. 2. At the algorithm level, we introduce a new HD encoding method, called dimension packing, to maximize storage density by leveraging multi-level PCM devices while maintaining the simplicity of the binary representation of HD vectors. 3. At the device level, we propose customized PCM devices to meet the distinct requirements of clustering and MS DB search by optimizing the materials differently for each task, based on measured characterization results from the fabricated devices. 4. We conduct hardware-software co-design through a comprehensive analysis to balance trade-offs between latency, energy efficiency, and accuracy, taking into account various parameters such as bits per cell, write-verify cycles, analog-to-digital converter (ADC) precision, and HD dimensions, all controlled by the instruction set. The results indicate that the proposed SpecPCM demonstrates speedup of up to 82\times for clustering and 143\times for database search over state-of-the-art (SoA) solutions, with a four-orders-of-magnitude in energy efficiency improvement, while maintaining on-par accuracy across datasets of different scales."
https://arxiv.org/html/2411.09546v1,Architectural Exploration of Application-Specific Resonant SRAM Compute-in-Memory (rCiM),"While general-purpose computing follows Von Neumann’s architecture, the data movement between memory and processor elements dictates the processor’s performance. The evolving compute-in-memory (CiM) paradigm tackles this issue by facilitating simultaneous processing and storage within static random-access memory (SRAM) elements. Numerous design decisions taken at different levels of hierarchy affect the figure of merits (FoMs) of SRAM, such as power, performance, area, and yield. The absence of a rapid assessment mechanism for the impact of changes at different hierarchy levels on global FoMs poses a challenge to accurately evaluating innovative SRAM designs. This paper presents an automation tool designed to optimize the energy and latency of SRAM designs incorporating diverse implementation strategies for executing logic operations within the SRAM. The tool structure allows easy comparison across different array topologies and various design strategies to result in energy-efficient implementations. Our study involves a comprehensive comparison of over 6900+ distinct design implementation strategies for EPFL combinational benchmark circuits on the energy-recycling resonant compute-in-memory (rCiM) architecture designed using TSMC 28 nm technology. When provided with a combinational circuit, the tool aims to generate an energy-efficient implementation strategy tailored to the specified input memory and latency constraints. The tool reduces 80.9% of energy consumption on average across all benchmarks while using the six-topology implementation compared to baseline implementation of single-macro topology by considering the parallel processing capability of rCiM cache size ranging from 4KB to 192KB.","Cache memory remains one of the critical components in our computing system, enhancing overall performance by bridging the speed gap between the main memory (RAM) and the central processing unit (CPU). Besides, in recent years, static random access memory (SRAM)-based in-memory computing paved a promising direction to enable energy-efficient computation. However, the lack of design and automation tools to map computation on optimal SRAM architecture increases design time-to-market, resulting in higher engineering costs. This research resolves this issue by proposing an architectural exploration tool that efficiently maps logic computations to optimal cache architecture. Figure 1: (a) Conventional Von Neumann architecture, where an operation f is performed on data D within the CPU, incurs high data movement overhead, which can be reduced using (b) a CiM architecture, where f is computed directly within the memory, with the CPU primarily functioning as a control unit. Computing-in-memory (CiM) architectures have emerged as highly promising solutions for data-intensive applications. They minimize data movement, enhance computational capabilities, and improve the system’s overall energy efficiency by processing and storing data within cache memory. As shown in Figure 1 (a), the traditional Von Neumann architecture relies on data communication between the arithmetic logic unit (ALU) and cache memory through address and data buses. However, as the CPU performance is significantly higher than the memory performance, the Von Neumann architectures often create memory bottlenecks. CiM architectures, as shown in Figure 1 (b), mitigate the impact of large memory access latencies by performing the computations within the memory. By reducing data movement and exploiting parallelism within the memory, CiM architectures significantly enhance computational efficiency and performance. SRAM-based CiM architectures have been heavily investigated for performing various operations, such as matrix-vector multiplication (MVM) [1, 2], multiply-and-accumulate (MAC) operations [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], boolean logic operations [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30], and content-addressable memory (CAM) [31, 32, 33, 34, 35, 36] operations for fast searching operations. However, none presents a generic energy-saving architecture that spans across various applications. This work utilizes a novel series-resonance-based resonant CiM (rCiM) architecture that reduces dynamic power consumption by recycling the wasted energy during writing operations. This work proposes an agile architectural exploration tool to map various logical operations to an optimal SRAM macro cache size. The primary objective of the tool is to facilitate the development of novel energy-efficient SRAM-based energy-recycling rCiM implementations individually designed for specific boolean logical applications. In particular, the main contributions of the paper are as follows: • A novel resonant Compute-in-Memory (rCiM) structure that incorporates a series inductor to recycle energy dissipated during write operations. • An architectural exploration toolflow that integrates open-source synthesis tools (Berkeley-ABC [37] & YOSYS [38]) to identify the optimal SRAM configuration within a specified range of SRAM cache memory and map efficient logical operations tailored to an optimal rCiM macro size. • Comprehensive analysis of 6900+ distinct logical design implementations for EPFL combinational benchmark circuits [39] using 12 different SRAM topologies."
https://arxiv.org/html/2411.08469v1,"Building Trustworthy AI: Transparent AI Systems via Large Language Models, Ontologies, and Logical Reasoning (TranspNet)","Growing concerns over the lack of transparency in AI, particularly in high-stakes fields like healthcare and finance, drive the need for explainable and trustworthy systems. While Large Language Models (LLMs) perform exceptionally well in generating accurate outputs, their ""black box"" nature poses significant challenges to transparency and trust. To address this, the paper proposes the TranspNet pipeline, which integrates symbolic AI with LLMs. By leveraging domain expert knowledge, retrieval-augmented generation (RAG), and formal reasoning frameworks like Answer Set Programming (ASP), TranspNet enhances LLM outputs with structured reasoning and verification. This approach ensures that AI systems deliver not only accurate but also explainable and trustworthy results, meeting regulatory demands for transparency and accountability. TranspNet provides a comprehensive solution for developing AI systems that are reliable and interpretable, making it suitable for real-world applications where trust is critical.","Symbolic AI, a fundamental branch of artificial intelligence, focuses on using structured representations of knowledge and formal logic to simulate human reasoning, problem-solving, and decision-making processes [27]. Unlike data-driven approaches such as machine learning and Large Language Models (LLMs), which derive patterns from vast amounts of unstructured data, symbolic AI models intelligence is based on rule-based systems that explicitly define rules, relationships, and logical structures. These structures include formal logic, ontologies, and semantic networks, enabling symbolic AI to work with well-defined rules to draw logical conclusions and make interpretable decisions [7]. A core component of symbolic AI is knowledge representation, where symbols denote real-world entities and their relationships. Representations in symbolic AI often employ hierarchical or graph-based structures such as semantic networks or ontologies [1, 15]. In contrast, LLMs GPT4 [22] and BERT [6], while highly effective in generating contextually relevant responses, face challenges in offering the same level of explainability due to their ""black box"" nature [32]. This gap is particularly concerning in high-stakes domains like healthcare, finance, and legal reasoning, where trust and transparency are paramount. This gap between the complexity of LLMs and the demand for transparency poses a significant challenge for AI development, particularly given the legal requirements imposed by regulations like the EU’s General Data Protection Regulation (GDPR), which grants individuals the right to receive an explanation when subjected to automated decision-making processes [14]. In addition, the proposed AI Act in the European Union, for example, mandates that AI systems—particularly those relying on LLMs in high-risk applications—be designed with human oversight, transparency, and risk management at their core [10]. One of the key barriers to achieving trustworthiness with LLMs is the inherent uncertainty in their predictions, which can be influenced by factors such as noisy data, biases in training data, or insufficient training on edge cases. Addressing these uncertainties while maintaining transparency and trustworthiness is a critical challenge in developing LLM-based systems for real-world applications [23]. To address this, systems like the proposed TranspNet pipeline combine the strengths of LLMs with symbolic AI by integrating domain expert knowledge, retrieval-augmented generation (RAG), and formal reasoning frameworks like Answer Set Programming (ASP). This hybrid approach allows LLMs to benefit from the structured, logical reasoning of symbolic AI, ensuring that their outputs are not only accurate but also explainable and trustworthy [19]. Ontologies play a key role in this integration, providing a framework for verifying LLM outputs and enhancing their transparency, a characteristic central to symbolic AI [27]. By combining the power of LLMs with formal reasoning, retrieval-augmented generation, multimodal data processing, and robust documentation practices, our pipeline addresses the key challenges associated with explainability and trustworthiness in AI systems. The integration of formal logic through ASP further distinguishes our approach by providing a mechanism for verifying the logical soundness of the LLM’s outputs and addressing uncertainty in a structured and interpretable manner. Ultimately, our pipeline offers a comprehensive solution for developing LLM-based AI systems that are not only accurate but also explainable and trustworthy, meeting the needs of both academic research and industry applications."
https://arxiv.org/html/2411.08463v1,Symbolic-AI-Fusion Deep Learning (SAIF-DL): Encoding Knowledge into Training with Answer Set Programming Loss Penalties by a Novel Loss Function Approach,"This paper presents a hybrid methodology that enhances the training process of deep learning (DL) models by embedding domain expert knowledge using ontologies and answer set programming (ASP). By integrating these symbolic AI methods, we encode domain-specific constraints, rules, and logical reasoning directly into the model’s learning process, thereby improving both performance and trustworthiness. The proposed approach is flexible and applicable to both regression and classification tasks, demonstrating generalizability across various fields such as healthcare, autonomous systems, engineering, and battery manufacturing applications. Unlike other state-of-the-art methods, the strength of our approach lies in its scalability across different domains. The design allows for the automation of the loss function by simply updating the ASP rules, making the system highly scalable and user-friendly. This facilitates seamless adaptation to new domains without significant redesign, offering a practical solution for integrating expert knowledge into DL models in industrial settings such as battery manufacturing.","Machine learning models, particularly deep learning (DL), have demonstrated remarkable success across a variety of fields, including image recognition, natural language processing, and predictive analytics [15]. However, in domains where rules, constraints, and logical reasoning are critical, such as healthcare, autonomous systems, engineering, and finance, these models can face significant limitations. Traditional DL models are primarily data-driven and often function as black boxes, lacking the ability to incorporate explicit domain knowledge or reasoning capabilities [7]. One of the key challenges is that DL models may overlook domain-specific knowledge that is not easily captured within the dataset, leading to suboptimal or even incorrect predictions. For instance, a DL model predicting medication dosages could fail to recognize contraindications between medications unless domain knowledge is introduced. This limitation underscores the need for integrating symbolic reasoning and domain knowledge into the learning process to enhance model performance and trustworthiness. To address this challenge, we present a hybrid methodology that integrates deep learning with domain expert knowledge using ontologies [3, 13] and answer set programming (ASP) [12]. Our approach involves embedding domain-specific constraints and logical rules directly into the loss function of the DL model. By doing so, we create a loss function that balances predictive accuracy (data-driven learning) with rule adherence (knowledge-driven learning). This integration ensures that the model not only learns from the data but also adheres to domain-specific constraints, enhancing its suitability for high-stakes applications. By combining data-driven learning with symbolic reasoning, our methodology produces models that are both accurate and aligned with domain knowledge. This integration enhances explainability, as the logical rules provide insights into the model’s decision-making process, thereby ensuring higher levels of trustworthiness and interpretability [2]. Furthermore, unlike other state-of-the-art methods that may require extensive redesign when applied to different domains, our approach offers scalability and ease of adaptation. By simply updating the ASP rules within the ontology, the same pipeline can be applied across various fields without significant modifications. This feature is particularly advantageous in industrial settings like battery manufacturing, where rapid adaptation to new processes or regulations is essential [6]. In summary, our contribution lies in presenting a scalable and flexible hybrid approach that seamlessly integrates domain expert knowledge into the training of DL models. This methodology is applicable to both regression and classification tasks and can be generalized across diverse fields, particularly those requiring strict adherence to domain constraints."
https://arxiv.org/html/2411.08353v1,Everything You Wanted to Know About Consumer Light Management in Smart Energy,"Consumer lighting plays a significant role in the development of smart cities and smart villages. With the advancement of (IoT) technology, smart lighting solutions have become more prevalent in residential areas as well. These solutions provide consumers with increased energy efficiency, added convenience, and improved security. On the other hand, the growing number of IoT devices has become a global concern due to the carbon footprint and carbon emissions associated with these devices. The overuse of batteries increases maintenance and cost to IoT devices and simultaneously possesses adverse environmental effects, ultimately exacerbating the pace of climate change. Therefore, in tandom with the principles of Industry 4.0, it has become crucial for manufacturing and research industries to prioritize sustainable measures adhering to smart energy as a prevention to the negative impacts. Consequently, it has undoubtedly garnered global interest from scientists, researchers, and industrialists to integrate state-of-the-art technologies in order to solve the current issues in consumer light management systems making it a complete sustainable, and smart solution for consumer lighting application. This manuscript provides a thorough investigation of various methods as well as techniques to design a state-of-the-art IoT-enabled consumer light management system. It critically reviews the existing works done in consumer light management systems, emphasizing the significant limitations and the need for sustainability. The top-down approach of developing sustainable computing frameworks for IoT-enabled consumer light management has been reviewed based on the multidisciplinary technologies involved and state-of-the-art works in the respective domains. Lastly, this article concludes by highlighting possible avenues for future research.","Smart city and Smart energy are interrelated concepts designed to enhance the efficiency and sustainability of urban areas. The idea of smart city represents the extensive integration of \acICT in the daily lives of human beings in urban areas [1]. Implementing these technologies aims to elevate the living standards of residents by optimizing the effectiveness of services and infrastructure associated with cities. Smart city also refers to a city that integrates the physical infrastructure, information technology infrastructure, social infrastructure, and business infrastructure through \acIoT to harness the collective intelligence of its community [2]. On the other hand, smart energy encompasses a much broader scope than conventional energy. It can be perceived as a model akin to the “Internet of Energy” that deals with smart power generation, smart energy management, smart energy storage, and smart energy consumption. Energy refers to the characteristics of an object or system that determine its capacity to perform work. It exists in multiple forms, including potential energy, kinetic energy, chemical energy, and thermal energy. It is worth highlighting that the wide range of energy sources includes solar, fossil fuels, electricity, vibration, biomass etc. The significance of smart energy in smart cities is fundamentally rooted in the rapid growth of smart cities and the subsequent exponential increase in energy supply demand. Smart cities are reported to improve energy efficiency, minimize electronic waste, and decrease carbon emissions through the use of smart energy. Moreover, all forms of traditional energy, clean energy, green energy, sustainable energy, and renewable energy, integrated with \acICT technology, constitute smart energy. Consequently, the integration of smart energy concept in various physical processess forms \acE-CPS in smart cities. \acE-CPS employs advanced sensors, communication networks, and control systems to enhance energy efficiency and minimize environmental effects. These systems can adeptly manage energy resources, forecast demand trends, and modify energy output appropriately by integrating real-time data and \acAI algorithms. Consumer lighting constitutes a crucial application in smart city and smart home frameworks. On the other hand, it is considered as one of the most energy-consuming applications, which accounts for 30% to 40% of the total energy demand of smart cities [1]. The global smart consumer lighting market is valued at roughly 16.25 billion USD, and it is anticipated to expand at a \acCAGR of 20%, potentially reaching over USD 83.81 billion by 2032 [3]. Undoubtedly, this rapid expansion is primarily driven by the rising need for energy efficient lighting solutions and the escalating trend towards smart homes and smart cities. Smart consumer light management systems serve an instrumental role in converting traditional consumer lighting systems to smart lighting systems. The integration of \acIoT, smart sensors, and robust control strategies for energy saving makes it a widely adopted solution. The number of features in smart consumer lighting continues to grow with the advancement in \acICT technology. Subsequently, energy consumption also increases with the number of features, leading to most state-of-the-art smart consumer light management systems being energy hungry, demanding average power in terms of hundreds of milli-watts [4]. Conversely, sustainability is an essential aspect that must be integrated on a larger scale to comply with Industry 5.0 [5]. Thus, it demands a focus on optimizing power consumption in order to feature smart consumer light management systems with energy autonomy using harvesters with small form factor. Consumer light management system helps in reducing energy consumption and increases total lighting efficiency via smart sensors and advanced data analytics techniques. It can automatically adjust lighting configurations according to various factors such as occupancy, ambient light, and weather conditions, aiding cities in minimizing their carbon emission and reducing energy expenses. It augments the safety and security of residents by ensuring adequately illuminated streets, public areas, and indoor environments. Moreover, the application has extensively improved both energy efficiency and quality of life in smart cities. The evolution of consumer lighting application over the past decades has been presented in Fig. 1. Figure 1: Evolution of Consumer Lighting Technology The remainder of this survey is structured into nine sections. Section 2 discusses the related prior work and contributions made in this manuscript. Section 3 elucidates smart consumer light management, its significance, and its attributes. Section 4 presents the need for sustainability and various elements of smart consumer light. Section 5 illustrates power management strategies in consumer lighting management systems. Section 6 delineates energy harvesting, power conditioning, and storage techniques. Section 7 presents the sustainable computing architectures for smart consumer light management systems. Task scheduling and energy optimization techniques of these systems have been detailed in Section 8. The open research directions have been highlighted in Section 9. The manuscript has been concluded in Section 10. An appendix containing a list of acronyms used is provided at the conclusion of the manuscript."
https://arxiv.org/html/2411.08261v1,Control of Biohybrid Actuators using NeuroEvolution,"In medical-related tasks, soft robots can perform better than conventional robots because of their compliant building materials and the movements they are able perform. However, designing soft robot controllers is not an easy task, due to the non-linear properties of their materials. Since human expertise to design such controllers is yet not sufficiently effective, a formal design process is needed. The present research proposes neuroevolution-based algorithms as the core mechanism to automatically generate controllers for biohybrid actuators that can be used on future medical devices, such as a catheter that will deliver drugs. The controllers generated by methodologies based on Neuroevolution of Augmenting Topologies (NEAT) and Hypercube-based NEAT (HyperNEAT) are compared against the ones generated by a standard genetic algorithm (SGA). In specific, the metrics considered are the maximum displacement in upward bending movement and the robustness to control different biohybrid actuator morphologies without redesigning the control strategy. Results indicate that the neuroevolution-based algorithms produce better suited controllers than the SGA. In particular, NEAT designed the best controllers, achieving up to 25% higher displacement when compared with SGA-produced specialised controllers trained over a single morphology and 23% when compared with general purpose controllers trained over a set of morphologies.","Soft robotics is a sub-field of robotics that studies machines built with flexible and ductile materials, such as silicone rubbers [1]. This type of robots have demonstrated better performance in specific tasks related to healthcare [2], due to their morphology and behaviour that are heavily inspired by living organisms. Despite their promising applicability, soft robots face significant challenges, i.e., defining an adequate morphology design. Under a traditional approach of robot designing, considerable time and material resources are utilised since numerous alternative prototypes are tested physically [3]. On the other hand, under a soft robots approach, designing process is more complex due to the materials’ flexibility and mechanical properties being non-linear and difficult to characterise [4]. When a suitable design is found, the next step consists of designing an appropriate controller for the soft robot, a process that can be considered as intricate as finding a suitable morphology. The required strategies to control soft robots have two main considerations: (i) the soft materials constituting the robot can deform at every point, resulting in infinite degrees of freedom, including bending, extension, contraction, and torsion, and (ii) soft materials present non-linear and time-dependent properties. These aspects make modeling of soft robot behaviour and movement a difficult task [5]. A methodology that can assist the design of controllers for soft robots which is worth investigating is Neuroevolution (NE). NE focuses on evolving the topology and weights of artificial neural networks (ANNs) employing a genetic algorithm (GA) methodology. Arguably, the most efficient NE algorithm has proved to be Neuroevolution of Augmenting Topologies (NEAT) [6]. Furthermore, under the rationale that natural structures are composed of shape repetition and patters, an extension of NEAT was developed, namely Hypercube-based Neuroevolution of Augmenting Topologies (HyperNEAT) [7]. HyperNEAT evolves a singular type of ANNs named Compositional Pattern-Producing Networks (CPPNs), whose difference to traditional ANNs focuses around the use of periodic functions (e.g., sine and square wave), in order to generate patterns, like symmetry and repetition that help evolve more interesting topologies [8]. The fundamental objective of the presented research is assessing the suitability of NEAT and HyperNEAT as design engines for controllers of biohybrid actuators (BHAs), a particular type of soft robots’ components that are built utilising biological material, such as tissues or cells. Specifically, we analyse the capabilities of the NEAT and HyperNEAT to design controllers whose objective is to induce an upward bending movement to a given BHA. That BHA may be embodied into a catheter for targeted drug delivery to areas of the human body that are difficult to reach."
https://arxiv.org/html/2411.08210v1,BOSON: Understanding and Enabling Physically-Roust Phtonic erse Design with Adaptive Variation-Aware ubspace ptimization,"Nanophotonic device design aims to optimize photonic structures to meet specific requirements across various applications. Inverse design has unlocked non-intuitive, high-dimensional design spaces, enabling the discovery of compact, high-performance device topologies beyond traditional heuristic or analytic methods. The adjoint method, which calculates analytical gradients for all design variables using just two electromagnetic simulations, enables efficient navigation of this complex space. However, many inverse-designed structures, while numerically plausible, are difficult to fabricate and highly sensitive to physical variations, limiting their practical use. The discrete material distributions with numerous local-optimal structures also pose significant optimization challenges, often causing gradient-based methods to converge on suboptimal designs. In this work, we formulate inverse design as a fabrication-restricted, discrete, probabilistic optimization problem and introduce BOSON-1, an end-to-end, adaptive, variation-aware subspace optimization framework to address the challenges of manufacturability, robustness, and optimizability. We explicitly consider the fabrication process and differentiably optimize the design in the fabricable subspace. To overcome optimization difficulty, we propose dense target-enhanced gradient flows to mitigate misleading local optima and introduce a conditional subspace optimization strategy to create high-dimensional tunnels to escape local optima. Furthermore, we significantly reduce the prohibitive runtime associated with optimizing across exponential variation samples through an adaptive sampling-based robust optimization method, ensuring both efficiency and variation robustness. On three representative photonic device benchmarks, our proposed inverse design methodology BOSON-1 delivers fabricable structures and achieves the best convergence and performance under realistic variations, outperforming prior arts with 74.3% post-fabrication performance. We open-source our codes at link.","Integrated photonics has shown a wide range of applications in computing, communication, and sensing. Currently, many photonic devices are manually architected by tuning a few design parameters via inefficient trial and error, which relies heavily on expert knowledge and time-consuming simulations. In contrast, inverse design requires minimal physical prior knowledge and opens up non-intuitive, high-dimensional design spaces, making it possible to discover highly efficient and compact device designs [8, 13]. The adjoint method-based inverse design [8] is particularly powerful for its ability to compute analytical gradients of an objective with respect to high-dimensional design variables using only two simulations. While the adjoint inverse design can produce numerically plausible designs, a significant gap exists between pre-fab and post-fab performance. Figure 1: Inverse design often yields non-fabricable devices. Optimization difficulty leads to suboptimal designs. As illustrated in Fig. 1, the inverse-optimized design exhibits high performance, but the tiny structures within the design pattern are non-manufacturable, leading to severe post-fabrication performance degradation. Additionally, fabrication variations during lithography and etching, along with operational variations such as temperature drift, introduce robustness concerns that further compromise the performance of inverse-designed devices. Previous approaches have attempted to address manufacturability issues by controlling the minimum feature size (MFS) during optimization through heuristic methods, such as blurring or adding curvature penalties, to eliminate non-fabricable structures and mitigate post-fabrication performance drops [1, 12, 18, 9, 16, 19, 17, 23, 4, 6]. To enhance fabrication robustness, prior work models variations as uniform erosion and dilation of the device geometry and simultaneously optimizes objectives under different variation corner cases. However, this oversimplified method only marginally improves robustness as it fails to capture actual variations accurately. When considering multiple variation effects, exhaustive Monte Carlo sampling of all corner cases induces exponential simulation cost. Beyond fabricability, solving this discrete high-dimensional stochastic optimization problem is particularly challenging. As the adjoint optimizer is only driven by a single objective sparsely defined in the device output port, it leads to a poor objective landscape shown in Fig. 1, making the optimization highly sensitive to initialization and prone to getting trapped in unreasonable suboptimal solutions. To address the photonic device inverse design challenge, which requires both fabricability and robustness, we formulate this task as a fabrication-restricted, robust stochastic optimization problem. We propose a novel inverse design framework, BOSON-1, which enables effective optimization directly within the fabricable subspace with full variation awareness, ensuring efficient inverse design toward robust device structures. Our main contributions are as follows: \bullet We provide a comprehensive analysis of the fabricability and optimization challenges in inverse design and introduce BOSON-1, an adaptive, variation-aware inverse design framework for physically robust photonic devices. \bullet Fabrication-Aware Subspace Optimization: BOSON-1 integrates differentiable fabrication modeling into adjoint optimization to ensure devices in the fabricable subspace. \bullet Loss Landscape Reshaping: BOSON-1 largely reduces the specious local optima by introducing auxiliary dense objectives, enhancing gradient flow and improving optimization. \bullet Local Optima Escaping: We introduce a light-concentrated initialization method with conditional subspace relaxation, facilitating escaping local optima toward better solutions. \bullet Linear-Cost Variation-aware Optimization: We propose an adaptive variation optimization method based on novel axial corner sampling and worse-case optimization, reducing simulation costs from exponential to linear. \bullet On three photonic device benchmarks, our BOSON-1 achieves 74.3% performance enhancement on average compared to previous art, enabling variation-robust photonic inverse design with high efficiency."
https://arxiv.org/html/2411.07902v1,Bayes2IMC: In-Memory Computing for Bayesian Binary Neural Networks,"Bayesian Neural Networks (BNNs) provide superior estimates of uncertainty by generating an ensemble of predictive distributions. However, inference via ensembling is resource-intensive, requiring additional entropy sources to generate stochasticity which increases resource consumption. In this work, we introduce Bayes2IMC, an in-memory computing (IMC) architecture designed for binary Bayesian neural networks that leverage nanoscale device stochasticity to generate desired distributions. Our novel approach utilizes Phase-Change Memory (PCM) to harness inherent noise characteristics, enabling the creation of a binary neural network. This design eliminates the necessity for a pre-neuron Analog-to-Digital Converter (ADC), significantly improving power and area efficiency. We also develop a hardware-software co-optimized correction method applied solely on the logits in the final layer to reduce device-induced accuracy variations across deployments on hardware. Additionally, we devise a simple compensation technique that ensures no drop in classification accuracy despite conductance drift of PCM. We validate the effectiveness of our approach on the CIFAR-10 dataset with a VGGBinaryConnect model containing 14 million parameters, achieving accuracy metrics comparable to ideal software implementations as well as results reported in the literature using other technologies. Finally, we present a complete core architecture and compare its projected power, performance, and area efficiency against an equivalent SRAM baseline, showing a 3.8 to 9.6\times improvement in total efficiency (in GOPS/W/mm2) and a 2.2 to 5.6\times improvement in power efficiency (in GOPS/W). In addition, the projected hardware performance of Bayes2IMC surpasses that of most of the BNN architectures based on memristive devices reported in the literature, and achieves up to 20\% higher power efficiency compared to the state-of-the-art.","I-A Context and Motivation The growing demand for edge AI in applications such as medical diagnostics [1], facial identification and surveillance [2] and self-driving cars [3] where reliability and safety are of paramount importance, has heightened the need for systems that provide a measure of uncertainty while operating under significant resource constraints in terms of area and power. Advances in deep neural networks have ensured very high accuracy, but this has come at the cost of overconfidence and poor calibration [4]. More importantly, such networks are incapable of uncertainty quantification, especially out-of-distribution identification measured by epistemic uncertainty.[5]. Bayesian neural networks (BNNs) overcome this problem by allowing the weight parameters to be probability distributions rather than point estimates, thus encoding uncertainty in the parameter distributions [6, 7]. During inference, the network weights are then sampled to create an ensemble, and the outputs are combined in Monte-Carlo (MC) fashion to obtain predictions, confidence and uncertainty (Fig. 1). The creation of ensembles can be done in time, i.e., instantiating each ensemble one after another on the same hardware [8], or in space, i.e., instantiating all the ensembles simultaneously, using multiple copies of the hardware [6]. Both methods are resource-intensive compared to a traditional network whose parameters are point estimates and require only a single instantiation. The ensembling-in-time approach reduces net decision-making throughput while optimizing for power and total area. In contrast, the ensembling-in-space expends more area and power in order to deliver high net throughput. In addition, both these approaches require noise sources to generate random numbers. Thus, the deployment of Bayesian neural networks at the edge is challenging and requires careful hardware-software co-optimization [9, 10]. Figure 1: Top: Illustration of a Bayesian neural network (BNN) where weights take binary values upon sampling. Middle: Bayesian inference is performed by an ensemble of N_{MC} predictions combined through Monte Carlo sampling to obtain predicted class, prediction confidence, and uncertainty. Bottom: Block diagram of the proposed Bayes2IMC core architecture implementing BNN inference. The crossbar array of memristive devices is divided into a weight plane (WP) and a noise plane (NP). The WP stores the parameters z_{w_{ji}} obtained by reparametrizing the probability parameters p_{w_{ji}}, and the noise plane generates the stochasticity required for synaptic sampling. Binary weights w_{ji} are then generated by comparing these variables in hardware. Unlike traditional IMC architectures, the input x_{j}, j^{th} element of input vector \mathbf{x}, is accumulated based on the sign of w_{ji}. In this paper, we design and validate an area and power-efficient ensembling-in-time Bayesian binary neural netwrok in-memory compute (IMC) architecture named Bayes2IMC that leverages the inherent stochasticity of nanoscale memory devices. Fig. 1 provides a high-level description of this core. The nanoscale devices are arranged in a crossbar fashion, which is partitioned into two sections: a weight plane (WP) that stores the distribution parameters, and a noise plane (NP) that provides stochasticity necessary for sampling [6]. We also modify the traditional in-memory computing approach by routing inputs to an accumulator for multiply-and-accumulate (MAC) operations. This combination of routing strategy, row-by-row read flow, and on-the-fly sample generation during sensing enables inference without the need for an analog-to-digital converter, thereby improving area and power efficiency. I-B Background and Related Work In-Memory Computing: The IMC approach has been proposed to address the ‘von Neumann bottleneck’ [11] plaguing traditional computing platforms [12, 13, 14, 15, 16]. In such systems, certain computational operations are performed in memory without data movement, enabling high throughput at low area and power. IMC implementations based on digital Static Random Access Memories (SRAMs) [17, 18, 16, 19, 20] as well as those based on non-volatile memory (NVM) devices such as Phase Change Memory (PCMs), Resistive RAMs (RRAMs), Spin-Transfer Torque RAMs (STT-RAMs), and Spin-Orbit Torque Magnetic RAMs (SOT-MRAMs) have been explored for implementing frequentist deep learning models [21, 14, 22]. Compared to SRAM IMC implementations, NVM devices have following advantages: • Multi-level conductance for multi-bit parameter storage. • Near \mathcal{O}(1) matrix-vector multiplication (MVM) operation when arranged in crossbar fashion with input vector encoded as drive voltage [23]. However, NVM devices also pose several challenges for IMC MVM implementation. The high level of state-dependent programming [24] results in write error, and conductance drift results in instability of programmed weights [24, 10]. This severely impacts the overall classification accuracy. Additional sources of error include MVM errors due to IR drops on source-line due to high current in the all-parallel operation mode [18], and non-linear dependence of device current on applied voltage [25]. Furthermore, Analog-to-Digital converters (ADCs) are necessary to convert the analog output of the MVM operation to multi-bit digital values, and this impacts both the overall power consumption and area efficiency [18, 12]. Related works: We now discuss proposed IMC solutions in the literature that leverage the inherent stochasticity of NVM devices with the goal of implementing ensembling methods. An RRAM array-based Bayesian network with training implemented via Markov Chain Monte Carlo (MCMC) was demonstrated in [26], though the need for training in situ is time and resource intensive [27]. Bernoulli distributed noise of Domain Wall Magnetic Tunnel Junction (DW-MTJ) devices and SOT-MRAM were used in [21] and [9] respectively to generate Gaussian random variables. The latter uses a local reparametrization technique to reduce resource consumption. However, both of these works require the mean and standard deviation parameters to be stored separately and the contributions from multiple devices to be combined to generate a single random variate per the central limit theorem. This lead to expensive hardware requirements. An efficient implementation was presented in [10] that performs device-aware training and combines multiple devices to generate the required stochasticity per weight parameter. The authors of [28] use STT-RAM to implement binary neural networks and perform Bayesian inference using Monte-Carlo Dropout (MCD) methods [29], where neurons are randomly dropped out during inference leveraging a random number generated using STT-RAM’s stochasticity. While MCD-based networks are easy to train and deploy, they are reported to be less expressive, especially for out-of-distribution uncertainty estimations [30, 31]. To mitigate effects of conductance drift, periodic calibrations of each layer of the network was used to obtain optimal affine factors to reduce error in [24]. Alternatively, [10], proposed reprogramming at different time intervals based on the effect of drift on the available domain of programmable conductance values. However, this involves multiple reprogramming steps, which is resource-intensive. Many of these works also employ a digital-to-analog-converter (DAC) to provide analog voltage input vectors. This can result in errors due to I-V non-linearity in some devices, as discussed in [25]. Working around this problem, references [18, 9, 10] propose bit-slicing, wherein the input bits are sent one by one, and the accumulated outputs after analog-to-digital conversion are scaled by their binary positional weights and summed. Pulse width modulation (PWM) can also be used, as suggested in [13]. In addition to extra circuitry overhead – shift-and-add circuits in the former or PWM generator in the latter – all the methods described require an ADC for readout, which consumes up to 87\% power and 60\% of the total area of a core [12, 22]. To reduce the power and area overhead, the ADC use is typically multiplexed amongst several columns, affecting the overall throughput [18]. I-C Main Contributions In this work, we introduce Bayes2IMC, a PCM-based computing architecture designed for BNNs with binary weights. Binary BNNs are less resource-intensive because they need only one parameter to describe the weight distribution. The main contributions of the paper are as follows: • We formulate a principled way to utilize the inherent device stochasticity as a noise source for ensembling. • We devise a hardware-software co-optimized technique that is applied only on the output logits to reduce accuracy variations across multiple network deployments. • We propose a simple global drift compensation mechanism to reverse the effect of state-dependent conductance drift on network performance. This does not require frequent reprogramming or layer-by-layer calibration, only a simple rescaling of the read pulse. Our approach ensures no drop in accuracy, expected calibration error (ECE) [4] and uncertainty performance up to 10^{7} seconds. • By introducing row-by-row read, we avoid high output currents. Additionally, every read operation handles the binarization in tandem without requiring any throughput throttling step. This permits an ADC-less architecture, significantly improving the efficiency of the design. • We avoid using DACs at every row by streaming inputs directly into pre-neuron accumulator and applying fixed width and amplitude read pulses on bit-lines to sample weights. By doing so, we improve the area and power efficiency and avoid the MVM errors due to the non-linear dependence of device conductance on applied voltages. To the best of our knowledge, this is the first work to demonstrate variational inference-based binary BNNs on IMC. We achieve a projected total efficiency improvement of 3.8-9.6\times as compared to an equivalent SRAM architecture. The rest of the paper is organized as follows. In Section II, we review background material on Bayesian neural networks, uncertainty quantification and PCM devices. In Section III, we discuss the design, architecture and the co-optimization between algorithm or software and hardware. The accuracy performance and the hardware projections are discussed in Section IV. Finally, Section V concludes the paper."
https://arxiv.org/html/2411.07842v1,Sparsity-Aware Optimization of In-Memory Bayesian Binary Neural Network Accelerators,"Bayesian Neural Networks (BNNs) provide principled estimates of model and data uncertainty by encoding parameters as distributions. This makes them key enablers for reliable AI that can be deployed on safety critical edge systems. These systems can be made resource efficient by restricting synapses to two synaptic states \{-1,+1\} and using a memristive in-memory computing (IMC) paradigm. However, BNNs pose an additional challenge – they require multiple instantiations for ensembling, consuming extra resources in terms of energy and area. In this work, we propose a novel sparsity-aware optimization for Bayesian Binary Neural Network (BBNN) accelerators that exploits the inherent BBNN sampling sparsity – most of the network is made up of synapses that have a high probability of being fixed at \pm 1 and require no sampling. The optimization scheme proposed here exploits the sampling sparsity that exists both among layers, i.e only a few layers of the network contain a majority of the probabilistic synapses, as well as the parameters i.e., a tiny fraction of parameters in these layers require sampling, reducing total sampled parameter count further by up to 86\%. We demonstrate no loss in accuracy or uncertainty quantification performance for a VGGBinaryConnect network on CIFAR-100 dataset mapped on a custom sparsity-aware phase change memory (PCM) based IMC simulator. We also develop a simple drift compensation technique to demonstrate robustness to drift-induced degradation. Finally, we project latency, energy, and area for sparsity-aware BNN implementation in both pipelined and non-pipelined modes. With sparsity-aware implementation, we estimate upto 5.3\times reduction in area and 8.8\times reduction in energy compared to a non-sparsity-aware implementation. Our approach also results in 2.9\times more power efficiency compared to the state-of-the-art BNN accelerator.","Bayesian neural networks (BNNs) offer a key advantage by quantifying uncertainty in their predictions, which makes them more reliable, especially for risk-sensitive applications like diagnostics, surveillance, and autonomous vehicles [1, 2]. This prevents overconfident decisions, which is crucial in high-risk environments [3]. BNNs encode epistemic uncertainty by treating each parameter as a probability distribution, requiring ensemble predictors [4, 5]. However, this inference process is hardware-intensive as multiple instantiations are required for sampling [6, 7]. This paper explores efficiency gains by leveraging the inherent sparsity in Bayesian Binary Neural Networks, reducing the resource demands of ensembling. Related Work: BNN accelerators on CMOS and FPGA have been explored [8, 9], but they face the ‘von Neumann bottleneck’, leading to high energy, area consumption, and latency due to excessive data movement [10]. In-memory computing addresses this by co-locating memory and processing, using crossbars of non-volatile devices like PCM, RRAM, STT-RAM, and SOT-MRAM [11, 7], enabling efficient matrix-vector multiplication through Ohm’s and Kirchhoff’s laws. Figure 1: Left: Illustration of BBNN inference, where only a small fraction of synapses are probabilistic and thus participate in sampling. Center: Sparsity-aware optimization to exploit the layer sparsity of VGGBinaryConnect (only synaptic layers shown here). Only layers with a significant concentration of probabilistic synapses are utilized to create an ensemble of predictions, all available simultaneously. Right: Utilizing parameter sampling sparsity within these layers by separating rows with probabilistic synapses into ‘Stochastic Plane (SP)’, and the rest into ‘Deterministic Plane (DP)’. The DP and one of the SP ensembles provide one set of predictions. Thus, predictions are available one by one. RRAM, PCM, STT-RAM, and SOT-MRAM have been used to implement Bayesian accelerators with Gaussian parameters [12, 13, 11, 7]. Nanoscale device stochasticity has also been considered as a resource to generate noise for sampling using the central limit theorem, though requiring additional hardware [11, 7]. In addition, these methods require Gaussian mean and standard deviation values to be stored per parameter. Architecture combining two devices to generate a Gaussian weight, co-locating entropy source and parameter storage was proposed in [12]. However, the multi-level conductance involved introduces programming noise and requires reprogramming for drift mitigation. Binary parameter storage with Monte-Carlo dropout-based inference[14] was used in [15], which while easier to train, are less expressive for out-of-distribution (OOD) data [16, 17]. Sparsity: Many of the aforementioned works implement dense BNNs, where all parameters are used to create ensembles. To improve ensembling efficiency, theoretical studies have explored partial Bayesian inference, which involves sampling only select parameters or layers for ensembles [18, 19, 20]. In binary networks, increased sampling sparsity (fewer probabilistic synapses) has been reported as training progresses [21] due to greater predictor confidence and reduced uncertainty for in-distribution (IND) data [22]. In hardware, [12] performs ensembling only in the fully connected (FC) layers after feature extraction by the convolutional layers. In [7] only the deepest layers were sampled, which improved hardware efficiency but resulted in a larger number of ensembles for the deepest layers, and a drop in accuracy was observed. Contributions: The main contributions of this work as: • We propose a sparsity-aware PCM-based in-memory computing (IMC) architecture optimization for Bayesian Binary Neural Networks (BBNNs) that leverages both layer sparsity (LS)—sampling only deep layers—and row sparsity (LS+RS)—sampling only rows with probabilistic synapses (Fig. 1). Simulation results show no loss in accuracy or uncertainty quantification compared to the FP32 baseline, and a simple drift compensation technique maintains performance stability up to 10^{7} s. • We use the NeuroSIM [23] simulator to project hardware efficiency, and achieve up to 5.3\times smaller area, 8.8\times power efficiency and 12.5\times total efficiency with LS only and 45\times with LS+RS, compared to non sparsity-aware implementation. We also achieve 2.9\times higher power efficiency compared to the state-of-the-art."
https://arxiv.org/html/2411.07654v1,Spike Talk in Power Electronic Grids,"Emerging distributed generation demands highly reliable and resilient coordinating control in microgrids. To improve on these aspects, spiking neural network is leveraged, as a grid-edge intelligence tool to establish a talkative infrastructure, Spike Talk, expediting coordination in next-generation microgrids without the need of communication at all. This paper unravels the physics behind Spike Talk from the perspective of its distributed infrastructure, which aims to address the Von Neumann Bottleneck. Relying on inferring information via power flows in tie lines, Spike Talk allows adaptive and flexible control and coordination itself, and features in synaptic plasticity facilitating online and local training functionality. Preliminary case studies are demonstrated with results, while more extensive validations are to be included as future scopes of work.","The energy consumption of data centers has become a major concern in modern society. As the distributed energy resources (DERs) are increasingly promoted, the carbon footprint is also becoming more critical in power grids where large amount of data are involved [1]. Meanwhile, distributed generation is escalating the demand for coordinating control in cyber-physical microgrids to ensure operational reliability. In turn, challenges persist thereupon, involving delays [2] and susceptibility to cyberattacks [3]. It is therefore of much value to study on a decentralized transition of the operation paradigm to address both challenges. Under this scenario, Talkative Power has been accordingly developed, aiming to co-transfer power and information along transmission lines [4]. System resilience is effectively improved, while additional energy consumption on the transmission line is inevitable. besides, as Talkative Power relies on request-respond information exchange protocol, its scalability is limited when multiple agents are involved in information exchange simultaneously. In contrast, we delve into the realm of a publish-subscribe protocol, where information is fetched locally as needed. Navigated by the biologically plausible neuron model [5, 6], spiking neural network (SNN) has emerged with great advantage in energy efficient computation due to its event-driven feature. Beyond the von-Neumann computing architecture activated by real numbers and perceptrons, SNN leverages a leaky-charge framework instead that are triggered by asynchronous spikes. Empowered by SNN, we formalize the Spike Talk tailored for microgrids, harnessing power flow dynamics to infer remote information locally. As spiking neurons and spiking neural networks have the features of synaptic plasticity and spike-timing-dependent plasticity (STDP), the neuromorphic infrastructure also shows potential in online learning and effectively reduces the data and energy requirements for training. Furthermore, the necessity for communication channels is eliminated, thus effectively addressing the resilience challenges in microgrids. This article thus delineates the infrastructure of Spike Talk and explains it from the perspective of its decentralized and online learning features. The remaining parts of this article is organized as follows: Section II introduces the inspiration of neuromorphic infrastructure for power grids from the von Neumann bottleneck. Section III elaborates on the online learning potential of Spike Talk by investigating the training principles. Section IV presents a case study, and Section V concludes the entire article. By discussing its inherent advantages, more promising real-world applications are implied, which should be the future scope of our work."
https://arxiv.org/html/2411.07433v1,SDN-Based Smart Cyber Switching (SCS) for Cyber Restoration of a Digital Substation,"In recent years, critical infrastructure and power grids have increasingly been targets of cyber-attacks, causing widespread and extended blackouts. Digital substations are particularly vulnerable to such cyber incursions, jeopardizing grid stability. This paper addresses these risks by proposing a cybersecurity framework that leverages software-defined networking (SDN) to bolster the resilience of substations based on the IEC-61850 standard. The research introduces a strategy involving smart cyber switching (SCS) for mitigation and concurrent intelligent electronic device (CIED) for restoration, ensuring ongoing operational integrity and cybersecurity within a substation. The SCS framework improves the physical network’s behavior (i.e., leveraging commercial SDN capabilities) by incorporating an adaptive port controller (APC) module for dynamic port management and an intrusion detection system (IDS) to detect and counteract malicious IEC-61850-based sampled value (SV) and generic object-oriented system event (GOOSE) messages within the substation’s communication network. The framework’s effectiveness is validated through comprehensive simulations and a hardware-in-the-loop (HIL) testbed, demonstrating its ability to sustain substation operations during cyber-attacks and significantly improve the overall resilience of the power grid.","The power system is a complex network comprising generation, transmission, and distribution stages, where substations play a pivotal role in altering voltage levels. Traditionally, substations have relied on hardwired communication lines to manually operate devices such as switches and circuit breakers (CBs), resulting in cumbersome and maintenance-heavy configurations. In addition, legacy communication protocols (e.g., DNP3 and Modbus) exacerbate complexities due to inconsistent data mapping across different vendors’ products [1]. The International Electrotechnical Commission (IEC) introduced the IEC 61850 standard to mitigate these challenges, offering substantial benefits such as multi-vendor interoperability, reduced configuration efforts, cost-effective installation, and high-speed, Ethernet-based communication for time critical signals. While these advancements have transformed substations into more efficient and intelligent systems, they have also introduced new cybersecurity vulnerabilities. The digitization and increased use of ICT in substations have exposed these critical infrastructures to risks of cyber-attacks, potentially leading to system failures and significant operational disruptions. Recent cyber-attacks on critical infrastructure, such as the coordinated attack on Ukraine’s power grid that disabled 30 substations and left approximately 230,000 residents without electricity for six hours, underscore the urgent need for improved cybersecurity measures. Despite numerous efforts to develop cybersecurity standards and defense mechanisms for power grids, significant limitations remain, including: • Limited Comprehensive Cybersecurity Solutions: Many approaches address isolated aspects of cybersecurity, lacking a holistic approach to mitigate all identified vulnerabilities. • Challenges in Localization and Isolation of Attacks: Current methods often emphasize intrusion detection without effective mechanisms to localize and isolate attacks within substations. The previous work [2] by the same authors focused on the localization of malicious hosts. • Inadequate Real-Time Response Capabilities: Existing solutions generally lack the dynamic reconfiguration abilities to respond effectively to real-time cyber threats. Software-defined networking (SDN) is an emerging paradigm known for its dynamic, controllable, and flexible nature, suitable for high-bandwidth and dynamic applications. It separates the control and forwarding planes, thus enhancing network scalability and flexibility. However, this separation also increases the attack surface, necessitating robust security measures. Centralized SDN management is susceptible to various cyber-attacks, such as fault injection and distributed denial-of-service (DDoS) attacks. In addition, some current commercial SDN switches lack the functionality for remote port reconfiguration. This limitation hampers the ability to isolate compromised network segments effectively. Other commercial SDN switches are prohibitively expensive, hindering their widespread adoption in substation cybersecurity applications. As a result, malicious packets can continue to spread during cyber-attacks, potentially disrupting substation operations. Redundant protective intelligent electronic devices (PIEDs) are vital for continuous operation in digital substations, as attackers can compromise PIED functions by injecting malicious sampled values (SV) or generic object-oriented system events (GOOSE) packets. While previous research has focused on creating backup IEDs [3], these efforts have not fully addressed the cybersecurity. To address these challenges, this paper introduces an innovative smart cyber switching (SCS) framework that incorporates concurrent intelligent electronic device (CIED) integration, representing the first implementation of these methods in tandem. This approach leverages the strengths of both SCS and CIED technologies, providing an advanced, integrated solution for real-time, resilient cyber-physical interactions within digital substations. The SCS framework comprises: • Adaptive Port Controller (APC): A module that dynamically manages OpenFlow table rules and policies to reconfigure the network in real-time. • Network-Based Intrusion Detection System (IDS): An advanced IDS [4] designed to detect severe cyber-attacks, triggering the SCS to isolate compromised devices and invoking CIED to take over essential protection functions. The primary contributions of this paper include: 1. Development and implementation of the SCS framework utilizing APC for real-time, dynamic network reconfiguration to isolate cyber threats. 2. Introduction of CIED that replicates the protection functions of compromised physical IEDs, ensuring continuous protection and control. 3. Demonstrations of the framework’s effectiveness in mitigating SV and GOOSE cyber-attacks, ensuring resilient substation operations. This integrated approach offers a comprehensive solution to the cybersecurity challenges faced by modern substations, enhancing resilience and reliability. The remainder of the paper is structured as follows: Section II provides the information of the existing hardware-in-the-loop (HIL) testbed. Section III elaborates on the proposed SCS framework and its components. Section IV focuses on the application of the proactive substation security algorithm (PSSA) for attack mitigation and isolation using SCS. Section V presents simulation and validation results and describes the transition of protection functions to CIED. Section VI concludes with recommendations for future work."
https://arxiv.org/html/2411.07276v1,Empirical Quantum Advantage Analysis of Quantum Kernel in Gene Expression Data,"The incorporation of quantum ansatz with machine learning classification models demonstrates the ability to extract patterns from data for classification tasks. However, taking advantage of the enhanced computational power of quantum machine learning necessitates dealing with various constraints. In this paper, we focus on constraints like finding suitable datasets where quantum advantage is achievable and evaluating the relevance of features chosen by classical and quantum methods. Additionally, we compare quantum and classical approaches using benchmarks and estimate the computational complexity of quantum circuits to assess real-world usability. For our experimental validation, we selected the gene expression dataset, given the critical role of genetic variations in regulating physiological behavior and disease susceptibility. Through this study, we aim to contribute to the advancement of quantum machine learning methodologies, offering valuable insights into their potential for addressing complex classification challenges in various domains.","In the face of rapid mutations and variations in diseases, the ever-expanding volume of biological data presents unparalleled opportunities for unraveling intricate biological phenomena. Among these, gene expression analysis emerges as a cornerstone tool for comprehending the molecular mechanisms underlying diverse physiological and pathological processes. This comprehension leads to the accurate classification of cancer subtypes, which is essential for guiding individualized treatment strategies[1]. In this endeavor, the Golub et al. gene expression dataset has been serving as an instrumental resource facilitating numerous studies aimed at precisely categorizing different subtypes of leukemia based on their distinctive gene expression profiles. However, the sheer magnitude and intricacy of gene expression data pose formidable challenges to extracting meaningful insights. Traditional classification methods often face computational limitations when tasked with discerning patterns amidst the noise and high-dimensional feature spaces inherent in gene expression data. In this context, harnessing these principles of quantum mechanics, quantum computing offers to exponentially accelerate data processing tasks, resulting in a paradigm shift in computational power and efficiency[2]. In this paper, we conduct an experiment investigation on gene expression data, exploring and evaluating the efficacy of both classical and quantum computing approaches for solving key challenges in gene expression classification. Our approach integrates innovative methodologies in feature selection, classification algorithms, and complexity analysis to advance our understanding of biological systems. Specifically, as part of our gene expression data analysis, we utilize quantile normalization[3] to preprocess the Golub et al. dataset, ensuring uniformity and reliability of data in our subsequent analyses. After preprocessing, it’s significant to filter out relevant features to simplify the model and improve computational efficiency. To conduct a comparative study, we adopt both quantum and classical approaches for this task. The paper[4] highlights the effectiveness of Lasso in identifying relevant features from high-dimensional datasets, particularly in the field of genomics where the number of features often exceeds the number of samples. So we have employed LASSO Regularization (L1)[5] for classical means of feature selection. For the quantum approach to feature selection, we utilized D-Wave’s hybrid quantum-classical framework[6] leveraging D-Wave’s quantum annealers[7] to formulate the Quantum Unconstrained Binary Optimization (QUBO) problem. This hybrid approach allows us to explore alternative solutions for feature selection tasks in high-dimensional datasets. We have classified the data using both quantum and classical kernels, utilizing the features selected by both approaches. To evaluate the performance of both the classical and quantum kernels using multiple metrics, including the F1 score, balanced accuracy, and Phase Terrain Ruggedness Index (PTRI), geometric difference[8]. Finally, we conduct a comparative analysis of the computational complexity of quantum and classical kernels to evaluate their practical feasibility in large-scale gene expression analysis."
https://arxiv.org/html/2411.07023v1,The Inherent Adversarial Robustness of Analog In-Memory Computing,"A key challenge for Deep Neural Network (DNN) algorithms is their vulnerability to adversarial attacks. Inherently non-deterministic compute substrates, such as those based on Analog In-Memory Computing (AIMC), have been speculated to provide significant adversarial robustness when performing DNN inference. In this paper, we experimentally validate this conjecture for the first time on an AIMC chip based on Phase Change Memory (PCM) devices. We demonstrate higher adversarial robustness against different types of adversarial attacks when implementing an image classification network. Additional robustness is also observed when performing hardware-in-the-loop attacks, for which the attacker is assumed to have full access to the hardware. A careful study of the various noise sources indicate that a combination of stochastic noise sources (both recurrent and non-recurrent) are responsible for the adversarial robustness and that their type and magnitude disproportionately effects this property. Finally, it is demonstrated, via simulations, that when a much larger transformer network is used to implement a Natural Language Processing (NLP) task, additional robustness is still observed.","Results Experimental validation of adversarial robustness To experimentally study the adversarial robustness, we employed a PCM-based AIMC chip with tiles comprising 256\times 256 synaptic unit-cells [29]. Each unit-cell contains four PCM devices (see Fig. 1b). The weights obtained via HWA training are stored in terms of the analog conductance values of the PCM devices, where two devices are used to store the positive and negative weight components, respectively. The conductance variations associated with these conductance values are thought to be the primary reason for potential adversarial robustness. As shown in Fig. 1a, the intrinsic stochasticity associated with the conductance variations is likely to make the design of an adversarial attack rather difficult. The conductance variations themselves fall into two different categories. There is a non-recurrent category that results from the inaccuracies associated with programming a certain analog conductance value. This is typically referred to as programming noise [29]. Besides this non-recurrent noise component, there is a recurrent variation in the conductance values arising from 1/f noise [30] and Random Telegraph Noise (RTN) characteristic of PCM devices. In subsequent sections, we will investigate the role of the recurrent and non-recurrent noise sources with respect to adversarial robustness. For this study, we consider three different types of adversarial attacks and five different target platforms. The different types of attacks are (i) Projected Gradient Descent (PGD) [31], (ii) Square [32], and (iii) OnePixel [33]. The attacks range from targeting small (localized) to larger (for some attacks, entire) input regions. Specifically, the PGD attack was chosen, as it is equivalent to the Fast Gradient Sign Method (FGSM) attack when the starting point is not randomly chosen and the L_{\infty} norm is used [34]. Hence, it is superior to FGSM when more than one iteration is used to generate adversarial inputs. The Square and OnePixel attacks do not rely on local gradient information, and thus, are not affected by gradient masking, which is a defensive strategy aimed at diminishing the efficacy of gradient-based attacks by obfuscating the model’s loss function, rendering it less informative or harder to optimize[35]. The OnePixel attack only permutes a small region (i.e., a single pixel) of the input, rather than a larger region, as typically targeted by other attacks. The five different attack target platforms considered are: (i) the original floating-point model (Original Network (FP32)), (ii) the floating-point HWA retrained model (HWA Retrained (FP32)), (iii) a digital hardware accelerator with 4-bit fixed-point weight precision and 8-bit activation precision (Digital), (iv) a PCM-based AIMC chip modelaaaThe PCM-based AIMC chip model is described in the Methods section and a match between the model and experimental data is provided in Supplementary Note 1. (AIMC Chip Model), and (iv) a PCM-based AIMC chip (AIMC Chip). For the first target, an existing pre-trained model is used. For the second target, the pre-trained model is re-trained using HWA training. For the third, fourth, and fifth targets, the parameters of the re-trained model are mapped to the corresponding hardware and deployedbbbAll auxiliary operations are performed in floating-point precision.. To determine the effectiveness of adversarial attacks, different evaluation metrics can be used. Typically, the clean (baseline) accuracy is directly compared to the accuracy when adversarial inputs are inferred. In this paper, we adopt the Adversarial Success Rate (ASR) metric [36], which considers only samples that are classified correctly by the network. More specifically, the ASR is determined as follows. First, model predictions are determined for both clean and adversarial inputs. Using target labels of the clean inputs, the incorrectly classified clean inputs are identified. These target labels and the predictions of adversarial inputs (adversarial predictions), which originated from the incorrectly classified clean inputs, are then masked and discarded. Traditional ML evaluation metrics can then be computed using the masked target labels and adversarial predictions. We concretely describe the determination of the ASR in terms of accuracy using Algorithm 1 (see Methods). By adopting the ASR metric, we can evaluate and compare the performance of adversarial attacks which are generated and/or evaluated using different target platforms. To demonstrate this ability, in Extended Data Table 2, we compute both the test set accuracy and ASR during training for a floating-point Resnet-based CNN trained for CIFAR-10 image classificationcccIn Supplementary Note 3, we perform simulations for a more challenging image classification task. using two different types of attacksdddThese attacks are considered in all fore-coming experiments, and are described in greater detail below., PGD and OnePixel, with different parameters. For the ASR to be truely agnostic to the target platform, it should be independent of the network accuracy. It is observed that the ASR is sufficiently independent of the test set accuracy. It is only notably perturbed for extreme scenarios (i.e., when the test set accuracy is <25%). For each type of attack, we vary two parameters. The first parameter relates to the number of attack iterations, whereas the second parameter relates to the attack magnitude, i.e., the maximum amount of distortion the attack can introduce to the input. When evaluating the ASR as a function of different attack parameters, especially for those relating to magnitude, caution must be taken so that the ground truth label is not changed by extreme permutations. If the severity of an attack or the number of attack iterations is too high, then the ground truth labels of the generated adversarial inputs can differ from the original inputs, which is problematic as the underlying semantics of the original input have now changed. Consequently, for all experiments, the maximum value for each parameter is determined by manually inspecting adversarial examples and determining the points at which the ground truth label is changed (see Methods). To comprehensively compare the adversarial robustness for all target platforms, while considering the number of attack iterations and magnitude, a contour or 3D plot, per target platform, is required. These are difficult to compare relative to each other. Hence, instead, we generate an ASR for each target platform, which is indicative of the average behavior of the objective as a function of both parameters. This is generated by projecting a straight line from the bottom-left to top-right corners of each contour plot. Along this line, the ASR is extracted. Each point in this line is associated with two values: one governing the number of attack iterations and another governing the attack magnitude. A dual x-axis is used to associate these. For each evaluation platform, aside from the AIMC chip, the strongest attack scenario is considered, i.e., attacks are both generated and evaluated using the same platform. To both generate and evaluate attacks on the AIMC chip, hardware-in-the-loop attacks would have to be performed – such attacks are non-trivial to perform on stochastic hardware, and hence, instead, are the sole focus of the Hardware-in-the-loop adversarial attacks section. Consequently, for the AIMC chip, adversarial inputs are generated using the AIMC chip model. For the Software (FP32) evaluation platform, attacks are generated using the same platform with FP32 and HWA Retrained (FP32) weights, respectively. In Fig. 2a-c, the ASR quantity is related to both aforementioned parameters for the AIMC chip evaluation platform. As can be observed in Fig. 2d-f, the envelope for the floating-point HWA retrained model is smaller than the original floating-point model, meaning it is more robust. This result is consistent with prior work [14, 13, 16, 37]. The digital hardware accelerator has an even smaller envelope, meaning it is more robust than the floating-point HWA retrained model. It is noted that like the operation of the digital hardware accelerator, the evaluation of the floating-point HWA retrained model is deterministic. Contrastingly, the digital hardware platform is subject to quantization noise and accumulation error [38]. While this error is introduced during normal system operation, as digital hardware is typically deterministic (as is assumed here), it can be predicted in advance, i.e., when adversarial inputs are generatedeeeIn fact, it has been demonstrated that adversarial attacks targeting these deterministic systems can be more effective [39].. Critically, (i) there is a good agreement between the modelled ASR for the AIMC chip and the ASR rate of the hardware models on the AIMC chip, and (ii) the hardware experiments on the AIMC chip result in the smallest envelopes, and hence, the highest level of robustness to all investigated adversarial attacks. Source of adversarial robustness In this section, we present a detailed study of the various noise sources that contribute to the higher adversarial robustness observed experimentally on the AIMC chip (see Methods). Specifically, we investigate the role of four different noise properties – the recurrence (or lack thereof), location, type, and magnitude. For this study, we rely on the hardware model of the AIMC chip. As described earlier, non-recurrent noise sources introduce noise once during operation, i.e., when weights are programmed, whereas recurrent noise sources introduce noise multiple times during operation, i.e., when a MVM operation is performed. In the AIMC chip model, the recurrent noise sources are sampled multiple times (specifically per mini-batch), whereas non-recurrent noise sources are only sampled once. All noise sources are assumed to be Gaussian and centered around zero. The non-recurrent component of stochastic weight noise primarily arises from programming noise. The recurrent components of the stochastic weight noise primarily arises from both device read noise (1/f and RTN) and output noise, which is introduced at the crossbar output, before currents are read out using Analog-to-Digital Converters. Output noise includes 1/f noise from amplifiers and other peripheral circuits, as well as other circuit non-linearities (IR drop, sneak paths, etc.) of small magnitude that can be approximated as a random perturbation on the crossbar output. Hence, in the model, non-recurrent weight noise is modelled, and at the output, these combined effects are modelled using additive recurrent noise of a fixed magnitude which is assumed to be independent of the total column conductance. For each configuration, other deterministic noise sources, e.g., input and output quantization, are also modelled. Other non-deterministic noise sources are assumed to contribute negligiblyfffAs evidenced by the strong match between the AIMC chip and the AIMC chip model across all hardware experiments., and hence, are not modelled. In order to assess how three properties of stochastic noise sourcesgggIn Supplementary Note 4, we investigate how the inherent adversarial robustness of AIMC-based hardware is effected by temporal drift., namely, recurrence, location, and type affect adversarial robustness, we modify the behavior of the AIMC chip model to focus exclusively on a single stochastic noise source. Separately, we investigate both recurrent and non-recurrent noise sources at two locations: the weights, whose effect is determined as a function of the input, and at the output, whose effect is input-independent. A total of four stochastic noise sources are considered: (i) recurrent output noise, (ii) recurrent weight noise, (iii) non-recurrent weight noise, and (iv) non-recurrent output noise. The effect of the noise magnitude is determined by, for each noise source, modifying the noise magnitude such that the resulting test set accuracy is lower than the floating-point test set accuracy by a desired percentage (i.e., drop). We consider drops of 5% and 10%. In Fig. 3a, for a large number, n=1,000, of repetitions, the test set accuracy is reported for both non-recurrent and recurrent output and weight noisehhhIn Supplementary Notes 2 and 5, we investigate adversarial robustness to varying degrees of stochasticity and combinations of output and weight noise.. It is observed that, for non-recurrent and recurrent noise sources normalized to produce the same test set accuracy, the noise magnitude is approximately equal, i.e., the recurrence property does not effect the resulting average test set accuracy. For non-recurrent noise sources the variation of the resulting test set accuracy is larger. For noise sources normalized to produce a larger test set accuracy drop (10%), the noise magnitude is larger, compared to the smaller drop (5%). Intuitively, as the noise magnitude is increased, the test set accuracy decreases and the network becomes more robust to adversarial attacks. To investigate the effect of the recurrence, location, and type properties on adversarial robustness, further experiments are performed in Fig. 3b. Comparisons are made using one of the selected attacks, PGD. For the noise magnitudes associated with the smaller test set accuracy drop (5%), the ASR envelope is determined for n=10 repetitions, for the four aforementioned noise sources. Two key observations can be made: (i) output noise exhibits greater adversarial robustness compared to weight noise, and (ii) in addition to not effecting the average test set accuracy, the recurrence property does not effect the ASR. We postulate that weight noise leads to less robustness when normalized to produce the same error, as the effect of weight noise is input dependent, whereas output noise is not. In Fig. 3c-e, we further compare the ASR envelope for the PGD, Square, and OnePixel attacks. It is observed that, on average, the model with only output noise exhibits the highest adversarial robustness. The model with only weight noise is the least robust. The AIMC chip model and AIMC chip, with test set accuracy values of 84.85% and 84.31%, respectively, i.e., drops of 3.57% and 4.11%, exhibit greater adversarial robustness than the modified AIMC chip model with only weight noise, but less adversarial robustness than the modified AIMC chip model with only output noise. From this analysis, it can be concluded that, the type and magnitude properties of stochastic noise sources have the greatest influence on adversarial robustness. The location and recurrence properties have negligible influence. Hardware-in-the-loop adversarial attacks Next, we determine the efficacy of hardware-in-the-loop attacks, i.e., where it is assumed that the attacker has full access to the AIMC chip. We compare the efficacy of one white- and one black-box attack (PGD and OnePixel). White box attacks are especially difficult to perform for stochastic hardware, as the construction of representative hardware models (with minimal mismatch) is non-trivial, and in many cases, even unfeasible. While automated ML-based in-the-loop modelling [40] approaches can be utilized, they require a significant amount of data, which is instance specific. Hence, they are not considered in this paper. For hardware-in-the-loop attacks, when a white-box attack is deployed, to perform backwards propagation, for each layer, weights and cached inputs are required [41]. Additionally, the output(s) of the network is(are) required. As ideal, i.e., floating-point precision, weights cannot be programmed, there is some deviation between the target and programmed conductances, so the target weights (if known) cannot simply be used by the attacker. Additionally, read noise introduces random fluctuations when MVM workloads are executed. Representative weights can be inferred by solving the following optimization problem, as described in [42] \mathbf{\hat{G}}=\operatorname*{argmin}_{\mathbf{\hat{G}}}\sum_{b=1}^{B}\|% \mathbf{\hat{G}}\mathbf{x}_{b}-\tilde{\mathbf{y}}_{b}\|_{2}. (1) Inputs to each layer can be cached during normal operation by probing input traces to Digital-to-Analog Converters. To perform backwards propagation, this information can be used to construct a proxy network graph (see Methods), for which gradients can be computed in floating-point precision using the chain-rule. For sake of practicality, an ideal backwards pass is assumed, i.e., straight-through-estimators are used for regions which are non-differentiable, and all values are assumed to be in floating-point precision. It is noted that, as the next candidate adversarial input to the network is usually dependent on the the result of backwards propagation of the previous candidate input, this process cannot normally be pipelined. Consequently, depending on the operation speed of the chip, attacks generated using AIMC chips can be susceptible to low-frequency noise and temporal variations, such as conductance drift [43]. To mitigate these effects, as reprogramming all devices after each adversarial example is presented is not desirable, \mathbf{\hat{G}} can be re-inferred. It is noted that, for black box attacks, these effects cannot be effectively mitigated - except in the scenario where the attacker is aware of exactly when the chip was programmed. In Fig. 4, we compare the efficacy of hardware-in-the-loop attacks generated and evaluated using the AIMC chip to three different attack scenarios, where: (i) adversarial inputs are generated and evaluated using the digital hardware accelerator, (ii) adversarial inputs are generated and evaluated using the AIMC chip model, and (iii) adversarial inputs are generated using the AIMC chip model and evaluated using the AIMC chip. When evaluated using the AIMC chip, adversarial attacks generated using hardware-in-the-loop attacks on the AIMC chip are more effective than on the AIMC chip model. Hardware-in-the-loop attacks generated and evaluated on the AIMC chip model are marginally less effective. Critically, for all scenarios involving either the AIMC chip or AIMC chip model, additional adversarial robustness is observed compared to hardware-in-the-loop attacks both generated and evaluated on the digital hardware platform. We emphasize, that to take the required circuit measurements to perform hardware-in-the-loop attacks, even for black-box attacks, where the output logits are needed, significant knowledge about the underlying hardware is required. In some instances, direct traces may be unavailable, meaning that a combination of other signals must be used as a proxy, reducing attack efficacy. In others, developing a realistic hardware model is too time consuming and not practically feasible. Similarly to when a representative hardware model is attacked, unlike for deterministic systems, e.g., digital hardware accelerators, the generated adversarial inputs are specific to that particular instance of the hardware, and hence not useful for large-scale attacks targeting many instances at the same time. Applicability to transformer-based models and natural language processing tasks Finally, to determine whether additional adversarial robustness is still observed for much larger transformer models and different input modalities, we simulate a pre-trained floating-point RoBERTa model fine-tuned for one GLUE task, MNLI. The RoBERTa model has approximately 125M parameters and the MNLI task comprises 393K training and 20K test samples (pairs of sentences). When fine-tuning the model for the down-stream MNLI task, HWA training was performed (see Methods). This model exceeds the weight capacity of the IBM HERMES Project Chip, so instead of performing hardware experiments, simulations were conducted using the AIMC chip model. A number of different hardware attacks for NLP tasks with text-based input exist. These usually target either specific characters, words, or tokens [44]. One major challenge when generating adversarial attacks for NLP tasks is semantic equivalence. For images, small permutations are usually not perceivable. However, for text, small permutations are much more notable (even at a character level), and are more likely to alter the semantics of the input. We consider the Gradient-based Distributional Attack (GBDA) attack [45], which instead of constructing a single adversarial example, as done by most types of attacks, searches for an adversarial distribution and considers semantic similarity using the BERTScore metric [46]. This enforces some degree of semantic equivalence. In Extended Data Table Supplementary Tables, we list a number of adversarial text-based inputs for different \lambda_{sim} and n_{iters} parameter values, in addition to their respective BERTScore values. Similarly to as done in Section Experimental validation of adversarial robustness, we consider varying two attack parameters, which relate to the number of attack iterations and the magnitude, n_{iter} and \lambda_{sim}, respectivelyiiiThe \lambda_{sim} parameter is inversely proportional to the attack magnitude.. In Fig. 5, the ASR is reported for the first four target platforms (listed in Section Experimental validation of adversarial robustness). Additional robustness is once again demonstrated for the AIMC chip model for a range of attack parameter values and BERTScore values. This is significant, as it indicates that additional adversarial robustness is still observed for (i) different input modalities, and (ii) much larger and more parameterized networks."
https://arxiv.org/html/2411.06863v1,Computable Model-Independent Bounds for Adversarial Quantum Machine Learning,"By leveraging the principles of quantum mechanics, QML opens doors to novel approaches in machine learning and offers potential speedup. However, machine learning models are well-documented to be vulnerable to malicious manipulations, and this susceptibility extends to the models of QML. This situation necessitates a thorough understanding of QML’s resilience against adversarial attacks, particularly in an era where quantum computing capabilities are expanding. In this regard, this paper examines model-independent bounds on adversarial performance for QML. To the best of our knowledge, we introduce the first computation of an approximate lower bound for adversarial error when evaluating model resilience against sophisticated quantum-based adversarial attacks. Experimental results are compared to the computed bound, demonstrating the potential of QML models to achieve high robustness. In the best case, the experimental error is only 10% above the estimated bound, offering evidence of the inherent robustness of quantum models. This work not only advances our theoretical understanding of quantum model resilience but also provides a precise reference bound for the future development of robust QML algorithms.","Machine learning (ML) is increasingly integral in various applications such as speech recognition [1], computer vision [2], and natural language processing [3]. A significant concern in ML is its vulnerability to adversarial attacks, in which malicious actors exploit weaknesses in the models to produce outcomes favorable to the adversary [4, 5, 6]. Moreover, it is demonstrated that various ML classification models are at risk when adversaries craft examples specifically designed to mislead them [7, 8]. Alongside classical machine learning (CML), quantum machine learning (QML) has emerged as a promising new paradigm. Conceptually similar to classical counterparts, QML involves iteratively optimizing models across training samples to achieve intended functionalities [9]. The key distinction between quantum and classical models lies in the use of quantum computing. QML exploits quantum mechanical phenomena such as superposition and entanglement, which are core principles of quantum computing. The potential of quantum computing has been demonstrated in various domains [10, 11, 12, 13]. Moreover, in the context of ML, quantum systems offer the possibility of manipulating exponentially large state spaces efficiently [14]. QML shows promise but faces challenges in practical implementation, particularly regarding vulnerability to adversarial attacks. While some studies suggest QML models may exhibit resilience to certain adversarial transfer attacks designed for classical models [15], the overall security landscape remains open. Studies have primarily focused on adversarial examples using classical input data in trained quantum classification models, observing similar misclassification phenomena to those in classical models [16]. However, these insights provide limited guidance for designing robust QML models. The field is further constrained by limitations in classical simulation capabilities and noisy quantum hardware, hindering extensive experimental studies. Theoretically, the security of QML in handling exponentially larger spaces remains debated [17]. However, some research indicates that, under certain assumptions, the scaling of security risks in QML is comparable to that in CML [18]. Along with these results, in the context of adversarial attacks, it is crucial to have a fundamental understanding of adversarial performance limitations posed by the data distribution and model accuracy. Such limitation can be formulated as a bound on adversarial error rate, which captures the chance of success for an adversary to induce an incorrect outcome for QML models. This provides detailed insights into the minimum inherent adversarial vulnerability in QML and informs us on more effective defense mechanisms to reach these limits. The proliferation of QML technologies requires a security guarantee because, in the future, QML may be used in sensitive applications. Contributions. In this work, we present a novel, computable lower bound on the adversarial error rate, which provides a practical reference for model performance in adversarial attacks regardless of the quantum model’s architecture. Our algorithm addresses the classical perturbation attack prevalent in the literature and the quantum perturbation attacks unique to the quantum model. We devise and test an example of such an attack based on the Projected Gradient Descent (PGD) [19]. In Sec. V, we validate the effectiveness of the bound by comparing the derived bounds with the actual adversarial error rates observed in quantum models. The experimental results demonstrate our proposed bounds’ practical effectiveness and applicability in real-world scenarios. Overall, the key contributions are the following: • Lower bound estimation method for QML: We present a new algorithm for quantum scenarios inspired by classical adversarial risk bound estimation methods [20]. Our approach is model-independent and is capable of addressing the unique quantum challenge of quantum perturbation attacks. Utilizing parallel computing, the algorithm efficiently computes the bound on an adversarial error and informs us of the limit of adversarial performance for any possible quantum models. • Evaluation of practical effectiveness: Our bound estimation algorithms are investigated against empirical benchmark data, showing a strong correlation between derived bound and observed adversarial error rates in quantum models. The results, detailed in Sec. V, confirm the practical applicability of our proposed bounds through benchmark scenarios. • Novel quantum attack strategy: We have developed a new quantum attack strategy in the attack scenario of quantum perturbation attack. It is based on the Projected Gradient Descent (PGD) [19], a strong and widely-studied gradient-based attack in CML."
https://arxiv.org/html/2411.06556v1,"EO-GRAPE and EO-DRLPE: Open and Closed Loop Approaches
for Energy Efficient Quantum Optimal Control","This research investigates the possibility of using quantum optimal control techniques to co-optimize the energetic cost and the process fidelity of a quantum unitary gate. The energetic cost is theoretically defined, and thereby, the gradient of the energetic cost for pulse engineering is derived. We empirically demonstrate the Pareto optimality in the trade-off between process fidelity and energetic cost. Thereafter, two novel numerical quantum optimal control approaches are proposed: (i) energy-optimized gradient ascent pulse engineering (EO-GRAPE) as an open-loop gradient-based method, and (ii) energy-optimized deep reinforcement learning for pulse engineering (EO-DRLPE) as a closed-loop method. The performance of both methods is probed in the presence of increasing noise. We find that the EO-GRAPE method performs better than the EO-DRLPE methods with and without a warm start for most experimental settings. Additionally, for one qubit unitary gate, we illustrate the correlation between the Bloch sphere path length and the energetic cost.","The field of quantum computing (QC) is undergoing rapid development in both theoretical and practical aspects from both academic and industrial stakeholders. At its foundation, QC involves orchestrating quantum mechanical properties for information processing [1, 2] via quantum algorithms, thereby promising a significant reduction in computational resources [3] for specific applications over their classical counterparts. Exemplary use cases include simulating quantum mechanical systems [4] towards novel discovery in material sciences and breaking cryptographic protocols [5] ubiquitous for secure transactions over the internet. The required orchestration of quantum information towards QC is physically implemented by engineering quantum processing units (QPU), which are currently prototyped using a myriad of technologies like superconducting circuits, trapped ions, photonics, electron spin, etc. A significant challenge in the QC field [6, 7, 8] is to manufacture better quality and scalable QPU against the fragility of quantum information from environmental noises and operational imperfections. Akin to classical computers, the interface between the QC applications and the processor is organized into translation layers, called the quantum computation stack [9, 10], as shown in figure 1. From top to bottom, first, the application is formulated as a quantum algorithm and expressed in a quantum programming language. Then, a quantum compiler decomposes and optimizes the high-level code into native operations supported by the target QPU. Thereafter, the quantum microarchitecture schedules and issues the low-level instructions in real-time. These instructions (like initialization, unitary gates, and measurements) further need to be translated to corresponding analog pulses that optimally control the accessible degrees of freedom of the quantum system. These electromagnetic analog signals perform the necessary transformation for synthesizing quantum unitary gates on specific addressable qubits while mitigating the undesirable effects of noise. Eventually, these hardware-aware signals implement the desired logical operation dictated by the hardware-agnostic quantum algorithm on the target QPU. Figure 1: Overview of the system design of a quantum accelerator with classical control and various software modules required for research and development is shown on the left. The different abstraction layers for full-stack quantum computing are shown on the right. This research pertains to the optimal control layer highlighted with a dotted outline. The QC stack, as discussed above, provides abstraction layers to organize the research and development. Thus, operations, including a computationally universal set of gates, initialization, and measurements, are provided as primitives for the microarchitecture layer. In the ideal scenario, the pulse-level implementation details of these operations are predetermined in the control layer during the characterization and calibration of the QPU prior to deployment. However, in the current noisy intermediate-scale quantum (NISQ) computing era, this abstraction [11] is preventing further optimization of the QPU performance. Pulse-level control, when exposed to higher stack layers, allows advanced operating procedures that can mitigate the effect of noise dynamics and fine-tune a more extensive set of unitary gates with respect to other dependencies (e.g., cross-talk). While there are many advantages to pulse control, such as a high degree of flexibility and higher speed of gate execution, there are also certain downsides to using pulse control, such as calibration requirements and overhead and scalability bottlenecks. Nevertheless, advancing research in this direction is not only imperative for current QC roadmaps to extract maximal performance but also allows a principled theoretical understanding of the limits of quantum information processing using classical control algorithms and electronics. Current pulse control approaches focus primarily on optimizing the fidelity score. Motivated by the theoretical possibility and operational need, this research concerns a novel approach for pulse-level unitary gate synthesis, that co-optimizes the fidelity along with the pulse energy. Within the allied field of quantum thermodynamics, there is a growing interest in the possibility of achieving quantum advantage through the perspective of energy efficiency [12] instead of computational resources like runtime and memory needs. This is imperative for at least two specific reasons. Firstly, one of the earliest motivations of quantum computation stems from reversible computation [13] with minimal energetic cost instead of a computational complexity advantage. Secondly, quantum processors within a quantum accelerator stack are benchmarked in performance against state-of-the-art classical high-performance computing (HPC) systems. In such HPC systems, optimizing and benchmarking the energetic cost [14] is motivated by both operational costs and environmental sustainability directives. While this direction is gaining traction, specific research in the energetic cost of a quantum computational process is both sparse and theoretical at present. For example, [15] proves an inequality bounding the change of Shannon information encoded in the logical quantum states by quantifying the energetic cost of Hamiltonian gate operations. Subsequently, [16] showed that optimal control problems can be solved within the powerful framework on quantum speed limits and derive state-independent lower bounds on energetic cost. Recent work in quantum optimal control (QOC) theory has primarily focused on developing control to carry out quantum processes with the highest fidelity possible. These processes include quantum processes such as state initialization, quantum measurements, and implementing quantum unitary gates. However, in the context of the growing impetus in achieving quantum advantage through energy efficiency [17], it seems crucial to investigate the energy efficiency of quantum operations, in particular, unitary quantum gates within a pragmatic quantum compiler framework. To this effect, we address three main research questions in this article: \mathcal{RQ}_{1}: How can we estimate the energetic cost of synthesizing a quantum unitary gate? \mathcal{RQ}_{2}: What is the relation between the fidelity of unitary synthesis and the energetic cost associated with the pulse? \mathcal{RQ}_{3}: How can fidelity and the energetic cost be co-optimized within existing quantum optimal control strategies? The core contributions of this research are summarized below: 1. A theoretical formulation of the energetic cost of implementing a quantum unitary gate using discrete control pulses and the gradient of the energetic cost with respect to the control parameters that are required to optimize the cost. 2. Development of a modified version of the gradient ascent pulse engineering (GRAPE) algorithm to co-optimize a quantum unitary gate’s fidelity and energetic cost. The proposed novel energy-optimized algorithm is called EO-GRAPE. 3. Identification and analysis of the trade-off between the fidelity and energetic cost of implementing a quantum unitary gate. 4. Development of a deep reinforcement learning (DRL) agent able to learn and generate energy-optimized control pulses for a universal set of quantum unitary gates. The proposed novel energy-optimized method is called EO-DRLPE. 5. Benchmarks to evaluate the performance of the EO-GRAPE algorithm and EO-DRLPE method with increasing noise levels. 6. Evaluation of the optimality of the geodesic path cost of the two methods for synthesizing 1-qubit unitary gates. The rest of the article is organized as follows. Section 2 provides background information on pulse engineering for unitary synthesis. Thereafter, a non-exhaustive survey and classification of quantum optimal control techniques is provided. The energetic cost of a quantum operation is introduced. In section 3, the GRAPE algorithm is introduced. Thereafter, we present our novel EO-GRAPE algorithm that co-optimizes the energy and process fidelity based on the derived gradient. The trade-off between these factors is explored. Section 4 introduces the alternative closed-loop approach, EO-DRLPE and its consecutive performance analysis. In section 5, we provide an analysis of the correlation between the energetic cost and Bloch sphere path length for single qubit unitary synthesis. Section 6 concludes the article and provides suggestions for future research directions."
https://arxiv.org/html/2411.06362v1,Will Central Bank Digital Currencies (CBDC) and Blockchain Cryptocurrencies Coexist in the Post Quantum Era?,"This paper explores the coexistence possibilities of Central Bank Digital Currencies (CBDCs) and blockchain-based cryptocurrencies within a post-quantum computing landscape. It examines the implications of emerging quantum algorithms and cryptographic techniques such as Multi-Party Computation (MPC) and Oblivious Transfer (OT). While exploring how CBDCs and cryptocurrencies might integrate defenses like post-quantum cryptography, it highlights the substantial hurdles in transitioning legacy systems and fostering widespread adoption of new standards. The paper includes comprehensive evaluations of CBDCs in a quantum context. It also features comparisons to alternative cryptocurrency models. Additionally, the paper provides insightful analyses of pertinent quantum methodologies. Examinations of interfaces between these methods and blockchain architectures are also included. The paper carries out considered appraisals of quantum threats and their relevance for cryptocurrency schemes. Furthermore, it features discussions of the influence of anticipated advances in quantum computing on algorithms and their applications. The paper renders the judicious conclusion that long-term coexistence is viable provided challenges are constructively addressed through ongoing collaborative efforts to validate solutions and guide evolving policies.","When considering CDBC and blockchain-based cryptocurrencies, the famous lines from the poem ’The Road Not Taken’ by Robert Frost come to mind: ’Two roads diverged in a yellow wood, And sorry I could not travel both and be one traveler, long I stood’ [1]. However, unlike the poem, in this paper, we will discuss the possibility of choosing both roads in the post-quantum era. Metaphorically, we can liken the practical aspects of this issue to those in quantum physics, known as ”quantum superposition”, where elementary particles exist in two parallel states simultaneously until their location is observed at a specific point [2]. The quantum world supports both digital currencies approaches, and it is up to the decision-maker to determine what to choose. In this era, a notable trend is our significant progress towards the post-quantum era. This phase denotes the time following the advancement of quantum computing to a level where it can potentially compromise prevalent public-key cryptosystems like Rivest–Shamir–Adleman (RSA) and Elliptic Curve Cryptography (ECC) [3]. In parallel, we see a rise in popularity of CDBC [4] side by side to rise in usage of cryptocurrency [5]. Within this context, the importance of protocols such as Oblivious Transfer/Protocol (OT) and Multi-Party Computation (MPC) remains paramount in ensuring secure and private transactions [6]. OT is a cryptographic protocol where a sender transmits one of potentially multiple pieces of information to a receiver without knowing which specific piece the receiver obtains, enabling the receiver to access information from the sender without disclosing the chosen item’s identity [6]. The sender merely acknowledges the transfer but remains unaware of the item’s index. OT finds utility in scenarios such as privacy-preserving data mining and secure two-party computation. Notable examples encompass 1-out-of-2 OT, where the sender presents two data pieces for the receiver to select from discretely, and 1-out-of-N OT, where the sender offers N data pieces for selection without revealing the specific item index [7]. OT serves as a foundational element in secure two-party and multi-party computations, facilitating collaborative function computations over private inputs while maintaining confidentiality. MPC extends privacy protection to scenarios involving multiple entities collaborating to compute functions over their respective confidential inputs [8]. For instance, banks can collectively calculate credit scores without disclosing individual customer data. In MPC, input data is partitioned and shared among distinct, non-colluding parties to prevent any single entity from accessing another’s confidential information. Subsequently, algorithms operate on these distributed data shares to produce outputs in a privacy-preserving manner. MPC methodologies commonly rely on secret sharing, garbled circuits, and cryptographic tools like Homomorphic Encryption (HE)111A cryptographic method enabling calculations on encrypted data without the need for decryption [9]. and Zero-Knowledge Proofs (ZKPs) 222A protocol where one party can prove the truth of a given statement to another party without revealing any additional information beyond the statement’s truth [10]., with OT serving as a fundamental component in many MPC protocols. The application spectrum of MPC spans privacy-centric technologies, blockchain ecosystems, and distributed analytics involving sensitive data, offering a secure framework for collaborative computation while safeguarding individual data privacy. 1.1 Reasons for Rise of Central Bank Digital Currencies (CDBC) Based on a recent survey, at the conclusion of 2021, 90% of the 81 respondent central banks in Europe were actively exploring the potential for a CBDC [11]. In the latter half of 2023, the traction supporting CBDCs has persisted. Recent findings from CBDC tracker reveal that 130 nations are currently delving into CBDCs, encompassing 98 percent of global GDP [12]. The surge in the development of CBDCs can be attributed to several key factors [13]. Firstly, there is an escalating demand for digital payments as online commerce expands and societies transition towards cashless transactions, necessitating government-backed digital currencies to accommodate modern payment methods, a need that CBDCs fulfill. Additionally, the emergence of decentralized cryptocurrencies like Bitcoin has posed a challenge to traditional currency monopolies, prompting central banks to explore hybrid digital currencies through CBDCs to maintain control while adapting to the changing landscape. Moreover, CBDCs hold promise in promoting financial inclusion by offering a universally accessible digital currency option, thereby expanding banking services to the unbanked and underbanked populations [14]. Furthermore, the utilization of digital currencies supported by distributed ledgers could revolutionize payment systems by enabling real-time, cost-effective domestic and cross-border transactions compared to conventional financial infrastructure. CBDCs also present an opportunity for central banks to introduce additional policy tools, allowing for tailored money issuance or transaction incentives to inject economic stimulus directly into the economy during periods of downturn. Many central banks view CBDCs as a means of modernizing their payment systems and embracing blockchain-based technologies that drive future financial innovations. Lastly, the development of CBDCs serves as a preemptive strategy for central banks to proactively address potential risks such as the growing dominance of non-sovereign cryptocurrencies or the disruptive influence of emerging technologies like quantum computing on existing systems. In essence, the rise of CBDCs is driven by the increasing demand for digital payments, competition from cryptocurrencies, financial inclusion objectives, central banks’ pursuit of innovative monetary policy tools, and the push for technological advancement in payment systems. 1.2 Reasons for Rise in Usage of Crypto-assets Preserving privacy in CBDCs poses a challenge. The core priorities of transparency, anonymity, traceability and compliance posed by a CBDC model introduce tensions with privacy that are challenging to resolve through technology and policy [15]. It will require novel solutions. This is one of the reasons that support the usage of cryptocurrencies. The rise in crypto-assets 333A crypto-asset is a digital representation of value or entitlement that can be electronically transferred or stored through distributed ledger technology or comparable mechanisms [16]. as well as cryptocurrencies’ popularity can be attributed to several key reasons [5]. Firstly, it serves as an alternative store of value to fiat currencies, appealing to individuals seeking a hedge against inflation and currency devaluation due to its capped supply, making it an attractive long-term investment. Moreover, cryptocurrency’s price volatility has drawn in speculators aiming to capitalize on its increasing valuation over time, attracting a broader investor base. Its utility for international payments offers a quick and cost-effective way to transfer value across borders independently of traditional banking systems, enhancing its use for remittances and global commerce. Additionally, cryptocurrency provides a level of financial anonymity unparalleled in conventional banking systems, enhancing its attractiveness to those valuing privacy and pseudonymity in transactions [17]. Ideologically, cryptocurrency aligns with the desire for decentralization and independence from governmental or corporate influence, appealing to individuals seeking a truly decentralized currency. The growing acceptance of cryptocurrencies by major payment processors, merchants, exchanges, and the proliferation of cryptocurrency-supporting ATMs have widened its accessibility and usability, expanding its user base. Functioning outside traditional financial systems, cryptocurrency offers an alternative financial network that is immune to censorship, appealing to those interested in utilizing censorship-resistant applications and money. The increasing interest in blockchain technology by businesses has fueled general awareness and excitement around cryptocurrencies like Bitcoin, further boosting its popularity. Ultimately, cryptocurrency’s scarcity, global accessibility, privacy features, and decentralized nature drive both speculative investment and practical adoption, establishing it as a versatile asset with a range of valuable use cases. 1.3 Development of Quantum Technology As of now, we are not yet in the post-quantum era, a period that will dawn once quantum computers attain the capability to break mainstream public-key cryptography such as RSA and ECC [18]. While the largest quantum computers currently boast around 100 qubits, the task of breaking AES-256 or factoring a 2048-bit RSA key demands millions of qubits, a milestone that remains distant [19]. Quantum computing experts project that realizing quantum computers capable of breaching current cryptographic standards is still a decade or two away at the earliest, given the ongoing progress in the field. Presently, we find ourselves in the ’noisy intermediate-scale quantum’ era, characterized by quantum computers with limited qubits and high error rates. These machines are primarily utilized for small proof-of-concept demonstrations rather than general computational tasks [20]. In addition, we find several recent trials for attacking to military grade encryption using quantum computer that seems ambitious and advance the post quantum era [21, 22]. Going back to 1983, Stephen Wiesner’s paper introduced groundbreaking ideas well ahead of their time. In it, he proposed using delicate quantum states as unforgeable banknotes, drawing on emerging studies of quantum physics [23, 24]. Specifically, Wiesner envisioned generating quantum money consisting of non-orthogonal quantum states, such as polarization states of single photons. These states could not be perfectly distinguished or cloned due to the fundamental no-cloning theorem of quantum mechanics. As such, any attempt to counterfeit a quantum banknote by copying or measuring it would necessarily introduce errors. The genuine banknote could then be verified by carefully measuring in designated bases. This exploiting of intrinsic quantum properties for authentication marked the earliest concept of quantum currency. Wiesner’s work was also seminal in that it helped lay the theoretical foundation for later advances in quantum information science and cryptography [23]. His money proposal directly inspired the seminal ideas behind quantum key distribution, wherein non-orthogonal states allow securely encoding private keys [25]. While initially a theoretical thought experiment, Wiesner’s work kicked off serious research efforts to devise practical quantum money schemes. It demonstrated how quantum physics could enable fundamentally new approaches to issues like counterfeiting prevention - applications that are only now becoming technologically feasible decades later with improvements in quantum control and devices. Overall, Wiesner’s paper pioneered the study of using quantum effects for tasks like encryption, key distribution and monetary systems. It served as a prescient early thought experiment that remains highly influential on modern developments in quantum technologies and their information-theoretic applications. Blockchain technology and cryptocurrencies, starting with Bitcoin in 2009, have revolutionized digital finance and payments [26]. By leveraging distributed ledger systems secured through cryptography, blockchains enable decentralized networks for value exchange without reliance on centralized intermediaries like banks. This ushered in new possibilities for trustless, permissionless value transfer across borders via digital assets. It appealed strongly to a vision of open, democratic money for the internet age. Crypto also spurred innovation through technologies like smart contracts and decentralized applications [27]. Within just over a decade, blockchain networks and cryptocurrencies grew exponentially in users, applications and total value. Bitcoin in particular saw rising adoption as an alternative currency and speculative asset [28]. This rapid rise disrupted traditional finance and forced rethinking of money and its underlying technologies. However, most blockchains rely on asymmetric key cryptography like RSA and ECC to digitally sign transactions and protect wallet addresses/keys [29]. While sufficiently secure today, these systems will be vulnerable once quantum computers achieve substantial processing power. Specifically, algorithms like Shor’s could break the prime number factorization and discrete logarithm problems that underpin RSA and ECC [30]. This threatens the core security of blockchains by enabling forgery of signatures and theft of funds from public keys. Urgent upgrades are needed to ward off these quantum hacks. The looming quantum computing threats thus challenge the long-term viability of current blockchain systems and stored crypto value, unless they pivot to quantum-resistant cryptographic schemes still being developed. This context introduces risk and uncertainty to the technology. Blockchain technology and cryptocurrencies have introduced revolutionary changes to digital financial system in recent years [27]. The ability to conduct secure peer-to-peer value transfers without centralized intermediaries has significant implications. Blockchains provide an open, decentralized platform for developing new applications of digital assets, smart contracts, and other FinTech innovations. Cryptocurrencies like Bitcoin have also seen rising adoption as alternative payment networks and speculative assets. However, most current blockchain networks rely on encryption algorithms that are susceptible to being broken by quantum computers. In particular, public-key cryptography schemes like RSA and ECC form the backbone of how transactions, wallets, and nodes are secured on blockchains. These asymmetrical encryption algorithms work by factoring large prime numbers, but Peter Shor brilliantly devised an algorithm in 1994 that can solve this problem exponentially faster on a sufficiently powerful quantum computer [31]. Shor’s algorithm threatens to render the encryption underpinning blockchains totally insecure [30]. A quantum computer able to run Shor’s algorithm would be able to derive the private keys needed to spend cryptocurrencies from public keys, impersonate nodes on a blockchain network, and rewrite transaction histories. This applies to the signature schemes, address generation, and other core cryptographic primitives used across blockchains today from Bitcoin to Ethereum. While quantum computers with this capability may still be 10-20 years away, the long-term implications are serious. Blockchain networks, DeFi applications, and the value and integrity of held cryptocurrencies could all be jeopardized without modifications to use quantum-resistant cryptography [32]. This highlights the urgent need for the blockchain industry to start transitioning to post-quantum secure algorithms and implementations. In response to these quantum computing threats and the rise of private cryptocurrencies, central banks have begun exploring the development of their own state-backed digital currencies. Known as CBDCs, these are digital forms of fiat currency issued and governed by a country’s monetary authority [33]. Most CBDC prototypes and pilot programs utilize a centralized database or distributed ledger architecture rather than a public blockchain [34]. This model gives central banks full control over key functions like money issuance, transaction management, interest rates, and compliance. Proponents argue CBDCs can provide many of the benefits of digital cash while preserving national monetary sovereignty. Some key benefits central banks hope to achieve with CBDCs include increasing payment efficiency, reducing transaction costs, expanding financial inclusion, and facilitating fiscal stimulus measures [35]. It also gives them a secure alternative to private cryptocurrencies at a time when quantum computers may undermine current blockchain networks. Countries like China are already testing CBDC projects aimed at replacing cash usage [36]. Meanwhile, the blockchain industry is proactively developing solutions to secure digital assets against quantum attacks. This involves transitioning blockchain networks, addresses, and cryptography to post-quantum secure algorithms theorized to withstand even fault-tolerant quantum computers [37]. Standards are being proposed for quantum-resistant signature schemes, zero-knowledge proofs, and other primitives. Overall, both central banks and the blockchain industry see the need to evolve for the quantum era. CBDCs aim to provide quantum-secure centralized digital fiat money. And continuous work on post-quantum cryptography seeks to future-proof decentralized assets on blockchains and allow them to coexist alongside CBDCs. As the capabilities of quantum computers continue advancing, it raises the crucial question of how digital currencies might evolve in response. Will there be space for both centralized CBDCs controlled by central banks as well as decentralized cryptocurrencies powered by blockchain technology to coexist? Or will one dominate the other as quantum computing disruptions play out? There are reasonable arguments on both sides. On one hand, central banks may prefer the sovereignty and oversight afforded by CBDCs, pushing for them to become the dominant digital legal tender. Powerful governments could potentially restrict or ban private cryptocurrencies in such a scenario. However, blockchain networks are also proactively working to bolster security and make cryptocurrencies quantum-resistant. Demand for decentralized, censorship-resistant assets like Bitcoin remains strong among many users as well. This suggests cryptocurrencies may find ongoing utility and market demand even in a post-quantum world. Some possibilities for coexistence could emerge if central banks integrate certain blockchain or distributed ledger features into CBDCs to gain efficiencies, while maintaining centralized issuance controls. Hybrid public-private models may also see central bank-backed digital tokens running on permissioned blockchain platforms. Overall, how dominant each model becomes will likely depend on evolving technical standards as well as policy priorities around financial sovereignty, inclusion, and innovation. New collaborative frameworks between government monetary authorities and the cryptocurrency industry may also need to be explored to tap their respective advantages for users."
https://arxiv.org/html/2411.06124v1,Exploring Structural Nonlinearity in Binary Polariton-Based Neuromorphic Architectures,"This study investigates the performance of a binarized neuromorphic network leveraging polariton dyads, optically excited pairs of interfering polariton condensates within a microcavity to function as binary logic gate neurons. Employing numerical simulations, we explore various neuron configurations, both linear (NAND, NOR) and nonlinear (XNOR), to assess their effectiveness in image classification tasks. We demonstrate that structural nonlinearity, derived from the network’s layout, plays a crucial role in facilitating complex computational tasks, effectively reducing the reliance on the inherent nonlinearity of individual neurons. Our findings suggest that the network’s configuration and the interaction among its elements can emulate the benefits of nonlinearity, thus potentially simplifying the design and manufacturing of neuromorphic systems and enhancing their scalability. This shift in focus from individual neuron properties to network architecture could lead to significant advancements in the efficiency and applicability of neuromorphic computing.","Artificial neural networks (ANNs) have revolutionized data processing by emulating the intricate network of neurons in the human brain, enabling significant advances in fields ranging from robotics to healthcare [1, 2]. These systems process information through interconnected nodes or neurons that can learn to perform complex tasks, leading to improvements in decision-making and pattern recognition technologies. As the demand for these technologies grows, so does the interest in developing various hardware implementations to support them [3]. These hardware platforms include electronic-based systems, which leverage silicon-based technologies, and photonic systems, which exploit the interaction of light and matter to enhance speed and reduce energy consumption [4, 5, 6, 7, 8]. Another promising option is the use of exciton-polaritons, quasiparticles that combine the properties of light and matter. These are being investigated for their potential in neuromorphic computing, particularly because of their rapid operation times and potentially low power consumption [9, 10]. Each of these platforms aims to offer unique advantages, whether in scalability, speed, or energy efficiency, to meet the growing computational demands of modern ANNs. In light of the 2024 Nobel Prize in Physics awarded to John Hopfield and Geoffrey Hinton for foundational advances that have shaped the modern era of neural networks and machine learning, our exploration into polariton-based neuromorphic architectures gains added relevance. Hopfield’s seminal contributions to the physics of exciton-polaritons [11, 12] and neural network theory [13, 14] have inspired new approaches that blend these fields. This fusion of knowledge is at the heart of our study, underscoring the potential of exciton-polaritons in neuromorphic computing to push the boundaries of processing speeds and the inherent capability for parallel data handling. Binarized neural networks (BNNs) represent a specific approach to enhancing the computational efficiency of artificial neural networks [15, 16, 17]. By simplifying the weights and activations within the network to just two levels, typically 0 and 1, BNNs drastically reduce the computational complexity and memory usage required for neural processing. Although this simplification often results in lower accuracy compared to networks with full-precision weights, BNNs excel in scenarios where speed, power efficiency, and low resource consumption are more critical than achieving the highest possible accuracy, making them well-suited for applications in internet of things, edge computing, and other environments where autonomy and limited resources are key considerations [17, 18, 19]. Binarized neural networks have been effectively realized using exciton-polaritons. In the notable implementation described in Ref. [9], artificial neurons function as XOR gates. The used technique utilizes nonresonant laser pulses, acting as the input signals, to selectively excite spatially localized exciton-polariton condensates, that interact with each other. The resulting output signals vary in energy, reflecting the different combinations of the inputs. This approach has proven successful in pattern recognition tasks, achieving approximately 96% accuracy on the MNIST (Mixed National Institute of Standards and Technology) dataset, a standard benchmark in machine learning for handwritten digit recognition, under noisy conditions using a single-hidden-layer network. The impressive potential of this solution is further underscored by subsequent assessments of its remarkable energy efficiency, as reported in [10]. Nonlinearity is a cornerstone in the operational efficiency of neural networks, essential for executing tasks beyond the scope of linear computational models. This includes distinguishing overlapping data sets or solving inherently complex problems. The nonlinear activation function within each neuron exemplifies this intrinsic nonlinearity, defining how inputs are transformed into outputs in a way that linear operations cannot [20]. Exciton polaritons, known for their pronounced nonlinear properties due to polariton-polariton interactions, are especially valuable in this context. The distinctive nonlinearity of polaritons is the key element that drives the functionality of both continuous-weight networks [5] and binarized neuromorphic systems [9]. Recent research [21, 22, 23, 24] challenge the emphasis traditionally placed on the inherent nonlinearity in individual neurons within neural networks, see also [25, 26]. Studies have demonstrated that nonlinear computations can be realized using purely linear optical systems by adjusting the parameters of these systems. This development underscores that achieving nonlinearity does not necessarily rely on the physical nonlinearity of the system’s components. By encoding inputs as parameters rather than direct signals, a linear system can emulate nonlinear behavior. This approach shifts the focus from the inherent properties of the materials to the configuration of the system itself, which facilitates structural nonlinearity arising from the arrangement and interactions among its components. In our recent paper [27], we have theoretically proposed a binarized neuromorphic network architecture based on a lattice of pairwise coupled exciton polariton condensates. In this geometry, each pair of condensates, referred to as a polariton dyad [28], serves as artificial binary neurons functioning similarly to OR gates. Unlike XOR gate neurons utilized in work [9], the OR operation is linear. Nevertheless, in [27], we demonstrated that our proposed architecture effectively addresses the inherently nonlinear challenge of image classification, exemplified by the recognition tasks in the MNIST dataset. Our current study elucidates the role of structural nonlinearity in solving recognition tasks. We explore the potential for modifying the operation of polariton neurons proposed in [27] to function as both linear (NAND and NOR) and nonlinear (XNOR) gates. Through numerical experiments, we compare the image classification accuracies, allowing us to question whether the significance of inherent nonlinearity, typical of individual computational elements such as neurons, might be overstated."
https://arxiv.org/html/2411.05863v1,Exploring the Feasibility of Affordable Sonar Technology: Object Detection in Underwater Environments Using the Ping 360,"This study explores the potential of the Ping 360 sonar device, primarily used for navigation, in detecting complex underwater obstacles. The key motivation behind this research is the device’s affordability and open-source nature, offering a cost-effective alternative to more expensive imaging sonar systems. The investigation focuses on understanding the behaviour of the Ping 360 in controlled environments and assessing its suitability for object detection, particularly in scenarios where human operators are unavailable for inspecting offshore structures in shallow waters. Through a series of carefully designed experiments, we examined the effects of surface reflections and object shadows in shallow underwater environments. Additionally, we developed a manually annotated sonar image dataset to train a U-Net segmentation model. Our findings indicate that while the Ping 360 sonar demonstrates potential in simpler settings, its performance is limited in more cluttered or reflective environments unless extensive data pre-processing and annotation are applied. To our knowledge, this is the first study to evaluate the Ping 360’s capabilities for complex object detection. By investigating the feasibility of low-cost sonar devices, this research provides valuable insights into their limitations and potential for future AI-based interpretation, marking a unique contribution to the field.","In recent years, there has been a growing emphasis on exploring underwater environments. Traditionally, tasks such as inspection, maintenance, and object retrieval were performed by human divers, often at significant physical and mental risk [1]. However, advances in technology have led to the development of Remotely Operated Vehicles (ROVs) and Autonomous Underwater Vehicles (AUVs), which have transformed ocean exploration by automating complex tasks and reducing human risks [2]. Despite these advancements, underwater navigation and object detection remain challenging due to the limitations of optical sensors. Water turbidity, light attenuation, and scattering severely degrade visual data quality, making traditional cameras unreliable in sub-sea operations [3]. While short-range improvements in visual data quality have been achieved, alternative sensing technologies are needed to overcome these limitations [4]. Sonar technology has emerged as a viable solution for underwater detection and navigation, effectively penetrating murky waters and low-light conditions through the use of sound waves [5][6]. There are two primary types of sonar systems: single-beam and multi-beam. Multi-beam sonar systems, which generate detailed 3D maps and provide extensive spatial coverage, offer superior resolution and accuracy. However, their high cost limits accessibility for many applications [7]. The Ping 360 sonar, developed by Blue Robotics, presents an affordable alternative as a single-beam system designed primarily for navigation. Capable of performing 360° scans, it offers potential for underwater exploration and obstacle avoidance [8]. However, its ability to perform more advanced tasks, such as complex object detection, remains under explored. Accurate object detection is critical for autonomous underwater vehicles operating without human intervention, where the reliable identification of underwater structures is essential. This study aims to investigate the feasibility of using the Ping 360 sonar for underwater object detection in real-world scenarios. Existing research has explored sonar-based object detection, particularly through imaging sonar techniques. However, these approaches often fail to account for the fundamental differences between sonar and optical imagery. Single-beam sonar systems like the Ping 360 generate acoustic representations of underwater environments, which are sometimes converted into image-like outputs. While this conversion can improve detection accuracy, it oversimplifies the inherent complexities of sonar data, such as non-homogeneous resolution, speckle noise, acoustic shadowing, and reverberation, as noted by Karimanzira et al. [9]. While the Ping 360 is an affordable option—about one-fifth the cost of multi-beam systems—it faces challenges such as noise interference and shadowing, which complicate object detection [10]. Despite these limitations, single-beam systems are practical for smaller-scale or budget-conscious operations. Kim et al. suggest that advanced image processing techniques can help mitigate these limitations, though they may still struggle in noisy or low-resolution environments [11]. In more complex underwater environments, such as turbid or structurally intricate coastal habitats, the resolution of the Ping 360 cannot match that of high-frequency multi-beam sonars, which offer real-time, camera-like visuals. However, McKay et al. showed that fine-tuning machine learning models for sonar data can improve detection accuracy, even with simpler systems like the Ping 360 [9]. Still, these models often rely on high-quality sonar data, which is not always realistic for single-beam systems. Our study addresses these challenges by evaluating the Ping 360 sonar’s potential for underwater object detection in real-world settings, moving beyond its traditional navigation role. Many existing studies focus on imaging sonar or synthetic data generation, such as Jiang et al.’s CycleGAN approach, which converts sonar data into image-like formats [12]. While these methods produce strong results in controlled environments, they often overlook the complexities of sonar data, such as variable resolution, acoustic noise, and shadowing. Forcing sonar data into image-based frameworks risks misinterpreting its unique characteristics, which are crucial for real-world detection accuracy. Recent studies frequently apply image processing and AI models to sonar data [13][14][15][16]. While these models may perform well in controlled environments, they often struggle in real-world conditions where sonar’s acoustic properties must be accounted for. Moreover, manual annotation processes are often misrepresented, leading to errors in sonar data interpretation and artificially inflated model performance metrics, as noted by Zhao et al. [12]. Factors such as object material, size, and positioning, which significantly impact detection reliability, are rarely considered in detail. Our study focuses on re-purposing the Ping 360; the navigation-sonar; for more complex underwater object detection tasks. Rather than developing AI solutions, we evaluate the device’s feasibility and limitations in environments where traditional image-based methods fail, such as in low-visibility or turbid waters. We emphasize the importance of treating, and interpreting sonar data based on its acoustic properties, rather than forcing it into an image-processing framework. By examining challenges such as surface reflections, noise, and acoustic shadows, we provide a realistic evaluation of the Ping 360’s capabilities for complex detection tasks. This research, being the first of its kind, offers a unique contribution to the field. Research Questions The primary research questions (RQ) addressed in this study are as follows: • RQ1: What are the main challenges in interpreting navigation sonar data in cluttered or reflective environments? • RQ2: How effective is manual annotation of sonar data in improving object detection when used with advanced segmentation AI models such as UNet? • RQ3: Can low-cost sonar devices like Ping 360 typically used for navigation, be effectively repurposed for complex underwater object detection? Contributions In line with finding out the answers to those questions, this research makes several key contributions: • For the first time, we present a detailed investigation into the feasibility of using the Ping 360 sonar device for object detection in underwater environments. • We are releasing the raw collected data from the Ping 360 sonar to enable future researchers to explore its potential usability, especially for those who may not have access to the device due to its cost, despite it being relatively affordable. • We provide insights into the challenges of sonar data interpretation, including surface reflections, object shadows, and the importance of careful data annotation for machine learning. • We developed a segmentation dataset from manually annotated sonar images and demonstrated that the Ping 360 sonar’s object detection performance is highly dependent on data pre-processing and careful interpretation, particularly when used with machine learning models like the U-Net segmentation algorithm."
