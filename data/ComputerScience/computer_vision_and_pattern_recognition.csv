URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10440v1,: Let Vision Language Models Reason Step-by-Step,"Large language models have demonstrated substantial advancements in reasoning capabilities, particularly through inference-time scaling, as illustrated by models such as OpenAI’s o1. However, current Vision-Language Models (VLMs) often struggle to perform systematic and structured reasoning, especially when handling complex visual question-answering tasks. In this work, we introduce LLaVA-o1111There are similar names of recent VLM works. To clarify, LLaVA-o1 is built upon Llama-3.2-Vision model [40], rather than LLaVA [32]., a novel VLM designed to conduct autonomous multistage reasoning. Unlike chain-of-thought prompting, LLaVA-o1 independently engages in sequential stages of summarization, visual interpretation, logical reasoning, and conclusion generation. This structured approach enables LLaVA-o1 to achieve marked improvements in precision on reasoning-intensive tasks. To accomplish this, we compile the LLaVA-o1-100k dataset, integrating samples from various visual question answering sources and providing structured reasoning annotations. Besides, we propose an inference-time stage-level beam search method, which enables effective inference-time scaling. Remarkably, with only 100k training samples and a simple yet effective inference time scaling method, LLaVA-o1 not only outperforms its base model by 8.9% on a wide range of multimodal reasoning benchmarks, but also surpasses the performance of larger and even closed-source models, such as Gemini-1.5-pro, GPT-4o-mini, and Llama-3.2-90B-Vision-Instruct.","Large language models, represented by OpenAI o1 [63], demonstrate strong capabilities for systematic and in-depth reasoning, validating the efficacy of inference-time scaling for language models [47]. However, vision is equally important for enabling models to fully understand the world and extend their cognitive abilities [6]. Therefore, developing a multimodal model that integrates both language and vision while facilitating effective, systematic, and deep reasoning holds substantial significance. Figure 1: Performance of LLaVA-o1 and other models across six multimodal reasoning benchmarks. Although LLaVA-o1 is fine-tuned from the Llama-3.2-11B-Vision-Instruct [40] model (which has the lowest average score), it outperforms many larger open-source models and even some closed-source models. Detailed benchmark results are shown in Table 7. Figure 2: Comparison of the base model and LLaVA-o1. As shown, the base model Llama-3.2-11B-Vision-Instruct exhibits obvious flaws in reasoning, with several errors occurring throughout the reasoning process. In contrast, LLaVA-o1 begins by outlining the problem, interprets relevant information from the image, proceeds with a step-by-step reasoning process, and ultimately reaches a well-supported conclusion. Early open-source vision language models (VLMs) mainly employ a direct prediction approach [32, 21, 30], generating brief answers immediately in response to a question. The main limitation of this direct-response paradigm is its lack of a structured reasoning process, making it less effective for tasks demanding logical reasoning [62]. Recent studies have shown that incorporating Chain-of-Thought (CoT) reasoning encourages the model to reason step by step, significantly improving its question-answering capabilities [52]. However, even with CoT reasoning, most VLMs frequently produce errors or hallucinated outputs during the reasoning progress [31, 24, 50]. Our findings suggest that a significant cause of these issues is the insufficiently systematic and structured nature of the reasoning process in existing VLMs. Specifically, by referring systematic, we mean that the model does not generate a direct reasoning chain but instead engages in multistage reasoning. Structured, on the other hand, refers to the model’s ability to clearly identify the reasoning stage it is in and understand the primary task to be addressed at each stage. We observe that VLMs often initiate responses without adequately organizing the problem and the available information. Moreover, they frequently deviate from a logical reasoning toward conclusions, instead of presenting a conclusion prematurely and subsequently attempting to justify it. Given that language models generate responses token-by-token, once an erroneous conclusion is introduced, the model typically continues along a flawed reasoning path. OpenAI o1 [63] addresses these issues effectively by enabling the model to independently engage in systematic and structured reasoning through language. Building on this insight, we design LLaVA-o1. While the community has made some preliminary explorations into the underlying mechanisms of OpenAI o1 [42, 54], the model remains a black box, with its technical details largely unknown. This work demonstrates a potential way to enhance a model’s ability to perform autonomous, stage-by-stage reasoning by employing supervised fine-tuning. Specifically, LLaVA-o1 is capable of generating four distinct stages: summary, caption, reasoning, and conclusion. Each stage serves a unique purpose in the reasoning process. • Summary: A brief outline in which the model summarizes the forthcoming task. • Caption: A description of the relevant parts of an image (if present), focusing on elements related to the question. • Reasoning: A detailed analysis in which the model systematically considers the question. • Conclusion: A concise summary of the answer, providing a final response based on the preceding reasoning. To enhance the understanding of CoT processes in LLM, LLaVA-o1 marks each stage with a dedicated tag (e.g., <SUMMARY>...</SUMMARY>) to denote the beginning and end of each stage. Those taggings enable the model to maintain clarity throughout the reasoning process. Unlike traditional CoT reasoning, which allows the model to think freely, our method promotes structured thinking by first organizing the problem and known information, followed by a detailed thought process, and then deriving a conclusion. To achieve this, we construct the LLaVA-o1-100k dataset by generating responses stage by stage using GPT-4o [3] and then train the model using supervised fine-tuning. The structured reasoning in LLaVA-o1 also facilitates efficient inference time scaling. In contrast to conventional scaling methods, such as best-of-N sampling [51, 4] and sentence-level beam search [16, 49], LLaVA-o1 employs a novel stage-level beam search method that generates multiple candidate results at each stage and selects the best one to continue the generation process. We conduct experiments on several multimodal reasoning benchmarks, including MMStar [9], MMBench [33], MMVet [60], MathVista [35], AI2D [23], and HallusionBench [17], and observed that LLaVA-o1 offers two primary advantages: First, enabling the model to perform structured reasoning independently substantially outperforms traditional CoT prompting, particularly in complex reasoning tasks that require systematic analysis. Second, our stage-level beam search method is scalable and improves performance reliability, making it more effective in achieving stable and accurate results. Our contributions are summarized as follows: • We introduce LLaVA-o1, a visual language model designed for systematic reasoning, demonstrating exceptional performance on tasks that require structured thinking and reasoning. • We demonstrate that LLaVA-o1, using stage-level beam search, is inference-time scalable. This means that with increased computational resources, the performance of our approach can be further enhanced, making it applicable to more complex tasks and scenarios. • Extensive experiments on various benchmarks demonstrate that our method achieves superior performance relative to larger and closed-source models, underscoring the effectiveness of LLaVA-o1 for multimodal reasoning."
https://arxiv.org/html/2411.10433v1,M-VAR: Decoupled Scale-wise Autoregressive Modeling for High-Quality Image Generation,"There exists recent work in computer vision, named VAR, that proposes a new autoregressive paradigm for image generation. Diverging from the vanilla next-token prediction, VAR structurally reformulates the image generation into a coarse to fine next-scale prediction. In this paper, we show that this scale-wise autoregressive framework can be effectively decoupled into intra-scale modeling, which captures local spatial dependencies within each scale, and inter-scale modeling, which models cross-scale relationships progressively from coarse-to-fine scales. This decoupling structure allows to rebuild VAR in a more computationally efficient manner. Specifically, for intra-scale modeling — crucial for generating high-fidelity images — we retain the original bidirectional self-attention design to ensure comprehensive modeling; for inter-scale modeling, which semantically connects different scales but is computationally intensive, we apply linear-complexity mechanisms like Mamba to substantially reduce computational overhead. We term this new framework M-VAR. Extensive experiments demonstrate that our method outperforms existing models in both image quality and generation speed. For example, our 1.5B model, with fewer parameters and faster inference speed, outperforms the largest VAR-d30-2B. Moreover, our largest model M-VAR-d32 impressively registers 1.78 FID on ImageNet 256\times256 and outperforms the prior-art autoregressive models LlamaGen/VAR by 0.4/0.19 and popular diffusion models LDM/DiT by 1.82/0.49, respectively. Code is avaiable at https://github.com/OliverRensu/MVAR.","Autoregressive models (Radford et al., 2018; Brown et al., 2020) have been instrumental in advancing the field of natural language processing (NLP). By modeling the probability distribution of a token given the preceding ones, these models can generate coherent and contextually relevant text. Prominent examples like GPT-3 (Brown et al., 2020) and its successors (OpenAI, 2022; 2023) have demonstrated remarkable capabilities in language understanding and generation, setting new benchmarks across various NLP applications. Figure 2: Fréchet inception distance (FID) on 256\times256 image generation. Our M-VAR-1.5B model outperforms the largest 2B VAR-d30 with fewer parameters and faster inference speed. Our largest M-VAR-3B achieves 1.78 FID. Building upon the success in NLP, the autoregressive modeling paradigm (Yu et al., 2022; Sun et al., 2024; Van den Oord et al., 2016) has also been extended to computer vision for image generation tasks, aiming to generate high-fidelity images by predicting visual content in a sequential manner. Recently, VAR (Tian et al., 2024) has further enhanced this image autoregressive pipeline by structurally reformulating the learning target into a coarse-to-fine “next-scale prediction”, which innately introduces strong semantics to interconnecting tokens along scales. As demonstrated in the VAR paper, this pipeline exhibits much stronger scalability and can achieve competitive, sometimes even superior, performance compared to advanced diffusion models. This paper aims to further optimize VAR’s computation structure. Our key insight lies in decoupling VAR’s cross-scale autoregressive modeling into two distinct parts: intra-scale modeling and inter-scale modeling. Specifically, intra-scale modeling involves bidirectionally modeling multiple tokens within each scale, capturing intricate spatial dependencies and preserving the 2D structure of images. In contrast, inter-scale modeling focuses on unidirectional causality between scales by sequentially progressing from coarse to fine resolutions — each finer scale is generated conditioned on all preceding coarser scales, ensuring that global structures guide the refinement of local details. Notably, the sequence length involved in inter-scale modeling is much longer than that of intra-scale modeling, resulting in significantly higher computational costs. But meanwhile, our analysis of attention scores for both intra-scale and inter-scale interactions (as discussed in Sec. 3.2) suggests a contrasting reality: intra-scale interactions dominate the model’s attention distribution, while inter-scale interactions contribute significantly less. Motivated by the observations above, we propose to develop a more customized computation configuration for VAR. For the intra-scale component, given the much shorter sequence lengths within each scale and its significant contribution to the model’s attention distribution, we retain the bidirectional attention mechanism to fully capture comprehensive spatial dependencies. This ensures that local spatial relationships and fine-grained details are effectively modeled at a reasonable computational overhead. Conversely, for the inter-scale component, which involves much longer sequences but demands relatively less comprehensiveness in modeling global relationships, we adopt Mamba (Gu & Dao, 2023; Dao & Gu, 2024), a linear-complexity mechanism, to handle such inter-scale dependencies efficiently. By segregating these two modeling modules and applying appropriate mechanisms to each, our approach significantly reduces computational complexity while preserving the model’s ability to maintain 2D spatial coherence and unidirectional coarse-to-fine consistency, making it well-suited for high-quality image generation. As shown in Figure 2, our proposed framework, which we term M-VAR, outperforms existing models in both image quality and inference speed. For instance, comparing to the largest VAR model, which has 2B parameters and attains an FID score of 1.97, our 1.5B parameter M-VAR model achieves an FID score of 1.93 with much fewer parameters and 1.2\times faster inference speed. M-VAR also scales well — our largest model, M-VAR-d32, achieves an impressive FID score of 1.78 on ImageNet at 256\times 256 resolution, outperforming the prior best autoregressive models LlamaGen by 0.4 and VAR by 0.19, respectively, and well-known diffusion models LDM by 1.82 and DiT by 0.49, respectively."
https://arxiv.org/html/2411.10414v1,Llama Guard 3 Vision: Safeguarding Human-AI Image Understanding Conversations,"We introduce Llama Guard 3 Vision, a multimodal LLM-based safeguard for human-AI conversations that involves image understanding: it can be used to safeguard content for both mutimodal LLM inputs (prompt classification) and outputs (response classification). Unlike the previous text-only Llama Guard versions (Inan et al., 2023; Llama Team, 2024b, a), it is specifically designed to support image reasoning use cases and is optimized to detect harmful multimodal (text and image) prompts and text responses to these prompts. Llama Guard 3 Vision is fine-tuned on Llama 3.2-Vision and demonstrates strong performance on the internal benchmarks using the MLCommons taxonomy. We also test its robustness against adversarial attacks. We believe that Llama Guard 3 Vision serves as a good starting point to build more capable and robust content moderation tools for human-AI conversation with multimodal capabilities.","The past few years have witnessed an unprecedented improvement in the capabilities of Large Language Models (LLMs), driven by the success in scaling up autoregressive language modeling in terms of data, model size, and the amount of compute used for training (Kaplan et al., 2020). LLMs have demonstrated exceptional linguistic abilities (Brown, 2020; Achiam et al., 2023), general tool use (Schick et al., 2024; Cai et al., 2023), and commonsense reasoning (Wei et al., 2022; OpenAI, 2024), among other impressive capabilities. The success of LLMs as general-purpose assistants motivates research and development to extend instruction-tuning to the vision-language multimodal space (Liu et al., 2023; Gemini Team, 2023). These vision-language multimodal models, which can process and generate both text and images, also achieve human-expert performance on a wide range of tasks, such as (document) visual question answering (Antol et al., 2015; Mathew et al., 2021), image captioning (Lin et al., 2014), and image-text retrieval (Plummer et al., 2015). While these vision-language multimodal models hold tremendous promise for many applications, they should be used along with proper system guardrails to ensure safe and responsible deployment, because they can generate or propagate harmful content when interacting with online users. However, most existing guardrails (Inan et al., 2023; Llama Team, 2024b, a; Yuan et al., 2024; Ghosh et al., 2024) for the interaction (e.g., conversation) between humans and AI agents are text-only: conversation data involving other modalities, such as images, cannot be used as inputs for such guardrails. This calls for a safeguard tool for classifying safety risks in prompts and responses for conversations with multimodal contents involved. In this work, we introduce Llama Guard 3 Vision, a multimodal LLM-based safeguard for human-AI conversations that involves image understanding: it can be used to safeguard content for both mutimodal LLM inputs (prompt classification) and mutimodal LLM responses (response classification). Unlike text-only Llama Guard versions (Inan et al., 2023; Llama Team, 2024b, a), it is specifically designed to support image reasoning use cases and is optimized to detect harmful multimodal (text and image) prompts and text responses to these prompts. Figure 1 gives an example of how Llama Guard 3 Vision classifies harmful content in the response classification task. Llama Guard 3 Vision is fine-tuned on Llama 3.2-Vision and demonstrates strong performance on our internal benchmark using the 13 hazards of the MLCommons taxonomy. To better understand the adversarial robustness of Llama Guard 3 Vision, we evaluate it against two existing powerful white-box attacks (Zou et al., 2023; Madry, 2017) using both textual and visual inputs. We find that Llama Guard 3 Vision is more robust in the response classification task compared to the prompt classification task under practical threat model scenarios, since Llama Guard 3 Vision primarily relies on model response for classification while effectively ignoring prompt-based attacks. We hope our white paper can help practitioners build better customizable safeguard models for multimodal use cases, and facilitate further research and development of multimodal LLM safety. Paper Organization In Section 2, we present the hazard taxonomy and policy used to train Llama Guard 3 Vision. In Section 3, we describe the process of building Llama Guard 3 Vision. Our experiment section (Section 4) is divided into two parts: In Section 4.1, we evaluate Llama Guard 3 Vision on our internal benchmark and compare it with baselines such as GPT-4o (Achiam et al., 2023), and in Section 4.2, we present the details and results of our adversarial robustness experiments. Figure 1: Llama Guard 3 Vision classifies harmful content in the response classification task."
https://arxiv.org/html/2411.10411v1,Repurposing Stable Diffusion Attention for Training-Free Unsupervised Interactive Segmentation,"Recent progress in interactive point prompt based Image Segmentation allows to significantly reduce the manual effort to obtain high quality semantic labels. State-of-the-art unsupervised methods use self-supervised pre-trained models to obtain pseudo-labels which are used in training a prompt-based segmentation model. In this paper, we propose a novel unsupervised and training-free approach based solely on the self-attention of Stable Diffusion. We interpret the self-attention tensor as a Markov transition operator, which enables us to iteratively construct a Markov chain. Pixel-wise counting of the required number of iterations along the Markov chain to reach a relative probability threshold yields a Markov-iteration-map, which we simply call a Markov-map. Compared to the raw attention maps, we show that our proposed Markov-map has less noise, sharper semantic boundaries and more uniform values within semantically similar regions. We integrate the Markov-map in a simple yet effective truncated nearest neighbor framework to obtain interactive point prompt based segmentation. Despite being training-free, we experimentally show that our approach yields excellent results in terms of Number of Clicks (NoC), even outperforming state-of-the-art training based unsupervised methods in most of the datasets.","The goal of point prompt based interactive image segmentation is to obtain a high-quality segmentation from limited user interaction in the form of clicking. Prompt based image segmentation gained popularity lately due to large supervised foundation models [15, 28]. In this paper, we focus on unsupervised methods, where no segmentation labels are used at all in the design and/or training of the models. Most recent approaches rely on self supervised backbones like ViT [6], trained either by DINO [2] or MAE [12] based self-supervised techniques. On the other hand, Stable Diffusion (SD) [29] has been used for many different computer vision applications such as monocular depth estimation [14], semantic segmentation [38], object detection [3] and image classification [17], often resulting in state-of-the-art solutions. Inspired by DiffSeg [38], we utilize SD’s self-attention maps in our training-free interactive segmentation approach. The main challenges are that attention maps are noisy and don’t distinguish between instances. To overcome the mentioned challenges, we interpret the self-attention tensors as a Markov transition operator, where the repeated application of the transition forms a Markov chain. We propose a novel Markov-iteration-map or simply Markov-map, where each pixel counts the number of iterations required to obtain a specific probability value. We show that the proposed Markov-map has less noise, the semantic boundaries are sharper and the semantic regions within Markov-maps are more uniformly distributed. Native Markov-maps do not distinguish between instances. Therefore, we further improve Markov-maps with a flood fill approach, which suppresses local minima, to enable instance based segmentation. Finally, we obtain Markov-maps of each prompt point and combine them with a truncated nearest neighbor approach to enable multi-prompt point interactive segmentation. Surprisingly, despite being training-free, we significantly improve the state-of-the-art in terms of Number of Clicks (NoC) and even surpass training based unsupervised approaches in most of the datasets. Our main contributions are: • We introduce Markov-Map Nearest Neighbor (M2N2), the first attention-based unsupervised training-free multi-point prompt segmentation framework. • We propose Markov-maps in order to improve the semantic information. • We enable instance aware Markov-maps by utilizing a modified flood fill approach. • We introduce a truncated nearest neighbor approach to combine multiple point prompts. • We conduct extensive experiments and achieve state-of-the-art results, surpassing even unsupervised training-based methods. Figure 1: M2N2 framework overview. We perform a single denoising step of the input image with Stable Diffusion 2 to obtain attention tensors. The tensors are aggregated and utilized to obtain a Markov-map M_{i} for each prompt point. The final segmentation is the result of a truncated nearest neighbor of scaled Markov-maps M_{i} as a measure of semantic distance for each prompt point. The green and red areas in the scaled Markov-maps denote regions where the distance is less or equal to the global background threshold. In this visualization, components in blue contain adjustable hyperparameters."
https://arxiv.org/html/2411.10389v1,Deep Learning for Micro-Scale Crack Detection on Imbalanced Datasets Using Key Point Localization,"Internal crack detection has been a subject of focus in structural health monitoring. By focusing on crack detection in structural datasets, it is demonstrated that deep learning (DL) methods can effectively analyse seismic wave fields interacting with micro-scale cracks, which are beyond the resolution of conventional visual inspection. This work explores a novel application of DL based key point detection technique, where cracks are localized by predicting the coordinates of four key points that define a bounding region of the crack. The study not only opens new research directions for non-visual applications but also effectively mitigates the impact of imbalanced data which poses a challenge for previous DL models, as it can be biased toward predicting the majority class (non-crack regions). Popular DL techniques, such as the Inception blocks are used and investigated. The model shows an overall reduction in loss when applied to micro-scale crack detection and is reflected in the lower average deviation between the location of actual and predicted cracks, with an average IOU being 0.511 for all micro cracks (>0.00 µm) and 0.631 for larger micro cracks (>4 µm).","Structural health monitoring is a critical area where the safety of infrastructures depends on the detection of internal cracks, which are not visible on the surface. These hidden cracks pose a significant challenge because detecting them requires a combination of domain expertise in both deep learning and material science to effectively extract the relevant features (Zhou et al., 2023; Liu and Zhang, 2019). Moreover, processing the numerical data generated demands high computational power, as the task involves analyzing wave propagation through materials and identifying changes caused by cracks. The ability to automate and enhance this detection process is vital for ensuring structural safety in industries such as civil engineering, aerospace, and manufacturing (Cha et al., 2024; Chen and Jahanshahi, 2018). In recent years, deep neural networks have significantly advanced the field of computer vision, particularly in areas like object detection(Krizhevsky et al., 2017). Traditionally, these networks were designed to grow deeper by adding more layers to capture increasingly complex patterns in data. However, this approach faces challenges such as vanishing gradients, increased computational costs, and difficulties in training. As a result, researchers began exploring the idea of wide networks, which emphasize increasing the number of neurons in each layer rather than stacking more layers. This shift has proven especially beneficial for tasks like object detection, where wider networks can capture more detailed features across a broader spatial rangeZagoruyko and Komodakis (2017), improving performance without the complications that come with deeper architectures (He et al., 2015). In this paper, we explore the transition from deep to wide convolutional networks in the context of object detection, specifically for crack detection using numerical data. Wide networks, with their ability to capture diverse features, offer advantages in handling spatial correlations across larger regions of the input, leading to more accurate detection results (Huang et al., 2018; Xie et al., 2017). Unlike traditional deep learning models in computer vision, we successfully detected patterns in numerical wave propagation data, demonstrating that our model can effectively identify and locate cracks. To the best of our knowledge, no prior work has applied this approach to crack detection, making this the first study to use an object detection model specifically for crack detection from numerical data. This work is a significant first step, proving that the model can detect cracks and accurately determine their location. However, due to the limited availability of data, we were unable to evaluate the model against samples with multiple or more complex cracks. In future work (Section Future Work), we aim to test the model with samples containing multiple cracks and more intricate crack patterns. This paper opens the door to further research on improving crack detection using wide convolutional networks and addressing more complex scenarios in structural health monitoring."
https://arxiv.org/html/2411.10377v1,Generation of synthetic gait data: application to multiple sclerosis patients’ gait pattern,"Multiple sclerosis (MS) is the leading cause of severe non-traumatic disability in young adults and its incidence is increasing worldwide. The variability of gait impairment in MS necessitates the development of a non-invasive, sensitive, and cost-effective tool for quantitative gait evaluation. The eGait movement sensor, designed to characterize human gait through unit quaternion time series (QTS) representing hip rotations, is a promising approach. However, the small sample sizes typical of clinical studies pose challenges for the stability of gait data analysis tools. To address these challenges, this article presents two key scientific contributions. First, a comprehensive framework is proposed for transforming QTS data into a form that preserves the essential geometric properties of gait while enabling the use of any tabular synthetic data generation method. Second, a synthetic data generation method is introduced, based on nearest neighbors weighting, which produces high-fidelity synthetic QTS data suitable for small datasets and private data environments. The effectiveness of the proposed method, is demonstrated through its application to MS gait data, showing very good fidelity and respect of the initial geometry of the data. Thanks to this work, we are able to produce synthetic data sets and work on the stability of clustering methods.","Multiple sclerosis (MS) is an autoimmune disease that affects the central nervous system by damaging the myelin sheath surrounding axons. This affects the transmission of electrical impulses leading to motor, sensory, cognitive, visual and sphincter disturbances [1]. MS is the leading cause of severe non-traumatic disability in young adults, with initial symptoms appearing around the age of 30. Among the various symptoms, gait impairment is one of the most prevalent affecting 41\% of the MS population. It is considered by MS patients as the most problematic symptom as it marks the beginning of loss of autonomy which directly impacts quality of life [2]. Gait impairment in MS is difficult to characterize due to its variability, making it essential to develop a non-invasive, sensitive, and cost-effective tool for quantitative gait evaluation in both MS patients and the general population. The eGait movement sensor, developed by the Jean Leray Mathematics Laboratory in Nantes (France), aims to characterize the human gait using unit quaternion time series (QTS) representing an average 3-dimensional hip rotations during a step, called the Individual Gait Pattern (IGP). In order to asses the stability of statistical tools developed to analyze the IGP a large volume of data is needed, which is proving to be complicated as conducting clinical trials can be lengthy and costly. The issue of small sample size is a recurrent issue in health studies, moreover, sharing clinical data is challenging because de-identification methods, such as removing directly identifying variables (e.g., names, social security numbers), have proven insufficient for protecting personal data [3, 4, 5]. Recently described by Wang et al. (2024) [6] as ”artificial data that can mimic the statistical properties, patterns, and relationships observed in real-world data”, synthetic data, first introduced by Rubin (1993) [7] and further developed by Raghunathan et al. (2003) [8] offers a successful solution. Synthetic gait data generation methods have been developed to improve our understanding of gait impairments. Some of these methods are based on visual representation of the human gait captured by cameras such as the VersatileGait [9], or the synthetic Parkinsonian Gait generated by Chavez et al. (2022) [10]. Kim et al. (2023) developed a method to generate gait patterns characterized by joint angles time series [11]. Most of the gait generation methods require already big original dataset as they are machine-learning based and need to be trained on a bigger data set to perform well. However, there is a significant gap in the literature regarding the generation of synthetic QTS data, which are nevertheless a very good choice to describe any joint movement during gait. n this article we propose two scientific contributions, which together form a complete solution for generating synthetic QTS describing gait data. The first contribution, described in Section 3.1 gives a comprehensive and flexible framework based on dimension reduction that transforms the data such that the individual information is contained in a score matrix while conserving the shape of the curves in forms of principal modes of variation. Thanks to this framework, any tabular synthetic data generation method can be used to generate QTS. The second contribution is the implementation of a tabular synthetic data generation method based on nearest neighbors weighting, aiming to generate some data with respect to the original geometry and with a high fidelity, that performs for small data sets and without any previous knowledge of the data. We also indicate how to use the method for private synthetic gait data. In the second section of this article, we describe the clinical data as well as the key concepts to understand the IGP and QTS objects (see Section 2). The third section details the comprehensive framework to reduce unit QTS data to tabular data (see Section 3.1) as well as the algorithm we propose to generate synthetic data: SynGait (see Section 3.3. We also give advice on how to tune the hyper-parameters of the method. The fourth section details two tabular synthetic data generation method (see Section 4.1) to compare our method with, as well as metrics to evaluate the quality of the synthetic data it produces (see Section 4.2). The fifth section illustrates the method with the presentation of MS gait data and synthetic data generated from it, the results show that this method performs really well in terms of fidelity. Metrics show a good geometry preservation, while guarantying that the data is new and the variability of the synthetic data is preserved as well as possible (see Section 5). The results are discussed in Section 6."
https://arxiv.org/html/2411.10369v1,"Towards High-Fidelity 3D Portrait Generation with Rich Details 
by Cross-View Prior-Aware Diffusion","Recent diffusion-based Single-image 3D portrait generation methods typically employ 2D diffusion models to provide multi-view knowledge, which is then distilled into 3D representations. However, these methods usually struggle to produce high-fidelity 3D models, frequently yielding excessively blurred textures. We attribute this issue to the insufficient consideration of cross-view consistency during the diffusion process, resulting in significant disparities between different views and ultimately leading to blurred 3D representations. In this paper, we address this issue by comprehensively exploiting multi-view priors in both the conditioning and diffusion procedures to produce consistent, detail-rich portraits. From the conditioning standpoint, we propose a Hybrid Priors Diffsion model, which explicitly and implicitly incorporates multi-view priors as conditions to enhance the status consistency of the generated multi-view portraits. From the diffusion perspective, considering the significant impact of the diffusion noise distribution on detailed texture generation, we propose a Multi-View Noise Resamplig Strategy integrated within the optimization process leveraging cross-view priors to enhance representation consistency. Extensive experiments demonstrate that our method can produce 3D portraits with accurate geometry and rich details from a single image. The project page is at https://haoran-wei.github.io/Portrait-Diffusion.","The generation of realistic 3D portraits from a single image [7, 39, 10, 35, 20] has become an important focus in computer vision and graphics, with broad applications in augmented reality, virtual reality, video conferencing, and gaming[14, 18, 45]. The most straightforward approach involves training GAN models [47, 1] on extensive portrait datasets to directly produce 3D representations. However, acquiring such training data can be costly and technically challenging, leading to failures in generating high-fidelity 360° full-head portraits [7, 8] and often resulting in a lack of diversity in the outputs. To address these limitations, recent developments [24, 46, 26, 31, 30] leverage text-to-image diffusion priors [44, 41, 5], which exhibit stronger generalization capabilities and higher generation quality, to produce novel perspectives. Most approaches incorporate additional priors, such as reference image latents [42, 50], ID features [28, 13], and view embeddings [28], to enhance the consistency between new perspectives and the primary viewpoint. Subsequently, they commonly employ Score Distilling Sampling (SDS) loss [25] to distill these 2D priors into 3D representations, ensuring consistent 3D generation. However, in single-image 3D portrait generation, these methods still face challenges: generated portraits often appear over-smoothed and fail to capture detailed textures like hair strands, as illustrated in Fig. 1, limiting their practical applications. We attribute this issue to the insufficient consideration of cross-view consistency during the diffusion process, resulting in significant disparities between different views. This 2D inconsistency results in blurred 3D output by SDS optimization. Although these methods attempt to improve consistency by incorporating additional priors, they rely solely on diffusion attentions to implicitly convey these priors. This reliance results in a lack of explict constraints, leading to inconsistent status across different viewpoints. Moreover, the diffusion procedure is inherently stochastic; even with the same conditions, a diffusion model can generate varied representations due to randomly sampled noises. By using view-independent procedures with purely random noise in diffusion, these methods overlook the impact of stochasticity on representation consistency. Consequently, these inconsistencies in status and representation jointly result in over-smoothed 3D models when optimized under the SDS loss, which enforces 3D consistency and continuity in sacrifice of texture details. To address these issues, we propose fully exploiting cross-view priors in both the conditioning and diffusion procedures to enhance multi-view consistency, thus yielding detail-rich 3D portraits, as showcased in Fig. 1. From a conditioning perspective, we propose Hybrid Priors Diffusion Model (HPDM). Our approach seeks to transfer and utilize cross-view prior information in both explicit and implicit ways to control the novel view generation. In an explicit manner, we begin by employing geometric priors to map pixels from the current view to the next, providing an explicit reference to dominate the generation process. Given that this reference encompasses only a limited overlapping region and contains artifacts introduced through perspective transformations, we further propose to utilize the robust modeling capabilities of attention mechanisms to mitigate these deficiencies. These mechanisms capture finer texture and geometry priors and implicitly transfer these priors into the control conditions, ensuring a more comprehensive and precise guidance for the portrait status of novel viewpoint. From a diffusion procedure perspective, our goal is to manage randomness in adjacent viewpoints so that they can share detailed, consistent representations. To achieve this, we introduce a Multi-View Noise Resampling Strategy (MV-NRS) integrated into the SDS loss, which manages each view’s noise distribution by passing cross-view priors. MV-NRS consists of two main components: first, a shared anchor noise initialization that leverages geometric priors to establish a preliminary representation; and second, an anchor noise optimization phase, where we resample and update the anchor noise based on denoising gradient consistency prior to progressively align the representations during the SDS optimization. To summarize, our main contributions are as follows: • We developed a Portrait Diffusion pipeline consisting of GAN-prior Initialization, Portrait Geometry Restoration, and Multi-view Diffusion Refinement modules to generate rich-detail 3D portraits. • We designed a Hybrid Priors Diffusion Model that emphasizes both explicit and implicit integration of multi-view priors to impose conditions, aiming to enhance the consistency of multi-view status. • We introduced a Multi-View Noise Resampling Strategy integrated within the SDS loss to manage randomness across different views through the transmission of cross-view priors, thereby achieving fine-grained consistent representations. • Through extensive experiments, we show that our proposed pipeline successfully achieves high-fidelity 3D full portrait generation with rich details. Figure 2: The Portrait Diffusion Framework. This framework comprises three integral modules. GAN-prior Portrait Initialization, employs existing Portrait GAN priors to derive initial tri-plane NeRF features from frontal-view portrait images. Portrait Geometry Restoration, is focused on reconstructing the geometry using these initialized tri-planes. Multi-view Diffusion Texture Refinement, transforms coarse textures into detailed representations."
https://arxiv.org/html/2411.10368v1,Mechanisms of Generative Image-to-Image Translation Networks,"Generative Adversarial Networks (GANs) are a class of neural networks that have been widely used in the field of image-to-image translation. In this paper, we propose a streamlined image-to-image translation network with a simpler architecture compared to existing models. We investigate the relationship between GANs and autoencoders and provide an explanation for the efficacy of employing only the GAN component for tasks involving image translation. We show that adversarial for GAN models yields results comparable to those of existing methods without additional complex loss penalties. Subsequently, we elucidate the rationale behind this phenomenon. We also incorporate experimental results to demonstrate the validity of our findings.","The advancement of large neural networks has significantly improved the performance of image-to-image translation tasks. Its high accuracy and flexibility attract many researchers in various fields. Industries, such as healthcare, automotive, and entertainment, utilize image-to-image translation technologies for different applications, including medical imaging, autonomous driving, and digital content creation [1, 2, 3]. In addition, researchers in academia and the private sectors are continuously innovating to explore new possibilities and advances in this area. Image-to-image translation encompasses a wide range of tasks, including edge-to-image, photo-to-painting, etc. [1, 4, 5]. All of these tasks need significant computational and data resources for the training model. Depending on the complexity of the model and the size of the dataset, training can take from hours to weeks. A myriad of methodologies have been advanced to address the image-to-image translation problem. Despite most existing models are able to solve the problem, they do not explain the mechanisms by which the network distinguishes content from style [6, 7, 8, 9, 10]. The nebulous definitions of content and style pose significant challenges in the mathematical characterization of the image translation process. Moreover, existing models for image-to-image translation often employ Generative Adversarial Networks (GANs) architecture, but encompass significant complexity, incorporating elements such as cycle loss, identity loss, and penalties on intermediate features. Rarely is the necessity of these intricate penalties examined. Previously, we introduced a GAN-based model to transform food images using only GAN penalty without any additional penalties [4]. In this paper, we investigate the similarity between Generative Adversarial Networks (GANs) [11] and autoencoders [12] to elucidate the GAN model mechanism for image-to-image translation without imposing additional penalties. Subsequently, we show the rationale behind the efficacy of employing solely the GAN component for image-to-image translation tasks. We offer a clear explanation that substantiates the primary role of GAN components in addressing the image-to-image translation problem. We have conducted a comprehensive review and analysis of the models employed for image generation and image-to-image translation. Our investigation focuses on identifying the efficacy of various components of the network. Notably, we discovered that the autoencoder and GAN models generate homologous output and provide an explanation for this phenomenon. This explanation also extends to the efficiency of GANs in the context of image-to-image translation. From our perspective, we employ a preliminary GAN for image-to-image translation. Furthermore, our findings elucidate why some examples in the network may fail. This paper makes the following contributions: (i) We demonstrate that with a discriminator of sufficient capacity to distinguish between real and synthetic images, adversarial training for autoencoder models yields results similar to those of traditional autoencoder models. This is substantiated through experimental validation. (ii) We extend adversarial training to the image-to-image translation problem, illustrating that a straightforward GAN model can preserve common features and generate novel ones, whereas previous methods impose additional penalties to maintain common features. (iii) Our work provides a rationale for the efficacy of GANs in the image-to-image translation context, clarifying that the decomposition of texture and content signifies common and differentiating characteristics determined by the dataset. This offers a more precise and comprehensive understanding compared to previous studies. The paper is structured as follows: The related works section gives a brief review of image generation and translation. The methods section provides our explanation, encompassing algebraic and geometric interpretations. Subsequently, the experiment section presents three experiments. The first experiment compares the performance of GANs and autoencoders, the second investigates the model’s capability for image-to-image translation, and the third examines the constraints outlined in the methods section. Finally, conclusions are drawn based on our analysis."
https://arxiv.org/html/2411.10334v1,"Y-MAP-Net: Real-time depth, normals, segmentation, multi-label captioning and 2D human pose in RGB images","We present Y-MAP-Net, a Y-shaped neural network architecture designed for real-time multi-task learning on RGB images. Y-MAP-Net simultaneously predicts depth, surface normals, human pose, semantic segmentation, and generates multi-label captions—all from a single network evaluation. To achieve this, we adopt a multi-teacher, single-student training paradigm, where task-specific foundation models supervise the network’s learning, enabling it to distill their capabilities into a lightweight architecture suitable for real-time applications. Y-MAP-Net exhibits strong generalization, simplicity, and computational efficiency, making it ideal for robotics and other practical scenarios. To support future research, we will release our code publicly.","Decades of research have yielded powerful methods that robustly tackle long standing problems for computer vision and pattern recognition. These so-called foundation models stand out for their exceptional generalization ability, achieved through immense scale and complexity. These models, with billions of parameters, are trained on vast datasets, enabling zero-shot problem-solving across diverse tasks and robust performance on in-the-wild data. However this sheer size comes at a cost: they require PetaFLOP-scale computational resources and data centers equipped with thousands of GPGPU accelerators for a single training session, hindering reproducibility. Finally, they are expensive to deploy and impractical for real-time applications. Meanwhile, we observe a class of much sparser convolutional models, such as the successful family of You Only Look Once (YOLO) [48] models, which are profoundly influential in many different real-world applications. This motivates us to develop a framework that bridges the gap between real-time, multifaceted scene understanding and high-fidelity dense output. Once provided, it can be used as a building block to solve other general problems, in vision, robotics, human-computer interaction, surveillance, etc, by providing a unified, streamlined source of multi-modal data. Figure 1: Given an RGB frame, Y-MAP-Net estimates human pose, depth, surface normals, segmentation and image captioning in real-time. The figure shows pose keypoints, depth and normal estimations on publicly available images from factory floors. Given this motivation, in this work, we propose Y-MAP-Net, a Y-shaped Multi Attribute Prediction neural network (NN) model that can be used as a closed-loop processor for RGB streams supplying human pose, depth, surface normals, image segmentation and a textual multiclass caption for each incoming frame (Fig. 6). To achieve this, we adopt a multi-teacher, single-student training paradigm [74, 4], where Y-MAP-Net learns from task-specific foundation models supervising its predictions. This approach distills large models into a compact architecture, enabling efficient multi-task learning without sacrificing generalization. In particular, Y-MAP-Net accepts monocular RGB input and delivers 44 heatmap / image outputs and 8 caption tokens. In summary, our main contributions are the following: • Y-MAP-Net is the first convolutional NN method to achieve simultaneous depth, normal and human pose estimation while also providing scene segmentation and captioning from monocular RGB in a monolithic network. • Due to its efficiency, the method we present performs in real-time and thus opens the way to many practical applications, especially in the field of robotics, complementing existing models for the various tasks we undertake. • The proposed Y-MAP-Net topology offers a novel, simple and pragmatic framework for multi-task learning."
https://arxiv.org/html/2411.10332v1,Number it: Temporal Grounding Videos like Flipping Manga,"Video Large Language Models (Vid-LLMs) have made remarkable advancements in comprehending video content for QA dialogue. However, they struggle to extend this visual understanding to tasks requiring precise temporal localization, known as Video Temporal Grounding (VTG). To address this gap, we introduce Number-Prompt (NumPro), a novel method that empowers Vid-LLMs to bridge visual comprehension with temporal grounding by adding unique numerical identifiers to each video frame. Treating a video as a sequence of numbered frame images, NumPro transforms VTG into an intuitive process: flipping through manga panels in sequence. This allows Vid-LLMs to “read” event timelines, accurately linking visual content with corresponding temporal information. Our experiments demonstrate that NumPro significantly boosts VTG performance of top-tier Vid-LLMs without additional computational cost. Furthermore, fine-tuning on a NumPro-enhanced dataset defines a new state-of-the-art for VTG, surpassing previous top-performing methods by up to 6.9% in mIoU for moment retrieval and 8.5% in mAP for highlight detection. The code will be available at https://github.com/yongliang-wu/NumPro.","Imagine you are watching a cooking video, and trying to locate the exact moment when the chef stirs in the spices. While recognizing such actions is feasible, translating that visual information into precise timing, i.e., a specific second or frame number, is surprisingly difficult. This challenge is central to the field of Video Temporal Grounding (VTG) [16, 4, 40, 44, 21]. In the realm of Video Large Language Models (Vid-LLMs) [51, 33, 67, 28], the integration of VTG allows for fine-grained visual and temporal understanding and reasoning of videos, which is pivotal for developing end-to-end video dialogue systems. Despite advances of Vid-LLMs, endowing these models with effective VTG abilities presents a unique challenge: enhancing the model’s visual recognition of an event within a video does not inherently enable it to describe when the event begins and ends using language [21, 44]. For instance, advanced Vid-LLMs like Qwen2-VL [51], while excelling at video comprehension, can struggle with grounding specific events in time. When asked, e.g., to locate “when does the woman eat food” in a 10-frame video, the model can hallucinate an illogical answer like “from frame 000 to 580.” This limitation arises because these models are primarily trained to align visual content with language descriptions (what happens) while lacking mechanisms to directly interpret the temporal boundaries (when does it happen). This gap in powerful Vid-LLMs leads us to think: How can we empower Vid-LLMs to extract temporal cues directly through visual recognition? A familiar human experience – flipping manga – provides an intuitive solution. When flipping manga, each numbered panel guides readers to follow the sequence of the narrative, linking visual content with a clearly defined timeline. Inspired by this, we introduce Number-Prompt (NumPro), which places unique numerical identifiers on each video frame, similar to manga panel numbers. Current Vid-LLMs process videos by considering them as a sequence of frame images. With NumPro, VTG is as intuitive as flipping manga. As shown in Figure 1, NumPro augments each frame with a unique numerical identifier denoting its position in the temporal sequence. Given a language query targeting an event, Vid-LLMs retrieve relevant visual features of video frames and associate with the frame numbers overlaid. These numerical identifiers are then directly translated into textual outputs. In practice, we strategically position frame numbers in the bottom-right corner, using a medium font size and distinct color. This design ensures numbers visibility without obstructing essential visual content. Overall, NumPro allows Vid-LLMs to “read” the video timeline, effectively converting visual recognition into a temporal narrative in language. NumPro’s elegance lies in its simplicity: by subtly adding frame numbers as temporal markers into video frames, we enable Vid-LLMs to naturally correlate each frame to its temporal location in the video sequence. Unlike previous approaches [44, 21, 17, 52, 31, 42, 22], NumPro does not introduce additional tokens or modify model vocabulary to provide temporal cues, thus avoiding additional learning complexities and maintaining strong transferability across various tasks and datasets. Temporal grounding, therefore, becomes an accessible, “free-lunch” enhancement for Vid-LLMs already proficient in understanding video content. Additionally, fine-tuning on a specially curated NumPro-enhanced VTG dataset (NumPro-FT) further advances state-of-the-art performance. Our contributions can be summarized as follows: • We introduce NumPro, a novel approach that enhances Video Temporal Grounding (VTG) capabilities of Video Large Language Models (Vid-LLMs) by overlaying frame numbers onto video frames, making temporal grounding as intuitive as following numbered panels in flipping manga. • Through an experimental study, we find a suitable NumPro design (font size, color, and position) that ensures high detectability by the model while minimally interfering with the original video content. • We thoroughly evaluate NumPro on standard VTG benchmarks and metrics in both training-free and fine-tuned scenarios, demonstrating its effectiveness across various models and datasets."
https://arxiv.org/html/2411.10330v1,CNN-Based Classification of Persian Miniature Paintings from Five Renowned Schools,"This article addresses the gap in computational painting analysis focused on Persian miniature painting, a rich cultural and artistic heritage. It introduces a novel approach using Convolutional Neural Networks (CNN) to classify Persian miniatures from five schools: Herat, Tabriz-e Avval, Shiraz-e Avval, Tabriz-e Dovvom, and Qajar. The method achieves an average accuracy of over 91%. A meticulously curated dataset captures the distinct features of each school, with a patch-based CNN approach classifying image segments independently before merging results for enhanced accuracy. This research contributes significantly to digital art analysis, providing detailed insights into the dataset, CNN architecture, training, and validation processes. It highlights the potential for future advancements in automated art analysis, bridging machine learning, art history, and digital humanities, thereby aiding the preservation and understanding of Persian cultural heritage.","The fusion of art and technology has opened new avenues in the study and appreciation of visual arts, leading to a renaissance in computational painting analysis. This interdisciplinary approach has significantly enhanced our understanding of various art forms, yet certain areas remain less explored. Among these is the rich tradition of Persian miniature painting, a genre that holds a unique position in the annals of art history due to its distinctive styles and cultural significance. Persian miniatures, characterized by their intricate designs and vivid storytelling, are more than mere artistic expressions; they are a window into the historical, cultural, and social fabric of their times. These paintings are distinguished not only by their aesthetic appeal but also by their deep symbolic meanings and the portrayal of contemporary life, making them invaluable for cultural studies. As illustrated in Figure 1, the diversity and richness of Persian artistic heritage are vividly captured through a carefully curated collage of Persian miniatures, highlighting the varied schools of art and cultural narratives. Figure 1: An exquisite collage of Persian miniatures, showcasing a rich tapestry of styles from various schools of art, each piece a testament to the diverse cultural narratives and artistic expressions of the Persian heritage. The schools of Persian miniatures, each a distinct universe of style and expression, are closely tied to the dynasties under which they flourished. For instance, the Tabriz school, prospering during the Safavid era, is renowned for its vivid colors and dynamic scenes, reflecting the opulence and grandeur of Safavid rule. In contrast, the Shiraz school, with its romantic and emotional style, often depicted in soft hues, illustrates the cultural milieu of the region, marked by poetic landscapes and love stories (Loukonine et al., 2010). The geographical diversity has also played a crucial role in shaping these schools. The Isfahan school, emerging as the artistic capital during the Safavid period, is distinguished by its balanced and harmonious compositions, blending miniature painting with calligraphy, a reflection of Isfahan’s status as a cultural hub. Similarly, the Qazvin school, known for its bold color schemes and aesthetic emphasis, represents the artistic evolution under the Safavid dynasty’s patronage. Moreover, Persian miniatures are a canvas where eastern and western influences converge. The impact of Chinese art is evident in the Herat school, particularly during the Timurid era, where elements like dragons and phoenixes, along with specific color palettes, are prominent. The Qajar school, on the other hand, reveals the influence of Western art, evident in its realistic portraiture and adoption of European painting techniques, highlighting the cultural exchanges during the Qajar dynasty (O’Kane, 2021). In exploring the distinctive features of Persian miniatures that allow for the differentiation of various schools within this wealthy artistic tradition, it is imperative to acknowledge the intricate symbols and motifs that carry profound cultural and historical significance. A notable example is the emblematic Qizilbash hat, which serves as a unique identifier of the Tabriz School during the Safavid dynasty. This highlights how elements such as the Qizilbash hat not only denote religious and political affiliations but also subtly convey narratives of power, loyalty, and societal order within the artwork. Figure 2 demonstrates the incorporation of the Qizilbash hat in the Tabriz school’s Persian miniatures, reflecting how art intricately reflects the rich tapestry of cultural, spiritual, and political influences of its era(Moghaddam, 2021). Figure 2: An illustration of the Qizilbash hat within the Persian miniatures of the Tabriz School. In the approach of this paper, we leverage the capabilities of pre-trained Convolutional Neural Networks (CNNs) for feature extraction from the images of Persian miniatures. Recognizing the unique and intricate details inherent in these artworks, CNNs offer a robust framework for capturing the nuanced visual elements that define each school of art. Upon extracting these features, we introduce shallow, fully connected layers tailored to the specific task of classifying these artworks into five distinct classes corresponding to the major schools of Persian miniature painting. Our research contributes significantly to the field of digital art analysis by presenting a meticulously curated dataset that encapsulates the distinct features of each school. This dataset not only serves as a foundation for our CNN-based methodology but also as a valuable resource for further studies in art history and digital humanities. Through this work, we aim to enhance the understanding and preservation of Persian miniatures, offering new perspectives and tools for art historians, curators, and enthusiasts alike. The remainder of this paper is structured as follows: Section 2, provides a comprehensive review of previous research in the field of art style classification, highlighting the advances and identifying the gaps that our study aims to fill. In Section 3, we delve into the core of our research approach, beginning with patch extraction, where we describe the process of segmenting Persian miniature images into patches, and transfer learning, outlining how we adapt a pre-trained neural network for the classification of these patches. Section 4, details the practical aspects of our research. This includes dataset, where we describe the unique collection of Persian miniatures used in our study, classification models, which explains the architecture and functioning of the neural network model we employ, and evaluation metrics, where we discuss the criteria used to assess the performance of our model. In Section 5, we present the findings of our experiments, analyzing and interpreting the results in context of Persian miniature classification. Finally Section 6, summarizes our study, reflecting on its contributions, limitations, and potential areas for future research."
https://arxiv.org/html/2411.10322v1,melanoma detection with uncertainty quantification,"Early detection of melanoma is crucial for improving survival rates. Current detection tools often utilize data-driven machine learning methods but often overlook the full integration of multiple datasets. We combine publicly available datasets to enhance data diversity, allowing numerous experiments to train and evaluate various classifiers. We then calibrate them to minimize misdiagnoses by incorporating uncertainty quantification. Our experiments on benchmark datasets show accuracies of up to 93.2% before and 97.8% after applying uncertainty-based rejection, leading to a reduction in misdiagnoses by over 40.5%. Our code and data are publicly available111Our GitHub repository, https://mpsych.org/melanoma, and a web-based interface222Our Web interface, https://mpsych.github.io/melanoma/ for quick melanoma detection of user-supplied images is also provided.","Melanoma is a severe skin cancer responsible for around 55,500 deaths annually [1]. Deep Neural Networks (DNNs) are effective in melanoma detection, but their evaluation lacks standardization, complicating comparisons. For example, DNN-based melanoma classification using segmented features [2] has shown promising results on the ISIC’16 [3] and ISIC’17 [4] datasets. Transfer learning methods [5] have also achieved high accuracy on ISIC’18 [6]. Additionally, Vision Transformers [7] have demonstrated exceptional accuracy on a private dataset. However, many studies operate under inconsistent testing conditions. Melanoma lesions exhibit significant variability, making detection challenging in real-world settings. While DNNs require extensive datasets for accuracy, only a few studies have integrated multiple datasets within a unified framework. Additionally, although DNN predictions can be accurate, they may be confidently incorrect, underscoring the need for reliable diagnostic methods. We develop a framework for consistent experimentation with datasets and DNNs to minimize misdiagnoses. It includes four modules: Input, Melanoma Recognition, Uncertainty Analysis, and Integration, as shown in Fig. 1. Our methodology involves importing multiple datasets, uniform preprocessing, and evaluating classifiers in classification, calibration, and uncertainty scores. We filter out uncertain predictions to enhance the classification and calibration performance of the remaining samples. Ultimately, we aim to develop reliable classifiers for clinical use and help researchers identify optimal dataset and classifier combinations."
https://arxiv.org/html/2411.10321v1,Probabilistic Prior Driven Attention Mechanism Based on Diﬀusion Model for Imaging Through Atmospheric Turbulence,"Atmospheric turbulence introduces severe spatial and geometric distortions, challenging traditional image restoration methods. We propose the Probabilistic Prior Turbulence Removal Network (PPTRN), which combines probabilistic diffusion-based prior modeling with Transformer-driven feature extraction to address this issue. PPTRN employs a two-stage approach: first, a latent encoder and Transformer are jointly trained on clear images to establish robust feature representations. Then, a Denoising Diffusion Probabilistic Model (DDPM) models prior distributions over latent vectors, guiding the Transformer in capturing diverse feature variations essential for restoration. A key innovation in PPTRN is the Probabilistic Prior Driven Cross Attention mechanism, which integrates the DDPM-generated prior with feature embeddings to reduce artifacts and enhance spatial coherence. Extensive experiments validate that PPTRN significantly improves restoration quality on turbulence-degraded images, setting a new benchmark in clarity and structural fidelity.","Imaging through atmospheric turbulence presents significant challenges due to severe and unpredictable distortions, including spatial and geometric aberrations that degrade image quality [2, 14, 11, 39, 32, 6, 18, 13]. Common in applications like surveillance, astronomy, and remote sensing, these distortions obscure fine details, complicating reliable image restoration for high-fidelity analysis. Traditional methods, including computational techniques and convolutional neural networks (CNNs), have been widely applied to mitigate turbulence-induced degradation [20, 34, 4, 12]. However, classical methods rely on simplified models unsuitable for dynamic conditions, while CNNs struggle to capture long-range dependencies essential for handling turbulence. As a result, these approaches often produce oversmoothed outputs lacking detail and structural consistency. Restoring turbulence-degraded images is challenging due to high uncertainty and multi-modal distortions. Atmospheric turbulence requires probabilistic modeling to account for multiple plausible reconstructions and avoid oversmoothing [21, 8, 32, 18]. Additionally, maintaining spatial coherence alongside fine details is critical, as traditional approaches often fail to balance these aspects, resulting in outputs that lack structural integrity or critical details [14, 2, 16]. To address these challenges, we propose the Probabilistic Prior Turbulence Removal Network (PPTRN), a model that combines Denoising Diffusion Probabilistic Model (DDPM)-based probabilistic prior modeling with Transformer-driven feature extraction. The core of PPTRN is the Probabilistic Prior Driven Cross Attention mechanism, which fuses a DDPM-generated latent prior with feature embeddings to enhance detail preservation and spatial coherence. PPTRN employs a two-stage training strategy: initially, a latent encoder and Transformer are jointly trained on clear images; subsequently, the encoder’s weights are frozen, and DDPM models the prior distribution over latent vectors, guiding the Transformer in robust image restoration. Our contributions include: 1. A novel framework (PPTRN) that integrates probabilistic prior modeling with Transformer-based feature extraction for turbulence-distorted images. 2. A Probabilistic Prior Driven Cross Attention mechanism that improves detail preservation and spatial coherence. 3. A two-stage training strategy that balances structural consistency and detail preservation, capturing the multi-modal features of turbulence-affected images. These innovations highlight PPTRN’s effectiveness in restoring turbulence-degraded images, advancing uncertainty handling in visual restoration. Figure 1: The network structure of the proposed Probabilistic Prior Turbulence Removal Network (PPTRN)."
https://arxiv.org/html/2411.10316v1,M3TR: Generalist HD Map Construction with Variable Map Priors,"Autonomous vehicles require road information for their operation, usually in form of HD maps. Since offline maps eventually become outdated or may only be partially available, online HD map construction methods have been proposed to infer map information from live sensor data. A key issue remains how to exploit such partial or outdated map information as a prior. We introduce M3TR (Multi-Masking Map Transformer), a generalist approach for HD map construction both with and without map priors. We address shortcomings in ground truth generation for Argoverse 2 and nuScenes and propose the first realistic scenarios with semantically diverse map priors. Examining various query designs, we use an improved method for integrating prior map elements into a HD map construction model, increasing performance by +4.3 mAP. Finally, we show that training across all prior scenarios yields a single Generalist model, whose performance is on par with previous Expert models that can handle only one specific type of map prior. M3TR thus is the first model capable of leveraging variable map priors, making it suitable for real-world deployment. https://github.com/immel-f/m3tr","In order to drive safely, autonomous vehicles need to understand the geometry and topology of the roads as well as the traffic rules that apply to them. Current systems employ detailed semantic high-definition (HD) maps that provide this rich knowledge, but are primarily created using offline SLAM approaches. However, mapping the entire world in advance and maintaining HD maps to account for changes in the road layout does not scale. Therefore, recent advances in computer vision aim to perceive HD map information with onboard sensors [11, 17, 13, 14, 5, 28]. This task of online vectorized HD map construction uses sensor data, e.g. from cameras or LiDAR sensors, to detect vectorized map elements (lane markings, road borders, etc.) with their semantic meaning. Compared to offline HD planning maps however, the output of online HD map construction models still lacks a large amount of information. Since HD maps only become outdated gradually, in addition to live sensor data, oftentimes some kind of vectorized prior map information is available and still up-to-date. This prior can range from navigation maps on a road level to parts of an HD map or even complete and recent HD maps. Existing work that incorporates prior information falls short for three main reasons: Detection transformer queries are used to provide vectorized priors to the model [22], but the option space for query design is left underexplored. Furthermore, previous approaches focus on specialized models that can only exploit a single type of map prior that is assumed to be known in advance – an unrealistic expectation for real-world deployment. Finally, current approaches lack a task definition and evaluation metric that both uses meaningful prior scenarios and can differentiate map elements with a prior and those that need to be perceived online. To address these points, we propose M3TR, a generalist HD map construction method with variable map priors. Figure 1: Overview of the model architecture of M3TR and the investigated point query encoder designs. For our evaluated task of HD map completion, we mask out instances from the ground truth map \mathcal{M_{\mathrm{GT}}} to create a map prior \mathcal{M_{\mathrm{P}}}. Using \mathcal{M_{\mathrm{P}}}, we try to reconstruct \mathcal{M_{\mathrm{GT}}}. The map prior instances are supplied to the model as queries, influenced by the shown point query encoder and the detection query set design which is further illustrated in Fig. 5. Contributions Our contributions can be summarized as follows: • We combine several recent label improvements and introduce semantically richer labels to move the HD map construction task closer towards real HD planning maps. • We systematically investigate the HD map completion task, i.e. map construction with prior map information. We introduce meaningful prior scenarios and quantify the importance of different prior element categories by evaluating their performance impact on those perceived online. • We use synthetic map masking as efficient augmentation for HD map construction with priors. • We propose a novel query design to incorporate map priors on a point query and query set level that considerably improves detection performance on the Argoverse 2 and nuScenes datasets by up to +4.3 mAP. • We introduce a novel training regime which results in a single model for all map prior scenarios. This Generalist model achieves performance on par with specialized models without needing to know which kind of map information is available."
https://arxiv.org/html/2411.10309v1,"Modification Takes Courage: 
Seamless Image Stitching via Reference-Driven Inpainting","Current image stitching methods often produce noticeable seams in challenging scenarios such as uneven hue and large parallax. To tackle this problem, we propose the Reference-Driven Inpainting Stitcher (RDIStitcher), which reformulates the image fusion and rectangling as a reference-based inpainting model, incorporating a larger modification fusion area and stronger modification intensity than previous methods. Furthermore, we introduce a self-supervised model training method, which enables the implementation of RDIStitcher without requiring labeled data by fine-tuning a Text-to-Image (T2I) diffusion model. Recognizing difficulties in assessing the quality of stitched images, we present the Multimodal Large Language Models (MLLMs)-based metrics, offering a new perspective on evaluating stitched image quality. Compared to the state-of-the-art (SOTA) method, extensive experiments demonstrate that our method significantly enhances content coherence and seamless transitions in the stitched images. Especially in the zero-shot experiments, our method exhibits strong generalization capabilities. Code: https://github.com/yayoyo66/RDIStitcher","Image stitching is a fundamental problem in computer vision, which aims to seamlessly integrate multiple images captured from different perspectives into a wide field-of-view composite image [19, 42, 9]. Image fusion is the core stage of the image stitching pipeline and focuses on combining aligned images without visible seams or artifacts. However, as illustrated in Fig.1, this stage encounters two major challenges: (1) Uneven hue. Due to variations in atmospheric lighting conditions and camera settings, images taken from different viewpoints of the same scene may display inconsistent hues. When fusing images with significant hue differences, visible seams will likely appear in the stitched image. (2) Large parallax. Large parallax refers to the significant difference in the relative positions of objects in a scene when captured from different viewpoints. Existing homography-based registration methods [21, 20, 16] struggle to accurately align images in large parallax scenes, leading to noticeable artifacts and misalignment of content in the stitched images. Current image fusion methods can be divided into three categories, including reconstruction-based (recon-based), seam-based, and inpainting-based. Recon-based methods [21, 24, 22] use pixel-by-pixel reconstruction to smooth the fused image, effectively handling scenes with uneven hues. However, in large parallax scenarios, recon-based methods can introduce notable artifacts, which degrade image quality. Additionally, seam-based methods [25, 7, 14] work by identifying optimal seams for image fusion. Nevertheless, these methods heavily rely on the assumption that perfect seams exist, which often fails to hold true in uneven hue and large parallax scenarios. Finally, the inpainting-based method [48] proposes to modify the fusion area to improve fusion effects. Unfortunately, the existing method is conservative in the size selection of modification fusion areas, so it is difficult to deal with uneven hue and large parallax scenarios. To detail the advantages and disadvantages of the three methods in different challenge scenarios, we conducted a small user experience survey presented in the Fig.2. To address the limitations of current methods, we propose a key principle: Modification takes courage, including area size and intensity. We develop the RDIStitcher, which utilizes a larger modified area for fusion than previous seam-based and inpainting-based methods. Compared to recon-based methods, RDIStitcher applies stronger modification intensity. However, larger and stronger modifications come at a cost, which is introducing more content instability into the stitched image. Due to the shortage of labeled data, SRStitcher is unable to train the model and can only maintain content consistency before and after stitching by limiting the size of the fusion modification area. Therefore, to implement RDIStitcher, we propose a self-supervised training method. Specifically, we leverage pre-knowledge from an unlabeled image stitching dataset [22] by using a pre-trained registration model to generate pseudo-stitching images based on single-view images. Subsequently, we apply a large-scale pre-trained T2I diffusion model [2] to learn the restoration of the single-view image based on the pseudo-stitching images, effectively teaching the model the new concept of stitching. Our experiments demonstrate that this self-supervised training method achieves remarkable results with high generalization ability, requiring only a few parameters to fine-tune the T2I model. Figure 2: A user experience survey of the recon-based method UDIS [22], the seam-based method UDIS++ [25], and the inpainting-based method SRStitcher [48] on uneven hue and large parallax scenes. Please see the Supplementary Material for more details. After designing the model and training method, the final task is to measure the stitched image quality in challenging scenarios without ground truth. The previous works UDIS [22] and UDIS++ [25] rely solely on small-scale user evaluations to assess stitched image quality, which is costly and lacks comprehensiveness. In addition, SRStitcher [48] introduces the No-Reference Image Quality Assessment (NR-IQA) metrics. However, existing NR-IQA techniques [30, 34] have significant flaws when applied to assessing stitched image quality, especially in detecting fine-grained stitching issues [48]. To address these challenges, we develop assessment methods based on MLLMs for stitched images, including the Single-Image Quality Score (SIQS) and the Multi-Image Comparative Quality Score (MICQS). We summarize the main contributions as follows: • We reformulate the fusion and rectangling tasks as a reference-driven inpainting model. This model achieves remarkable stitching effects in challenging scenarios including uneven hues and significant parallax, while preserving the original structure and content of the input images. (Sec.3.1 and Sec.3.2) • We introduce a self-supervised training method that enables RDIStitcher to be trained without the need for labeled data. This method requires fine-tuning only a small number of parameters in a large-scale pre-trained T2I model, resulting in low hardware requirements. To our knowledge, this is the first unsupervised training method for the rectangling problem. (Sec.3.3) • We propose quality metrics for assessing stitched images by MLLMs. By incorporating the MLLMs into the image stitching domain for the first time, we offer a pioneering research perspective for the evaluation of stitched images. (Sec.4.2)"
https://arxiv.org/html/2411.10308v1,A Realistic Collimated X-Ray Image Simulation Pipeline,"Collimator detection remains a challenging task in X-ray systems with unreliable or non-available information about the detectors position relative to the source. This paper presents a physically motivated image processing pipeline for simulating the characteristics of collimator shadows in X-ray images. By generating randomized labels for collimator shapes and locations, incorporating scattered radiation simulation, and including Poisson noise, the pipeline enables the expansion of limited datasets for training deep neural networks. We validate the proposed pipeline by a qualitative and quantitative comparison against real collimator shadows. Furthermore, it is demonstrated that utilizing simulated data within our deep learning framework not only serves as a suitable substitute for actual collimators but also enhances the generalization performance when applied to real-world data.","In digital radiography, the detection of collimator-covered areas is essential to present diagnostically relevant regions to radiologists. Geometric alignment algorithms, as described in [9], can be employed in X-ray systems with known extrinsic projection parameters. However, despite their availability, these often suffer from inaccuracies sabotaging effectiveness in practice. Due to the inherent geometrical variability in conventional X-ray systems, particularly with mobile flat panel detectors, precise information of the relative position to the detector is unavailable. Moreover, imprecise collimator movement further complicates the detection process, necessitating analysis within image domain. Contrary to a simplistic threshold-based approach, the identification of relevant areas is challenging due to the presence of physical effects like edge-blurring, noise, and scattered radiation. Even human visual perception faces difficulties due to these complexities, as depicted in Fig. 1. (a) Full contrast (b) Contrast adjusted (c) Collimator mask (d) Lineplot Figure 1: Illustrative case for collimator detection depicted in two contrast settings. (a) Contrast adjusted to full image. (b) Contrast adjusted to the orange box. The collimated area (c) is shown as a binary mask. In (d), the intensity profile along the dashed line is compared to the collimated area to visualize the complexity of image-based collimator detection. Deep neural networks (DNNs) show promise for collimator detection, but the limited availability of pre-processed raw data poses a challenge for training robust networks in medical applications. So far, machine learning approaches for collimator detection have not significantly outperformed classic analytical methods in the literature. For instance, comparing the plane detection Hough transform proposed by Kawashita et al. [6] with Mao et al.’s [11] approach that combines random forest learning with a landmark detector in a multi-view learning approach, both methods demonstrate similar performance on unseen data. According to Mao et al. [11], each classifier was trained using only 200 training images. To enhance the performance of machine learning algorithms, it is reasonable to assume that the implementation of robust data augmentation techniques is beneficial. These techniques aim to increase the quantity and variety of datasets. In this context, suitable augmentation techniques can be categorized into deep learning-based methods, such as generative adversarial networks (GANs) [5], and physically motivated approaches. Although GANs have shown promising potential for post-processed X-ray image augmentation (without collimators) in studies like Bowles et al. [1], Madani et al. [10], Kora et al. [7], and Ng et al. [12], they require sophisticated techniques and lack comprehensibility when aiming to serve as reliable training data. Unlike this concept, physically motivated approaches offer a robust alternative for augmentation. These methods leverage an understanding of the underlying physics involved in imaging processes. By incorporating physical principles, these approaches ensure reproducibility and reliability, as demonstrated by Eckert et al. [4] and Xu et al. [15]. In this paper a physically motivated image processing pipeline is presented that simulates the characteristics of real collimators enabling the expansion of limited datasets of X-ray images without collimators. The data augmentation method enables the generation of unlimited pre-processed image data e.g. for training DNNs."
https://arxiv.org/html/2411.10293v1,"RETR: Multi-View Radar Detection Transformer 
for Indoor Perception","Indoor radar perception has seen rising interest due to affordable costs driven by emerging automotive imaging radar developments and the benefits of reduced privacy concerns and reliability under hazardous conditions (e.g., fire and smoke). However, existing radar perception pipelines fail to account for distinctive characteristics of the multi-view radar setting. In this paper, we propose Radar dEtection TRansformer (RETR), an extension of the popular DETR architecture, tailored for multi-view radar perception. RETR inherits the advantages of DETR, eliminating the need for hand-crafted components for object detection and segmentation in the image plane. More importantly, RETR incorporates carefully designed modifications such as 1) depth-prioritized feature similarity via a tunable positional encoding (TPE); 2) a tri-plane loss from both radar and camera coordinates; and 3) a learnable radar-to-camera transformation via reparameterization, to account for the unique multi-view radar setting. Evaluated on two indoor radar perception datasets, our approach outperforms existing state-of-the-art methods by a margin of 15.38+ AP for object detection and 11.77+ IoU for instance segmentation, respectively.","Perception information encompasses the processes and technologies to detect, interpret, and understand their surroundings. Complementary to the mainstream camera and LiDAR sensors, radar can enhance the safety and resilience of perception under low light, adversarial weather (e.g., rain, snow, dust), and hazardous conditions (e.g., smoke, fire) at affordable device and maintenance cost. An emerging application of radar perception is indoor sensing and monitoring for elderly care, building energy management, and indoor navigation [7]. A notable limitation of indoor radar perception is the low semantic features from radar signals. Earlier efforts use radar detection points [42, 30] to support simple classification tasks such as fall detection and activity recognition over a limited number of patterns. To support challenging perception tasks such as object detection, pose estimation, and segmentation, lower-level radar signal representation such as radar heatmaps is more preferred. Along this line, the earliest work is RF-Pose [43] using a convolution-based autoencoder network to fuse features from the two radar views and regress keypoints for 2D image-plane pose estimation. It is later extended to 3D human pose estimation [44]. It is noted that RF-Pose is not publicly accessible. More recently, RFMask [38] borrows the Faster R-CNN framework [27] by proposing candidate regions only in the horizontal radar heatmap via a region proposal network (RPN). A corresponding proposal in the vertical radar heatmap is automatically determined using a fixed-height candidate region at the same depth as the horizontal proposal. The combined horizontal and vertical proposals are then projected into the image plane for bounding box (BBox) estimation. In addition, RFMask calculates the BBox loss only over the 2D horizontal radar view and disregards features from the vertical radar heatmap for BBox estimation. Figure 1: By taking horizontal-view and vertical-view radar heatmaps as inputs, RETR introduces a depth-prioritizing positional encoding (exploit the shared depth between the two radar views) into transformer self-attention and cross-attention modules and outputs a set of 3D-embedding object queries to support image-plane object detection and segmentation via a calibrated or learnable radar-to-camera coordinate transformation and 3D-to-2D pinhole camera projection. In this paper, we exploit features from both horizontal and vertical radar views for object estimation and segmentation and introduce Radar dEtection TRansformer (RETR) (Fig. 1). RETR extends the popular Detection Transformer (DETR) [3], which effectively eliminates the need for hand-crafted components such as non-maximum suppression and proposal/anchor generation, to the multi-view radar perception. More importantly, RETR incorporates carefully designed modifications to exploit the unique multi-view radar setting such as shared depth dimension and the transformation between the radar and camera coordinate systems. Our contributions are summarized below: 1. Extending DETR for Multi-View Radar Perception: 1) Encoder: we associate features from both radar views by applying self-attention over the pooled multi-view radar tokens, eliminating the need for a cumbersome association scheme. We introduce a top-K feature selection to allow only K features from each view to keep the complexity low. 2) Decoder: the DETR decoder provides a natural way to associate the same object query to corresponding features from the two radar views via cross-attention. As such, the object query is able to learn 3D spatial embedding of objects in the radar coordinate (see Fig. 1). 2. Tunable Positional Encoding: To enhance feature association across the two radar views, we further exploit the fact that the two radar views share the depth dimension and introduce a tunable positional encoding (TPE) as an inductive bias. TPE imposes constraints in the attention map to prioritize the relative importance of depth dimension and avoid exhaustive correlations between radar views. 3. Tri-Plane Loss from Both 3D Radar Coordinate and 2D Image Plane: we enforce the output queries of the DETR decoder to directly predict 3D BBoxes in the radar coordinate system and convert them into the 2D image plane. We introduce a tri-plane loss that combines the BBox loss in the 3D radar plane and that in the 2D image plane, to calculate the global set-prediction loss. 4. Learnable Radar-to-Camera Coordinate Transformation: We employ a calibrated radar-to-camera coordinate transformation via a calibration process and a learnable coordinate transformation via reparameterization by preserving the orthonormal (i.e., 3D special orthogonal group \mathcal{SO}\left({3}\right)) structure of the rotation matrix. We demonstrate the effectiveness of our contributions through evaluations on two open datasets: the HIBER dataset [38] and the MMVR dataset [26]."
https://arxiv.org/html/2411.10281v1,"Multidimensional Byte Pair Encoding:
Shortened Sequences for Improved Visual Data Generation","In language processing, transformers benefit greatly from text being condensed. This is achieved through a larger vocabulary that captures word fragments instead of plain characters. This is often done with Byte Pair Encoding. In the context of images, tokenisation of visual data is usually limited to regular grids obtained from quantisation methods, without global content awareness. Our work improves tokenisation of visual data by bringing Byte Pair Encoding from 1D to multiple dimensions, as a complementary add-on to existing compression. We achieve this through counting constellations of token pairs and replacing the most frequent token pair with a newly introduced token. The multidimensionality only increases the computation time by a factor of 2 for images, making it applicable even to large datasets like ImageNet within minutes on consumer hardware. This is a lossless preprocessing step. Our evaluation shows improved training and inference performance of transformers on visual data achieved by compressing frequent constellations of tokens: The resulting sequences are shorter, with more uniformly distributed information content, e.g. condensing empty regions in an image into single tokens. As our experiments show, these condensed sequences are easier to process. We additionally introduce a strategy to amplify this compression further by clustering the vocabulary.","Figure 1: Our algorithm compresses visual data in order to make tasks like generation more efficient: Shorter sequences, even if they are from a larger vocabulary, are easier to handle for deep learning architectures like transformers. The images show representative examples after the same training time, with training on shortened sequences (right) producing better results faster. Much of the recent success and widespread application of deep learning has come with the understanding that generation can form a building block, a foundation model, for further tasks. These models then can be tuned to various tasks for practical applications, from chat assistants [17, 41, 34] to image [11, 28, 10] or video editing [43, 20, 55]. For natural language processing (NLP), condensation from long sequences with small vocabulary into shorter sequences with larger vocabulary significantly improves the training of models. This approach not only increases efficiency by encoding more information per token, resulting in faster and more effective learning, but also plays well with the quadratic scaling of the underlying attention mechanism [4] that has been driving many recent improvements [3]. In practice, it is easier to assemble sentences from word fragments rather than individual characters. One widespread strategy to obtain larger vocabularies is Byte-Pair Encoding (BPE) [56, 54]. By merging frequent pairs of tokens - initially single letters - BPE forms compact, reusable word fragments. This approach offers two main advantages: It is lossless, meaning it preserves information while enhancing efficiency, and it is adaptive, balancing what is best to compress given a certain vocabulary budget. For visual data like images, condensation often happens in a content agnostic grid format, e.g. through autoencoders [61, 49, 52, 24], which does not address uneven information density: A patch of uniform colours in the background is represented through the same amount of tokens as a patch containing complex patterns in the foreground. While global information mechanisms such as attention or U-Net [51] architectures can enhance learned representations by distributing information more evenly, they do not fundamentally resolve the issue: The resulting compression is not adaptive to the content. Adaptive approaches like Byte Pair Encoding on images that are reshaped into 1D sequences only apply compression along one axis. Therefore, we extend this form of compression to true multidimensionality to yield similar benefits for visual data as for BPE on text. Our multidimensional approach retains the strengths of traditional BPE, i.e. it is also lossless and adaptive, while offering more flexibility in the choice of pairs and can be integrated as an add-on into various discrete setups seamlessly [61, 49, 19, 50]. We see applications not only for faster generation, but also e.g. in multimodal LLMs, where fewer tokens to represent information accurately is of the essence. This work is therefore centred around a simple hypothesis: Shorter sequences from visual data are better suited for processing with neural networks. Our contributions are as follows: • Motivated by 1D token compression from NLP, we provide a preprocessing step to compress visual data. • We show how shortened sequences with varying lengths can be better processed by transformers. • To further improve effectiveness, we show a lossy variant to improve compression rates for discrete tokens. • We evaluate the improved performance of the resulting compressed sequences on generation tasks. Our work leads to earlier convergence at better final scores, while enabling e.g. large sizes of voxel grids that would otherwise not fit into the memory in the first place. It further aligns well with the recent trend of including multimodal data into LLMs [17, 41]. We provide the complete codebase for our approach as easily accessible Jupyter Notebooks, along with a faster C++ implementation for the compression algorithm at the core111We provide a small MNIST demo that can be run within minutes here: https://github.com/DaiDaiLoh/MDBPE_TF There, we will eventually release the full version including the faster C++ code.."
https://arxiv.org/html/2411.10273v1,Fill in the blanks: Rethinking Interpretability in vision,"Model interpretability is a key challenge that has yet to align with the advancements observed in contemporary state-of-the-art deep learning models. In particular, deep learning aided vision tasks require interpretability, in order for their adoption in more specialized domains such as medical imaging. Although the field of explainable AI (XAI) developed methods for interpreting vision models along with early convolutional neural networks, recent XAI research has mainly focused on assigning attributes via saliency maps. As such, these methods are restricted to providing explanations at a sample level, and many explainability methods suffer from low adaptability across a wide range of vision models. In our work, we re-think vision-model explainability from a novel perspective, to probe the general input structure that a model has learnt during its training. To this end, we ask the question:“How would a vision model fill-in a masked-image”. Experiments on standard vision datasets and pre-trained models reveal consistent patterns, and could be intergrated as an additional model-agnostic explainability tool in modern machine-learning platforms. The code will be available at https://github.com/BoTZ-TND/FillingTheBlanks.git","The rapid advancements of Machine Learning (ML) and Deep Learning(DL) models show no signs of slowing down, with multimodal models further alluring researchers into pushing performance limits. However, the challenge of adopting DL architectures into critical applications such as medical imaging is largely due to our limited understanding of the inner workings of models. In an effort to understand these “black box” models, the field of explainable AI (XAI) emerged with a special focus on ‘interpretability’, and ‘explainability’. Although these terms are sometimes used interchangeably, following efforts to define these terms more concretely [1], we shall define them for our work in the context of vision models. ‘Interpretability’ is when a concept such as a model prediction, is mapped back to an image that makes sense to a human. It answers the question “What input would generate a given output/ activation?”. ‘Explainability’ aims to find the contribution of image features towards the model’s output. In other words “What features of this image were significant for the model’s decision?”. The former was the main focus among researchers when convolutional neural network (CNN) architectures were being developed for vision tasks. Particularly focused on classification, models [2, 3] were trained to predict to search for input images that maximized a certain activation node in the network. Consequently, input features which contributed to a particular class were visualised in the image space. We shall refer to this as a ‘global’ understanding of what the model has learnt as the concept of a given class. However, as we shall elaborate in section 2, these methods involved a separate model to be trained, and hence is impractical for fast inference on new models. A more ‘local’ approach developed, where given an input image and an activation, each pixel was scored corresponding to its contribution towards that activation [4]. These patterns were visualized using heat-maps to reveal the pixel patterns within the input that an activation/node is particularly sensitive to. Therefore, the focus was on explaining the input, rather than interpreting what the model had learnt as a whole. In the context of an image classifier, this meant observing which pixels made the model choose a given class (e.g: digit ‘three’), but not observing what the model generally perceives as ‘three’. This would not give an idea on what is the ‘prototypical digit’ that the model learnt after training on samples from an image distribution. In this light, we strive to re-visit the global approach of visually interpreting vision models. However, instead of generating a prototypical image, we ask a different question: “How would a vision model fill-in a masked image?”. Ideally, we expect to visualise how the model would fill in masked out portions of an image, based on the limited priors that the unmasked patches provide. We answer this in the context of image classification, and propose a method that does not require to train a separate image generator. Through this novel approach of interpreting image classifiers, we present the following: 1. Introduce a mask-filling approach for visually interpreting vision models, as an alternative to existing generative methods. 2. Present different approaches of masking, and interpret the visual results on standard data sets. 3. Demonstrate consistency of the visual patterns and present the effects of changing masking parameters."
https://arxiv.org/html/2411.10261v1,Partial Scene Text Retrieval,"The task of partial scene text retrieval involves localizing and searching for text instances that are the same or similar to a given query text from an image gallery. However, existing methods can only handle text-line instances, leaving the problem of searching for partial patches within these text-line instances unsolved due to a lack of patch annotations in the training data. To address this issue, we propose a network that can simultaneously retrieve both text-line instances and their partial patches. Our method embeds the two types of data (query text and scene text instances) into a shared feature space and measures their cross-modal similarities. To handle partial patches, our proposed approach adopts a Multiple Instance Learning (MIL) approach to learn their similarities with query text, without requiring extra annotations. However, constructing bags, which is a standard step of conventional MIL approaches, can introduce numerous noisy samples for training, and lower inference speed. To address this issue, we propose a Ranking MIL (RankMIL) approach to adaptively filter those noisy samples. Additionally, we present a Dynamic Partial Match Algorithm (DPMA) that can directly search for the target partial patch from a text-line instance during the inference stage, without requiring bags. This greatly improves the search efficiency and the performance of retrieving partial patches. We evaluate the proposed method on both English and Chinese datasets in two tasks: retrieving text-line instances and partial patches. For English text retrieval, our method outperforms state-of-the-art approaches by 8.04% mAP and 12.71% mAP on average, respectively, among three datasets for the two tasks. For Chinese text retrieval, our approach surpasses state-of-the-art approaches by 24.45% mAP and 38.06% mAP on average, respectively, among three datasets for the two tasks. The source code and dataset are available at https://github.com/lanfeng4659/PSTR.","In recent years, scene text understanding has attracted significant research interest from the computer vision community due to a large amount of everyday scene images that contain texts. Scene text retrieval, introduced by Mishra et al. [1], aims to search for all scene text instances that are the same or similar to given query text from a collection of natural images. Apart from handling scene text instances, partial scene text retrieval further probes their partial patches. Such a task is quite valuable in many applications, such as book search in libraries [2], key frame extraction of videos [3], and visual search [4, 5]. Recent text retrieval methods [6, 7, 8] built on deep learning frameworks extract Pyramidal Histogram Of Character (PHOC) [9] or global features of scene text for measuring distances to query text. However, these methods lack local features of scene text, which prevents them from searching for partial patches. Another feasible solution to scene text retrieval is based on an end-to-end text recognition system, such as [10, 11]. Under this setting, the retrieval results are determined according to the occurrences of the query text within the spotted words. Such methods often achieve unsatisfactory retrieval performance and fall into a local optimum, as they are optimized with a different evaluation metric that requires high accuracy on both detection and recognition. Thus, when the model produces missed detections or incorrectly spotted words, the target text instance becomes unsearchable. Figure 1: Result examples of retrieving scene text instances. The target text instances cover various types of instances, such as text-line instances (a), continuous partial patches (b), and non-continuous partial patches (c). The word in blue is the English translation corresponding to the Chinese word in black. This paper proposes a novel method for partial scene text retrieval that is capable of retrieving both text-line instances, as shown in Fig. 1 (a), and their partial patches, as shown in Fig. 1 (b) and (c). We refer to the tasks of retrieving text-line instances and partial patches as Text-line Instance Retrieval (TIR) and Partial Patches Retrieval (PPR), respectively. Our approach is based on the idea of embedding query text and scene text instances into a shared feature space and measuring their cross-modal similarities in this space. To achieve this, we train the model to learn the similarities between query text and scene text instances using their normalized edit distances, which are commonly used in string matching. However, while training data includes precise annotations of text-line instances, it lacks annotations for partial patches, which makes it difficult to detect them and measure their similarity. Therefore, the supervised learning strategy used for the TIR task cannot be directly applied to the PPR task. Despite the lack of translation labels for partial patches, it is feasible to determine whether a patch translated as a specific query text is located within a text-line instance by checking whether the translation of the text-line instance contains the query text. This enables us to measure similarities between query text and partial patches using a Multiple Instance Learning (MIL) method. Specifically, we follow the approach proposed in [12] and sample partial patches from a text-line instance to form a bag, where the bag label indicates whether the query text exists within the bag. Similarities between partial patches and query text are learned from the constructed bag data. Unlike general objects such as persons and cars, scene text is a kind of sequence-like object. An image patch of a general object has its discriminative features, but the ones of a scene text usually associate with another word. This makes it challenging to precisely determine whether the bag contains the target instance based on its label, resulting in noisy data that would significantly hurt the accuracy of the model. However, for the query text included in a text-line instance, we observe that the bag always contains a patch that is more similar to the query text than this text-line instance. Therefore, we propose a Ranking MIL (RankMIL) approach, which ensures that the similarity between the query text and the patch in the bag is greater than the similarity between the query text and the text-line instance. In the inference stage, for the PPR task, MIL method constructs a bag from each text-line instance and searches for the target instance by calculating the pairwise similarities between features of query text and all patches within the bag. However, each bag needs many patch111more than 50 patches on average for each text-line instance on the ReCTS dataset. to guarantee that at least one patch can cover the target instance, making the search process time-consuming. To overcome this challenge, we propose an approach called the Dynamic Partial Match Algorithm (DPMA), which eliminates the need to construct a bag. Instead, it directly obtains the feature of the target instance from the feature of the text-line instance. Specifically, the DPMA represents query text and text-line instances as sequential features, consisting of multiple local features. It then selects a set of local features of the text-line instance to form a new feature by a dynamic programming algorithm. The selection strategy of this algorithm is to maximize the similarity between the feature of the query text and the selected feature. Since the selected local features are not necessarily spatially adjacent, the DPMA can handle both continuous partial patches (as in Fig. 1 (b)) and non-continuous partial patches (as in Fig. 1 (c)). As a result, the DPMA not only improves inference efficiency but also enhances the performance of the PPR task. This paper is an extension of its conference version [13]. It broadens the application scope of the conference version to the PPR task, achieved from two aspects. Firstly, the RankMIL training strategy is presented for better optimizing the model to reduce accuracy loss brought by the absence of partial patch annotations. Secondly, we propose the DPMA that dynamically searches for partial patches from a text-line instance. This algorithm significantly improves search efficiency and PPR performance. The contributions in this work are four-fold. 1. We first introduce a novel end-to-end trainable network for learning cross-modal similarities between query text and scene text instances. This network allows for efficient searching of text instances containing a query text from natural images in both Latin and non-Latin scripts. 2. We, for the first time, discuss the partial scene text retrieval and optimize it by a MIL method. Moreover, we present the RankMIL algorithm for better optimization, significantly improving the performance of the PPR task. Experimental results verify its superiority to the conventional MIL. 3. The proposed DPMA enhances search efficiency and can handle not only continuous partial patches but also non-continuous ones without requiring additional data or model parameters. 4. We collect and annotate a new dataset CSVTRv2 for Chinese scene text retrieval, consisting of 53 pre-defined query words and 3400 Chinese scene text images. This dataset is adopted to verify the effectiveness of text retrieval methods over non-Latin scripts. Figure 2: The training phase of our proposed framework. Given an image, text-line proposals are detected, and a bag is constructed within text-line instances.The features of text-line proposals are extracted to the cross-modal similarity learning for the TIR task. Meanwhile, features of instances within the bag are extracted to ranking multiple instance learning for the PPR task. For visualization simplification, we only show the training process of one text-line instance."
https://arxiv.org/html/2411.10257v1,The Unreasonable Effectiveness of Guidance for Diffusion Models,"Guidance is an error-correcting technique used to improve the perceptual quality of images generated by diffusion models. Typically, the correction is achieved by linear extrapolation, using an auxiliary diffusion model that has lower performance than the primary model. Using a 2D toy example, we show that it is highly beneficial when the auxiliary model exhibits similar errors as the primary one but stronger. We verify this finding in higher dimensions, where we show that competitive generative performance to state-of-the-art guidance methods can be achieved when the auxiliary model differs from the primary one only by having stronger weight regularization. As an independent contribution, we investigate whether upweighting long-range spatial dependencies improves visual fidelity. The result is a novel guidance method, which we call sliding window guidance (SWG), that guides the primary model with itself by constraining its receptive field. Intriguingly, SWG aligns better with human preferences than state-of-the-art guidance methods while requiring neither training, architectural modifications, nor class conditioning. The code will be released.","Diffusion models (DMs) have emerged as a powerful approach for generative tasks, achieving remarkable success in areas such as image synthesis and text-to-image generation [43, 17, 24, 44, 27, 40, 1]. DMs are a class of generative models that iteratively transform noise samples into samples that are close to a desired data distribution. Despite their success, DMs often fail to generate high-quality samples in the visual domain [3] and require guidance techniques to improve visual fidelity (Fig. 1). The current most popular method, classifier-free guidance (CFG), improves image quality by increasing the probability that an image belongs to a certain class label [25]. Unlike its predecessor, classifier guidance [12], which relies on training an external classifier on labeled noisy images, CFG combines conditional and unconditional denoisers, which can be trained jointly [16]. In the following, we denote by \bm{x} a noisy image and by \bm{\epsilon}(\bm{x},t;c) and \bm{\epsilon}(\bm{x},t) the class conditional and unconditional noise predictors at timestep t of the denoising process [12]. CFG linearly combines noise predictions during sampling using the extrapolation scheme Figure 1: Left: Even state-of-the-art diffusion models can fail to generate globally coherent images without guidance. Right: Sliding window guidance (SWG) upweights long-range dependencies and thereby improves global coherence on average. \tilde{\bm{\epsilon}}(\bm{x},t;c)=\bm{\epsilon}(\bm{x},t;c)+w[\bm{\epsilon}(% \bm{x},t;c)-\bm{\epsilon}(\bm{x},t)], (1) with guidance weight w>0. CFG can be viewed as an error-correcting method [44, 7]. Equivalent extrapolation schemes can be found for all diffusion model formulations, such as target prediction [24] or flow matching [41]. Despite the widespread use of CFG in conditional synthesis [36], it comes with notable limitations. First, it increases the training budget: when trained jointly, the unconditional task can consume up to 20% of the computational cost [16]. Additionally, while CFG reduces class mismatch between samples and condition c of the noise predictor [40], this benefit comes at the expense of sample diversity, as this sampling method focuses on regions with high class probability [25]. Since guidance is a linear extrapolation scheme, the sampling trajectory can overshoot the desired distribution, leading to highly simplified images [30]. Finally, CFG sampling is restricted to class conditional generation by design. Recently, a new class of guidance methods has been developed to address some of the limitations of CFG. These methods utilize the extrapolation scheme of Eq. 1 in a more generic way \tilde{\bm{\epsilon}}(\bm{x},t)=\bm{\epsilon}_{\text{pos}}(\bm{x},t)+w[\bm{% \epsilon}_{\text{pos}}(\bm{x},t)-\bm{\epsilon}_{\text{neg}}(\bm{x},t)], (2) where the subscripts pos for positive and neg for negative simply refer to the sign of the noise predictors. We consider \bm{\epsilon}_{\text{pos}} to be a trained, well-performing DM. The idea behind Eq. (2) is to extrapolate into high likelihood regions by designing a negative model that has an inferior performance compared to \bm{\epsilon}_{\text{pos}}. Typically, \bm{\epsilon}_{\text{neg}} is derived from \bm{\epsilon}_{\text{pos}} by re-training using fewer parameters, architecture-based heuristic manipulations, or shorter training times [2, 25]. We refer to this class of guidance methods as weak model guidance (WMG). While WMG methods seem promising, they often require training additional models, heavy model-specific hyperparameter tuning at sampling time [25], or manual selection of specific layers to impair [2]. In this paper, we first introduce a 2D toy example to show that (i) WMG samples closer to high likelihood regions than CFG and (ii) guidance works best if \bm{\epsilon}_{\text{neg}} makes similar errors as \bm{\epsilon}_{\text{pos}} but stronger. Assuming that the generative error of \bm{\epsilon}_{\text{pos}} can be primarily attributed to scaling factors, such as training time and number of parameters [22], further constraining these factors allows us to construct \bm{\epsilon}_{\text{neg}} in a principled way. To this end, we show that constructing \bm{\epsilon}_{\text{neg}} by re-training or fine-tuning \bm{\epsilon}_{\text{pos}} under increased weight regularization results in competitive performance compared to state-of-the-art guidance methods. Additionally, to improve perceptual image quality, we introduce sliding window guidance (SWG), a novel guidance method designed to upweight long-range dependencies (Fig. 1). In contrast to existing guidance techniques [40, 16, 2, 25, 38, 30, 47], SWG requires neither training, architectural modifications nor class conditioning and can be applied to any DM that can process multiple image resolutions. Finally, SWG achieves competitive generative performance while aligning better with human preferences than state-of-the-art techniques. (a) w=0 (b) w=w^{*} (c) w=3w^{*} (d) Guidance triangle Figure 2: Inference trajectories for our 2D-toy model. (a) w=0 yields the trajectories of the positive model. (b) w^{*} denotes the guidance weight that leads to best (CFG) or onset of saturating (WMG) performance (w^{*}=1 for CFG and w^{*}=5 for WMG). (c) Extreme guidance weights lead to even smaller endpoint errors, but unstable trajectories for WMG and lead CFG to diverge from the data distribution. (d) Geometry of guidance updates for WMG. The linear guidance correction pushes WMG closer to the data point but pushes CFG further away, even though the conditional model correctly predicts the direction toward the data point."
https://arxiv.org/html/2411.10252v1,Visual-Linguistic Agent: Towards Collaborative Contextual Object Reasoning,"Multimodal Large Language Models (MLLMs) excel at descriptive tasks within images but often struggle with precise object localization, a critical element for reliable visual interpretation. In contrast, traditional object detection models provide high localization accuracy but frequently generate detections lacking contextual coherence due to limited modeling of inter-object relationships. To address this fundamental limitation, we introduce the Visual-Linguistic Agent (VLA), a collaborative framework that combines the relational reasoning strengths of MLLMs with the precise localization capabilities of traditional object detectors. In the VLA paradigm, the MLLM serves as a central Linguistic Agent, working collaboratively with specialized Vision Agents for object detection and classification. The Linguistic Agent evaluates and refines detections by reasoning over spatial and contextual relationships among objects, while the classification Vision Agent offers corrective feedback to improve classification accuracy. This collaborative approach enables VLA to significantly enhance both spatial reasoning and object localization, addressing key challenges in multimodal understanding. Extensive evaluations on the COCO dataset demonstrate substantial performance improvements across multiple detection models, highlighting VLA’s potential to set a new benchmark in accurate and contextually coherent object detection.","Object detection models have achieved remarkable success, particularly on large-scale datasets like COCO [12]. However, most existing object detection networks [2, 28, 6, 27] primarily focus on generating high-quality proposals, employing separate branches to predict the class and bounding box for each proposal. While these models are effective in localizing objects with precision, they often overlook spatial and logical relationships between objects, resulting in detections that may lack contextual coherence, as shown in Figure LABEL:FIG:att. This gap highlights the need to incorporate spatial and logical relationship modeling to enhance the reasoning and overall accuracy of detection results. Despite its importance, this aspect has received relatively limited attention in the field. Several studies [14, 29, 25] have attempted to address this limitation by modeling spatial and logical relationships between objects using Graph Convolutional Networks (GCNs) [15] or Gated Recurrent Units (GRUs) [3, 32] applied to the visual representations of detected bounding boxes. These approaches primarily improve the bounding box regression branch by explicitly modeling relationships among objects. However, they are often constrained to relationships between local objects, neglecting the broader contextual environment and overall spatial structure among objects. Furthermore, as these methods typically operate independently of the classification branch, they provide limited improvements in the coherence and reasoning of final detection outcomes. In contrast, multimodal large language models (MLLMs) [10, 13, 11, 21, 31, 4] have demonstrated exceptional capabilities in modeling relationships across modalities, such as in image captioning [17] and visual question answering [1]. MLLMs are proficient at constructing both spatial and logical relationships between objects, offering a more holistic understanding of visual scenes. However, despite their strong reasoning abilities, MLLMs struggle with precise object localization. Our motivation arises from the observation that combining the reasoning strengths of MLLMs with the robust localization capabilities of traditional object detection models can enhance both the accuracy and contextual coherence of object detection results. In this paper, we propose the Visual-Linguistic-Agent (VLA), a collaborative framework that leverages the reasoning and relationship-modeling capabilities of MLLMs to enhance the contextual accuracy of traditional object detection models. In the VLA paradigm, the MLLM, which acts as a Linguistic Agent, collaborates with both an object detection Vision Agent and a classification Vision Agent. As illustrated in Figure LABEL:FIG:att, upon receiving a user request, the MLLM generates an image caption and evaluates detection outputs provided as text prompts from the Vision Agent, subsequently correcting false detections using the classification Vision Agent. This collaborative interaction effectively combines the MLLM’s reasoning capabilities with the object localization strengths of the Vision Agents, resulting in detection outcomes that are more accurate and contextually coherent. In summary, our contributions are as follows: 1. We propose the Visual-Linguistic-Agent (VLA), a novel collaborative framework that utilizes the reasoning and relationship-modeling capabilities of MLLMs to enhance the contextual coherence of object detection. 2. Within VLA, the MLLM operates as a Linguistic Agent, collaborating with both object detection and classification Vision Agents to filter out false detections by reasoning over spatial relationships and leveraging the classification agent for corrective feedback. This approach maximizes the MLLM’s reasoning capabilities while strengthening its localization accuracy, resulting in more contextually consistent detection outcomes. 3. Extensive experiments on the COCO dataset demonstrate that VLA achieves up to 3% improvement in AP50:95 over state-of-the-art object detection models, underscoring the benefits of integrating linguistic reasoning with visual understanding in object detection and establishing a new benchmark for multimodal capabilities in this field."
https://arxiv.org/html/2411.10251v1,Morpho-Aware Global Attention for Image Matting,"Vision Transformers (ViTs) and Convolutional Neural Networks (CNNs) both face challenges in image matting, especially in preserving fine structural details. ViTs, with their global receptive field in the self-attention mechanism, often lose local details, such as hair, while CNNs, limited by their local receptive field, require deeper layers to approximate global context—yet still struggle with retaining fine structures in deeper architectures. To address these limitations, we propose a novel Morpho-Aware Global Attention (MAGA) mechanism, specifically designed to better capture the morphology of fine structures. Specifically, MAGA first aligns the local shapes of fine structures using Tetris-like convolutional patterns, achieving optimal local correspondence while remaining sensitive to local morphology. This local morphology information serves as the query embeddings for MAGA, which are then projected onto the global structure of the key embeddings. This allows MAGA to emphasize local morphological details within a global context. Finally, by projecting onto the value embeddings, MAGA seamlessly integrates the emphasized local morphology into a coherent global structure. Through this approach, MAGA enables a stronger focus on local morphology while unifying these details into a cohesive whole, effectively preserving fine structural details. Extensive experiments demonstrate that our MAGA-based ViT significantly outperforms state-of-the-art methods across two benchmarks, achieving improvements of 4.3% on average in SAD and 39.5% in MSE.","Image matting is a foundational task in computer vision aimed at isolating foreground objects from their backgrounds by predicting an alpha matte for each object pixel, a process also known as alpha matting [25]. This technique is widely applied in areas such as image and video editing, digital human creation, and special effects [2]. Mathematically, a natural image I can be represented as a linear combination of the foreground F and background B using an alpha matte \alpha: I=\alpha F+(1-\alpha)B,\quad\alpha\in[0,1] (1) since F, B, and \alpha are unknown, determining \alpha poses an ill-posed problem. To mitigate this, many methods use a manually labeled trimap, which segments the image into three regions: foreground, background, and an uncertain area in between. Deep Image Matting (DIM) [24] marked a significant shift by introducing deep learning to image matting, combining high-level semantic information (such as object category and shape) extracted by CNNs with low-level appearance cues (texture and boundary details). This shift inspired increasingly sophisticated CNN architectures and, subsequently, the use of Vision Transformers (ViTs) to capture advanced semantics with global context. However, as shown in Figure LABEL:FIG:compare(a), CNNs’ inherent local receptive fields limit them to capturing only localized patterns. Increasing the network depth to approximate a global receptive field often leads to a loss of fine details, particularly in delicate structures. This restriction results in extracted patterns that lack coherence and logical consistency [20, 17, 27, 26, 18, 22]. In contrast, as shown in Figure LABEL:FIG:compare(b), ViTs provide a global receptive field, allowing them to capture overall structures but often at the expense of fine details. Consequently, ViTs struggle to capture complete boundaries of intricate structures, such as individual hairs in a cat’s fur [25, 2, 13, 19, 15]. Recent methods [2, 5] attempt to leverage both CNNs and ViTs, combining CNNs’ local pattern extraction with ViTs’ global structure capture through simple addition or hierarchical fusion. However, without specific guidance, these approaches often fail to effectively extract and integrate local structural information within a global context. Therefore, developing a method to accurately capture local structures from CNNs and integrate them within the global context provided by ViTs remains a critical yet underexplored challenge in image matting. In this paper, we introduce a novel Morpho-Aware Global Attention (MAGA) mechanism to address the challenges of preserving coherent morphology and fine structural details in image matting. The rationale behind our approach is that aligning local morphological features with global structural information enables a unified representation of fine structures within the global context. MAGA functions in the following steps: first, it extracts local morphological details from the global feature map by aligning fine structures with Tetris-like convolutional kernel shapes to generate query embeddings enriched with local structure information. These enriched query embeddings are then projected onto the global structure (key embeddings), allowing MAGA to contextualize local details within the overall image. Finally, the enhanced key embeddings are projected onto the value embeddings, seamlessly integrating fine structures into a coherent global morphology. In summary, our key contributions are threefold: • We propose the Morpho-Aware Global Attention (MAGA) mechanism for preserving the overall coherence and morphology of fine structures. • We introduce Tetris-like convolutional kernel shapes in MAGA that align with the local geometries of fine structures. This local morphology information serves as query embeddings, which are projected onto global key and value embeddings, allowing MAGA to enhance morphological awareness within the global feature map and seamlessly integrate local details into the global structure. • Extensive experiments on the Composition-1k and Distinctions-646 datasets show that MAGA outperforms state-of-the-art methods, achieving improvements of 6.4%/12.5% and 2.3%/66.6% in the SAD/MSE metrics, respectively."
https://arxiv.org/html/2411.10237v1,ScribbleVS: Scribble-Supervised Medical Image Segmentation via Dynamic Competitive Pseudo Label Selection,"In clinical medicine, precise image segmentation can provide substantial support to clinicians. However, achieving such precision often requires a large amount of finely annotated data, which can be costly. Scribble annotation presents a more efficient alternative, boosting labeling efficiency. However, utilizing such minimal supervision for medical image segmentation training, especially with scribble annotations, poses significant challenges. To address these challenges, we introduce ScribbleVS, a novel framework that leverages scribble annotations. We introduce a Regional Pseudo Labels Diffusion Module to expand the scope of supervision and reduce the impact of noise present in pseudo labels. Additionally, we propose a Dynamic Competitive Selection module for enhanced refinement in selecting pseudo labels. Experiments conducted on the ACDC and MSCMRseg datasets have demonstrated promising results, achieving performance levels that even exceed those of fully supervised methodologies. The codes of this study are available at https://github.com/ortonwang/ScribbleVS.","In recent years, deep neural networks have demonstrated their potential across diverse visual tasks [14], from object recognition to scene comprehension. They have also achieved notable successes in medical image segmentation [35][33]. Precise image segmentation within clinical medical practice offers clinicians crucial auxiliary data, facilitating swift and precise diagnostic decisions [1]. However, the success of these methods relies on comprehensive manual annotations, necessitating detailed and intensive labor. In medical imaging, annotating a single image can demand hours from an experienced physician, necessitating considerable expertise and resources [49] [28]. To address these issues, some approaches have integrated unlabeled data into model training, utilizing semi-supervised learning (SSL) [45][38][22][47]. Nonetheless, SSL still requires a subset of precisely annotated images, leading to substantial annotation efforts. To improve annotation efficiency, researchers have started exploring segmentation networks based on weak annotations [29], such as scribbles [17], bounding boxes [25], points [2], and image-level labels [26]. Several studies have investigated image-level labels as a basis for segmentation [6][37][36][41], yet these methods often rely on large-scale training datasets and may exhibit poor performance when applied to small medical image datasets. Figure 1: Examples of pixel level annotation and scribble annotations. BG, RV, Myo, LV, and UA represent the background, right ventricle, myocardium, left ventricle, and unannotated pixels respectively. In contrast, scribbles are suitable for annotating nested structures and are easily to obtain in practice, offering significantly higher annotation efficiency compared to dense manual annotation (as shown in the example in Figure 1). Some work has already demonstrated their potential in semantic and medical image segmentation [7][11]. Therefore, we propose to investigate this specific form of weakly supervised segmentation, which solely relies on scribble annotations for model training, making it more suitable for costly annotation of medical images. Figure 2: The schematic diagram of the proposed ScribbleVS framework, which comprises the RPD module and the DCS module. The areas in the \mathcal{PL} indicate regions to be disregarded when evaluating losses, and the EMA signifies that the teacher network implements parameter updates from the student network through the exponential moving average. Currently, Several studies have explored and utilized scribble annotation techniques in diverse contexts. Lin et al. [18] proposed a graph-based method to propagates information from scribbles to unannotated pixels and train the models jointly. Subsequently, Tang et al. [31] introduced Conditional Random Field (CRF) regularization loss into the training of segmentation networks. In the domain of medical imaging, Can et al. [4] proposed an iterative framework for model training with scribble annotations. Kim et al. [8] introduced a regularization function based on level sets[24] to train deep networks with weak annotations. Lee et al. [15] combined pseudo labeling and label filtering to generate reliable labels for network training with scribble annotations. Liu et al. [20] proposed a unified weakly supervised framework for training networks from scribble annotations, which consists of an uncertainty-aware mean teacher and a transformation-consistency strategy. Valvano et al. [34] introduced a scribble-based segmentation model using multi-scale Generative Adversarial Network (GAN) and attention gates mechanism, introducing a novel unpaired segmentation mask strategy that requires additional annotation masks for training. Simultaneously, Luo et al. [23] developed a dual-branch framework, employing dynamically mixed pseudo label supervision (DMPLS) for scribble-annotated medical image segmentation. Further, Cyclemix [43] was employed to generate blended images, enhancing the training goal with consistency losses to address inconsistent segmentations. Li et al. presented ScribbleVC [16], integrating scribble-based methods with segmentation networks and class embedding modules for enhanced segmentation masks. Although scribble annotations can reduce the necessity for extensive, expert-driven annotation efforts, their constrained supervisory signals could affect model precision. Furthermore, medical images often exhibit various quality defects that may adversely affect model performance. Yet, the critical accuracy requirements of medical imaging tasks underscore the need for further enhancement of algorithm accuracy. To this end, we introduce ScribbleVS, a novel framework tailored for medical image segmentation with scribble annotations. Overall, the main contributions of this paper are as follows: • We introduce ScribbleVS, a novel framework based on a mean teacher network for scribble-annotated training. • We launch a Regional Pseudo labels Diffusion Module to expand supervisory scope, enabling the use of pseudo labels while minimizing noise. • We propose a Dynamic Competition Selection Module to boost the model’s robustness by selectively using appropriate pseudo labels for training. • Experimental results on ACDC and MSCMRseg dataset demonstrate promising improvements over previous state-of-the-art methods, achieving segmentation precision surpassing that of fully-supervised models."
https://arxiv.org/html/2411.10232v1,ColorEdit: Training-free Image-Guided Color editing with diffusion model,"Text-to-image (T2I) diffusion models, with their impressive generative capabilities, have been adopted for image editing tasks, demonstrating remarkable efficacy. However, due to attention leakage and collision between the cross-attention map of the object and the new color attribute from the text prompt, text-guided image editing methods may fail to change the color of an object, resulting in a misalignment between the resulting image and the text prompt. In this paper, we conduct an in-depth analysis on the process of text-guided image synthesizing and what semantic information different cross-attention blocks have learned. We observe that the visual representation of an object is determined in the up-block of the diffusion model in the early stage of the denoising process, and color adjustment can be achieved through value matrices alignment in the cross-attention layer. Based on our findings, we propose a straightforward, yet stable, and effective image-guided method to modify the color of an object without requiring any additional fine-tuning or training. Lastly, we present a benchmark dataset called COLORBENCH, the first benchmark to evaluate the performance of color change methods. Extensive experiments validate the effectiveness of our method in object-level color editing and surpass the performance of popular text-guided image editing approaches in both synthesized and real images.","Figure 1: Multi-Object color editing. Each outcome image is changing the color of the hat first and then changing the color of the bowl, using the associated reference color image. Figure 2: Example of color change. Text-guided editing methods may fail to change the color of an object while maintaining the structure of it or the background information. Color is one of the most important visual perceptions for humans. The color of an object significantly influences the emotional responses and perceptions of a person toward it, which makes color a key element in both functional and aesthetic design decisions across industries, particularly in the design field. With the development of diffusion models, some studies [18, 47, 23] have applied the stable diffusion (SD) model to the colorization task, which involves adding color to grayscale or black-and-white images. However, currently, no studies specifically investigate the task of color change. Although some methods [13, 42, 24, 3, 10, 9] demonstrate the capability to modify an object’s color, those text-guided techniques often fail to change the color of an object as expected, as shown in Fig. 2. In this paper, we conduct an in-depth exploration on the cross-attention layer, which aligns and transforms text information into synthesized images. Specifically, we visualize the cross-attention maps of objects in different blocks of the model through different stages of the denoising process to elucidate how textual information directs the generation of images. We identify the semantic information captured by various cross-attention blocks and demonstrate when and where an object’s shape, contour, and texture are established. For the unsuccessful outcome of text-guided color editing, we argue that there are two main factors: (a) the imprecise distribution of color attribute attention weights on the spatial area, called cross-attention leakage. (b) The collision of information on attributes in the cross-attention map from the original object and the color term in the target prompt. We find that, compared to altering the Key matrices in the cross-attention layer, modifying the Value matrices of the target image results in a more stable color change effect. Based on our findings, we introduce a simplified yet stable and effective method called training-free Image-Guided Color Editing. Our method performs object color editing by aligning the Value matrices of the target image with the Value matrices extracted from a reference color image in specific cross-attention layers of the diffusion model in the early stages of the denoising process (see an example in Fig. 1). Our contributions are as follows: (1) We demonstrate that the shape, contour, and texture of an object are determined in the U-Net decoder in the early stage of the denoising process. (2) We propose a tuning-free image-guided method to edit the color of an object through Value matrices alignment in the cross-attention layer. (3) We introduce COLORBENCH, the first benchmark dataset to evaluate the color editing task. (4) Experimental results demonstrate that our Image-Guided Color Editing method surpasses current popular text-guided image editing methods on both synthesized and real images."
https://arxiv.org/html/2411.10231v1,"A Low-Resolution Image is Worth 1x1 Words: Enabling Fine Image
Super-Resolution with Transformers and TaylorShift","Transformer-based Super-Resolution (SR) models have recently advanced image reconstruction quality, yet challenges remain due to computational complexity and an over-reliance on large patch sizes, which constrain fine-grained detail enhancement. In this work, we propose TaylorIR to address these limitations by utilizing a patch size of 1x1, enabling pixel-level processing in any transformer-based SR model. To address the significant computational demands under the traditional self-attention mechanism, we employ the TaylorShift attention mechanism, a memory-efficient alternative based on Taylor series expansion, achieving full token-to-token interactions with linear complexity. Experimental results demonstrate that our approach achieves new state-of-the-art SR performance while reducing memory consumption by up to 60% compared to traditional self-attention-based transformers.","Image Super-Resolution (SR) remains a foundational yet significant low-vision challenge, aiming to reconstruct High-Resolution (HR) images from Low-Resolution (LR) inputs. The applications encapsulated by SR are broad, spanning security, medical imaging, and even astronomical analysis [20, 21]. Despite the powerful advances made with deep learning, limitations persist, especially regarding high-frequency detail enhancements [19, 13, 9]. With the introduction of deep learning, SR methods have leaned heavily on Convolutional Neural Networks (CNNs) [30, 32, 12], delivering impressive performance. Short after, transformer-based architectures have demonstrated an aptitude for capturing intricate relationships across input sequences, making them a dominant choice for regression-based image SR [25, 6]. Prominent transformer-based architectures are SwinIR [11], Restormer [27], and HAT [3], which have demonstrated promising gains, applying self-attention mechanisms for precise context-aware upscaling. Yet, transformer-based SR methods face notable challenges: high memory requirements and quadratic time complexity associated with self-attention, limiting practicality for real-time and large-scale applications. As a result, current methods reduce the contextual scope within which attention is operating, e.g., 8\times8 windows, and sometimes operate within these windows with patch-sizes greater than 1\times1, leading to non-pixel level detail enhancement. This restriction compromises the ability to capture fine-grained dependencies across the entire image. Figure 1: Overview of TaylorIR’s impact on low-vision applications like image SR. By embedding the input as 1\times1 patches, TaylorIR transforms inputs to long, pixel-level sequences, allowing fine-grained detail enhancement. In addition, it exploits the TaylorShift [22] attention mechanism, which significantly reduces memory consumption compared to classical self-attention, making it a more efficient solution for image SR. To address these issues, we introduce TaylorIR, a novel transformer-based SR approach that makes use of pixel-level detail refinement. Moreover, it substitutes traditional self-attention with TaylorShift [22] attention, a memory-efficient mechanism inspired by Taylor series expansion that approximates full token-to-token interactions with linear complexity. By enabling fine-grained attention on a per-pixel level, our approach significantly enhances context information while improving computational efficiency. For Swin-based SR models like SwinIR, we showcase TaylorSwinIR, a version of SwinIR adapted with TaylorIR to support 1\times1 patch embeddings and large window sizes, i.e., from 8\times8 with 64 tokens to 48\times48 with 2304 tokens. TaylorShift [22] enables efficient, global attention at a lower memory footprint, supporting broader context and enhanced detail without the resource strain of traditional attention mechanisms. Compared to the baseline SwinIR, applying TaylorIR achieves state-of-the-art performance while reducing memory consumption by up to 60%, as demonstrated in extensive experiments across standard benchmarks and exemplified in Figure 1. Our key contributions are as follows: • Pixel-Wise Patch Embedding: We adopt a 1\times1 patch size approach for transformer-based SR, allowing for per-pixel processing and sharper detail reconstruction. • Efficient Large-Window Self-Attention: By enabling extended windows in SwinTransformer-based SR models, our employed TaylorShift [22] attention improves SR quality with reduced memory and computational costs associated with standard window attention. • Improving State-of-the-Art SR models: TaylorSwinIR outperforms current SR models in both PSNR and SSIM across multiple benchmark datasets, establishing a new efficiency-performance balance."
https://arxiv.org/html/2411.10224v1,MCL: Multi-view Enhanced Contrastive Learning for Chest X-ray Report Generation,"Radiology reports are crucial for planning treatment strategies and enhancing doctor-patient communication, yet manually writing these reports is burdensome for radiologists. While automatic report generation offers a solution, existing methods often rely on single-view radiographs, limiting diagnostic accuracy. To address this problem, we propose MCL, a Multi-view enhanced Contrastive Learning method for chest X-ray report generation. Specifically, we first introduce multi-view enhanced contrastive learning for visual representation by maximizing agreements between multi-view radiographs and their corresponding report. Subsequently, to fully exploit patient-specific indications (e.g., patient’s symptoms) for report generation, we add a transitional “bridge” for missing indications to reduce embedding space discrepancies caused by their presence or absence. Additionally, we construct Multi-view CXR and Two-view CXR datasets from public sources to support research on multi-view report generation. Our proposed MCL surpasses recent state-of-the-art methods across multiple datasets, achieving a 5.0% F1 RadGraph improvement on MIMIC-CXR, a 7.3% BLEU-1 improvement on MIMIC-ABN, a 3.1% BLEU-4 improvement on Multi-view CXR, and an 8.2% F1,mic-14 CheXbert improvement on Two-view CXR.","Radiology reports, crafted by experienced radiologists, meticulously document imaging findings from examinations such as X-rays, PET scans, and CTs, detailing abnormalities and initial diagnostic conclusions. These reports deliver vital imaging insights that enable physicians to develop efficient patient treatment strategies (Messina et al. 2022). However, the manual writing process is time-consuming and requires significant expertise, making it increasingly difficult to meet the demands of modern healthcare (Bannur et al. 2024), particularly in regions with limited medical resources. Figure 1: Comparison of existing methods and our proposed method for chest X-ray report generation. Existing methods generate reports from single-view radiographs, while our approach utilizes multi-view radiographs. Automatic chest X-ray report generation aims to produce detailed and accurate free-text reports from multi-view radiographs, helping radiologists improve diagnostic efficiency and consistency by providing high-quality draft reports. In clinical practice, limitations like X-ray equipment constraints and the complexity of human anatomical structures can prevent a single-view radiograph from achieving optimal imaging quality and adequately displaying the overall anatomical structure. As a result, multi-view imaging examinations, such as postero-anterior (PA), antero-posterior (AP), and lateral views, are crucial for accurate diagnostics and personalized treatment. Consequently, the number of radiographs varies across studies. Typically, each study comprises a collection of radiographs, a corresponding report, and a patient-specific indication (which may sometimes be absent). This variability is also evident in public chest X-ray report generation datasets such as MIMIC-CXR (Johnson et al. 2019) and IU X-ray (Demner-Fushman et al. 2016). Such variability makes it challenging to effectively utilize multi-view radiographs from the same study to enhance the clinical accuracy of generated reports. One intuitive approach (Li et al. 2019; Chen et al. 2020; Liu et al. 2024c, b) treats each radiograph as an individual study, generating reports from single-view radiographs (see Figure 1). However, this scheme fails to fully exploit the rich anatomical information available in multi-view radiographs, potentially leading to inaccurate and inconsistent reports. In addition, since the IU X-ray dataset predominantly includes studies with two-view radiographs, several studies (Chen et al. 2020, 2021; Yang et al. 2023) have developed two-view report generation approaches, showing promise in producing informative reports. Despite this, these methods often struggle to integrate into clinical workflows due to the varying number of radiographs per study. To address this challenge, we introduce a novel two-stage method, called Multi-view enhanced Contrastive Learning (MCL) for generating chest X-ray reports. In the representation learning stage, we propose multi-view enhanced contrastive learning for visual representation by leveraging semantic correspondences between multi-view radiographs within the same study and between these radiographs and their corresponding report. In the report generation stage, we utilize the cross-attention mechanism to exploit available indications fully, providing the model with patient background information. Additionally, we incorporate a transitional “bridge” for missing indications to reduce embedding space differences caused by the presence or absence of these indications. For multi-view enhanced contrastive learning, we first employ multi-positive contrastive learning to bring multi-view radiographs within the same study closer, improving the consistency of visual features. Then, we develop a multi-view fusion module to integrate varying numbers of radiographs per study, producing fused visual features for subsequent cross-modal alignment. Finally, we apply contrastive learning for instance-wise and token-wise alignment, maximizing agreement between multi-view radiographs and their corresponding report. We evaluate our proposed MCL on the MIMIC-CXR (Johnson et al. 2019), MIMIC-ABN (Ni et al. 2020; Hou et al. 2023a), and our curated Multi-view CXR and Two-view CXR datasets. Experiment results demonstrate the effectiveness of MCL, showing a 5.0% F1 RadGraph improvement on MIMIC-CXR, a 7.3% BLEU-1 improvement on MIMIC-ABN, a 3.1% BLEU-4 improvement on Multi-view CXR, and an 8.2% F1,mic-14 CheXbert improvement on Two-view CXR. Our key contributions are outlined as follows: • We propose a novel multi-view enhanced contrastive learning that facilitates cross-modal alignment between multi-view radiographs and their corresponding report, addressing the issue that existing algorithms cannot handle varying numbers of views. • We incorporate a transitional “bridge” for missing indications to reduce embedding space differences caused by their presence or absence, thereby enhancing the capture of patient background information. • We curate Multi-view CXR and Two-view CXR datasets from two public sources, ensuring that each study includes multiple radiographs. This supports research on multi-view report generation, particularly for scenarios involving varying numbers of views or two-view setups. Figure 2: Illustration of our proposed MCL, which comprises a visual encoder, a text encoder, and a text generator. MCL is a two-stage method: 1) Representation learning using multi-view enhanced contrastive learning. 2) Report generation based on patient-specific indications. Model inference solely involves stage 2, excluding the language modeling (LM) loss."
https://arxiv.org/html/2411.10203v1,Learning Generalizable 3D Manipulation With 10 Demonstrations,"Learning robust and generalizable manipulation skills from demonstrations remains a key challenge in robotics, with broad applications in industrial automation and service robotics. While recent imitation learning methods have achieved impressive results, they often require large amounts of demonstration data and struggle to generalize across different spatial variants. In this work, we present a novel framework that learns manipulation skills from as few as 10 demonstrations, yet still generalizes to spatial variants such as different initial object positions and camera viewpoints. Our framework consists of two key modules: Semantic Guided Perception (SGP), which constructs task-focused, spatially aware 3D point cloud representations from RGB-D inputs; and Spatial Generalized Decision (SGD), an efficient diffusion-based decision-making module that generates actions via denoising. To effectively learn generalization ability from limited data, we introduce a critical spatially equivariant training strategy that captures the spatial knowledge embedded in expert demonstrations. We validate our framework through extensive experiments on both simulation benchmarks and real-world robotic systems. Our method demonstrates a 60–70% improvement in success rates over state-of-the-art approaches on a series of challenging tasks, even with substantial variations in object poses and camera viewpoints. This work shows significant potential for advancing efficient, generalizable manipulation skill learning in real-world applications.111 https://github.com/renyu2016/Generalized-3D-Manipulation","I INTRODUCTION Learning robust and generalizable manipulation skills from demonstrations[1, 2, 3, 4] is a longstanding goal in robotics research, with broad applications in various aspects of human life, such as industrial robot assembly [5, 6] and service robots performing housework [7, 3]. Recently, several imitation learning methods[8, 9, 10] have shown remarkable performance in learning manipulation skills. However, these methods still suffer from the need for large amounts of demonstration data and exhibit limited generalization ability. Recent approaches combining 3D representations with Diffusion Policy [11, 10, 12] have shown potential performance in learning manipulation tasks from limited demonstrations, generalizing to different visual appearances, instance geometries, and camera viewpoints. However, these methods tend to overfit specific training trajectories rather than capturing the spatial relationships[13, 14, 15, 16, 17] needed for generalization. This limitation leads to poor performance in tasks with varying initial object and target positions, or when the starting pose of a manipulated object significantly deviates from the scenes contained in training data, i.e., these methods lack the 3D generalization ability. Figure 1: In a), we report the average success rates of the two methods on a series of challenging tasks. In b), we show the average success rates of the two methods under different camera viewpoint settings. In c), we progressively expand the random initialization region where the manipulated object is located at the start of the tasks. Compared to the state-of-the-art 3D manipulation learning method DP3, our framework demonstrates significant improvements in success rates and generalization ability. To efficiently learn manipulation skills from a few demonstrations while ensuring 3D generalization, we propose a framework that leverages the spatial information in demonstrations to achieve 3D-generalized manipulation skill learning. Our framework consists of two key components: Semantic Guided Perception (SGP) and Spatial Generalized Decesion (SGD). SGP constructs a 3D representation from RGB-D image pairs that is both task-focused and spatially aware. SGD is an efficient decision-making module based on diffusion policy[18, 19, 20, 21, 22]. To enable SGD to generalize across different 3D spatial variations, we introduce a spatially equivariant training strategy that fully explores the spatial knowledge embedded in expert trajectories. To evaluate our framework, we conducted extensive experiments on both simulation benchmarks and real-world hardware system. Using the same task settings as current state-of-the-art methods, our approach achieved a 60-70% improvement in success rates on a series of challenging simulation tasks with only 10 demonstrations. Additionally, we designed comprehensive experiments to demonstrate the strong generalization ability of our framework in handling spatial variations and viewpoint changes. Our contributions are summarized as follows: • We propose a framework that learns 3D generalized manipulation skills with only 10 demonstrations. Our framework generalizes to varying object initial poses and camera viewpoints. • We develop an easy to use yet highly effective training strategy for manipulation policies, enabling the exploration of spatial knowledge embedded in demonstration trajectories. This training strategy is easily integrated with any frameworks that use point cloud as input for decision policy learning. • We validate our framework through extensive simulation and real-world experiments, achieving over a 60% improvement on a series of challenging tasks compared to state-of-the-art methods, demonstrating its strong effectiveness."
https://arxiv.org/html/2411.10200v1,Block based Adaptive Compressive Sensing with Sampling Rate Control,"Compressive sensing (CS), acquiring and reconstructing signals below the Nyquist rate, has great potential in image and video acquisition to exploit data redundancy and greatly reduce the amount of sampled data. To further reduce the sampled data while keeping the video quality, this paper explores the temporal redundancy in video CS and proposes a block based adaptive compressive sensing framework with a sampling rate (SR) control strategy. To avoid redundant compression of non-moving regions, we first incorporate moving block detection between consecutive frames, and only transmit the measurements of moving blocks. The non-moving regions are reconstructed from the previous frame. In addition, we propose a block storage system and a dynamic threshold to achieve adaptive SR allocation to each frame based on the area of moving regions and target SR for controlling the average SR within the target SR. Finally, to reduce blocking artifacts and improve reconstruction quality, we adopt a cooperative reconstruction of the moving and non-moving blocks by referring to the measurements of the non-moving blocks from the previous frame. Extensive experiments have demonstrated that this work is able to control SR and obtain better performance than existing works.","Compressive Sensing (CS) (Donoho, 2006) is a novel sampling and reconstruction method that can recover the original signal from fewer measurements than the Nyquist sampling theorem if the original signal is sparse. Mathematically, given an original signal x\in\mathbb{R}^{N} , the measurements y\in\mathbb{R}^{M} is obtained as follows: (1) y=\Phi x where \Phi\in\mathbb{R}^{M\times N} is the sampling matrix, and the sampling rate (SR) is defined as M/N(M\ll N). Due to its potential to greatly enhance the energy efficiency of sensors, it is expected to be applied in various fields such as single-pixel imaging (Duarte et al., 2008), magnetic resonance imaging (MRI) (Lustig et al., 2007, 2008), and image/video source coding (Mun and Fowler, 2012). The primary challenge of CS is recovering x from the obtained y, which is an uncertain inverse problem to solve. Many traditional model-driven methods focus on using structural prior distributions with theoretical guarantees, such as sparse representation (Elad, 2010), low-rank (Dong et al., 2014). However, they are computationally expensive and slow, which has presented challenges for practical applications. Recently, CS performance has been dramatically improved by incorporating a deep convolutional neural network (CNN). Learning a nonlinear mapping from measurements to the reconstruction image, (Kulkarni et al., 2016; Shi et al., 2019) improve the recovery accuracy and speed. Also, by incorporating neural networks into the optimization algorithm problem, (Zhang and Ghanem, 2018; Zhang et al., 2020; Chen et al., 2022) further improved the reconstruction quality. Considering human visual attention, (Yu et al., 2010; Zhou et al., 2020; Chen and Zhang, 2022) applied a saliency detection algorithm to achieve adaptive SR assignment to each block. Although these methods have achieved excellent results for image CS, video CS is still a challenging problem since it requires consideration of both intra-frame and inter-frame correlations. In addition, video CS is mainly used to capture a scene from a fixed position in real applications. These scenes have many non-moving regions, such as the background. However, many existing methods don’t consider this characteristic, leading to redundant compression. Inspired by this background, the author of (Du et al., 2021) proposed a method to identify the region of interest (ROI) from the difference between the first and subsequent frames and compress only the ROI with high SR. But, this method cannot flexibly adapt to changes in the background since the first frame is considered the background. Then, VCSL (Yang et al., 2023) introduced the reference frame renewal method into a similar framework and made it possible to adapt to changes in the background. However, these methods still have three disadvantages: (1) the average SR is not controllable since the SR of each frame depends on the area of the ROI; (2) Correlations between consecutive frames are sometimes disregarded.; (3) The detection accuracy of the moving regions is low. To solve these problems, we propose a novel video CS method incorporating moving block detection and a SR controller. The proposed method utilizes moving block detection between consecutive frames to cope with background changes and compresses videos more efficiently by reducing the measurements of non-moving blocks. It also introduce a block storage system and dynamic threshold to enable control of the average SR and adaptive SR allocation to each frame based on the area of moving regions and target SR. Furthermore, we adopt a cooperative reconstruction to reduce blocking artifacts and improve reconstruction quality instead of reconstructing the moving and non-moving blocks separately and combining them. The main contributions of this paper are as follows: • We propose a novel and effective video CS method. By incorporating moving block detection between consecutive frames, it avoids redundant compression of non-moving regions and achieves efficient compression. • In order to control the average SR, we propose a block storage system which provides adaptive SR allocation to each frame based on the area of moving regions and target SR. Furthermore, to balance the reconstruction quality and the control of the average SR, we introduce a dynamic threshold method to adjust the threshold appropriately for each frame. • To reduce blocking artifacts and improve reconstruction quality, we adopt a cooperative reconstruction of the non-moving and moving regions. • Experiments on the VIRAT dataset (Oh et al., 2011) demonstrate that our method outperforms state-of-the-art methods in terms of control of the average SR and reconstruction quality."
https://arxiv.org/html/2411.10193v1,"DiMoDif:
scourse dality-information ferentiation
for Audio-visual Deepfake Detection and Localization","Deepfake technology has rapidly advanced, posing significant threats to information integrity and societal trust. While significant progress has been made in detecting deepfakes, the simultaneous manipulation of audio and visual modalities, sometimes at small parts but still altering the meaning, presents a more challenging detection scenario. We present a novel audio-visual deepfake detection framework that leverages the inter-modality differences in machine perception of speech, based on the assumption that in real samples – in contrast to deepfakes – visual and audio signals coincide in terms of information. Our framework leverages features from deep networks that specialize in video and audio speech recognition to spot frame-level cross-modal incongruities, and in that way to temporally localize the deepfake forgery. To this end, DiMoDif employs a Transformer encoder-based architecture with a feature pyramid scheme and local attention, and optimizes the detection model through a composite loss function accounting for frame-level detections and fake intervals localization. DiMoDif outperforms the state-of-the-art on the Temporal Forgery Localization task by +47.88% AP@0.75 on AV-Deepfake1M, and performs on-par on LAV-DF. On the Deepfake Detection task, it outperforms the state-of-the-art by +30.5% AUC on AV-Deepfake1M, +2.8% AUC on FakeAVCeleb, and performs on-par on LAV-DF. Code available at https://github.com/mever-team/dimodif.","Audio-visual deepfakes are AI-generated content involving manipulations to one or both modalities, sometimes at small time intervals, with the intention to deceive [45]. Deepfake content can widely spread online, and mislead viewers, contributing to the growing issue of information disorder. A common feature of audio-visual deepfakes is the presence of incongruities between the visual and audio signals, which are, however, increasingly hard to detect manually. This necessitates the development of robust AI-based deepfake detection methods. Deepfake detection is a growing area of research [54, 51, 46]. Existing approaches primarily focus on pixel-level analysis, which examines inconsistencies in gradient variations [65], color variations [30], or artifacts introduced during the generation process [11]. Also, feature-based methods analyze facial landmarks [72], temporal coherence [27], and other visual cues to identify inconsistencies in deepfake content [40]. Furthermore, there has been growing interest in detecting and localizing deepfakes in audio-visual content, with both tasks being particularly challenging due to the complex interplay between the two modalities. Key approaches include audio-visual synchronization that analyzes the consistency between visual and auditory cues namely face movements and speech [26, 9, 14], feature reconstruction learning with cross-reconstruction similarity estimation to detect anomalies prevalent in fake videos [76], and self-supervised approaches relying on the synchronization patterns learned from real videos only [28, 18]. Humans integrate multimodal sensory information, such as visual and auditory input, to extract meaningful features and perform a wide range of recognition tasks. Cognitive neuroscience research has extensively documented the interplay between video and audio [63, 64], often resulting in the alteration of perceptual content [70, 13] or even the induction of perceptual illusions [52, 60]. In the context of speech perception, the brain actively constructs content representations by combining visual, auditory, and contextual cues to predict ongoing and future utterances [12]. When visual information conflicts with auditory input, as in the McGurk effect [49] or in the case of deepfakes, increased prediction errors are observed, accompanied by significant changes in brain activity, which often manifest as higher-frequency neural oscillations and localized activation patterns, indicative of heightened cognitive effort [4]. (a) Example of speech recognition differences between lip-reading and speech-to-text models. (b) Aggr. FakeAVCeleb [34]. (c) Aggr. LAV-DF [10]. (d) Aggr. AV-Deepfake1M [8]. Figure 1: The process of identifying audio-visual inconsistency for deepfake detection. In (a), a video’s visual and audio components are separately processed by lip-reading and speech-to-text models, then a difference score is calculated. In (b,c,d) the difference score distributions are illustrated in the form of violin plots for three different datasets. We illustrate how state-of-the-art AI analysis tools could emulate this process in Figure 1. In Figure 1(a), an input video is first decomposed into its constituent visual and audio streams. These are then fed into state-of-the-art models for lip-reading and speech-recognition [44], and the resulting textual outputs are compared111Python’s \mathtt{difflib} is used (cf. https://docs.python.org/3/library/difflib.html).. Their difference is computed using the normalized Levenshtein distance, i.e. the ratio between the edit distance \mathtt{delta} of the two sentences and the sum of their lengths, which results in scores between 0% (identical) and 100% (completely different). Figures 1(b), 1(c) and 1(d) illustrate aggregate results through violin plots computed on the evaluation sets of three popular audio-visual deepfake detection benchmarks, FakeAVCeleb [34], LAV-DF [10], and AV-Deepfake1M [8]. On average, lower difference is observed in real videos compared to fake ones across all datasets, indicating the suitability of this score as a measure of incongruity between audio and video. Even though these differences are significant on FakeAVCeleb, they are smaller on LAV-DF, and much smaller on AV-Deepfake1M, which render naive thresholding on this score unsuitable for robust detection. This is due to the fact that FakeAVCeleb’s videos are fully manipulated, while LAV-DF’s samples are manipulated only in parts, while samples from AV-Deepfake1M are manipulated in even smaller parts (on average half the length of those in LAV-DF). Motivated by the above, we propose Discourse Modality-information Differentiation (DiMoDif), a deep learning-based deepfake detection architecture that exploits the inter-modality differences in machine perception of speech. DiMoDif first decomposes the input video into its visual and audio streams, and extracts the respective video and audio representations based on state-of-the-art lip reading and speech-to-text pre-trained models [44]. At its core, DiMoDif employs a Transformer encoder architecture [66] with a feature pyramid scheme and local attention configuration, to spot frame-level cross-modal incongruities between the two modality-specific representations, and in that way detect and localize the deepfake forgeries. Note that low difference scores frequently occur in partly manipulated fake videos (e.g. Figure 1(a)); however, DiMoDif manages to classify them as fake by capturing audio-visual divergence at the frame level. In addition, high difference scores occasionally occur in real videos; however, DiMoDif leverages alternative features, e.g., hard phonemes [6], to classify them as real (cf. Section 5.2). The architecture is optimized using a composite loss function that accounts for frame-specific detections, overlaps between predicted and ground truth fake intervals, and divergence in corresponding boundaries. We evaluate DiMoDif on Deepfake detection (DFD) and Temporal Forgery Localization (TFL) tasks using three audio-visual deepfake detection benchmarks, FakeAVCeleb [34], LAV-DF [10], and AV-Deepfake1M [8]. DiMoDif outperforms the state-of-the-art on FakeAVCeleb by +2.8% AUC, exhibits a significant performance increase of +30.5% AUC and +47.88% AP@0.75 on AV-Deepfake1M, and performs on-par on LAV-DF dataset. We also provide an analysis of DiMoDif’s generalization abilities, and an extensive ablation analysis to ground our methodological choices."
https://arxiv.org/html/2411.10189v1,NeISF++: Neural Incident Stokes Field for Polarized Inverse Rendering of Conductors and Dielectrics,"Recent inverse rendering methods have greatly improved shape, material, and illumination reconstruction by utilizing polarization cues. However, existing methods only support dielectrics, ignoring conductors that are found everywhere in life. Since conductors and dielectrics have different reflection properties, using previous conductor methods will lead to obvious errors. In addition, conductors are glossy, which may cause strong specular reflection and is hard to reconstruct. To solve the above issues, we propose NeISF++, an inverse rendering pipeline that supports conductors and dielectrics. The key ingredient for our proposal is a general pBRDF that describes both conductors and dielectrics. As for the strong specular reflection problem, we propose a novel geometry initialization method using DoLP images. This physical cue is invariant to intensities and thus robust to strong specular reflections. Experimental results on our synthetic and real datasets show that our method surpasses the existing polarized inverse rendering methods for geometry and material decomposition as well as downstream tasks like relighting.","Inverse rendering is a fundamental task in computer vision and computer graphics, which aims to decompose the target scene into 3D properties like geometry, material, and lighting. It is crucial for applications like virtual reality, material science, and game design. The recent progress of inverse rendering has been dominated by neural representations [38], which utilize multilayer perceptrons (MLPs) to efficiently represent geometry [56, 51], material [6], and lighting [54], and greatly improve the reconstruction accuracy. However, the inherent challenge in inverse rendering, the ambiguity problem, still exists. Recovering 3D properties from 2D images is essentially an ill-posed problem, as different combinations of geometry, material, and lighting may result in the same appearance. An active research field for reducing ambiguity is applying neural representations beyond conventional cameras. Advanced sensors such as event [45, 44], infrared [58], hyper-spectral [43], fisheye [35], and time-of-flight cameras [2] have been extensively explored. One sophisticated sensor worth mentioning is the polarization camera, which can capture the oscillation direction of light in addition to intensity and color. When the light interacts with the object’s surface, the polarization changes according to the geometry and material. In other words, the captured polarization image contains rich information about geometry and material, thus disambiguating the inverse rendering. To our knowledge, PANDORA [14] is the first work that successfully combines polarization cues and neural representations for an inverse rendering problem. Since then, many follow-up polarized inverse rendering works have been proposed to improve it by supervising with tangent space consistency [10, 20], specifically designed polarimetric loss [11], or considering indirect illumination [33]. However, the same problem with the above polarized inverse rendering works is that they only support dielectric materials. Although dielectrics like rubber, wood, and plastic are common materials, conductors like steel, gold, and aluminum are also unignorable. Applying dielectrics-based inverse rendering methods to conductors causes significant errors. The error mainly comes from two aspects. The first one is dielectrics and conductors have different polarimetric properties, they thus require different material models. The second one is conductors are usually glossy and have strong specular reflections, which increases the ambiguity and requires special treatment. To solve these issues, we propose NeISF++, a polarized inverse rendering method that supports both conductors and dielectrics. Our framework mainly follows NeISF [33]. Given multi-view polarized images, we represent the geometry as a signed distance field (SDF), the material as a Bidirectional Reflectance Distribution Function (BRDF) field, and the multi-bounced polarized light as an incident Stokes field. Then, a physically-based polarimetric renderer calculates the final outgoing polarized light, and the model is self-supervised. This work focuses on solving the briefly mentioned two error sources: the material model and the strong specular reflection. For the first error source (material model), we propose a general polarimetric BRDF (pBRDF) that describes both conductors and dielectrics. Existing polarized inverse rendering works [33, 14] use Baek pBRDF [4] as the material model, which is specially designed for dielectrics. It describes the captured polarization signal as the combination of diffuse and specular polarization. The diffuse polarization comes from multi-bounced subsurface scattering, and the specular polarization comes from single-bounced mirror reflection. Extending Baek pBRDF to support conductors faces many challenges. For example, visible light can not penetrate the surface of conductors [13]. Therefore, the diffuse polarization does not exist for conductors. To solve this discrepancy, we propose using a binary indicator to control the existence of the diffuse polarization term. In addition, even for the single-bounced mirror reflection, the properties of conductors and dielectrics are still different. Because the refractive index of dielectrics is a real number, while the refractive index for conductors is a complex number, we implement a general Fresnel reflection term that supports both real and complex numbers to address this problem. For the second error source (strong specular reflection), we propose using the degree of linear polarization (DoLP) images to initialize the SDF. Training the SDF from the initialization of volume rendering works like VolSDF [55] is a common technique used in the existing inverse rendering works [59, 33]. However, poor geometry initialization can damage the final reconstruction results, and the initialization quality is usually low when strong specular reflection exists. An advantage of using polarized images is that we can utilize physical properties such as DoLP, which is independent of the light intensity and strongly related to the geometry. With these advantages, we argue that DoLP is a better image domain for geometry initialization than intensity images. By solving the two error sources, NeISF++ significantly improves geometry and material reconstruction when both conductors and dielectrics exist. Additionally, because of the correct modeling of conductors, the relighting results are much more realistic than the previous work [33] (Fig. 1). To summarize, our contributions can be seen as proposing: \bullet NeISF++, the first polarized inverse rendering pipeline with pBRDF supporting conductors and dielectrics. \bullet A novel geometry initialization approach using DoLP images, which is robust to strong specular reflections. \bullet A real and synthetic multi-view polarimetric dataset consists of objects containing conductors and dielectrics. The code and dataset will be made public upon acceptance."
https://arxiv.org/html/2411.10187v1,Try-On-Adapter: A Simple and Flexible Try-On Paradigm,"Image-based virtual try-on, widely used in online shopping, aims to generate images of a naturally dressed person conditioned on certain garments, providing significant research and commercial potential. A key challenge of try-on is to generate realistic images of the model wearing the garments while preserving the details of the garments. Previous methods focus on masking certain parts of the original model’s standing image, and then inpainting on masked areas to generate realistic images of the model wearing corresponding reference garments, which treat the try-on task as an inpainting task. However, such implements require the user to provide a complete, high-quality standing image, which is user-unfriendly in practical applications. In this paper, we propose Try-On-Adapter (TOA), an outpainting paradigm that differs from the existing inpainting paradigm. Our TOA can preserve the given face and garment, naturally imagine the rest parts of the image, and provide flexible control ability with various conditions, e.g., garment properties and human pose. In the experiments, TOA shows excellent performance on the virtual try-on task even given relatively low-quality face and garment images in qualitative comparisons. Additionally, TOA achieves the state-of-the-art performance of FID scores 5.56 and 7.23 for paired and unpaired on the VITON-HD dataset in quantitative comparisons.","Image-based virtual try-on aims to naturally dress a model with given reference garment images, widely used in online shopping. A key challenge for try-on is to fit the non-rigid warping of a garment to a target body shape, while not making distortions in garment pattern and texture (Han et al. 2018; Choi et al. 2021; Wang et al. 2018). Most existing try-on methods adopt multi-stage approaches (Ge et al. 2021a; Han et al. 2018) which accomplish try-on tasks by including structure estimation, clothes warping (Han et al. 2019; Ge et al. 2021b; Wang et al. 2020), and image generation step by step, which couples the entire pipeline of try-on tasks into several subtasks. Recently, deep generative models (DGMs) and especially (score-based) diffusion models (Sohl-Dickstein et al. 2015; Ho, Jain, and Abbeel 2020; Song et al. 2020; Karras et al. 2022) have made remarkable progress in various domains, including text-to-image generation (Ho et al. 2022a; Dhariwal and Nichol 2021; Xu et al. 2022; Bao et al. 2023), audio generation (Kong et al. 2020; Popov et al. 2021), video generation (Ho et al. 2022b), text-to-3D generation (Poole et al. 2022; Wang et al. 2024b). Beyond remarkable generation performance, another interesting aspect is that large-scale diffusion models provide rich prior knowledge. These rich priors lie in diffusion models that have already understood the color, the texture, and the pose. This eliminates the requirements to train the network from scratch to acquire a satisfactory performance. Previous studies complete the try-on tasks via stable diffusion models such as StableViton (Kim et al. 2023) and Street TryOn (Cui et al. 2023). These methods formulate the try-on tasks as inpainting tasks, which mask certain parts of the whole standing image and then generate the model’s image wearing the reference garments utilizing the surplus unmasked areas. However, such constraints of inpainting reduce the control ability in try-on tasks and are user-unfriendly due to the difficulty of obtaining high-quality standing images. Therefore, to achieve simplified input and flexible control for try-on tasks, we aim to find a paradigm that can refer to given items, such as the face and garment. Then, we seek to ""imagine"" the missing parts that are not provided. Finally, we desire to combine all these components to generate one realistic image. Such a paradigm is similar to outpainting, which involves extending the area outside a given image. The difference, however, is that we need to integrate a given reference image naturally. To realize natural integration of different items in the outpainting paradigm and generate a realistic image, the image-as-prompt technique is a choice. Previous studies such as IP-Adapter (Ye et al. 2023), InstantID (Wang et al. 2024a), and PhotoMaker (Li et al. 2023) are built upon the foundation of face recognition models by adding extra layers, enabling Stable Diffusion to interpret the images as text. As a result, joint image and text-guided generation can be achieved during the inference process. Refer to these related works, we believe that the model can understand and combine items such as face images and garment images through some extra adapters and effective training, without destroying its rich priors and remarkable generation ability. Figure 1: Results of Try-On-Adapter. The first column gives reference face (top) and reference pose (bottom). The second column shows two target garments. The third to fifth columns show try-on results generated by TOA with different conditions. The third column is generated by the reference face and target garments, conditioned on null text guidance, without pose control. The fourth column is generated by the reference face and target garments, conditioned on text guidance, without pose control. The last column is generated by the reference face and target garment, conditioned on both text guidance and pose control. In this paper, we reformulate the try-on task as an outpainting task, which generates realistic and naturally dressed images of the models via ""imagine"" the rest parts given the reference face and garment images, and propose the Try-On-Adapter (TOA) to accomplish the outpainting ability for try-on task. To train the Try-On-Adapter, we put forward a pipeline of processing the original try-on dataset to obtain the facial image and corresponding prompt for the original image and then train the Try-On-Adapter via the denoising score matching with the processed dataset. Compared with existing methods, Try-On-Adapter has the following threefold advantages: (1) Try-On-Adapter has a simpler input, it only requires a given facial image and a reference garment, instead of a high-quality standing image, which is much more user-friendly in practice. (2) Try-On-Adapter possesses a more flexible control ability, where users can edit the properties of garments and the human poses which is hard to change in traditional try-on methods, details in Fig. 1. (3) Experimental results show that our Try-On-Adapter outperforms the latest OOTDiffusion (Xu et al. 2024) in qualitative comparisons and achieves the state-of-the-art performance of FID scores 5.56 and 7.23 for paired and unpaired on the VITON-HD dataset."
https://arxiv.org/html/2411.10185v1,Efficient Progressive Image Compression with Variance-aware Masking,"Learned progressive image compression is gaining momentum as it allows improved image reconstruction as more bits are decoded at the receiver. We propose a progressive image compression method in which an image is first represented as a pair of base-quality and top-quality latent representations. Next, a residual latent representation is encoded as the element-wise difference between the top and base representations. Our scheme enables progressive image compression with element-wise granularity by introducing a masking system that ranks each element of the residual latent representation from most to least important, dividing it into complementary components, which can be transmitted separately to the decoder in order to obtain different reconstruction quality. The masking system does not add further parameters nor complexity. At the receiver, any elements of the top latent representation excluded from the transmitted components can be independently replaced with the mean predicted by the hyperprior architecture, ensuring reliable reconstructions at any intermediate quality level. We also introduced Rate Enhancement Modules (REMs), which refine the estimation of entropy parameters using already decoded components. We obtain results competitive with state-of-the-art competitors, while significantly reducing computational complexity, decoding time, and number of parameters.","††This article has been accepted for publication at the 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV 2025). In recent years, Learned Image Compression (LIC) has attracted significant interest, outperforming standardized codecs [39, 33, 5] in Rate-Distortion (RD) efficiency for natural images [43, 26]. In a learnable codec, an encoder on the transmitter side projects an image to a latent space that is quantized and entropy coded into a compressed bitstream. On the receiver side, the bitstream is processed by a decoder that reverses the encoding process, recovering (a distorted version of) the original image. Figure 1: Compression results for three different qualities, which increase across rows. Adding details via the masking system (a) increases the standard deviation in the non-masked latent representation (b) to add details (c) for a better reconstruction (d). However, LIC codecs still face the challenge of meeting the rate of channels whose capacity changes as a function of connection type and congestion at the nodes. Scalable coding [31] consists in encoding a content as one base bitstream enabling to recover a low-quality version of the content and a few enhancement bitstreams enabling improved quality when received. With progressive image compression, sometime known also as fine-grained scalability (FGS), this concept is further extended improving the quality of the reconstructed image as each extra bit from the same bitstream is received [23], allowing to truncate it at. A few learnable image compression schemes with progressive decoding properties have been known to exist to date; some allow adjusting the tradeoff between compressed bitrate and quality by exploiting a single rate-variable model [8, 21, 18]. However, a different bitstream must be encoded and delivered for each different bitrate target. Early models such as Diao et al. [9] and Lu et al. [27] achieved progressive representation through distributed recurrent autoencoder and nested quantization, respectively. Lee et al. [22] introduced a novel way to represent each element of the latent representation based on trit-plane coding, achieving state-of-the-art RD efficiency, without exploiting any kind of context model, which were introduced by [15] for both rate and distortion reduction. A RD prioritized transmission system was introduced to be able to find the trit-planes with more information and give them priority for being encoded first. However, exploiting such modules for both encoding and decoding and finding the right priority transmission distribution make [15] and [22] costly in terms of computational resources and time. In this work, we propose a learnable, efficient, and progressive image compression architecture. It has two initial levels, base and top; the first defines the lowest quality while the second allows the system to achieve all the other higher qualities. To achieve FGS, we first compute a residual latent representation by means of element-wise difference between the top and base ones, and then break it down into complementary parts, which form the final bitstream, which can be encoded and sent separately, resulting in reconstruction at multiple qualities, as shown in Fig. 1. Furthermore, we added in the main architecture learnable rate enhancement modules (REMs) to further improve the estimation of entropy parameters. Our progressive approach is built on a channel-wise entropy parameter module [30]. The main contribution of this paper are the follows: • We introduced method where, to achieve FGS, complementary portions of a residual latent representation are added to the base one, which represents the lowest level in terms of bitrate. • We exploited a lightweight masking policy which ranks the elements of the residual latent representation from most to least important, also identifying their positions, which allows for the creation of a progressive system where more elements are added incrementally. • We achieved competitive results with respect to [15] in RD performance, significantly reducing complexity in terms of computational resources, decoding time, and number of parameters."
https://arxiv.org/html/2411.10183v1,Visual question answering based evaluation metrics for text-to-image generation,"Text-to-image generation and text-guided image manipulation have received considerable attention in the field of image generation tasks. However, the mainstream evaluation methods for these tasks have difficulty in evaluating whether all the information from the input text is accurately reflected in the generated images, and they mainly focus on evaluating the overall alignment between the input text and the generated images. This paper proposes new evaluation metrics that assess the alignment between input text and generated images for every individual object. Firstly, according to the input text, chatGPT is utilized to produce questions for the generated images. After that, we use Visual Question Answering(VQA) to measure the relevance of the generated images to the input text, which allows for a more detailed evaluation of the alignment compared to existing methods. In addition, we use Non-Reference Image Quality Assessment(NR-IQA) to evaluate not only the text-image alignment but also the quality of the generated images. Experimental results show that our proposed evaluation approach is the superior metric that can simultaneously assess finer text-image alignment and image quality while allowing for the adjustment of these ratios.","In recent years, there has been significant progress in generative AI. In particular, image generation tasks, such as text-guided image manipulation [2, 3] and text-to-image generation [4, 5], have received considerable attention and experienced remarkable growth. These image generation tasks involve generating and manipulating images based on input text. These models are often trained on datasets such as COCO [6], CUB [7], DiffusionDB [8], and comparisons of performance are conducted using existing evaluation metrics. However, current automated evaluation metrics face several challenges. For example, Fréchet Inception Distance (FID) [9] is a widely used metric in image generation that measures the realism of generated images by quantifying the distance between embeddings of real and generated images. However, FID [9] doesn’t consider the alignment with input text. Methods like image quality assessment can evaluate image quality but lack consideration for the text-image relationship like FID [9] does. Another metric, CLIPScore [10], assesses the image-text relationship by measuring the cosine similarity between generated image features and text features. However, CLIPScore [10] primarily emphasizes global feature similarity in the CLIP [11] space and often falls short in evaluating the precise alignment between generated images and given text. Therefore, we propose a new automatic evaluation method for text-guided image manipulation and text-to-image generation. In this proposed method, we incorporate Visual Question Answering(VQA) model to assess whether the generated images are correctly conditioned on the input text. This enables a more detailed evaluation of the coherence between the input text and the generated images. Additionally, our method combines Non-Reference Image Quality Assessment(NR-IQA) to incorporate the evaluation of image quality along with the alignment between the generated images and the text. Furthermore, in our proposed method, the weighting of the text-image alignment score and the image quality score can be adjusted at the user’s preference. This allows for a stronger influence of the preferred aspect’s score on the final score, or the display of evaluations for either aspect alone. Experimental results comparing our proposed method with CLIPScore [10] and the existing NR-IQA method, MANIQA [12], demonstrate that our approach can simultaneously reflect image quality assessment and more detailed text-image alignment in the scores. In addition, comparisons with the state-of-the-art evaluation method for text-to-image generation, ImageReward [13], reveal the superiority of our method in that it achieves comparable accuracy in evaluating text-image alignment and image quality, and furthermore, the ratio of these evaluations can be arbitrarily adjusted. To summarize, our contributions are twofold: • We introduce a novel automatic evaluation approach for text-guided image manipulation and text-to-image generation. This novel approach incorporates VQA and NR-IQA, allowing for the simultaneous evaluation of text-image alignment and image quality of generated images. Moreover, the weighting of these evaluations can be adjusted flexibly. • Through comparisons with CLIPScore [10], MANIQA [12], and ImageReward [13], we have demonstrated the superiority of our method that offers simultaneous evaluation capability and adjustability."
https://arxiv.org/html/2411.10180v1,CART: Compositional Auto-Regressive Transformer for Image Generation,"In recent years, image synthesis has achieved remarkable advancements, enabling diverse applications in content creation, virtual reality, and beyond. We introduce a novel approach to image generation using Auto-Regressive (AR) modeling, which leverages a “next-detail” prediction strategy for enhanced fidelity and scalability. While AR models have achieved transformative success in language modeling, replicating this success in vision tasks has presented unique challenges due to the inherent spatial dependencies in images. Our proposed method addresses these challenges by iteratively adding finer details to an image compositionally, constructing it as a hierarchical combination of base and detail image factors. This strategy is shown to be more effective than the conventional “next-token” prediction and even surpasses the state-of-the-art “next-scale” prediction approaches. A key advantage of this method is its scalability to higher resolutions without requiring full model retraining, making it a versatile solution for high-resolution image generation.","Recent advancements in Generative AI for image synthesis and editing have garnered significant interest within both research and industry sectors. Conventional approaches in Generative AI, including Generative Adversarial Networks (GANs) [13, 27] and Variational Autoencoders (VAEs) [20, 37], typically aim to produce entire scenes in a single pass. However, human perception and understanding of visual scenes are inherently compositional. For example, when creating a scene, an artist typically follows an iterative process, beginning with rough outlines, refining shapes, and gradually adding details and shading. Generating entire scenes in one attempt can preclude this iterative addition of detail, posing challenges in scaling to high-resolution images. Recent research has introduced step-wise approaches to the image generation problem, where each step incorporates a subset of details. For instance, diffusion-based methods [15, 38] initiate with a noisy vector and employ a denoising model to incrementally remove noise, progressively revealing a coherent image. Similarly, auto-regressive (AR) models [40, 35, 14, 29] tackle image generation in a patch-wise manner, further supporting an iterative image generation approach. Specifically, AR models in image generation, such as VQGAN [12] and DALLE [32], aim to parallel the success of AR-based models in large language models (LLMs). These models use visual tokenizers that convert continuous images into grids of 2D tokens, enabling AR models to learn next-token prediction. Despite the success of AR methods in natural language processing, replicating similar advancements in computer vision has proven challenging. Recent studies in AR modeling [39] indicate that the sequence in which image tokens are processed during AR learning can substantially affect model performance. In this paper, we introduce a novel Auto-Regressive Image Generation approach that constructs high-quality images by progressively assembling a scene in a hierarchical manner. The process begins with the creation of a smooth base image, which is then enhanced through iterative addition of finer details, resulting in a coherent final image (see Figure 1). This method closely emulates a human approach to image creation—starting with a foundational sketch and refining it with increasing levels of detail. Our approach first decomposes a training image into “base” and “detail” components using an edge-aware smoothing technique. These components are then encoded into multi-scale detail token maps. The Auto-Regressive process initiates with a 1\times 1 token, predicting successive token-maps to construct the base component of the image. Once the base is established, the model transitions to predicting the detail components, incrementally layering them to enhance the base image. This structured, iterative process aligns with a natural order of image formation, enhancing both quality and interpretability in the generation process. The Contributions of this paper include: • A novel approach enabling iterative image composition which aligns well with the natural order of image formation. • A tokenization approach that performs quantization of the image decomposition into a base layer and multiple detail layers • High-resolution image generation without the need for model retraining, demonstrating scalability and adaptability to higher resolutions."
https://arxiv.org/html/2411.10161v1,*○: No-reference Image Quality Assessment for Regions of Interest via Vision-Language Instruction Tuning,"Existing Image Quality Assessment (IQA) methods achieve remarkable success in analyzing quality for overall image, but few works explore quality analysis for Regions of Interest (ROIs). The quality analysis of ROIs can provide fine-grained guidance for image quality improvement and is crucial for scenarios focusing on region-level quality. This paper proposes a novel network, Seagull, which can SEe and Assess ROIs quality with GUidance from a Large vision-Language model. Seagull incorporates a vision-language model (VLM), masks generated by Segment Anything Model (SAM) to specify ROIs, and a meticulously designed Mask-based Feature Extractor (MFE) to extract global and local tokens for specified ROIs, enabling accurate fine-grained IQA for ROIs. Moreover, this paper constructs two ROI-based IQA datasets, Seagull-100w and Seagull-3k, for training and evaluating ROI-based IQA. Seagull-100w comprises about 100w synthetic distortion images with 33 million ROIs for pre-training to improve the model’s ability of regional quality perception, and Seagull-3k contains about 3k authentic distortion ROIs to enhance the model’s ability to perceive real world distortions. After pre-training on Seagull-100w and fine-tuning on Seagull-3k, Seagull shows remarkable performance on fine-grained ROI quality assessment. Code and datasets are publicly available at the link.","Image quality assessment (IQA) is a long-standing research in image processing fields. Compared to the full-reference IQA (FR-IQA) and the reduced-reference IQA, the no-reference IQA (NR-IQA) receives more attention since the reference images are unavailable in real-word applications. Figure 1: (A) Illustrations of the typical Vision-based and VLM-based IQA. Both of them are designed to analyze the quality of overall image. (B) Our Seagull has the capability in fine-grained quality assessment for specified ROI. The mask-based ROI is extracted by SAM [30]. Best viewed in color. As illustrated in Fig.1-(A), NR-IQA methods can be categorized into two main types based on their inputs: Vision-based IQA [26, 62, 8, 50, 48] and Vision-Language Model (VLM)-based IQA [57, 55, 67, 66]. Vision-based IQA predicts quality scores for the input images. This approach lacks of interpretability. VLM-based IQA provides detailed quality descriptions based on the input images and textual prompts. This approach has achieved a significant success in analyzing quality for overall images, but overlooks the exploration of quality assessment for specified Regions of Interest (ROIs). Analyzing the quality for ROIs provides more refined guidance for image quality improvement, which has wide applications across various domains, such as video optimization for focused objects [31, 32, 61, 28], image enhancement for ROIs [25, 63, 15, 29, 52] and image compression for interested regions [41, 18, 24]. One method to achieve the quality assessment for ROIs is cropping ROIs and then directly feeding cropped regions into existing vision-based IQA models. Alternatively, drawing bounding boxes (BBoxes) on images can also be used to indicate ROIs. However, both crop-based ROIs and BBox-based ROIs include irrelevant background and struggle to precisely indicate interested objects, leading to inaccurate instructions to IQA models. In contrast, using masks to indicate ROIs (mask-based ROIs) accurately delineates the focused regions. Recently, numerous segmentation models [30, 72, 27] exhibit excellent zero-shot performance and many advanced VLMs [1, 38, 64] show impressive capabilities in visual understanding. These advancements make it potential to achieve IQA for ROIs by utilizing segmentation models to generate masks that indicate ROIs and VLMs for fine-grained ROI quality assessment. Nevertheless, there are two challenges: (1) The performance of these VLMs in quality analysis is limited [55, 57, 67], since they are designed and trained for high-level tasks and ignore to effectively extract low-level features. (2) Most existing IQA datasets [46, 10, 20, 57] only provide quality scores or descriptions for the overall images, which make them unsuitable for ROI-based IQA training. In this paper, we achieve fine-grained IQA for ROIs from the perspective of network and dataset. Firstly, we design a novel network that consists of the Segment Anything Model (SAM) [30] to extract mask-based ROIs for accurately seeing and a VLM to comprehend the ROIs’ quality for fine-grained assessing. We name this network Seagull, which can SEe and Assess ROIs quality with GUidance from a Large vision-Language model. Additionally, to enhance the capacity of Seagull in ROIs assessing, we meticulously design a mask-based feature extractor (MFE). MFE extracts global and local view tokens using mask-based ROIs, providing Seagull with more perspectives to understand the quality of the ROIs. At last, by clicking any region on a full image, SAM first extracts the mask to indicate the ROI, and Seagull subsequently performs a fine-grained quality assessment on the ROI using the extracted mask. Secondly, we construct two ROI-based IQA datasets, namely Seagull-100w and Seagull-3k. Both of them provide labels for ROIs from three dimensions: ROI Quality Score, ROI Importance Score, and ROI Distortion Analysis. ROI Quality Scores quantitatively measure the quality for a specific ROI. Importance Scores reflect the impact of an ROI on the overall image. Distortion Analysis provides distortion types and the severity degree. Specifically, Seagull-100w, consisting of about 100w synthetic distortion (Dist.) images and approximately 33 million mask-based ROIs, is constructed for pre-training to enhance the quality perception ability of models. Each ROI is carefully annotated with the three dimensions labels using reliable models. Specially, Dist. images in Seagull-100w are derived from the RAW images using an Image Signal Processor (ISP) under different distortions settings. Compared to RGB-derived Dist. images, RAW-derived Dist. images are more authentic, since more detailed information captured from the camera sensor is reserved in the RAW. However, the discrepancy between synthetic and authentic Dist. image hinders the robustness of models in real-world images. Thus, we additionally construct the manually annotated Seagull-3k dataset for fine-tuning. This dataset consists of 3,261 ROIs for real-world authentic Dist. images and is annotated by 24 trained annotators. For each ROI, annotators are required to evaluate the distortion types, distortion severity degrees, quality score of the ROI and its importance on the overall image quality. To reduce bias, each ROI is annotated by at least 7 annotators, and the labels are determined based on their average results. As shown in Fig.1-(B), after pre-training on Seagull-100w and fine-tuning on Seagull-3k, Seagull achieves fine-grained quality analysis for the specified ROI. Extensive experiments demonstrate the superiority of Seagull. Our contributions can be summarized as follows: 1. We propose Seagull, a novel IQA framework that accomplishes fine-grained quality assessment for any specified ROIs from the aspects of Quality Score, Importance Score and Distortion Analysis. 2. We construct two ROI-based IQA datasets: Seagull-100w and Seagull-3k. Compared to existing datasets, ours provide more detailed labels for ROI-based IQA. 3. Experimental results demonstrate that Seagull remarkably surpasses existing advanced IQA models and VLMs in ROI quality analysis."
https://arxiv.org/html/2411.10141v1,Matrix-Valued LogSumExp Approximation for Colour Morphology,"Mathematical morphology is a part of image processing that uses a window that moves across the image to change certain pixels according to certain operations. The concepts of supremum and infimum play a crucial role here, but it proves challenging to define them generally for higher-dimensional data, such as colour representations. Numerous approaches have therefore been taken to solve this problem with certain compromises.In this paper we will analyse the construction of a new approach, which we have already presented experimentally in paper [15]. This is based on a method by Burgeth and Kleefeld [6], who regard the colours as symmetric 2\times 2 matrices and compare them by means of the Loewner order in a bi-cone through different suprema. However, we will replace the supremum with the LogExp approximation for the maximum instead. This allows us to transfer the associativity of the dilation from the one-dimensional case to the higher-dimensional case. In addition, we will investigate the minimality property and specify a relaxation to ensure that our approach is continuously dependent on the input data.","Mathematical morphology is a theory used to analyse spatial structures in images. Over the decades it has developed into a very successful field of image processing, see e.g. [18, 20, 21] for an overview. Morphological operators basically consist of two main components. The first of these is the structuring element (SE), which is characterised by its shape, size and position. These in turn can be divided into two types of SEs, flat and non-flat cf. [10]. A flat SE basically defines a neighbourhood of the central pixel where appropriate morphological operations are performed, while a non-flat SE also contains a mask with finite values that are used as additive offsets. The SE is usually implemented as a window sliding over the image. The second main component is used to perform a comparison of values within an SE. The basic operations in mathematical morphology are dilation and erosion, where a pixel value is set to the maximum and minimum, respectively, of the discrete image function within the SE. Many morphological filtering procedures of practical interest, such as opening, closing or top hats, can be formulated by combining dilation and erosion operations. Since dilation and erosion are dual operations, it is often sufficient to restrict oneself to one of the two when constructing algorithms. Let us also briefly extend this concept to colour morphology, as it is the underlying concept for our further considerations. As already mentioned, the most important operation in morphology is to perform a comparison of the tonal values or, in our case, the colour values within the SE over certain sets of pixels in an image domain. For the simpler application areas such as binary or grey value morphology, one can act directly on complete lattices in order to obtain a total order of the colour values, see [21]. In the case of colour morphology, this is no longer the case, as there is no total order of the colour values. For this reason, corresponding semi-orders and different basic structures are used, cf. [2]. The first approach that could be used for this would be to regard each colour channel of an image as an independent image and to perform grey value morphology on each of them. This approach has the serious disadvantage that we lose the correlated information between the colour channels, which could be used to further improve the filtering results. The other approach, which is more popular, uses a vector space structure in which each colour is considered a vector in an underlying colour space. In order to compute a supremum or infimum, it is necessary to have an order for the vector space. However, there are a plethora of ordering approaches for colour morphology. For details of the most commonly used approaches, we refer the reader to the overview provided in [1]. We will take the latter approach, but use symmetric matrices instead of vectors. Since there is also no total order for colour matrices either, we will order the elements by means of a semi-order, namely the Loewner order, see [5]. This means, though, that we need an additional function to select a minimum upper bound, namely the supremum function. To calculate two of the basic operations of colour morphology, dilation and erosion, it is necessary to determine the supremum or infimum. Because of the duality between these two operations, it is common to consider only one of them. Here we will concentrate on dilation and the construction of the supremum. However, there are several approaches how to choose the supremum of a set of symmetric matrices, based on different norms. To give some examples, we mention here the nuclear norm, the Frobenius norm and the spectral norm. For a comparison of these norms we refer to the work by Welk, Kleefeld and Breuß [25]. Here we want to consider another approach, namely the approximation of the supremum by a so-called LogSumExp approximation of Maslov [17]. This is an approximation which has already given promising results in the work of Kahra, Sridhar and Breuß [13] for grey-scale images and for colour images in [24] in conjunction with a fast Fourier transform. However, the latter only represents a one-dimensional channel-wise approach to colour morphology. Another connection worth mentioning is the work [7] of Burgeth, Welk, Feddern and Weickert, where root and power functions were used for symmetric positive semidefinite matrices instead of logarithm and exponential function. However, our approach does not require a positive semidefinite matrix, but works with any colour matrix, and preserves the so-called transitivity of grey-scale morphology. This paper will be the next step from [15] for transferring the LogSumExp approach to colour morphology with tonal vectors/matrices. The goal is to present a clear characterisation of this approach for tonal value matrices to close the gap in the reasoning of [15] and to extend it with regard to certain properties. In this way, we will end up with a dilation operator that, with a few minor compromises, combines many of the advantageous properties of the other multidimensional approaches while preserving the associativity of the dilation, which, as far as we know, is not the case with the other multidimensional approaches. In addition, we will present a relaxed formulation of the operator, which addresses one of the primary limitations of the operator."
https://arxiv.org/html/2411.10136v1,CoSAM: Self-Correcting SAM for Domain Generalization in 2D Medical Image Segmentation,"Medical images often exhibit distribution shifts due to variations in imaging protocols and scanners across different medical centers. Domain Generalization (DG) methods aim to train models on source domains that can generalize to unseen target domains. Recently, the segment anything model (SAM) has demonstrated strong generalization capabilities due to its prompt-based design, and has gained significant attention in image segmentation tasks. Existing SAM-based approaches attempt to address the need for manual prompts by introducing prompt generators that automatically generate these prompts. However, we argue that auto-generated prompts may not be sufficiently accurate under distribution shifts, potentially leading to incorrect predictions that still require manual verification and correction by clinicians. To address this challenge, we propose a method for 2D medical image segmentation called Self-Correcting SAM (CoSAM). Our approach begins by generating coarse masks using SAM in a prompt-free manner, providing prior prompts for the subsequent stages, and eliminating the need for prompt generators. To automatically refine these coarse masks, we introduce a generalized error decoder that simulates the correction process typically performed by clinicians. Furthermore, we generate diverse prompts as feedback based on the corrected masks, which are used to iteratively refine the predictions within a self-correcting loop, enhancing the generalization performance of our model. Extensive experiments on two medical image segmentation benchmarks across multiple scenarios demonstrate the superiority of CoSAM over state-of-the-art SAM-based methods.","Medical image segmentation plays an important role in computer-aided diagnosis, yet models pre-trained on labeled datasets (source domain) often experience performance degradation when applied to data from different medical centers (target domain). This decline is primarily caused by distribution shifts [34, 36, 13], which arise from variations in imaging scanners and protocols. To address this challenge, domain generalization (DG) techniques [50, 29] have been developed to train models on source data that can generalize well to target data. Due to the data-driven nature of deep learning models [24, 30], a more direct approach compared to other complex DG methods [6, 49, 40, 39] is to train a generalized model on massive amounts of data. A notable example is the segment anything model (SAM) [20], which was trained on over a billion masks from diverse images and has shown exceptional generalization capabilities when transferred to new data distributions. Existing SAM-based methods can be broadly divided into two categories. The first category consists of prompt-free methods [22, 12, 9, 4, 46, 32, 33, 3], which leverage the robust representation ability of SAM’s image encoder and mask decoder, enabling inference without additional prompts (see Figure 1 (a)). However, these methods cannot fully capitalize on the advantages of prompts within SAM, limiting their performance potential. The second category includes prompt-based methods [41, 8, 43, 45, 14, 42, 23, 2], which generate prompts automatically through a prompt generator, removing the need for manual input (see Figure 1 (b)). While these methods are more flexible, they face several limitations in clinical applications: (1) inaccurate prompts from the generator can mislead SAM, requiring manual corrections by clinicians, and (2) they typically generate only one type of prompt, failing to leverage the full range of SAM’s prompt options (e.g., point, box, and mask). To overcome these challenges, we propose a novel SAM-based method for domain generalization, called Self-Correcting SAM (CoSAM), illustrated in Figure 1 (c). CoSAM first generates coarse masks using SAM itself in a prompt-free manner, providing prior prompts for subsequent processes and eliminating the need for a prompt generator. Next, we introduce a generalized error decoder to generate error maps. The error maps represent the difference between predictions and labels, where correctly-predicted pixels (correct points) are marked as 0, and incorrectly-predicted pixels (error points) are marked as 1. We utilize the error maps to correct the coarse masks, simulating the manual correction process performed by clinicians and resulting in more accurate masks. Finally, diverse prompts are constructed from the corrected masks and fed back into SAM to further refine the predictions within a self-correcting loop, improving prediction quality iteratively. We evaluated CoSAM against other state-of-the-art SAM-based methods on two medical image segmentation tasks: (1) prostate segmentation on MRI cases from six domains, and (2) optic disc and cup segmentation on fundus images from four domains. Our results demonstrate that CoSAM outperforms existing methods on both tasks across multiple domain generalization scenarios. The contributions of this work are three-fold. (1) We train a generalized error decoder that corrects coarse masks to simulate the manual correction process, producing more accurate prompts. (2) We introduce a self-correcting loop, where diverse prompts based on corrected masks are fed back into SAM to progressively refine predictions. (3) Extensive experiments demonstrate the superiority of CoSAM over other SAM-based methods on two medical image segmentation benchmarks."
https://arxiv.org/html/2411.10133v1,Efficient Density Control for 3D Gaussian Splatting,"3D Gaussian Splatting (3DGS) excels in novel view synthesis, balancing advanced rendering quality with real-time performance. However, in trained scenes, a large number of Gaussians with low opacity significantly increase rendering costs. This issue arises due to flaws in the split and clone operations during the densification process, which lead to extensive Gaussian overlap and subsequent opacity reduction. To enhance the efficiency of Gaussian utilization, we improve the adaptive density control of 3DGS. First, we introduce a more efficient long-axis split operation to replace the original clone and split, which mitigates Gaussian overlap and improves densification efficiency. Second, we propose a simple adaptive pruning technique to reduce the number of low-opacity Gaussians. Finally, by dynamically lowering the splitting threshold and applying importance weighting, the efficiency of Gaussian utilization is further improved. We evaluate our proposed method on various challenging real-world datasets. Experimental results show that our Efficient Density Control (EDC) can enhance both the rendering speed and quality.","Novel view synthesis (NVS) is a classical problem in computer vision, with widespread applications in virtual reality, cultural heritage preservation, autonomous driving, and other fields. Neural Radiance Field (NeRF) [15] introduced the use of neural networks to learn the structure and features of a scene, requiring only multi-view 2D images as training data to synthesize high-quality novel views. However, NeRF suffers from long synthesis times for individual views [17, 5], making real-time rendering challenging. Recently, 3D Gaussian Splatting (3DGS) [8] has attracted attention due to its explicit representation and real-time rendering performance. 3DGS represents a scene using a large number of 3D Gaussian ellipsoids. The properties of these Gaussians include position, size, shape, opacity, and color, all of which can be optimized through differentiable rendering. 3DGS generates an initial set of Gaussians from the sparse points obtained through Structure from Motion (SfM) [18], and subsequently refines the scene representation by increasing Gaussian density via adaptive density control. The rendering and storage overhead of 3DGS is proportional to the number of Gaussians. A key issue is how to achieve the same or even better rendering quality with fewer Gaussians. By analyzing the number distribution of Gaussians across different opacity intervals, we observe an excessive proportion of Gaussians with low opacity. For instance, in the bicycle scene , more than half of the Gaussians have opacity values less than 0.1 (see Figure 2). The rendering contribution of each Gaussian is proportional to its opacity. Gaussians with low opacity contribute minimally to the rendering process, yet they significantly increase both rendering and storage costs. We find that flaws in the clone and split operations during the densification process result in an abnormal proportion of low-opacity Gaussians. The split operation replaces a large Gaussian with two smaller ones, where the positions of the smaller Gaussians are determined through probabilistic sampling. This leads to the split Gaussians overlapping with a certain probability. The clone operation simply replicates a small Gaussian and hopes the original Gaussian will move away from the clone through parameter updates. However, the extent of each parameter update is limited and gradually decreases as training progresses. As a result, the Gaussian and its clone are likely to overlap. To prevent the opacity in the overlapping regions from becoming too large, the opacities of the overlapping Gaussians are gradually reduced during optimization, resulting in a high proportion of low-opacity Gaussians in the scene. Figure 2: We compared the proportions of Gaussians within different opacity ranges in the bicycle [1] scene, trained using 3DGS and our proposed method. To improve the efficiency of Gaussian utilization, we enhance the adaptive density control of 3DGS. First, we replace the original clone and split with long-axis split. Specifically, we use the longest axis of the Gaussian as the splitting dimension. The spacing of the child Gaussians is controled to avoid overlap. We adjust the shape and opacity of the child Gaussians to minimize changes in the shape and density distribution of the covered area before and after splitting, thus maximizing the efficiency of the splitting process. Second, to further reduce the proportion of low-opacity Gaussians, we propose a simple adaptive pruning method. Finally, by dynamically adjusting the splitting threshold and using importance weighting when computing the average gradient, we further improve the efficiency of Gaussian utilization. Our method not only significantly reduces the number of Gaussians in the scene but also enhances the ability to recover fine details (see Figure 1). On the challenging Mip-NeRF 360 dataset, we reduced the number of Gaussians by 30% while improving the average PSNR from 25.52 to 25.82. Our method is easy to implement and will be open-sourced in the future. Summary of our contributions: • We found that the clone and split operations used during the densification process of 3DGS have inherent limitations. These limitations result in a large number of low-opacity Gaussians, which in turn cause unnecessary overhead. The proposed long-axis split operation effectively mitigates this issue, thereby improving densification efficiency. • We propose a simple adaptive pruning method that significantly reduces the proportion of low-opacity Gaussians without affecting rendering quality. • The dynamic thresholding and adaptive weighting we introduced further enhance the efficiency of Gaussian utilization."
https://arxiv.org/html/2411.10130v1,Towards Multi-View Consistent Style Transfer with One-Step Diffusion via Vision Conditioning,"The stylization of 3D scenes is an increasingly attractive topic in 3D vision. Although image style transfer has been extensively researched with promising results, directly applying 2D style transfer methods to 3D scenes often fails to preserve the structural and multi-view properties of 3D environments, resulting in unpleasant distortions in images from different viewpoints. To address these issues, we leverage the remarkable generative prior of diffusion-based models and propose a novel style transfer method, OSDiffST, based on a pre-trained one-step diffusion model (i.e., SD-Turbo) for rendering diverse styles in multi-view images of 3D scenes. To efficiently adapt the pre-trained model for multi-view style transfer on small datasets, we introduce a vision condition module to extract style information from the reference style image to serve as conditional input for the diffusion model and employ LoRA in diffusion model for adaptation. Additionally, we consider color distribution alignment and structural similarity between the stylized and content images using two specific loss functions. As a result, our method effectively preserves the structural information and multi-view consistency in stylized images without any 3D information. Experiments show that our method surpasses other promising style transfer methods in synthesizing various styles for multi-view images of 3D scenes. Stylized images from different viewpoints generated by our method achieve superior visual quality, with better structural integrity and less distortion. The source code is available at https://github.com/YushenZuo/OSDiffST.","Image style transfer is a compelling research area in computer vision that aims to render an image with artistic features derived from a style reference while preserving its original content. Neural style transfer models using deep convolutional neural networks (CNNs) have shown impressive results in synthesizing 2D images with diverse artistic styles [9, 15, 16, 29, 24, 45]. In the past years, researchers have attempted to extend these 2D style transfer methods to 3D scenes based on multi-view images, aiming to render the artistic stylized version. However, these 2D methods often struggle to preserve the structural information of objects and the multi-view consistency of 3D scenes, leading to undesirable distortions. Consequently, customizing 3D scenes with desired artistic styles based on multi-view images remains an open and challenging problem. Recently, large-scale diffusion-based text-to-image models have demonstrated remarkable performance in image synthesis and content creation, enabling users to produce diverse image content in various styles based on given text prompts. By leveraging the advanced generative capabilities of these models, many researchers have explored methods for 3D artistic stylization of scenes using various 3D representations. Yoo et al. [52] introduced the As-Plausible-as-Possible (APAP) mesh deformation technique, which utilizes 2D diffusion priors to preserve mesh plausibility under user-controlled deformation, resulting in significant improvements. Text2tex [7] incorporates inpainting techniques into a pre-trained depth-aware image diffusion model to progressively synthesize high-quality textures from multiple viewpoints. Zhuang et al. [56] proposed a text-to-3D scene editing model that distills prior knowledge from text-to-image diffusion models and manipulates the content and style of input images based on text and image prompts. TexFusion [4] employs regular diffusion model sampling on different 2D rendered views, demonstrating promising results in texture synthesis. Despite the wide use of diffusion models in various 3D editing and generation tasks, few studies focus on stylizing 3D scenes based on their multi-view images using diffusion models. Generally, 2D diffusion-based models have two intrinsic limitations when applied to the stylization of 3D scenes: (1) they typically require numerous sampling steps, often up to tens or hundreds, resulting in a slow inference process, and (2) it is challenging for 2D diffusion models to maintain the multi-view consistency of 3D scenes during the rendering process. Therefore, it is nontrivial to apply these 2D diffusion models to the stylization of multi-view images of 3D scenes. To address these challenges, we propose OSDiffST, a one-step diffusion model for multi-vew consistent style transfer. OSDiffST effectively synthesizes images from different viewpoints with style reference while preserving image content and multi-view consistency. Our method leverages the diffusion prior from a large-scale pre-trained text-to-image diffusion model called SD-Turbo [40] as our generative backbone, which efficiently produces diverse images in a single diffusion sampling step. However, fine-tuning such a large-scale pre-trained diffusion model for new tasks typically requires large image datasets, which is computationally expensive and impractical in many real-world scenarios (e.g., style transfer). To overcome this, we employ the LoRA [18] technique into our generative backbone to significantly reduce the number of trainable parameters during fine-tuning, enabling efficient model adaptation for multi-view style transfer. Extracting and injecting style information into the model is crucial for the performance of style transfer. Unlike previous methods based on textual inversion techniques [14], our method introduces a vision condition module that uses a pre-trained CLIP [36] image encoder and a vision-language projector to extract style information from the reference style image and transfer to the conditional input of our generative backbone for generating stylized images from different viewpoints. To further enhance the quality of stylized content and preserve multi-view consistency, we introduce two loss functions in model training that jointly align the color distribution between the stylized image and the reference style image, and improve the structural similarity between the stylized image and the input content image. The main contributions of this paper are summarized as follows: 1. We focus on the stylization of multi-view images in 3D scenes and propose OSDiffST, a novel style transfer method based on a one-step diffusion model that effectively preserves the structural information and multi-view consistency of images from different viewpoints. 2. To rapidly adapt the pre-trained diffusion model for style transfer, we incorporate LoRA adapters into the pre-trained model, significantly reducing the number of trainable parameters in the training stage. We propose a vision condition module for efficient style information extraction and injection. 3. Our method uses two additional loss functions to align color distribution and improve structural similarity. As a result, our approach enhances visual quality and maintains multi-view consistency across images from different viewpoints without requiring any 3D information. 4. Experiments show that our method has superior capability in rendering artistic styles across images from different viewpoints while preserving multi-view consistency. Compared with other promising style transfer methods, our approach produces results with better visual quality and minimal distortion."
https://arxiv.org/html/2411.10100v1,Multi-Task Adversarial Variational Autoencoder for Estimating Biological Brain Age with Multimodal Neuroimaging,"Despite advances in deep learning for estimating brain age from structural MRI (sMRI), incorporating functional MRI (fMRI) data presents significant challenges due to its complex data structure and the noisy nature of functional connectivity measurements. To address these challenges, we present the Multitask Adversarial Variational Autoencoder (M-AVAE), a bespoke deep learning framework designed to enhance brain age predictions through multimodal MRI data integration. The M-AVAE uniquely separates latent variables into generic and unique codes, effectively isolating shared and modality-specific features. Additionally, integrating multitask learning with sex classification as a supplementary task enables the model to account for sex-specific aging nuances. Evaluated on the OpenBHB dataset—a comprehensive multisite brain MRI aggregation—the M-AVAE demonstrates exceptional performance, achieving a mean absolute error of 2.77 years, surpassing conventional methodologies. This success positions M-AVAE as a powerful tool for metaverse-based healthcare applications in brain age estimation. The source code is made publicly available at: https://github.com/engrussman/MAVAE.","The advent of multimodal neuroimaging, which combines functional magnetic resonance imaging (fMRI) for the assessment of functional connectivity and structural magnetic resonance imaging (sMRI) for cortical morphology, offers a nuanced approach to the detection of cognitive impairment and the prediction of brain age [10]. However, the exploration of anatomical and functional differences in the brain between sexes using multimodal imaging for the estimation of brain age remains underexplored. Sex differences play a vital role in the brain’s ageing process, with notable anatomical and functional variations between male and female brains [11]. Incorporating sex information into age estimation models improves accuracy and has shown promise in deep learning applications [12]. Our research addresses this gap by integrating sex considerations in a multimodal imaging framework within the metaverse context, aiming to improve the accuracy and applicability of brain age predictions in personalised healthcare. Specifically, we propose a novel metaverse-based AI application for brain age estimation: the Multi-Task Adversarial Variational Autoencoder (M-AVAE). This innovative model merges adversarial learning and variational auto-encoding capabilities within a multitask learning framework, aiming for simultaneous estimation of brain age and prediction of sex from multimodal MRI data, including both sMRI and fMRI. The design of M-AVAE meticulously segregates the latent features of each imaging modality into distinct components, effectively disentangling the shared and unique attributes across modalities. This method not only improves the accuracy in capturing commonalities, but also minimises interference during data fusion, presenting a novel approach to multimodal neuroimaging analysis suitable for integration into metaverse platforms. Our major contributions can be summarised as follows. • We introduce a novel multimodal framework for the estimation of brain age within the metaverse ecosystem for healthcare. Integrating our AI model into a metaverse environment, may enable continuous, real-time updates and interactions, enhancing the precision and reliability of brain age estimations and may allow personalised, predictive healthcare. • Our approach is unique in creating a disentangled representation of brain imaging data by applying both adversarial and variational principles within a single architecture. This disentanglement allows for the clear differentiation of shared versus modality-specific information, paving the way for more nuanced interpretations of neuroimaging data within a metaverse platform. • Through rigorous evaluation of publicly available datasets, our extensive experiments validate the efficacy and robustness of the proposed framework, thereby establishing a new benchmark for brain age estimation models suitable for metaverse integration. Table 1: Summary of comparison of our work with the existing studies in term of availability of different components, i.e., multimodal, multitask, adversarial, and distangled learning. Author Multimodal Multitask Adversarial Disentangled (Year) learning learning learning learning He et al. 2022 [13] ✗ ✗ ✗ ✗ He et al. 2022 [14] ✗ ✗ ✗ ✗ Cheng et al. 2021 [15] ✗ ✗ ✗ ✗ Armanious et al. 2021 [16] ✗ ✓ ✗ ✗ Zhang et al. 2022 [17] ✗ ✓ ✗ ✗ Liu et al. 2023 [18] ✗ ✓ ✗ ✗ Dular et al. 2024 [19] ✗ ✓ ✗ ✗ Wang et al. 2023 [20] ✗ ✓ ✗ ✗ Mouches et al. 2022 [21] ✓ ✓ ✗ ✗ Cai et al. 2023 [22] ✓ ✗ ✗ ✓ Hu et al. 2020 [23] ✓ ✗ ✓ ✓ Our Study ✓ ✓ ✓ ✓"
https://arxiv.org/html/2411.10086v1,CorrCLIP: Reconstructing Correlations in CLIP with Off-the-Shelf Foundation Models for Open-Vocabulary Semantic Segmentation,"Open-vocabulary semantic segmentation aims to assign semantic labels to each pixel without relying on a predefined set of categories. Contrastive Language-Image Pre-training (CLIP) demonstrates outstanding zero-shot classification capabilities but struggles with the pixel-wise segmentation task as the captured inter-patch correlations correspond to no specific visual concepts. Despite previous CLIP-based works improving inter-patch correlations by self-self attention, they still face the inherent limitation that image patches tend to have high similarity to outlier ones. In this work, we introduce CorrCLIP, a training-free approach for open-vocabulary semantic segmentation, which reconstructs significantly coherent inter-patch correlations utilizing foundation models. Specifically, it employs the Segment Anything Model (SAM) to define the scope of patch interactions, ensuring that patches interact only with semantically similar ones. Furthermore, CorrCLIP obtains an understanding of an image’s semantic layout via self-supervised models to determine concrete similarity values between image patches, which addresses the similarity irregularity problem caused by the aforementioned restricted patch interaction regime. Finally, CorrCLIP reuses the region masks produced by SAM to update the segmentation map. As a training-free method, CorrCLIP achieves a notable improvement across eight challenging benchmarks regarding the averaged mean Intersection over Union, boosting it from 44.4% to 51.0%.","Open-vocabulary semantic segmentation (OVSS) [4, 46, 59] aims to partition an image into multiple segments and assign corresponding categories to each segment based on textual descriptions. This challenge requires models to demonstrate strong generalization abilities and effectively align visual representations with textual descriptions. Contrastive Language-Image Pre-training (CLIP) [35] models, trained on large-scale image-text pair datasets, have shown remarkable zero-shot classification capabilities, providing a viable solution for OVSS. (a) Attention maps of selected patches (red-star symbol). (b) Segmentation performance of CLIP. Figure 1: Qualitative (a) and quantitative (b) improvements achieved by our proposed designs on COCO dataset with ViT-L. Vanilla CLIP denotes remaining query-key and Baseline denotes transforming query-key into query-query for the final layer’s attention map of ViT in CLIP. SR* denotes scope reconstruction using labels in our preliminary experiment and SR denotes our scope reconstruction using SAM. VR denotes our value reconstruction. However, the goal of image-text alignment forces CLIP to focus on the image’s overall visual representations, preventing the establishment of coherent inter-patch correlations required for localizing specific visual concepts. To this end, recent research modifies the computation of the attention map in the last layer of the Vision Transformer (ViT) [13] used in CLIP’s image encoder, transforming vanilla query-key into query-query [23], key-key, value-value [27], or their combinations [3, 42]. Although these methods improve inter-patch correlations in CLIP to some extent, they still encounter an inherent limitation: patches often exhibit high similarity to semantically unrelated patches, known as outlier patches [11]. As illustrated in Fig. 1(a) (second column), the selected patches in CLIP display high similarity to certain semantically unrelated patches, leading to confusing inter-patch correlations. To address the aforementioned challenge, we propose scope reconstruction to compel patches to interact only with semantically similar ones. To validate the effectiveness of this method, we conduct a preliminary experiment with image labels. Specifically, we set the similarity between all patches to be equal but restrict the patches to interact only with those belonging to the same semantic category. This adjustment leads to a significant improvement in segmentation performance, as demonstrated in Fig. 1(b). Thus, we provide our key insight: confining the scope of patch interactions to semantically similar regions can effectively improve inter-patch correlations and substantially enhance the segmentation capabilities of CLIP. It is challenging to accurately evaluate the semantic similarity between patches when relying solely on CLIP. To this end, we define semantically similar regions according to the impressive zero-shot class-agnostic segmentation capabilities of the Segment Anything Model (SAM) [22]. Specifically, We use SAM to divide an image into regions and limit patch interactions to occur only within these regions. However, it faces two challenges. First, the segmented regions tend to be fragmented. To tackle this issue, we apply a clustering algorithm to merge regions. Second, some regions remain unsegmented due to thresholding. We define patches in unsegmented regions with similarity greater than the threshold as having similar semantics. When combined with the above two strategies, scope reconstruction significantly improves the segmentation capabilities of CLIP as illustrated in Fig. 1(b). As indicated in Fig. 1(a) (third column), the similarity between patches in the region after scope reconstruction may be irregular because patches have high similarity to some patches but low similarity to one another. We further propose value reconstruction to address the above similarity irregularity problem and construct more coherent inter-patch correlations. Specifically, we use the semantic layout information included in the self-supervised model DINO [6, 11, 40] to determine similarity values. Inspired by previous work [42, 38], which combines query and key in CLIP, we employ the sum of DINO’s query and key to acquire more comprehensive similarity values. Figure 1 demonstrates the improvements in CLIP resulting from our proposed scope reconstruction and value reconstruction. Besides, we propose two simple yet effective designs to correct the defects during CLIP’s segmentation. We reuse region masks to correct spatial inconsistency and combine the category name with its plural form to correct concept ambiguity. By incorporating the four proposed designs, we present a training-free method called CorrCLIP that effectively reconstructs coherent inter-patch correlations and substantially enhances segmentation capacities of CLIP. Extensive experiments have confirmed the effectiveness of CorrCLIP, which surpasses state-of-the-art methods by a notable margin of 6.6% in averaged mIoU across eight benchmarks. CorrCLIP will continue to improve as the CLIP series, the SAM series, and self-supervised models develop, requiring only model replacements. Our contributions are summarized as follows: (1) We reveal that limiting the scope of patch interactions to semantically similar regions can effectively improve inter-patch correlations and substantially enhance the segmentation capabilities of CLIP. (2) We propose scope reconstruction and value reconstruction to reconstruct coherent inter-patch correlations of CLIP. (3) We present segmentation map correction and class name correction to ensure spatial consistency and enhance concept understanding. (4) We introduce CorrCLIP for OVSS. Extensive experiments show that CorrCLIP outperforms state-of-the-art methods."
https://arxiv.org/html/2411.10081v1,Influence of Depth Camera Noise Models on Respiration Estimation,"Depth cameras are an interesting modality for capturing vital signs such as respiratory rate. Plenty approaches exist to extract vital signs in a controlled setting, but in order to apply them more flexibly for example in multi-camera settings, a simulated environment is needed to generate enough data for training and testing of new algorithms. We show first results of a 3D-rendering simulation pipeline that focuses on different noise models in order to generate realistic, depth-camera based respiratory signals using both synthetic and real respiratory signals as a baseline. While most noise can be accurately modelled as Gaussian in this context, we can show that as soon as the available image resolution is too low, the differences between different noise models surface.","Camera-based sensing of vital signs is seeing an increase in popularity, as implied by the rising number of publications111https://pubmed.ncbi.nlm.nih.gov/?term=camera-based+vital+signs. Possible reasons for that include the fact that contact-less estimation reduces the risk of transmitting diseases such as COVID [1] and the possibility to exploit multiple sources of biosignals such as skin perfusion, ballistocardiographic effects, thoracic/abdominal motion, body temperature and affordability of image sensors [2]. One common type of camera is depth cameras that estimate the distance of points in a scene. They are especially useful in public screening settings as they provide inbuilt anonymisation (if raw RGB-camera streams are not recorded), while still enabling tracking of individuals for short time frames [3, 4]. This is true for time-of-flight (ToF), stereo and structured light technologies. Even though new approaches to employing depth cameras are plenty, most studies use as few as 10 participants for training and evaluating their algorithms [2]. If many camera-configurations, and sensor fusion are to be tested, this is far from enough. At the same time, larger studies are expensive or might not even be feasible. This is where simulation can fill a gap. While simulating depth images is straight forward, the noise properties are highly dependent on the used device and even the environment [5]. We propose a framework for simulating realistic sensor noise on depth cameras to enable the identification of respiratory patterns from depth camera information alone in difficult settings. We simulate realistic thoracic respiration on a human 3D model and show the influence of an array of depth camera specific noise sources on the raw signal. We then analyse the noise sources with respect to signal-to-noise ratio (SNR), since recent research emphasized the relevance of SNR to biosignal estimation [6]."
https://arxiv.org/html/2411.10072v1,Real-Time AI-Driven People Tracking and Counting Using Overhead Cameras,"Accurate people counting in smart buildings and intelligent transportation systems is crucial for energy management, safety protocols, and resource allocation. This is especially critical during emergencies, where precise occupant counts are vital for safe evacuation. Existing methods struggle with large crowds, often losing accuracy with even a few additional people. To address this limitation, this study proposes a novel approach combining a new object tracking algorithm, a novel counting algorithm, and a fine-tuned object detection model. This method achieves 97% accuracy in real-time people counting with a frame rate of 20-27 FPS on a low-power edge computer.","I INTRODUCTION Keeping track of the number of people who have entered, exited, and are remaining inside a building or public transport is essential for crowd, facility, and safety management. Even though recent advancements in technology have paved the path for smart buildings and intelligent public transportation systems, it remains challenging to develop economical, efficient, and accurate systems to count people in real-time. Several works have been conducted since the early 2000s for real-time people counting using signal processing and deep learning techniques. Regardless of the technology they use, the reliability, accuracy of these, and cost to implement them are questionable. Either the highly accurate systems are extremely expensive or low-cost systems are not accurate and reliable. The use of time-of-flight concept sensors such as laser beam, thermal, and ultrasound sensors tends to fail when two or more people need to be counted. Jae Hoon et al. [1] attempted using laser range finders. The system requires two sensors to function properly, and these sensors need specific placement depending on the environment. This customization makes it difficult to implement in various settings. In order to address these issues Jeong Woo et al. [2] proposed a new method using two ultra-wide band radar sensors. Even in this approach, the placement of two sensors is crucial, placing them very close leads to undercounting, and placing them far apart leads to miscalculations such as double counting. Yanni et al. [3] and Tiang et al. [4] have done studies employing WiFi and analyzing the phase, but they are not ideal as WiFi easily get distorted by other objects leading the system for inaccurate results. In this deep learning and image processing era, several works have been carried out to count people. In [5], they introduced an image processing-based method employing background subtraction and blob detection to track objects, but it suffered from sensitivity to illumination changes and false positives caused by non-human objects in the frame. Li Guangqin et al. [6] proposed a depth camera-based solution to mitigate illumination issues but still faced challenges with false positives, while [7], inspired by [6], used depth cameras for background subtraction and 3D reconstruction for person detection, yet struggled with detecting individuals outside predefined human model dimensions. In response to challenges with traditional image processing methods, researchers have explored deep learning approaches for person detection and tracking. Guojin et al. [8] utilizes a convolutional neural network (CNN) for head detection and a spatio-temporal context tracking algorithm but faces issues with computational efficiency and sensitivity to head rotations, while [9] proposes “cluster pruning” to enhance real-time performance, achieving improved frames per second (FPS) values for people counting tasks but still falling short for monitoring door crossings due to low FPS rates. Therefore, a low-cost yet reliable system with a high FPS rate is needed for real-time people counting on edge devices. To this end, we propose a new efficient tracking algorithm, a counting algorithm, and a fine-tuned model for object detection in any complex environment. With these proposed methods we obtain higher FPS rates even when there are more than two people in the frame in both good and low lighting conditions. This achieves an overall accuracy of 97% in real-time video testing which is a 2% improvement compared to the best state-of-the-art and high frame rate: 20-27 FPS on average."
https://arxiv.org/html/2411.10071v1,Evidential Federated Learning for Skin Lesion Image Classification,"We introduce FedEvPrompt, a federated learning approach that integrates principles of evidential deep learning, prompt tuning, and knowledge distillation for distributed skin lesion classification. FedEvPrompt leverages two sets of prompts: b-prompts (for low-level basic visual knowledge) and t-prompts (for task-specific knowledge) prepended to frozen pre-trained Vision Transformer (ViT) models trained in an evidential learning framework to maximize class evidences. Crucially, knowledge sharing across federation clients is achieved only through knowledge distillation on attention maps generated by the local ViT models, ensuring enhanced privacy preservation compared to traditional parameter or synthetic image sharing methodologies. FedEvPrompt is optimized within a round-based learning paradigm, where each round involves training local models followed by attention maps sharing with all federation clients. Experimental validation conducted in a real distributed setting, on the ISIC2019 dataset, demonstrates the superior performance of FedEvPrompt against baseline federated learning algorithms and knowledge distillation methods, without sharing model parameters. In conclusion, FedEvPrompt offers a promising approach for federated learning, effectively addressing challenges such as data heterogeneity, imbalance, privacy preservation, and knowledge sharing.","In recent decades, deep learning has played a leading role in medical image analysis, including skin lesion classification. However, most of the existing methods rely on centralized learning, assuming data uniformity and accessibility, which often does not align with the reality of decentralized and privacy-sensitive clinical settings. This disparity not only limits progress in the field, but also exacerbates inequalities, with wealthier regions having a data advantage over poorer areas, leading to disparities in model performance and clinical support. Federated learning (FL) emerges as a promising solution to this challenge, enabling model training across distributed devices while preserving data privacy. Methods like FedAvg McMahan et al. (2017) and FedProx Li et al. (2020) have addressed issues such as non-i.i.d. data and system heterogeneity, yet they still face obstacles, particularly in scenarios with class imbalances and data heterogeneity. Evidential Deep Learning (EDL) Sensoy et al. (2018a) has found adoption in FL to handle these limitations in medical data, thereby enhancing model confidence and reliability, crucial for clinical applications. For example, the recent work on uncertainty-aware aggregation of federated models for diabetic retinopathy classification demonstrates its efficacy in improving model performance and reliability Wang et al. (2023). Furthermore, the scarcity of data poses an additional significant challenge, often leading to model overfitting and suboptimal federation performance. Recent techniques like learnable prompting Li and Liang (2021), particularly effective in low-data regimes, offer a promising solution by facilitating personalized model tuning across distributed clients Li et al. (2023). Nonetheless, privacy concerns persist, particularly due to the sharing and aggregation of model parameters, which poses the risk of reconstructing training images, as demonstrated by recent studies Zhang et al. (2023); Geiping et al. (2020); Zhu et al. (2019). To mitigate these concerns, one strategy involves sharing suitably-constructed synthetic data generated through generative models Pennisi et al. (2024). Yet, the use of generative models carries its own risks, potentially incorporating and synthesizing sensitive training samples, thus exacerbating privacy concerns. We here propose FedEvPrompt, a novel approach that integrates principles of evidential deep learning, prompt tuning, and knowledge distillation to address existing limitations comprehensively. FedEvPrompt leverages prompts prepended to pre-trained ViT models trained in an evidential learning setting, maximizing class evidence. Knowledge sharing across federation clients is achieved only through knowledge distillation on attention maps generated by ViT models, which offers greater privacy preservation compared to sharing parameters or synthetic images, as it lacks pixel-level details and reconstructive qualities. While our approach maintains a high level of abstraction for minimizing privacy leaks, it also provides richer information than average logits, as in FedDistill Seo et al. (2022), or prototypes, as in FedProto Tan et al. (2022). Thus, FedEvPrompt represents a principled way to share insights into the decision-making process of local models for enhanced federated performance, as demonstrated by the results achieved on a real-word distributed setting for skin lesion classification."
https://arxiv.org/html/2411.10070v1,Step-wise Distribution Alignment Guided Style Prompt Tuning for Source-free Cross-domain Few-shot Learning,"Existing cross-domain few-shot learning (CDFSL) methods, which develop training strategies in the source domain to enhance model transferability, face challenges when applied to large-scale pre-trained models (LMs), as their source domains and training strategies are not accessible. Besides, fine-tuning LMs specifically for CDFSL requires substantial computational resources, which limits their practicality. Therefore, this paper investigates the source-free CDFSL (SF-CDFSL) problem to solve the few-shot learning (FSL) task in target domain with only pre-trained model and a few target samples without accessing source data and training strategy. However, the inaccessibility of source data prevents explicitly reducing the domain gaps between the source and target. To tackle this challenge, this paper proposes a novel approach, Step-wise Distribution Alignment Guided Style Prompt Tuning (StepSPT), to implicitly narrow the domain gaps from the perspective of prediction distribution optimization. StepSPT initially proposes a style prompt that adjusts the target samples to mirror the expected distribution. Furthermore, StepSPT tunes the style prompt and classifier by exploring a dual-phase optimization process that combines external and internal processes. In the external process, a step-wise distribution alignment strategy is introduced to tune the proposed style prompt by factorizing the prediction distribution optimization problem into a multi-step simple distribution alignment problem. In the internal process, the classifier is updated via standard cross-entropy loss. Evaluation on 5 datasets illustrates the superiority of StepSPT over existing prompt tuning-based methods and SOTAs. Furthermore, ablation studies and performance analyzes highlight the efficacy of StepSPT. The code will be made public at https://github.com/xuhuali-mxj/StepSPT.","Cross-domain few-shot learning (CDFSL) [1] aims to address the target task with limited data by leveraging a vast dataset from a different domain, i.e. source domain. Existing CDFSL methods [2, 3, 4, 5, 6, 7, 8, 9] tackle the target FSL task by utilizing massive data from alternative (source) domains and the corresponding training or adapt strategy, which requests the source data to be accessible and the training strategy can be designed. However, these methods may not always be feasible in real-world scenarios [10] due to the following reasons: (1) The computational burden of training with a large source dataset, particularly for edge devices, poses a significant challenge. (2) Concerns regarding confidentiality, privacy, and copyright may render the source dataset inaccessible. (3) Large-scale pre-training models (LMs) [11, 12, 13, 14, 15, 16], renowned for their robust generalization capabilities, is crucial for CDFSL. Existing methods fail in LMs application scenarios, where source data is inaccessible and training strategy cannot be designed. To address the above mentioned problems, researchers delve into the exploration of a novel Source-Free CDFSL (SF-CDFSL) [17] problem. SF-CDFSL address the FSL task in the target domain exclusively through an existing pretrained model (referred to as the source model), devoid of any access to the source data and training strategy. Figure 1 indicates the differences between the vanilla CDFSL and SF-CDFSL. By relaxing the requirements on accessing source domain data and designing training strategies, the exploring of SF-CDFSL, on the one hand, protects source data privacy, which promote the development of fields with high data privacy requirements, such as medicine [18, 19], remote sensing [20, 21], etc. On the other hand, SF-CDFSL can ignore the source data transmission costs [10, 22]. Moreover, the reduced restrictions on training strategy design make it feasible to apply LMs to CDFSL tasks. However, in addition to the challenges inherited from CDFSL, SF-CDFSL has unique challenges [17]: Firstly, in contrast to CDFSL utilizes source and target data, SF-CDFSL relies solely on scarce labeled target data to tackle the FSL challenge, making overfitting easily. Secondly, bridging domain disparities by explicitly aligning source and target distributions becomes unattainable given the unknown of the source data distribution. Figure 1: The difference between CDFSL and SF-CDFSL. A: The CDFSL setup. B & C: The CDFSL baselines and existing solutions, e.g. introducing the source/target data into the adapt/training process, or exploring the training strategy. D: The SF-CDFSL setup. E: The proposed StepSPT makes both the pretrained source model and target domain adapt to each other through the learnable style prompt and classifier. To address these challenges, instead of explicitly reducing the gaps between source and target distributions, this paper aims to implicitly reduce these gaps by optimizing the prediction distribution. Using LMs as the source model, we propose Step-wise Distribution Alignment Guided Style Prompt Tuning (StepSPT) to solve the FSL task in the target domain. First, inspired by the ability of batch normalization to adjust style and influence data distribution [23], StepSPT designs a learnable style prompt to reduce the difference between the target and expected distributions, while also constraining the complexity of the hypothesis space. Second, StepSPT utilizes a dual-phase optimization process consisting of external and internal stages. In both phases, the parameters of the LMs remain frozen, while the style prompt and classifier parameters are updated iteratively. In the external phase, the classifier parameters are frozen, and the transductive learning is introduced to achieve the distribution alignment. Through in-depth analysis of the target domain distribution optimization problem [24, 25], we found that it can be transformed into a multi-step distribution alignment problem. Thus, a step-wise distribution alignment strategy is proposed to optimize the prediction distribution incrementally. This means during the training process, the distribution difference between each two adjacent steps is small. Therefore, this paper gradually reduce the gaps between target and ideal distribution by constraining the domain gap between each two steps. The query set is introduced into the external process. Since the data in the query set are unlabeled, the step-wise distribution alignment strategy uses the data distribution of the previous step as the label and aligns the distribution of the current step to the distribution of the previous step. To ensure the distribution optimization of the subsequent step is not misled by the previous step’s distribution during the alignment process, a credible group is formed in the prior step to identify and select trustworthy features for alignment. The prompt learned by multiple alignment steps to make the target distribution gradually approach the ideal distribution. In the internal phase, a standard meta-training strategy is applied to the support set to update the classifier while keeping the prompt parameters fixed. This phase helps the model adapt to the specific characteristics of the target domain. The contributions of this paper are as follows: • We focus on a new SF-CDFSL problem, and transform the domain alignment challenge between the source and target domains in SF-CDFSL into a target distribution optimization problem. Furthermore, we provide a theoretical analysis and guidance to address this distribution optimization. • We propose the Step-wise Distribution Alignment Guided Style Prompt Tuning (StepSPT) method to tackle the SF-CDFSL problem. StepSPT introduces a style prompt to adjust the target distribution and employs a dual-phase optimization process including external and internal stages, where the style prompt and classifier are alternately updated in the external and internal processes. In the external process, we make the target distribution close to the ideal distribution, while let the model adapt to the target FSL task in the internal process. Specifically, in external process, through theoretical analysis, we turn the difficult one-step alignment into simple multi-step alignment, Based on the this, we explore a step-wise distribution alignment strategy to learn the proposed style prompt. Besides, a credible group is introduced to avoid misleading information during alignment, while the traditional cross-entropy loss is used in the internal process to update the classifier. • Extensive evaluation on 5 datasets indicates the superior performance of the proposed StepSPT, with detailed ablation study illustrating the contribution of each component."
https://arxiv.org/html/2411.10036v1,"Rethinking Normalization Strategies and Convolutional Kernels 
for Multimodal Image Fusion","Multimodal image fusion (MMIF) aims to integrate information from different modalities to obtain a comprehensive image, aiding downstream tasks. However, existing methods tend to prioritize natural image fusion and focus on information complementary and network training strategies. They ignore the essential distinction between natural and medical image fusion and the influence of underlying components. This paper dissects the significant differences between the two tasks regarding fusion goals, statistical properties, and data distribution. Based on this, we rethink the suitability of the normalization strategy and convolutional kernels for end-to-end MMIF. Specifically, this paper proposes a mixture of instance normalization and group normalization to preserve sample independence and reinforce intrinsic feature correlation. This strategy promotes the potential of enriching feature maps, thus boosting fusion performance. To this end, we further introduce the large kernel convolution, effectively expanding receptive fields and enhancing the preservation of image detail. Moreover, the proposed multipath adaptive fusion module recalibrates the decoder input with features of various scales and receptive fields, ensuring the transmission of crucial information. Extensive experiments demonstrate that our method exhibits state-of-the-art performance in multiple fusion tasks and significantly improves downstream applications. The code is available at https://github.com/HeDan-11/LKC-FUNet.","In reality, single-modal images capture limited information and each modality contains essentially different information [9]. Image fusion technology fully integrates the complementary information of different modalities to generate a comprehensive representation of the image [52]. It is widely used for scene information enhancement or restoration [23, 24, 51]. Moreover, object detection [60, 59] and semantic segmentation [25, 27] can also benefit from clearer representations of scenes and objects in the fused images [61]. Fusion tasks include infrared and visible image fusion (IVIF)[18, 47], medical image fusion (MIF) [29, 34], multi-exposure image fusion [19], and so on, where IVIF and MIF are very similar and challenging subcategories [40]. Specifically, in IVIF, fused images can avoid the drawbacks of visible (VIS) images that are sensitive to illumination conditions and infrared (IR) images that are noisy and low resolution [24, 63]. Similarly, MIF generates images that comprehensively reflect the information of tissues, organs, and metabolism to assist medical diagnosis and improve reliability [55]. (a) Histogram analysis of varied images. SD: standard deviation. (b) Impact of diverse normalization methods and large kernel convolution (LKC) on fusion performance. Figure 1: Evaluating pixel intensity distribution discrepancies and fusion results exhibition. In recent years, many deep learning methods [32, 38, 50, 56, 58, 64] have been developed to address challenges in image fusion. Common encoder-decoder models demonstrating promising results utilize convolutional neural networks (CNNs) [54, 57] and Transfomer [21, 33, 65] to extract features and reconstruct images. It has achieved relatively satisfactory convergence performance in IVIF. However, it has focused more on the improvement of objective indicators in MIF, without sufficiently considering the needs of MIF itself. In fact, there are the following significant differences between IVIF and MIF: 1) Fusion goals: IVIF emphasizes the overall structure and salient objects. Whereas in MIF, details and structural information must be fully retained, and tiny textures may indicate important lesion information. 2) Data distribution. Fig. 1(a) shows that IR and VIS images are Gaussian distributed, while medical images are highly sparsely distributed. 3) Statistical properties. Medical images possess more complex details, and their statistical values, such as average gradient (AG), spatial frequency (SF), and standard deviation (SD), are far higher than those of IR and VIS images. 4) Intra-task inter-sample differences. Fig. 1(a) shows that the pixel distributions of VIS images differ significantly between daytime and nighttime scenes. Meanwhile, computed tomography (CT) images have more high-brightness regions compared to magnetic resonance imaging (MRI) and positron emission tomography (PET) images, which might result in mutual interference during the fusion process. Some methods [5, 63] are trained to achieve only unified fusion on IR and VIS images. These methods attain excellent performance in IVIF but severe sacrifice of details in medical images, clearly overlooking differences 1-3. SDNet [54] and CDDFuse [61] are trained with different parameters to deal with the two tasks. Nevertheless, the emphasis continues to be on IVIF, while for MIF, only its model parameters are adjusted. This neglects the limited effective feature and the strict fusion requirements for detail retention in MIF and fails to solve the conflict between simultaneously retaining the high-brightness regions in CT/functional images and the detail information in MRI. We aim to rethink the fitness of the underlying components in this discrepancy case and design suitable models to resolve fusion conflicts and reduce inter-sample interference during training. Firstly, early fusion frameworks [8, 57] are derived from advanced vision tasks, in which the employed batch normalization (BN) seeks to normalize the feature distribution across the entire batch; but this approach ignored the independence of samples, leading to data smoothing. This has less impact on IVIF which emphasizes structure preservation. However, for medical images that are highly sparsely distributed and require strict detail retention, the interaction between samples will also cause a conflict between regions of high brightness and detail retention. While some methods [5, 12, 14, 25] forgo normalization to preserve sample independence, they fail to account for the inherent properties of images and the intrinsic relationships between features, resulting in limited improvements in fusion performance. Secondly, large kernel convolution (LKC) can capture spatial information within a wider range and is crucial for preserving image structure and details. However, its exploration in image fusion is limited, which may be related to its performance bottleneck. As depicted in Fig. 1(b), when BN is applied, the interaction among samples results in data smoothing, further reducing effective features. Here, the large receptive fields of LKC have a limited or even hindering effect on detail retention. Not using normalization fails to offer better image features and makes it hard to raise the upper limit of fusion performance. Finally, when using UNet [35], simple skip connections do not consider the relative importance of feature maps in different paths during the fusion process. This may result in crucial features being neglected. To address the above issues, we focus on exploring an efficient UNet to cope with the challenges of substantial differences among tasks and limited feature extraction. We employ a mixture of IN and GN to fully consider the sample independence, image properties, and intrinsic connections among features. This strategy enhances the generation of rich feature maps while more effectively preserving the distinctive attributes of source image pairs, as illustrated in Fig. 1(b). The strategy elevates the upper bound of fusion performance, at this point, the application of LKC enhances detail retention capabilities. Finally, the feature maps in the input decoder are recalibrated by combining spatial, channel attention, and bidirectional interactions. We fully consider four critical differences, thereby guaranteeing not only outstanding performance in MIF, but also full applicability to IVIF. Our contributions are summarized as follows: • A UNet with LKC is proposed to achieve multimodal image fusion (MMIF), namely LKC-FUNet, including IVIF and MIF. • Rethink the impact of normalization and LKC on image fusion. Verify the inappropriateness of BN in MMIF. Mixing IN and GN preserves image properties and is well suited for highly sparsely distributed fusion tasks. Under the above strategy, LKC enlarges the ”effective receptive field” to better preserve the detailed information and significantly improve the fusion performance. • A multipath adaptive fusion module is designed for feature fusion across different receptive fields and at various scales. Spatial-channel dual-attention feature maps, bidirectional interactions, and recalibration are used to provide more comprehensive inputs to the decoder. • Our method significantly improves multiple metrics in both tasks and achieves breakthroughs in MIF visualization. It is also shown to facilitate downstream multimodal object detection and semantic segmentation. Figure 2: Overall architecture of LKC-FUNet."
https://arxiv.org/html/2411.10033v1,GSEditPro: 3D Gaussian Splatting Editing with Attention-based Progressive Localization,"With the emergence of large-scale Text-to-Image(T2I) models and implicit 3D representations like Neural Radiance Fields (NeRF), many text-driven generative editing methods based on NeRF have appeared. However, the implicit encoding of geometric and textural information poses challenges in accurately locating and controlling objects during editing. Recently, significant advancements have been made in the editing methods of 3D Gaussian Splatting, a real-time rendering technology that relies on explicit representation. However, these methods still suffer from issues including inaccurate localization and limited manipulation over editing. To tackle these challenges, we propose GSEditPro, a novel 3D scene editing framework which allows users to perform various creative and precise editing using text prompts only. Leveraging the explicit nature of the 3D Gaussian distribution, we introduce an attention-based progressive localization module to add semantic labels to each Gaussian during rendering. This enables precise localization on editing areas by classifying Gaussians based on their relevance to the editing prompts derived from cross-attention layers of the T2I model. Furthermore, we present an innovative editing optimization method based on 3D Gaussian Splatting, obtaining stable and refined editing results through the guidance of Score Distillation Sampling and pseudo ground truth. We prove the efficacy of our method through extensive experiments. {CCSXML} <ccs2012> <concept> <concept_id>10010147.10010371.10010372</concept_id> <concept_desc>Computing methodologies Rendering</concept_desc> <concept_significance>500</concept_significance> </concept> <concept> <concept_id>10010147.10010371.10010396.10010400</concept_id> <concept_desc>Computing methodologies Point-based models</concept_desc> <concept_significance>500</concept_significance> </concept> <concept> <concept_id>10010147.10010178.10010224.10010240</concept_id> <concept_desc>Computing methodologies Computer vision representations</concept_desc> <concept_significance>500</concept_significance> </concept> </ccs2012> \ccsdesc[500]Computing methodologies Rendering \ccsdesc[500]Computing methodologies Point-based models \ccsdesc[500]Computing methodologies Computer vision representations","In the rapidly evolving field of computer graphics, developing user-friendly methods for 3D generation and editing is crucial, as these methods can be widely applied in domains such as virtual reality and digital gaming. In the field of 3D generation, text-based model generation technology [PJBM22, LGT∗22, MRP∗23, WLW∗24] has made significant progress due to the success of large-scale Text-to-Image (T2I) models [SCS∗22, RDN∗22, YXK∗22]. These models demonstrate remarkable creativity and significantly reduce the cost of model generation, gaining increasing attention. However, they often lack editing abilities, and even slight text prompt variations may lead to different output results. In addition to 3D generation tasks, editing existing 3D models is also crucial, which enables efficient and precise modifications to 3D models, thereby increasing the flexibility and adaptability of the existing models for various applications. In recent years, the emergence of implicit 3D representation Neural Radiance Fields (NeRF)[MST∗21] has made significant progress in scene reconstruction and novel view synthesis. The high-fidelity rendering ability of NeRF and its significant scalability provide excellent support for subsequent work. Consequently, most text-driven 3D editing techniques[HTE∗23, MPS∗23, WCH∗22] have been designed based on NeRF for quite some time. However, editing neural radiance fields is difficult due to its implicit encoding of shape and texture information in high-dimensional neural network features. Thus, accurate locating and direct modification during the editing process are challenging, hindering the obtainment of precise and high-quality editing results, thereby impeding their practical applications. A pioneering work recently emerging in the field of 3d reconstruction is 3D Gaussian Splatting (3D-GS)[KKLD23], a real-time rendering technology based on explicit representation. The explicit nature of 3D-GS gives it a significant advantage in editing tasks. Each 3D Gaussian distribution exists independently, allowing for editing 3D scenes easily by directly manipulating the 3D Gaussians required for editing constraints. Recently, some editing methods[CCZ∗23, FWZ∗23, YDYK23, ZKC∗24] based on 3D-GS have emerged. However, they still encounter various issues such as inaccurate locating or requiring users to manually locate editing areas in some cases[ZKC∗24, CCZ∗23], difficulty in ensuring consistency of non-editing areas before and after editing[YDYK23, CCZ∗23], inability to perform object insertion operations effectively[CCZ∗23, FWZ∗23], and failure to guarantee consistency between different viewpoints after editing[FWZ∗23]. To overcome these issues, we propose a novel text-driven editing framework based on 3D-GS called GSEditPro, which enables users to perform 3D editing intuitively and precisely using text prompts. Our framework achieves this through two key designs: (1) Attention-based editing area localization in 3D: We leverage the explicit representation advantage of 3D-GS to classify Gaussians based on their relevance to the attention maps derived from cross-attention layers of T2I models, assigning semantic labels to each Gaussian, thereby obtaining accurate 3D editing mask areas. (2) Guidance for a detailed Optimization from DreamBooth and pseudo-GT images: We create an optimization process that balances generative capability and detail preservation. It uses simple text prompts to effectively perform 3D scene editing by conducting score distillation sampling within the 3D editing mask area, thus ensuring high-quality editing. Additionally, we maintain the details of the scenes by constructing pseudo-GT images to ensure consistency of the irrelevant regions using pixel-level guidance. We conducted experiments using the proposed method in various synthetic and real-world 3D scenes. The experiments demonstrate that our editing method can achieve precise editing both on object changing and object insertion with irrelevant areas naturally preserved after editing. Furthermore, since editing is accomplished through simple text prompts, our method is highly user-friendly, showcasing significant practical application potential. Qualitative and quantitative comparisons also indicate that our method outperforms previous methods in terms of editing accuracy, visual fidelity, and user satisfaction. Our contributions can be summarized as follows: 1. We propose GSEditPro, a novel 3D editing method that enables users to perform various creative and precise editing operations using only text prompts. This approach is more convenient than previous Gaussian editing methods, which require additional user input as prior. Our extensive experiments demonstrate that our framework still offers advantages in both qualitative and quantitative metrics with user-friendly interactions. 2. We design a method to add semantic labels to Gaussians using the cross-attention mechanism of the T2I model and the explicit representation advantage of 3D-GS. Our special attention-based localization module assists in achieving more accurate 3D editing area localization and more effective editing control. 3. We present how to preserve details with pixel-level guidance, which creates a pseudo-GT image using our localization module to minimize unnecessary modifications and guide 3D Gaussian rendering for more detailed results."
https://arxiv.org/html/2411.10028v1,MOT_FCG++: Enhanced Representation of Motion and Appearance Features,"The goal of multi-object tracking (MOT) is to detect and track all objects in a scene across frames, while maintaining a unique identity for each object. Most existing methods rely on the spatial motion features and appearance embedding features of the detected objects in consecutive frames. Effectively and robustly representing the spatial and appearance features of long trajectories has become a critical factor affecting the performance of MOT. We propose a novel approach for appearance and spatial feature representation, improving upon the clustering association method MOT_FCG. For spatial motion features, we propose Diagonal Modulated GIoU, which more accurately represents the relationship between the position and shape of the objects. For appearance features, we utilize a dynamic appearance representation that incorporates confidence information, enabling the trajectory appearance features to be more robust and global. Based on the baseline model MOT_FCG, we achieved 76.1 HOTA, 80.4 MOTA and 81.3 IDF1 on the MOT17 validation set, and also achieved competitive performance on the MOT20 and DanceTrack validation sets.","Multi-object tracking (MOT) aims to identify dynamic objects in each frame of a given video sequence and assign the same identity identifier to the same object across consecutive frames, thereby forming individual motion trajectories for different objects. The goal of MOT is to simultaneously track and recognize multiple targets in a scene, demonstrating enormous potential in fields such as video surveillance, intelligent transportation, autonomous driving, and sports broadcasting. Detection-based tracking methods (DBT) are mainstream methods for MOT, typically divided into two stages: detection and tracking. In the first stage, a convolutional neural network-based detector is used to label the objects of interest in each frame of the video sequence with bounding boxes. In the second stage, the tracker utilizes the output from the detector to extract the appearance and spatial motion features of the objects again, and calculates the similarity between these detection features and the existing trajectory features to perform trajectory association. How to effectively represent the appearance and spatial motion features of trajectories is a key factor affecting the performance of DBT methods. In this paper, we improve upon the clustering association method MOT_FCG by proposing a more global appearance embedding representation method and a more precise spatial motion information representation method. Ablation experiments validate the effectiveness of our proposed modules, and as shown in Figure 1 we achieve excellent performance on the MOT17 datasets. The main contributions of this paper are threefold: \bullet We point out that the use of median elements as trajectory appearance embedding features in MOT_FCG has limitations. To address this, we adopt a dynamic appearance embedding representation method. This method can adaptively adjust the weighting based on confidence. It integrates the global appearance embedding features of the trajectory, leading to a more comprehensive and holistic representation. \bullet To more accurately represent spatial motion information, we propose a new spatial metric method—Diagonal Modulated GIoU. This method can more precisely characterize the positional relationships between objects and to some extent reflect the shape information of the object bounding boxes. \bullet We achieved competitive results on the MOT17 and MOT20 datasets, and also validated the effectiveness of the proposed modules on the DanceTrack dataset."
https://arxiv.org/html/2411.10019v1,Towards Utilising a Range of Neural Activations for Comprehending Representational Associations,"Recent efforts to understand intermediate representations in deep neural networks have commonly attempted to label individual neurons and combinations of neurons that make up linear directions in the latent space by examining extremal neuron activations and the highest direction projections. In this paper, we show that this approach, although yielding a good approximation for many purposes, fails to capture valuable information about the behaviour of a representation. Neural network activations are generally dense, and so a more complex, but realistic scenario is that linear directions encode information at various levels of stimulation. We hypothesise that non-extremal level activations contain complex information worth investigating, such as statistical associations, and thus may be used to locate confounding human interpretable concepts. We explore the value of studying a range of neuron activations by taking the case of mid-level output neuron activations and demonstrate on a synthetic dataset how they can inform us about aspects of representations in the penultimate layer not evident through analysing maximal activations alone. We use our findings to develop a method to curate data from mid-range logit samples for retraining to mitigate spurious correlations, or confounding concepts in the penultimate layer, on real benchmark datasets. The success of our method exemplifies the utility of inspecting non-maximal activations to extract complex relationships learned by models.","Many previous interpretability works have gained valuable insights by taking high levels of neuron activations [64, 2, 8] and concept projections [65, 24, 17]. Analysing the highest activation levels necessarily removes a level of complexity that makes it manageable to gain insight into representations encoded by neurons, and concepts contained by groups of neurons within layers. This coarse-grain analysis has yielded invaluable insights into the nature of neural network representations through finding human interpretable concepts. However, models are a function of the data they are trained on, so in reality the representations of concepts are often not disentangled. It is well known that they may not separate the interpretable, independent and informative factors of variations in the data [11, 4]. Understanding how these factors intersect can inform us of the relationships between concepts in a model and the robustness of its performance. The meaning and utility of non-maximal activations of neurons and directions, more generally, have not received the same level of attention as maximal activations. Explanations for various levels of neuron activations is a nascent subfield of interpretability research [41], which we will argue is an underused technique to understand model representations. This paper focuses on understanding some of the complexities contained in deep network representations that are not easily extracted from maximal activations. There are many popular approaches that focus on the highest activation levels, including visualisation and inversion procedures. These work by iteratively altering an initially noisy input to obtain an input that maximises a neuron’s activation [64, 63, 15, 49]. Dataset-based approaches are also commonly used. These techniques typically select samples based on which samples activate a neuron or concept vector the most [2, 3, 59, 65, 24]. This paper takes the dataset-based approach, where we obtain insights by looking at dataset examples, including samples outside the highest activation range, in order to comprehend representational associations. 1We study the advantage of this lens in the penultimate embedding layer and the output neurons of a model in a classification setting. We show that considering different levels of output neuron activations can not only highlight mislabels and atypical inputs, but can afford insight into the entanglement of concepts allowing us to find and mitigate spurious patterns learned by a model. Figure 1: An illustration of the spurious correlation data analysis and automatic selection for retraining to mitigate the found spurious bias in the CelebA dataset. 1. A model is trained using the standard empirical risk minimisation (ERM) approach, which typically absorbs spurious correlations. 2. We select an output class and order the logits by magnitude. Samples within the mid-level range where the model has lower prediction confidence in its prediction may include many mislabels, low spurious images, and counterexamples to the spurious trend. We filter the data to keep only mid-range activating examples. 3. We cluster the corresponding embeddings from the penultimate layer. 4. In this way, less spurious data is selected to retrain the model, such as by simply fine-tuning the classification layer. To better understand the extent of the connection between activation levels and confounding concepts, we take the case of output neurons, and first examine harmful spurious patterns - patterns that are correlated with the data class labels but are inherently irrelevant to the task - to the data class labels related to shortcuts in classification [60]. This phenomenon of shortcuts permeates the landscape of deep learning [29, 84]. Models learn patterns from their training data and may not preferentially use the same semantic features as humans [20]. For example, models have been found to take advantage of background cues [70] or co-occurring object locations [20] if such patterns are present in the training data. See further discussion in Sec. A.1 We begin with a motivating demonstration of a scenario where inspecting maximal examples fails to highlight a known bias learned by the model, whereas exploring mid-range activating samples reveals samples containing the bias. In particular, we consider a toy setting of synthetic data with disjoint sets of core (shape of the object to be classified, i.e., square, circle, oval) and spurious features (location of object, i.e., left, right, or middle of the image). Here, in a setting where factors of variation in the data are known to be correlated with the ground truth label, we demonstrate that the complexity of deep network representations is not easily extracted from maximal activations. In this setting, we make observations about the nature of spurious correlations that inspire our experiments on two spurious feature benchmark datasets (CelebA [44] and Waterbirds [88, 89]). These experiments serve to present empirical evidence that non-extremal activations are useful 1) to identify mislabeled samples; 2) to characterise the nuances of the representations learned by a model, such as a concept’s representational relationship to other concepts, and 3) to select samples useful to finetune a model so that the model becomes less reliant on spurious shortcuts. We validate the advantage of exploring mid-range activating examples by identifying counter-spurious data, i.e., groups of data that break the spurious trend, and fine-tuning to improve model performance on such minority groups. We illustrate our lens and method to identify mislabels, spurious and counter-spurious data, and data we employ to mitigate spurious correlations a model has learned in Fig. 1, which will be discussed in detail in Sec. 5.1. In this manuscript, we make a step towards utilising the rich information contained in non-maximal levels of neuron activations and concept projections by evidencing the utility of a range of activations in recognising potential confounding concepts and data mislabels. We employ our insights to develop a simple method to identify and mitigate potential spurious correlations. We hope our contributions grant insight into the messy, non-binary nature of neural representations, emphasise the need for examining various levels of the relevant unit of interpretability, and inspire further work in connecting interpretability research and confounding concepts. Our code will be made publicly available111https://github.com/lauraaisling/MID-level-activations."
https://arxiv.org/html/2411.10015v1,"MicroCrackAttentionNeXt: 
Advancing Microcrack Detection in Wave Field Analysis Using Deep Neural Networks through Feature Visualization","Micro Crack detection using deep neural networks(DNNs) through an automated pipeline using wave fields interacting with the damaged areas is highly sought after. However, these high dimensional spatio-temporal crack data are limited, moreover these dataset have large dimension in the temporal domain. The dataset presents a substantial class imbalance, with crack pixels constituting an average of only 5% of the total pixels per sample. This extreme class imbalance poses a challenge for deep learning models with the different micro scale cracks, as the network can be biased toward predicting the majority class, generally leading to poor detection accuracy. This study builds upon the previous benchmark SpAsE-Net, an asymmetric encoder–decoder network for micro-crack detection. The impact of various activation and loss functions were examined through feature space visualisation using manifold discovery and analysis (MDA) algorithm. The optimized architecture and training methodology achieved an accuracy of 86.85%.","Micro crack detection in materials is of significant importance due to the potential for catastrophic failures, which can lead to substantial financial losses and safety hazards in industries (Malekloo et al., 2022; Golewski, 2023). Detecting cracks in complex structures, like aircraft bodies or intricate machinery components, poses a substantial challenge using conventional methods like visual inspection or standard cameras, especially when dealing with complex geometries. The use of wave-based approaches for crack detection offers a powerful solution, as these methods allow for the analysis of structures that are not easily accessible or too complex to inspect manually. Convolutional Neural Networks (CNNs) are especially good at processing spatial data due to their ability to capture local spatial correlations within an image (LeCun et al., 2015). Nevertheless, standard segmentation methods, such as vanilla architectures, demonstrate limited performance on this particular dataset, due to the complex spatio-temporal nature of the crack patterns. This becomes even more significant when the cracks represent a tiny minority in the dataset, leading to poor detection accuracy. This issue is enhanced when dealing with very small cracks, as they not only lead to data imbalance but may also cause minimal disruption in wave behaviour. In such cases, the waves may exhibit minimal changes, making it difficult for the model to detect the cracks accurately. This challenge necessitates the development of a more tailored custom model. Our proposed MicroCrackAttentionNeXt is designed to overcome the limitations of vanilla models like UNet by incorporating enhanced spatial and temporal feature extraction. Unlike UNet Ronneberger et al. (2015), where the input and target share the same modality (image-to-image translation). Our model processes spatio-temporal input data and outputs spatial crack predictions, enabling it to handle more complex data while improving micro-scale detection accuracy. The asymmetric encoder-decoder structure, with attention layers is particularly effective as it focuses on capturing critical crack patterns rather than relying heavily on skip connections. The attention mechanism ensures that the model prioritizes the time steps when the waves interact with the cracks, improving detection precision. The DNNs capacity to recognise minute details and complex patterns in high dimensional data is impacted by the activation functions used, which becomes crucial in the micro-scale setting where accuracy is much needed. Activation functions enhance the network’s expressive power, enabling it to capture diverse features and representations. Rectified Linear Unit (ReLU) Nair and Hinton (2010) and its variants are commonly used activation functions. ReLU introduces non-linearity by setting negative values to zero, allowing positive ones to pass unchanged, which aids in deep network training. The ""dying ReLU"" issue, where neurons become inactive, hampers learning Xu et al. (2015); He et al. (2015a). Variants like Leaky ReLU mitigate this by allowing small negative slopes. SELU (Scaled Exponential Linear Unit) Klambauer et al. (2017) scales outputs to maintain self-normalizing properties, keeping activations near zero mean and unit variance. GeLU (Gaussian Error Linear Unit) Hendrycks and Gimpel (2023) enhances representation learning by incorporating probabilistic elements, though it has higher computational complexity. ELU (Exponential Linear Unit) Clevert et al. (2016) improves learning dynamics but is computationally expensive. Various Loss functions have been proposed in the literature to combat class imbalance issues in the DNN model. The loss functions tested are: 1)Dice LossLin et al. (2018), 2)Focal LossLin et al. (2018), 3)Weighted Dice LossYeung et al. (2021) and, 4)Combined Weighted Dice LossJadon (2020). These activation functions aim to strike a delicate balance between adaptability and computational efficiency, essential considerations in the micro-material domain, where capturing fine details is crucial for accurate crack detection. Empirical exploration and meticulous fine-tuning of these activation functions is imperative to identify the optimal choice that aligns with the distinctive characteristics of micro-material images. Ultimately, a nuanced and effective approach to crack detection in micro-materials relies on the thoughtful selection and optimization of activation functions within the CNN architecture. The extent of the influence of different activations is difficult to determine against conventional metrics such as accuracy and F1 score. Hence, it is imperative to analyse the internal dynamics of the model. Methods like Principal Component Analysis, t-SNE van der Maaten and Hinton (2008) and UMAP McInnes et al. (2020) are used to analyse the higher dimensional feature maps of these blackbox models against the target. However, these methods provide little to no insight when used on segmentation problems. In this study, we use the recently proposed Manifold Discovery Analysis (MDA) Islam et al. (2023) to qualitatively assess the impacts of various activation functions. Moreover, through this, we were able to analyse the effects activations had on the feature maps of the model, allowing us to choose the best activation function for the given problem. The primary contributions of this paper are: • Introducing MicroCrackAttentionNeXt – an incremental improvement over (Moreh et al., 2024). • Qualitative Investigation of the impact of activations MicroCrackAttentionNeXt through Manifold Discovery and Analysis. The paper’s structure is outlined in the following manner: Section 2 encompasses a concise yet informative overview of relevant studies. Section 3 deals with the dataset used and the proposed methodology. The assessment of the performance of the proposed system and the results obtained are included in Section 4. Ultimately, concluding remarks and future works are presented in Section 5."
https://arxiv.org/html/2411.10013v1,Efficient Depth Estimation for Unstable Stereo Camera Systems on AR Glasses,"Stereo depth estimation is a fundamental component in augmented reality (AR) applications. Although AR applications require very low latency for their real-time applications, traditional depth estimation models often rely on time-consuming preprocessing steps such as rectification to achieve high accuracy. Also, non standard ML operator based algorithms such as cost volume also require significant latency, which is aggravated on compute resource-constrained mobile platforms. Therefore, we develop hardware-friendly alternatives to the costly cost volume and preprocessing and design two new models based on them, MultiHeadDepth and HomoDepth. Our approaches for cost volume is replacing it with a new group-pointwise convolution-based operator and approximation of consine similarity based on layernorm and dot product. For online stereo rectification (preprocessing), we introduce homograhy matrix prediction network with a rectification positional encoding (RPE), which delivers both low latency and robustness to unrectified images, which eliminates the needs for preprocessing. Our MultiHeadDepth, which includes optimized cost volume, provides 11.8-30.3% improvements in accuracy and 22.9-25.2% reduction in latency compared to a state-of-the-art depth estimation model for AR glasses from industry. Our HomoDepth, which includes optimized preprocessing (Homograhpy + RPE) upon MultiHeadDepth, can process unrectified images and reduce the end-to-end latency by 44.5%. We adopt a multi-task learning framework to handle misaligned stereo inputs on HomoDepth, which reduces theAbsRel error by 10.0-24.3%. The results demonstrate the efficacy of our approaches in achieving both high model performance with low latency, which makes a step forward toward practical depth estimation on future AR devices.","Depth estimation serves as a foundational component in augmented and virtual reality (AR/VR) [16] with many downstream algorithms, which include novel-view rendering [14, 28], occlusion reasoning [17], world locking for AR object placement [18], and determining the scale of AR objects [3]. In the AR domain, stereo depth estimation is often deployed [28] rather than mono depth estimation due to its superior accuracy and the ease of deploying stereo cameras on the each side of the AR glasses frame. One key objective for depth estimation models targeting AR glasses is the low latency, in addition to the good performance, because many AR use cases are realtime applications [16]. However, since AR glasses are in a compute resource-constrained wearable form factor, enabling desired latency (less than 100 ms on device) is not trivial. One challenge towards the latency optimization originates from the preprocessing, which is a indispensable step for achieving high model performance in practical applications. Examples include camera distortion and alignment using camera intrinsic and extrinsic. In addition to image preprocessing, depth estimation models often involve traditional algorithms such as cost volume [7, 28]. We present their significance in latency in Fig. 1, which shows that preprocessing accounts for 30.2% and cost volume accounts for 29.3% of the total latency of a state-of-the-art model, Argos [28]. Note that preprocessing latency can be more dominant when camera intrinsic and extrinsic parameters are unknown, which results in 200 to 2000 ms latency to solve for the extrinsic parameters and subsequently process the images [15, 30]. This aggravates the long preprocessing latency challenge further. Therefore, to reduce depth estimation model latency for AR glasses, we propose new methodologies that significantly reduce the preprocessing (online stereo rectification not required) and cost volume latency, as shown in Fig. 1. Our approach for the cost volume is (1) to replace traditional algorithm with group-pointwise convolutions, which are highly optimized in hardware and compiler and (2) adopt an efficient approximation of consine similarity using layernorm and dot product. For preprocessing (stereo matching), we adopt homography matrix-based approximation and estimate homography using a head attached to the depth estimation model, which allows to utilize homography matrix-based approximation with dynamically varying extrinsic parameters in unstable AR glasses platform or no access to camera extrinsic parameters. Since our methodologies are complementary to existing depth estimation models, we augment our methodologies on the Argos [28] and develop two versions of new models, MultiHeadDepth and HomoDepth, to show the effectiveness of our methodologies. As presented in Fig. 1, MultiHeadDepth focuses on cost volume optimization, which reduces AbsRel by 7% and inference latency by 25% compared to the original cost volume. HomoDepth targets scenarios that require stereo matching preprocessing, which can directly accept unrectified images as input, eliminating the needs for preprocessing step. HomoDepth not only provides high-quality outputs on unrectified images as presented in Tab. 4, but also significantly reduces the end-to-end latency by 43%. Our evaluation includes ADT [20] dataset collected by real research AR glasses by Meta, Aria [5], which demonstrates the effectiveness of our approach on realistic AR glasses platform. We summarize our contributions as follows: • We develop multi-head cost volume block that replaces costly cost volume blocks in previous depth estimation models. Our new block provides significantly lower latency as well as higher accuracy compared to the original cost volume. • We introduce the homograpghy of stereo images to reveal their position relationship to enable to accept unrectified images without preprocessing. We merge a small homograpghy estimation head within the depth estimation network, which significantly reduces the latency compared to the preprocessing-based approach. • We augment our homography estimation head with 2D rectification position encoding, which helps translate relative positional information from homograpghy matrix to emdeding coding format. It enables the neural network to effectively understand relative position information, which plays a key role in eliminating the need for rectification preprocessing. Figure 1: The latency breakdown analysis of a state-of-the-art model Argos [28] and ours, MultiHeadDepth & HomoDepth on Intel Core(TM) i7-12700H CPU (laptop). The ”RPE” refers to the 2D rectification position encoding process. ”Others” refers to all the other part of the neural network excluding cost volume blocks, such as Conv, Norm, FC, and ReLU6."
https://arxiv.org/html/2411.09986v1,Unlocking Transfer Learning for Open-World Few-Shot Recognition,"Few-Shot Open-Set Recognition (FSOSR) targets a critical real-world challenge, aiming to categorize inputs into known categories, termed closed-set classes, while identifying open-set inputs that fall outside these classes. Although transfer learning where a model is tuned to a given few-shot task has become a prominent paradigm in closed-world, we observe that it fails to expand to open-world. To unlock this challenge, we propose a two-stage method which consists of open-set aware meta-learning with open-set free transfer learning. In the open-set aware meta-learning stage, a model is trained to establish a metric space that serves as a beneficial starting point for the subsequent stage. During the open-set free transfer learning stage, the model is further adapted to a specific target task through transfer learning. Additionally, we introduce a strategy to simulate open-set examples by modifying the training dataset or generating pseudo open-set examples. The proposed method achieves state-of-the-art performance on two widely recognized benchmarks, miniImageNet and tieredImageNet, with only a 1.5% increase in training effort. Our work demonstrates the effectiveness of transfer learning in FSOSR.","Few-shot learning (FSL) has got a lot of attention due to the importance in enabling models to adapt to novel tasks using a few examples (e.g., N-way K-shot: a task involving N distinct classes, each represented by K examples) [42, 17, 37, 34, 27, 10]. However, in practical applications, FSL models inevitably encounter instances that do not belong to the N classes, also known as open-set instances. Addressing this challenge has led to the emergence of the field of few-shot open-set recognition (FSOSR) [24, 16, 19, 15, 41]. In FSOSR, if two N-way K-shot FSOSR tasks have distinct closed sets, their corresponding open sets will also differ. This interdependence presents a key challenge when adapting to novel tasks in FSOSR. Namely, it is essential to redefine not only the closed set but also the open set, since the open set is inherently shaped by its closed set. Consequently, the open set lacks a universal definition across various FSOSR tasks; instead, it requires contextual consideration based on the closed set of a specific target task. Figure 1: Difficulty of straightforward extension of the transfer learning from FSL methods [34, 17] to FSOSR. Compared to the pre-trained model without transfer learning (w/o TL), in open-set recognition, [34, 17] are less effective as much as in closed-set, or even degrade the performance. Despite recent advancements in the field, current works [24, 16, 19, 15, 41] have commonly focused on leveraging prior knowledge from a large training dataset. Then, they frequently struggle to balance closed-set accuracy with open-set recognition capabilities, often prioritizing open-set recognition at the expense of closed-set accuracy. Then, these approaches face challenges in achieving broad generalization across various benchmarks. In this work, we bring attention to the novel application of transfer learning within this field. Transfer learning [14, 7, 6, 30] has been extensively studied and demonstrated its efficacy leveraging a pre-trained model to generalize it to other tasks. Recent FSL methods [31, 34, 39, 17, 35] have shown the efficacy of this approach. However, when they come to FSOSR, open-set examples are inherently not present, which significantly undermines the effect of transfer learning in terms of open-set recognition. Then, as in Fig. 1, the naive extension of the transfer learning techniques of FSL, e.g. IER-distill [34] and Label Halluc. [17] fails to attain the same level of improvement in open-set recognition as seen in closed set, or even results in decreased result. Tackling this point, we propose a two-staged FSOSR learning framework. Our method involves two stages: open-set aware meta-learning (OAL) and open-set free transfer learning (OFL). During the meta-learning stage, our objective extends beyond the meta-training of the feature encoder; we also aim to establish a universal open-set representation. This equips us with a decent starting point for the subsequent open-set free transfer learning. In the transfer learning stage, we commence by initializing the model using the parameters obtained from the meta-learning stage. To counteract the absence of open-set examples, we develop two alternative open-set sampling strategies. The first approach curates a training dataset of the previous stage as a source of open-set examples for open-set free transfer learning. For more pragmatic application, our second strategy is confined to the closed-set examples present in the target task, and exploits an episodic learning framework. Here, we generate pseudo FSOSR episodes by randomly dividing the closed-set categories into a closed set and a pseudo open set. As a result, our OAL-OFL method attains a marked enhancement in performance metrics on the standard FSOSR benchmarks as depicted in Fig. 1 while incurring a minimal extra training expense of only 1.5% compared to training without transfer learning. This allows OAL-OFL to surpass the existing state-of-the-art (SOTA) methods. Our contributions are summarized as four-fold: • We introduce a novel two-staged learning called OAL-OFL, bringing transfer learning to FSOSR for the first time with only a minimal additional cost. • We show the importance of preparing the model through open-set aware meta-learning, which is a sturdy starting point for transfer learning. • We suggest two breakthroughs to handle the lack of open-set examples during the transfer learning stage. • By leveraging the effectiveness of transfer learning, our proposed OAL-OFL achieves SOTA on miniImageNet and tieredImageNet datasets. This underscores its ability to generalize across various tasks, enhancing both closed-set classification accuracy and open-set recognition capabilities."
https://arxiv.org/html/2411.09971v1,"Explanation for Trajectory Planning using
Multi-modal Large Language Model
for Autonomous Driving","End-to-end style autonomous driving models have been developed recently. These models lack interpretability of decision-making process from perception to control of the ego vehicle, resulting in anxiety for passengers. To alleviate it, it is effective to build a model which outputs captions describing future behaviors of the ego vehicle and their reason. However, the existing approaches generate reasoning text that inadequately reflects the future plans of the ego vehicle, because they train models to output captions using momentary control signals as inputs. In this study, we propose a reasoning model that takes future planning trajectories of the ego vehicle as inputs to solve this limitation with the dataset newly collected.","Figure 1: Pipeline of our proposed method. In order to improve accountability of an ego-vehicle action, trajectory planning information is embedded as a trajectory image and combined to a camera image in a Image-Trajectory Encoder. Research on autonomous driving has been actively conducted since 2010s after the Darpa Grand Challenge [1]. The advancement of deep learning has significantly improved performance of machine learning tasks such as object detection and semantic segmentation, and has contributed to autonomous driving research. A typical architecture of a autonomous driving system is a modular system, which consists of separate components such as localization, perception, prediction, planning, and control [22]. On the other hand, an end-to-end approach[3], which integrates all components and performs from inputs of sensor data to outputs of control signals consistently, has been emerged recently[21]. It has expected to avoid cumulative errors occurred in each component of the modular system, but it is difficult to know the process of decision making in the approach. Passengers feel anxiety when they cannot understand what the ego vehicle recognizes and why it takes the action. Recent researches have tried to add a large language model (LLM) such as GPT-4[11] and generate captions describing behaviors of the ego vehicle and reasons for them in order to solve the above problem[5, 21, 6, 16, 13, 10, 14]. However, the existing methods have a limitation that they can describe only current or past actions. In this paper, we propose a method, in which a visual image and future driving plans (trajectory planning information) are combined and their fused features are used to generate accurate captions of actions which the ego vehicle will take and reasons for them. To this end, we collect and build a new dataset which includes both trajectory planning information and its captions which the existing datasets do not have, because where the ego vehicle should pay attention depends on the trajectory planning information. Let us consider the scenario in which a vehicle ahead of the ego vehicle is stopping. If the ego vehicle wants to stop in front of the vehicle ahead, it will confirm the space between them. Conversely, if the intention of the ego vehicle is to proceed by avoiding the front vehicle to the left and right sides of the road, it should pay attention to the surrounding situations, including the presence of pedestrians and vehicles in adjacent lanes. The main contributions of this paper are summarized as follows: • We present a new approach to the spatial fusion of visual information and trajectory planning information using cross attention. • We demonstrate improvement of generated captions that describe and justify future behaviors of the ego vehicle, using the fused features and a BLIP-2[7] based vision-language model. • We compile and annotate a new dataset consisting of videos, trajectory planning information, and captions for them."
https://arxiv.org/html/2411.09968v1,Seeing Clearly by Layer Two: Enhancing Attention Heads to Alleviate Hallucination in LVLMs,"The hallucination problem in multimodal large language models (MLLMs) remains a common issue. Although image tokens occupy a majority of the input sequence of MLLMs, there is limited research to explore the relationship between image tokens and hallucinations. In this paper, we analyze the distribution of attention scores for image tokens across each layer and head of the model, revealing an intriguing and common phenomenon: most hallucinations are closely linked to the pattern of attention sinks in the self-attention matrix of image tokens, where shallow layers exhibit dense attention sinks and deeper layers show sparse attention sinks. We further analyze the attention heads of different layers and find that heads with high-density attention sink in the image part play a positive role in alleviating hallucinations. In this paper, we propose a training-free method named Enhancing Attention Heads (EAH), an approach designed to enhance the convergence of image tokens attention sinks in the shallow layers. EAH identifies the attention head that shows the vision sink in a shallow layer and extracts its attention matrix. This attention map is then broadcast to other heads in the layer, thereby strengthening the layer to pay more attention to the image itself. With extensive experiments, EAH shows significant hallucination-mitigating performance on different MLLMs and metrics, proving its effectiveness and generality.","Figure 1: We found a common phenomenon through the attention map: In the range of image token, the attention head of shallow Sparse attention sink is prone to hallucination, while the attention head of Dense attention sink is much less likely to hallucinate. Multimodal large language models (MLLMs) [1, 29, 3, 46, 10, 25, 32, 13, 44] have made significant strides in cross-modal tasks, especially in handling both text and image modalities. However, hallucinations remain a persistent challenge, particularly in tasks such as Visual Question Answering (VQA) or image captioning. Current methods for addressing hallucinations often involve changing decoding strategies, incorporating external knowledge bases, or retraining models with additional data [22, 28, 30]. These approaches, however, often require significant resources and time. Recent research into attention sink has offered new insights into hallucinations. The concept of attention sink as an information flow is introduced in “Label Words are Anchors” [35], which shows how information flow often converges on a specific user token in large language models (LLMs). OPERA [17] further explores the connection between attention sink in user tokens and output tokens in MLLMs. It observes that when a token has a high attention weight across subsequent tokens, this over-reliance on the token can lead to hallucinations in the model’s outputs. Although these methods clarify the relationship between attention sink, user tokens, and output tokens, the relationship between attention sink, image tokens, and hallucination remains unclear. It’s important to note that MLLM’s output tokens are generated by the decoder based on logits, whereas input tokens, which constitute most of the input sequence, are more likely to directly reflect the MLLM’s internal mechanisms. Figure 2: Definition of dense vision sink head and its layer-wise distribution. In this case, \beta = 0.0015, \gamma = 15% Figure 3: Relationship between text tokens and the average proportion of dense vision sink heads within a single layer by layer2, analyzed across 5,000 randomly selected MSCOCO images using LLaVA1.5-7B. Most dense vision sink heads occur by layer2: As previously mentioned by FastV [5], the information flow of image tokens is primarily concentrated in the first and second layers. Building on this, we conduct experiments on the shallow layers of several models, including LLaVA1.5 [29], Minigpt4 [46], MiniGemini [26], and Intern-VL [6]. As shown in Fig. 2, we calculate the average number of dense vision sink heads across these layers to further investigate the distribution of attention sinks across different layers. We define h_{i,j} as the attention-map of a head, a “dense vision sink head” as a head (i,j) in which the proportion \alpha^{i,j} of columns in the attention map that meet the vision sink condition exceeds a threshold \gamma. Specifically, we define \alpha^{i,j} as: \text{vision sink}=\frac{\sum_{x=k}^{r}h_{i,j}[x][y]\cdot M}{r-k}>\beta,\quad k% \in[36,611], (1) \alpha^{i,j}=\frac{\text{Num(vision sinks)}}{576}, (2) A head is classified as a “dense vision sink head” if: \alpha^{i,j}\geq\gamma. (3) Observations show that most vision attention sinks occur by layer 2. Fewer dense vision sink heads lead to hallucination output: p=\frac{\text{Num(dense vision sink heads)}}{32} (4) This proportion p quantifies how many of the total 32 heads are classified as ”dense vision sink heads,” meaning they have a high proportion of columns that meet the vision sink condition within the image token range. We conducted the image captioning task on 5,000 randomly selected MSCOCO images using LLaVA1.5-7B. When the model generates a new token, we first determine whether it is a hallucination token. Then, we backtrack to layer 2 and analyze the model at the granularity of attention heads. We calculate the proportion of dense vision sink heads in layer 2 relative to the total number of heads (e.g., 32 heads in total). This analysis is repeated for layer 1, and the average across layers 1 and 2 is then computed. As shown in Fig. 3, we observe that non-hallucination tokens typically activate a larger number of dense vision sink heads, whereas hallucination tokens are generally associated with only a few dense vision sink heads, with the majority of heads being sparse. Through our analysis of different models, such as LLaVA1.5 [29], Minigpt4 [46], MiniGemini [26] and Intern-VL [6], it appears that fewer dense vision sinks heads lead to more probable hallucination output. Figure 4: (a) A example of distribution of dense vision head and the corresponding proportions/densities of vision sinks within these heads when model output hallucination token; (b) Relationship between the average skewness and CHAIRI on 150 randomly selected MSCOCO images, using LLaVA1.5-7B for captioning; (c) Comparative skewness scatter plot for Hallucination and Non-Hallucination classification on 150 randomly selected MSCOCO images, using LLaVA1.5-7B for VQA. Lower density of vision sinks and fewer vision sink heads lead to a higher probability of hallucinations: However, the average number of dense vision sink heads across shallow layers does not reveal the individual contributions of each dense vision head, some of which may be negative while others are positive for hallucination. As noted by ITI [23], in current large language models (LLMs) using transformer architecture, only a subset of attention heads plays a more significant role. Effectively optimizing these heads and leveraging them will likely lead to substantial improvements in model efficiency and overall performance. In this case, we conducted a more detailed view for each head, as shown in Figure 4 (a), the sink densities within different vision sink heads vary across the shallow layers (layer1-layer2), with an overall negatively skewed distribution. As shown in Figure 4 (b), for the image captioning task, the average skewness of the distribution of dense vision sink head and its corresponding vision sink densities in layer1 and layer2 is recorded each time a token is output. Once the output token is completed, the CHAIRI for the entire output is calculated, and the average skewness for all tokens in layer1 and layer2 is obtained. As shown in Figure 4 (c), for the VQA task (with only a single output token), the average skewness of the distribution of vision sink head and its corresponding vision sink densities in layer1 and layer2 is directly recorded for the answer token. It is observed that, regardless of the task (image captioning or VQA), a lower skewness coefficient correlates with a lower hallucination rate. In other words, a higher density of vision sinks within a dense vision sink head and a larger number of vision sink heads lead to a lower probability of hallucination. These observations highlight the critical role of attention head and vision sink distribution in understanding the attention sink phenomenon, particularly as it relates to alleviating hallucination issues in MLLMs. When the vision sink is sparse, visual tokens concentrate too heavily on specific elements, leading to reduced attention to other parts of the image. Conversely, a dense vision sink helps maintain a global perspective, preventing the model from narrowing its focus too much and minimizing information loss. Our goal is to ensure the model maintains a high-density vision sink within shallow layers. To achieve this, we design a training-free method called Enhancing Attention Heads (EAH). This plug-and-play approach focuses on each attention head in the early layers, systematically identifying the head with the densest vision sinks. It then broadcasts this attention distribution across the layer, aligning the layer’s attention and the head’s vision sink distribution with that of the selected head. We conduct extensive evaluations, focusing specifically on hallucination issues, and test mainstream MLLMs to validate the effectiveness of EAH in reducing hallucinations across various model architectures. Our results demonstrate that EAH is a highly effective plug-and-play solution for mitigating hallucinations across various MLLMs. Specifically, our contributions can be summarized as follows: • This paper investigates how information flow relates to hallucinations in MLLMs. Our analysis reveals a consistent pattern where denser vision sinks and a larger number of vision sink heads in the shallow layers are associated with fewer hallucinations. • We propose a plug-and-play training-free method called Enhancing Attention Head, which alleviates hallucinations by finding the head with the densest vision sink and broadcasting it to other heads. • Experiments on multiple models validate the plug-and-play convenience and strong generalization of this method."
https://arxiv.org/html/2411.09955v1,Instruction-Guided Editing Controls for Images and Multimedia: A Survey in LLM era,"The rapid advancement of large language models (LLMs) and multimodal learning has transformed digital content creation and manipulation. Traditional visual editing tools require significant expertise, limiting accessibility. Recent strides in instruction-based editing have enabled intuitive interaction with visual content, using natural language as a bridge between user intent and complex editing operations. This survey provides an overview of these techniques, focusing on how LLMs and multimodal models empower users to achieve precise visual modifications without deep technical knowledge. By synthesizing over 100 publications, we explore methods from generative adversarial networks to diffusion models, examining multimodal integration for fine-grained content control. We discuss practical applications across domains such as fashion, 3D scene manipulation, and video synthesis, highlighting increased accessibility and alignment with human intuition. Our survey compares existing literature, emphasizing LLM-empowered editing, and identifies key challenges to stimulate further research. We aim to democratize powerful visual editing across various industries, from entertainment to education. Interested readers are encouraged to access our repository at https://github.com/tamlhp/awesome-instruction-editing.","Visual design tools have become essential in various multimedia fields, although they often require prior knowledge to use effectively. Recent research has emphasised text-guided image editing as a way to make these tools more accessible and controllable (Li et al., 2020a; Patashnik et al., 2021; Gal et al., 2022; Crowson et al., 2022), as in Fig. 1. Studies have shown the effectiveness of diffusion models in creating realistic images and their application in image editing through techniques like swapping latent cross-modal maps for visual manipulation (Ho et al., 2020; Kim et al., 2022). Additionally, specific region editing is made possible through guided masks (Nichol et al., 2022; Avrahami et al., 2022). Moving away from complex descriptions and masks, instruction-based editing has gained traction for its straightforward approach, allowing users to directly command how and what aspects of an image to edit (Hertz et al., 2023; Mokady et al., 2023; Kawar et al., 2023). This paradigm is noted for its practicality, aligning closely with human intuition (Fu et al., 2024; El-Nouby et al., 2019; Fu et al., 2020). The latest text-to-image generative models offer impressive image quality and accuracy in reflecting the given captions, marking a significant leap in content generation technologies (Alayrac et al., 2022; Ramesh et al., 2022; Rombach et al., 2022). Among these advancements, instructional image editing has emerged as a particularly promising application (Brooks et al., 2023). This method streamlines the editing process by eliminating the need for detailed before-and-after captions (Avrahami et al., 2022; Wallace et al., 2023). Instead, users can provide simple, human-readable instructions, such as “change the dog to a cat”, making the editing process more intuitive and aligned with how humans naturally approach image modification (Zhang et al., 2024f). In recent years, advancements in large language models (LLMs) (Touvron et al., 2023; Brown et al., 2020) have dramatically reshaped the landscape of image and video manipulation. The convergence of these technologies has enabled more intuitive, flexible, and high-fidelity editing processes, largely driven by natural language instructions (Wu et al., 2023c; Feng et al., 2024b; Chakrabarty et al., 2023a). These innovations span various applications, from fashion image editing and 3D scene manipulation to video-to-video synthesis and audio-driven editing, empowering users to achieve fine-grained control over visual content. Moreover, Multimodal large language models (MLLMs), building upon the foundational capabilities of traditional LLMs, have extended the boundaries of vision-language tasks (Zhang et al., 2024b). By integrating latent visual knowledge and treating images as input, MLLMs enhance performance in tasks requiring both textual and visual reasoning. The emergence of diffusion models, such as LLaVA (Liu et al., 2024a) and MiniGPT-4 (Zhu et al., 2024a), has further elevated the potential of these frameworks by improving image-text alignment through instruction tuning. These models, including GILL (Koh et al., 2024) and SEED (Ge et al., 2023), facilitate coherent image generation from textual input while preserving rich visual semantics, marking a pivotal evolution in instruction-based editing. This review paper explores the evolution and diversity of techniques underpinning instruction-based image and video editing, synthesizing cutting-edge approaches that integrate human feedback, multimodal signals, and advanced neural architectures. The focus spans from early models leveraging generative adversarial networks (GANs) (Patashnik et al., 2021) to the latest innovations using diffusion models, including frameworks like Pix2Pix (Brooks et al., 2023), InstructBrush (Zhao et al., 2024), and FlexEdit (Nguyen et al., 2024a). Additionally, specialized models for audio- and video-driven editing, such as Noise2Music (Huang et al., 2023a) and Fairy (Wu et al., 2023a), are examined, demonstrating the versatility and creativity unlocked by these methods. By analyzing over 100 recent key publications, this review delves into key technological breakthroughs, evaluates their effectiveness, and considers potential avenues for further innovation. From 3D image editing (Sabat et al., 2024) to fashion editing (Wang and Ye, 2024), this paper highlights how these models are reshaping industries ranging from entertainment and fashion to education and remote sensing (Han et al., 2024b). Through this comprehensive overview, we aim to identify emerging trends, challenges, and opportunities in the growing field of text-driven, instruction-guided image and video editing. Differences with Existing Surveys. Our survey differs from existing surveys in its specific focus on instruction-based image and video editing empowered by LLMs. While Li et al. (Li et al., 2024b) focus on the integration of various modalities for retrieval tasks, our paper highlights the use of instructions for precise visual editing. Qin et al. (Qin et al., 2024) evaluate instruction-following abilities in LLMs but does not address their application in visual manipulation, which is a key focus of our review. Similarly, Yin et al. (Yin et al., 2023) address instruction-following in language models with a broader emphasis on ethical concerns, whereas our review emphasizes the technical advancements in using these capabilities for visual content generation and editing across various domains, including image, video, and 3D manipulation. Closest to our review is (Zhan et al., 2023), which explores generative AI techniques but lacks the detailed exploration of instruction-following in visual editing contexts, as seen in our paper. Especially, we consider caption-based image editing (Chen et al., 2018; Couairon et al., 2022b; Lin et al., 2023a) is a part of instruction-based image editing but we do not fully focus on the former. Rather, we are interested in user-friendly instructions that have practical implications for broad audience when editing images. Table 1 summarises the difference between our surveys and existing ones. Table 1. A comparison between existing surveys Survey Focused Task Focused Modality Key Contents (Qin et al., 2024) Editing Text Instruction development, Evaluation concerns (Yin et al., 2023) Editing Text LLM-empowered instructions, Instruction tuning (Li et al., 2024b) Retrieval Image, Video, Audio Image-text composite retrieval, Multimodal composite retrieval (Zhan et al., 2023) Generation Image Text guidance, Audio guidance, Sketch guidance, etc. Ours Editing Image, Video, Audio Instruction mechanisms, Augmentations, Learning stragies, Model designs, Loss functions Paper Collection Methodology. To map the research landscape on this subject, we used a range of keyword searches and combinations such as “image editing”, “image manipulation”, “text-guided”, “instruction-followed”, and “instruction-guided”. Initially, we relied on platforms like Google Scholar, Semantic Scholar, and the AI-enhanced tool Scite.ai to compile an initial set of studies. We then expanded this collection by conducting backward searches, reviewing the references in the selected papers, and forward searches to identify works that cited them. To ensure accuracy, we manually evaluated the relevance of each study, given that some focused on related areas like image generation or retrieval but employed similar techniques. This thorough process ultimately resulted in the identification of over 100 pivotal papers relevant to the field. Contributions. The main contributions of this survey are: • Comprehensive Review: This study provides a comprehensive review of LLM-empowered image and media editing. We have gathered and summarised an extensive body of literature, including both published works and pre-prints up to October 2024. • Process-based Taxonomy: We have organised the literature according to the developmental stages of an image editing framework. Fig. 2 presents the taxonomy we developed to structure the existing works in the field. • Optimisation Tools: We have curated a set of optimisation tools for developing end-to-end image editing frameworks, covering model designs, learning strategies, instruction mechanisms, data augmentations, and loss functions. • Practical Applications: We discuss various practical applications across multiple domains, including style, fashion, face editing, scene manipulation, charts, remote sensing, 3D, speech, music, and video editing. • Challenges and Future Directions: Instruction-guided visual design remains an emerging area of research. Based on the surveyed literature, we identify several unresolved challenges and propose future research directions to explore more editing use cases and user-friendly editing controls. • Sources, Datasets, and Metrics: To support empirical research, we provide a comprehensive overview of available source codes, datasets, and evaluation metrics that have been utilised in the field. • Online Updating Resource: To support ongoing research in LLM-empowered visual design, we have created an open-source repository111https://github.com/tamlhp/awesome-instruction-editing, which consolidates relevant studies, including links to papers and available code. Figure 2. Process-based taxonomy of instruction-guided image editing."
https://arxiv.org/html/2411.09952v1,GGAvatar: Reconstructing Garment-Separated 3D Gaussian Splatting Avatars from Monocular Video,"Avatar modelling has broad applications in human animation and virtual try-ons. Recent advancements in this field have focused on high-quality and comprehensive human reconstruction but often overlook the separation of clothing from the body. To bridge this gap, this paper introduces GGAvatar (Garment-separated 3D Gaussian Splatting Avatar), which relies on monocular videos. Through advanced parameterized templates and unique phased training, this model effectively achieves decoupled, editable, and realistic reconstruction of clothed humans. Comparative evaluations with other costly models confirm GGAvatar’s superior quality and efficiency in modelling both clothed humans and separable garments. The paper also showcases applications in clothing editing, as illustrated in Figure 1, highlighting the model’s benefits and the advantages of effective disentanglement. The code is available at https://github.com/J-X-Chen/GGAvatar/.","Reconstructing realistic clothed digital humans and their garments is a significant task in computer graphics and computer vision. This type of work aims to synthesize high-resolution clothed human body images from an unprecedented view or generate human imagery in a novel pose. Previous research has delved into explicit modelling methods under costly capture systems to obtain suboptimal reconstruction outcomes (Seitz et al., 2006; Szeliski et al., 1996). Recent advancements have shifted towards direct construction from single RGB images or monocular videos, utilizing models with implicit representation such as Neural Radiance Field (NeRF)(Mildenhall et al., 2020) to capture fine textures on the surface. However, these models (Peng et al., 2021; Weng et al., 2022; Feng et al., 2022; Chen et al., 2021) require dozens of training hours. Consequently, current studies(Jiang et al., 2023; Geng et al., 2023; Qian et al., 2024; Lei et al., 2024; Hu et al., 2024b; Kocabas et al., 2024; Hu et al., 2024a) are increasingly focused on enhancing rendering speed and modelling efficiency by turning neural rendering techniques into Instant-NGP(Müller et al., 2022) or 3D Gaussian Splatting (3DGS)(Kerbl et al., 2023). Nevertheless, the lack of disentanglement functions in these existing avatar models may constantly limit their applicability in real-world scenarios. This paper argues that an ideal avatar model should not only produce high-quality, rapid, and thorough reconstruction results, but also possess the decoupling capability necessary for applications such as virtual try-ons. Unfortunately, creating a perfect editable and drivable avatar is a demanding task that presents several challenges. Firstly, to effectively disentangle the body and garments, integrity and anti-interference properties must be maintained between distinct components. Specific estimations are required for the unsupervised areas where the human body is obstructed. Secondly, a precise transformation between canonical space and various pose spaces must be established to locate the partitioned point cloud at the target position. Lastly, it is essential to capture diverse and intricate clothing details, including textures, and to achieve high-quality reconstructions from sparse monocular inputs, particularly for loose-fitting attire. However, works such as (Li et al., 2024; Feng et al., 2023; Corona et al., 2021; Jiang et al., 2020; Li et al., 2022; Kim et al., 2024; Pons-Moll et al., 2017) are limited to recovering geometry without providing corresponding appearance information. In response to these challenges, this paper proposes a novel framework, GGAvatar, designed to construct realistic avatars from monocular videos while effectively and completely separating the garments. Specifically, this paper builds and fits garment templates alongside the corresponding body template to achieve a preliminary state of separation and interference resistance, resulting in partitioned point sets. Phased trainable modules (isolation and joint training) reasonably prevent the intersection of point sets during the training process. Subsequently, the target Gaussian positions are ensured by constructing deformation fields based on a concentric skeleton. Simultaneously, high-quality rendering is accomplished using 3DGS. Notably, GGAvatar enables thorough separation of clothed humans in novel view synthesis tasks from monocular inputs—potentially a first in this field, to my knowledge. The paper evaluates the GGAvatar model by comparing it with baseline approaches and other works on the People Snapshot Dataset (Alldieck et al., 2018) or the ZJU Mocap Dataset (Peng et al., 2021). The results indicate that GGAvatar demonstrates a high level of reconstruction quality for clothed humans, comparable to that of other 3DGS-based models. Notably, the proposed model outperforms nearly every traditional NeRF-based model while exhibiting significantly faster training speeds—approximately hundreds of times faster than the NeRF counterparts. Furthermore, ablation studies are conducted to validate the effectiveness of each component. To highlight the superiority and practical utility of GGAvatar, this paper compares it with existing non-fully decoupled models on clothing transfer. The contributions are summarized as follows: • This paper proposes the GGAvatar model, based on phased training methods, to achieve high-quality and efficient construction for various viewpoints or pose synthesis tasks of clothed humans. • The method of constructing parameterized templates for garments is introduced to solve the challenge of complex clothes modelling. • The GGAvatar enables a thorough separation between different garments, allowing applications such as colour editing and clothing transfer."
https://arxiv.org/html/2411.09933v1,JRadiEvo: A Japanese Radiology Report Generation Model Enhanced by Evolutionary Optimization of Model Merging,"With the rapid advancement of large language models (LLMs), foundational models (FMs) have seen significant advancements. Healthcare is one of the most crucial application areas for these FMs, given the significant time and effort required for physicians to analyze large volumes of patient data. Recent efforts have focused on adapting multimodal FMs to the medical domain through techniques like instruction-tuning, leading to the development of medical foundation models (MFMs). However, these approaches typically require large amounts of training data to effectively adapt models to the medical field. Moreover, most existing models are trained on English datasets, limiting their practicality in non-English-speaking regions where healthcare professionals and patients are not always fluent in English. The need for translation introduces additional costs and inefficiencies. To address these challenges, we propose a Japanese Radiology report generation model enhanced by Evolutionary optimization of model merging (JRadiEvo). This is the first attempt to extend a non-medical vision-language foundation model to the medical domain through evolutionary optimization of model merging. We successfully created a model that generates accurate Japanese reports from X-ray images using only 50 translated samples from publicly available data. This model, developed with highly efficient use of limited data, outperformed leading models from recent research trained on much larger datasets. Additionally, with only 8 billion parameters, this relatively compact foundation model can be deployed locally within hospitals, making it a practical solution for environments where APIs and other external services cannot be used due to strict privacy and security requirements.","In recent years, foundational models (FMs) have seen remarkable advancements, transforming various fields by offering more sophisticated and powerful solutions [1]. A key driver of this progress has been the rise of large language models (LLMs), which have greatly expanded the capabilities of FMs, particularly in processing and generating text with high accuracy and contextual understanding. This has sparked exponential growth in research [2], leading to the development of vision-language models that integrate visual and textual data [3, 4, 5], as well as fine-tuning approaches that enhance model performance for specific tasks [6, 7]. Healthcare is one of the most critical application areas for foundational models. The need to develop models tailored to healthcare is essential, particularly because physicians often face the challenge of reviewing large volumes of medical data, such as X-rays, which can be time-consuming and demanding. Advanced FMs can help alleviate this burden by enabling quicker and more efficient diagnoses, improving the overall effectiveness of healthcare delivery and patient outcomes. In response to this need, various FMs have been fine-tuned specifically for the healthcare domain, further enhancing their accuracy and effectiveness in clinical settings [7, 8, 9]. However, despite these advancements, several challenges remain. One significant issue is that most of the models developed so far, such as LLaVA-Med [7] and MedPaLM 2 [8], are predominantly in English, whereas many healthcare professionals and patients are not always proficient in English. For these models to be truly practical, there is a pressing need to expand their capabilities to non-English languages. Relying on a two-step process, where the model first generates output in English and then translates it, can introduce additional costs and complexity, making it less efficient and accessible. Additionally, publicly available datasets that can be used to train these models, such as MIMIC-CXR [10] and IU X-Ray [11], are overwhelmingly in English, with very few datasets available in other languages. Translating the large amounts of data needed for training into other languages with high quality is a costly and resource-intensive process. This scarcity of non-English datasets makes it difficult to develop models that can handle non-English languages. Furthermore, due to privacy concerns, it is challenging to collect and use patient data for model training, further complicating the creation of such datasets. Also, the use of large models through APIs, such as GPT-4 [12], is often impractical in healthcare settings because of the stringent privacy regulations that protect patient data, which limits the deployment of these models in real-world clinical environments. To address these challenges, this paper presents a Japanese Radiology report generation model enhanced by Evolutionary optimization of model merging [13] (JRadiEvo), a first attempt to extend a multimodal vision-language model for non-English medical text generation by utilizing evolutionary optimization of model merging [13]. JRadiEvo was developed by merging a non-medical vision-language model, medical text-to-text models, and a Japanese-language text-to-text model using an evolutionary algorithm. This innovative approach enabled the efficient creation of a Japanese radiology report generation model using only a minimal amount of Japanese-language data, addressing the critical need for non-English medical text generation in a resource-constrained environment. Below we outline our key contributions, which aim to advance the field of multimodal foundational models in healthcare: 1. Efficient use of limited non-English medical data: In the context of the difficulty in collecting non-English datasets, JRadiEvo demonstrates the ability to create a non-English medical report generation model by translating and utilizing only 50 cases from publicly available English datasets. This approach highlights the efficiency of the development process, demonstrating how a non-English medical report generation model can be created using extremely limited data and annotations. Additionally, it is noteworthy that not only was the dataset used after translation limited to 50 cases, but the entire dataset used to create JRadiEvo consisted of just 50 cases. This underscores the fact that JRadiEvo efficiently utilizes a very limited amount of data, demonstrating an effective approach to handling medical data under strict privacy and security constraints. 2. Novel application of model merging in the medical vision-language model: Traditionally, adapting models to the medical domain has relied on fine-tuning or training from scrach. To the best of our knowledge, there are no existing study of applying model merging alone to adapt a vision-language model to the medical domain. While recent research [13] has proposed using evolutionary optimization of model merging for vision-language models, this approach has been limited to natural images. To our knowledge, no prior studies have extended this technique to medical images or other domain-specific imagery beyond natural images. 3. Lightweight model for local deployment: JRadiEvo is an 8B parameter model, making it lightweight enough to be deployed on local hospital computing systems without the need for external APIs. This local deployment capability addresses critical privacy and security concerns, allowing hospitals to maintain control over patient data. Additionally, given the challenges of equipping facilities with expensive GPUs proportional to patient numbers, JRadiEvo’s compact size and low GPU memory requirements make it practical for widespread use. 4. Cost-efficient training process: JRadiEvo eliminates the need for computationally expensive backpropagation during training, enabling a far more efficient learning process compared to training a new model or fine-tuning. Additionally, by leveraging model merging instead of fine-tuning, JRadiEvo avoids the common issue of catastrophic forgetting [14, 15, 16] that often occurs during fine-tuning, allowing for a more stable and efficient development process."
https://arxiv.org/html/2411.09924v1,A Polarization Image Dehazing Method Based on the Principle of Physical Diffusion,"Computer vision is increasingly used in areas such as unmanned vehicles, surveillance systems and remote sensing. However, in foggy scenarios, image degradation leads to loss of target details, which seriously affects the accuracy and effectiveness of these vision tasks. Polarized light, due to the fact that its electromagnetic waves vibrate in a specific direction, is able to resist scattering and refraction effects in complex media more effectively compared to unpolarized light. As a result, polarized light has a greater ability to maintain its polarization characteristics in complex transmission media and under long- distance imaging conditions. This property makes polarized imaging especially suitable for complex scenes such as outdoor and underwater, especially in foggy environments, where higher quality images can be obtained. Based on this advantage, we propose an innovative semi-physical polarization dehazing method that does not rely on an external light source. The method simulates the diffusion process of fog and designs a diffusion kernel that corresponds to the image blurriness caused by this diffusion. By employing spatiotemporal Fourier transforms and deconvolution operations, the method recovers the state of fog droplets prior to diffusion and the light inversion distribution of objects. This approach effectively achieves dehazing and detail enhancement of the scene.","Complex environments such as bad weather or low light can affect most outdoor optical imaging systems, reducing visibility significantly. Traditional image processing techniques mainly rely on light intensity and color information, but polarized light can provide a richer description of the scene than these conventional methods[1], thus extending the potential of image processing and computer vision applications. Currently, polarization vision technology has demonstrated significant advancements in areas such as object recognition in natural scenes, hull damage detection, and marine biology research. Additionally, atmospheric scattering is one of the main causes of polarization phenomena in nature, and the haze resulting from atmospheric scattering can be effectively mitigated through polarization analysis, thereby enhancing the visibility of long-distance targets[2]. In haze scenarios, compared with images acquired by conventional cameras, polarized light images can reduce the loss of details and target information, improve contrast and color reproduction, and are crucial for subsequent tasks such as target detection and recognition, image segmentation, and target tracking. Therefore, the study of polarized image enhancement algorithms holds significant application value in the field of image processing. Polarization dehazing technology estimates atmospheric light intensity by capturing multiple polarized images of the same scene and performs inversion processing on the degraded scene reflection to obtain a dehazed image. This technology can be categorized into four main methods: differential imaging, the Stokes vector method, polarized differential active imaging, and circularly polarized optical imaging. Among these, differential imaging and the Stokes vector method are classified as passive dehazing techniques, relying solely on the polarization characteristics of natural light for image enhancement. These methods depend on a physical degradation model of the atmosphere[3], which states that the total radiation reaching the detector is the sum of direct transmitted light and airlight. The direct transmitted light contains the intensity information of the scene’s objects, while airlight, or scattered atmospheric light[4], serves as a significant source of interference in the imaging system. Differential imaging technology[5] captures images of the same scene at different polarization angles to estimate atmospheric light intensity through differential operations, thereby recovering dehazed images. Although this method effectively eliminates the influence of airlight, it presents challenges in data collection and involves complex computations. The Stokes vector method[6],[7] leverages the partially polarized nature of atmospheric light, treating direct transmitted light as unpolarized. It separates airlight by utilizing information from the degree of polarization and the angle of polarization, thus recovering direct transmitted light. However, this method requires capturing polarized images from multiple angles and necessitates accurately segmenting the sky region in the image when extracting atmospheric light intensity from that area[8]. Polarization differential active imaging and circularly polarized optical imaging techniques are categorized as active dehazing methods, which utilize artificial polarized light sources to illuminate the scene and analyze the polarization state. Polarization differential active imaging[9] exploits the characteristic that the polarization state of scattered light changes within a scattering medium, while the polarization state of the target reflection remains relatively stable, thereby preserving the target reflection through differential processing. Although this method demonstrates significant effectiveness in strongly scattering environments, the differential process may result in the loss of image details. Circularly polarized optical imaging technology[10] is based on the principle that circularly polarized light maintains its original characteristics while passing through scattering media, whereas the polarization characteristics of linearly polarized light gradually diminish. By detecting the reflected light, this method can recover the true image. Although it exhibits strong anti-scattering capabilities and effectively suppresses noise, it incurs high system design costs and has limited real-time performance. Image enhancement techniques have made significant research progress across various fields. Histogram equalization[11] is a common method that improves overall image quality by adjusting the grayscale distribution to enhance contrast. However, its effectiveness in improving local contrast is limited, prompting researchers to propose numerous subsequent improvements[12],[13]. The Retinex algorithm[14] simulates the color constancy of the human visual system by separating the illumination and reflection components of an image to enhance it. The Retinex model based on partial differential equations (PDE) [15] constructs a PDE to differentiate between the illumination and reflection components of the image. In contrast, the variational method-based Retinex model[16],[17] effectively eliminates halo effects by optimizing the image’s brightness and contrast. In the field of biomimetics, image enhancement methods based on visual cortical neural networks have also made significant progress. For instance, the Pulse-Coupled Neural Network (PCNN)[18] simulates the neural network structure of the mammalian visual cortex, enhancing images by capturing features and utilizing synchronized oscillation characteristics. Additionally, deep learning-based image enhancement methods have gained widespread application. Generative Adversarial Networks (GAN)[19] excel in image enhancement tasks through unsupervised or weakly supervised learning. Furthermore, multi-level feature fusion methods[20] leverage convolutional neural networks to extract and combine multi-level image features, effectively improving the quality of low-light images. In[21], an algorithm for image enhancement through specific thermal imaging steps is introduced to reduce diffusion effects. This method first applies a Fourier transform to the acquired thermal image, converting the image data from the spatiotemporal domain to the frequency domain. Subsequently, a deconvolution operation is performed to restore the pseudo-thermal flow, followed by phase-locking after reconversion to the temporal domain, effectively enhancing the contrast and clarity of the thermal image. In[22], a method for defocusing single-frame astronomical images is presented, which combines prior-weighted total variation and blind deconvolution techniques. This approach iteratively optimizes the skeleton image and the blur kernel, calculating the image Point Spread Function (PSF) and performing deconvolution to mitigate the decline in image quality caused by atmospheric turbulence distortions. Moreover, diffusion models[23],[24] have shown remarkable performance in image enhancement. In the forward process, noise is gradually injected until the image is completely covered by noise. Subsequently, a trained neural network is employed to progressively restore the noisy image, removing noise frame by frame, ultimately generating or recovering a clearer image. In order to obtain recovered clear images from polarized images of foggy scenes, this paper proposes a defogging method based on the principle of physical diffusion. By collecting data on foggy days using a polarization camera, we simulate the diffusion of fog and apply the RPHF algorithm[21] to perform spatiotemporal Fourier transformation and deconvolution on the polarized images, thereby reducing the blurring effects caused by fog diffusion. Experimental results demonstrate that the proposed method outperforms several classic dehazing algorithms in terms of effectiveness and metrics, effectively restoring object contours and details, making it suitable for subsequent applications such as object detection and recognition. The remainder of this paper is organized as follows: Section II elaborates on the principles of the proposed method. Section III describes the experimental setup and data acquisition. Section IV presents the experimental results and metric comparisons. Finally, Section V concludes the paper."
https://arxiv.org/html/2411.09921v1,Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at Pixel Level,"In this paper, we introduce Motion-Grounded Video Reasoning, a new motion understanding task that requires generating visual answers (video segmentation masks) according to the input question, and hence needs implicit spatiotemporal reasoning and grounding. This task extends existing spatiotemporal grounding work focusing on explicit action/motion grounding, to a more general format by enabling implicit reasoning via questions. To facilitate the development of the new task, we collect a large-scale dataset called GroundMoRe, which comprises 1,715 video clips, 249K object masks that are deliberately designed with 4 question types (Causal, Sequential, Counterfactual, and Descriptive) for benchmarking deep and comprehensive motion reasoning abilities. GroundMoRe uniquely requires models to generate visual answers, providing a more concrete and visually interpretable response than plain texts. It evaluates models on both spatiotemporal grounding and reasoning, fostering to address complex challenges in motion-related video reasoning, temporal perception, and pixel-level understanding. Furthermore, we introduce a novel baseline model named Motion-Grounded Video Reasoning Assistant (MoRA). MoRA incorporates the multimodal reasoning ability from the Multimodal LLM, the pixel-level perception capability from the grounding model (SAM), and the temporal perception ability from a lightweight localization head. MoRA achieves respectable performance on GroundMoRe outperforming the best existing visual grounding baseline model by an average of 21.5% relatively. We hope this novel and challenging task will pave the way for future advancements in robust and general motion understanding via video reasoning segmentation.","Understanding motions (Aggarwal & Cai, 1999, Corona et al., 2020, Zhou et al., 2012, Tevet et al., 2022) in dynamic video scenes has long been an important topic in the computer vision community. It plays a crucial role in many vital real-world applications, such as scene/video understanding (Saleemi et al., 2010, Sturgess et al., 2009, Mottaghi et al., 2016, Tsai et al., 2011, Fan et al., 2018), autonomous driving (Chen et al., 2015, Singh et al., 2022, Leon & Gavrilescu, 2019, Hu et al., 2023), and human-computer interaction (Aggarwal & Park, 2004, Wren & Pentland, 1999, Schmidt, 2000). Existing motion understanding tasks (e.g., action recognition (Soomro et al., 2012, Carreira & Zisserman, 2017), temporal action localization (Caba Heilbron et al., 2015, Jiang et al., 2014), spatiotemporal action/object detection (Gkioxari & Malik, 2015, Gu et al., 2018, Li et al., 2021, Vu et al., 2018, Jiang et al., 2020), video object segmentation (Xu et al., 2018, Seo et al., 2020, Khoreva et al., 2019, Cheng et al., 2023b, Ding et al., 2023)) are designed to either comprehend spatial interactions or detect motions in temporal span. However, motion is a complex spatiotemporal concept involving interactions between visual entities over time. Understanding motion-related attributes abstracted from dynamic scenes is crucial for comprehensive motion understanding. Table 1 highlights that existing tasks only address this challenge from specific aspects. As shown in Figure 1(a), action recognition focuses on identifying actions within a curated video clip, primarily using spatial features. The models are not required to distinguish fine-grained motion patterns over time but to recognize ""the motion"" mostly based on spatial features in a temporal-agnostic (Huang et al., 2018) manner due to potential single-frame bias (Lei et al., 2022). It leads to overlook fine-grained temporal motion patterns. Conversely, temporal action localization in Figure 1(b) emphasizes the temporal dimension but lacks detailed spatial analysis at the object level, relying on snippet-level features. Spatiotemporal action detection aims to localize actions in both dimensions but typically focuses only on humans in predefined actions (e.g., AVA (Gu et al., 2018), MultiSports (Li et al., 2021)), neglecting other interacting objects. It impairs the integrity of the spatial perception of motion understanding. Previous compositional action recognition investigates subject-object interaction and examines whether the model could distinguish pretended actions, but the benchmark (Goyal et al., 2017) only contains short clips, making the task fall short in analyzing the temporal context of motions. Thus, a crucial question arises: What will be a more comprehensive task for motion understanding? Inspired by the recent reasoning segmentation task in image domain (Lai et al., 2023), and considering the spatiotemporal nature of the motion as mentioned above, a feasible answer is to design an implicit video reasoning segmentation task where all necessary spatial and temporal factors of the motion of interest are taken into account, and then the motion-related object, which could be viewed as the medium of the corresponding motion, will be masked out as the final response. Figure 1: The illustration of the comparison between our Motion-Grounded Video Reasoning and previous video motion understanding tasks. Existing video motion understanding tasks (a)-(d) could at most address one or two key problems, either lacking fine-grained spatiotemporal perception or ignoring motion-related reasoning. (e) Our Motion-Grounded Video Reasoning considers both subject and object in motion as well as temporally adjacent events, performing challenging reasoning given four types of questions (Causal, Sequential, Counterfactual, and Descriptive) carefully designed in our GroundMoRe dataset and output spatiotemporal masks to indicate the answer visually at the pixel level. For instance, in the question ‘‘who needs to be passed or else the man in grey cannot easily score?’’, the motion ‘‘pass’’ and the subject ‘‘the man in grey’’ as well as an adjacent event ‘‘easily score’’ are provided in this question, the model needs reason about the object ‘‘the man in pink shorts’’, while output spatiotemporal masks (only between 0 to 32s where the motion ‘‘pass’’ happens). Such a paradigm fully grasps the spatiotemporal contexts of motion and provides an explainable response to evaluate the motion understanding ability. The colors of the questions are corresponded to the spatiotemporal masks. First, understanding specific motions requires analyzing their spatial contexts. For instance, in the interaction scenario ‘‘a boy kicked the ball for entertainment’’, the entities ‘‘a boy’’ and ‘‘the ball’’ constitute the spatial context for the motion ‘‘kicked’’. A comprehensive understanding of ‘‘kicked’’ involves grasping the interaction tuple <a boy, kick, the ball>. While spatiotemporal action localization tasks might address this problem, current benchmarks (e.g., AVA (Gu et al., 2018)) focus primarily on human-centric cases and overlook the bidirectional nature of interactions. A more effective approach would involve a question-answering format that leverages motion-related objects to visualize and reason about the interaction, enhancing spatial understanding. Second, temporal context, which provides chronological order to distinguish different motions, is also crucial for motion understanding. Temporal information not only delineates temporal boundaries but also enables an understanding of cause-and-effect relationships between actions. For example, in ‘‘the woman opened the refrigerator before taking out the milk’’, the two motions are connected, necessitating understanding of both for full comprehension. Thus, a question-answering paradigm can be designed, where a complete scene description with spatiotemporal context is converted into a motion-related question. However, merely answering the question cannot fully convey motion understanding, as language alone, if not visually grounded, is not the most direct explanation of visual concepts (Glenberg & Kaschak, 2002), and temporal information cannot be precisely represented by words (Xiao et al., 2024). Recent studies Yan et al. (2024), Liu et al. (2023a), Bai et al. (2024) have introduced a new task called video reasoning segmentation, which is closely related to our work. However, they primarily emphasize the spatial grounding of target objects through implicit reasoning, while neglecting temporal localization—a critical component for motion understanding. To address these issues and facilitate comprehensive motion understanding, we introduce a novel task: Motion-Grounded Video Reasoning as illustrated in Figure 1(e). This task requires models to take the motion-related question along with the video as input and output spatiotemporal segmentation masks of a specific object as a pixel-level visual answer. Such detailed spatiotemporal grounding allows for advanced motion comprehension. To further evaluate versatile spatiotemporal reasoning, we carefully design four types of questions in our newly collected dataset GroundMoRe (Grounding via Motion Reasoning). As shown in Figure 1(e), Causal questions explore the motivations behind motions, Sequential questions probe the order of temporally adjacent motions, Counterfactual questions are designed for imagining and reasoning about false reality and Descriptive questions ask about the general dynamic scene or abstract motion-related attributes such as enregetic, naughty, excited, etc. GroundMoRe consists of about 1,715 video clips, 7,577 questions and 249K object masks involving 3,942 different objects, ensuring a robust evaluation of motion understanding. Additionally, our task aligns with Video Object Segmentation (VOS) (Xu et al., 2018, Ding et al., 2023) but introduces additional challenges: 1) the use of implicit question inputs versus explicit referring expressions, and 2) the requirement for spatiotemporal object masks rather than spatial-only (no temporal localization requirement in current RVOS datasets), emphasizing the need for accurate temporal perception. We emphasize the practical benefits of the new task in diverse real-world applications. For example, localizing potential threats in public transportation often involves ambiguous information about the suspects (Yu et al., 2023b, Sultani et al., 2018). A robust Motion-Grounded Video Reasoning system can address this by processing queries like ‘‘Who is acting suspiciously in this airport?’’, effectively identifying unusual behaviors with implicit reasoning and spatiotemporal grounding. Table 1: Comparison of different motion understanding tasks. Spatial Context means whether to consider object-level interaction, Temporal Context indicates the influence of temporally adjacent motions/events, Motion Abstraction means understanding of motion-related abstract attributes, Pixel-level Output means whether output object segmentation mask as the final response and Implicit Reasoning means the ability to understand textual input without explicit object information. Tasks Datasets & Benchmarks Spatial Context Temporal Context Motion Abstraction Pixel-level Output Implicit Reasoning Action Recognition Kinetics400 (Carreira & Zisserman, 2017), UCF101 (Soomro et al., 2012) ✗ ✗ ✗ ✗ ✗ Temporal Action Localization ActivityNet (Caba Heilbron et al., 2015), THUMOS14 (Jiang et al., 2014) ✗ ✓ ✗ ✗ ✗ Spatiotemporal Action Localization AVA (Gu et al., 2018), MultiSports (Li et al., 2021) ✓ ✓ ✗ ✗ ✗ Motion Expression Video Segmentation MeViS (Ding et al., 2023) ✓ ✗ ✗ ✓ ✗ Video Reasoning Segmentation ReVOS (Yan et al., 2024), VideoReasonSeg Zheng et al. (2024) ✓ ✗ ✗ ✓ ✓ Motion-Grounded Video Reasoning GroundMoRe (Ours) ✓ ✓ ✓ ✓ ✓ We conduct an extensive evaluation for various image/video grounding baselines on GroundMoRe, though scoring competitive performances in other benchmarks (Kazemzadeh et al., 2014, Xu et al., 2018, Ding et al., 2023), none of them performs satisfyingly on our new task as shown in Table 3. Considering the spatiotemporal reasoning and grounding nature of the task, we further propose a new baseline model called Motion-Grounded Video Reasoning Assistant (MoRA). MoRA integrates LLaVA (Liu et al., 2023a), which is capable of complex multimodal reasoning, as the reasoning module, and a pretrained SAM (Kirillov et al., 2023b) decoder as the mask head. To further empower the model of temporal awareness, we additionally introduce a novel [LOC] token for temporal information embedding and add a temporal localization head to decode a binary temporal mask; thus inhibiting false temporal activation during spatiotemporal mask decoding. Our MoRA achieves overall SOTA performance on the proposed GroundMoRe, but there still remains a large room for future improvement (e.g., HTR (Miao et al., 2024) could reach 67.1 with \mathcal{J}\&\mathcal{F} metric on Ref-YouTubeVOS as its SoTA, while only 10.41 on GroundMoRe), which also underscores the increased difficulty of GroundMoRe. Our contributions are as follows: • We introduce a new task, Motion-Grounded Video Reasoning, designed to assess multimodal models’ reasoning and perception capabilities for motion understanding, filling the gap between referring VOS/action detection and motion-related video reasoning. • We collect a large-scale and versatile video dataset, named GroundMoRe for the proposed Motion-Grounded Video Reasoning task. • We comprehensively evaluate existing image/video grounding baseline models on our GroundMoRe, revealing their deficient motion understanding abilities. On the other hand, our proposed MoRA method achieves SOTA performance on GroundMoRe. The results also suggest substantial room for future improvement."
https://arxiv.org/html/2411.09911v1,DiffFNO: Diffusion Fourier Neural Operator,"We introduce DiffFNO, a novel diffusion framework for arbitrary-scale super-resolution strengthened by a Weighted Fourier Neural Operator (WFNO). Mode Rebalancing in WFNO effectively captures critical frequency components, significantly improving the reconstruction of high-frequency image details that are crucial for super-resolution tasks. Gated Fusion Mechanism (GFM) adaptively complements WFNO’s spectral features with spatial features from an Attention-based Neural Operator (AttnNO). This enhances the network’s capability to capture both global structures and local details. Adaptive Time-Step (ATS) ODE solver, a deterministic sampling strategy, accelerates inference without sacrificing output quality by dynamically adjusting integration step sizes ATS. Extensive experiments demonstrate that DiffFNO achieves state-of-the-art (SOTA) results, outperforming existing methods across various scaling factors by a margin of 2–4 dB in PSNR, including those beyond the training distribution. It also achieves this at lower inference time (Fig. DiffFNO: Diffusion Fourier Neural Operator (a)). Our approach sets a new standard in super-resolution, delivering both superior accuracy and computational efficiency.","Image super-resolution (SR) reconstructs high-resolution (HR) images from low-resolution (LR) inputs, recovering lost fine details to enhance visual quality. SR is crucial for applications like medical imaging [12], satellite imagery [23, 52], and video games [38]. The challenge in SR lies in its ill-posed nature: multiple HR images can correspond to the same LR input due to information loss during downsampling. This ambiguity requires sophisticated algorithms capable of inferring plausible and perceptually accurate high-frequency content from limited data. Deep learning, particularly Convolutional Neural Networks (CNNs) [58], has significantly advanced SR. Dong et al. introduced SRCNN [9], demonstrating the effectiveness of end-to-end learning for SR. Subsequent models achieved remarkable performance using deeper architectures and attention mechanisms [31, 57, 6, 34, 30]. Figure 1: The proposed Diffusion Fourier Neural Opeartor (DiffFNO) architecture for arbitrary-scale super-resolution begins by lifting a low-resolution input image \mathbf{x}_{\text{LR}}(\mathbf{r}) into a feature space using a convolutional encoder. Features extracted by the Weighted Fourier Neural Operator (WFNO) and an Attention-based Neural Operator (AttnNO) are combined using a Gated Fusion Mechanism (GFM). The fused features are then projected into RGB space, where Adaptive Time-Step (ATS) ODE solver efficiently completes the reverse diffusion process with both accuracy and speed. This pipeline generates \mathbf{x}_{\text{HR}}(\mathbf{r}), a high-resolution version of the input image. Diffusion models have emerged as powerful generative frameworks modeling complex data distributions via iterative denoising processes [14, 45, 11]. Their ability to generate high-fidelity images is well-suited for inferring missing fine details. In SR, diffusion models progressively refine an LR image by modeling the conditional distribution of HR images given the LR input [15, 41, 25, 51]. This iterative process reconstructs intricate textures and high-frequency components, producing realistic outputs. However, diffusion models are computationally intensive due to the iterative reverse diffusion process [46]. To address this, recent research explores efficient sampling strategies to accelerate reverse diffusion. One approach is approximating the diffusion process through deterministic Ordinary Differential Equation (ODE), which can be solved in fewer steps [33]. This accelerates inference and provides consistent, reproducible results. Arbitrary-scale SR models [17, 24, 7], which can upsample images at user-defined scales beyond those seen in training, have gained attention in recent years. Methods involving attention mechanisms [5] and representing images as continuous functions [10] have been explored. Operator-learning methods such as Super-Resolution Neural Operators (SRNO) [54] and HiNOTE [35] have further advanced this field. However, challenges remain due to the inherent differences between physics simulations—where operator learning excels [26]—and real-world images, computational demands, and the difficulty in restoring high-frequency details. To address these limitations, our contributions are: (1) We propose Weighted Fourier Neural Operator (WFNO) strengthened by iterative refinement from a diffusion framework for high-frequency reconstruction, detailed in Fig. 1. Through Mode Rebalancing (MR), WFNO learns to emphasize the most critical frequency components. This greatly enhances high-frequency image detail reconstruction, overcoming the limitations of standard FNOs and MLPs, which underrepresent such details due to mode truncation and spectral bias, respectively. (2) We develop Gated Fusion Mechanism (GFM) to dynamically adjust the influence of Fourier space features from WFNO and complementary spatial domain features from an Attention-based Neural Operator (AttnNO). AttnNO is lightweight, sharing an encoder with and running in parallel to WFNO. (3) Additionally, we present Adaptive Time-Step (ATS) ODE solver, which flexibly adjusts integration step sizes based on data characteristics by assessing the complexity of image regions, thereby reducing computational overhead without compromising quality. (4) DiffFNO achieves state-of-the-art results on multiple SR benchmarks, outperforming existing methods by 2–4 dB in PSNR in reconstruction quality. It also offers competitive inference time as Fig. DiffFNO: Diffusion Fourier Neural Operator (a) shows. DiffFNO remains robust across various upscaling factors—even those unseen during training."
https://arxiv.org/html/2411.09894v1,Free Lunch in Pathology Foundation Model: Task-specific Model Adaptation with Concept-Guided Feature Enhancement,"Whole slide image (WSI) analysis is gaining prominence within the medical imaging field. Recent advances in pathology foundation models have shown the potential to extract powerful feature representations from WSIs for downstream tasks. However, these foundation models are usually designed for general-purpose pathology image analysis and may not be optimal for specific downstream tasks or cancer types. In this work, we present Concept Anchor-guided Task-specific Feature Enhancement (CATE), an adaptable paradigm that can boost the expressivity and discriminativeness of pathology foundation models for specific downstream tasks. Based on a set of task-specific concepts derived from the pathology vision-language model with expert-designed prompts, we introduce two interconnected modules to dynamically calibrate the generic image features extracted by foundation models for certain tasks or cancer types. Specifically, we design a Concept-guided Information Bottleneck module to enhance task-relevant characteristics by maximizing the mutual information between image features and concept anchors while suppressing superfluous information. Moreover, a Concept-Feature Interference module is proposed to utilize the similarity between calibrated features and concept anchors to further generate discriminative task-specific features. The extensive experiments on public WSI datasets demonstrate that CATE significantly enhances the performance and generalizability of MIL models. Additionally, heatmap and umap visualization results also reveal the effectiveness and interpretability of CATE. The source code is available at https://github.com/HKU-MedAI/CATE.","Multiple Instance Learning (MIL) lu2021data ; shmatko2022artificial ; lipkova2022artificial ; chen2022scaling is widely adopted for weakly supervised analysis in computational pathology, where the input of MIL is typically a set of patch features generated by a pre-trained feature extractor (i.e., image encoder). Although promising progress has been achieved, the effectiveness of MIL models heavily relies on the quality of the extracted features. A robust feature extractor can discern more distinctive pathological features, thereby improving the predictive capabilities of MIL models. Recently, several studies have explored using pretrained foundation models on large-scale pathology datasets with self-supervised learning as the feature extractors for WSI analysis wang2022transformer ; filiot2023scaling ; chen2024towards ; vorontsov2023virchow . Additionally, drawing inspiration from the success of Contrastive Language-Image Pretraining (CLIP) radford2021learning ; lai2023clipath in bridging visual and linguistic modalities, some works have aimed to develop a pathology vision-language foundation model (VLM) to simultaneously learn representations of pathology images and their corresponding captions ikezogwo2024quilt ; lu2024visual . The intrinsic consistency between the image feature space and caption embedding space in the pathology VLM enables the image encoder to extract more meaningful and discriminative features for downstream WSI analysis applications lu2024visual . Although the development of these pathology foundation models has significantly advanced computational pathology, these models are designed for general-purpose pathology image analysis and may not be optimal for specific downstream tasks or cancer types, as the features extracted by the image encoder may contain generic yet task-irrelevant information that will harm the performance of specific downstream tasks. For example, as illustrated in Figure 1(a), the features extracted by the image encoder of a pathology VLM can include both task-relevant information (e.g., arrangement or morphology of tumor cells) and task-irrelevant elements(such as background information, stain styles, etc.). The latter information may act as ""noise"", distracting the learning process of MIL models tailored to specific tasks, and potentially impairing the generalization performance of these models across different data sources. Consequently, it is crucial to undertake task-specific adaptation to enhance feature extraction of generic foundation models and enable MIL models to concentrate on task-relevant information and thus improve analysis performance and generalization robey2021model ; wu2021collaborative . Figure 1: (a) Illustration of the key idea of concept-guided information bottleneck to enhance the task-relevant information and discard the task-irrelevant information. (b) Task-specific model adaptation with CATE to enhance the generalization across different data sources. In this paper, we propose a novel paradigm, named Concept Anchor-guided Task-specific Feature Enhancement (CATE), to enhance the generic features extracted by the pathology VLM for specific downstream tasks (e.g., cancer subtyping). Without requiring additional supervision or significant computational resources, CATE offers an approximately ""free lunch"" in the context of pathology VLM. Specifically, we first derive a set of task-specific concept anchors from the pathology VLM with task-specific prompts, and these prompts rely on human expert design or are generated through querying large language models (LLMs), necessitating a certain level of pathological background knowledge. Based on these concept anchors, we design two concept-driven modules, i.e., the Concept-guided Information Bottleneck (CIB) module and the Concept-Feature Interference (CFI) module, to calibrate and generate task-specific features for downstream analysis. Particularly, with the task-specific concepts as the guidance, the CIB module enhances task-relevant features by maximizing the mutual information between the image features and the concept anchors and also eliminates task-irrelevant information by minimizing the superfluous information, as shown in Figure 1(a). Moreover, the CFI module further generates discriminative task-specific features by utilizing the similarities between the calibrated image features and concept anchors (i.e., concept scores). By incorporating the CATE into existing MIL frameworks, we not only obtain more discriminative features but also improve generalization regarding domain shift by eliminating task-irrelevant features and concentrating on pertinent information, as shown in Figure 1(b). In summary, the main contributions of this work are threefold: • We introduce a novel method, named CATE, for model adaptation in computational pathology. To the best of our knowledge, this is the first initiative to conduct task-specific feature enhancement based on the pathology foundation model for MIL tasks. • We design a new CIB module to enhance the task-relevant information and discard irrelevant information with the guidance of task-specific concepts, and a new CFI module to generate task-specific features by exploiting the similarities between image features and concept anchors. • Extensive experiments on Whole Slide Image (WSI) analysis tasks demonstrate that CATE significantly enhances the performance and generalization capabilities of MIL models."
https://arxiv.org/html/2411.09893v1,Memory Proxy Maps for Visual Navigation,"Visual navigation takes inspiration from humans, who navigate in previously unseen environments using vision without detailed environment maps. Inspired by this, we introduce a novel no-RL, no-graph, no-odometry approach to visual navigation using feudal learning to build a three tiered agent. Key to our approach is a memory proxy map (MPM), an intermediate representation of the environment learned in a self-supervised manner by the high-level manager agent that serves as a simplified memory, approximating what the agent has seen. We demonstrate that recording observations in this learned latent space is an effective and efficient memory proxy that can remove the need for graphs and odometry in visual navigation tasks. For the mid-level manager agent, we develop a waypoint network (WayNet) that outputs intermediate subgoals, or waypoints, imitating human waypoint selection during local navigation. For the low-level worker agent, we learn a classifier over a discrete action space that avoids local obstacles and moves the agent towards the WayNet waypoint. The resulting feudal navigation network offers a novel approach with no RL, no graph, no odometry, and no metric map; all while achieving SOTA results on the image goal navigation task.","Visual navigation is motivated by the idea that humans likely navigate without ever building detailed 3D maps of their environment. In psychology, the concept of cognitive maps and graphs Tolman (1948); Chrastil & Warren (2014); Peer et al. (2021); Epstein et al. (2017) formalizes this intuition, and experiments have shown the validity of the idea that humans build approximate graphs of their environment, encoding relative distances between landmarks. In vision and robotics, these ideas have translated to the construction of topological graphs and latent maps based primarily on visual observations. These visual navigation paradigms seek new representations of environments that are rich with semantic information, easy to dynamically update, and can be constructed faster and more compactly than full 3D metric maps Gupta et al. (2017); Savinov et al. (2018); Chaplot et al. (2020b); Mirowski et al. (2018); Chen et al. (2019a); Gervet et al. (2023). In this work, we focus on visual navigation in environments where odometry information is not readily available, which limits the efficacy of current SOTA work that assumes access to noise-less GPS+compass sensors as in popular image-goal navigation challenges Yadav et al. (2023). This lack of odometry data also limits the efficacy of SLAM based methods that require the camera pose to be known Kwon et al. (2023); Chaplot et al. (2019), graph based methods that rely on distance to define edge features Kim et al. (2023), and reinforcement learning (RL) methods which use distance-based rewards Wijmans et al. (2019). Inspired by NRNS Hahn et al. (2021), which questions the necessity for RL and simulation to create an effective visual navigation agent, this work leverages feudal learning and latent maps as a memory proxy to show that it is possible to create a performant visual navigation agent that uses no odometry, no RL, no training in simulators, no graphs, and no metric maps. To do this, we take advantage of a three-tiered, feudal learning network structure and show its benefits under supervised and self-supervised learning paradigms. Feudal learning Vezhnevets et al. (2017) decomposes a task into sub-components, providing performance advantages that we find particularly well-suited for visual navigation. The feudal framework identifies workers and managers, and allows for multiple levels of hierarchy (ie. mid-level and high-level managers). Each of these entities observes different aspects of the task and operates at a different temporal or spatial scale. For navigation in unseen environments, this dichotomy is ideal to make the overall task more manageable. The worker-agent can focus on local motion, while manager-agents direct navigation and assess when to move to new regions. Key to our approach is representing the traversed environment with a learned latent map (instead of a graph) that acts as a sufficient memory proxy during navigation. This memory proxy map (MPM) is obtained using self-supervised learning. The high-level manager of our feudal learning agent maintains the MPM as the agent navigates in novel environments, and the MPM’s density is used to determine when a region is well-explored, and it’s time to move away from the current region. A second key aspect to our approach is a waypoint network (WayNet) for the middle-level manager which outputs waypoints (visible sub-goals that act as stepping stones towards a certain goal) for the worker agent to move towards. We train WayNet to imitate human exploration policies in a supervised manner using point-click navigation trajectories from the LAVN dataset Johnson et al. (2024). The intuition is that when humans navigate a simulated environment using point-click teleoperation, they use the skill of choosing a single point in the observation to move toward. For example, the chosen point may be toward the end of a hallway, toward a door, or further into a room. We demonstrate that this skill is easily learnable and generalizes to new environments with zero-shot transfer. Finally, we train a low-level worker to choose actions that avoid obstacles in the environment while following this point-wise navigation supervision. We show SOTA performance on the image-goal navigation task in previously unseen Gibson environments Xia et al. (2018) in Habitat AI Savva et al. (2019) (a simulation environment comprised of scans of real scenes). Our contributions are fourfold: 1) A self-supervised memory proxy map (MPM) that enables lean, no-odometry, no-graph, no-RL navigation, 2) A waypoint network (WayNet) for local navigation through supervised learning of human exploration policies, 3) A hierarchical navigation framework using agents operating at different spatial scales, and 4) SOTA performance on the image-goal task in Habitat indoor environments (testing and training on different environments)."
https://arxiv.org/html/2411.09871v1,Content-Aware Preserving Image Generation,"Remarkable progress has been achieved in image generation with the introduction of generative models. However, precisely controlling the content in generated images remains a challenging task due to their fundamental training objective. This paper addresses this challenge by proposing a novel image generation framework explicitly designed to incorporate desired content in output images. The framework utilizes advanced encoding techniques, integrating subnetworks called content fusion and frequency encoding modules. The frequency encoding module first captures features and structures of reference images by exclusively focusing on selected frequency components. Subsequently, the content fusion module generates a content-guiding vector that encapsulates desired content features. During the image generation process, content-guiding vectors from real images are fused with projected noise vectors. This ensures the production of generated images that not only maintain consistent content from guiding images but also exhibit diverse stylistic variations. To validate the effectiveness of the proposed framework in preserving content attributes, extensive experiments are conducted on widely used benchmark datasets, including Flickr-Faces-High Quality, Animal Faces High Quality, and Large-scale Scene Understanding datasets.","Remarkable progress has been made in computer vision tasks with the emergence of deep artificial neural networks, specifically convolutional neural networks (CNN) (Tan and Le, 2019; He et al., 2016; Hu et al., 2018; Szegedy et al., 2016; Simonyan and Zisserman, 2015; Brock et al., 2021) and vision transformers (ViT) (Dosovitskiy et al., 2021; Bai et al., 2022; Zhang et al., 2022). These models demonstrate remarkable performance across a spectrum of supervised vision tasks in various industrial domains, for example, defect detection and quality control in manufacturing, anomaly detection in security applications, object recognition for robot vision (Tan and Le, 2021; Redmon and Farhadi, 2018; He et al., 2017; Dosovitskiy et al., 2021; Lee et al., 2022; Zhang et al., 2021; Wang et al., 2023a), and scene understanding for autonomous vehicle (Liu et al., 2021; Lu et al., 2023; Han et al., 2024b). However, the process of data cleaning and annotation required for supervised tasks are highly costly (Deng et al., 2009; Lin et al., 2014; Krizhevsky et al., 2009), making it challenging to apply these networks without readily available annotations. To address this, generative models have emerged as promising solutions. For instance, generative adversarial networks (GANs) allow for high-quality image generation that closely aligns with the desired dataset distribution, offering a way to obtain the data needed for vision tasks (Karnewar and Wang, 2020; Karras et al., 2020a, 2019, b; Odena et al., 2017; Choi et al., 2018, 2020; Karras et al., 2021; Zhou et al., 2021; Mahendren et al., 2023). Similarly, recent diffusion models (Ho et al., 2020; Song et al., 2020; Zhuang et al., 2023; Wang et al., 2023b) have gained attention for their ability to produce high-quality images. With advancements in large language models, diffusion models have increasingly focused on integrating visual models with other modalities, such as text encoders, to enable text-conditional generation. This integration makes them particularly effective for tasks like text-to-image synthesis (Rombach et al., 2022; Hertz et al., 2022; Han et al., 2024a). While diffusion models are currently preferred for text-based applications, GANs still have significant potential, especially in scenarios focused solely on image-based applications. Once trained, GANs offer the advantage of faster image generation and have been very successful in producing sharp, high-quality images where their specific strengths can be fully exploited. The GAN is a framework that estimates generative models through an adversarial process (Goodfellow et al., 2014). GANs have played a significant role in various computer vision applications and have been an active research area since their invention. Despite the considerable success of GANs in the literature, effectively managing and controlling specific contents of generated images–such as underlying spatial structure or precise attributes–remains a challenging task. This challenge stems from the fundamental training objective of GANs, which primarily focuses on mapping the distribution of output images to an input distribution rather than explicitly generating images with desired contents (Goodfellow et al., 2014; Karras et al., 2019, 2020b; Karnewar and Wang, 2020), as depicted in Figure 1(a). Consequently, generating images that meet users’ requirements using GANs is still a difficult problem. (a) (b) Figure 1: Comparison of (a) typical image generation with (b) the proposed content-preserving image generation. Typical GANs generate random images following the distribution of real images. In contrast, the proposed framework allows users to exert control over image generation, enabling them to specify desired content attributes in the generated images. To overcome such a challenge, several methods have been proposed to impose specific constraints on the generated images, aiming to gain control over the image generation process. One approach is to convert GANs into a supervised learning manner by integrating auxiliary information, such as class labels or reference images, to guide the content of the generated outputs (Mirza and Osindero, 2014). This is done by concatenating additional inputs with random noise and feeding them into the networks. Additionally, researchers have proposed that discriminators can perform the classification task using categorical labels in conjunction with the critic to distinguish between real and fake samples. This is usually achieved by using a cross-entropy loss in the objective function (Odena et al., 2017). This approach has shown the possibility of controlling the content of generated images (Choi et al., 2018, 2020). However, manually annotating training images is highly labor-intensive and time-consuming. Moreover, widely-used benchmark datasets for image generation, such as Flickr-Faces High Quality (FFHQ) (Karras et al., 2019), Animal Faces High Quality (AFHQ) (Choi et al., 2018, 2020), and Large-scale Scene Understanding (LSUN) (Yu et al., 2015), are typically unlabeled. Hence, converting unsupervised tasks into supervised learning models may not be a practical solution to address the challenge of efficiently controlling the content of generated images by GANs. Another recent approach involves understanding the latent space of GANs and using it to control image generation (Shen et al., 2020; Karras et al., 2019). These methods typically utilize a multilayer perceptron encoder to transform the Gaussian noise input into a disentangled distribution of the same dimension. By using low-distance samples from this disentangled distribution, it becomes easier to generate images with the desired style. However, these methods do not have the ability to directly generate feature vectors from real-world images that would influence the output images. Instead, they only provide control over output images through a predefined set of synthesized samples. More recently, an unsupervised method was proposed in (Balakrishnan et al., 2022) to control the attributes of generated images by finding the optimal direction in the latent space based on the differential changes in different feature sets. However, the acquisition of the feature sets requires additional models, such as a face recognition model for identity features or an age regression model for age features. To tackle the previously mentioned labeling challenge and contending with constraints on control in limited settings, we propose a novel framework that can generate high-fidelity images with specific desired content characteristics as requested by users. The proposed framework leverages a real-world image to guide the generation process, resulting in output images that closely resemble the content of the input image, as illustrated in Figure 1(b). To achieve this, we introduce an encoding module incorporating two essential components: an encoder module for extracting content features and a content fusion module for generating refined content-guided vectors. This module analyzes generated images with known feature embeddings to determine the direction and intensity of content perturbations. Using this analyzed content feature vector, the proposed framework can produce images that preserve the contents of the guided images without being confined to predefined sets or constraints. Specifically, the encoding module is designed to concentrate solely on selected frequency components linked to intriguing content features. Its aim is to adeptly learn desired features while excluding undesired features from the learning process. The effectiveness of frequency selection in the generation process is verified by conducting an analysis to explore the impact of frequency bands on the overall process. This analysis provides convincing evidence that frequency analysis can serve as a meaningful guide in determining which components should be processed effectively to enhance content preservation and image quality. This finding also aligns with recent evidence that manipulating the frequency components of input images can significantly improve model performance (Ehrlich and Davis, 2019; Yin et al., 2019; Schwarz et al., 2021; Wang et al., 2020). In summary, this work makes the following contributions: 1. We propose a novel framework, named Content-Aware Preserving Image Generation Framework (CAP-GAN), which enables image generation with explicit control over the content of the output images, ensuring content fidelity with guiding images. 2. We investigate the impact of frequency bands on image generation and demonstrate that frequency analysis can serve as a valuable guide for content preservation and image quality improvement. 3. Extensive experiments have been conducted to validate the effectiveness of the proposed framework in accurately predicting the content of the generated images. The remainder of this paper is organized as follows: Section 2 provides a comprehensive review of various types of GANs and diffusion models, and it explores the role of frequency analysis in vision processing. In Section 3, we present a detailed description of the proposed framework, including the structure of each block and the underlying design principles. We also explain the process of extracting frequency components and their seamless integration into the proposed framework. Section 4 presents the experimental results and additional studies, offering a thorough analysis and interpretation of the findings. Finally, in Section 5, we conclude the paper with final remarks."
https://arxiv.org/html/2411.09863v1,Face De-identification: State-of-the-art Methods and Comparative Studies,"The widespread use of image acquisition technologies, along with advances in facial recognition, has raised serious privacy concerns. Face de-identification usually refers to the process of concealing or replacing personal identifiers, which is regarded as an effective means to protect the privacy of facial images. A significant number of methods for face de-identification have been proposed in recent years. In this survey, we provide a comprehensive review of state-of-the-art face de-identification methods, categorized into three levels: pixel-level, representation-level, and semantic-level techniques. We systematically evaluate these methods based on two key criteria, the effectiveness of privacy protection and preservation of image utility, highlighting their advantages and limitations. Our analysis includes qualitative and quantitative comparisons of the main algorithms, demonstrating that deep learning-based approaches, particularly those using Generative Adversarial Networks (GANs) and diffusion models, have achieved significant advancements in balancing privacy and utility. Experimental results reveal that while recent methods demonstrate strong privacy protection, trade-offs remain in visual fidelity and computational complexity. This survey not only summarizes the current landscape but also identifies key challenges and future research directions in face de-identification.","The rapid proliferation of artificial intelligence (AI) technologies, particularly deep learning-based models, has significantly transformed the landscape of face recognition systems. These advancements have brought about a wide range of applications, such as social media platforms. Unfortunately, accompanying these flourishing advancements, privacy concerns have also grown fast. Especially, faces are regarded as one of the most privacy-sensitive biological information directly related to personal identity. The essence of face recognition is biometric authentication, whose characteristics are unique and irrevocable. Once face recognition technology is used for cross-referencing with other databases, it will further disclose the user’s other sensitive information. The previous study [1] has shown how faces can be used as the link across different databases and the trails associated with their different personas, thus violating privacy. Facial privacy issues are receiving more attention these days. Restrictive laws and regulations such as the General Data Protection Regulations (GDPR) [2] have taken effect, which stipulates that data collection, sharing, and analysis by companies without the user’s knowledge is considered illegal and regular consent is required for any use of their personal data to ensure data privacy. In GDPR, privacy information was defined as “personal data that are related to an identified or identifiable natural person”, so personal identity is the most important part of facial image protection. To address this privacy issue, face de-identification (aka. face anonymization and face obfuscation) techniques have emerged as a crucial means of protecting privacy by anonymizing facial features in images without compromising their utility for non-identification tasks. Face de-identification refers to the process of concealing or altering personally identifiable facial features to prevent recognition. These techniques have a broad range of applications, from protecting individuals’ identities in media interviews and video surveillance [3], and medical research [4], to ensuring privacy in surveillance footage and social platforms [5, 6]. To date, many face de-identification methods have been proposed, which aim at protecting facial sensitive information (especially identity) while preserving utility for identity-unrelated applications. In addition, several related surveys have been conducted on related topics over the past decade, which are outlined in Table I. 1. Padilla et al. [7] provided a review and classification of visual privacy protection methods, as well as an analysis of how visual privacy protection techniques are deployed in existing privacy-aware intelligent monitoring systems. 2. Ribaric et al. [8] reviewed existing face de-identification in still images and videos at the time, namely non-deep-learning image filtering methods and k-Same based methods. 3. Ribaric et al. [9] presented a review of de-identification methods for non-biometric identifiers, physiological, behavioral and soft-biometric identifiers in multimedia contents. 4. Liu et al. [10] targeted privacy issues in dynamic Online Social Networks from a user-centric perspective. They proposed a privacy analysis framework consisting of three stages with different principles, observable privacy, contextual privacy and inferential privacy. 5. Cai et al. [11] systematically summarized the application of GAN in privacy and security, which discussed privacy from the perspectives of both data type and model and security from model robustness, malware detection, etc. They also provided unsolved challenges in the application scenario, model design and data utilization. 6. The relation between privacy and machine learning has been discussed in [12], which covers three categories: private machine learning, machine learning-aided privacy protection and machine learning-based privacy attack and corresponding methods. 7. The proposed taxonomy of [13] was tied to biometric recognition systems and partitioned biometric privacy-enhancing techniques into image-level, representation-level and inference-level, which also reviewed existing datasets, relevant standards and regulations. 8. Shopon et al. [14] summarized fragmented research on biometric de-identification and provided a classification mechanism based on the modalities employed and the types of biometric traits preserved after de-identification. They also discussed their applications in various domains such as cybersecurity. 9. The survey [15] started with a face recognition system and classified anti-facial recognition tools according to their targets, from data collection and model training to inference. They created a systematic framework to analyze the benefits and trade-offs of different AFR approaches and then considered the technical and societal challenges. 10. Zhang et al. [16] reviewed privacy attack methods and corresponding defense mechanisms in both visual data and visual systems under the context of deep learning. 11. Park et al. [17] focused on reviewing GAN-based facial de-identification algorithms. Particularly, their study evaluated existing methods in terms of data utility pertaining to the preservation of dermatologically interesting features such as skin color, pigmentation and texture. 12. Maity et al. [18] conducted a thorough exploration into the intersection of video analytics and privacy preservation, focusing on two core techniques: face de-identification and background blurring. They rigorously analyze the latest advancements, highlight both their efficacy and inherent limitations, so as to underscore the practical significance of these techniques through real-world applications in surveillance and the dynamic landscape of social platforms. TABLE I: Summary on prior survey articles Ref Focuses [7] Visual Privacy Protection Methods and Privacy-Aware Monitoring Systems [8] Traditional Face De-identification Methods [9] Privacy Protection in Multimedia Contents [10] Image Privacy in Online Social Networks [11] Generative Adversarial Networks in Privacy and Secure Applications [12] Interactions between Privacy and Machine Learning [13] Face Biometric Privacy-Enhancing Techniques [14] Biometric Systems De-Identification [15] Benefits and Trade-offs of Different Anti-Facial Recognition Technology [16] Visual Privacy Attack and Defense Methods [17] GAN-based De-Identification Methods in Dermatology Use Cases [18] Privacy preservation in video analytics This Survey Face De-identification Methods Despite the development of numerous techniques, the field still lacks a comprehensive survey that categorizes these methods and systematically evaluates their performance under different criteria. This paper aims to fill that gap by presenting a state-of-the-art review of face de-identification methods. We categorize existing approaches based on their operational levels (pixel, representation, and semantic) and assess their strengths and limitations in terms of privacy protection and image utility preservation. Our key contributions are summarized as follows. • We present a state-of-the-art survey on face de-identification, categorizing existing works by different image processing levels, i.e., pixel-level, representation-level, and semantic-level, and further discuss the characteristics of each category. • We summarize relevant metrics from the perspectives of privacy protection and image utility, and propose a comprehensive evaluation framework for de-identification algorithms. • We perform qualitative and quantitative experimental comparisons of several algorithms to discuss their advantages and disadvantages. • We summarize the analysis and comparative research of face de-identification methods, and further point out open problems and possible future research directions in this field. The remainder of this paper is organized as follows. Section II reviews preliminary knowledge on face recognition and de-identification technologies. Subsequently, we review the existing face de-identification methods in Section III and classify them into pixel-level, representation-level, and semantic-level, which can help readers gain a high-level understanding of current work and basic ideas. In Section IV, we show practical applications of face de-identification, including usage in specific domains and identity-agnostic computer vision tasks. Afterwards, the evaluation criteria commonly used in face de-identification algorithms are summarized in Section V. Additionally, quantitative and qualitative studies are implemented to compare representative methods in Section VI and the results provide intuitions and insights of of existing algorithms to readers. Finally, we propose some future directions in our view for this task in Section VII and conclude our work in Section VIII."
https://arxiv.org/html/2411.09858v1,Masked Image Contrastive Learning for Efficient Visual Conceptual Pre-training,"This paper proposes a scalable and straightforward pre-training paradigm for efficient visual conceptual representation called masked image contrastive learning (MiCL). Our MiCL approach is simple: we randomly mask patches to generate different views within an image and contrast them among a mini-batch of images. The core idea behind MiCL consists of two designs. First, masked tokens have the potential to significantly diminish the conceptual redundancy inherent in images, and create distinct views with substantial fine-grained differences on the semantic concept level instead of the instance level. Second, contrastive learning is adept at extracting high-level semantic conceptual features during the pre-training, circumventing the high-frequency interference and additional costs associated with image reconstruction. Importantly, MiCL learns highly semantic conceptual representations efficiently without relying on hand-crafted data augmentations or additional auxiliary modules. Empirically, MiCL demonstrates high scalability with Vision Transformers, as the ViT-L/16 can complete pre-training in 133 hours using only 4 A100 GPUs, achieving 85.8% accuracy in downstream fine-tuning tasks.","Self-supervised learning (SSL) is considered the cornerstone towards building a world model, particularly in the pre-training of vision model [4, 12, 2, 19, 23, 28], attributed to its ability to acquire versatile visual representations without the need for human annotations. Currently, two dominant learning paradigms in visual self-supervised learning are the Masked Image Modeling (MIM) [12, 23, 17, 31, 11] and Contrastive Learning (CL) [4], exhibiting promising scalability characteristics for vision models, notably Vision Transformers (ViTs) [8]. Despite the success of these methods, both of these prevailing paradigms suffer from efficient visual representation, due to image sparsity and conceptual redundancy. The image sparsity leads to an excessive focus on local details during pixel-level reconstruction in MIM, rather than the highly semantical concepts. On the flip side, conceptual redundancies typically result in transformed images that fail to exhibit significant distinctions in CL. Hence, a pertinent question emerges: beyond the existing MIM and CL paradigms, how to reconcile the divergence between efficient visual representation and effective conceptual pre-training? In elucidating this query, our initial step involves a revisit to the present pre-training paradigms, namely MIM and CL. Regarding MIM, it aims to learn visual representatives by reconstructing the masked image patches, as illustrated in Fig.1(b). Among these methods, BEiT [2] and MAE [12] are representatives. It is important to mention, that MAE indicates that there remains significant semantic redundancy within images, that only a small high-level understanding of parts, objects, and scenes is required to recover missing patches from neighbouring patches. However, pixel-level restoration is overly fine-grained for the pre-training of a vision model, excessively focusing on high frequencies and local details of the image. It runs counter to the core of pre-training, which is targeted at encapsulating high-hierarchical semantic image concepts. Despite this intricate task aids in the acquisition of visual representatives by the model, it regrettably fails to address concerns regarding pre-training efficiency. In terms of CL, the underlying principle of it hinges on a quite simple concept: maximum agreement between varying views from a single image, as exhibited in Fig.1(a), where SimCLR [4, 5], MoCo v3 [6] and DINO [3] are typical representatives. Within this cohort, sophisticated pre-processing techniques and intricate auxiliary networks are required to capture distinct views of the image, thereby presenting a challenging task of optimizing the agreement among different views. Consequently, it can be derived that the essence of contrastive learning lies in the creation of distinct views characterized by substantial disparities. However, this endeavour presents a formidable challenge owing to the inherent conceptual redundancy prevalent in images. Moreover, large batch sizes are prerequisites for the generation of negative sample pairs, leading to a long training time and huge computing resources consumption. Driven by this analysis, we found that these two paradigms can complement each other: masked tokens have the potential to significantly diminish the conceptual redundancy inherent in images, whereas contrastive learning is adept at extracting high-level semantic features during the pre-training phase. Thus, we present a novel and straightforward paradigm for self-supervised visual representation learning: masked image contrastive learning (MiCL). MiCL tackles the above issues systematically: I) Masked image tokens offer diverse views of a single image with substantial fine-grained conceptual differences. II) Contrastive learning enables pre-training to concentrate exclusively on the high-level semantic information contained within images while disregarding high-frequency redundancies. III) The proposed paradigm obviates the need for auxiliary modules and expedites the efficient extraction of model features. (a) Contrastive Learning (b) Masked Image Modeling (c) Our Masked Image Contrastive Learning Figure 1: Comparison between different pre-training paradigms. The Model in blue is the pre-training model, and the orange modules indicate auxiliary modules. (a) Contrastive Learning endeavours to maximize the agreement between different views of an image. (b) Masked Image Modeling aims to restore masked image patches. (c) Our Masked Image Contrastive Learning: Through non-overlapping masking, distinct tokens within an image are categorized as intraclass, while across-images tokens within a batch are viewed as interclass. Our objective is to enhance intraclass compactness and interclass separability through a contrastive learning approach. It is worth noting that our method does not necessitate any auxiliary modules, ensuring that no training resources are expended on redundant networks. MiCL has a particularly simple and straightforward workflow, as presented in Fig.1(c). Here is how it works: Firstly, we mask a batch of images with a high rate, dividing visible patches within one image into two non-overlapping groups. In succession, the pre-train model extracts the features of these two groups of batch image tokens, respectively. Subsequently, contrastive learning is employed to predict the correct pairings for a batch of visible image tokens. Positive samples are different visible tokens in the same image, while negative samples are from different images of the mini-batch. Finally, inspired by T-distributed classifier [26, 27], we introduce the T-distributed spherical loss to constrains the inter-class margins in the pre-training. Comprehensive experiments demonstrate the scalability and efficacy of our approaches, where ViT-L/16 can complete pre-training in 133 hours using only 4 A100 GPUs and attain an 85.8% top-1 accuracy in fine-tuning classification. In particular, our model stands out from other pre-training methods as it operates without the need for auxiliary modules or hand-crafted data augmentation to generate diverse views. In summary, our paper mainly makes the following contributions: 1. We endeavour to explore an alternative of using masked images to create diverse views with fine-grained conceptual differences for contrastive learning. By forgoing the conventional approach of employing instance-level hand-crafted data augmentation to generate distinct views, MiCL diminishes the conceptual redundancy inherent in images and improves efficiency. 2. Our approach eschews the reconstruction of masked images in favor of leveraging contrastive loss to steer the entire model. Independently of additional auxiliary modules, MiCL is adept at extracting high-level semantic concept features from images more efficiently. 3. Extensive experiments are conducted to verify the efficiency and scaling capability of our method. ViT-L/16 can complete pre-training in 133 hours using only 4 A100 GPUs with 85.8% accuracy in fine-tuning. Additionally, we have structured ablation experiments to delve into the implications of different configurations within MiCL, with a particular focus on the need of the MLP head in contrastive learning."
https://arxiv.org/html/2411.09850v1,"Enhancing Diffusion Posterior Sampling
for Inverse Problems by Integrating Crafted Measurements","Diffusion models have emerged as a powerful foundation model for visual generation. With an appropriate sampling process, it can effectively serve as a generative prior to solve general inverse problems. Current posterior sampling based methods take the measurement (i.e., degraded image sample) into the posterior sampling to infer the distribution of the target data (i.e., clean image sample). However, in this manner, we show that high-frequency information can be prematurely introduced during the early stages, which could induce larger posterior estimate errors during the restoration sampling. To address this issue, we first reveal that forming the log posterior gradient with the noisy measurement ( i.e., samples from a diffusion forward process) instead of the clean one can benefit the reverse process. Consequently, we propose a novel diffusion posterior sampling method DPS-CM, which incorporates a Crafted Measurement (i.e., samples generated by a reverse denoising process, compared to random sampling with noise in standard methods) to form the posterior estimate. This integration aims to mitigate the misalignment with the diffusion prior caused by cumulative posterior estimate errors. Experimental results demonstrate that our approach significantly improves the overall capacity to solve general and noisy inverse problems, such as Gaussian deblurring, super-resolution, inpainting, nonlinear deblurring, and tasks with Poisson noise, relative to existing approaches.","Diffusion models (Ho et al., 2020) have achieved remarkable generative performance on images (Amit et al., 2021; Baranchuk et al., 2021; Brempong et al., 2022), videos (Singer et al., 2022; Wu et al., 2023), audios (Popov et al., 2021; Yang et al., 2023a), natural language (Austin et al., 2021; Hoogeboom et al., 2021; Li et al., 2022) and molecular generation (Hoogeboom et al., 2022; Jing et al., 2022). Besides its strong modeling capacity for complex and high dimensional data, diffusion models have exhibited a strong generative prior to form the diffusion conditional sampling (Song et al., 2020) that can be harnessed for diffusion posterior sampling. In the context of noisy inverse problems, this sampling process effectively approximates precise data distributions from noisy and degraded measurements. Noisy inverse problems, such as super-resolution, inpainting, linear and nonlinear deblurring, are targeted to restore an unknown image \bm{x} from its noise-corrupted measurement \bm{y} given the corresponding forward measurement operators \mathcal{A}(\cdot):\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}. Recently, Diffusion models have been extensively utilized for these tasks (Zhu et al., 2023; Song et al., 2022; Wang et al., 2022), offering a robust framework for reconstructing high-quality images from degraded measurements. Current diffusion-based methods generally employ two distinct strategies to solve inverse problems. The first strategy is to train problem-specialized diffusion models (Saharia et al., 2022; Whang et al., 2022; Luo et al., 2023; Chan et al., 2023) given measurements and clean image pairs. In contrast, methods of the second strategy only capitalize on problem-agnostic pre-trained diffusion models to benefit the zero-shot diffusion restoration sampling by posterior estimate (Song et al., 2023a; Rout et al., 2024; Peng et al., 2024; Mardani et al., 2023) or enforcing data consistency (Chung et al., 2022b). In this paper, we concentrate on the second manner, the posterior estimate for sampling, to solve noisy inverse problems universally. Posterior estimate based methods enable controllable generations (Dhariwal & Nichol, 2021) via diffusion conditional reverse-time SDE (Song et al., 2020), such as classifier guidance (Dhariwal & Nichol, 2021), loss-guided diffusion (Song et al., 2023b). Applying Bayesian, the gradient of log posterior \nabla_{\bm{x}_{t}}\log p_{t}\left(\bm{x}_{t}\mid\bm{y}\right) can be easily adopted into conditional reverse-time SDE sampling as a log-likelihood gradient term \nabla_{\bm{x}_{t}}\log p\left(\bm{y}\mid\bm{x}_{t}\right) and an unconditional prior term \nabla_{\bm{x}_{t}}\log p_{t}\left(\bm{x}_{t}\right). In the context of solving inverse problems, however, p\left(\bm{y}\mid\bm{x}_{t}\right) is an intractable distribution due to the unclear dependency between the measurement \bm{y} and the diffusion generation \bm{x}_{t} at time t. To tackle this issue, existing posterior estimate methods for inverse problems, such as Diffusion Posterior Sampling (DPS (Chung et al., 2022a)), form the measurement model p\left(\bm{y}\mid\hat{\bm{x}}_{0}\right) as the likelihood estimate, where \hat{\bm{x}}_{0} is the denoising prediction given diffusion intermediate output \bm{x}_{t}. This process maps \hat{\bm{x}}_{0} into the measurement \bm{y}’s space, which can be interpreted as a reconstruction loss guidance to push \mathcal{A}\left(\hat{\bm{x}}_{0}\right) close to \bm{y}, while narrowing the gap between \bm{x}_{t} and the clean image \bm{x}. However, via empirical examples in Section 3.1, we show that solving inverse problems in the manner of diffusion posterior sampling follows a similar pattern of diffusion reverse process (Yang et al., 2023b), i.e., focusing on low-frequency recovery at first and posing increasing attention on high-frequency generation in the late stages. With such observation, the log posterior gradient estimate \nabla_{\bm{x}_{t}}\log p\left(\bm{y}\mid\hat{\bm{x}}_{0}\right) in DPS with sharp measurement \bm{y} will easily introduce abrupt high-frequency gradient signals for the subsequent step, which unfits the appropriate input pattern for the pre-trained model \bm{s}_{\theta}\left(\bm{x}_{t},t\right) during the early stages with large t. In fact, Song et al. (2023b) also notes that DPS significantly miscalculates the scale of the guidance term with different variance levels which results in accumulated errors in posterior sampling. In Section 3.1, we have similar observations that DPS amplifies posterior sampling errors. We also find that applying the likelihood estimate p\left(\bm{y}_{t}\mid\hat{\bm{x}}_{0}\right) with randomly sampled noisy measurement \bm{y}_{t} instead of the clean measurement \bm{y} in the DPS for each timestep t leads to a smaller approximation error during the early stages and thus benefits the restoration generation. Posterior approximation with noisy measurement \bm{y}_{t} at timestep t, compared with the clean one, has the advantage that it adaptively matches the frequency pattern of the diffusion model’s generation at the timestep t. Therefore, with this insight, we propose the posterior approximation that leads to less high-frequency signal recovery during the early stages. Specifically, we propose the Diffusion Posterior Sampling with Crafted Measurements (DPS-CM), which can introduce a less biased posterior estimate by combining crafted measurement \mathbf{y}_{t} 111In this work, we denote the crafted measurement as \mathbf{y}_{t}, which is the intermediate generation of the diffusion reverse process with diffusion model \theta, and the noisy measurement as \bm{y}_{t}, which is generated by the diffusion forward process as in Eq. 3., the intermediate generation of another diffusion reverse-time trajectory \{\mathbf{y}_{t}\}_{t=0}^{T} from posterior p\left(\mathbf{y}_{t}\mid\bm{y}\right). As \{\mathbf{y}_{t}\}_{t=0}^{T} shares a similar frequency distribution pattern with the target generation trajectory \{\bm{x}_{t}\}_{t=0}^{T}, i.e., low-frequency recovery at first, the approximated log-likelihood gradient \nabla_{\bm{x}_{t}}\log p\left(\hat{\mathbf{y}}_{0}\mid\hat{\bm{x}}_{0}\right) will bring in less high-frequency gradient signal and thus benefit the subsequent generations. Besides, leveraging crafted measurements \mathbf{y}_{t} brings a lower bias compared with directly using randomly sampled noisy measurement \bm{y}_{t}. Our extensive experiments results on various noisy linear inverse problems, e.g. super-resolution, random masked/fixed box inpainting, Gaussian/Motion deblurring, and nonlinear inverse problems such as nonlinear deblurring, demonstrate that the proposed DPS-CM significantly outperforms existing unsupervised methods on both FFHQ (Karras et al., 2019) and ImageNet (Deng et al., 2009) datasets while keeping the algorithm simplicity."
https://arxiv.org/html/2411.09823v1,": Generating Vivid and Interactive 
3D Scenes with Hierarchical 2D Inpainting","Creating large-scale interactive 3D environments is essential for the development of Robotics and Embodied AI research. However, generating diverse embodied environments with realistic detail and considerable complexity remains a significant challenge. Current methods, including manual design, procedural generation, diffusion-based scene generation, and large language model (LLM) guided scene design, are hindered by limitations such as excessive human effort, reliance on predefined rules or training datasets, and limited 3D spatial reasoning ability. Since pre-trained 2D image generative models better capture scene and object configuration than LLMs, we address these challenges by introducing Architect, a generative framework that creates complex and realistic 3D embodied environments leveraging diffusion-based 2D image inpainting. In detail, we utilize foundation visual perception models to obtain each generated object from the image and leverage pre-trained depth estimation models to lift the generated 2D image to 3D space. While there are still challenges that the camera parameters and scale of depth are still absent in the generated image, we address those problems by “controlling” the diffusion model by hierarchical inpainting. Specifically, having access to ground-truth depth and camera parameters in simulation, we first render a photo-realistic image of only back-grounds in it. Then, we inpaint the foreground in this image, passing the geometric cues in the back-ground to the inpainting model, which informs the camera parameters. This process effectively controls the camera parameters and depth scale for the generated image, facilitating the back-projection from 2D image to 3D point clouds. Our pipeline is further extended to a hierarchical and iterative inpainting process to continuously generate placement of large furniture and small objects to enrich the scene. This iterative structure brings the flexibility for our method to generate or refine scenes from various starting points, such as text, floor plans, or pre-arranged environments. Experimental results demonstrate that Architect outperforms existing methods in producing realistic and complex environments, making it highly suitable for Embodied AI and robotics applications.111Project page: https://wangyian-me.github.io/Architect/","Figure 1: We present Architect, a generative framework to create diverse, realistic, and complex Embodied AI scenes. Leveraging 2D diffusion models, Architect generates scenarios in an open-vocabulary manner. Here, we showcase two cases in detail: an apartment and a grocery store. Collecting or generating large-scale training data has recently emerged as a promising direction for advancing Robotics and Embodied AI research. A major focus in recent works pursuing this direction advocates for data generation in simulated environments (Wang et al., 2023b, a; Ha et al., 2023; Dalal et al., 2023), as simulation offers a cost-effective approach to data collection that scales naturally with computational resources; this thrust holds the potential for producing realistic physics and rendering data, and meanwhile grants access to valuable ground-truth state information for speeding up policy learning. Among the types of data needed for training Embodied AI agents, diverse and realistic environments with the possibility of interacting with surrounding entities is crucial. However, obtaining vivid interactive scene and environment data remains challenging. Recent studies have attempted to tackle this problem by developing generative models for environment creation via various approaches, including procedural generation with predefined rules (Deitke et al., 2022), diffusion-based scene generation (Tang et al., 2023a; Yang et al., 2024b; Feng et al., 2024), and large language model (LLM) guided scene population and design (Wang et al., 2023b; Yang et al., 2024c; Wen et al., 2023). Despite these recent efforts, generating diverse, realistic, and complex Embodied AI environments still remains a challenging problem due to the inherent drawbacks and assumptions made in the pipeline designs of existing methods. For example, manually designed environment datasets (Ramakrishnan et al., 2021; Weihs et al., 2021; Li et al., 2023a; Fu et al., 2020a, b) require excessive human effort and are hence inherently difficult to scale. Procedural generation methods (Deitke et al., 2022; Khalifa et al., 2020; Earle et al., 2021; Zhao et al., 2021) rely on predefined rules, which are limited in their ability to learn from and resemble real-world distributions, and struggle to generate open-vocabulary scenes. Large language model (LLM) guided scene generation process (Wang et al., 2023b; Yang et al., 2024c; Wen et al., 2023; Lin et al., 2023; Aguina-Kang et al., 2024; Feng et al., 2024) also presents its own limitations, as LLMs operate in language space and have limited 3D understanding and spatial reasoning capabilities. Moreover, existing LLM-based scene generation methods still rely on certain simplifications and hand-designed rules, such as primarily focusing on placements of large furniture pieces on the floor or against walls, and only considering simple inter-object relationships such as random placement of small items on top of large background furniture. Therefore, these methods struggle to generate more complex and cluttered object arrangements that are often encountered in daily life, such as “an organized dining table”, “an office desk drawer cluttered with objects”, or “a shelf of toys”, which typically require nuanced object placement and context-aware positioning. As a result, there still exists a gap in current literature for generating interactive 3D scene with detailed and complex configurations that closely resemble real-world distributions. To this end, we propose Architect, a generative framework for creating realistic and interactable 3D scenes via diffusion-based 2D inpainting (Podell et al., 2023). Our pipeline leverages controllable and hierarchical generation in 2D image space. Compared to LLMs which operate in language space, pre-trained image-based generative models are able to better capture scene and object configurations from massive image data readily available, both at the scene level and in fine-grained inter-object spatial information. Pre-trained depth estimation models (Ke et al., 2024; Bhat et al., 2023; Yang et al., 2024a) can then be used to lift the generated 2D static image to 3D environments. However, images created from 2D generative models do not provide accurate camera parameters, which are crucial for reconstructing accurate 3D environments. In addition, the predicted depth images also present scale ambiguity. To address these challenges, we propose to “control” the 2D generative models with 3D constraints via hierachical inpainting. First, we render a photo-realistic image in a simulated empty scene with only a static background, where we have access to ground-truth depth and camera parameters. We then use this image as a template for inpainting the foreground using 2D diffusion models. During this process, the generation respects the camera parameters informed by the geometric cues in the background image and ensures that the inpainted components are both semantically and spatially consistent with existing components in the input image. By generating images this way, we effectively control the camera parameters and depth scale for the generated image, which allows us to project it back to 3D point clouds. Subsequently, utilizing visual recognition models (Kirillov et al., 2023; Liu et al., 2023; Ren et al., 2024), we segment the 2D image to obtain the semantics and geometric information of each generated object. These objects are then instantiated in the actual simulated environments, by either retrieving from large-scale asset databases (Deitke et al., 2023a; Mo et al., 2019) or generating using image-to-3D generative models (Xu et al., 2024). While the pipeline described above is able to generate 3D scene configurations described from a single camera view, our goal is to generate complete scenes observable from multiple views. In addition, we aim to generate scenes with real-world complexity, where objects of different scales together form a holistic environment (e.g. ideally we want to also generate small items placed on a shelf or in a drawer). Therefore, we further extend the pipeline to perform iterative and hierarchical inpainting, during which we continuously render new image patches of different locations of the scene to further enhance the complexity when needed. Specifically, given a text description of a target scene, we (i) generate the floor plan following previous works (Yang et al., 2024c; Wen et al., 2023), (ii) add background assets such as walls and floors into a simulated environment, (iii) render images of this empty scene and perform the aforementioned, proposed iterative inpainting process for scene-level generation from multiple camera views, (iv) hierarchically, apply inpainting again at a finer level to place small objects in various semantically plausible locations in the interior space, and (v) finally resulting in a complex 3D scene. Note that such iterative process results in a flexible generative pipeline that can handle different levels of inputs: text descriptions, floor plans, or even pre-arranged scenes. Our pipeline is able to generate complex scenes that are fully interactable, with detailed asset placement and configurations at multiple scales, as shown in Figure 1. Note that since we make use of powerful prior knowledge encoded in 2D pre-trained generative models, we are able to generate open-vocabulary scenes in a zero-shot manner, for not only diverse room types in home settings, but also non-home environments such as grocery stores. Our experiments show that our framework outperforms prior scene creation approaches in generating interactable scenes that are more complex and realistic. We summarize our main contributions as follows: • We introduce Architect, a zero-shot generative pipeline that creates diverse, complex, and realistic 3D interactive scenes to advance Embodied AI agents and Robotics research. • We propose to leverage 2D prior from vision generative models to facilitate the 3D interactive scene generation process, and make such process controllable by initializing from simulation-rendered image for hierarchical inpainting, ensuring consistent spatial features and controllable camera parameters and depth scale, allowing accurate 2D to 3D lifting. • The experimental results show that our method outperforms previous approaches in generating more complex and realistic interactive 3D scenes, both quantitatively and qualitatively. Our code will be made publicly available."
https://arxiv.org/html/2411.09822v1,A Self-Supervised Model for Multi-modal Stroke Risk Prediction,"Predicting stroke risk is a complex challenge that can be enhanced by integrating diverse clinically available data modalities. This study introduces a self-supervised multimodal framework that combines 3D brain imaging, clinical data, and image-derived features to improve stroke risk prediction prior to onset. By leveraging large unannotated clinical datasets, the framework captures complementary and synergistic information across image and tabular data modalities. Our approach is based on a contrastive learning framework that couples contrastive language-image pretraining with an image-tabular matching module, to better align multimodal data representations in a shared latent space. The model is trained on the UK Biobank, which includes structural brain MRI and clinical data. We benchmark its performance against state-of-the-art unimodal and multimodal methods using tabular, image, and image-tabular combinations under diverse frozen and trainable model settings. The proposed model outperformed self-supervised tabular (image) methods by 2.6% (2.6%) in ROC-AUC and by 3.3% (5.6%) in balanced accuracy. Additionally, it showed a 7.6% increase in balanced accuracy compared to the best multimodal supervised model. Through interpretable tools, our approach demonstrated better integration of tabular and image data, providing richer and more aligned embeddings. Gradient-weighted Class Activation Mapping heatmaps further revealed activated brain regions commonly associated in the literature with brain aging, stroke risk, and clinical outcomes. This robust self-supervised multimodal framework surpasses state-of-the-art methods for stroke risk prediction and offers a strong foundation for future studies integrating diverse data modalities to advance clinical predictive modeling.","Stroke ranks as the second leading cause of death worldwide, responsible for 11.6% of global fatalities in 2019. It often results in neurological damage and long-term disability in adults, imposing significant health and economic challenges Pu et al. [2023], Feigin et al. [2021]. Early detection through predictive models is crucial in preventing severe outcomes, as cerebrovascular events can cause irreversible brain damage within hours Flora and Nayak [2019]. The complexity of stroke, driven by multiple risk factors, highlights the importance of integrating multi-modal data to improve diagnostic accuracy and treatment strategies. Among the various imaging techniques, Magnetic Resonance Imaging (MRI) stands out as a highly effective tool, offering high-resolution, non-invasive assessments of structural abnormalities and detailed visualization of the brain’s vascular network Hartwig et al. [2009]. Uni-modal predictive models Prior works mainly use convolutional neural networks (CNN) that can leverage the high-dimensional imaging information for diagnosing patients Zhang and Qie [2023]. Yu et al. applied deep learning algorithms to extract meaningful imaging features in an increasing order of hierarchical complexity to make predictions of the infarct volume Yu et al. [2020]. Other models that use only clinical data, often assume linear relationships between traditional risk factors such as age, gender, smoking status, blood pressure, diabetes, cholesterol levels, and body mass index Hippisley-Cox et al. [2024], An et al. [2020], You et al. [2023]. Alaa et al. used AutoPrognosis, an ensemble machine learning approach, to outperform conventional models like the Framingham score and Cox models Alaa et al. [2019]. A major limitation of these models is that they don’t integrate complementary information from other modalities, similar to how clinicians diagnose using multiple data sources. Biobanks like the UK Biobank (UKB) have become invaluable in this context, providing vast datasets integrating imaging and clinical information to train machine learning models for disease prediction Littlejohns et al. [2020], UK BioBank [2021]. Multi-modal predictive models Several studies have employed multi-modal data to improve diagnostic capabilities by integrating diverse data types Liu et al. [2015]. For example, MultiSurv model has shown success by fusing image and tabular data for cancer survival prediction Vale-Silva and Rohr [2021]. In another study, integration of retinal images and clinical data was leveraged to improve cardiovascular disease prediction Huang et al. [2024]. Multi-modal models combining image and clinical data have demonstrated better prediction performance for disability prediction in stroke patients White et al. [2023], Liu et al. [2023]. However, CNNs tend to prioritize image features, and simple image-tabular CNN concatenation fails to enhance predictive models due to insufficient cross-modal interactions. To address this, Wolf et al. developed the Dynamic Affine Feature Map Transform (DAFT), which conditions convolutional feature maps on both image and tabular data, enabling a two-way information exchange via an auxiliary neural network Wolf et al. [2022]. While DAFT reduces issues related to the large number of trainable parameters in standard 3D CNNs and the curse of dimensionality, it may sacrifice some predictive power compared to deeper models like ResNet. Although recent models show promise in biomedical prediction tasks, their clinical translation is hindered by limited annotated datasets, low disease prevalence, and the risk of overfitting. Self-supervised learning (SSL) is a powerful technique for extracting representative features from unlabeled data, making it valuable for early disease risk identification. Self-supervised models Unlike traditional supervised learning, SSL defines pretext tasks that allow models to learn meaningful representations from raw data Balestriero et al. [2023]. One prominent SSL technique is contrastive learning, which trains encoders to generate augmented views of a sample, maximizing similarity between these views while minimizing similarity with other samples Balestriero et al. [2023]. Popular methods such as SimCLR Chen et al. [2020], BYOL Grill et al. [2020], and MOCO He et al. [2020] have demonstrated success in imaging tasks, while VIME Houthooft et al. [2016] and SCARF Bahri et al. [2022] are leading approaches for tabular data. Emerging approaches, like contrastive language-image pre-training (CLIP) strategy, have evolved from unimodal methods to integrate diverse modalities. While there was an extensive work done for cardiovascular diseases prediction Radhakrishnan et al. [2023], Du et al. [2024], Hager et al. [2023], Girlanda et al. [2024], stroke risk prediction through volumetric brain images and clinical health records remains underexplored. We present for the first time, to the best of our knowledge, a self-supervised multi-modal approach integrating 3D brain MRIs with clinical tabular data for stroke risk prediction. As depicted in Figure 1, our methodology incorporates cross-modal interactions via CLIP loss Radford et al. [2021] and image-tabular matching (ITM) loss Li et al. [2021], Du et al. [2024]. We demonstrate that our learning strategy outperforms leading (self-)supervised unimodal methods and that multi-modal image-tabular pre-training leads to better representations and improved downstream performance. Lastly, we validate the model’s learned features through visual activation maps, which align with established clinical and neurological findings on stroke-related brain pathology. Code is available at https://github.com/CamilleDelgrange/SSMSRPM."
https://arxiv.org/html/2411.09798v1,Video Denoising in Fluorescence Guided Surgery,"Fluorescence guided surgery (FGS) is a promising surgical technique that gives surgeons a unique view of tissue that is used to guide their practice by delineating tissue types and diseased areas. As new fluorescent contrast agents are developed that have low fluorescent photon yields, it becomes increasingly important to develop computational models to allow FGS systems to maintain good video quality in real time environments. To further complicate this task, FGS has a difficult bias noise term from laser leakage light (LLL) that represents unfiltered excitation light that can be on the order of the fluorescent signal. Most conventional video denoising methods focus on zero mean noise, and non-causal processing, both of which are violated in FGS. Luckily in FGS, often a co-located reference video is also captured which we use to simulate the LLL and assist in the denoising processes. In this work, we propose an accurate noise simulation pipeline that includes LLL and propose three baseline deep learning based algorithms for FGS video denoising.","There are 320 million major surgeries performed worldwide every year in which 20 to 30\% of patients require re-admittance or have serious postoperative morbidity [1]. Many of these issues are due to difficulty in visualizing or identifying tissues that need removal or that should be avoided; for example, in an estimated 21\% of prostate cancer removal surgeries, cancer is found on the margin of removed tissues indicating cancer was likely left in the patient [2]. The current standard of care (SOC) in many surgeries rely heavily on non-quantitative measures such as surgeon perception of tissue under normal lighting or tactile tissue cues. Fluorescence guided surgery (FGS) is a promising technique to improve the SOC by giving surgeons a quantifiable fluorescence video feed that helps identify the state of different tissues leading to an improvement in surgical decision making and an improvement in patient outcomes [3]. FGS relies on a fluorescent contrast agent, either a drug or a naturally occurring fluorophore, that when imaged by an FGS imaging system helps delineate or classify tissues of interest. The most commonly used clinical contrast agents, such as indocynanine green (ICG), are bright and operate in the near infrared while others are dim and exist in the visible light spectrum so are more challenging to capture; these dim agents may not produce enough photons at video frame rates to be clinically viable. In this work, we consider software video denoising as a relatively unexplored and promising path forward to increase the sensitivity of current systems which will increase the number of clinically viable contrast agents. The operation of an FGS imaging system is shown in Fig. 1(a); first the system emits excitation light that excites the contrast agent, then the agent emits fluorescent emission light. The system collects both emission light and reflected excitation light from a scene point then an emission filter removes much of the excitation light. However, filters are imperfect so some light is not filtered out; we call unfiltered excitation light the Laser Leakage Light (LLL) which can be similar in brightness to emission light, and is a core difficulty in improving FGS systems [4, 5, 6]. The LLL and emission photons are added together by the fluorescence camera which outputs the fluorescence video (FV). A secondary reference video (RV) with the same field of view is simultaneously captured by a RGB camera using spectral and temporal filtering strategies [7]. The goal of a FGS denoiser is to take as input the FV and RV to produce a clean denoised FV while removing LLL. We find conventional video denoisers struggle in FGS denoising and LLL removal, so we develop a new set of baseline methods for FGS video denoising. FGS video denoising differs from standard video denoising in four key ways. First, the RV provides a helpful source of secondary information for computer vision algorithms such as providing structural and motion cues that can be used to improve denoising performance [8]. We find the RV is key to simulating and removing the LL. Second, the noise levels in FGS may be much higher than in standard video denoising problems potentially requiring long range temporal integration or larger efficient models. Third, a usable FGS denoiser must be real-time capable to fit into the clinical workflow. To remain hardware agnostic, we require methods to be causal where only the past and current frames are available; most conventional video denoising methods are non-causal. Finally, the noise present in the FV contains the standard shot and camera noise terms, but also an additional LLL noise term. Unlike prior work [8] which does not consider LLL, we model the LLL term as a spatially varying shot noise term that we predict using our LLL prediction network (LLL-PN). The LLL-PN takes as input a RV frame and outputs a LLL prediction. We note our strategy for dealing with LLL may also be used to deal with noise from naturally occurring fluorescence called auto-fluorescence if it is correlated with the RV. We then use our LLL-PN within a realistic noise model, shown in Fig. 1(b), that accurately simulates data seen on a commercial FGS system. We use simulated data to train our denoising algorithms for a wide array of signal and noise levels before testing them on real noisy data. Surprisingly, we find that state of the art video denoisers when adapted and retrained for this problem struggle, as shown in Fig. 1(c). For example, the causal version of BasicVSR++ [9] has trouble removing the strong LLL in this example leading to signal in the denoised result where there would be none. Surprisingly, we find that NafNet [10], an image denoiser that uses no temporal information, outperforms these video denoisers on this dataset while the opposite is true for conventional video denoising. NafNet also is more efficient to train, taking one third of the training time as BasicVSR++. Training efficiency is extremely important when dealing with data intensive problems such as video denoising, because it allows for practical training of larger models. Motivated by these findings we combine NafNet with different temporal propagation techniques from video denoisers to create a strong baseline model for FGS video denoising. We propose a recurrent structure, BL-RNN, with a NafNet backbone that provides robust performance with minimal network complexity and efficient training times. Contributions: • We double the size of existing FGS datasets, including new data necessary for properly simulating noise and real dim signals for testing. • We propose a novel method for simulating and removing spatially varying LLL in FGS, and simulate a specific commercial camera’s sensor noise. • We propose new network architectures for causal FGS video denoising that incorporates the most effective aspects of state of the art standard video and image denoising methods. Figure 2: Dataset Example Images: Here we show two example images for both OL-2023 and OL-2024. OL-2023 focuses on vasculature where as OL-2024 focuses on local fluorescent regions."
https://arxiv.org/html/2411.09766v1,NACNet: A Histology Context-aware Transformer Graph Convolution Network for Predicting Treatment Response to Neoadjuvant Chemotherapy in Triple Negative Breast Cancer,"Neoadjuvant chemotherapy (NAC) response prediction for triple negative breast cancer (TNBC) patients is a challenging task clinically as it requires understanding complex histology interactions within the tumor microenvironment (TME). Digital whole slide images (WSIs) capture detailed tissue information, but their giga-pixel size necessitates computational methods based on multiple instance learning, which typically analyze small, isolated image tiles without the spatial context of the TME. To address this limitation and incorporate TME spatial histology interactions in predicting NAC response for TNBC patients, we developed a histology context-aware transformer graph convolution network (NACNet). Our deep learning method identifies the histopathological labels on individual image tiles from WSIs, constructs a spatial TME graph, and represents each node with features derived from tissue texture and social network analysis. It predicts NAC response using a transformer graph convolution network model enhanced with graph isomorphism network layers. We evaluate our method with WSIs of a cohort of TNBC patient (N=105) and compared its performance with multiple state-of-the-art machine learning and deep learning models, including both graph and non-graph approaches. Our NACNet achieves 90.0% accuracy, 96.0% sensitivity, 88.0% specificity, and an AUC of 0.82, through eight-fold cross-validation, outperforming baseline models. These comprehensive experimental results suggest that NACNet holds strong potential for stratifying TNBC patients by NAC response, thereby helping to prevent overtreatment, improve patient quality of life, reduce treatment cost, and enhance clinical outcomes, marking an important advancement toward personalized breast cancer treatment.","Breast Cancer is the leading malignant disease and cause of cancer death in women worldwide [1]. Of all breast cancer subtypes, Triple Negative Breast Cancer (TNBC) presents the most aggressive progression, with a dismal prognosis and a high recurrence rate [1]. In current clinical practice, treatment option for this cancer subtype is limited. Although new immunotherapies have recently emerged, chemotherapy remains the primary treatment for patients with TNBC at both early and advanced stages. In particular, because of the absence of estrogen receptors (ER), progesterone receptors (PR), and human epidermal growth factor receptor 2 (HER2), TNBCs do not respond to hormone therapies or HER2-targeted treatments, significantly narrowing the range of therapeutic options. Consequently, chemotherapy, particularly neoadjuvant chemotherapy (NAC), continues to be the standard-of-care and predominant treatment option for this aggressive cancer subtype [2, 3, 4, 5, 6]. NAC treatment response is evaluated in resected tissues from the surgery using the residual cancer burden (RCB) metric. Based on RCB scores, two response classes are defined: Pathology Complete Response (pCR) with a post-treatment RCB of zero, and Residual Disease (RD), indicating an incomplete response. Clinically, pCR is often used as an endpoint for reoperative treatment and is strongly correlated with long-term clinical benefits [7, 8]. However, only about 30%\sim40% of TNBC patients respond well to NAC treatment, while the remaining patients either respond moderately or are refractory to NAC [9]. Despite its clear clinical benefits, for example a reduction in the tumor size or a downgrade in breast cancer before the surgery, NAC can substantially decrease patient life quality with its adverse treatment effects [10]. For non-responders, NAC not only introduces unnecessary toxicity but also delays alternative treatments and surgery, leading to adverse outcomes and overtreatment. Unfortunately, there remains a critical, unmet need to accurately predict NAC treatment response at the time of diagnosis. An accurate treatment response prediction remains a major clinical challenge. In recent years, machine learning has been successfully applied to histopathology images for predicting survival outcomes and treatment responses in the clinical oncology [11, 12]. Numerous studies have demonstrated the effectiveness of image-based machine learning in improving clinical decision-making. For example, deep learning models have been used to predict early treatment response in metastatic colorectal cancer by identifying subtle morphological changes in tumors, surpassing traditional size-based assessment methods [13]. In breast cancer research, deep learning frameworks have been used to integrate histopathology images with additional data types, such as genomic and radiomic features, to predict NAC response [14, 15, 16]. These models provide non-invasive methods for assessing treatment efficacy and guiding personalized therapy strategies. The successful applications of deep learning across various cancer types suggest its potential to revolutionize clinical treatment response prediction, offering more accurate and personalized approaches to support patient care. Thanks to significant advances in tissue scanning technology, whole slide images (WSIs) of tissue sections can now be routinely produced to capture cell-level tissue details. However, due to their giga-pixel scale, WSIs pose significant computational challenges, making it difficult to directly deploy machine learning and image analysis techniques for large-scale automated analyses. [17, 18, 19]. To manage the computational demand, it is a common practice to partition giga-pixel WSIs into numerous small image tiles and analyze them using a multiple instance learning (MIL) strategy [20, 21, 22, 23, 24, 25]. In MIL, tiles from each WSI are treated as instances from a bag that share the same patient-level label [26, 27]. Each tile is assigned a predicted label and these tile-level labels are then aggregated to form a comprehensive patient-level representation [28, 29, 30]. However, this MIL strategy may be less effective due to the high tissue heterogeneity within the breast cancer tumor microenvironment (TME), which breaks the MIL assumption that all instances in the negative (RD) bag should have a negative label [31]. Notably, TNBCs exhibit significant TME heterogeneity, where the presence of tumor-infiltrating lymphocytes are associated with a better prognosis and an improved response to chemotherapy [6]. Numerous studies further highlight that TME histological interactions strongly correlated with NAC response in TNBC patients [31, 32, 33, 34]. However, MIL-based analyses ignore these spatial TME histology distributions and interactions, remaining insensitive to local tissue context and the global structure organization. Graph Convolution Network (GCN) has been developed to partially address this problem. It represents each WSI as a graph consisting of nodes and edges, and predicts the WSI label by aggregated features of nodes in local neighborhoods [35, 36, 37, 25]. Learning a structured graph requires an effective graph representation [38]. However, constructing a context-aware, effective, and minimal WSI topological representation to hence predictive discriminating power remains an open challenge. Additionally, there is limited research on how to generate biologically meaningful graph node representations and edge connections for cancer grading and subtyping tasks using histopathology WSIs. Most current approaches cutting WSIs into tiles, then develop GCNs at the tile level, and finally aggregate the outcomes from the tile nodes to create overall WSI-level predictions [25, 39]. Although tile-level approaches have advanced research in WSI processing, they fall short in constructing WSI graphs with well-informed node labels and attributes. When the objective is to identify the entire tumor region or capture the TME connectivity in WSIs, where nodes characterize disease stage, it becomes important to incorporate both regional and WSI-level information with accurately labeled nodes and detailed attribute features. Therefore, label-informed WSI graph structure learning methods are needed for these prediction and analysis. To address the challenges posed by spatial heterogeneity in TNBC WSIs, we developed a multi-step NAC prediction network (NACNet) that leverages a WSI-derived graph representation and a transformer-based graph convolution network (GCN) for NAC response prediction in TNBC patients. Our approach leverages clinically relevant TME information such as cellular (e.g., lymphocyte infiltration) and extracellular matrix (e.g., collagen) heterogeneity for an accurate treatment response prediction. First, we train a deep learning model to recognize local histology labels and produce histology label maps for WSIs. Next, we construct a spatial TME graph using tile-level histology label map, extract context aware graph features, and predict the NAC treatment response using a transformer GCN model that is boosted with Graph Isomorphism Network (GIN) layers [40]. We applied the NACNet to a TNBC patient cohort, and demonstrated that incorporating accurate spatial TME structures enhanced NAC response prediction."
https://arxiv.org/html/2411.09758v1,Partial Multi-View Clustering via Meta-Learning and Contrastive Feature Alignment,"Partial multi-view clustering (PVC) presents significant challenges practical research problem for data analysis in real-world applications, especially when some views of the data are partially missing. Existing clustering methods struggle to handle incomplete views effectively, leading to suboptimal clustering performance. In this paper, we propose a novel dual optimization framework based on contrastive learning, which aims to maximize the consistency of latent features in incomplete multi-view data and improve clustering performance through deep learning models. By combining a fine-tuned Vision Transformer and k-nearest neighbors (KNN), we fill in missing views and dynamically adjust view weights using self-supervised learning and meta-learning. Experimental results demonstrate that our framework outperforms state-of-the-art clustering models on the BDGP and HW datasets, particularly in handling complex and incomplete multi-view data.","Multiview data encompasses diverse features from different perspectives, such as sensors, multimodal inputs, observation points, and sources. Nowadays, multiview data has become quite common in practice due to the availability of rich multimedia data sampling equipment. For example, a video can contain image views, text views, and voice views (Wang et al., 2023a). However, the challenge of processing multiview data arises from the lack of available labels. A fundamental solution is to apply multiview clustering (MVC) (Li et al., 2019; Wen et al., 2022; Xu et al., 2022) to separate unlabeled multiview data into different clusters. Data within the same cluster is highly likely to belong to the same group while also belonging to the same category. Due to its expected performance, multiview clustering has been extensively studied, and numerous methods for multiview clustering (Tao et al., 2020; Zhang et al., 2017; Jiang et al., 2022) have been proposed."
https://arxiv.org/html/2411.10442v1,"Enhancing the Reasoning Ability of Multimodal Large Language Models 
via Mixed Preference Optimization","Existing open-source multimodal large language models (MLLMs) generally follow a training process involving pre-training and supervised fine-tuning. However, these models suffer from distribution shifts, which limit their multimodal reasoning, particularly in the Chain-of-Thought (CoT) performance. To address this, we introduce a preference optimization (PO) process to enhance the multimodal reasoning capabilities of MLLMs. Specifically, (1) on the data side, we design an automated preference data construction pipeline to create MMPR, a high-quality, large-scale multimodal reasoning preference dataset; and (2) on the model side, we explore integrating PO with MLLMs, developing a simple yet effective method, termed Mixed Preference Optimization (MPO), which boosts multimodal CoT performance. Our approach demonstrates improved performance across multiple benchmarks, particularly in multimodal reasoning tasks. Notably, our model, InternVL2-8B-MPO, achieves an accuracy of 67.0 on MathVista, outperforming InternVL2-8B by 8.7 points and achieving performance comparable to the 10\times larger InternVL2-76B. We hope this study could inspire further advancements in MLLMs. Code, data, and model shall be publicly released.","Figure 1: Open-source model performance on MathVista. The X- and Y-axes represent the accuracy evaluated with direct-answer responses and CoT responses, respectively. The bubble size is positively correlated with the number of model parameters. The values in parentheses indicate the performance gap between CoT and direct-answer responses. Notably, most open-source models perform worse when answering with CoT. With the remarkable success of large language models (LLMs) [92, 93, 26, 5, 89, 11, 10, 1] in the field of natural language processing, the training paradigm comprising pre-training and supervised fine-tuning (SFT) have also swept the multimodal field, becoming the primary choice for the research and development of multimodal large language models (MLLMs). Benefiting from the large-scale pre-training corpora [48, 99, 80, 90, 114, 43] and high-quality SFT data [98, 55, 53, 20, 24], a series of open-source MLLMs [98, 20, 52, 46, 105, 44, 6, 96] exhibit strong performance across various domain and tasks, some even achieving results comparable to commercial models such as GPT-4o [70] and Gemini [88, 78]. However, open-source MLLMs still exhibit limited reasoning capabilities. As shown in Figure 1, InternVL2-8B [20] achieves a score of 58.3 on MathVista [61], a benchmark for multimodal reasoning, when using direct answers but drops to 56.8 with Chain-of-Thought (CoT) reasoning, indicating that CoT reasoning actually reduces its performance. This decline is commonly observed across open-source MLLMs [44, 105, 20, 96]. We attribute this phenomenon primarily to a distribution shift introduced by the SFT loss. Specifically, SFT relies on teacher forcing, where the model is trained to predict the next token based on previous ground-truth tokens. However, during inference, models must predict each token based on their own prior outputs, leading to a distribution shift between training and inference. Since the direct-answer approach requires only brief responses, while CoT reasoning involves generating a long rationale, the distribution shift problem becomes more severe during CoT. This results in models performing worse with CoT reasoning compared to direct-answer responses. To address the limitations of CoT reasoning in MLLMs, we draw inspiration from recent NLP approaches [74, 42, 103] that use Preference Optimization (PO) techniques to align model outputs with desired reasoning patterns. Specifically, methods like Direct Preference Optimization (DPO) [76] allow models to learn from preference signals to generate responses that better align with user requirements, offering the foundation for Reinforcement Learning from Human Feedback (RLHF). While RLHF has been explored for MLLMs primarily to reduce hallucinations [85, 106, 18], its application for enhancing multimodal reasoning remains under-explored. Building on these insights, we conduct a systematic study on using PO to strengthen the multimodal reasoning capabilities of MLLMs. Enhancing the multimodal reasoning abilities of MLLMs through PO presents several challenges: (1) Limited multimodal reasoning preference data and high annotation cost. Existing multimodal preference datasets [107, 106, 85, 47, 111] primarily address hallucination issues and focus on natural images and perception data, lacking scientific images and reasoning data. Annotating these types of data requires human annotators to carefully compare the given reasoning processes, making it both time-consuming and costly. (2) Lack of open-source methods for improving multimodal reasoning via PO. Although previous works have explored fine-tuning MLLMs using feedback from various sources, these models typically exhibit performance gains on hallucination benchmarks, with little enhancement in general reasoning abilities. Thus, leveraging PO to improve multimodal reasoning capabilities remains largely under-explored. This work addresses these challenges from both the data and model sides. (1) On the data side, we design an automated preference data construction pipeline to create MMPR, a high-quality, large-scale multimodal reasoning preference dataset. (2) On the model side, we explore various PO methods with MLLMs, introducing a simple yet effective method, termed Mixed Preference Optimization (MPO), which boosts multimodal CoT performance without the requirement for a reward model. Specifically, we propose a continuation-based pipeline called Dropout Next Token Prediction (DropoutNTP) for samples lacking clear ground truth and a correctness-based pipeline for samples with clear ground truth. In DropoutNTP, the responses generated by InternVL2-8B are considered as positive samples. For a given chosen response, we truncate it by half and then prompt InternVL2-8B to complete the remaining portion of the truncated answer without access to the image input. This generated completion serves as the rejected answer for the paired sample. Experimental results in Section 5.2 demonstrate that this straightforward method achieves comparable performance in reducing hallucinations compared to the divide-and-conquer method proposed in RLAIF-V [107]. In the correctness-based pipeline, multiple solutions to each question are sampled from InternVL2-8B. Solutions matching the ground truth answer are used as chosen responses, while those that do not are used as rejected responses. Additionally, we propose the MPO method. The key insight behind this algorithm is that an effective PO process should enable the model to learn the relative preference between pairs of responses, the absolute quality of individual responses, and the process for generating preferred responses. Compared to previous multimodal PO methods [107, 106, 85, 47, 75, 111], our approach excels in the following aspects: (1) Efficient automated data construction pipeline: Our pipeline enables high-quality preference pair generation at a controlled cost. (2) Effectiveness across diverse domains: Models fine-tuned with our data and approach show superior performance across reasoning, question-answering, and hallucination benchmarks. (3) Improvements over SoTA settings: Our results are based on InternVL2-8B, one of the leading open-source MLLMs, further highlighting the potential of our method. In summary, our main contributions are as follows: (1) We propose an efficient preference data construction pipeline. Based on this pipeline, we create MMPR, a high-quality, large-scale multimodal reasoning preference dataset containing approximately 3 million samples. (2) We introduce MPO, an effective PO algorithm designed to improve the reasoning abilities of MLLMs. The resulting model, InternVL2-8B-MPO, exhibits enhanced multimodal reasoning ability and fewer hallucinations compared to its baseline model (i.e., InternVL2-8B). (3) We conduct extensive experiments to explore practical approaches for improving multimodal reasoning via PO. Results show that PO significantly improves reasoning abilities over SFT. Notably, the proposed InternVL2-8B-MPO achieves an accuracy of 67.0 on MathVista [61], outperforming InternVL2-8B by 8.7 points and achieving performance comparable to the 10\times larger InternVL2-76B."
https://arxiv.org/html/2411.10436v1,Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization,"Multimodal Large Language Models (MLLMs) are known to hallucinate, which limits their practical applications. Recent works have attempted to apply Direct Preference Optimization (DPO) to enhance the performance of MLLMs, but have shown inconsistent improvements in mitigating hallucinations. To address this issue more effectively, we introduce Hallucination-targeted Direct Preference Optimization (HDPO) to reduce hallucinations in MLLMs. Unlike previous approaches, our method tackles hallucinations from their diverse forms and causes. Specifically, we develop three types of preference pair data targeting the following causes of MLLM hallucinations: (1) insufficient visual capabilities, (2) long context generation, and (3) multimodal conflicts. Experimental results demonstrate that our method achieves superior performance across multiple hallucination evaluation datasets, surpassing most state-of-the-art (SOTA) methods and highlighting the potential of our approach. Ablation studies and in-depth analyses further confirm the effectiveness of our method and suggest the potential for further improvements through scaling up.","Large Language Models (LLMs) have been verified in various field OpenAI (2024); Dubey et al. (2024); Sun et al. (2024), while they encounter challenges such as hallucination. Multimodal Large Language Models (MLLMs) are also known to hallucinate Bai et al. (2024). Specifically, they often produce unfaithful content that does not align with the visual input, which undermines their reliability and practicality, particularly in critical applications such as autonomous driving Cui et al. (2024) or medical tasks Liu et al. (2023a). Hence, addressing MLLM hallucination (M-hallu) is essential. Recent efforts have aimed at mitigating M-hallu through various approaches, including inference-stage strategies like contrastive decoding Leng et al. (2024), and post-hoc corrections that employ external visual models to refine responses Yin et al. (2023). While these methods are simple and training-free, they do not fundamentally enhance the model’s intrinsic capabilities. Meanwhile, some pioneer preference optimization methods like Direct Preference Optimization (DPO) Rafailov et al. (2024) have been introduced, which encourage the model to learn from the comparisons between positive and negative samples, alleviating hallucinations Zhao et al. (2023); Pi et al. (2025). However, most current methods cannot deliver consistent improvements across all types of MLLM hallucination tasks (e.g., VQA and captioning tasks, as shown in our experiments of Table 1). Additionally, it appears that the model’s improvement on specific tasks is closely related to the format of the training data. For instance, the DPO data of SeVa Zhu et al. (2024) primarily consists of VQA, which explains its strong performance on VQA-related hallucination evaluation. However, its results on captioning tasks are relatively unsatisfactory. Moreover, these methods do not explicitly consider diverse sources of M-hallu. Hence, we argue that if we focus on mitigating multimodal hallucinations, we should be able to address diverse types of hallucination causes and tasks, and design hallucination-targeted preference pairs for DPO accordingly. Our goal is to comprehensively alleviate all multimodal hallucination problems, including both discriminative tasks (e.g., VQA) and generative tasks (e.g., image captioning). Different from the hallucinations in LLMs, M-hallu primarily arises from the following three aspects: (1) Insufficient visual capability: This occurs when the MLLM’s visual encoder lacks the necessary strength, being distracted by relatively unimportant visual information, leading to hallucinations; (2) Incapable long-context generation: We observe that hallucinations become more pronounced as the generated content grows longer, similar to long-range forgetting, which needs to be addressed in practical applications; (3) Multimodal conflicts: Multimodal conflicts frequently arise in real-world scenarios due to the inevitable noises in texts and images. MLLMs are more prone to hallucinations with conflicting information existing between text and image Liu et al. (2024b). To address the aforementioned challenges, we propose Hallucination-targeted Direct Preference Optimization (HDPO) to mitigate M-hallu. Our approach constructs hallucination-targeted preference pairs, specifically designed to address various forms and causes of hallucinations. Specifically, we design three types of DPO data reflecting the corresponding hallucination causes as follows: (1) For insufficient visual capability, during the model’s autoregressive decoding, we preserve only some visual tokens with the lowest attention scores to produce targeted negative responses that reflect incorrect visual information distraction, urging MLLMs to pay attention to more effective visual information. (2) For incapable long context generation, we specifically select positive examples from high-quality long-form captions, while creating negative examples where the latter part of the response deviates from the image content, simulating long-form hallucinations. (3) For multimodal conflicts, we add conflicting information with images into prompts to generate negative examples. We provide positive and negative pairs with questions featuring conflicting prefixes to train the model to correctly respond to the question even containing conflicting information. We conduct extensive experiments to evaluate our approach across various types of M-hallu tasks. The results demonstrate that our HDPO framework achieves the overall best performance in effectively mitigating MLLM hallucinations on various tasks. Our contributions are summarized as follows: • We analyze three key causes behind MLLM hallucinations from visual capability, long-context generation, and multimodal conflicts aspects, offering valuable insights to guide future advancements. • Based on these analyses, we propose a novel HDPO, aiming to jointly address all types of M-hallu tasks. To the best of our knowledge, we are the first to adopt hallucination-targeted DPO from diverse aspects with our novel DPO data construction strategies. • Through extensive experiments on different datasets, HDPO demonstrates consistent improvements in all types of M-hallu tasks."
https://arxiv.org/html/2411.10403v1,On the Foundation Model for Cardiac MRI Reconstruction,"In recent years, machine learning (ML) based reconstruction has been widely investigated and employed in cardiac magnetic resonance (CMR) imaging. ML-based reconstructions can deliver clinically acceptable image quality under substantially accelerated scans. ML-based reconstruction, however, also requires substantial data and computational time to train the neural network, which is often optimized for a fixed acceleration rate or image contrast. In practice, imaging parameters are often tuned to best suit the diagnosis, which may differ from the training data. This can result in degraded image quality, and multiple trained networks are needed to fulfill the clinical demands. In this study, we propose a foundation model that uses adaptive unrolling, channel-shifting, and Pattern and Contrast-Prompt-UNet (PCP-UNet) to tackle the problem. In particular, the undersampled data goes through a different number of unrolled iterations according to its acceleration rate. Channel-shifting improves reconstructed data quality. The PCP-UNet is equipped with an image contrast and sampling pattern prompt. In vivo CMR experiments were performed using mixed combinations of image contrasts, acceleration rates, and (under)sampling patterns. The proposed foundation model has significantly improved image quality for a wide range of CMR protocols and outperforms the conventional ML-based method.","Cardiac magnetic resonance (CMR) imaging is a widely applied clinical tool for the diagnosis of various cardiac diseases. CMR exams are time-consuming and research currently focuses on ways to reduce exam times. Most approaches to faster CMR exams rely on undersampling the acquired k-space data and using advanced reconstruction methods to generate images of acceptable clinical quality. In particular, recent advances in machine learning (ML) based reconstruction [1, 2, 3, 4, 5, 8] have enabled substantial increases in acceleration rates (e.g. 10-20x) for CMR, which extends its availability to patients, as well as enabling advanced CMR techniques [9]. These approaches, however, are generally bespoke for each acquisition protocol. ML-based reconstruction approaches also require substantial amounts of data and compute time to be trained, but once trained provide very fast reconstruction times that outperform conventional methods (e.g. compressed sensing). Existing ML-based reconstructions can be split into two major categories: Scan-specific and data-driven approaches. In the former, neural networks learn to interpolate k-space from the fully sampled k-space center (i.e. calibration lines) of the image to be reconstructed [5, 6, 7]. In the latter, a neural network is trained using a large imaging database [1, 2, 3, 4]. Restricted by the limited number of calibration lines, scan-specific approaches are typically forced to employ compact network designs, and perform poorly when reconstructing images acquired with high acceleration rates. In contrast, when sufficient data is available for training, the data-driven approaches leverage more advanced network designs, including transformers [10] and diffusion models [11]. The data-driven approach can pushing the acceleration rates even higher while maintaining satisfactory image quality. To optimize image quality and minimize the risk of distribution shifts, protocol-specific learning is commonly adapted in data-driven approaches [1, 2, 9], which requires that the images for used training and inference are matched in image contrast, sampling patterns, and acceleration rates. Any mistmatch between training and inference will typically lead to a significant degradation in image quality [1]. This is a major practical issue of the data-driven approaches because the imaging protocol and parameters are generally refined for each CMR exam. In a typical CMR exam, images of multiple views and contrast are acquired to assist diagnosis. Further adjustments in sampling pattern and acceleration rates may also be needed [12, 13, 14]. One could deploy multiple networks matched to all exam possibilities, but this adds substantial complexity and is not generally feasible. In this work, we propose a universal foundation model for machine learning-based CMR reconstruction that allows reconstruction of mixed contrast, views, sampling patterns, and acceleration rates. To do so we employ an unrolled network [1] architecture to solve a regularized reconstruction problem. We inherit the analytical conclusion from compressed sensing [15], to handle different acceleration rates by using different numbers of unrolled steps. We adapt the Prompt-UNet [16] for mixed image contrasts and views, and extend it with additional sampling embedding, which we refereed as Pattern and Contrast-Prompt-UNet (PCP-UNet) to optimize image quality of different sampling patterns. To better reconstruct various sampling patterns, we also apply a novel channel-shift technique to achieve a wider receptive field in the PCP-UNet with minor computational overhead. Experiments were conducted using in vivo CMR data from the CMRxRecon 2024 challenge. The proposed method outperformed a simple unrolled network with a fixed number of unrolled steps using either UNet or Prompt-UNet. The proposed PCP-UNet outperformed Prompt-UNet using fixed or adaptive unrolls. Statistical results showed improved structural similarity index measure (SSIM) were obtained using PCP-Unet, which is suitable for handling reconstruction of images with a variety of image contrast, views, sampling patterns, and acceleration rates."
https://arxiv.org/html/2411.10323v1,"The Dawn of GUI Agent: A Preliminary 
Case Study with Claude 3.5 Computer Use","The recently released model, Claude 3.5 Computer Use, stands out as the first frontier AI model to offer computer use in public beta as a graphical user interface (GUI) agent. As an early beta, its capability in the real-world complex environment remains unknown. In this case study to explore Claude 3.5 Computer Use, we curate and organize a collection of carefully designed tasks spanning a variety of domains and software. Observations from these cases demonstrate Claude 3.5 Computer Use’s unprecedented ability in end-to-end language to desktop actions. Along with this study, we provide an out-of-the-box agent framework for deploying API-based GUI automation models with easy implementation. Our case studies aim to showcase a groundwork of capabilities and limitations of Claude 3.5 Computer Use with detailed analyses and bring to the fore questions about planning, action, and critic which must be considered for future improvement. We hope this preliminary exploration will inspire future research into the GUI agent community. All the test cases in the paper can be tried through the project: https://github.com/showlab/computer_use_ootb.","Automating desktop tasks has become an increasingly popular area of research, driven by the need to enhance users’ productivity and accessibility across various application environments. From web navigation to professional software and even video games, users frequently encounter repetitive tasks that could benefit from automation. While large language models like GPT-4 and Qwen-2-VL have demonstrated their potential in automating tasks through general GUI interaction, the capacity of these models is still far from enough for applicable desktop task automation. Recent studies in GUI automation agents have leveraged general-purpose LLMs to interact with graphical user interfaces (GUIs) by understanding the GUI state and generating actions. However, the release of Claude 3.5 Computer Use by Anthropic marks a significant advancement in this domain, introducing the first frontier AI model to offer computer use in public beta. Unlike previous models, Claude 3.5 Computer Use offers an end-to-end solution through API calls, actions will be generated from user instruction and observed purely visual GUI state, without requiring further external knowledge such as reference plan and GUI parsing. Despite this advancement, the community needs a comprehensive analysis that evaluates the performance of API-based GUI automation models in depth. To take the first steps to explore the capacities and limitations of such models, we propose a comprehensive case study based on real-world desktop environments, encompassing a diverse range of software domains, including web navigation, professional tools, and games. The selected cases are designed to reflect the needs of various user groups, ensuring that the evaluation covers a broad spectrum of desktop automation tasks. To isolate specific aspects of the model’s capability, we evaluate the performance of API-based GUI automation models rigorously across three dimensions: • Planning: Assessing the model’s ability to generate an executable plan from the user’s query. The plan should have a correct flow, allowing the overall successful operations of the software, with each step being clear and executable. • Action: Evaluating whether the model can accurately ground the interactable GUI elements and execute the action step-by-step from the derived plan. • Critic: Measuring the model’s awareness of the changing environment, including its ability to adapt to the outcomes of its actions, such as retrying tasks if unsuccessful or terminating execution when the task is completed. To our best knowledge, this is the first comprehensive case study on Claude 3.5 Computer Use and API-based GUI automation models. We hope that our research provides the community with valuable insights into the capacities and limitations of these models. Our case study aim to lay the foundation for the continued exploration and benchmarking of API-based GUI automation. Additionally, to facilitate the community to discover and benchmark the newly released model, we also release an out-of-the-box universal framework, namely Computer Use OOTB, providing a seamless solution for users and researchers to deploy these models in local environments without the need for complex setup or configuration, aiming to improve the accessibility of GUI automation research field. Our contributions in this report are summarized as follows. • We present a comprehensive case study for Claude 3.5 Computer Use on desktop task automation, covering domains such as web search, professional software, and games, designed to reflect the needs of various user groups. • We introduce an out-of-the-box, cross-platform agent framework for deploying API-based GUI automation models, offering a universal solution for easy implementation and benchmarking. • We conduct extensive human evaluations and provide in-depth analyses, demonstrating both the advancements and limitations of the newly released API-based GUI automation model."
https://arxiv.org/html/2411.10175v1,The Surprising Ineffectiveness of Pre-Trained Visual Representations for Model-Based Reinforcement Learning,"Visual Reinforcement Learning (RL) methods often require extensive amounts of data. As opposed to model-free RL, model-based RL (MBRL) offers a potential solution with efficient data utilization through planning. Additionally, RL lacks generalization capabilities for real-world tasks. Prior work has shown that incorporating pre-trained visual representations (PVRs) enhances sample efficiency and generalization. While PVRs have been extensively studied in the context of model-free RL, their potential in MBRL remains largely unexplored. In this paper, we benchmark a set of PVRs on challenging control tasks in a model-based RL setting. We investigate the data efficiency, generalization capabilities, and the impact of different properties of PVRs on the performance of model-based agents. Our results, perhaps surprisingly, reveal that for MBRL current PVRs are not more sample efficient than learning representations from scratch, and that they do not generalize better to out-of-distribution (OOD) settings. To explain this, we analyze the quality of the trained dynamics model. Furthermore, we show that data diversity and network architecture are the most important contributors to OOD generalization performance. 00footnotetext: *Correspondence to: moritz.schneider@de.bosch.com Project website: https://schneimo.com/pvr4mbrl","Reinforcement Learning (RL) provides an elegant alternative to classic planning and control schemes, as it allows for complex behaviors to emerge by just specifying a reward, rather than hand-modelling and tuning environments and agents. Despite their success, most methods need extensive data and can be used only on their respective task, lacking the generalization capabilities needed to handle the complexity of real world tasks. On hardware, RL is costly in terms of time and wear, therefore model-based approaches are attractive as they promise to improve sample efficiency. For many real-life problems, vision is an invaluable source of state information, but due to its high-dimensional nature it is challenging to incorporate it in RL algorithms. Therefore, the use of pre-trained visual representations (PVRs) is attractive as, intuitively, it promises to improve sample efficiency and generalization. Most existing approaches use or investigate PVRs in the context of model-free RL. For example, CLIP [1] is already widely used as pre-trained vision model for model-free robotic RL tasks [2, 3, 4]. One would assume that the benefits such representations yield for model-free settings equally apply to model-based methods. In model-based reinforcement learning (MBRL), features of convolutional neural networks (CNNs) are usually used as visual state representations, whereas other representation types such as keypoints, surface normals, depth, segmentation and pre-trained representations are often ignored. Moreover, model-based methods are usually trained under an objective mismatch as the training process is required to optimize the accuracy of the dynamics model and the overall performance of the agent at the same time [5]. Naturally, this makes the training procedure different and more difficult than in model-free settings. In this work, we focus on model-based RL and benchmark a set of representative PVRs on a set of challenging control tasks. To this end we want to answer the following question: i Is MBRL more sample efficient when using PVRs in contrast to learning a representation from scratch? Furthermore, PVRs are nowadays increasingly often trained on general datasets (e.g. ImageNet [6], Ego4D [7], etc.) and should be reusable in a wide variety of downstream tasks without further finetuning on in-domain data. We would like to empower downstream RL algorithms with corresponding generalization capabilities. Most existing implementations only investigate the distribution shift for the PVRs, but not for the downstream RL algorithm. This leads us to the additional questions: ii Can model-based agents generalize better to out-of-distribution (OOD) settings with PVRs (i.e. can PVRs pass on their generalization capabilities to model-based agents)? iii How important are different training properties like data diversity and network architecture of a PVR for model-based agents on a downstream RL task? Compared with the model-free RL approach, MBRL methods learn accurate models of the environment for efficient learning and planning. Therefore, additionally we want to investigate the final question: iv How does the quality of the learned dynamics models in terms of accumulation errors and prediction quality differ between models trained from scratch and those based on PVRs? Contributions. The key contributions of this paper are summarized as follows: • Benchmarking PVRs for MBRL. Using PVRs trained on diverse and general data, we study the generalization capabilities to out-of-distribution (OOD) settings of model-based agents utilizing these PVRs. To the best of our knowledge, we perform the first such comparison for MBRL. • OOD Evaluation for MBRL and PVRs. Most other benchmarks only look into the case that PVRs should facilitate better training performance. We additionally look into the case of shifting the distribution also for the underlying MBRL agent, i.e., we have large differences between training and evaluation set. In this way, we investigate to what extent PVRs transfer their generalization capabilities to downstream RL agents. Our experiments reveal that PVRs are often ineffective for the MBRL agents. Furthermore, agents using representations learned from scratch outperform the PVR-based agents most of the time. • Important OOD Properties of PVRs. We investigate and discuss important properties of PVRs for generalization in downstream control tasks in a MBRL context. We find that data diversity and network architecture are the most important contributors to OOD generalization performance. • Analysis of Model Quality. To explain our results more in depth, we analyze the quality of the trained world models and find that those which use representations learned from scratch are in general more accurate and have less accumulation errors regarding predicted rewards than the ones using PVRs."
https://arxiv.org/html/2411.10063v1,Federated Domain Generalization via Prompt Learning and Aggregation,"Federated domain generalization (FedDG) aims to improve the global model’s generalization in unseen domains by addressing data heterogeneity under privacy-preserving constraints. A common strategy in existing FedDG studies involves sharing domain-specific knowledge among clients, such as spectrum information, class prototypes, and data styles. However, this knowledge is extracted directly from local client samples, and sharing such sensitive information poses a potential risk of data leakage, which might not fully meet the requirements of FedDG. In this paper, we introduce prompt learning to adapt pre-trained vision-language models (VLMs) in the FedDG scenario, and leverage locally learned prompts as a more secure bridge to facilitate knowledge transfer among clients. Specifically, we propose a novel FedDG framework through Prompt Learning and AggregatioN (PLAN), which comprises two training stages to collaboratively generate local prompts and global prompts at each federated round. First, each client performs both text and visual prompt learning using their own data, with local prompts indirectly synchronized by regarding the global prompts as a common reference. Second, all domain-specific local prompts are exchanged among clients and selectively aggregated into the global prompts using lightweight attention-based aggregators. The global prompts are finally applied to adapt VLMs to unseen target domains. As our PLAN framework requires training only a limited number of prompts and lightweight aggregators, it offers notable advantages in computational and communication efficiency for FedDG. Extensive experiments demonstrate the superior generalization ability of PLAN across four benchmark datasets. We have released our code at https://github.com/GongShuai8210/PLAN.","In today’s era of distributed data sources and edge computing, there exists a substantial demand for collaboratively training machine learning models across multiple clients. Federated learning (FL) [1, 2] has emerged as a promising solution, enabling the development of accurate and robust models while maintaining data privacy. Unlike traditional centralized approaches, FL allows each local client to learn from its own data. Then, a central server periodically aggregates the local model parameters from all clients to generate a global model. Figure 1: The novel problem setting of FedDG aims to learn a global model from multiple decentralized source domains, enabling it to directly generalize to completely unseen target domains. Despite significant progress, traditional FL approaches [3, 4, 5, 6] primarily concentrate on enhancing model performance within the federation of clients. These methods often assume that data across all clients are identically distributed. In reality, however, clients independently collect local data, which naturally form mutiple source domains with distinct distributions. During deployment, the test data may also come from a target domain with previously unseen distributions. Therefore, how to generalize the federated model under domain shifts remains an underexplored issue. As a common solution to the domain shift issue, domain generalization (DG) [7, 8] techniques have been developed to enable models trained on source domains with heterogeneous distributions to generalize well to unseen target domains. Nevertheless, applying existing DG methods in the FL setting is not straightforward, as they typically operate in a centralized manner that requires full access to data from different source domains. To overcome this challenge, federated domain generalization (FedDG) [9] has been further introduced to synergize FL and DG. As illustrated in Fig. 1, FedDG facilitates collaborative model learning across diverse source domains for effective generalization to unseen target domains while keeping data privacy. Many prior studies on FedDG aim to share domain knowledge among clients under the privacy-preserving constraint, thereby enhancing the generalization ability of each local model by integrating knowledge from diverse domains. For example, Liu et al. [9] exchanged the amplitude spectrum information in frequency space among clients. Huang et al. [10] allowed the sharing of class prototypes of local samples across different clients. Chen et al. [11] performed cross-client style transfer by exchanging the mean and variance of each pixel-level feature channel. Notably, the aforementioned domain knowledge, including amplitude spectrum, class prototypes, and data style, is directly extracted from local samples and can be considered a reflection of their inherent characteristics. Sharing such sensitive information among clients potentially poses the risk of data leakage [12], which might not fully meet the requirements of FedDG. Recently, numerous foundational vision-language models (VLMs) [13, 14] have been developed by utilizing large-scale image-text pairs from the web. These VLMs acquire general knowledge about the associations between visual patterns and their corresponding textual descriptions. To preserve this general knowledge and mitigate the transfer gap to downstream tasks, the technique of prompt learning [15, 16] has been proposed, which freezes the parameters of VLMs while inserting a few learnable prompt tokens as additional inputs to VLMs. In a FL scenario, prompt learning can be conducted in each client, enabling the learned prompts to encapsulate domain knowledge and facilitate the adaptation of VLMs to the tasks at hand. It is important to note that prompts are optimized via learning rather than being generated directly from local samples in each client. Therefore, these prompts can serve as a more secure bridge to transfer domain knowledge among clients for FedDG. In this paper, we present a novel FedDG framework through Prompt Learning and AggregatioN (PLAN). Our PLAN method comprises two stages in each federated round: first, learning local prompts in each client, and then, synthesizing global prompts that effectively generalize to unseen target domains. A few recent works [17, 18] have begun to explore prompt learning in the context of FL. However, there remain two major limitations: 1) Clients independently learn local prompts, resulting in prompts biased towards each client’s private data. This bias can compromise the generalization of the global model; and 2) A common practice for aggregating local prompts into global prompts is by using fixed weights. However, diverse domain knowledge tends to contribute unequally to the global model, and neglecting these differences may significantly diminish model performance [19]. To address the above issues, PLAN introduces a reference-based prompt learning mechanism, which requires the local prompts in each client to align with a common reference—the global prompts generated in the previous round and shared among all clients. In this way, PLAN facilitates the indirect synchronization of local prompts across clients without the need for data sharing. Additionally, PLAN is equipped with the lightweight attention-based prompt aggregators, which measures the importance of local prompts from different clients and selectively aggregates them into the global prompts. During the aggregation, all local prompts remain unchanged, with only the aggregators’ parameters being updated. Note that for further improved performance, PLAN performs both text and visual prompt learning and aggregation on multiple blocks in VLMs. In each federated round, clients only need to train and exchange a limited set of prompt tokens and the lightweight aggregators with the server, rather than the entire model. Therefore, PLAN not only offers robust privacy-preserving capability but also enhances computational and communication efficiency for FedDG. Experimental results on four benchmark datasets demonstrate that our PLAN method significantly outperforms conventional FL, DG, and FedDG methods, as well as prompt learning-based methods. Ablation studies and hyperparameter analyses validate the key components of PLAN. We further verify the effectiveness of PLAN when faced with insufficient local data and confirm its advantages in both computational and communication costs. Visualization studies are also conducted to provide deeper insights into PLAN. In a nutshell, our main contributions are as follows: • Instead of sharing domain knowledge directly extracted from local samples, we introduce prompts learned with local data as a more secure means of transferring domain knowledge among clients in FedDG. • We propose a novel FedDG framework, PLAN, which integrates a reference-based prompt learning mechanism to facilitate the indirect synchronization of local prompts across clients and utilizes the lightweight attention-based prompt aggregators to selectively aggregate local prompts into global prompts. • We conduct extensive experiments on four benchmark datasets and demonstrate that our PLAN method achieves new state-of-the-art performance while maintaining communication and computation efficiency for FedDG. The remainder of the paper is structured as follows: Section II provides a review of related work. Section III presents the preliminary knowledge relevant to our study. Section IV details our PLAN method for FedDG. Experimental results and analysis are discussed in Section V, followed by the conclusions in Section VI."
https://arxiv.org/html/2411.10061v1,"EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation","Recent work on human animation usually involves audio, pose, or movement maps conditions, thereby achieves vivid animation quality. However, these methods often face practical challenges due to extra control conditions, cumbersome condition injection modules, or limitation to head region driving. Hence, we ask if it is possible to achieve striking half-body human animation while simplifying unnecessary conditions. To this end, we propose a half-body human animation method, dubbed EchoMimicV2, that leverages a novel Audio-Pose Dynamic Harmonization strategy, including Pose Sampling and Audio Diffusion, to enhance half-body details, facial and gestural expressiveness, and meanwhile reduce conditions redundancy. To compensate for the scarcity of half-body data, we utilize Head Partial Attention to seamlessly accommodate headshot data into our training framework, which can be omitted during inference, providing a free lunch for animation. Furthermore, we design the Phase-specific Denoising Loss to guide motion, detail, and low-level quality for animation in specific phases, respectively. Besides, we also present a novel benchmark for evaluating the effectiveness of half-body human animation. Extensive experiments and analyses demonstrate that EchoMimicV2 surpasses existing methods in both quantitative and qualitative evaluations.","Diffusion-based video generation has seen significant advancements[8, 12, 21, 8, 12, 21, 11, 5, 1, 9, 33, 3], prompting extensive research in human animation. Human animation, as a subset of video generation domain, aims at synthesizing natural and realistic human-centric video sequences from a reference character images. Current research on human animation commonly employs pretrained diffusion models as backbone, and involves corresponding condition injection modules to introduce control conditions[6, 14, 38, 26, 30, 31, 37, 17], so that lifelike animation can be well generated. Unfortunately, there is still gaps between academic study and industrial needs: 1) Head region limitation; 2) Condition-injection complexity. 1) Head Region Limitation. On the one hand, prior human animation works[6, 30, 31, 37] mainly focus on generating head-region videos, neglecting the synchronization of the audio and the shoulders-below body. The most recent work[17] has improved half-body animation with auxiliary conditions and injection modules beyond audio-driven module. 2) Condition-Injection Complexity. On the other hand, the commonly-used control conditions (e.g. text, audio, pose, optical flow, movement maps) can provide a solid foundation for lifelike animation. Particularly, current research efforts concentrate on aggregating supplementary conditions, which result in unstable training due to multi-condition incoordination, and elevated inference latency stemming from intricate condition-injection modules. For the first challenge, a straightforward baseline exists to accumulate conditions related to shoulder-below body, such as half-body key points maps. However, we discover that this approach remains infeasible because of increased complexity of conditions (for the second challenge). In this paper, to remedy the aforementioned issues, we introduce an novel end-to-end method-EchoMimicV2, building upon the portrait animation method EchoMimic [6]. Our proposed EchoMimicV2 strives for striking quality of half-body animation yet with simplified conditions. To this end, EchoMimicV2 exploits the Audio-Pose Dynamic Harmonization (APDH) training strategy to modulate both audio and pose conditions, and meanwhile reduce the redundancy of the pose condition. Additionally, it utilizes a stable training objective function, termed Phase-specific Loss (PhD Loss), to enhance motion, details, and low-level quality, replacing the guidance of redundant conditions. Specifically, APDH is inspired by the waltz dance step, where audio and pose condition perform as synchronized dance partners. As the pose gracefully steps back, the audio seamlessly advances, perfectly filling in the space to create a harmonious melody. As a result, the control scope of the audio condition is extended from the mouth to the entire body via Audio Diffusion, and meanwhile the pose condition is confined from the entire body to the hands via Pose Sampling. Given that the primary regions responsible for audio expression located in the mouth lips, we initiate our audio condition diffusion from the mouth part. Additionally, due to the complementarity of gestural and verbal communication, we retain hand pose condition for the gesture animation so that the Head Region Limitation challenge is overcome, extending to half-body region animation. Throughout this process, we find a free lunch for data augmentation. When audio condition only controls the head region via Head Partial Attention, we can seamlessly incorporate padded headshots data to enhance facial expressions without requiring additional plugins like[17]. We also list the advantages of our proposed EchoMimicV2 in Table 1. Moreover, we propose a stable training objective function, Phase-specific Loss (PhD Loss), with two goals: 1) to enhance the motion representation with incomplete pose; 2) to improve details and low-level visual quality not governed by audio. While it is intuitive to employ a multi-loss mechanism that integrates pose, detail, and low-level visual objective functions concurrently, such an approach typically requires extra models, including Pose Encoders and VAE Decoders. Given that the ReferenceNet-based backbone already demands significant computational resources[14], implementing a multi-losses training becomes impractical. Through experimental analysis, we segments the denoising process into three distinct phases, each with its primary focus: 1) Pose-dominant phase, where motion poses and human contours are initially learned; 2) Detail-dominant phase, where character-specific details are refined; and 3) Quality-dominant phase, where the model enhances the color and other low-level visual qualities. Consequently, the proposed PhD Loss is tailored to optimize the model for each specific denoising phase, that is, Pose-dominant Loss for the early phase, Detail-dominant Loss for the middle phase, and Low-level Loss for the final phase, ensuring a more efficient and stable training process. Additionally, to facilitate the community in quantitative evaluation of half-body human animation, we have curated a test benchmark, named EMTD, comprising half-body human videos sourced from the Internet. We conducted extensive qualitative and quantitative experiments and analyses, demonstrating that our method achieves state-of-the-art results. EchoMimicV2 CyberHost[17] Audio+RefImage + Hand Pose Sequence + Body Movement Sequence - + Face Crop Injection Module - + Hand Crop Injection Module - + Full-body Pose Guidence Table 1: The simplification of our proposed EchoMimicV2. In summary, our contributions are as follows: • We propose EchoMimicV2, an end-to-end audio-driven framework to generate striking half-body human animation yet driven by simplified conditions; • We propose APDH strategy to meticulously modulate audio and pose condition, and reduce pose condition redundancy; • We propose HPA, a seamlessly data augmentation to enhance the facial expressions in half-body animations, no need for additional modules; • We propose PhD Loss, a novel objective function to enhance the motion representation, appearance details and low-level visual quality, alternating the guidance of complete pose condition; • We provide a novel evaluation benchmark for half-body human animation. • Extensive experiments and analyses are conducted to verify the effectiveness of our proposed framework, which surpasses other state-of-the-arts methods. Figure 1: The overall pipeline of our proposed EchoMimicV2."
https://arxiv.org/html/2411.09998v1,"Adaptive Non-uniform Timestep Sampling for 
Accelerating Diffusion Model Training","As a highly expressive generative model, diffusion models have demonstrated exceptional success across various domains, including image generation, natural language processing, and combinatorial optimization. However, as data distributions grow more complex, training these models to convergence becomes increasingly computationally intensive. While diffusion models are typically trained using uniform timestep sampling, our research shows that the variance in stochastic gradients varies significantly across timesteps, with high-variance timesteps becoming bottlenecks that hinder faster convergence. To address this issue, we introduce a non-uniform timestep sampling method that prioritizes these more critical timesteps. Our method tracks the impact of gradient updates on the objective for each timestep, adaptively selecting those most likely to minimize the objective effectively. Experimental results demonstrate that this approach not only accelerates the training process, but also leads to improved performance at convergence. Furthermore, our method shows robust performance across various datasets, scheduling strategies, and diffusion architectures, outperforming previously proposed timestep sampling and weighting heuristics that lack this degree of robustness.","In recent years, diffusion models have demonstrated their high expressive power and achieved significant success across a wide range of domains, including image [9, 24, 23], video [18, 35], text [22, 4, 12], audio generation [26], combinatorial optimization [30, 25], reinforcement learning [1, 15, 32], and more. However, with the increasing complexity of target data distributions, achieving convergence in diffusion model training has become increasingly computationally demanding. For example, training images with Stable-Diffusion-2.0 [24] requires 24,000 A100 GPU hours, while Open-Sora [35] requires 48,000 H800 GPU hours for video training. These high computational costs not only pose significant barriers to advancing generative AI applications but also have negative environmental consequences. While numerous studies have explored ways to accelerate diffusion model training, our research specifically targets modifications to the uniform training process of diffusion models across diffusion timesteps. It is well recognized that non-uniform training schemes over timesteps can enhance training speed. For example, several heuristics have been proposed [7, 11, 31, 17]. However, the underlying reasons for the success of these methods remain largely unexplored, and they are often regarded as heuristics that may have potential limitations in robustness across diverse experimental conditions. Figure 1: Comparison of FID scores of our approach and other acceleration methods against relative wall clock time, where 1D represents the time it takes for the baseline to converge. Although our learning method is initially slower than the heuristics due to its learning-based nature, it converges to a point with better optimality within 1D, and achieves a significantly lower FID score by 1.2D. Through a series of experiments, we observed significant variation in the stochastic gradient variance across different diffusion training timesteps. We hypothesize that this variability plays a key role in the imbalanced training observed with uniform sampling and may explain the effectiveness of previously proposed heuristics in improving diffusion model training. However, due to the strong interdependence of gradients across different timesteps, we found that this insight alone does not directly lead to optimal acceleration in diffusion training. To this end, we propose a more direct approach by developing an efficient algorithm designed to sample timesteps that are estimated to yield the greatest reduction in the objective. This algorithm approximates the impact of gradient updates for each timestep on the variational lower bound and increases the sampling frequency of specific timesteps that require further optimization. This enables the diffusion training process to converge significantly faster than previous heuristics across various settings. To summarize, our three main contributions are as follows: 1. Through an analysis of gradient variance in diffusion model training, we offer an explanation for why a non-uniform training process across timesteps can lead to faster convergence. 2. Unlike previous heuristic-based diffusion acceleration methods, we propose a learning-based approach that adaptively samples timesteps by approximately minimizing the variational bound on the negative log-likelihood. 3. We conduct experiments across various image datasets, noise scheduling strategies and diffusion architectures to demonstrate the robustness of our method."
https://arxiv.org/html/2411.09914v1,mmSpyVR: Exploiting mmWave Radar for Penetrating Obstacles to Uncover Privacy Vulnerability of Virtual Reality,"Virtual reality (VR), while enhancing user experiences, introduces significant privacy risks. This paper reveals a novel vulnerability in VR systems that allows attackers to capture VR privacy through obstacles utilizing millimeter-wave (mmWave) signals without physical intrusion and virtual connection with the VR devices. We propose mmSpyVR, a novel attack on VR user’s privacy via mmWave radar. The mmSpyVR framework encompasses two main parts: (i) A transfer learning-based feature extraction model to achieve VR feature extraction from mmWave signal. (ii) An attention-based VR privacy spying module to spy VR privacy information from the extracted feature. The mmSpyVR demonstrates the capability to extract critical VR privacy from the mmWave signals that have penetrated through obstacles. We evaluate mmSpyVR through IRB-approved user studies. Across 22 participants engaged in four experimental scenes utilizing VR devices from three different manufacturers, our system achieves an application recognition accuracy of 98.5% and keystroke recognition accuracy of 92.6%. This newly discovered vulnerability has implications across various domains, such as cybersecurity, privacy protection, and VR technology development. We also engage with VR manufacturer Meta to discuss and explore potential mitigation strategies. Data and code are publicly available for scrutiny and research. 111https://github.com/luoyumei1-a/mmSpyVR/","Virtual reality (VR) gains widespread popularity among enthusiasts and professionals in various activities, ranging from immersive gaming experiences to virtual chatting and online shopping (Wu et al., 2023a; Hu et al., 2023). These users interact with their VR devices through VR actions, such as body and hand controller motion. However, these VR actions inadvertently reveal the user’s privacy, specifically the user’s activity type and keystroke typing. Existing methods for compromising the privacy of VR users fall into two distinct categories: (i) Physically entering the VR user’s environment. (ii) Virtually establishing a connection with the VR devices. In the first category, researchers hack into webcams and place hidden cameras (Wang et al., 2023; Cao et al., 2022) to record VR actions (Luo et al., 2022; Meteriz-Yıldıran et al., 2022). These approaches face constraints due to security barriers and non-line-of-sight scenarios. In the second category, researchers attempt to directly access the internal sensors by hacking the devices (Shi et al., 2021; Yang et al., 2024). The viability of these methods is limited due to the requirement of establishing a connection with the victim device and the fact that the user’s hand movements in the virtual scene do not reflect their actual movements. Recent studies explore the utilization of wireless signals (Ding et al., 2020), such as WiFi and mmWave, for VR privacy spying (Arafat et al., 2021; Mei et al., 2024). However, WiFi-based methods are susceptible to interference from environmental signals, which affects detection accuracy (Qiao et al., 2023; Salim et al., 2024; Wang et al., 2019, 2021; Yang et al., 2021). Moreover, they struggle to achieve the high-precision hand position detection and reconstruction necessary for effective VR privacy spying. Meanwhile, existing mmWave-based posture recognition methods (Santhalingam et al., 2020; Xie et al., 2024) are not directly applicable to VR privacy spying, as these approaches fail to extract VR privacy information from mmWave signals attenuated by obstacles. The challenge lies in the complex relationship between VR user motions and their corresponding input. For instance, if a VR user clicks with an identical click twice but changes the headset orientation, it results in different inputs. Subsequently, existing mmWave-based motion sensing research does not explore the utilization of VR privacy spying. In contrast to the aforementioned approaches, this paper proposes a novel side-channel vulnerability that exploits mmWave radar to penetrate obstacles to spy on VR user’s privacy without physical intrusion and virtual connection with the VR devices. We identify several opportunities to exploit this vulnerability. Firstly, the substantial bandwidth and high frequency provided by mmWave signals offer precision enhancement, improving the accuracy of sensory data (Wang et al., 2024b). Moreover, the penetration capabilities of mmWave signals pose significant threats to the privacy and security of VR devices (Chen et al., 2006). In addition, the unique reflection characteristics of VR controllers and headsets present an opportunity for motion tracking, enhancing the point cloud features and reducing point cloud sparsity. Our system utilizes these opportunities to uncover the user’s privacy. To intercept VR user activity from mmWave signals through obstacles, we face three unique challenges: (i) What impact do indoor obstacles have on millimeter wave signals for VR users? (ii) How to precisely reconstruct user actions from sparse point clouds across obstacles? Due to the sparsity of mmWave point clouds, single-frame actions are insufficient to discern whether a user is inputting characters and words. (iii) How to establish a correspondence between VR actions and the user’s VR privacy information? The same actions at different times correspond to different VR privacy information, requiring contextual information for accurate interpretation. To address these challenges, the proposed mmSpyVR system incorporates a transfer learning-based VR feature extraction module and an attention-based VR privacy spying module. We develop a novel data augmentation technique to mitigate the impact of signal attenuation caused by obstacles, enhancing the system’s robustness in real-world environments. The privacy spying module leverages augmented point clouds of the user’s body for comprehensive activity surveillance, focusing primarily on keyboard input activities. By recognizing VR motion, keyboard layout, position, and key presses, the system monitors various activity types and keystroke inputs across diverse VR environments. A continual learning approach is used to adapt to new activities, while a network with contextual temporal understanding captures VR privacy information embedded in user actions. The model further integrates multi-task learning for key press detection and keystroke prediction, accounting for different orientations, distances, and head movements during input processes. To the best of our knowledge, this is the first system to exploit mmWave signals for spying on VR users’ privacy through obstacles, revealing a new security vulnerability. Our key contributions are as follows: • To the best of our knowledge, mmSpyVR is the first attack on VR users’ privacy via mmWave radar without physical and virtual connection with VR users. We investigate the feasibility of VR spying in various practical system settings and verify the ability of mmWave signals to penetrate obstacles. • Technically, our design incorporates a VR feature extraction module founded on transfer learning to extract VR features despite the presence of obstacles. Additionally, we design an attention-based VR privacy spying model aimed at spying on the private information of VR users. • Experimentally, we evaluate our system on 22 participants in 4 experimental scenes utilizing commodity VR devices from three different manufacturers and collect 12TB data in total. Experimental results show that mmSpyVR achieves 98.5% and 92.6% accuracy in activity type and keystroke spying."
https://arxiv.org/html/2411.09838v1,OneNet: A Channel-Wise 1D Convolutional U-Net,"Many state-of-the-art computer vision architectures leverage U-Net for its adaptability and efficient feature extraction. However, the multi-resolution convolutional design often leads to significant computational demands, limiting deployment on edge devices. We present a streamlined alternative: a 1D convolutional encoder that retains U-Net’s accuracy while enhancing its suitability for edge applications. Our novel encoder architecture achieves semantic segmentation through channel-wise 1D convolutions combined with pixel-unshuffle operations. By incorporating PixelShuffle, known for improving accuracy in super-resolution tasks while reducing computational load, OneNet captures spatial relationships without requiring 2D convolutions, reducing parameters by up to 47%. Additionally, we explore a fully 1D encoder-decoder that achieves a 71% reduction in size, albeit with some accuracy loss. We benchmark our approach against U-Net variants across diverse mask-generation tasks, demonstrating that it preserves accuracy effectively. Although focused on image segmentation, this architecture is adaptable to other convolutional applications. Code for the project is available at https://github.com/shbyun080/OneNet.","With advancements in encoder-decoder architectures, the accuracy and versatility of vision models have reached unprecedented levels. However, deploying these high-parameter models on edge devices (e.g., mobile phones) poses a significant challenge due to their limited computational resources. Techniques like optimization and quantization have become essential to enable the use of state-of-the-art models on these devices. Many modern architectures, including diffusion models [25, 28, 3], rely heavily on the U-Net [23] architecture as an encoder-decoder backbone. Yet, its structure is not optimized for efficiency in resource-constrained environments as they often overlook the inherent architectural inefficiencies. Since they typically employ a standard convolutional backbone such as ResNet [11], the parameter count can escalate rapidly, impacting efficiency and increasing the chance of overfitting. Although the architecture is highly adaptable, minimal research has been conducted to streamline its size for edge deployment. To solve this issue, we propose modifying the U-Net [23] backbone to reduce the number of parameters, thereby decreasing computing costs and model download size. This optimization would make U-Net [23] more feasible for edge deployment and open up possibilities for more complex models by reallocating resources to tasks of greater importance. (a) (b) Figure 1: Channel-Wise 1D Convolution Block (a) Encoder convolution block with pixel-unshuffle downscaling replacing max pooling operation, followed by a single spatial and two channel-wise layers. (b) Decoder convolution block with pixel-shuffle upscaling for tensor upsampling, followed by a spatial layer between two channel-wise layers. In contrast, areas like image super-resolution have long benefited from techniques like PixelShuffle [24], significantly improving pipeline efficiency without compromising spatial information. However, despite the clear advantages of these scaling techniques, they have not been widely explored in other domains. Additionally, while lightweight architectures like MobileNet [12] have been effective for more straightforward tasks such as classification, such structures remain underutilized in generative models. This knowledge gap suggests an opportunity to explore alternative efficiency-driven architectures for demanding vision tasks, potentially unlocking new performance levels and adaptability in model deployment. This paper proposes a novel adaptation of the U-Net [23] architecture that bridges the gap between state-of-the-art performance and edge-deployability by reducing the model size while preserving accuracy. Our approach is the first to leverage channel-wise 1D convolutions in conjunction with pixel-shuffling operations to enable efficient feature and spatial attention without reliance on 2D computations. By eliminating these operations that are often challenging to parallelize on resource-constrained edge devices, we ease the burden on sequential computing cores and make the model more suitable for lightweight deployment. Additionally, we optimize spatial processing by reducing kernel sizes, focusing instead on cross-feature interactions to further minimize memory requirements. This novel architecture can seamlessly replace the standard U-Net in existing pipelines, offering a versatile and high-efficiency solution for edge applications. Our major contributions are as follows: • We design a novel convolution block that only uses 1D convolutions while retaining spatial information through the introduction of pixel-unshuffle downscaling, pixel-shuffle upscaling, and channel-wise 1D convolutions. • We implement two versions of U-Net [23] (1D encoder with 2D-decoder and 1D encoder-decoder) using our novel convolution block, effectively reducing the model size by 47% and 67% while maintaining reasonable accuracy. • We evaluate our proposed method variations on multiple mask-prediction datasets and compare our results to commonly used backbones for U-Net to display performance retention while reducing the model’s total size and the number of computations."
https://arxiv.org/html/2411.09821v1,Automatic Classification of General Movements in Newborns,"General movements (GMs) are spontaneous, coordinated body movements in infants that offer valuable insights into the developing nervous system. Assessed through the Prechtl GM Assessment (GMA), GMs are reliable predictors for neurodevelopmental disorders. However, GMA requires specifically trained clinicians, who are limited in number. To scale up newborn screening, there is a need for an algorithm that can automatically classify GMs from infant video recordings. This data poses challenges, including variability in recording length, device type, and setting, with each video coarsely annotated for overall movement quality. In this work, we introduce a tool for extracting features from these recordings and explore various machine learning techniques for automated GM classification.","General movements (GMs) refer to spontaneous movements of the entire body observable in infants from early fetal life until approximately six months post-term (Einspieler and Prechtl, 2005). These movements include coordinates sequences of the arms, legs, neck, and trunk, with variations in intensity, force, and speed. GMs are inherently complex, variable, and fluid, and their quality can provide critical insights into the integrity of the developing nervous system. Assessing GM quality, known as the Prechtl General Mouvement Assessment (GMA), is a sensitive and reliable diagnostic tool for predicting the likelihood of cerebral palsy and other neurodevelopmental disorders (Einspieler and Prechtl, 2005). The types of movements predictive of later neurodevelopmental disorders vary significantly with age, and age-based distinctions are essential when studying newborns. From early fetal life until approximately two months post-term, GMs exhibit consistent writhing movements. Between 6 to 9 weeks post-term, these writhing patterns diminish, and fidgety movements emerge, persisting until around the middle of the first year of life, at which point intentional movements become predominant. Alterations in these movement patterns can indicate later disorders. While GMA is typically conducted in hospitals by specialized clinicians, there are too few trained experts to screen all newborns. This motivates the need to develop a tool for automatic and reliable classification of movements. Initial attempts at addressing this challenge focused on computer-based approaches using feature extraction techniques (Baccinelli et al., 2020; Raghuram et al., 2022). However, recent advances in deep learning and computer vision have spurred the development of more sophisticated methods (Silva et al., 2021; Irshad et al., 2020). For instance, Reich et al. (2021) proposed to use a pose estimator, OpenPose, to extract skeletons and used these as input to a shallow multilayer neural network to discriminate between movements. Other deep learning solutions, e.g. by Schmidt et al. (2019), work in controlled conditions. Despite their promise, these approaches often rely on controlled environments with fixed sensors and constant recording length, limiting their scalability and applicability in real-world clinical settings. In this work, we present a preliminary study introducing a method to label key anatomical points, automatically process videos, and classify GMs based on the tracked anatomical points across videos, using a dataset from Barmherzige Brüder Regensburg Hospital. The challenges posed by the variability in recording lengths and devices, the diversity of video scenarios (including both hospital and home environments), and the nature of the movement quality annotations—one per recording—are significant for reliable GM classification. Addressing these challenges is crucial to the generalization of an automatic GMA that can be effectively deployed in varied settings. The goal is to enable comprehensive newborn screening and follow-up studies by experts, ultimately improving the early detection and treatment of neurodevelopmental disorders."
https://arxiv.org/html/2411.09751v1,"Analyzing the AI Nudification Application
Ecosystem","Given a source image of a clothed person (an image subject), AI-based nudification applications can produce nude (undressed) images of that person. Moreover, not only do such applications exist, but there is ample evidence of the use of such applications in the real world and without the consent of an image subject. Still, despite the growing awareness of the existence of such applications and their potential to violate the rights of image subjects and cause downstream harms, there has been no systematic study of the nudification application ecosystem across multiple applications. We conduct such a study here, focusing on 20 popular and easy-to-find nudification websites. We study the positioning of these web applications (e.g., finding that most sites explicitly target the nudification of women, not all people), the features that they advertise (e.g., ranging from undressing-in-place to the rendering of image subjects in sexual positions, as well as differing user-privacy options), and their underlying monetization infrastructure (e.g., credit cards and cryptocurrencies). We believe this work will empower future, data-informed conversations — within the scientific, technical, and policy communities — on how to better protect individuals’ rights and minimize harm in the face of modern (and future) AI-based nudification applications.Content warning: This paper includes descriptions of web applications that can be used to create synthetic non-consensual explicit AI-created imagery (SNEACI). This paper also includes an artistic rendering of a user interface for such an application.","Computer vision and generative AI techniques can undress111We define undress functionality as that which takes a representation (image) depicting a clothed individual and produces an representation of that individual without clothes. someone depicted in a picture or video [61, 25, 20, 4]. Such technology is increasingly publicly accessible, leading to a proliferation of “nudification” applications available to end-users online. Nudification applications enable end-users without technical or even Photoshop skills to artificially generate intimate imagery of someone without their consent. We refer to such a resulting image as a synthetic non-consensual explicit AI-created imagery, or SNEACI222Synthetic non-consensual explicit AI-Created imagery, or SNEACI, refers to images or videos that depict a nude or semi-nude subject, including those that contain intimate body parts and/or depict the subject engaged in a sexual act, without the subject’s consent.. Non-consensual imagery created through the use of AI has a starkly different paradigm compared to that of images created through Photoshop or technical skills. AI makes the creation of these images easier, faster, and more realistic because of how advanced generative AI has become. Similar to malware-as-a-service, synthetic non-consensual explicit AI-created imagery brings non-experts the ability to harm at scale. The creation of SNEACI is a form of sexual abuse against the subject who, by definition, is non-consensually depicted in the resulting content [32]. In addition to the serious mental health impacts victim-survivors sustain from the violation of being depicted in SNEACI [18], SNEACI may be used by the creator to extort and/or otherwise harass the subject of the image [30]. As such, there is increasing concern amongst policymakers and new legislation about SNEACI and the applications that facilitate its creation [37, 22, 2]. Despite the severity of abuse possible with nudification applications, little is known about this software ecosystem. In order to secure potential victims of an abusive software ecosystem, we must first understand how that ecosystem operates in practice [3]. We take a first step toward filling this gap through measuring and characterizing the ecosystem by answering three key research questions. Our first research question is: • RQ1. How do nudification applications position themselves to clients via text and visual descriptions? Acknowledging that nudification applications could be used with the consent of the image subject, we seek to understand whether existing applications foster and support those (and only those) use cases. Additionally, we seek to understand the general experience of users as they interact with these sites. Among the questions we ask: do users need to confirm that they are adults, do the sites support the nudification of all people, and how do the sites communicate about the role of consent from the image subject? Understanding the user experience on these sites, and how these sites position themselves and communicate with their users, is essential for having an informed conversation about their dominant use cases, the benefits and harms of these applications, and, to the extent that harms dominate benefits, how the broader research, industry, and policy communities might go about mitigating those harms. Next, we seek to answer the following research question: • RQ2. What features do nudification applications advertise? In our study, we seek to understand and catalogue the advertised features of popular nudification applications — these are the features that they purport to offer, and hence the features that users believe that the applications will provide. We chose not to experimentally verify that each nudification site actually provides all (and has no hidden features) the features that they claim, nor did we upload images of people for nudification to these sites to test these features, as we did not believe it would be ethical to upload images of real people to potentially adversarial entities, including images of people on our research team, or even already-public stock photos or images of celebrities. Instead, for our purposes, we focus on what features users believe they will have access to if they choose to use the nudification site. Even if not all nudification platforms provide all the features that they claim to, if users seek those features today, we conjecture that future instantiations of these platforms will provide those features. Hence, knowing the features that these applications advertise, and what features these applications believe that their users want or will want, is valuable. Lastly, we seek to answer the following research question: • RQ3. How do nudification applications monetize? Prior to the full initiation of our research study, we gained preliminary experience through the interaction with several nudificaiton sites. Our preliminary interactions uncovered a diverse monetization ecosystem, including both conventional payment systems (like credit cards) as well as newer, less conventional payment systems (like cryptocurrencies). Under the hypothesis that at least some nudification sites are problematic, one approach toward curbing their existence might be to challenge their ability to monetize. Additionally, as part of studying the monetization ecosystem, we sought to assess these sites’ monetization strategies, e.g., do certain features (like full nudification) require a paid subscription whereas other features (like changing clothing) do not? To answer our research questions, we collected a sample of 20 nudification applications (websites) and systematically analyzed them using the application walkthrough method [24] — a methodology used in several fields within and beyond computer science (e.g., [31, 45, 39, 55, 21, 12]). In analyzing the applications, we found a problematic ecosystem: • 19 out of 20 applications explicitly specialize in the undressing of women; only half of the websites mention that they expect the user to have the image subject’s consent and fewer ask for affirmation that consent has been obtained. • Most of the applications allowed for additional features beyond “undressing” (e.g., making the image-subject nude with their breasts and vulva visible in the imagery). For example, half of the applications allowed users to put image-subjects into sexual acts. • These nudification applications make up a commercial ecosystem and, hence, targeting their commercialization features might be one way to protect against SNEACI. Furthermore, we see purposeful repackaging of these nudification features with 5 out of the 20 applications offering API access to their highest paying customers. Stepping back, the computer security research community, as it is often defined, focuses on computing in the presence of adversaries. In some cases, the research focuses on studying adversarial features and the adversarial ecosystem, and in other cases, the research focuses on studying defenses against said adversaries. The ecosystem of image-based sexual abuse is an example of an adversarial usage of technology, and this work sits within that context and builds on prior research in and adjacent to the computer security research community on understanding and studying the emerging realm of synthetic explicit non-consensual AI-created imagery [6, 56, 54]."
https://arxiv.org/html/2411.09749v1,"Adversarial Attacks Using Differentiable Rendering:
A Survey","Differentiable rendering methods have emerged as a promising means for generating photo-realistic and physically plausible adversarial attacks by manipulating 3D objects and scenes that can deceive deep neural networks (DNNs). Recently, differentiable rendering capabilities have evolved significantly into a diverse landscape of libraries, such as Mitsuba, PyTorch3D, and methods like Neural Radiance Fields and 3D Gaussian Splatting for solving inverse rendering problems that share conceptually similar properties commonly used to attack DNNs, such as back-propagation and optimization. However, the adversarial machine learning research community has not yet fully explored or understood such capabilities for generating attacks. Some key reasons are that researchers often have different attack goals, such as misclassification or misdetection, and use different tasks to accomplish these goals by manipulating different representation in a scene, such as the mesh or texture of an object. This survey adopts a task-oriented unifying framework that systematically summarizes common tasks, such as manipulating textures, altering illumination, and modifying 3D meshes to exploit vulnerabilities in DNNs. Our framework enables easy comparison of existing works, reveals research gaps and spotlights exciting future research directions in this rapidly evolving field. Through focusing on how these tasks enable attacks on various DNNs such as image classification, facial recognition, object detection, optical flow and depth estimation, our survey helps researchers and practitioners better understand the vulnerabilities of computer vision systems against photorealistic adversarial attacks that could threaten real-world applications.","Differentiable rendering has become a powerful tool for solving inverse problems in computer vision and graphics [baumgart_geometric_1974]. By enabling gradient propagation through the rendering process, the underlying scene representation can be optimized [nimier-david_mitsuba_2019] for tasks such as 3D reconstruction, where mesh vertices are optimized to achieve a desired form, material and texture display properties are refined for a target appearance, or for estimating the pose of an object or scene illumination. Some more recent techniques, such as Neural Radiance Fields (NeRF) [mildenhall_nerf_2020] and 3D Gaussian Splatting [kerbl_3d_2023] have generated significant interest in the research community by enabling novel view synthesis by only processing a few representative images to reconstruct textured 3D models or entire scenes. These capabilities have also sparked vibrant growth in the development of many open source libraries to facilitate their use, such as OpenDR111http://open-dr.org, Redner222https://github.com/BachiLi/redner, Kaolin333https://developer.nvidia.com/kaolin, PyTorch3D444https://pytorch3d.org, and Mitsuba555http://www.mitsuba-renderer.org. Besides research and development, there are easy-to-use platforms [chris_heinrich_polycam_2024, tancik_nerfstudio_2023] that allow users without technical expertise to create textured 3D models or entire scenes, using just a few photos. However, differentiable rendering has also shown potential as a tool for adversaries to manipulate deep neural networks (DNNs) by optimizing scene representations. DNNs are vulnerable to adversarial examples [szegedy_intriguing_2014, goodfellow_explaining_2015], which can lead to misclassification or misdetection. Such attacks have far-reaching effects, including manipulating computer vision systems in cars to misclassify stop signs [chen_shapeshifter_2019], causing LiDAR systems to misdetect objects [cao_adversarial_2019], fooling facial recognition systems [sharif_accessorize_2016], and misclassifying 3D models [xiao_meshadv_2019]. Adversaries exploit DNN differentiability by accessing gradients during training to optimize inputs, training, or outputs for malicious purposes. Fig. 1: Perturbing textures on objects in realistic scenes (top row) induces misdetections of an underwater cube as an airplane, a mailbox as a stop sign, and a mesa as a bus (bottom row). Figure 1 shows a few examples of how an attacker perturbed the texture of an object to maximize a loss function to induce misdetections. With differentiable rendering, attackers can similarly optimize the underlying 3D representation (objects, materials, and lighting) by computing the gradient of the loss function and adjusting parameters to achieve their goal. Understanding these attacks remains a challenge, as differentiable rendering is a newer fast-evolving approach for adversarial ML, current attack research studies often leverage the differentiable rendering capabilities available at the time and results are scattered across: 1. Choosing attack goals, e.g., some aim to induce misclassfication, while some induce motion or depth misestimation; 2. Identifying attackable components in a model, e.g., some targets pre-processing steps, while some target inference; 3. Manipulating 3D scene, e.g., some target only texture, while others target geometry or a combination thereof. In other words, while there has been progress at this important research intersection of adversarial attacks using differentiable rendering, it is challenging to systematically compare them, summarize their strengths and limitations, and importantly identify research gaps or future directions. Figure 2 shows how our survey serves as this crucial missing piece that our community needs to unite the frontiers of ML research with advanced differentiable rendering techniques, providing a unifying framework joining diverse goals and tasks to systematically describe how DNNs are vulnerable to adversarial attacks. We present these capabilities within a task-oriented framework, organizing specific tasks such as manipulating textures, altering illumination, and modifying 3D meshes within a unifying structure. This approach sets our work apart from existing surveys, as it emphasizes not just the techniques but also how they can be used by adversaries to exploit DNN vulnerabilities, highlighting the most significant and feasible threats to DNNs. Fig. 2: Visual overview of our unifying survey framework that, by unifying the diverse goals and tasks in identifying attackable components and manipulating scene representations, enables systematic summarization and comparison with existing differentiable rendering related adversarial attack research."
https://arxiv.org/html/2411.09703v1,MagicQuill: An Intelligent Interactive Image Editing System,"As a highly practical application, image editing encounters a variety of user demands and thus prioritizes excellent ease of use. In this paper, we unveil MagicQuill, an integrated image editing system designed to support users in swiftly actualizing their creativity. Our system starts with a streamlined yet functionally robust interface, enabling users to articulate their ideas (e.g., inserting elements, erasing objects, altering color, etc.) with just a few strokes. These interactions are then monitored by a multimodal large language model (MLLM) to anticipate user intentions in real time, bypassing the need for prompt entry. Finally, we apply the powerful diffusion prior, enhanced by a carefully learned two-branch plug-in module, to process the editing request with precise control. Please visit https://magic-quill.github.io to try out our system.","Performing precise and efficient edits on digital photographs remains a significant challenge, especially when aiming for nuanced modifications. As shown in Fig. 1, consider the task of editing a portrait of a lady where specific alterations are desired: converting a shirt to a custom-designed jacket, adding a flower crown at an exact position with a well-designed shape, dyeing portions of her hair in particular colors, and removing certain parts of the background to refine her appearance. Despite the rapid advancements in diffusion models [19, 47, 14, 6, 68, 10, 62, 35, 37, 38, 36] and recent attempts to enhance control [20, 48, 69, 23], achieving such fine-grained and precise edits continues to pose difficulties, typically due to a lack of intuitive interfaces and models for fine-grained control. The challenges highlight the critical need for interactive editing systems that facilitate precise and efficient modifications. An ideal solution would empower users to specify what they want to edit, where to apply the changes, and how the modifications should appear, all within a user-friendly interface that streamlines the editing process. We aim to develop the first robust, open-source, interactive precise image editing system to make image editing easy and efficient. Our system seamlessly integrates three core modules: the Editing Processor, the Painting Assistor, and the Idea Collector. The Editing Processor ensures a high-quality, controllable generation of edits, accurately reflecting users’ editing intentions in color and edge adjustments. The Painting Assistor enhances the ability of the system to predict and interpret the users’ editing intent. The Idea Collector serves as an intuitive interface, allowing users to input their ideas quickly and effortlessly, significantly boosting the editing efficiency. The Editing Processor implements two kinds of brushstroke-based guidance mechanisms: scribble guidance for structural modifications (e.g., adding, detailing, or removing elements) and color guidance for modification of color attributes. Inspired by ControlNet [66] and BrushNet [23], our control architecture ensures precise adherence to user guidance while preserving unmodified regions. Our Painting Assistor reduces the repetitive process of typing text prompts, which disrupts the editing workflow and creates a cumbersome transition between prompt input and image manipulation. It employs an MLLM to interpret brushstrokes and automatically predicts prompts based on image context. We call this novel task Draw&Guess. We construct a dataset simulating real editing scenarios for fine-tuning to ensure the effectiveness of the MLLM in understanding user intentions. This enables a continuous editing workflow, allowing users to iteratively edit images without manual prompt input. The Idea Collector provides an intuitive interface compatible with various platforms including Gradio and ComfyUI, allowing users to draw with different brushes, manipulate strokes, and perform continuous editing with ease. We present a comprehensive evaluation of our interactive editing framework. Through qualitative and quantitative analyses, we demonstrate that our system significantly improves both the precision and efficiency of performing detailed image edits compared to existing methods. Our Editing Processor achieves superior edge alignment and color fidelity compared to baselines like SmartEdit [20] and BrushNet [23]. The Painting Assistor exhibits superior user intent interpretation capabilities compared to state-of-the-art MLLMs, including LLaVA-1.5 [31], LLaVA-Next [30], and GPT-4o [21]. User studies indicate that the Idea Collector significantly outperforms baseline interfaces in all aspects of system usability. By leveraging advanced generative models and a user-centric design, our interactive editing framework significantly reduces the time and expertise required to perform detailed image edits. By addressing the limitations of current image editing tools and providing an innovative solution that enhances both precision and efficiency, our work advances the field of digital image manipulation. Our framework opens possibilities for users to engage creatively with image editing, achieving their goals easily and effectively."
https://arxiv.org/html/2411.09693v1,CropCraft: Inverse Procedural Modeling for 3D Reconstruction of Crop Plants,"The ability to automatically build 3D digital twins of plants from images has countless applications in agriculture, environmental science, robotics, and other fields. However, current 3D reconstruction methods fail to recover complete shapes of plants due to heavy occlusion and complex geometries. In this work, we present a novel method for 3D reconstruction of agricultural crops based on optimizing a parametric model of plant morphology via inverse procedural modeling. Our method first estimates depth maps by fitting a neural radiance field and then employs Bayesian optimization to estimate plant morphological parameters that result in consistent depth renderings. The resulting 3D model is complete and biologically plausible. We validate our method on a dataset of real images of agricultural fields, and demonstrate that the reconstructions can be used for a variety of monitoring and simulation applications.","Plants are ubiquitous objects that appear all around the world and serve as the foundation for agriculture, which underpins our civilization’s growth and survival. The ability to automatically build 3D digital twins of plants from images has countless applications in agriculture, environmental science, robotics, and other fields. In particular, the development of such reconstruction methods in the context of agriculture will enable automatic, large-scale monitoring of crops. The collected data can provide decision support for farmers, aid carbon budgeting for decision-makers, support the development of new agricultural techniques, and inform the design of new genotypes [59, 10, 40]. All of these advances will contribute to increasing crop productivity, alleviating the rising food crisis of today’s world [4, 30]. Figure 1: Inverse procedural modeling for agricultural crops. We propose a novel method for 3D reconstruction of agricultural crops based on inverse procedural modeling. Unlike standard multi-view reconstruction pipelines, our method outputs a complete, interpretable, and biologically plausible 3D mesh model of the crop canopy, lending itself to simulations of important biophysical processes such as photosynthesis. However, 3D reconstruction of plants remains to be a challenging vision problem. Many plants, including most of those found in agriculture, are composed of complex arrangements of thin leaves and branches that heavily occlude one another. These low visibility conditions cause existing reconstruction pipelines to fail. Multi-view reconstruction methods such as those based on neural rendering [29, 46, 49, 54, 24] or multi-view stereo [53, 38, 50, 57] do not reconstruct the invisible regions of the scene, leading to incomplete plant shapes and extraneous geometry. Learning-based methods [15, 26] can overcome this issue but require large amounts of ground-truth 3D data for training, which is extremely difficult to collect for dense vegetation due to the same occlusion reasons. On the other hand, there is a large body of work that has found success in modeling 3D plant shapes via procedural generation [23, 33, 27, 43, 42, 34]. These procedural models are grounded in scientific knowledge and are carefully designed to produce biologically plausible plant shapes that consist of anatomically complete arrangements of plant organs with realistic shapes. However, these models generally require human input to set their parameters, and the task of automatically generating plants that accurately represent plant instances observed in the real world (“inverse procedural modeling”) remains to be difficult. In this work, we present a novel method for 3D reconstruction of agricultural crops based on optimizing the parameters of a procedural plant morphology model. Our method combines the flexibility of data-driven neural reconstruction methods with the robust foundational knowledge in procedural models. To this end, we first use neural radiance field (NeRF) techniques [29, 46] to estimate the geometry of the visible surfaces in the scene. We then apply RANSAC [36] to estimate plant row locations and determine a canonical camera pose from which to render depth maps. Next, we render depth maps from both the NeRF and a virtual scene generated by a procedural generation model, and use Bayesian optimization to minimize a loss function with respect to the procedural model’s parameters. The design of the loss function and the parameterization of the procedural model are crucial for obtaining a 3D reconstruction that is useful for field-level analysis applications. Importantly, we note that the fine-grained positions of each individual leaf are mostly irrelevant, as long as the aggregate shape characteristics of the crop canopy are faithful. Thus, we define our loss function on histogram statistics of the depth map and optimize a highly compact set of parameters to ensure that the key canopy characteristics for determining crop productivity are accurately captured without being distracted by fitting irrelevant details. And thanks to the strong priors imposed by the procedural model, this process jointly optimizes the complete plant shape, including portions not visible in the input images. To validate our approach, we collect a multi-view dataset of real soybean and maize fields, paired with manual measurements of leaf area and leaf angle. We show that the proposed method can successfully reconstruct realistic crop canopies across different growth stages and estimate key canopy structure variables more accurately than baselines. We also show that the reconstructed 3D canopies can be directly fed into radiative transfer modeling software to provide accurate predictions of photosynthesis rates. The results highlight the potential for monitoring crop productivity directly from camera images instead of costly flux tower equipment. Our contributions are twofold: • We present a novel approach for reconstructing complete 3D morphological models of large-scale crop plant fields from a collection of images. • We introduce the first framework for image-based crop productivity quantification, making scalable carbon exchange monitoring a possibility."
https://arxiv.org/html/2411.09691v1,"Advancing Fine-Grained Visual Understanding with Multi-Scale Alignment 
in Multi-Modal Models","Multi-modal large language models (MLLMs) have achieved remarkable success in fine-grained visual understanding across a range of tasks. However, they often encounter significant challenges due to inadequate alignment for fine-grained knowledge, which restricts their ability to accurately capture local details and attain a comprehensive global perception. While recent advancements have focused on aligning object expressions with grounding information, they typically lack explicit integration of object images, which contain affluent information beyond mere texts or coordinates. To bridge this gap, we introduce a novel fine-grained visual knowledge alignment method that effectively aligns and integrates multi-scale knowledge of objects, including texts, coordinates, and images. This innovative method is underpinned by our multi-scale fine-grained enhancement data synthesis pipeline, which provides over 300K essential training data to enhance alignment and improve overall performance. Furthermore, we present TinyGroundingGPT, a series of compact models optimized for high-level alignments. With a scale of approximately 3B parameters, TinyGroundingGPT achieves outstanding results in grounding tasks while delivering performance comparable to larger MLLMs in complex visual scenarios. The data and code will be released in https://github.com/wwangweii/TinyGroundingGPT.git.","Recent advancements in multi-modal large language models (MLLMs) have showcased remarkable capabilities in multi-modal understanding, reasoning, and interaction, garnering unprecedented attention [35, 2, 29, 1, 36, 6]. MLLM research in fine-grained visual understanding has advanced significantly, particularly through early contributions from Shikra [2] and Kosmos-2 [29] in textually formatting positional vocabularies or object coordinates. Subsequent studies aimed at improving model performance primarily focused on common strategies, including parameter enlargement [2, 29, 21, 1] and dataset enrichment [3, 1, 36, 6]. Additionally, there is a growing interest in developing efficient, smaller fine-grained MLLMs [19, 11, 41, 47, 46] for real-world applications. Regardless of the methods used, the core of fine-grained models lies in achieving better alignment between object texts and visual features, encompassing both coordinate and semantic information. Figure 1: The comparison of alignment for multi-scale object representations. The C, T, I denote object coordinates, texts and images respectively. The “X-Y” denote MLLMs handle input “X” and output “Y”. Figure 2: Illustration of the proposed multi-modal fine-grained visual knowledge alignment method. It adopts a three-stage training strategy that progresses from easy to hard and the multi-scale fine-grained enhancement data synthesis pipeline constructs over 300K fine-grained alignment data. While effective, these methods face a significant challenge, i.e., the lack of fine-grained alignments. Visual objects typically encompass multi-scale representations with varying levels of information, including coordinates, texts, and images, as illustrated in Fig. 1. In this context, coordinates provide low-level object grounding information, texts offer primary descriptions that may not capture every detail, and images convey high-level information that extends beyond words. Most fine-grained models [2, 42, 21] primarily focus on alignments between object texts and coordinates (i.e., T-C and C-T), often neglecting direct interactions with object images. Although recent models like Qwen2-VL [1] and InternVL2 [6] can process multiple image inputs and understand relationships between the main image and object images (T-I), they still struggle to establish explicit alignments among object texts, coordinates, and images. This limitation can lead to hallucinations and insufficient grounding capabilities [4]. To achieve high-level alignments and integrate multi-granularity knowledge, as illustrated in Fig. 2(a), we introduce a fine-grained visual knowledge alignment method that effectively aligns object texts, coordinates, and images across multiple scales. Our method adopts a three-stage training strategy that progresses from easy to hard: 1) Object and Relation Perception Pretraining: To develop a foundational understanding of object texts and images, we implement a progressive training approach for MLLMs based on a pretrained LLM. 2) Multi-scale Fine-grained Local Knowledge Alignment: To attain fine-grained visual understanding and share multi-scale object knowledge, we conduct data-driven high-level alignments among object text descriptions, bounding box coordinates, and image features. 3) Detailed Global Knowledge Alignment: To enhance the model’s global understanding by integrating fine-grained knowledge, we guide the MLLMs to bridge different objects with multi-scale representations. To support this method, we propose a multi-scale fine-grained enhancement data synthesis pipeline (see Fig. 2(b)) that constructs alignment data from both local and global perspectives. Leveraging this framework, we propose TinyGroundingGPT, which requires less storage for deployment while outperforming larger models across multiple benchmarks, particularly in grounding tasks. Our contributions can be summarized as follows: • We introduce a fine-grained visual knowledge alignment method that enables the model to progressively enhance its fine-grained visual understanding through both global and local multi-scale object alignments. • We develop a multi-scale fine-grained enhancement data synthesis pipeline that leverages open-source datasets and advanced models to generate over 300K essential training data for fine-grained alignment. • We present TinyGroundingGPT, a series of compact models (1.5B and 3B parameters) that excel in multi-modal understanding and grounding capabilities, achieving performance comparable to that of larger 7B MLLMs."
https://arxiv.org/html/2411.09604v1,Local-Global Attention: An Adaptive Mechanism for Multi-Scale Feature Integration,"In recent years, attention mechanisms have significantly enhanced the performance of object detection by focusing on key feature information. However, prevalent methods still encounter difficulties in effectively balancing local and global features. This imbalance hampers their ability to capture both fine-grained details and broader contextual information—two critical elements for achieving accurate object detection. To address these challenges, we propose a novel attention mechanism, termed Local-Global Attention, which is designed to better integrate both local and global contextual features. Specifically, our approach combines multi-scale convolutions with positional encoding, enabling the model to focus on local details while concurrently considering the broader global context. Additionally, we introduce learnable \alpha parameters, which allow the model to dynamically adjust the relative importance of local and global attention, depending on the specific requirements of the task, thereby optimizing feature representations across multiple scales. We have thoroughly evaluated the Local-Global Attention mechanism on several widely used object detection and classification datasets. Our experimental results demonstrate that this approach significantly enhances the detection of objects at various scales, with particularly strong performance on multi-class and small object detection tasks. In comparison to existing attention mechanisms, Local-Global Attention consistently outperforms them across several key metrics, all while maintaining computational efficiency. Code is available at the link.","In recent years, significant progress has been made in the field of object detection, with many methods achieving remarkable improvements in performance metrics [8, 17, 20]. Nevertheless, researchers continue to explore new approaches to further enhance accuracy and efficiency, especially in challenging scenarios such as multi-class and small object detection. Attention mechanisms have emerged as an effective means to improve model performance [25, 12, 27, 28, 36], gaining considerable attention in deep learning due to their ability to significantly boost performance with only a limited increase in computational cost. Among widely-used attention mechanisms, local and global attention are particularly noteworthy [19]. Local attention focuses on fine-grained, localized details within the input, capturing essential local information [33]. In contrast, global attention emphasizes the overall content and global context of the input, which is crucial for understanding broader relationships and larger patterns [1, 20]. Despite their strengths, both types of attention have inherent limitations: local attention often overlooks global dependencies, while global attention sacrifices detailed local information. Although many researchers have attempted to combine local and global features, these efforts often face challenges in effectively balancing the two, resulting in either suboptimal model performance or significant computational overhead that offsets performance gains [29, 35, 18, 32]. In this paper, we propose a novel attention mechanism called Local-Global Attention, designed to balance local and global features by integrating multi-scale convolution and positional encoding. This enables the model to capture both local details and global context. Additionally, we introduce learnable \alpha parameters, allowing the model to dynamically adjust the balance between local and global attention in a data-driven manner, achieving consistent improvements across various datasets. Overall, Local-Global Attention ensures optimized feature representation across different scales, enhancing detection performance while maintaining low computational costs. Specifically, the implementation of the Local-Global Attention mechanism is as follows: After initial feature extraction, we apply multi-scale convolutions with smaller kernels to capture localized features, which are then aggregated to form a local attention map. In parallel, larger kernel convolutions combined with positional encoding are used to extract broader global features, generating a global attention map. These two attention maps are then fused using learnable \alpha parameters, creating a unified attention map, which is subsequently passed through additional network layers for further refinement and processing. This approach provides several key advantages. Firstly, it captures both fine-grained and large-scale information, offering a more comprehensive understanding of the input, which enables more accurate localization and identification of targets. Secondly, the mechanism is lightweight and modular, making it easy to integrate into a variety of existing network architectures, such as MobileNetV3 [10] and ResNet [9]. By prioritizing informative features and filtering out irrelevant data, Local-Global Attention significantly enhances detection accuracy in detection tasks, as shown in Figure 1. To validate the effectiveness of our method, we conducted extensive experiments on multiple benchmark datasets, including VOC2007 [6], VOC2012 [7], VisDrone2019-DET [37], TinyPerson [34], COCO2017 [16], GWHD2020 [3], COCO minitrain [23], DOTA-v1.0 [30], as well as MNIST [4] and Fashion-MNIST [31]. The results demonstrate that Local-Global Attention outperforms existing attention mechanisms with similar computational requirements, consistently improving the model’s detection accuracy. In conclusion, we believe that Local-Global Attention is a practical and efficient solution that addresses some limitations of existing attention mechanisms, offering a flexible approach that moves closer to more accurate and computationally feasible object detection models. Figure 1: The mAP@50 and mAP@50:95 results on the COCO2017 [16] and VOC2007 [6] datasets compare the performance of MobileNetV3 [10] with its enhanced version using the Local-Global Attention mechanism. On COCO2017 [16], all models were trained for 20 epochs using the Adam optimizer, while on VOC2007 [6], training was conducted for 200 epochs with the AdamW optimizer. In both cases, other settings followed the YOLOv8 [14] default configuration."
https://arxiv.org/html/2411.09572v1,Dynamic Reconstruction of Hand-Object Interaction with Distributed Force-aware Contact Representation,"We present ViTaM-D, a novel visual-tactile framework for dynamic hand-object interaction reconstruction, integrating distributed tactile sensing for more accurate contact modeling. While existing methods focus primarily on visual inputs, they struggle with capturing detailed contact interactions such as object deformation. Our approach leverages distributed tactile sensors to address this limitation by introducing DF-Field. This distributed force-aware contact representation models both kinetic and potential energy in hand-object interaction. ViTaM-D first reconstructs hand-object interactions using a visual-only network, VDT-Net, and then refines contact details through a force-aware optimization (FO) process, enhancing object deformation modeling. To benchmark our approach, we introduce the HOT dataset, which features 600 sequences of hand-object interactions, including deformable objects, built in a high-precision simulation environment. Extensive experiments on both the DexYCB and HOT datasets demonstrate significant improvements in accuracy over previous state-of-the-art methods such as gSDF and HOTrack. Our results highlight the superior performance of ViTaM-D in both rigid and deformable object reconstruction, as well as the effectiveness of DF-Field in refining hand poses. This work offers a comprehensive solution to dynamic hand-object interaction reconstruction by seamlessly integrating visual and tactile data. Codes, models, and datasets will be available.","Human manipulates objects with tactile feedback. Reconstructing the manipulation process of hand-object interaction is an important task that can benefit many downstream tasks, such as VR/AR, robot imitation learning, and human behavior understanding. Previous works [31, 34, 4, 27, 12] on dynamic hand-object interaction reconstruction are primarily visual-only. Visual-only methods can recover the global geometry and poses of hand and object, but they struggle with contact details such as object deformation due to a lack of information near contact areas. Recently, visual-tactile methods [29, 23, 28] have increasingly drawn attention in hand-object reconstruction tasks, with the development of tactile sensing techniques [24, 16, 37, 21], which can supplement the perceptual ability near contact areas (Fig. 1.a). Among the fast-developing tactile sensors, distributed tactile sensors (Fig. 1.b) [16, 36, 24], which are wearable or attachable to a hand and allow collecting tactile information in a natural human-like hand-object interaction process, are more promising in human manipulation data collection than other types of tactile sensors [25, 37, 15, 21]. However, how to integrate such sensors with visual perception to reconstruct the states of hand-object interaction is seldom explored. Figure 1: (a): The relationship between tactile information and contact geometry. Grasping different bowls with the same hand poses shows that the distributed tactile arrays can capture occlusion contacts and object states. (b): Different types of distributed tactile sensors proposed in previous works (1). [16], (2). [36], (3). [24]. Due to the conformality between the distributed tactile sensor and the hand, the hand geometry will not be significantly influenced during manipulation. Therefore, we can use the same visual-only approach to reconstruct the hand-object state and integrate the tactile information to refine the result. Based on this observation, we propose a novel Distributed Force-aware contact representation, DF-Field, and a Visual-Tactile Manipulation reconstruction framework with Distributed tactile sensing, ViTaM-D. DF-Field models the contact by considering both kinetic and potential energy in hand-object interaction, which allows modeling object deformation. With this representation, ViTaM-D first reconstructs the hand-object interaction with visual-only observations by the proposed Visual Dynamic Tracking network, VDT-Net, and refines the contact details with DF-Field via a Force-aware hand-pose Optimization process, FO. Such design allows us to inherit the wisdom of fast-growing visual-only hand-object reconstruction methods [34, 4, 27, 12] and easily integrate the tactile information into an existing motion capture or estimation system. To train the VDT-Net, we need large-scale hand-object interaction datasets since previous datasets [3, 33, 9] of hand-object interactions mostly cover rigid or articulated object manipulation, failing to contain deformable objects. And they generally do not provide accurate tactile readings. Therefore, aside from the public dataset, DexYCB [3], which we adopt to benchmark on rigid objects with common baselines, we also create a new dataset, HOT dataset, to fully benchmark our method on deformable object reconstruction. The dataset is built with ZeMa [8], a high-precision physics-based simulation environment that supports penetration-free frictional contact modeling with finite element method (FEM) to model the object deformation. The HOT dataset contains 600 sequences of hand-object interaction, with 30 deformable objects from 5 different categories and 8 camera views for each sequence. To evaluate the method, we compare our proposed ViTaM-D with previous state-of-the-art methods gSDF [6] and HOTrack [4] on DexYCB. Extensive experiments proved that our method realizes great improvements in the previous works both quantitatively and qualitatively. Besides, we also prove the great capability of our method in tracking deformable objects on the HOT Dataset and the effectiveness of the FO optimization in refining hand poses from penetrations and bad contact states. Our contributions are summarized as follows: (1) A visual-tactile learning framework, ViTaM-D. It contains a visual-only dynamic tracking network, VDT-Net, for reconstructing hand-object interactions and a force-aware optimization process, FO, to integrate the tactile information into reconstruction refinement based on a novel distributed force-aware contact representation, DF-Field. (2) A new dataset, HOT. It contains 600 RGB-D manipulation sequences on 30 deformable objects from 5 categories with penetration-free hand-object poses and accurate tactile information."
https://arxiv.org/html/2411.09567v1,VpbSD: Vessel-Pattern-Based Semi-Supervised Distillation for Efficient 3D Microscopic Cerebrovascular Segmentation,"3D microscopic cerebrovascular images are characterized by their high resolution, presenting significant annotation challenges, large data volumes, and intricate variations in detail. Together, these factors make achieving high-quality, efficient whole-brain segmentation particularly demanding. In this paper, we propose a novel Vessel-Pattern-Based Semi-Supervised Distillation pipeline (VpbSD) to address the challenges of 3D microscopic cerebrovascular segmentation. This pipeline initially constructs a vessel-pattern codebook that captures diverse vascular structures from unlabeled data during the teacher model’s pretraining phase. In the knowledge distillation stage, the codebook facilitates the transfer of rich knowledge from a heterogeneous teacher model to a student model, while the semi-supervised approach further enhances the student model’s exposure to diverse learning samples. Experimental results on real-world data, including comparisons with state-of-the-art methods and ablation studies, demonstrate that our pipeline and its individual components effectively address the challenges inherent in microscopic cerebrovascular segmentation.","Cerebrovascular segmentation plays a pivotal role in visualizing and extracting intricate details of cerebral blood vessel structures. As a fundamental aspect of neuroscience research, it facilitates comprehensive analysis of brain function and structural changes within cerebral vessels, while also enabling the assessment of vascular abnormalities[1]. These structural changes and abnormalities serve as critical visual indicators for various diseases and degenerative conditions [2, 3]. Cerebrovascular malformations, for instance, are lesions stemming from congenital developmental abnormalities, characterized by dynamic structural variations, including localized structural and numerical vessel irregularities[4]. Cerebral small vessel disease refers to brain parenchymal damage induced by diverse structural or functional abnormalities of the brain’s small blood vessels[5]. Likewise, cerebral aneurysms alter vascular structures, resulting in functional impairments[6]. Given this context, accurately segmenting the entire vascular network, with a specific focus on small vessels, continues to be a principal focus for researchers[7, 3, 8]. Small-diameter vessels such as arterioles, capillaries, and venules impose specific requirements on medical imaging modalities to achieve comprehensive segmentation of the cerebral vascular network. High-resolution microscopic cerebrovascular images, characterized by intricate and diverse vascular structures, provide valuable insights for precise analysis in biological research. Therefore, leveraging high-resolution microscopy proves crucial for effectively addressing the challenges associated with cerebrovascular segmentation. Fig. 1: A set of 2D segmentation results selected from the final 3D reconstruction of the whole brain segmentation for a CD1 mouse strain. However, while some studies have explored cerebrovascular segmentation through imaging modalities such as MRA and CT[9, 10, 11], the challenges inherent to microscopic imaging have constrained the development of 3D deep learning approaches for cerebrovascular analysis, leading to relatively few contributions in this area. The high resolution of microscopic images results in each cerebral vascular slice containing millions of pixels. When expanded to the task of 3D whole-brain vascular segmentation, the data for a single 3D microscopic vascular sample can easily reach hundreds of gigabytes. Consequently, the size of an entire dataset may soar to terabyte levels, presenting significant challenges for storage, annotation and processing[2]. The difficulty of obtaining detailed annotations, along with variability in the shape and size of cerebral vessels and multiple branching levels at their termini, introduces substantial obstacles to the implementation of deep learning methods for cerebrovascular segmentation[12, 7]. Additionally, the considerable file sizes associated with high-resolution microscopic images not only render the segmentation of an entire brain using large models exceedingly time-consuming but also necessitate substantial computational resources, leading to significant memory consumption[13, 2]. In response to these challenges, we developed a novel approach VpbSD for 3D microscopic cerebrovascular segmentation. This pipeline consists of two main stages: the pretraining stage for the teacher model and the subsequent stage of knowledge transfer and distillation. Initially, VpbSD employs a self-supervised pretrained teacher model to capture diverse vascular structures from extensive unlabeled data. The self-supervised learning technique leverages large volumes of unlabeled data to train models on valuable features without the need for extensive annotations[14, 15, 16]. Then, by integrating semi-supervised learning with a novel codebook-based feature knowledge distillation, VpbSD effectively enriches the information acquisition of the student model while ensuring fast and precise segmentation. This approach not only reduces model size and computational complexity but also enhances overall performance. Our work makes the following key contributions: • First, based on the characteristics of 3D microscopic cerebrovascular data, we design a novel Vessel-Pattern-Based Semi-Supervised Distillation pipeline (VpbSD) for 3D cerebrovascular segmentation, which reduces computational complexity while effectively leveraging unlabeled microscopic data to enhance performance. • Second, we develop a vessel-pattern based knowledge distillation approach that constructs a vessel-pattern codebook during the teacher model’s pretraining phase. This codebook enables the teacher model to capture intricate vascular structures from unlabeled data and facilitates efficient knowledge transfer to the student model, thereby improving segmentation accuracy through the sharing of learned vascular diversity and structural context. • Third, we evaluate our approach on real datasets, comparing it with other state-of-the-art methods and conducting dilation study, demonstrating the effectiveness of our strategy. The reminder of this paper is organized as follows: Section 2 reviews strategies and methods relevant to this work, including microscopic cerebrovascular segmentation, knowledge distillation, self/semi-supervised learning, and vector quantization. Section 3 provides a detailed description of the proposed cerebrovascular segmentation approach. Section 4 presents experimental validations. Finally, Section 5 concludes the paper."
https://arxiv.org/html/2411.09558v1,Adaptive Deviation Learning for Visual Anomaly Detection with Data Contamination,"Visual anomaly detection targets to detect images that notably differ from normal pattern, and it has found extensive application in identifying defective parts within the manufacturing industry. These anomaly detection paradigms predominantly focus on training detection models using only clean, unlabeled normal samples, assuming an absence of contamination; a condition often unmet in real-world scenarios. The performance of these methods significantly depends on the quality of the data and usually decreases when exposed to noise. We introduce a systematic adaptive method that employs deviation learning to compute anomaly scores end-to-end while addressing data contamination by assigning relative importance to the weights of individual instances. In this approach, the anomaly scores for normal instances are designed to approximate scalar scores obtained from the known prior distribution. Meanwhile, anomaly scores for anomaly examples are adjusted to exhibit statistically significant deviations from these reference scores. Our approach incorporates a constrained optimization problem within the deviation learning framework to update instance weights, resolving this problem for each mini-batch. Comprehensive experiments on the MVTec and VisA benchmark datasets indicate that our proposed method surpasses competing techniques and exhibits both stability and robustness in the presence of data contamination.111Our code is available at https://github.com/anindyasdas/ADL4VAD/","Visual anomaly detection (VAD) seeks to identify unusual patterns within established normal visual data that are specific to a particular domain. It is of great interest because of its diverse applications, including industrial defect detection [31, 43], medical lesion detection [33], video surveillance [20], and many others. Most existing solutions train learning models using a substantial amount of clean, normal data, assuming such data is readily available in real-world scenarios. However, in numerous real-world applications, the data often obtained is contaminated or contains noisy labels due to errors in the human annotation process or labels inherited from legacy systems. This can lead to abnormal behavior when such systems are deployed in real industrial settings. While there has been extensive research addressing learning under the assumption of noisy labels, very few studies specifically focused on visual anomaly detection in corrupted data settings. Prior efforts to address the generic problem of learning with noisy labels include identifying and correcting label noise [14], label smoothing [35], and devising robust loss functions [21], etc.. In a recent work [17], instance reweighting is employed to dynamically assign relative importance weights to data samples. This technique aims to reduce the impact of potentially noisy examples, as noisy or mislabeled instances tend to have higher loss values [2, 23] compared to clean ones. Anomaly Detection (AD) methods predominantly focus on normal samples to train networks. However, some recent works also utilize both normal and anomalous clean samples for training. Since obtaining large-scale anomalous data with accurate labels is both expensive and challenging, these methods utilize a limited number of labeled anomalous samples to train the network. Deviation Learning [24, 25, 6] leverages a small set of labeled anomalous examples to train an anomaly-conscious detection networks. In this work, we adapted deviation learning within a data contamination setup to learn an anomaly score in a self-supervised manner, thereby introducing a novel approach that enhances model robustness. The primary contributions of our work can be summarized as follows: i) We propose a novel adaptive deviation learning framework that integrates constrained optimization into our self-supervised learning setup to dynamically assign sample importance weights to our deviation loss-based objective, thereby enhancing the robustness of anomaly score learning. ii) We also introduce a soft deviation-based objective that utilizes the likelihood of each instance being an anomaly instead of relying on hard labels, which effectively addresses corrupt data. iii) We perform comprehensive experiments to set performance benchmarks for our proposed method on two publicly available datasets. iv) We provide a detailed analysis of various state-of-the-art methods, examining how increased contamination affects their performance across different metrics. v) Our study demonstrates that our method attains state-of-the-art performance in anomaly detection under data contamination setup, supported by extensive empirical results from the MVTec and VisA datasets."
https://arxiv.org/html/2411.09555v1,Image Processing for Motion Magnification,"Motion Magnification (MM) is a collection of relative recent techniques within the realm of Image Processing. The human visual field can not capture all possible displacement of an object of interest, in particular the smallest one. This is the main motivation of introducing these techniques, in fact, the goal is to opportunely process a video sequence to obtain as output a new video in which motions are magnified and visible to the viewer. These techniques can amplify very subtle motions, imperceptible to the human eye, on proper video sequences, made by the aid of high resolution and high speed cameras. In this work, we propose some preliminary results on MM, developing a technique called Phase-Based Motion Magnification which is performed in the Fourier Domain and rely on the Fourier Shifting Property. In particular, we show the mathematical motivation at the foundation of this method, focusing on some basic test made up using synthetic images.","Image Processing, which usually abbreviates Digital Image Processing (DIP), consists on an ensemble of methods which enable to manipulate or altering an image to obtain a desired result, typically for improving its visual quality or extracting useful information from it. Image processing requires the use of a variety of techniques and algorithms to modify or analyze images and it is a fundamental component of artificial intelligence, computer vision and many other fields, [12]. In general, an image, can be seen as a two-dimensional function I(x,y), where x and y are the spatial (or plane) coordinates and the amplitude I at any pair of coordinates (x,y) is called intensity or gray level of the image at that point. The manipulation of the image is realized through calculators, for this reason both x and y has to be discrete quantities, and so we refer to the image as a Digital Image. Moreover, since it carries information through the two axes, we can intend it as a two dimensional signal, and for this cause DIP can be seen as a specialized form of Digital Signal Processing, where the signal is a two dimensional array rather than a one-dimensional time series. Many of the same mathematical tools are used in both fields, such as the Fourier Transform. The discussion we have done until now can be naturally extended to videos, since they are composed by a sequence of images, also known as frames. So, a video, is a three dimensional signal I(x,y,t) since the information varies no solely over the spatial coordinates, but also over time. In the context of DIP, we introduce a technique called Motion Magnification (MM). This is a relatively recent technique, proposed by a research group of the Massachusetts Institute of Technology [15], [20], [21]. More specifically, this technique establish its roots in DIP. In fact, given a input video I(x,y,t) it is possible, through appropriate manipulation - by working on the individual frames that make up the video - to produce an output \tilde{I}(x,y,t). This output video can reveals subtle color changes and, more importantly, movements that would be invisible to the naked eye. The primary goal of MM is the detection of those “invisible” movements that are indeed present, and that, when analyzed, can provide significant information about the object, structure, or even person captured in the video. In general, the input video, which may seem static to the observer, can actually contain extremely subtle movements. Objects in the scene could shift by as little as \frac{1}{50} or \frac{1}{100} of a pixel. Through a opportune procedure, these sub-pixel motions can be amplified, transforming them into more pronounced displacements that stretch across multiple pixels, making them visible. Essentially, we can think of MM as a kind of “microscope” that, instead of zooming in to reveal more detailed visuals, actually “zoom in” on movements or color changes. Imperceptible movements are often much faster than what the human eye can detect, which explains why it is necessary to use appropriate video cameras. These cameras offer the possibility of collecting high-density spatial data (high pixel resolution) at high-speed (high frame rate) from a distant scene of interest. MM has a various field of applications. For instance, there are numerous examples of its application in the medical field. These methodologies can be applied for monitoring respiration, specifically aiming to estimate breath rate using non-contact methods, [2], [17], or to introduce non invasive method based on MM for the study of heart failure, [1], [14]. Another particular branch of application of these techniques is vibration analysis, [6]. This has become a widely and powerful tool that allows engineers to study how structures respond to vibrations. This has become a widely and powerful tool that allows engineers to study how structures respond to vibrations. The estimation of the vibration, is usually performed using the classical contact devices, such as accelerometers. Recently, to perform this task, non-contact devices such as video cameras, are used for monitoring vibrating systems, [3], [4], [5], [8]. The scientific literature about this subject is extensive. For example, [9] applied motion magnification and image processing techniques to extract the frequency content of vibrations in several ancient structures in Rome and Istanbul. Additionally, [7] conducted a study on Structural Health Monitoring, aiming to identify the exact instant of occurrence for damage or sudden structural changes. Moreover, [22] for rotating fault diagnosis can be mentioned. In this paper, we propose a preliminary study of the so called Phase-based MM, [20]. This approach to MM is performed in the frequency domain, and exploits the Fourier Shift Theorem. This algorithm is effective for global movements due to the Fourier basis, which is defined in the entire domain. The magnification procedure has been implemented in MATLAB and the “Code metadata” are: Current code version v. 1.0 Permanent link to repository https://github.com/eTrebo98/MotMagArt1 Code versioning system used git Software code languages MATLAB This paper is organized as follows. In section 2, we explore the mathematical background under the proposed procedure, describing the steps needed to perform the magnification of a shift between two consecutive frame of a video. In section 3, since the algorithm needs to be applied on a calculator, we focus our attention on the discretized version of it, introducing the Discrete Fourier Transform (DFT). Next, in section 4, we show some numerical results made up with MATLAB using synthetic gray-scale images. We put much emphasis on presenting the amplification of the movement between two consecutive frames, furthermore we consider also an example of an output video sequence in which the shift are exaggerated respect to the input one. In section 5, we provide conclusions and future developments."
https://arxiv.org/html/2411.09553v1,OOD-SEG: Out-Of-Distribution detection for image SEGmentation with sparse multi-class positive-only annotations,"Despite significant advancements, segmentation based on deep neural networks in medical and surgical imaging faces several challenges, two of which we aim to address in this work. First, acquiring complete pixel-level segmentation labels for medical images is time-consuming and requires domain expertise. Second, typical segmentation pipelines cannot detect out-of-distribution (OOD) pixels, leaving them prone to spurious outputs during deployment. In this work, we propose a novel segmentation approach exploiting OOD detection that learns only from sparsely annotated pixels from multiple positive-only classes. These multi-class positive annotations naturally fall within the in-distribution (ID) set. Unlabelled pixels may contain positive classes but also negative ones, including what is typically referred to as background in standard segmentation formulations. Here, we forgo the need for background annotation and consider these together with any other unseen classes as part of the OOD set. Our framework can integrate, at a pixel-level, any OOD detection approaches designed for classification tasks. To address the lack of existing OOD datasets and established evaluation metric for medical image segmentation, we propose a cross-validation strategy that treats held-out labelled classes as OOD. Extensive experiments on both multi-class hyperspectral and RGB surgical imaging datasets demonstrate the robustness and generalisation capability of our proposed framework.","Despite significant progress of deep neural networks based segmentation methods for medical and surgical image analysis in recent years, the efficacy of these methods is highly dependent on the quality and quantity of pixel-level annotations. Specifically, the manual annotation of medical images necessitates professional expertise from experienced domain experts, making the process both costly and time-consuming, thereby leading to a shortage of labelled data in clinical settings. To reduce the intensive workload in acquiring pixel-level dense annotation from clinical experts, many efforts have been made to advance Weakly Supervised Learning (WSL) [49, 9, 57] and train learning-based algorithms from coarse-grained annotations instead of precise segmentation masks. WSL could allow domain experts to annotate regions that they are confident of, leaving intricate details unlabelled. Such annotations are already starting to be adopted in some open-access datasets. For example, a recent hyperspectral imaging (HSI) dataset adopted a sparse annotation protocol by annotating representative image regions, omitting marginal areas, superficial blood vessels, adipose tissue and other artefacts [47]. Similarly, the Dresden Surgical Anatomy Dataset (DSAD) offers sparse positive-only annotations for RGB surgical imaging [10]. Yet, the proper application of WSL approaches to such cases lacking background class annotations remains an open question. WSL also preserves less information compared to dense annotations, losing supervisory signal for some object structures. Such difficulties make the training process from sparse positive-only labels challenging. Furthermore, to deploy a fully automated system in a safety-critical environment, the system should not only able to produce reliable results in a known context, but should also be able to flag situations in which it may fail [1, 13]. Conventional segmentation frameworks follow an assumption that all training data and testing data are drawn from the same distribution and are thus considered in-distribution (ID). Under this assumption, at inference, the model should only be used in a similar context, which may imply limiting the acquisition hardware and the presence of unexpected classes such as a new model of surgical instrument. This poses a safety issue when trying to deploy the model for real-world clinical use. Out-of-distribution (OOD) detection may thus be considered a mandatory feature in many clinical applications. It is an active research topic in many classification tasks [21, 33, 31, 22], but has rarely been exploited in medical imaging [30]. We argue that these two challenges outlined above: sparse annotations and the need for OOD detection, share enough similarities to address them under a single methodological approach. In medical image segmentation with sparse annotations, the absence of an annotation does not necessarily imply that a region is identified as negative. Two other possibilities could explain why a positive pixel remains unlabelled: 1) it may be deemed ambiguous by the annotator; or 2) it may simply be skipped due to time-constraints. The most straightforward, albeit wrong approach to handle unlabelled data would be to assume that all such data belongs to the negative or background class. In contrast, positive-unlabelled (PU) learning [4] assumes that an unlabelled example could belong to either the positive or negative class. Most existing work in PU learning focuses on binary classification problems rather than multi-class ones. PU learning can be seen as a specific case within the broader domain of OOD detection. Given the absence of the negative class, traditional PU learning methods are frequently formulated as one-class semi-supervised learning problems [58]. However, research on segmentation within the frameworks of both PU learning and OOD detection is limited. Image segmentation problems often require multi-class learning for which little PU-learning approaches have been proposed. This scarcity of published work is also partly due to the lack of OOD-based evaluation protocols and publicly available benchmark datasets. One potential solution could be to use a different dataset as OOD data during testing [25]. However, this approach poses significant challenges as annotating multiple medical datasets is labor-intensive and requires domain-specific expertise for each dataset. In this paper, we propose a simple but effective medical image segmentation framework to achieve pixel-level OOD detection using sparsely annotated data from positive-only classes. Our framework effectively learns feature representations using sparsely annotated labels, enabling reliable detection of OOD pixels with classical OOD approaches [58] designed for classification purposes. This allows for state-of-the-art OOD detection performance without compromising the classification accuracy for ID classes. To evaluate the model performance on OOD data, we propose a protocol that involves isolating part of the labelled classes during training. These held-out annotations do not contribute to updating the model weights during training but are grouped as an additional outlier class for validation purposes. To effectively evaluate OOD performance for segmentation tasks, we propose using two threshold-independent metrics to measure model performance. Building on these metrics, we further design a threshold selection strategy to visualise OOD segmentation results. Based on our framework, we compare four different classical OOD detection methods integrated in a common U-Net based backbone segmentation model. Our cross-validation results show that combining a model calibration method with the proposed framework achieves the best overall performance. Our contributions are mostly along three folds: 1. We introduce a novel framework based on positive-only learning for multi-class medical image segmentation. Our approach effectively segments negative/OOD data without compromising performance for multi-class positive/ID data. 2. To assess model performance in both ID and OOD scenarios, we propose a two-level cross-validation method and metrics for evaluation. The cross-validation is based on both subjects/patients and classes present in the dataset. Our evaluation approach eliminates the need for an additional OOD testing set. 3. The proposed framework can seamlessly incorporate any given OOD detection method or backbone architecture. In particular, we introduce a novel convolutional adaptation of the GODIN method, extending its applicability to segmentation tasks within our framework. To the best of our knowledge, this represents the first work to address the setting of positive-only learning for multi-class medical image segmentation."
https://arxiv.org/html/2411.09551v1,MFTIQ: Multi-Flow Tracker with Independent Matching Quality Estimation,"In this work, we present MFTIQ, a novel dense long-term tracking model that advances the Multi-Flow Tracker (MFT) framework to address challenges in point-level visual tracking in video sequences. MFTIQ builds upon the flow-chaining concepts of MFT, integrating an Independent Quality (IQ) module that separates correspondence quality estimation from optical flow computations. This decoupling significantly enhances the accuracy and flexibility of the tracking process, allowing MFTIQ to maintain reliable trajectory predictions even in scenarios of prolonged occlusions and complex dynamics. Designed to be “plug-and-play”, MFTIQ can be employed with any off-the-shelf optical flow method without the need for fine-tuning or architectural modifications. Experimental validations on the TAP-Vid Davis dataset show that MFTIQ with RoMa [16] optical flow not only surpasses MFT but also performs comparably to state-of-the-art trackers while having substantially faster processing speed. Code and models available at https://github.com/serycjon/MFTIQ .","Point-level visual tracking is a hot research topic [31, 27, 12, 10]. Instead of the classical task of tracking objects by bounding boxes [2, 26, 8] or segmentation masks[28, 43], the goal is to track arbitrary points lying on surfaces in the scene. The resulting point correspondences are useful for various downstream applications, like SLAM[18, 38] or motion prediction[56]. While most current methods[10, 12, 11, 57, 27] focus on sparse point-tracking, applications like 3D reconstruction, video editing, or augmented reality, benefit from dense correspondences, i.e., correspondences estimated for every pixel of the initial video frame. Traditionally, long-range dense tracking may be achieved by sequential chaining of optical flow (OF). However this approach has major drawbacks. The estimated trajectory drift over time due to error accumulation, and tracking stops to be reliable in presence of occlusion since the sequential chaining has no mechanism to recover. \begin{overpic}[width=173.90271pt]{figures/placeholders/query.jpg} \put(2.0,50.0){\color[rgb]{1,1,1}\definecolor[named]{pgfstrokecolor}{rgb}{% 1,1,1}\pgfsys@color@gray@stroke{1}\pgfsys@color@gray@fill{1} {{Input - frame 1% }}}\end{overpic} \begin{overpic}[width=173.90271pt]{figures/placeholders/mft.jpg} \put(2.0,50.0){\color[rgb]{1,1,1}\definecolor[named]{pgfstrokecolor}{rgb}{% 1,1,1}\pgfsys@color@gray@stroke{1}\pgfsys@color@gray@fill{1} {{MFT - frame 80 % }}}\end{overpic} \begin{overpic}[width=173.90271pt]{figures/placeholders/mftiq.jpg} \put(2.0,50.0){\color[rgb]{1,1,1}\definecolor[named]{pgfstrokecolor}{rgb}{% 1,1,1}\pgfsys@color@gray@stroke{1}\pgfsys@color@gray@fill{1} {{MFTIQ - frame 8% 0}}}\end{overpic} Figure 1: Dense long-term tracking – MFT and MFTIQ comparison. Visualisation of query positions (red) tracked from frame 1 to frame 80. MFTIQ generates a lower number of false re-detections than MFT, especially on the grass in bottom-left, which was out-of-view on frame 1. Best viewed zoomed-in and in color. Recently, Multi-Flow Tracker (MFT) [39] revisited flow chaining [7, 6] for not only consecutive frames, but also for temporally distant frame pairs. MFT produces long, dense trajectories by selecting the most reliable chain of optical flows for each tracked point. The flow chain reliability is determined by accumulating uncertainties and occlusion state computed for each optical flow in the flow chain. However, this uncertainty accumulation can lead to error accumulation and drift. Moreover, MFT is tightly coupled with the RAFT optical flow, but it has been shown [25] that other OF methods perform better. We propose MFTIQ, a dense, long-term tracker. Like other dense point-trackers [5, 29, 39, 53, 25] it is based on optical flow computation. We design the method to be trained once and then work with an arbitrary optical flow method in a “plug-and-play” fashion and without any fine-tuning or architecture changes. This allows the user to choose a suitable speed/performance trade-off by using the appropriate flow. The MFTIQ generalizes to multiple optical flow methods not seen during training, as we have experimentally evaluated. We expect future faster and/or higher quality flows to improve the proposed tracker performance for free, i.e. without any re-training needed. The proposed method replaces the flow-chain-selection method proposed in MFT, with an improved Independent Quality and occlusion estimation module (IQ). Unlike the MFT method, which estimates occlusion and correspondence quality (uncertainty) jointly with optical flow estimation, MFTIQ decouples these estimations from the optical flow computation. This separation also allows the occlusion and correspondence quality to be estimated directly for the optical flow chain between the template and the current frame, without need for error-prone uncertainty accumulation. Figure 1 shows an example where the MFTIQ strategy produces significantly less false re-detections than MFT. MFTIQ achieves results comparable to state-of-the-art trackers when using RoMa [16] as the optical flow estimator and consistently outperforms MFT across most tested optical flow methods. It is important to note that MFTIQ was not trained with RoMa, highlighting the “plug-and-play” functionality of the proposed method. Moreover, for dense tracking, MFTIQ is significantly faster than state-of-the-art trackers, even with the slowest optical flow methods tested. MFTIQ is also causal, i.e., it only uses the current frame and the previous ones, which is not the case for most point-trackers. In this work, we introduce MFTIQ, a novel dense, long-term tracking method improving on the MFT [39] flow-chaining idea. Our contributions are as follows: 1) We have developed the Independent Quality (IQ) module, which decouples occlusion and correspondence quality estimation from optical flow computation. This separation enhances tracking accuracy and flexibility. 2) MFTIQ features “plug-and-play” functionality, allowing integration with any off-the-shelf optical flow method without re-training or fine-tuning. This flexibility enables users to tailor tracker performance to specific needs. 3) We conducted experimental evaluations using multiple optical flow methods, demonstrating that MFTIQ matches the performance of state-of-the-art trackers while being significantly faster in dense tracking scenarios."
https://arxiv.org/html/2411.09540v1,Prompting the Unseen: Detecting Hidden Backdoors in Black-Box Models,"Visual prompting (VP) is a new technique that adapts well-trained frozen models for source domain tasks to target domain tasks. This study examines VP’s benefits for black-box model-level backdoor detection. The visual prompt in VP maps class subspaces between source and target domains. We identify a misalignment, termed class subspace inconsistency, between clean and poisoned datasets. Based on this, we introduce BProm, a black-box model-level detection method to identify backdoors in suspicious models, if any. BProm leverages the low classification accuracy of prompted models when backdoors are present. Extensive experiments confirm BProm’s effectiveness.","Deep neural networks (DNNs) are commonly used in complex applications but require extensive computational power, leading to significant costs. Users often access these models through online platforms like BigML model market111https://bigml.com/ and ONNX zoo222https://github.com/shaoxiaohu/model-zoo, or via Machine Learning as a Service (MLaaS) platforms. However, DNNs can include backdoors (Gu et al., 2017; Liu et al., 2018b; Tang et al., 2021; Qi et al., 2023b; Nguyen & Tran, 2021; Chen et al., 2017), which manipulate model responses to inputs with specific triggers (like certain pixel patterns) while functioning correctly on other inputs. In backdoor attacks, attackers embed these triggers in the training data, leading the model to associate the trigger with a particular outcome and misclassify inputs containing it. Why Black-Box Model-Level Detection. Black-box backdoor detection, which uses only black-box queries to the suspicious model (i.e., the model to be inspected), is gaining attention. This detection method is divided into input-level (Li et al., 2021c; Qiu et al., 2021; Gao et al., 2022; Liu et al., 2023; Qi et al., 2023c; Zeng et al., 2023; Guo et al., 2023; Hou et al., 2024; Xu et al., 2024; Mo et al., 2024) and model-level (Huang et al., 2020; Dong et al., 2021; Guo et al., 2022; Xu et al., 2019; Wang et al., 2024) techniques. Input-level detection identifies trigger samples in an infected model, while model-level detection determines if a model contains backdoors. Input-level detection relies on the model having backdoors; otherwise, its accuracy drops significantly. For example, as shown in Table 1, TeCo (Liu et al., 2023) and SCALE-UP (Guo et al., 2023), state-of-the-art input-level detectors, show AUROCs of 0.8113 and 0.7877, respectively, on a BadNets-infected model (Gu et al., 2017), but only 0.4509 and 0.5103 on a clean model. If a model is clean, many legitimate samples may be misclassified as triggers, reducing the model’s practical utility. Thus, model-level detection should be performed first. If backdoors are found but the model must still be used, input-level detection should then be applied to each input. Table 1: A significant drop of F1-score and AUROC in black-box input-level detection methods, TeCo (Liu et al., 2023) and SCALE-UP (Guo et al., 2023). BadNet (Gu et al., 2017) Blended (Chen et al., 2017) WaNet (Nguyen & Tran, 2021) TeCo (Liu et al., 2023) Backdoored Clean Backdoored Clean Backdoored Clean F1 0.8014 0.5263 0.7621 0.5033 0.9295 0.5137 AUROC 0.8113 0.4509 0.7259 0.3954 0.9345 0.4406 ScaleUp (Guo et al., 2023) Backdoored Clean Backdoored Clean Backdoored Clean F1 0.7964 0.5236 0.7991 0.5046 0.7199 0.4768 AUROC 0.7877 0.5103 0.7694 0.4643 0.7772 0.4246 Design Challenge. Despite its importance, black-box model-level detection faces two main challenges. First, unlike input-level detection, which benefits from the presence of an infected model, model-level detection has limited ground truth, relying on only a few clean samples. Second, it needs a stable feature to differentiate between clean and infected models across various backdoor types, which is difficult to find. For instance, B3D (Dong et al., 2021) targets trigger localization but is mainly effective for patch-based triggers. Similarly, AEVA (Guo et al., 2022) may struggle with larger triggers due to its dependence on adversarial peak analysis. (a) The “3” is from MNIST, while the middle part shows the visual prompt. The prompted sample is ready for ImageNet classifier. (b) The prompted sample can be fed into the ImageNet classifier, whose output has a mapping between the labels from MNIST and ImageNet. Figure 1: How a frozen ImageNet classifier is adapted for the MNIST classification when VP is used. Our Design. Visual prompting (VP) (Bahng et al., 2022; Jia et al., 2022) allows a frozen, pre-trained model from a source domain to correctly predict samples from a target domain by applying a visual prompt. This technique can work across very different domains; for example, an ImageNet classifier (source) can detect melanoma (target) via VP (Tsai et al., 2020). Figure 1 illustrates VP, where the visual prompt (trainable noise in Figure 1(a)) maps between class subspaces of the source and target domains, enabling the frozen classifier to handle the target task efficiently. In an infected model, the target class subspace in the feature space is adjacent to all other class subspaces (Wang et al., 2019). We identify a class subspace inconsistency where misalignment between class subspaces in the poisoned (source) and clean (target) datasets leads to low classification accuracy of the prompted model. This phenomenon is illustrated in Figure 2 and experimentally validated in both Figure 3 and Section C. Based on this, we propose BProm for black-box model-level backdoor detection. BProm applies VP to a suspicious model using an unrelated clean dataset; poor accuracy in the prompted model indicates the presence of backdoors. Contribution. Our contributions can be summarized as follows. 1) We identify a class subspace inconsistency in VP on backdoor-infected models. This misalignment between class subspaces of the poisoned dataset and an external clean dataset signals backdoor infection. 2) Utilizing this inconsistency, we develop BProm, a black-box model-level backdoor detection method. (a) Class subspace inconsistency does not occur: visual prompt as a mapping between two clean datasets. (b) Subspace inconsistency occurs: visual prompt as a mapping between clean and poisoned datasets. Figure 2: A conceptual illustration of (a) VP on clean model and (b) VP on backdoor-infected model. (a) Clean source model shows clear class separation. (b) Clean target model preserves clear separation. (c) Target class (0) is adj. to others in infected source model. (d) Infected target model shows severe class confusion. Figure 3: Class subspaces inconsistency (CIFAR-10 for source model and STL-10 for target model)."
https://arxiv.org/html/2411.09538v1,Marker-free Human Gait Analysis using a Smart Edge Sensor System,"The human gait is a complex interplay between the neuronal and the muscular systems, reflecting an individual’s neurological and physiological condition. This makes gait analysis a valuable tool for biomechanics and medical experts. Traditional observational gait analysis is cost-effective but lacks reliability and accuracy, while instrumented gait analysis, particularly using marker-based optical systems, provides accurate data but is expensive and time-consuming. In this paper, we introduce a novel markerless approach for gait analysis using a multi-camera setup with smart edge sensors to estimate 3D body poses without fiducial markers. We propose a Siamese embedding network with triplet loss calculation to identify individuals by their gait pattern. This network effectively maps gait sequences to an embedding space that enables clustering sequences from the same individual or activity closely together while separating those of different ones. Our results demonstrate the potential of the proposed system for efficient automated gait analysis in diverse real-world environments, facilitating a wide range of applications.","Locomotion, particularly walking, is an essential ability for humans. It is learned at an early age and is usually a subconscious process. Moving on two legs gives us independence and makes physical activities like running possible. Because of its complexity and uniqueness, gait analysis attracts significant interest from both experts in biomechanics and medical professionals. Observing and analyzing changes in gait can not only provide insight into neurological and physiological conditions, but can also aid in the development and evaluation of individualized treatments [1, 2]. The clinical use of gait analysis mainly focuses on observational gait analysis performed with human eye and brain. This method is simple and cost-efficient but suffers from low validity, reliability, and responsiveness. Instrumented gait analysis on the other hand promises accurate and reliable gait data for medical use [2]. Marker-based optical motion capture systems are considered the gold standard in instrumented gait analysis. They provide a high level of precision, but are expensive and time-consuming to use [3]. Time(a)(b)(c) Figure 1: Sample movement sequence (b) captured with smart edge sensors (a) consisting of a Nvidia Jetson Orin compute board and an Intel RealSense RGB-D camera. (c) t-SNE visualization of activities from Human 3.6M dataset based on the learned gait embedding. In this work, we instead employ a markerless approach to capture human movement, developed in previous work [4, 5, 6]. A multi-camera system is used to estimate the 3D poses of multiple persons in real time, as illustrated in Fig. 1 (a, b). We refer to the camera nodes as smart edge sensors, as they contain an integrated inference accelerator for local, on-board, semantic image interpretation. Unlike marker-based systems that require applying markers to the participant’s body (e.g. wearing a marker suit), the smart edge sensors detect human joints by inferring heatmaps of human body keypoints from the camera images. These keypoint detections are streamed to a central backend, where the pose estimates of each camera are fused into a 3D skeleton per observed person. The fusion process incorporates priors on typical bone lengths to enhance the accuracy of the pose estimation [4, 5]. The proposed system enables the capture of human motion sequences and gait measurements in real time, making gait analysis accessible for wider use in diverse, real-world environments. Another significant challenge in clinical gait analysis is the wide range of parameters affecting human gait. Efficiently capturing gait patterns and accurately recognizing individuals based on them could simplify the differentiation between patient groups. Identifying similar gait characteristics across different patients may indicate a shared underlying condition, aiding in the selection of appropriate treatments. Integrating machine learning methods into this analysis facilitates recognizing individual gait patterns and clustering similar traits or activities across different individuals [7, 8, 9, 10]. In this work, we design a Siamese embedding network based on TriNet [11] for the identification of individuals by their walking patterns and for the differentiation of activities. The network takes gait sequences as input and maps them into an embedding space, using a ResNet 18 backbone [12]. We train the network using the Triplet Loss [13] on L_{2}-distances in the embedding space to ensure that motion sequences from the same person or activity are positioned close together, while sequences from different individuals or actions are pushed apart (cf. Fig. 1 (c)). In summary, our main contributions in this paper are: • We propose a novel deep learning-based framework to cluster human walking patterns in an embedding space to identify similar gait patterns or activities. • We collect accurate gait data from multiple subjects in a real-world environment using a smart edge sensor network to capture human movement without the need to apply markers or sensors to the body. • We quantitatively and qualitatively evaluate the proposed integrated system for human motion capture and gait analysis using the collected real-world data and the Human 3.6M database [14]."
https://arxiv.org/html/2411.09484v2,"Image Matching Filtering and Refinement
by Planes and Beyond","This paper introduces a modular, non-deep learning method for filtering and refining sparse correspondences in image matching. Assuming that motion flow within the scene can be approximated by local homography transformations, matches are aggregated into overlapping clusters corresponding to virtual planes using an iterative RANSAC-based approach, with non-conforming correspondences discarded. Moreover, the underlying planar structural design provides an explicit map between local patches associated with the matches, enabling optional refinement of keypoint positions through cross-correlation template matching after patch reprojection. Finally, to enhance robustness and fault-tolerance against violations of the piece-wise planar approximation assumption, a further strategy is designed for minimizing relative patch distortion in the plane reprojection by introducing an intermediate homography that projects both patches into a common plane. The proposed method is extensively evaluated on standard datasets and image matching pipelines, and compared with state-of-the-art approaches. Unlike other current comparisons, the proposed benchmark also takes into account the more general, real, and practical cases where camera intrinsics are unavailable. Experimental results demonstrate that our proposed non-deep learning, geometry-based approach achieves performances that are either superior to or on par with recent state-of-the-art deep learning methods. Finally, this study suggests that there are still development potential in actual image matching solutions in the considered research direction, which could be in the future incorporated in novel deep image matching architectures.","1.1 Background Image matching constitutes the backbone of most higher-level computer vision tasks and applications that require image registration to recover the 3D scene structure, including the camera poses. Image stitching [1, 2], Structure-from-Motion (SfM) [2, 3, 4, 5], Simultaneous Localization and Mapping (SLAM) [6, 2] and the more recently Neural Radiance Fields (NeRF) [7, 8] and Gaussian Splatting (GS) [9, 10] are probably nowadays the most common, widespread and major tasks critically relying on image matching. Image matching traditional paradigm can be organized into a modular pipeline concerning keypoint extraction, feature description, and the proper correspondence matching [11]. This representation is being neglected in modern deep end-to-end image matching methods due to their intrinsic design which guarantees a better global optimization at the expense of the interpretability of the system. Notwithstanding that end-to-end deep networks represent for many aspects the State-Of-The-Art (SOTA) in image matching, such in terms of the robustness and accuracy of the estimated poses in complex scenes [12, 13, 14, 15, 16, 17, 18], handcrafted image matching pipelines such as the popular Scale Invariant Feature Transform (SIFT) [19] are employed still today in practical applications [4, 2] due to their scalability, adaptability, and understandability. Furthermore, handcrafted or deep modules remain present in post-process modern monolithic deep architectures, to further filter the final output matches as done notably by the RAndom SAmpling Consensus (RANSAC) [20] based on geometric constraints. Image matching filters to prune correspondences are not limited to RANSAC, which in any of its form [21, 22, 23, 24] still remains mandatory nowadays as final step, and have been evolved from handcrafted methods [25, 26, 27, 28, 29, 30, 31, 32, 33] to deep ones [34, 35, 36, 37, 38, 39, 40, 41]. In addition, more recently methods inspired by coarse-to-fine paradigm, such as the Local Feature TRansformer (LoFTR) [14], have been developed to refine correspondences [42, 43, 44], which can be considered as the introduction of a feedback-system in the pipelined interpretation of the matching process. 1.2 Main contribution As contribution, within the former background, this paper introduces a modular approach to filter and refine matches as post-process before RANSAC. The key idea is that the motion flow within the images, i.e. the correspondences, can be approximated by local overlapping homography transformations [45]. This is the core concept of traditional image matching pipelines, where local patches are approximated by less constrained transformations going from a fine to a coarse scale level. Specifically, besides the similarity and affine transformations, the planar homography is introduced at a further level. The whole approach proceeds as follows: • The Multiple Overlapping Planes (MOP) module aggregates matches into soft clusters, each one related to a planar homography roughly representing the transformation of included matches. On one hand, matches unable to fit inside any cluster are discarded as outliers, on the other hand, the associated planar map can be used to normalize the patches of the related keypoint correspondences. As more matches are involved in the homography estimation, the patch normalization process becomes more robust than the canonical ones used for instance in SIFT. Furthermore, homographies better adapt the warp as they are more general than similarity or affine transformations. MOP is implemented by iterating RANSAC to greedily estimate the next best plane according to a multi-model fitting strategy. The main difference from previous similar approaches [46, 47], is in maintaining matches during the iterations in terms of strong and weak inliers according to two different reprojection errors to guide the cluster partitioning. • The Middle Homography (MiHo) module additionally improves the patch normalization by further minimizing the relative patch distortion. More in detail, instead of reprojecting the patch of one image into the other image taken as reference according to the homography associated with the cluster, both the corresponding patches are reprojected into a virtual plane chosen to be in the middle of the base homography transformations so that globally the deformation is distributed into both the two patches, reducing the patch distortions with respect to the original images due to interpolation but also to the planar approximation. • The Normalized Cross-Correlation (NCC) is employed following traditional template matching [48, 2] to improve the keypoint localization in the reference middle homography virtual plane using one patch as a template to being localized on the other patch. The relative shift adjustment found on the virtual plane is finally back-projected through into the original images. In order to improve the process, the template patch is perturbed by small rotations and anisotropic scaling changes for a better search in the solution space. MOP and MiHo are purely geometric-based, requiring only keypoint coordinates, while NCC relies on the image intensity in the local area of the patches. The early idea of MOP+MiHo+NCC can be found in [49]. Furthermore, MOP overlapping plane exploitation concept have been introduced in [50], while MiHo definition was draft in [51]. The proposed approach is handcrafted and hence more general as is does not require re-training to adapt to the particular kind of scene and its behavior is more interpretable as not hidden by deep architectures. As reported and discussed later in this paper, RANSAC takes noticeable benefits from MOP+MiHo as pre-filter before it in most configurations and scene kinds, and in any case does not deteriorate the matches. Concerning MOP+MiHo+NCC instead, keypoint localization accuracy seems to strongly depend on the kind of extracted keypoint. In particular, for corner-like keypoints, which are predominant in detectors such as the Keypoint Network (Key.Net) [52] or SuperPoint [53], NCC greatly improves the keypoint position, while for blob-like keypoints, predominant for instance in SIFT, NCC application can degrade the keypoint localization, due probably to the different nature of their surrounding areas. Nevertheless, the proposed approach indicates that there are still margins for improvements in current image matching solutions, even the deep-based ones, exploiting the analyzed methodology. 1.3 Additional contribution As further contribution, an extensive comparative evaluation is provided against eleven less and more recent SOTA deep and handcrafted image matching filtering methods on both non-planar indoor and outdoor standard benchmark datasets [54, 55, 11], as well as planar scenes [56, 57]. Each filtering approach, including several combinations of the three proposed modules, has been tested to post-process matches obtained with seven different image matching pipelines and end-to-end networks, also considering distinct RANSAC configurations. Moreover, unlike the mainstream evaluation which has been taken as current standard [14], the proposed benchmark assumes that no camera intrinsics are available, reflecting a more general, realistic, and practical scenario. On this premise, the pose error accuracy is defined, analyzed, and employed with different protocols intended to highlight the specific complexity of the pose estimation working with fundamental and essential matrices [45], respectively. This analysis reveal the many-sided nature of the image matching problem and of its solutions. 1.4 Paper organization The rest of the paper is organized as follows. The related work is presented in Sec. 2, the design of MOP, MiHo, and NCC is discussed in Sec. 3, and the evaluation setup and the results are analyzed in Sec.4. Finally, conclusions and future work are outlined in Sec. 5. 1.5 Additional resources Code and data are freely available online111https://github.com/fb82/MiHo. Data also include high-resolution versions of the images, plots, and tables presented in this paper222https://github.com/fb82/MiHo/tree/main/data/paper."
https://arxiv.org/html/2411.09471v1,Renal Cell Carcinoma subtyping: learning from multi-resolution localization,"Renal Cell Carcinoma is typically asymptomatic at the early stages for many patients. This leads to a late diagnosis of the tumor, where the curability likelihood is lower, and makes the mortality rate of Renal Cell Carcinoma high, with respect to its incidence rate. To increase the survival chance, a fast and correct categorization of the tumor subtype is paramount. Nowadays, computerized methods, based on artificial intelligence, represent an interesting opportunity to improve the productivity and the objectivity of the microscopy-based Renal Cell Carcinoma diagnosis. Nonetheless, much of their exploitation is hampered by the paucity of annotated dataset, essential for a proficient training of supervised machine learning technologies. This study sets out to investigate a novel self supervised training strategy for machine learning diagnostic tools, based on the multi-resolution nature of the histological samples. We aim at reducing the need of annotated dataset, without significantly reducing the accuracy of the tool. We demonstrate the classification capability of our tool on a whole slide imaging dataset for Renal Cancer subtyping, and we compare our solution with several state-of-the-art classification counterparts.","Renal Cell Carcinoma (RCC) is a highly malignant tumor and the most widespread type of kidney cancer, accounting for 90% of the overall entities. This tumor is also the 7th most common histological type in the west, and it is continuously increasing [1]. Its mortality rate is considered high, with respect to its incidence rate, as this tumor is typically asymptomatic at the early stages for many patients [1, 2]. This leads to a late diagnosis of the tumor, where the curability likelihood is lower. RCC can be categorized into multiple histological subtypes, mainly: Clear Cell Renal Cell Carcinoma (ccRCC) forming 75% of RCCs, Papillary Renal Cell Carcinoma (pRCC) accounting for 10%, and Chromophobe Renal Cell Carcinoma (chRCC) accounting for 5%. Some of the other sutypes include Collecting Duct Renal Cell Carcinoma (cdRCC), Tubulocystic Renal Cell Carcinoma (tRCC), and unclassified [1]. Approximately 10% of renal tumors belong to the benign entities neoplasms, being Oncocytoma (ONCO) the most frequent subtype with an incidence of 3–7% among all RCCs [3, 2]. These subtypes show different cytological signature as well as histological features [2], which ends up in significantly different prognosis. The correct categorization of the tumor subtype is indeed of major importance, as prognosis and treatment approaches depend on it and on the disease stage. For instance, the overall 5-year survival rate significantly differs among the different histological subtypes, being 55–60% for ccRCC, 80–90% for pRCC and 90% for chRCC. This points out the need for the most accurate subclassification [4, 5]. Existing literature emphasizes also the critical role of the differential diagnosis between chromophobe and oncocytoma, known to be arduous and prone to errors due to overlapping morphological characteristics in a relevant number of cases [2, 6, 3]. Currently, the gold standard to classify RCC subtypes consists in the microscopic visual assessment of Hematoxylin and Eosin (H&E) samples, performed by a pathologist though the microscope. These specimen consist most often of physical slides and, in some centers provided with scanner, of virtual slides: the so-called Whole histological Slide Images (WSIs). The visual diagnosis of the large WSIs is known to be both effort-requiring and time-consuming, resulting in poor alignment between pathologists in some cases. Ultimately, these aspects have a great impact on diagnosis, prognosis and treatment of RCC neoplasms [7]. Computerized methods represents an interesting opportunity to improve the productivity, as well as the objectivity, of the microscopy-based RCC diagnosis [7]. In this regards, recent evidences suggest that Convolutional Neural Networkss (CNNs), a famous class of supervised deep learning algorithms, may be proficiently applied to the classification of RCC subtypes [7]. This is mainly due to the CNNs’ capability to discover unseen data structures and extract robust features representation [7, 8]. Nonetheless, much of their strength depend on the exploitation of large labeled datasets: CNNs are data-hungry supervised algorithms, which demand a large amount of annotated training sample to learn an effective data representation [9, 10]. This aspect is a strong limitation, especially in the histological field [11], where the samples annotation requires a skilled pathologist to visually scrutinise each WSIs and to divide it into sub-regions, homogeneous in terms of tissue architecture, lastly assigned to a corresponding label. This approach, known in the literature as \sayRegion Of Interest (ROI)-cropping procedure [11], is known to be tedious and time consuming for the pathologist, and also prone to error as well, with the concrete risk of compromising the models training phase due to an inaccurate labelling. As a consequence, Self-Supervised Learning (SSL) has been recently attracting considerable interest, being able to describe data structures via robust featurization, without requiring any supervision in term of samples annotations [9, 12]. Unfortunately, most self-supervised techniques exploit natural-scene image properties, which are not suitable for histopathology specimen, as evidenced by some recent works [9, 12]. This study sets out to assess RCC subtype classification, based on WSIs, leveraging a novel self-supervised paradigm. Our solution, inspired by the decision-making procedure of the pathologist, incorporates features learnt at different magnification level. With our experiments, we show that through histologic-specific SSL paradigm, it is possible to classify the four most common RCC subtypes with performance comparable to the fully supervised solutions, but without requiring massive data annotation."
https://arxiv.org/html/2411.09462v2,SINETRA: a versatile framework for evaluating single neuron tracking in behaving animals,"Accurately tracking neuronal activity in behaving animals presents significant challenges due to complex motions and background noise. The lack of annotated datasets limits the evaluation and improvement of such tracking algorithms. To address this, we developed SINETRA, a versatile simulator that generates synthetic tracking data for particles on a deformable background, closely mimicking live animal recordings. This simulator produces annotated 2D and 3D videos that reflect the intricate movements seen in behaving animals like Hydra Vulgaris. We evaluated four state-of-the-art tracking algorithms highlighting the current limitations of these methods in challenging scenarios and paving the way for improved cell tracking techniques in dynamic biological systems.","Numerous algorithms have been developed to track neurons in fluorescence live imaging of behaving animals, particularly in the worm Caenorhabditis (C.) Elegans, Zebrafish or the freshwater cnidarian Hydra Vulgaris. These animals undergo significant body deformations during behaviors such as worm crawling or Hydra’s contraction and complex somersaulting [1]. A first class of tracking methods employs analytical approaches, such as Bayesian filtering and global distance minimization, to track neuronal activity [2, 3]. These methods can be complemented with tracklet stitching to assemble fragmented cell trajectories into continuous tracks [4, 5]. These Bayesian frameworks can also integrate optical flow (OF) estimation of cell velocity allowing for more precise updates of Kalman filters during tracking [6]. A second class of methods relies on deep learning to perform image registration and establish neuron correspondence across different poses of the animal over time. [7, 8, 9]. To objectively compare cell tracking algorithms, several software tools[10, 11] and annotated datasets[12] have been developed. However, these mainly address classical cell tracking challenges, focusing on randomly moving and dividing cells in vitro, which do not reflect the conditions of neuron tracking in behaving animals. Neurons do not divide and their motion is driven by the animal’s body deformations. As a result, evaluating neuron tracking algorithms typically requires labor-intensive manual annotations of time-lapse sequences, which is impractical for large datasets, especially in three dimensions[13, 9, 4]. To facilitate the objective comparison and improvement of SIngle NEurons TRAcking algorithms, we developed SINETRA, a versatile framework for simulating two- and three-dimensional time-lapse sequences reproducing the complex motions of neurons in live fluorescence imaging of behaving animals. This simulator models particles (neurons) as spots within a structured background that accounts for the auto-fluorescence of the embedding tissue. The tissue deformations related to animal behaviors are either modeled with a system of damped harmonic oscillators subjected to random contraction or elongation forces, or are directly extracted from optical flow estimates of live animal deformations. Using this simulation framework, we benchmarked four tracking algorithms on our synthetic data: eMHT [3] and u-track [2] that are standard Bayesian filtering and distance minimization methods, KOFT [6] that uses optical flow to accurately estimate cell velocity in a Kalman filter and ZephIR [9] which propagates semi-annotated tracks using image registration. This benchmark underscores the existing limitations of these approaches when dealing with complex movements, opening the door for enhanced cell tracking methods in dynamic biological systems."
https://arxiv.org/html/2411.09449v1,Image Regeneration: Evaluating Text-to-Image Model via Generating Identical Image with Multimodal Large Language Models,"Diffusion models have revitalized the image generation domain, playing crucial roles in both academic research and artistic expression. With the emergence of new diffusion models, assessing the performance of text-to-image models has become increasingly important. Current metrics focus on directly matching the input text with the generated image, but due to cross-modal information asymmetry, this leads to unreliable or incomplete assessment results. Motivated by this, we introduce the Image Regeneration task in this study to assess text-to-image models by tasking the T2I model with generating an image according to the reference image. We use GPT4V to bridge the gap between the reference image and the text input for the T2I model, allowing T2I models to understand image content. This evaluation process is simplified as comparisons between the generated image and the reference image are straightforward. Two regeneration datasets spanning content-diverse and style-diverse evaluation dataset are introduced to evaluate the leading diffusion models currently available. Additionally, we present ImageRepainter framework to enhance the quality of generated images by improving content comprehension via MLLM guided iterative generation and revision. Our comprehensive experiments have showcased the effectiveness of this framework in assessing the generative capabilities of models. By leveraging MLLM, we have demonstrated that a robust T2M can produce images more closely resembling the reference image.","With the rise of generative AI, there has been a surge in the development of text-to-image (Rombach et al. 2022a; Podell et al. 2023; Ramesh et al. 2022; Saharia et al. 2022; Yu et al. 2022) and text-to-video (Khachatryan et al. 2023; Liu et al. 2024) models in recent years. The primary goal of such algorithms is to generate visual content based on user prompts. These technologies have broad applications (Yang et al. 2024b), such as style transfer (Zhang et al. 2023; Wang, Zhao, and Xing 2023), image editing (Gal et al. 2022; Ruiz et al. 2023a, b), and more, holding significant importance in academic and creative domains. Despite rapid advancements in related algorithms and applications, a research gap remains in evaluating the quality of these generative models. Current T2I model evaluation focuses on two modalities: text input and image output. For example, CLIP score (Radford et al. 2021) is widely used to measure the semantic information within images based on user prompts. However, the CLIP score has lower sensitivity to fine-grained details and visual variances and cannot reflect the quality of generated images. Similarly, the recently popular evaluation method QG&QA (Question Generation and Question Answering) generates several questions and answers related to the generated image using the textual input and employs VQA methods to compare and evaluate the image, such as T2I-CompBench (Huang et al. 2023). While such methods have achieved some results in measuring the consistency between text and image content, they cannot effectively assess the model’s overall performance under complex prompt conditions. We can derive two insights from the current evaluation methods. First, current methods focus on two different modalities, text input and image output, which inherently involve differences and information asymmetry between modalities, making evaluation challenging. Second, a good generative model should perform well in complex, real-world scenarios. Current methods focus on evaluating single attributes, but they lack comprehensive assessment for complex real-world scenarios involving multiple conditions. Motivated by these insights, we propose image regeneration task, similar to ”Painting Reproduction”, given a reference image, we require the model under evaluation to generate an image based on it. The generated image is then assessed by comparing it to the reference image to determine the model’s generation ability. Figure 1 compares the architecture of image regeneration and current metrics. The reference image is more informative than text prompts and aligns the output modalities, leading to more reasonable evaluation results. Since the generative model uses text inputs, we have developed an MLLM(Yang et al. 2023a; Nyberg et al. 2021) based method to convert from image to text input. We propose a framework ImageRepainter for evaluating the quality of text-to-image models based on the task of image regeneration. The framework involves two stages. Specifically: (1) Image understanding: Firstly, based on MLLMs, the image information is organized to generate a tree-like structure called the image understanding tree (IUT), and then text prompts are generated using the information from it. (2) Iterative generation: Iterative exploration (Yang et al. 2023b, 2024a; Wang et al. 2023) is typically involved in T2I generation for better images. This stage includes 4 parts: prompt generation/revision, image generation, image selection, and feedback generation. Furthermore, We introduce two benchmarks respectively designed for the evaluation of the content and style of the generated results. The contributions if this work can be summarized as: • Novel T2I Model Evaluation Task: This framework’s conception is rooted in the concept of “Painting Reproduction” which naturally aligns with the human judgment methodology. The task is able to measure the overall generative capability of the T2I model as well as reflect its generative speciality. • Effective Image Comprehensive Mechanism: We introduce Image Understanding Tree(IUT) to enhance MLLMs’ mutimodel ability. By defining fundamental rules, we incentivize MLLMs to interact with images and summarize the image information in a hierarchical tree structure, namely IUT. • Two Diverse Benchmarks: We propose two benchmarks respectively designed for the evaluation of the content and style of the generated results. The experimental results indicate that our benchmark aligns with human perception in assessing the generation capabilities of T2I models."
https://arxiv.org/html/2411.09439v1,Spider: Any-to-Many Multimodal LLM,"Multimodal LLMs (MLLMs) have emerged as an extension of Large Language Models (LLMs), enabling the integration of various modalities. However, Any-to-Any MLLMs are limited to generating pairwise modalities ’Text + X’ within a single response, such as Text + {Image or Audio or Video}. To address this limitation, we introduce Spider, a novel efficient Any-to-Many Modalities Generation (AMMG) framework, which can generate an arbitrary combination of modalities ’Text + Xs’, such as Text + {Image and Audio and Video}. To achieve efficient AMMG, our Spider integrates three core components: a Base Model for basic X-to-X (i.e., Any-to-Any) modality processing, a novel Efficient Decoders-Controller for controlling multimodal Decoders to generate Xs (many-modal) contents, and an Any-to-Many Instruction Template designed for producing Xs signal prompts. To train Spider, we constructed a novel Text-formatted Many-Modal (TMM) dataset, which facilitates the learning of the X-to-Xs (i.e., Any-to-Many) capability necessary for AMMG. Ultimately, the well-trained Spider generates a pseudo X-to-Xs dataset, the first-ever X-to-Xs many-modal dataset, enhancing the potential for AMMG task in future research. Overall, this work not only pushes the boundary of multimodal interaction but also provides rich data support for advancing the field.","In recent years, Large Language Models (LLMs) such as Vicuna [10], LLaMA [51], ChatGPT [37], and GPT-4 [1] have revolutionized the field of natural language processing by demonstrating human-level proficiency in language understanding and generation. These models, with their extensive training on large-scale corpora, have become foundational tools across a wide range of applications, from chatbots to advanced reasoning systems. However, as the demand for more complex, real-world applications grew, the need for integrating LLMs with multiple types of input and output modalities (e.g., text, images, audio, video) became apparent. This evolution has led to the rise of Multimodal LLMs (MLLMs), which extend LLMs’ capabilities by incorporating multimodal perception modules [21, 67, 46, 26, 2, 27, 33, 47, 59]. Figure 1: Comparison between (a) the X-to-X (Any-to-Any) MLLMs support the input and output of pairwise modalities ’Text + X’, and (b) our X-to-Xs (Any-to-Many) Spider model produces many modalities ’Text + Xs’. X denotes any-one-modality such as one of image or video or audio, and Xs means arbitrary-combination-modalities such as the combination of image and video and audio. An example is shown in Fig.2. Our ’Text + Xs’ many-modal output is a general paradigm, while ’Text + X’ is a special case of it. The development of MLLMs marks a significant advancement in enabling comprehensive understanding and generation across various modalities. Initially, models like LLaVA1.5 [32] and MiniGPT-4 [67] were capable of processing only two modalities: text and images. This setup, while useful for tasks such as image captioning and simple visual question answering, was limited in its ability to handle richer interactions involving more than two modalities. Further innovations saw the rise of models like PandaGPT [47], OneLLM [17], Gemini [50], and NExT-GPT [59], which expanded support to four modalities, incorporating text, images, audio, and video into their multimodal frameworks. Models like MiniGPT-v2 [9], KOSMOS-2 [38], and NExT-Chat [63] further enhanced capabilities by integrating region-level reasoning with modalities of box and mask. However, as depicted in Fig.1 (a), these X-to-X (Any-to-Any) MLLMs are restricted to generate pairwise modalities ’Text + X’ within a single interaction, such as ’Text + Image’ or ’Text + Audio’. For example, when a user asks for a text description of a dog, the model might first respond with the text output. In subsequent interactions, the user could request an image of the dog and then an audio clip of the dog’s bark, prompting two separate responses for the image and the audio. These MLLMs based on Multi-Round Dialogue Generation paradigm, require several rounds of user questions and do not allow for a seamless integration of multiple modalities within a single interaction. Each pair of modalities is handled independently, resulting in a fragmented user experience where the responses feel disjointed rather than cohesive. Figure 2: Example of Spider providing a many-modal travel guide. In contrast, as illustrated in Fig.1 (b), our proposed X-to-Xs (Any-to-Many) Spider model, aims to achieve Any-to-Many Modalities Generation (AMMG) in a single response, which supports arbitrary combinations of a broader range of modalities: text, image, audio, video, box, and mask. For instance, with Spider, a user could pose a single question like, ”Describe a dog using text, image, and audio.” In response, Spider would generate a cohesive output that combines text, images, and audio in a single response, greatly enhancing the user experience by providing comprehensive many-modal contents all at once. Figure 3: (a) The Spider structure comprises four parts including Encoders, LLM, Decoders-Controller, and Decoders. The LLM is utilized as the core to process input multimodal information encoded by Encoders for semantic understanding and reasoning. Then, the LLM not only generates Text response, but also produces Text Prompt (T-Prompt) and Modality Prompt (M-Prompt) for the subsequent Decoders-Controller to schedule and control multimodal Decoders. (b) With Any-to-Many Instruction Template, T-Prompt and M-Prompt are gathered to form many-modal signal prompts, which are able to control Decoders to generate many-modal contents. There is an example of many-modal signal prompts: ’<IMAGE> dog [{IMAGE}_{0}]</IMAGE>. <AUDIO> dog’s bark [{AUDIO}_{0}]</AUDIO>.’ Where ’<IMAGE>...</IMAGE>’ and ’<AUDIO>...</AUDIO>’ are the begin-end signal pairs of image and audio, respectively. ’dog’ and ’dog’s bark’ are T-Prompt. ’[{IMAGE}_{0}]’ and ’[{AUDIO}_{0}]’ are M-Prompt. Overall, we call this Modality-wise Grouping, i.e., each modality signal prompt is grouped by the corresponding begin-end signal pair containing the T-Prompt and M-Prompt inside. It allows arbitrary concatenation of different modality signal prompts. The X-to-X MLLMs only require the LLM to perform X-modality instruction comprehension and prompt generation, which is a one-to-one task. Differently, X-to-Xs (Any-to-Many Modalities Generation) is a more complex one-to-many task, which needs to perform Xs-modality instruction comprehension and prompt generation. Specifically, the X-to-Xs model needs the LLM to accurately understand the instructional requirements of any combination of Xs-modalities in the input question, and also produce the task prompts to correctly guide different decoders for Xs-modalities generation. For example, for the question “Generate an image of a dog, and I also would like to hear the dog’s bark,” due to the explicit appearance of “image,” the LLM may interpret it only as an image generation task, potentially overlooking the audio generation of “dog’s bark” or wrongly outputting the image generation task prompt for “dog’s bark”. To address the above challenges and achieve efficient Any-to-Many Modalities Generation, we first designed a novel model named Spider as presented in Fig.3, and then constructed a novel Text-formatted Many-Modal (TMM) dataset to train the proposed model. Our Spider framework incorporates three key components, including Base Model, Efficient Decoders-Controller, and Any-to-Many Instruction Template: \bullet Base Model (Encoders-LLM-Decoders structure), supports the basic ability of X-to-X modality processing: Using multimodal Encoders to encode multimodal inputs, followed by an LLM to perform semantic understanding and reasoning, and finally the produced prompts by LLM are used to control multimodal Decoders for generating multiple modality outputs. Figure 4: Comparison between (a) current MLLMs with Multiple Projectors for LLM-Decoders alignment, and (b) our Unified Decoder Projector, where {M-Query} denotes the list of learnable Modality Query (M-Query) for the corresponding output modalities. The parameter of {M-Query} is far less than Projector. As the number of modalities increases, the number of Projectors (i.e., the parameters) in (a) grows linearly. However, our Unified Decoder Projector adds the modality query in {M-Query}, with merely parameter growth. \bullet Efficient Decoders-Controller, enables the LLM to efficiently schedule and control multiple task decoders for generating many-modal contents: (a) Efficient Learning: Our TM-Fusion module integrates Text Prompt (T-Prompt) and Modality Prompt (M-Prompt) to control multiple task decoders. Let LLM generate specific Text Prompt is learning efficient. The Modality Prompt is only considered as supplementary information for the Text Prompt to retain input modality information, which lowers the learning difficulty. Besides, the Modality Prompt also serves as a modality signal to schedule the corresponding task decoders, which ensures to correctly guide different decoders for Xs-modalities generation. (b) Efficient Training: Only training the output-side Decoders-Controller to achieve efficient feature alignment with pre-trained LLM and the multimodal Decoders, ensures efficient training. (c) Efficient Structure: As illustrated in Fig.4, we design a Unified Decoder Projector to align the LLM with multimodal Decoders, instead of using multiple projectors in current MLLMs like NExT-GPT. \bullet Any-to-Many Instruction Template: To enable the LLM to understand multimodal instructions and produce many-modal signal prompts, thereby achieving accurate Any-to-Many Modalities Generation, we design an Any-to-Many Instruction Template applying the proposed Modality-wise Grouping rule. An example of the signal prompts is presented in Fig.3. Then, we constructed a novel Text-formatted Many-Modal (TMM) dataset to train the Spider model, enabling it to learn the X-to-Xs capability, i.e., to achieve Any-to-Many Modalities Generation. Existing datasets are mostly in the form of ’Text + X’, which does not satisfy the X-to-Xs capability. NextGPT has constructed multimodal multi-round dialogue datasets, but each response in the dialogue is still in the form of ’Text + X’. Therefore, we need to construct a new TMM dataset to achieve the X-to-Xs capability. In the TMM dataset, the input is in the form of ’Text + X’, while the output is in the form of Text-formatted Xs (TXs), that is text, containing many-modal signal prompts (where an example is presented in Fig.3). There is a complete example of a Text-formatted Xs output: ’Here is the generated image of a dog: <IMAGE> dog [{IMAGE}_{0}]</IMAGE>, and the dog’s bark: <AUDIO> dog’s bark [{AUDIO}_{0}]</AUDIO>’. Eventually, the TMM dataset contains three types of datasets for different usage in training: T-to-TXs dataset for T-to-Xs capability finetuning, X-to-TXs dataset for X-to-Xs capability finetuning, and T-to-TXs instruction dataset for T-to-Xs instruction finetuning. Finally, we use the Spider model well-trained on the TMM (X-to-TXs) dataset to generate a new pseudo X-to-Xs dataset. This is a first-ever X-to-Xs many-modal dataset for the Any-to-Many Modalities Generation task, providing rich data support for future research. The output form of TMM (X-to-TXs) dataset is TXs (i.e., text only) without diverse modalities, while the pseudo X-to-Xs dataset contains arbitrary combination modalities. With TMM (X-to-TXs) dataset, our Spider is able to perform X-to-Xs generation, due to no need to train the multimodal Decoders. With the pseudo X-to-Xs dataset, the multimodal Decoders can be end-to-end fine-turning with LLM if needed in future work, due to having the ground truth modalities to supervise the Decoders. More details of the pseudo X-to-Xs dataset are described in APPENDIX. In summary, our Contributions are listed below: \bullet Beyond the Any-to-Any Modality Generation paradigm, we introduce a novel Any-to-Many Modalities Generation paradigm that enables each response to contain “Text + Xs”. \bullet We propose a novel efficient AMMG framework named Spider, which can generate arbitrary combinations and quantities of modalities. To achieve this, Spider integrates a Base Model, a novel Efficient Decoders-Controller, and a designed Any-to-Many Instruction Template. \bullet We propose a novel Efficient Decoders-Controller that enables the LLM to efficiently schedule and control multiple task decoders to generate many-modal contents. \bullet We design a novel Any-to-Many Instruction Template, which enables the LLM to produce many-modal signal prompts, thereby achieving accurate AMMG. \bullet We construct a novel Text-formatted Many-Modal (TMM) dataset to trian Spider, enabling it to learn the X-to-Xs capability, i.e., to achieve AMMG. \bullet A new pseudo X-to-Xs dataset is generated by the well-trained Spider model, which is a first-ever X-to-Xs many-modal dataset, providing rich data support for future research on the AMMG task."
https://arxiv.org/html/2411.09435v1,"ReMP: Reusable Motion Prior for Multi-domain
3D Human Pose Estimation and Motion Inbetweening","We present Reusable Motion prior (ReMP), an effective motion prior that can accurately track the temporal evolution of motion in various downstream tasks. Inspired by the success of foundation models, we argue that a robust spatio-temporal motion prior can encapsulate underlying 3D dynamics applicable to various sensor modalities. We learn the rich motion prior from a sequence of complete parametric models of posed human body shape. Our prior can easily estimate poses in missing frames or noisy measurements despite significant occlusion by employing a temporal attention mechanism. More interestingly, our prior can guide the system with incomplete and challenging input measurements to quickly extract critical information to estimate the sequence of poses, significantly improving the training efficiency for mesh sequence recovery. ReMP consistently outperforms the baseline method on diverse and practical 3D motion data, including depth point clouds, LiDAR scans, and IMU sensor data. Project page is available in https://hojunjang17.github.io/ReMP.","Figure 1: We extract rich motion priors from the large-scale motion dataset and reuse them for various applications, such as 3D human pose estimation and motion inbetweening. Foundation models have recently demonstrated the power of large-scale datasets, demonstrating innovative results in diverse downstream tasks. However, compared to natural language processing [6, 2, 28] (trained with 350 GB-45 TB of data), or image processing [31, 4, 19] (trained with 10-400 million images), the counterpart of human motion has remained less explored. Although we may not be able to collect an internet scale of data with accurate human motion, we argue that existing motion data, such as AMASS [26] (\sim 11,000 motions), are sufficient to learn a strong motion prior. Compared to the possible space of image pixels or arrangement of words, the space of possible human body poses is relatively more constrained and can be successfully modeled with a handful set of parameters [23]. If we consider the temporal sequence of motion, only a subset of pose sequences is physically plausible and realistic, confined by the human body’s skeletal structure and natural capabilities. Our prior deliberately focuses on accurately reconstructing human mesh sequences with 3D dynamic information. The prior knowledge can be adapted to other modalities and various downstream tasks that require the capture of 3D motion. In contrast to the abundant literature that estimates poses in 2D images or video inputs [13, 33, 25, 42, 9], our prior is composed of complete 3D configurations that are free from projective distortion, occlusion, or appearance changes. We also examine a temporal sequence of motion instead of estimating frame-wise poses, which should incorporate temporal context, increasing the robustness of estimation in various adversaries. The 3D action space with temporal context can better represent human intents and gestures or temporally predict subsequent motions, enabling us to develop applications that accurately capture human motion or provide necessary services. In this paper, we obtain a Reusable Motion Prior (ReMP), which contains a tight correlation in 3D temporal sequences from existing datasets [26]. We take a sequence of motion and use a temporal transformer followed by a variational autoencoder (VAE) architecture to obtain a latent distribution. The comprehensive parameters provide the geometric variations within the movement, while the transformer applies attention to temporal dependencies. Our training formulation is specifically designed to capture fine-grained motion variations within the sequence of motion. Specifically, we adapt continuous latent representation to maximize expressivity and maintain a temporal sequence of latent embeddings that correspond to individual time stamps for the encoded sequence. We apply random masking in the transformer during training to promote modeling temporal context. We also incorporate effective input encodings, such as 6D rotation representation, incremental translation, and velocities, and allow the neural network to comprehend detailed dynamics. Compared to generative priors, our latent space can accurately track the full 3D mesh of human poses. Once we obtain a comprehensive prior from complete 3D data, we can reuse the prior to extract essential information from various measurement characteristics. We propose a distillation method that adapts the prior to estimate plausible 3D motion even under noisy, incomplete data or fill the in-between motions given sparse poses. Even when the data is occluded or sparse, our motion prior encourages the network to detect critical features and quickly train the network to perform motion estimation. Our work can also regress motion parameters in other modalities that measure dynamic human movements, such as IMU, and show robust performance to estimate accurate movement. The aid of pretrained motion prior additionally enhances efficiency and results in better performance than the baselines in smaller training datasets. In summary, our contributions are threefold: • We introduce ReMP, a novel framework for learning reusable motion prior from large-scale 3D motion dataset, which significantly enhances accuracy and robustness in human pose estimation and motion inbetweening tasks. • We demonstrate an effective formulation for 3D motion prior that can accurately encode dynamic dependencies inherent in human motion sequences that can increase the data usage efficiency. • We present remapping techniques of our pretrained motion prior to different measurements or various adversaries, demonstrating its versatility and efficiency across diverse motion capture modalities."
https://arxiv.org/html/2411.09434v1,Mediffusion: Joint Diffusion for Self-Explainable Semi-Supervised Classification and Medical Image Generation,"We introduce Mediffusion – a new method for semi-supervised learning with explainable classification based on a joint diffusion model. The medical imaging domain faces unique challenges due to scarce data labelling – insufficient for standard training, and critical nature of the applications that require high performance, confidence, and explainability of the models. In this work, we propose to tackle those challenges with a single model that combines standard classification with a diffusion-based generative task in a single shared parametrisation. By sharing representations, our model effectively learns from both labeled and unlabeled data while at the same time providing accurate explanations through counterfactual examples. In our experiments, we show that our Mediffusion achieves results comparable to recent semi-supervised methods while providing more reliable and precise explanations.","Deep learning methods achieve remarkable performance across various computer vision domains, including medical image analysis [6]. However, this field faces unique challenges compared to other vision tasks due to the scarcity of labeled data and the need for methods explainability [10, 9]. Annotating medical images requires specialized expert knowledge, making the labeling process costly and time-consuming. This often results in datasets that are insufficient in size for effective machine learning training. Moreover, the nature of healthcare imaging applications often demands that the final solutions be transparent and interpretable, as decisions made on their basis can have severe consequences [9]. In this work, we propose to tackle those two problems with Denoising Diffusion Probabilistic Models (DDPM) [48, 17]. Applications based on those methods have already established new state-of-the-art in many domains such as image [11], video [16], music [31] or speech [42] generation. In the field of medical imaging, the same methods are used for synthetic image generation [24] or their reconstruction [61, 22]. Figure 1: Mediffusion training and capabilities. Our proposed method utilizes both labeled and unlabeled data samples to build joint representations suitable for generative and discriminative tasks. We evaluate our method in 3 tasks (1) semi-supervised classification (2) inherent visual explainability of classifier decision (3) synthesising new pseudo-labeled data samples. Nevertheless, apart from the astonishing performance in synthesizing new data samples, DDPMs can also be used as building blocks for other downstream tasks such as classification [63], image segmentation [3] or correspondence [34], with some extensions to explainable methods [1]. In this work, we propose to benefit from the representations learned by diffusion models in the medical domain. In particular, we introduce Mediffusion – a joint diffusion model that uses a shared parametrization in the form of UNet [46] model to solve both generative and discriminative tasks. We show how generative task with a diffusion objective can improve the performance of the model trained in the semi-supervised regime. Moreover, thanks to the shared features between the discriminative and generative models, we can provide accurate explanations for the decisions of the classifier by generating counterfactual examples. Such explanations are crucial in the application of ML methods in medical domain, where the decision of whether to use a model or not is often based on the trust in the system. In our experiments, we show that Mediffusion achieves performance comparable to recent state-of-the-art semi-supervised techniques while at the same time bringing accurate explainability and the possibility for sampling synthetic examples. The contributions of this work can be summarized as follows: • We introduce Mediffusion – a new method for joint medical data modeling with shared parametrization between generative and discriminative tasks. • We show how we can improve the performance of the medical classifier in a semi-supervised setup through the diffusion-based generative task. • We present the method for providing explanations to the classifier decisions through counterfactual generations."
https://arxiv.org/html/2411.09420v1,"SAG-ViT: A Scale-Aware, High-Fidelity Patching Approach with Graph Attention for Vision Transformers","Image classification is a computer vision task where a model analyzes an image to categorize it into a specific label. Vision Transformers (ViT) improve this task by leveraging self-attention to capture complex patterns and long-range relationships between image patches. However, a key challenge for ViTs is efficiently incorporating multi-scale feature representations, which is inherent in CNNs through their hierarchical structure. In this paper, we introduce the Scale-Aware Graph Attention Vision Transformer (SAG-ViT), a novel framework that addresses this challenge by integrating multi-scale features. Using EfficientNet as a backbone, the model extracts multi-scale feature maps, which are divided into patches to preserve semantic information. These patches are organized into a graph based on spatial and feature similarities, with a Graph Attention Network (GAT) refining the node embeddings. Finally, a Transformer encoder captures long-range dependencies and complex interactions. The SAG-ViT is evaluated on benchmark datasets, demonstrating its effectiveness in enhancing image classification performance.","The field of image classification has experienced significant advancements with the introduction of deep learning architectures. CNNs have long been the foundation for image classification tasks due to their proficiency in capturing local spatial hierarchies through convolutional operations [9]. However, their inherent limitations in modeling long-range dependencies restrict their ability to fully exploit global contextual information within images [12]. The introduction of Vision Transformers (ViT) [18, 4] has opened new avenues by leveraging self-attention mechanisms to model global relationships within images. ViTs treat images as sequences of patches (tokens) and have demonstrated competitive performance compared to traditional CNNs. Despite their success, ViTs often require large-scale datasets for effective training and may overlook fine-grained local details due to their fixed-size patch tokenization [17]. Recent research has highlighted the importance of multi-scale feature representations in enhancing ViTs’ performance across various vision tasks [1]. Multi-scale approaches enable models to capture objects and patterns of varying sizes, providing a more comprehensive understanding of the image content. While CNNs inherently capture multi-scale features through hierarchical layers, integrating this capability efficiently into Transformer-based models remains a challenge. To handle this challenge, we propose a novel Transformer-based framework called Scale-Aware Graph Attention Vision Transformer (SAG-ViT). Our model begins by extracting rich, multi-scale feature maps from input images using a pre-trained EfficientNet backbone [16]. We then divide these feature maps into patches, preserving high-level semantic information and reducing information loss compared to raw image patching. We then construct graphs where each node represents a feature map patch, and edges are established based on spatial adjacency and feature similarity using a k-connectivity scheme. This graph captures both local and global relationships among image regions. A Graph Attention Network (GAT) [19, 24] processes the graph, dynamically focusing on the most relevant patches. The enriched node embeddings are then passed through a Transformer encoder, which captures long-range dependencies and complex interactions. Our contributions are summarized as follows: • We introduce a patching mechanism that operates on CNN-derived feature maps, retaining rich semantic information and efficiently capturing multi-scale features. • A k-connectivity and similarity-based edge weighting scheme is developed in the proposed Transformer architecture to construct graphs that model intricate spatial relationships between patches. • We employ a GAT Network to process the information-rich graph embeddings to effectively model both local and global dependencies within images. • We validate our method on multiple benchmark datasets across different domains, demonstrating higher performance compared to other transformer-based approaches. The remainder of this paper is organized as follows: Section 2 reviews related work on graph transformers, attention mechanisms, multi-scale feature embedding, and their integration in image classification. Section 3 details our proposed method, including the architecture and graph construction process. Section 4 presents the experimental setup, datasets, and evaluation metrics. Section 5 discusses the results, and Section 6 concludes the paper."
https://arxiv.org/html/2411.09413v1,SCRIPT-CENTRIC BEHAVIOR UNDERSTANDING FOR ASSISTED AUTISM SPECTRUM DISORDER DIAGNOSIS,"Observing and analyzing children’s social behaviors is crucial for the early diagnosis of Autism Spectrum Disorders (ASD). This work focuses on automatically detecting ASD using computer vision techniques and large language models (LLMs). Existing methods typically rely on supervised learning. However, the scarcity of ASD diagnostic datasets and the lack of interpretability in diagnostic results significantly limits its clinical application. To address these challenges, we introduce a novel unsupervised approach based on script-centric behavior understanding. Our pipeline converts video content into scripts that describe the behavior of characters, leveraging the generalizability of large language models to detect ASD in a zero-shot or few-shot manner. Specifically, we propose a scripts transcription module for multimodal behavior data textualization and a domain prompts module to bridge LLMs. Our method achieves an accuracy of 92.00% in diagnosing ASD in children with an average age of 24 months, surpassing the performance of supervised learning methods by 3.58% absolutely. Extensive experiments confirm the effectiveness of our approach and suggest its potential for advancing ASD research through LLMs.","ASD, characterized by deficits in social communication and the presence of restricted, repetitive behaviors or interests, is a neurodevelopmental disorder affecting approximately 2.3% of children and approximately 2.2% of adults [1]. Individuals with ASD often experience additional developmental, behavioral, and mental health challenges, which can impose a lifelong burden on families [2]. Experts generally agree that special training and therapy for ASD treatment should begin as early as possible [3], making early and accurate diagnosis critically important. Traditional ASD diagnostic methods include the Autism Diagnostic Interview (ADI) [4], the Autism Diagnostic Observation Schedule (ADOS), and ADOS-2 [5]. These methods require an experienced physician to spend amount of considerable time on each assessment. Furthermore, diagnosis relies on specialized scales and is heavily dependent on the physician’s subjective judgment and clinical experience. Consequently, the promotion of traditional ASD diagnostic methods is limited, particularly in regions with underdeveloped healthcare resources. Many previous studies have demonstrated significant potential in assisted ASD diagnosis using sensor technology [6] and artificial intelligence techniques [7]. We focus on automatically detecting ASD from audio-visual behavior data. Mainstream methods can be divided into two categories: the first is based on behavioral signal. These methods [8, 9, 10] analyze behavior-related features from paradigmatic videos using machine learning techniques, such as body movements, head movements, and eye patterns. The second category [11, 12, 13] involves deep learning methods, which directly train raw video data to predict labels. While both types of methods perform well on their respective datasets through supervised learning, the scarcity of ASD data limits their accuracy in practice. Moreover, these methods only provide a binary prediction for the diagnostic outcome and lack interpretive explanation. Figure 1: The overview of our proposed Script-Centeric Behavior Understanding(SCBD). Behavior Transcription Module converts audio-visual data into behavioral logs using multiple well-trained behavior signal processing models. Scipt Transcription Module textualizes Behavior Logs in steam and integrate domain prompt. Large Language Models are used to understand and anwser script content. To address the challenges of limited data and interpretability, we propose a novel approach to perform assisted ASD diagnosis from audio-visual behavior data. The recent rapid development of LLMs [14, 15, 16, 17, 18] has demonstrated their extraordinary capabilities, including understanding, reasoning, and question answering. These capabilities allow LLMs to perform well a wide range of downstream tasks in a zero-shot manner. Because LLMs have absorbed lots of human knowledge from massive textual data, they only require very little domain knowledge, reducing the need to collect clinical data from ASD patients. Furthermore, their question-answering abilities enable the interpretation of diagnostic results. However, LLMs currently specialize in text data or text-image data, understanding domain-specific audio-visual behavior signals remains challenging for two main reasons: 1) There is a significant gap between different modalities[19, 20], and multimodal LLMs only do not perform well on real audio-visual data in complex scenarios[21, 22]. 2) Due to privacy protection, there is very limited audio-visual behavior data from ASD children[23]. As a result, this paper introduces a novel methodology that utilize LLMs to detect ASD and explain the reasons. Specifically, to bridge the gap between video and LLMs, we employ the Behavioral Transcription Module(BTM) to transform video content into human behavioral data. Subsequently, we design the Script Transcription Module (STM) to process this behavioral log, generating video script descriptions based on characters’ behavioral responses, expressions, languages and the timing of their occurrences. To enhance diagnostic accuracy, we created the Domain Prompts Module (DPM) to incorporate domain knowledge and produce domain-adaptive behavior descriptions. Ultimately, we leverage a pretrained LLM to detect ASD and provide explanations by answering questions. Our main contributions can be summarized as follows: \bullet To the best of our knowledge, our approach is the first to use LLMs for detecting ASD from audio-visual data. \bullet To accurately describe the behavior data and leverage prior knowledge, we propose STM and DPM respectively. \bullet Extensive experimental results and ablation studies verify the effectiveness of our approach, making the exploration of LLMs on ASD detection more promising."
https://arxiv.org/html/2411.09411v1,Building Height Estimation Using Shadow Length in Satellite Imagery,"Estimating building height from satellite imagery poses significant challenges, especially when monocular images are employed, resulting in a loss of essential 3D information during imaging. This loss of spatial depth further complicates the height estimation process. We addressed this issue by using shadow length as an additional cue to compensate for the loss of building height estimation using single-view imagery. We proposed a novel method that first localized a building and its shadow in the given satellite image. After localization, the shadow length is estimated using a regression model. To estimate the final height of each building, we utilize the principles of photogrammetry, specifically considering the relationship between the solar elevation angle, the vertical edge length of the building, and the length of the building’s shadow. For the localization of buildings in our model, we utilized a modified YOLOv7 detector, and to regress the shadow length for each building we utilized the ResNet18 as backbone architecture. Finally, we estimated the associated building height using solar elevation with shadow length through analytical formulation. We evaluated our method on 42 different cities and the results showed that the proposed framework surpasses the state-of-the-art methods with a suitable margin. Our code is available at: https://github.com/nullptr-code/building-height-model.git.","Accelerating the pace of urbanization results in urban sprawl and creates immense pressure on urban land use and resources. More than 68\% of the world population will live in urban areas by the year 2050 [1]. Integrated planning and management regarding the development of urban areas is one of the biggest challenges in the world today. Vertical expansion of urban areas can be a solution to shelter the growing urban population [2]. But this results in high-rise buildings that have already been established in big cities. Building height estimation is essential urban geographic information for the assessment of land changes [3], smart cities modeling [4], and autonomous driving [5]. The estimation of building heights is always been challenging in the field of remote sensing [6]. Building heights can be estimated using light detection and ranging (LiDAR) [7, 8] and synthetic aperture radar (SAR) [9, 10]. Fig. 1: Overview of the proposed framework for building height estimation using shadow length. Fig. 2: Sample image showing the screenshot of annotation tools. Bounding boxes along with the annotations for vertical edge and shadow length for the encompassing building are shown for each bounding box. Soergel et al. proposed a segmentation-based building height estimation method using SAR imagery. Wegner et al. proposed building height estimation using a pair of SAR and single optical imagery [11]. Brunner et al. proposed SAR based building height estimation method and evaluated over flat and gable building roofs [12]. These methods performed building height estimation with high efficiency while utilizing multi-sensor data which is constrained by high computational resources and acquisition cost [13]. The above-mentioned difficulty can be handled using high-resolution optical satellite imagery. Many building heights can be estimated with optical satellite imagery using deep learning methods [14, 15]. But deep learning models perform end-to-end mapping from optical satellite image to height map. Here our goal is to utilize the mathematical models based on shadow length along with deep learning models. Unlike, data-driven end-to-end based height estimation [16], there are three major formulations to compute the height of an object using imagery [17]. This includes i) radial displacement method, ii) parallax method, and iii) shadow method. Qi et al. [18] utilized shadow length and vertical edge length-based celestial geometry formulation using Google Earth Imagery. Qi et al. [18] performed an evaluation over a local scale for 21 buildings in Lin’an. Whereas, we performed experimentation over densely populated cities in China. Secondly, we combined the detection model and regression networks to get the final building heights. We designed an annotation tool to mark the shadow lengths of buildings in images and passed that shadow length to a mathematical model to get the final building height. Our key contributions to this study are listed as follows: • We developed a localization and regression-based novel frame for building height estimation using monocular satellite imagery. • We developed a data annotation tool that annotates buildings and shadow lengths. We have developed a new dataset for the remote sensing community. • We also incorporated mathematical formulation along with our regression model to get improved building height estimation results. • When we evaluated our proposed framework on densely populated 42 cities of China, our model surpassed the performance of state-of-the-art methods. The remainder is organized as Section 2 regarding the dataset, Section 3 describes the detailed methodology, results are reported in Section 4, and in the end, the conclusion is drawn in Section 5."
https://arxiv.org/html/2411.09387v1,Instruction-Driven Fusion of Infrared-Visible Images: Tailoring for Diverse Downstream Tasks,"The primary value of infrared and visible image fusion technology lies in applying the fusion results to downstream tasks. However, existing methods face challenges such as increased training complexity and significantly compromised performance of individual tasks when addressing multiple downstream tasks simultaneously. To tackle this, we propose Task-Oriented Adaptive Regulation (T-OAR), an adaptive mechanism specifically designed for multi-task environments. Additionally, we introduce the Task-related Dynamic Prompt Injection (T-DPI) module, which generates task-specific dynamic prompts from user-input text instructions and integrates them into target representations. This guides the feature extraction module to produce representations that are more closely aligned with the specific requirements of downstream tasks. By incorporating the T-DPI module into the T-OAR framework, our approach generates fusion images tailored to task-specific requirements without the need for separate training or task-specific weights. This not only reduces computational costs but also enhances adaptability and performance across multiple tasks. Experimental results show that our method excels in object detection, semantic segmentation, and salient object detection, demonstrating its strong adaptability, flexibility, and task specificity. This provides an efficient solution for image fusion in multi-task environments, highlighting the technology’s potential across diverse applications.","1 Introduction Figure 1: Comparison of the proposed method with existing approaches. Existing methods (a) perform well only on the specific downstream tasks for which they are trained, thereby limiting their multi-task adaptability. In contrast, our method (b) uses T-OAR with task instructions to fine-tune the fusion network, thereby simultaneously meeting the requirements of multiple downstream tasks without retraining. Infrared images capture thermal radiation from objects, making them suitable for target detection and recognition in nighttime or adverse weather conditions such as haze, rain, and snow. However, they generally have lower resolution and lack color. Visible images, on the other hand, provide rich color, texture, and detail but suffer from quality loss in low-light or adverse weather conditions. Infrared-visible (IR-VIS) image fusion effectively combines the strengths of both, generating a fused image that retains thermal information from infrared images while incorporating the color and texture details of visible images. This provides a more comprehensive and accurate scene representation. Consequently, IR-VIS image fusion has broad applications in military, aerospace, environmental monitoring, medicine, and other fields, leading to the development of various fusion methods [15, 14, 12, 21, 40, 45, 16, 13, 11]. For IR-VIS image fusion, the key value of this technology is its application to downstream visual tasks. As a result, existing methods have incorporated these tasks into fusion network training. These methods can be categorized into two types based on how they integrate downstream tasks: downstream task-driven fusion methods [30, 19, 35] and downstream task-embedded fusion methods [43, 29, 32, 20]. The former directly inputs the fusion results into downstream task networks and adjusts the fusion network based on the task-specific loss. Notable examples include SeAFusion [30], TarDAL [19], and IRFS [35], with common tasks such as object detection (OD) [7, 17], semantic segmentation (SS) [24, 36], and salient object detection (SOD) [2, 27]. The latter integrates the intermediate features of downstream task networks into the fusion network, refining them through task-specific adjustments or dual-task interactions to improve fusion quality. While both approaches enhance fusion quality and task performance, it remains challenging to meet the requirements of multiple downstream tasks simultaneously without retraining the fusion network (as shown in Figure 1(a)). Introducing multiple tasks into the training of a fusion model can increase its complexity and potentially compromise performance on specific tasks. Faced with the above issues, one might ask: Can the output features of a fusion model be adaptively adjusted based on downstream tasks, enabling the fusion results to meet specific task requirements? If this vision is realized, retraining the fusion network for different tasks would no longer be necessary, as the output features could be dynamically modified to align with the task-specific demands. To address this challenge, as shown in Figure 1(b), we explore a method for constructing a fusion model that adapts to multiple downstream tasks without significantly compromising performance on any single task, offering new solutions for multi-task image fusion applications. Specifically, we introduce downstream task-related text instructions into the feature extraction process and develop a Task-Oriented Adaptive Regulation (T-OAR) mechanism. In our network architecture, T-OAR comprises multiple Task-related Dynamic Prompt Injection (T-DPI) modules. T-DPI adaptively generates dynamic prompts based on user-input task instructions and injects these prompts into features requiring fine-tuning, guiding them to meet specific task requirements. Compared with existing methods, the proposed approach offers a significant advantage: it dynamically fine-tunes features based on task instructions without the need to retrain the network, thereby efficiently addressing the diverse needs of multiple tasks. Structurally simple yet powerful, this method demonstrates superior performance across various tasks in experimental evaluations, confirming notable improvements in adaptability, flexibility, and task specificity. Overall, this paper presents a new solution for feature extraction and fusion in multi-task environments, paving the way for IR-VIS image fusion applications in diverse scenarios. With T-OAR and embedded T-DPI modules, the constructed network adapts to different task requirements with lower computational costs and improved performance. The main contributions of this paper are summarized as follows: • New Requirements for IR-VIS Image Fusion. This paper highlights the importance of generating fusion results tailored to the specific needs of downstream tasks without requiring retraining of the fusion network. The method we propose not only broadens the scope of IR-VIS image fusion research but also enhances its practical applicability, expanding the application range of fusion techniques. • Adaptive Adjustment Mechanism with Dynamic Prompt Injection. We introduce a T-OAR mechanism that includes multiple T-DPI modules. This mechanism generates task-specific prompts based on user instructions and integrates them into the feature extraction process, aligning outputs precisely with task requirements and enabling flexible adaptation across diverse scenarios. • Flexibility Without Retraining and Efficient Structural Design. The proposed method distinguishes itself by adapting outputs to task instructions without network retraining, allowing efficient adaptation across a variety of downstream tasks. This method is simple yet effective, demonstrating excellent performance across multiple downstream task benchmarks. Figure 2: Overview of the proposed method. The proposed method integrates downstream task instruction features obtained from LLaMA into T-DPI, which generates dynamic prompts closely aligned with the specific task. This enables the method to flexibly adjust the output features of VI-E and IR-E based on input instructions, ensuring that these features meet the specific requirements of downstream tasks."
https://arxiv.org/html/2411.09371v1,DSCformer: A Dual-Branch Network Integrating Enhanced Dynamic Snake Convolution and SegFormer for Crack Segmentation,"In construction quality monitoring, accurately detecting and segmenting cracks in concrete structures is paramount for safety and maintenance. Current convolutional neural networks (CNNs) have demonstrated strong performance in crack segmentation tasks, yet they often struggle with complex backgrounds and fail to capture fine-grained tubular structures fully. In contrast, Transformers excel at capturing global context but lack precision in detailed feature extraction. We introduce DSCformer, a novel hybrid model that integrates an enhanced Dynamic Snake Convolution (DSConv) with a Transformer architecture for crack segmentation to address these challenges. Our key contributions include the enhanced DSConv through a pyramid kernel for adaptive offset computation and a simultaneous bi-directional learnable offset iteration, significantly improving the model’s performance to capture intricate crack patterns. Additionally, we propose a Weighted Convolutional Attention Module (WCAM), which refines channel attention, allowing for more precise and adaptive feature attention. We evaluate DSCformer on the Crack3238 and FIND datasets, achieving IoUs of 59.22% and 87.24%, respectively. The experimental results suggest that our DSCformer outperforms state-of-the-art methods across different datasets.","Monitoring the quality of concrete structures is crucial in industrial settings. With the aging of concrete buildings, such as pavement, building exteriors, bridges, etc., cracks inevitably appear, resulting in significant safety hazards (Lee and Su 2024). Cracks in road surfaces potentially expand into large potholes during a rainy night, and bridge cracks may lead to structural collapse. In recent years, such catastrophic accidents resulting in significant casualties have occurred multiple times in countries like China and the United States (Zou et al. 2018). Therefore, regular crack inspection is very important, in which crack segmentation and subsequent quantitative assessment provide valuable information for the health analysis of concrete structures. Numerous researchers focus on developing more accurate crack segmentation methods. Figure 1: Overview of the Enhanced Dynamic Snake Convolution. A pyramid kernel is employed to obtain offsets on the input feature map. These offsets are iterated into chains through bi-direction, which is then convolved with the input feature map to gain the output feature map. Traditional segmentation methods struggle with robustness due to background noise, such as lighting and graffiti. However, deep learning models have shown exceptional performance due to their accuracy, robustness and generalization. Several studies (Liu et al. 2019; Cheng et al. 2021; Zhang et al. 2022) have employed Convolutional Neural Networks (CNNs) for crack segmentation. CNNs leverage the principle of local connectivity, where convolutional kernels focus on specific regions of an image. These kernels are particularly adept at detecting edges and textures, endowing the networks with strong translational invariance, which is advantageous for crack segmentation tasks. In addition, Convolutional attention mechanisms like Squeeze-and-Excitation (SE) (Hu, Shen, and Sun 2018) and the Convolutional Block Attention Module (CBAM) (Woo et al. 2018) make the networks more focused on key information. Nevertheless, CNNs still struggle with complex backgrounds due to their limited receptive fields, leading to sub-optimal performance in such scenarios. Figure 2: Illustration of the proposed DSCformer. We employ the enhanced dynamic snake convolution (DSConv) and Segformer as dual-branch encoder and use Weighted Convolutional Attention Module (WCAM) and Spatial Attention Module (SAM) to adjust the convolution focus. In contrast, Transformer networks offer a novel perspective by utilizing embeddings and self-attention mechanisms to operate on the entire input sequence simultaneously. This enables them to capture global information and model long-range dependencies effectively, which is beneficial for segmenting cracks in complex backgrounds (Shamsabadi et al. 2022; Guo et al. 2023). However, Transformers often lack precision in identifying fine details. The hybrid approach of combining CNNs and Transformers holds significant potential by leveraging both local detail extraction and global context modeling (Gao, Zhou, and Metaxas 2021; Liu et al. 2021a; Wang et al. 2024). However, the above methods often overlook the specific structure of crack segmentation when selecting convolutional and Transformer components, leading to inadequate fusion outcomes. Additionally, the integration of CNNs and Transformers can introduce complex training fluctuation, making it difficult to fine-tune the model to effectively handle both local details and global structures. Consequently, while these hybrid models have demonstrated performance improvements, they still requires further refinement to reach its potential. In this context, Dynamic Snake Convolution (DSConv)(Qi et al. 2023) is very suitable for crack segmentation task. DSConv adaptively focuses on the fine and curved local features of tubular structures, enhancing geometric perception. Despite its effectiveness, the original DSConv has certain limitations in its flexibility and adaptability when dealing with complex structures, indicating room for improvement. In this paper, we introduce DSCformer, a novel crack segmentation model that combines proposed enhanced DSConv with a Transformer-based architecture. Our approach introduces two key improvements to the original DSConv: a) a pyramid kernel for offset computation, consisting of convolutional kernels of varying sizes, and b) a method for learnable offset iteration that simultaneously operates in vertical and horizontal direction. Fig. 1 shows the process for enhanced DSConv. These enhancements allow the model to capture fine crack structures more accurately. Furthermore, we have introduced the Weighted Convolutional Attention Module (WCAM), which refines channel attention by utilizing separate Multi-Layer Perceptrons for average and max pooling, aggregated through a weighted sum of learnable parameters. This modification facilitates more adaptive and precise channel attention. The SAM remains unaltered, continuing to contribute spatial focus to the network. By integrating these modifications, we have developed the DSC block, which, when stacked in sequence, forms a DSConv encoder branch. This branch is aimed at yielding fine-grained and tubular topologically attentive feature maps. Additionally, we integrate a pre-trained SegFormer (Xie et al. 2021) encoder branch to capture long-range dependencies. The features extracted by these two branches are fused and progressively upsampled to generate the final crack segmentation prediction map. The principal contributions of this study are encapsulated as follows: • We propose an enhanced DSConv, incorporating a pyramid kernel for offset calculation and a simultaneous bi-direction learnable offset iteration, significantly improving ability to capture crack structure. • We propose WCAM, which refines the channel attention mechanism, improving its adaptability and precision. • We propose DSCformer, a hybrid model combining enhanced DSConv and SegFormer architectures, outperforming state-of-the-art crack segmentation methods across two public datasets."
https://arxiv.org/html/2411.09361v1,Time-to-Event Pretraining for 3D Medical Imaging,"With the rise of medical foundation models and the growing availability of imaging data, scalable pretraining techniques offer a promising way to identify imaging biomarkers predictive of future disease risk. While current self-supervised methods for 3D medical imaging models capture local structural features like organ morphology, they fail to link pixel biomarkers with long-term health outcomes due to a missing context problem. Current approaches lack the temporal context necessary to identify biomarkers correlated with disease progression, as they rely on supervision derived only from images and concurrent text descriptions. To address this, we introduce time-to-event pretraining, a pretraining framework for 3D medical imaging models that leverages large-scale temporal supervision from paired, longitudinal electronic health records (EHRs). Using a dataset of 18,945 CT scans (4.2 million 2D images) and time-to-event distributions across thousands of EHR-derived tasks, our method improves outcome prediction, achieving an average AUROC increase of 23.7% and a 29.4% gain in Harrell’s C-index across 8 benchmark tasks. Importantly, these gains are achieved without sacrificing diagnostic classification performance. This study lays the foundation for integrating longitudinal EHR and 3D imaging data to advance clinical risk prediction.","Foundation models for medical imaging have the potential to transform healthcare by assisting doctors in complex clinical decision making (Saab et al., 2024; Sox et al., 2024) and identifying novel pixel biomarkers predictive of future disease risk (Pai et al., 2024; Sriram et al., 2021). Such models rely on self-supervised learning (SSL) for obtaining supervision at the scale required to train on growing collections of 3D data (e.g., CT scans, MRIs). SSL captures local structural features by leveraging pretraining signal directly from images or cross-modal pairs (e.g., images and their text descriptions) (Zhang et al., 2022). While excelling at segmentation tasks and diagnostic classification of pathologies (Pinto-Coelho, 2023), SSL fails to learn prognostic biomarkers because the current pretraining regimens suffer from a missing context problem (see Figure 1). This issue arises when supervision sources are restricted to narrow time windows around the image, thus excluding long-term temporal patterns that are correlated with disease progression, which limits a model’s ability to identify prognostic biomarkers. These long-term temporal patterns, which we refer to as longitudinal context, are readily available in a patient’s electronic health record (EHR) which is routinely used by clinicians to guide interpretation of images and inform treatment planning (Leslie et al., 2000; Holste et al., 2024). Longitudinal EHRs contain temporal information about the progression of disease, as well as years of patients’ health outcomes. However, the full breadth of these outcome data, in terms of temporal structure and task diversity, is rarely used as a source of supervision when training image foundation models. Current approaches for training 3D imaging models typically restrict labels to diagnosis codes sourced from the same or nearby temporal context as the image and its textual description (Blankemeier et al., 2024). Because EHR data is readily available, it offers an untapped resource for large-scale pretraining of medical image models in a manner that uses long-term temporal context. More generally, image foundation models must reflect the settings in which they will be used, which is to assist in prognosis in the clinic (Negro-Calduch et al., 2021; Yala & Hughes, 2023). However, performing effective risk estimation using imaging models involves navigating several challenges. Developing such models requires capturing correlations between pixels and outcomes spanning years, which is difficult with current SSL methods. Direct approaches to identifying pixel-level biomarkers, such as sequential image capturing (Lu et al., 2021; Bera et al., 2022), are difficult to scale for collecting large, high-quality datasets. Moreover, doing risk estimation requires addressing right censoring, where the outcome of interest remains unobserved by the study’s end. Naively excluding censored patients introduces bias and reduces the available training data. In medicine, time-to-event (TTE) modeling (also known as survival modeling) is commonly used to estimate future risk of an outcome at a specific time point conditioned on feature representations. Although TTE models offer many theoretical advantages, including the ability to estimate instantaneous risk at any given time point (Collett, 2023) and naturally handling right censoring (Kleinbaum & Klein, 1996), their use in image pretraining remains underexplored. Prior deep learning studies exploring TTE modeling in medical imaging have been restricted to small-scale, single-task applications, typically using 2D, end-to-end models (Zhu et al., 2016; Shu et al., 2021; Lu et al., 2021). Large-scale TTE pretraining for 3D imaging has not yet been investigated, likely because multimodal medical datasets linking 3D images with longitudinal EHR data have only recently become available (Huang et al., 2024). In this work, we propose time-to-event pretraining for 3D medical imaging models as a way to address the missing context problem. Our central claim is that temporal supervision, defined by TTE distributions sourced from longitudinal EHRs, provides a readily available, scalable source of contextual information for pretraining that better captures prognostic pixel biomarkers. Moreover, by naturally handling right-censorship, TTE-based methods improve data efficiency and mitigate censorship bias. Our contributions are as follows: • We present the first large-scale evaluation of time-to-event pretraining for 3D medical imaging encoders. We use a public dataset of 18,945 chest CT scans (equivalent to 4.2 million 2D images) linked to longitudinal EHR data containing 225M clinical events with a median follow-up time of 5 years. • Our approach converts longitudinal EHR data into a source of time-to-event supervision, thus predicting not only if a clinical event will occur but also when. This richer pretraining signal goes beyond diagnostic classification explored in prior work and enables generating many pretraining tasks (8,192 in this work) that capture the temporal event structure available in longitudinal EHR data. This choice also increase per-image label density by an average of 3 times over prior approaches. • Our approach substantially improves performance in predicting future medical outcomes, achieving on average a 23.7% increase in AUROC and a 29.4% improvement in Harrell’s C-index over baseline models for 8 benchmark tasks without negatively impacting diagnostic classification performance in 8 external tasks. Our approach also improves model calibration, measured by the Integrated Brier Score, by an average of 54%. All our experiments are conducted using public medical datasets to ensure full reproducibility. We also make all of our experiment code and pretrained model checkpoints available for download 111https://github.com/som-shahlab/tte-pretraining 222https://huggingface.co/StanfordShahLab, to contribute to the community for continued pretraining of imaging foundation models with prognosis as added benefit. Figure 1: The missing context problem in medical imaging. Existing supervision sources (red boxes) are localized to the image itself (i.e., pixel features and descriptions of those features via text) or immediate clinical context via diagnosis codes. Doing so misses future information on disease progression (black boxes), which reduces the ability to learn correlations necessary for identifying prognostic pixel biomarkers. Time-to-event pretraining provides a principled framework for incorporating the vast amount of temporal supervision available in EHR data to estimate future risk in the presence of right censorship as well as leverage a large, diverse number of clinical tasks, beyond just diagnoses, for pre-training."
https://arxiv.org/html/2411.09344v1,"Adaptively Augmented Consistency Learning: 
A Semi-supervised Segmentation Framework 
for Remote Sensing","Remote sensing (RS) involves the acquisition of data about objects or areas from a distance, primarily to monitor environmental changes, manage resources, and support planning and disaster response. A significant challenge in RS segmentation is the scarcity of high-quality labeled images due to the diversity and complexity of RS image, which makes pixel-level annotation difficult and hinders the development of effective supervised segmentation algorithms. To solve this problem, we propose Adaptively Augmented Consistency Learning (AACL), a semi-supervised segmentation framework designed to enhances RS segmentation accuracy under condictions of limited labeled data. AACL extracts additional information embedded in unlabeled images through the use of Uniform Strength Augmentation (USAug) and Adaptive CutMix (AdaCM). Evaluations across various RS datasets demonstrate that AACL achieves competitive performance in semi-supervised segmentation, showing up to a 20\% improvement in specific categories and 2\% increase in overall performance compared to state-of-the-art frameworks.","Remote Sensing (RS) technology has revolutionized the observation and analysis of Earth’s surface and atmosphere, becoming an indispensable tool with critical applications across various sectors. It is widely used in environmental monitoring [20, 7], precise agriculture [18] and urban planning [13]. In each of these fields, the ability to measure and analyze RS images with high accuracy is essential for effective management and decision-making in real world scenarios. Unlike other computer vision tasks, the effectiveness of RS technique is frequently compromised by the scarcity of high-quality labels due to the high resolution and rich information content of RS images. Accurate RS techniques heavily depends on the availability of high-quality labels to supervise the model training. To overcome the limitations imposed by the scarcity of high-quality labels, we introduce a novel semi-supervised segmentation framework, called Adaptively Augmented Consistency Learning (AACL), to enhance the performance of RS image segmentation. Figure 1: Comparison of natural image and remote sensing image. The image on the left side is from Pascal VOC dataset [4], the image on the right side is from Potsdam dataset. In AACL, two advanced methods, Uniform Strength Augmentation (USAug) and Adaptive CutMix (AdaCM), contribute to the performance improvement. USAug applies consistent but varied strong augmentation to unlabeled images, introducing discrepancies and enriching the embedded information. AdaCM tailors the application of the CutMix [29] based on the model’s maturity, dynamically applying CutMix to augment unlabeled images, further introducing discrepancies and mitigating confirmation bias [1]. The performance of AACL has been validated on multiple mainstream RS datasets, demonstrating notable improvements over existing state-of-the-art (SOTA) semi-supervised segmentation frameworks in RS. In this paper, we introduce the Adaptively Augmented Consistency Learning (AACL), a novel semi-supervised segmentation framework that significantly advances the efficacy of RS image segmentation. This method is specifically designed to address the prevalent issue of scarce high-quality labels, which hampers the accuracy of RS technology. The contributions of this work can be summarized as: • we propose Adaptively Augmented Consistency Learning (AACL), a framework designed to enhance RS by effectively leveraging unlabeled images during the training process. This framework addresses the critical challenge of label scarcity in RS, ensuring that the accuracy and reliability of measurements can be maintained with limited labeled data. • We design two novel modules, Uniform Strength Augmentation (USAug) and Adaptive CutMix (AdaCM), which significantly contribute to the performance enhancement of RS image segmentation. • AACL has been rigorously tested and validated on mainstream RS datasets, demonstrating competitive performance and surpassing previous SOTA methods by up to 20\% in specific categories and 2\% in overall performance."
https://arxiv.org/html/2411.09310v1,Exploring Zero-Shot Anomaly Detection with CLIP in Medical Imaging: Are We There Yet?,"Zero-shot anomaly detection (ZSAD) offers potential for identifying anomalies in medical imaging without task-specific training. In this paper, we evaluate CLIP-based models, originally developed for industrial tasks, on brain tumor detection using the BraTS-MET dataset. Our analysis examines their ability to detect medical-specific anomalies with no or minimal supervision, addressing the challenges posed by limited data annotation. While these models show promise in transferring general knowledge to medical tasks, their performance falls short of the precision required for clinical use. Our findings highlight the need for further adaptation before CLIP-based models can be reliably applied to medical anomaly detection.","Anomaly detection (AD) in medical imaging plays a critical role in identifying and diagnosing diseases, often detecting rare or subtle anomalies that may go unnoticed by human observers [1]. In particular, zero-shot anomaly detection (ZSAD), which aims to identify anomalies without specific training on abnormal samples, holds great promise for medical applications where obtaining labeled data is both challenging and costly. Despite the progress in anomaly detection, the majority of current models are trained on large, domain-specific datasets, which limits their applicability to novel tasks and categories—especially in the medical field, where training data encompassing real world variability are often difficult to obtain [2]. Recently, models based on Contrastive Language-Image Pretraining (CLIP) have shown remarkable success in zero- and few-shot tasks across various domains [3]. These models, trained on vast amounts of diverse, publicly available image-text pairs, excel at generalizing to unseen categories with minimal task-specific fine-tuning. However, their effectiveness in the medical domain, where anomaly detection often involves identifying subtle, domain-specific abnormalities, remains underexplored [4]. In this paper, we explore the potential of CLIP-based models for medical anomaly detection by focusing on a brain tumor detection task using the BraTS dataset [5]. While CLIP-based models have demonstrated superior performance in industrial AD tasks, it is unclear whether these models are capable of achieving similar success in the medical domain, where higher accuracy and domain specificity are critical for clinical applications. We compare the performance of several CLIP-based models on the BraTS dataset, aiming to assess whether these zero-shot models are ready for medical anomaly detection or if further domain adaptation is required. Our findings reveal that, while CLIP models show promise in transferring knowledge from general tasks to medical imaging, their performance in detecting anomalies such as brain tumors is not yet sufficient for clinical use. Therefore, significant improvements and adaptations are needed to fully harness the potential of CLIP-based models in medical anomaly detection, especially for tasks that require high sensitivity and precision."
https://arxiv.org/html/2411.09301v1,LHRS-Bot-Nova: Improved Multimodal Large Language Model for Remote Sensing Vision-Language Interpretation,"Automatically and rapidly understanding Earth’s surface is fundamental to our grasp of the living environment and informed decision-making. This underscores the need for a unified system with comprehensive capabilities in analyzing Earth’s surface to address a wide range of human needs. The emergence of multimodal large language models (MLLMs) has great potential in boosting the efficiency and convenience of intelligent Earth observation. These models can engage in human-like conversations, serve as unified platforms for understanding images, follow diverse instructions, and provide insightful feedbacks. In this study, we introduce LHRS-Bot-Nova, an MLLM specialized in understanding remote sensing (RS) images, designed to expertly perform a wide range of RS understanding tasks aligned with human instructions. LHRS-Bot-Nova features an enhanced vision encoder and a novel bridge layer, enabling efficient visual compression and better language-vision alignment. To further enhance RS-oriented vision-language alignment, we propose a large-scale RS image-caption dataset, generated through feature-guided image recaptioning. Additionally, we introduce an instruction dataset specifically designed to improve spatial recognition abilities. Extensive experiments demonstrate superior performance of LHRS-Bot-Nova across various RS image understanding tasks. We also evaluate different MLLM performances in complex RS perception and instruction following using a complicated multi-choice question evaluation benchmark, providing a reliable guide for future model selection and improvement. Data, code, and models will be available at https://github.com/NJU-LHRS/LHRS-Bot.","Interpreting remote sensing (RS) imagery and understanding multi-level features, object relationships, and their dynamic trends, play a significant role in various applications, such as urban sustainable development (Zhang et al., 2022; Wu et al., 2023; Sun et al., 2020), early-warning systems (Ravuri et al., 2021; Reichstein et al., 2024; Ravuri et al., 2021; Xu et al., 2024a), and earth surface processes (Qian et al., 2024; Zhu et al., 2022b). Artificial intelligence (AI) has revolutionized RS data analysis (Zhao et al., 2024; Ma et al., 2019; Reichstein et al., 2019; Zhang and Zhang, 2022; Zhu et al., 2017), and recent advancements in visual foundation models have further improved the efficiency and quality of interpretation of Earth’s surface using RS data (Xiong et al., 2024; Guo et al., 2024; Hong et al., 2024). However, a major drawback of visual foundation models is their need for tailored designs for specific downstream tasks, leading to fixed functions and limited generalization capabilities. Additionally, they lack the ability to interact with humans, making it difficult to fully address diverse human needs (Zhou et al., 2024). Language, as the primary medium of human communication, plays a fundamental role in facilitating interaction with machines. Large language models (LLMs), such as ChatGPT (OpenAI, 2023a), have demonstrated remarkable conversational abilities, step-by-step reasoning skills, and the capacity to serve as general-purpose task solvers (Touvron et al., 2023; Brown et al., 2020; Zhao et al., 2023). Taking a further step toward human-level AI, multimodal large language models (MLLMs) enhance LLMs with visual perception, enabling them to see and understand the world (Fei et al., 2024; Yin et al., 2023; Achiam et al., 2023). These models have demonstrated scalability and generalizability as general-purpose assistants (Li et al., 2024c), and have already shown their strong capabilities in understanding RS data for real-world application (Zhang and Wang, 2024; Tan et al., 2023). Developing specialized MLLMs for interpreting RS images offers several advantages: 1) unified modeling: MLLMs provide a versatile framework for handling a wide range of visual tasks across different granularities; 2) human-computer interaction: MLLMs can interpret human intent and incorporate auxiliary information through conversational interactions; 3) reasoning: advanced reasoning capabilities, such as chain-of-thought methods (Wei et al., 2022; Mitra et al., 2024), enable MLLMs to understand complex relations and deal with complex scenarios; and 4) enhanced multimodal task potential: pretrained on extensive and diverse datasets, MLLMs establish a robust baseline for addressing complex multimodal problems. The potential of specialized MLLMs has been widely acknowledged by the research community (Moor et al., 2023; Zhou et al., 2024; Li et al., 2024e), prompting several early efforts to develop large-scale vision-language datasets and RS-specific MLLMs (Muhtar et al., 2024; Kuckreja et al., 2024; Zhang et al., 2024b). However, we have identified three main drawbacks in current studies. 1) Lack of high-quality, large-scale image-caption dataset: High-quality vision-language pretraining datasets are crucial for developing robust multimodal models (Nguyen et al., 2022; Fang et al., 2022; Gadre et al., 2024; Xu et al., 2023). Several extensive RS image-caption datasets have been developed to enhance vision-language training (Wang et al., 2024b; Zhang et al., 2023c; Muhtar et al., 2024). However, these datasets often suffer from noisy and uninformative captions, limited semantic richness, poor sentence diversity, and an overfocus on salient objects, which undermine effective modality alignment for RS MLLMs. 2) Weaknesses in spatial recognition and hallucination tendencies: We discover that current RS MLLMs exhibit low accuracy in spatial positioning and frequently produce hallucinated responses (Bai et al., 2024) when confronted with questions beyond their capabilities. 3) Challenges in holistically evaluating MLLMs: The ability of MLLMs to solve various visual tasks serves them as versatile multi-taskers. Although they often excel in metrics for common tasks like classification, visual question answering, and visual grounding, these metrics fall short of fully reflecting the all-round capabilities of MLLMs—particularly in recognizing complex scenes, objects, attributes, spatial relationships, and, most importantly, following human instructions. In this study, we mitigate the above issues and propose LHRS-Bot-Nova, an improved RS-specialized MLLM for holistic interpreting RS images with human instructions (Muhtar et al., 2024). LHRS-Bot-Nova can respond to user instructions and achieve various RS interpretation tasks with state-of-the-art performance. To enhance RS-oriented vision-language alignment, we construct a large-scale RS image-caption dataset, LHRS-Align-Recap, by prompting an off-the-shelf multimodal captioner with RS images paired with their OpenStreetMap (OSM) features. Compared to captions generated solely by a language model using textual information (Muhtar et al., 2024), utilizing a vision-capable captioner results in richer captions with significantly enhanced language richness and improved alignment between images and captions (Table 2). It also provides more detailed descriptions, including additional recognition of geographical objects and a wider range of attributes such as location and color (Fig. 1). To enhance the model’s spatial awareness, we extend the LHRS-Instruct dataset (Muhtar et al., 2024) with conversations that primarily focus on localization and perception. Furthermore, we integrate an off-the-shelf robust visual instruction dataset that includes abundant negative samples, helping to balance the dataset and reduce the occurrence of hallucinations (Liu et al., 2023a). Considering the necessity of a vision-centric (Tong et al., 2024) design for holistic visual understanding, we scale up the vision encoder to accommodate inputs with larger resolutions. Additionally, we propose a novel bridge layer using the MoE architecture (Jiang et al., 2024) to further extend the model’s capacity, enabling lossless compression of visual information and dynamic mapping to the language domain. With enhanced instructional data and an optimized modeling architecture, LHRS-Bot-Nova showcases exceptional spatial recognition capabilities while significantly minimizing the risk of hallucinations. Finally, we conduct a thorough evaluation of various general-purpose and RS-specific MLLMs, not only on standard RS tasks such as classification, visual question answering, and visual grounding, but also on a multiple-choice question (MCQ) evaluation benchmark, LHRS-Bench (Muhtar et al., 2024), which is designed to comprehensively assess MLLMs in the RS domain. This facilitates a holistic assessment of instruction-following abilities and other RS-specific capabilities across multiple dimensions, such as perception and spatial awareness. The main contributions of this study are: 1. We propose a large-scale RS image-caption dataset, LHRS-Align-Recap, with high-quality captions generated through feature-guided image captioning. Additionally, we enlarge our instruction dataset by generating more conversations that emphasize spatial recognition and robustness. 2. We scale up the vision encoder for higher-resolution inputs and design an MoE-based bridge layer to enhance model capacity. This enables more efficient compression of visual information with limited vision tokens, thereby improving language-vision alignment performance and visual understanding. 3. We introduce a RS-specialized MLLM, LHRS-Bot-Nova, and systematically evaluate its performance across a range of tasks to assess the fundamental task-solving capabilities. Additionally, our comprehensive evaluation through an MCQ dataset provides a deeper understanding of the reliability of MLLMs as task solvers and offers valuable insights for future improvements."
https://arxiv.org/html/2411.09293v1,LLV-FSR: Exploiting Large Language-Vision Prior for Face Super-resolution,"Existing face super-resolution (FSR) methods have made significant advancements, but they primarily super-resolve face with limited visual information, original pixel-wise space in particular, commonly overlooking the pluralistic clues, like the higher-order depth and semantics, as well as non-visual inputs (text caption and description). Consequently, these methods struggle to produce a unified and meaningful representation from the input face. We suppose that introducing the language-vision pluralistic representation into unexplored potential embedding space could enhance FSR by encoding and exploiting the complementarity across language-vision prior. This motivates us to propose a new framework called LLV-FSR, which marries the power of large vision-language model and higher-order visual prior with the challenging task of FSR. Specifically, besides directly absorbing knowledge from original input, we introduce the pre-trained vision-language model to generate pluralistic priors, involving the image caption, descriptions, face semantic mask and depths. These priors are then employed to guide the more critical feature representation, facilitating realistic and high-quality face super-resolution. Experimental results demonstrate that our proposed framework significantly improves both the reconstruction quality and perceptual quality, surpassing the SOTA by 0.43dB in terms of PSNR on the MMCelebA-HQ dataset.","Face super-resolution (FSR) is a technique that can recover the high-resolution (HR) face image from the low-resolution (LR) one. Due to the constraints of low-cost cameras and suboptimal imaging conditions, the captured face images are often of low quality, leading to poor visual effects and negatively impacting downstream tasks such as face recognition [8], attribute analysis [38], etc. FSR can improve image quality and boost the downstream tasks, which has gained more of spotlight in recent decades. Conventional model-based FSR methods rely on specific assumptions and prior and thus are less effective and practical on real complex scenes when the assumptions do not hold. Recent deep-learning based methods have shown considerable superiority over conventional algorithms in performance [12]. However, FSR is an ill-posed problem. Specifically, an LR face may correspond to multiple HR faces due to the difference in spatial dimensions, bringing great challenges to FSR task. Figure 1: Comparison of FSR framework. The green part denotes existing FSR methods super-resoling face with original LR input while the red part presents our LLV-FSR generates high-quality output with language-vision prior. Introducing additional prior is a common strategy for regularizing the solution space. For example, some efforts employ facial visual prior (i.e., facial parsing map, heatmap, and so on) to capture facial semantic structure information [18, 39, 5] for better reconstruction. Later on, generative prior-based FSR technologies exhibit remarkable capability to generate high-quality face images [2, 31, 3], but not faithful to the ground truth. In addition, these methods have the following obvious drawbacks: i) overly reliance on specific individual visual prior, which is non-trivial to optimize the uncertainty of FSR; ii) predominantly centering around visual perception, while neglecting non-visual language textual information, which consequently results in incomplete scene representation and ultimately declines reconstruction performance of face images. With the continuous advancement of large-scale models, some efforts have integrated language-vision prior into various computer vision tasks (e.g., video understanding [13]), and achieved impressive and thrilling effects. Unlike the pixel-wise visual presentation, language (text) knowledge provides higher-level semantic understanding and abstraction which serve as a supplement to visual perception. This characteristic is valuable for ill-posed FSR problem and under explored in existing technologies. Based on the aforementioned analyses and observations, developing a unified framework to leverage more pluralistic language-vision prior for robust and photo-realistic FSR tasks is highly promising and worth exploring. Following this thread of thinking, we develop a novel framework LLV-FSR, which advocates for marrying language-vision prior for regularizing the FSR task, as depicted in Fig. 1. Specifically, we generate the corresponding language-vision prior, involving image caption, description, facial semantic mask and depth map from the observed LR face with some pretrained large-scale models. Compared to limited visual knowledge (parsing map or depth) used in existing FSR methods, our language-vision prior balance visual semantics and advanced text abstraction expression, which are more comprehensive and in line with physical scene representation. Furthermore, we carefully design a language-vision prior fusion block to fully exploit the complementarity across language-vision features. Benefiting from the language-vision prior and elaborated fusion, it enables the network with powerful capacity to characterize the human face, thus enjoying the state-of-the-art FSR performance to generate visually pleasing FSR results. We highlight the contributions as follows: • We propose LLV-FSR, which makes the first attempt to marry the power of large language model and higher-order visual prior with challenging FSR task. • We carefully design an effect language-vision prior fusion block to utilize the complementary information contained in the language-vision representation, alleviating the ill-posedness of the FSR problem. • Experimental results demonstrate that the proposed method achieves the state-of-the-art performance in terms of visual quality and quantitative metrics."
https://arxiv.org/html/2411.09268v1,"LES-Talker: Fine-Grained Emotion Editing 
for Talking Head Generation in Linear Emotion Space","While existing one-shot talking head generation models have achieved progress in coarse-grained emotion editing, there is still a lack of fine-grained emotion editing models with high interpretability. We argue that for an approach to be considered fine-grained, it needs to provide clear definitions and sufficiently detailed differentiation. We present LES-Talker, a novel one-shot talking head generation model with high interpretability, to achieve fine-grained emotion editing across emotion types, emotion levels, and facial units. We propose a Linear Emotion Space (LES) definition based on Facial Action Units to characterize emotion transformations as vector transformations. We design the Cross-Dimension Attention Net (CDAN) to deeply mine the correlation between LES representation and 3D model representation. Through mining multiple relationships across different feature and structure dimensions, we enable LES representation to guide the controllable deformation of 3D model. In order to adapt the multimodal data with deviations to the LES and enhance visual quality, we utilize specialized network design and training strategies. Experiments show that our method provides high visual quality along with multilevel and interpretable fine-grained emotion editing, outperforming mainstream methods. Project page: https://peterfanfan.github.io/LES-Talker/","Talking head generation has attracted significant attention from researchers in recent years and has many applications in the fields of digital human creation [45], virtual reality [7], etc. To enhance the diversity and expressiveness of talking head generation, emotion editing tasks were emphasized. However, existing studies have the following two notable drawbacks: (1) the interpretability of methods is lacking; (2) the effectiveness of fine-grained emotion editing is still unsatisfactory. On the one hand, some studies offer expression reference images without clear emotion definition [33, 34, 20], while others employ discrete emotion labels [17, 32, 24]. Additionally, some extract emotion features from latent spaces [34, 25] leading to an implicit transformation of emotion. Although Facial Action Units (AUs) effectively describe emotions, various methods [13, 16, 14] use different combinations of AUs for the same emotion. Moreover, AUs alone remain insufficient. On the other hand, several studies [35, 25, 33] focus on generating videos with specific emotions, realizing only coarse-grained emotion editing. One of the few fine-grained emotion studies [34, 31] also shows only discrete emotion levels in three emotion types. We argue that for an approach to be considered fine-grained, it needs to offer clear definitions and detailed differentiation. Motivated by the above, we propose Linear Emotion Space (LES), a definition that explicitly characterizes emotion transformations. LES supports us in proposing LES-Talker, a video generation model for editing multiple emotion types, continuous emotion levels, and individual Facial Action Units. To provide an interpretable theoretical foundation, we develop an LES definition based on elements derived from AUs. Using a coarse-to-fine emotion strategy, LES characterizes emotion transformations as vector changes in two subspaces: the Action Subspace, representing facial unit actions, and the Isolation Subspace, capturing subtle emotion details. When fine-grained emotion editing is required, the vectors in the LES can be used as intermediate representations of emotions. Specifically, when editing a neutral emotion video, each frame is extracted with AUs values and mapped into the Action and Isolation Subspaces as vector representations. The target emotion with specified level corresponds to a vector in these spaces, allowing fine-grained emotion transformations through vector transformations. Further, each dimension of LES has a physical counterpart, making the meaning of these edits explicit. To achieve a satisfactory editing effect, our LES-Talker utilizes LES vectors and 3DMM coefficients as two intermediate representations. We divide the task into three major components. First, we aim to adapt the multi-modal data to the LES definition. We transform the AUs sequences, enhance audios and optimize predicted 3D coefficients, through direct adaptation, wav2vec-based [2] audio encoder, and residual learning, respectively. After generating the accurate representations in LES, we apply vector operation to inject emotion according to the definition in LES. We also propose a Cross-Dimension Attention Net (CDAN). By combining CDANs in series and parallel, we enable detailed guidance of the LES representation in the deformation of the 3D model. Inspired by Sadtalker [42], we use the identity and pose information from reference images and edited 3D coefficients to generate a video through a novel render. Our study makes the following four main contributions: • We propose the Linear Emotion Space (LES), a fine-grained emotion definition that provides an interpretable theoretical foundation for fine-grained emotion research. • We introduce LES-Talker, a novel one-shot talking head generation model with high interpretability, designed to achieve fine-grained emotion editing across emotion types, levels, and facial units. • We propose a novel and universal Cross-Dimension Attention Network to mine potential correlations between 3D model representation and LES representation, which enables the detailed guidance of LES representation in the deformation of the 3D model. • Quantitative and qualitative experiments demonstrate our method generates videos with high visual quality and is capable of fine-grained editing at multiple levels. Figure 2: Pipeline of LES-Talker. Inputs include an identity image, audio, optional AU Source, and user editing targets. The Linear Emotion Space Recon. generates emotion vectors in LES. Emo Injector transforms these vectors based on user targets (emo,level)\text{ or }(au,bias). Two levels of Cross-Dimension Attention Net (CDAN) process decomposed vectors \boldsymbol{u} and \boldsymbol{v} to produce 3D coefficients, optimized via Offset Decoder. These coefficients, along with identity information, create the rendered video."
https://arxiv.org/html/2411.09266v1,"How Good is ChatGPT at Audiovisual Deepfake Detection: A Comparative Study of ChatGPT, AI Models and Human Perception","Multimodal deepfakes involving audiovisual manipulations are a growing threat because they are difficult to detect with the naked eye or using unimodal deep learning-based forgery detection methods. Audiovisual forensic models, while more capable than unimodal models, require large training datasets and are computationally expensive for training and inference. Furthermore, these models lack interpretability and often do not generalize well to unseen manipulations. In this study, we examine the detection capabilities of a large language model (LLM) (i.e., ChatGPT) to identify and account for any possible visual and auditory artifacts and manipulations in audiovisual deepfake content. Extensive experiments are conducted on videos from a benchmark multimodal deepfake dataset to evaluate the detection performance of ChatGPT and compare it with the detection capabilities of state-of-the-art multimodal forensic models and humans. Experimental results demonstrate the importance of domain knowledge and prompt engineering for video forgery detection tasks using LLMs. Unlike approaches based on end-to-end learning, ChatGPT can account for spatial and spatiotemporal artifacts and inconsistencies that may exist within or across modalities. Additionally, we discuss the limitations of ChatGPT for multimedia forensic tasks.","Figure 1: Illustration of audiovisual deepfake manipulations. Original video content is represented as RVRA (real video with real audio. Through deepfake manipulation techniques, three manipulated types are generated: FVRA (fake video with real audio), RVFA (real video with fake audio), and FVFA (fake video with fake audio). Blue text represents the “real modality” of the video content, while red text represents the “fake modality”. Synthetic multimedia content has become both innovative and a significant threat in recent years. Deepfake images and videos created using artificial intelligence (AI) and deep learning (DL) techniques have attracted public and academic attention. This synthetic content is generated by generative adversarial networks (GANs) [1] and more sophisticated AI techniques such as diffusion models [2]. While deepfake technology has many innovative applications in education, entertainment, and other fields [3], it is a double-edged sword that can be used for unethical purposes, such as pornography, political defamation, identity theft, fraud, misinformation, and disinformation [4, 5, 6]. Unethical use of this technology can lead to political instability and social violence [6]. On the one hand, deepfake technology continues to evolve to create more convincing and realistic fake multimedia content. Social media, on the other hand, plays a catalytic role in spreading such content. Therefore, timely detection of deepfake content is crucial to avoid any damage and loss to human society [4]. Audiovisual deepfakes that involve multimodal manipulation are a more convincing type of forgery, with attackers attacking audio, video, or both modalities. Unimodal video forgery detectors [7, 8, 9, 10] and spoofed audio detectors [11, 12, 13, 14] are generally unable to identify forgeries across multiple modalities, although they may be good at detecting forgeries in the specific modality they focus on. To address this challenge, the research community has developed sophisticated tools and algorithms to detect audiovisual forgeries in videos. These specialized tools require knowledge of multimedia forensics as well as knowledge of deep learning. Furthermore, these tools do not generalize well to other unseen datasets and manipulations. Large language models (LLMs) are a major advancement in the field of artificial intelligence. They are trained on a large amount of data and can perform well in various natural language processing (NLP) tasks such as text generation, summarization, classification, completion, sentimental analysis, machine translation, and question answering. Their applications even go beyond the aforementioned NLP tasks and can be used as writing assistants, learning tools, productivity tools, coding assistants, software development, healthcare, legal assistance, entertainment, and more. Despite being primarily designed for NLP tasks, OpenAI’s ChatGPT can analyze image, audio, and video content. Taking advantage of its support for multimodal input, we studied the potential and limitations of ChatGPT for audiovisual deepfake detection. The research questions we aimed to address in this study are as follows: • Can ChatGPT perform multimedia forensic tasks? • Is ChatGPT capable of detecting forgery based on artifacts in audio and visual modalities? • What is the role of prompt engineering in using ChatGPT to detect audiovisual deepfakes? • Which performs better at identifying forgeries in audiovisual deepfakes, ChatGPT, humans, or AI models? • How interpretable is ChatGPT for forgery detection? • What are the limitations of ChatGPT in detecting multimodal deepfakes? The main contributions of our work are threefold: • We explore for the first time the potential of ChatGPT for audiovisual forgery detection tasks. • We compare the performance of ChatGPT with human and state-of-the-art AI models on audiovisual forgery detection tasks. • We highlight the strengths and limitations of ChatGPT on audiovisual forgery detection tasks."
https://arxiv.org/html/2411.09265v1,: Benchmarking the Adversarial Robustness for Dataset Distillation,"Dataset Distillation (DD) is an emerging technique that compresses large-scale datasets into significantly smaller synthesized datasets while preserving high test performance and enabling the efficient training of large models. However, current research primarily focuses on enhancing evaluation accuracy under limited compression ratios, often overlooking critical security concerns such as adversarial robustness. A key challenge in evaluating this robustness lies in the complex interactions between distillation methods, model architectures, and adversarial attack strategies, which complicate standardized assessments. To address this, we introduce BEARD, an open and unified benchmark designed to systematically assess the adversarial robustness of DD methods, including DM, IDM, and BACON. BEARD encompasses a variety of adversarial attacks (e.g., FGSM, PGD, C&W) on distilled datasets like CIFAR-10/100 and TinyImageNet. Utilizing an adversarial game framework, it introduces three key metrics: Robustness Ratio (RR), Attack Efficiency Ratio (AE), and Comprehensive Robustness-Efficiency Index (CREI). Our analysis includes unified benchmarks, various Images Per Class (IPC) settings, and the effects of adversarial training. Results are available on the BEARD Leaderboard, along with a library providing model and dataset pools to support reproducible research. Access the code at BEARD.","Deep Neural Networks (DNNs) [22] have achieved significant success across various applications, primarily due to the availability of large datasets [20, 34, 30, 18]. These extensive datasets enable DNNs to learn valuable representations tailored to specific tasks. However, the acquisition of such large datasets and the training of DNNs can be prohibitively expensive. Dataset Distillation (DD), an emerging technique that compresses large datasets into smaller sets of synthetic samples [36, 48, 2, 28, 6, 52], offers a cost-effective alternative by reducing training demands and simplifying dataset acquisition. DD has a profound impact on both research and practical applications, facilitating the efficient handling and processing of vast amounts of data across various fields. Significant progress in DD has been driven by advanced algorithms, which can be categorized into Meta-Model Matching (e.g., DD [36], KIP [28, 29], RFAD [23], and FRePo [50]), Gradient Matching (e.g., DC [48], MTT [2], TESLA [6], and FTD [9]), and Distribution Matching (e.g., DM [46], CAFE [35], IDM [49], and BACON [52]). Figure 1: Illustration of evaluating adversarial robustness for dataset distillation: The process is divided into three stages: 1) Distillation stage, where diverse dataset distillation methods such as DC [48], DSA [47], and DM [46] generate distilled datasets. 2) Training stage, where models are trained on these distilled datasets. 3) Evaluating stage, where adversarial attacks (e.g., FGSM [12], PGD [26], and C&W [1]) are applied to the test set of standard datasets like CIFAR-10/100 [19] and TinyImageNet [8], and model performance is evaluated both with and without adversarial attacks, summarized using specific metrics. Despite these advancements, DNNs remain highly susceptible to adversarial attacks. These attacks involve perturbations that are imperceptible to the human eye but can effectively deceive classifiers when added to clean images (i.e., adversarial examples) [33, 11, 25, 51], as illustrated in Figure 1. Such vulnerabilities pose significant security risks in applications like face recognition [38, 39], object detection [53, 15], and autonomous driving [37, 45], thereby undermining the reliability of DNNs. Research Gap. While some studies [41, 43, 24, 3] suggest that DD may enhance adversarial robustness, they do not fully address the complex vulnerabilities introduced by adversarial attacks. The robustness of models trained on distilled datasets has not been systematically investigated. Evaluating this robustness is uniquely challenging due to the intricate interactions between distillation methods, model architectures, and diverse attack strategies, which cannot be captured by standard attack protocols. This gap highlights the need for a rigorous framework and tailored metrics to comprehensively assess and improve the adversarial robustness of models trained on distilled data. To address this gap, we introduce BEARD, an open and unified benchmark designed to systematically evaluate the adversarial robustness of existing DD methods. We conducted extensive evaluations using representative DD techniques, including DC [48], DSA [47], DM [46], MTT [2], IDM [49], and BACON [52]. These evaluations span a range of datasets, from large-scale collections like TinyImageNet [8] to smaller ones such as CIFAR-10/100 [19], encompassing diverse scenarios. We incorporated a broad spectrum of both typical and state-of-the-art attack methods for robustness evaluation, including FGSM [12], PGD [26], C&W [1], DeepFool [27], and AutoAttack [4]. To thoroughly assess the adversarial robustness of DD methods, we employed the adversarial game framework to unify various DD tasks and attack scenarios, proposing three primary evaluation metrics: Robustness Ratio (RR), Attack Efficiency Ratio (AE), and Comprehensive Robustness-Efficiency Index (CREI). Additionally, we developed a straightforward evaluation protocol using both a Dataset Pool and a Model Pool. Our large-scale experiments involved cross-evaluating attack methods under multiple threat models, including both targeted and untargeted attacks. This analysis offers insights into adversarial robustness through unified benchmarks, diverse IPC settings, and the effect of adversarial training with BEARD. Our contributions are summarized as follows: • We introduce BEARD, a unified benchmark for evaluating adversarial robustness in dataset distillation, employing an adversarial game framework to systematically assess DD methods under various attack scenarios. • We propose new metrics to evaluate the adversarial robustness of distilled datasets against different attacks, accompanied by a leaderboard that ranks existing DD methods based on these metrics. • We provide open-source code with comprehensive documentation and easy extensibility, along with a Model Pool and Dataset Pool to facilitate adversarial robustness evaluations. • We conduct a comparative analysis of the benchmark results, offering insights and recommendations for enhancing adversarial robustness in dataset distillation."
https://arxiv.org/html/2411.09259v1,Jailbreak Attacks and Defenses against Multimodal Generative Models: A Survey,"The rapid evolution of multimodal foundation models has led to significant advancements in cross-modal understanding and generation across diverse modalities, including text, images, audio, and video. However, these models remain susceptible to jailbreak attacks, which can bypass built-in safety mechanisms and induce the production of potentially harmful content. Consequently, understanding the methods of jailbreak attacks and existing defense mechanisms is essential to ensure the safe deployment of multimodal generative models in real-world scenarios, particularly in security-sensitive applications. To provide comprehensive insight into this topic, this survey reviews jailbreak and defense in multimodal generative models. First, given the generalized lifecycle of multimodal jailbreak, we systematically explore attacks and corresponding defense strategies across four levels: input, encoder, generator, and output. Based on this analysis, we present a detailed taxonomy of attack methods, defense mechanisms, and evaluation frameworks specific to multimodal generative models. Additionally, we cover a wide range of input-output configurations, including modalities such as Any-to-Text, Any-to-Vision, and Any-to-Any within generative systems. Finally, we highlight current research challenges and propose potential directions for future research. The open-source repository corresponding to this work can be found at https://github.com/liuxuannan/Awesome-Multimodal-Jailbreak.","In recent years, multimodal generative models have made significant advancements in both understanding and generation [1, 2]. For multimodal understanding, Multimodal Large Language Models (MLLMs) [3, 4, 5] have demonstrated notable capabilities in Any-to-Text comprehension, excelling in tasks such as visual, audio, and video question-answering [6, 7, 8]. For multimodal generation, denoising diffusion probabilistic models (DDPMs) [9] have achieved impressive performance in Any-to-Vision generation [10, 11, 12, 13]. Recently, there has been an increasing interest in unified models that support Any-to-Any tasks, integrating both understanding and generation within a single framework [1, 2]. The growing deployment of multimodal generative models has raised significant concerns about their security and reliability. Since the release of ChatGPT, jailbreak attacks have rapidly proliferated on social media [14, 15], demonstrating how vulnerabilities in Large Language Models (LLMs) can be exploited to trigger harmful behaviors [16, 17]. Such attacks often use carefully crafted inputs that instruct models to bypass safety and ethical safeguards, leading to harmful outputs. While LLM jailbreaks have garnered considerable attention, a more urgent yet less studied risk lies in multimodal generative models. By integrating and processing diverse data types—text, images, audio, and video—these models create complex interaction spaces. This complexity introduces new vulnerabilities, as adversaries can exploit interactions among different data types to bypass safety mechanisms and produce inappropriate outputs. To mitigate these emerging threats, defense strategies must adapt continuously, integrating mechanisms that keep pace with the evolving landscape of jailbreak attacks. Figure 1: Illustrated examples of jailbreak attacks on multimodal generative models to induce harmful outputs across various modalities, including harmful text via the Jailbreak in Pieces [18], harmful images via the MMA-diffusion [19], harmful videos via the T2VSafetyBench [20] and harmful audio via the Voice Jailbreak [21]. Figure 2: Jailbreak attack and defense against multimodal generative models. Existing reviews on jailbreak methods [22, 23, 24, 25] have primarily concentrated on content output within a specific modality, addressing tasks either Any-to-Text [26, 23] or Any-to-Image [24, 25]. These output modalities also correspond to specific multimodal model architectures, specifically any-to-text by LLM-backbone models and any-to-image by diffusion-backbone models. While these surveys have made valuable contributions, they lack a unified framework that spans a broad range of modalities (i.e., text, images, audio, and video) within different generative systems, as shown in Fig. 1. To address this gap, our survey introduces the first unified framework which systematically summarizes jailbreak attacks and defense mechanisms across various input-output modalities and different generative structures. Specifically, we break down the lifecycle of multimodal jailbreak and abstract four discrete levels – input, encoder, generator, and output. This structured approach helps bridge the gaps between different models, each of which may have unique architecture but share common vulnerabilities within these four key levels. We outline the four general steps (as shown in Fig. 2) to devise jailbreak attack and defense techniques: 1. Input Level: Attackers and defenders operate solely on the input data. Attackers modify inputs to execute attacks, while defenders incorporate protective cues to enhance detection. 2. Encoder Level: With access to the encoder, attackers optimize adversarial inputs to inject malicious information into the encoding process, while defenders work to prevent harmful information from being encoded within the latent space. 3. Generator111We refer to the entire generative model as the “Generator” without loss of generality. Level: With full access to the generative models, attackers leverage inference information, such as activations and gradients, and fine-tune models to increase adversarial effectiveness, while defenders use these techniques to strengthen model robustness. 4. Output Level: With the output from the generative model, attackers can iteratively refine adversarial inputs, while defenders can apply post-processing techniques to enhance detection. Note that we encompass a broader range of input-output modality configurations, including text, image, audio, and video, alongside multiple types of multimodal generative models such as Any-to-Text, Any-to-Vision, and Any-to-Any models. Meanwhile, we conduct a comparative analysis of various evaluation datasets and metrics used for benchmarking, along with insightful observations and suggestions for future research directions. By highlighting the landscape of jailbreak attacks against multimodal generative models, our survey enhances the understanding of security challenges and provides direction for developing effective defenses. We aim to equip researchers, practitioners, and policymakers with valuable insights to safeguard foundation models against malicious exploitation. In summary, our key contributions are as follows: • Through a comprehensive review of existing attack methodologies (see TABLE III) and defense strategies (see TABLE IV), we abstract and summarize a general categorization for launching and defending jailbreak against multimodal generative models, comprising four distinct stages (see Fig. 2). • We present a comprehensive and systematic review of attack, defense, and evaluation strategies across various input-output modalities and different model structures. • We thoroughly discuss the limitations, challenges, and future directions for real-world applications, facilitating future research in this domain. The remaining paper is organized as follows. We first provide a brief introduction to the preliminaries in Section II, which cover essential topics and concepts for the proper understanding of this work. In Section III and Section IV, we summarize existing approaches for jailbreak attacks and defense strategies, based on the stages of interaction with generative models respectively, including input-level, encoder-level, generator-level, and output-level. Section V introduces the commonly used datasets and evaluation metrics. Moreover, we provide discussions and future research opportunities in Section VI. Finally, we conclude this review in Section VII."
https://arxiv.org/html/2411.09250v1,Embedding Space Allocation with Angle-Norm Joint Classifiers for Few-Shot Class-Incremental Learning,"Few-shot class-incremental learning (FSCIL) aims to continually learn new classes from only a few samples without forgetting previous ones, requiring intelligent agents to adapt to dynamic environments. FSCIL combines the characteristics and challenges of class-incremental learning and few-shot learning: (i) Current classes occupy the entire feature space, which is detrimental to learning new classes. (ii) The small number of samples in incremental rounds is insufficient for fully training. In existing mainstream virtual class methods, for addressing the challenge (i), they attempt to use virtual classes as placeholders. However, new classes may not necessarily align with the virtual classes. For the challenge (ii), they replace trainable fully connected layers with Nearest Class Mean (NCM) classifiers based on cosine similarity, but NCM classifiers do not account for sample imbalance issues. To address these issues in previous methods, we propose the class-center guided embedding Space Allocation with Angle-Norm joint classifiers (SAAN) learning framework, which provides balanced space for all classes and leverages norm differences caused by sample imbalance to enhance classification criteria. Specifically, for challenge (i), SAAN divides the feature space into multiple subspaces and allocates a dedicated subspace for each session by guiding samples with the pre-set category centers. For challenge (ii), SAAN establishes a norm distribution for each class and generates angle-norm joint logits. Experiments demonstrate that SAAN can achieve state-of-the-art performance and it can be directly embedded into other SOTA methods as a plug-in, further enhancing their performance.","Deep Neural Network (DNN) methods have excelled in the field of computer vision [1, 2, 3, 4]. Leveraging vast datasets and known classification targets, DNNs have demonstrated remarkable capabilities. However, in real-life scenarios, datasets are often limited in size, so the application of few-shot learning (FSL) is more prevalent [5, 6, 7]. Moreover, in many application contexts, not only is the number of samples limited, but the number of classes continues to grow. Constrained by computational resources and practical constraints, retraining the entire model from scratch with each new class addition is not feasible. Therefore, the design of efficient and accurate algorithms for achieving Few-Shot Class-Incremental Learning (FSCIL) has attached increasing attention [8, 9, 10, 11, 12, 13]. Figure 1: The motivation of SAAN. In only CE, old categories fill the entire feature space, making it hard for new categories to be inserted without overlapping. In SAAN, the old categories are allocated to a limited space, guided by the category centers, allowing new categories to be inserted into the reserved space without overlapping. In addition, compared to NCM, which has only angular decision boundaries, SAAN has both angular and norm boundaries, which fully utilizes the feature information to enhance the basis for discrimination. The goal of FSCIL is to acquire new knowledge effectively while preventing the forgetting of previously learned knowledge. Previous research on FSCIL shows that models trained incrementally are prone to overfit on limited new data and forget the old knowledge catastrophically [14, 15, 8]. To combat overfitting and catastrophic forgetting, some studies propose the use of a feature extractor freezing paradigm [9, 10, 11, 16]. This paradigm freezes most layers in the Convolutional Neural Network (CNN), which is the feature extractor, and uses the Nearest Class Mean (NCM) algorithm [17] instead of Fully Connected (FC) layers as the classifier during the incremental phase. According to the neural collapse phenomenon [18, 19], researchers discover that without any intervention, current training samples would occupy the entire embedding space. This phenomenon is beneficial for enhancing the neural network’s capabilities in conventional classification tasks. However, in the context of Class-Incremental Learning (CIL), learning new classes becomes extremely challenging. This phenomenon addresses the challenge (i) we aim to solve. To address this issue, some researchers [16, 11] propose occupying some of the embedding space with virtual classes generated through data augmentation. Then, during the incremental phase, the space occupied by these virtual classes can be used for inserting new classes. However, these methods cannot control the location of the space occupied by virtual classes, as the occupied positions are determined by the features of the virtual samples. Some researchers [12] propose using pre-allocated fixed prototypes to reduce the feature misalignment problem during the incremental process. However, this approach of completely fixed and randomly allocated prototypes ignores the intrinsic meaning of the samples, which may lead to a discrepancy between the semantic distance of the samples and their distance in the feature space. Because of the FSL setting, the small number of samples in incremental sessions is insufficient for large-scale training, addressing the challenge (ii) we aim to solve. For choosing the neural network classifier, NCM [20] does not rely on extensive training and is suitable for the FSCIL problem, which is wildly used in the mainstream methods [10, 11, 16]. NCM solely computes the distance between samples and the mean of each class to classify them, which does not rely on a large amount of data. Some researchers [21, 22] find that cosine similarity is well-suited for comparing embeddings in DNN. NCM based on cosine similarity has achieved good results in FSCIL [10, 11, 16]. In the FSL setting, we believe that leveraging as much sample information as possible during the incremental sessions is essential. Based on our experiments, we observe that in the FSCIL scenario, there is a significant difference in the number of samples across different categories, leading to variations in norms between categories. However, this norm information is not used by NCM. To address these issues, we propose the class-center guided embedding Space Allocation with Angle-Norm joint classifiers (SAAN) learning framework. SAAN consists of the Class-Center guided embedding Space Allocation (CCSA) and the Angle-Norm Joint classifiers (ANJ). CCSA is designed to address the challenge (i) posed by CIL. Specifically, CCSA divides the feature space into multiple subspaces and allocates different subspaces for different incremental sessions by setting class centers, using our designed Cosine Center (CC) loss. CCSA resolves two issues found in previous work. First, in prior virtual class methods [11, 16], due to the lack of guidance for new classes, the reserved space for virtual classes may not necessarily be utilized by new classes. CCSA imposes explicit spatial constraints on new classes, ensuring that they enter the reserved space. Second, unlike methods with completely fixed prototype allocation [12, 23], where random allocation may cause similar classes to be far apart in the feature space, SAAN enables the adjustment of class centers within the designated space to ensure that prototype similarity aligns with the samples’ semantic information. ANJ is a new classifier, designed to address the challenge (ii) posed by FSL. ANJ establishes a norm distribution for each class to estimate the norm logit and combines this with NCM to estimate the angular logit, ultimately generating joint logits. In this way, ANJ can leverage previously overlooked norm information, which is especially important for the information-scarce FSL setting. Since SAAN is designed to address issues in previous virtual class methods, it can be directly integrated into these State-Of-The-Art (SOTA) methods as a plug-in. Experiments show that SAAN can achieve performance close to SOTA and can further improve the performance of existing SOTA methods. The main contributions of this paper are summarized as follows: 1) CCSA guides sample points into specific subspaces through our meticulously designed CC loss. This resolves two issues found in previous methods: new classes may fail to appear in the reserved feature space, and similar classes may be far apart in the feature space. 2) Our experiments demonstrate the importance of norm information for classification in the context of FSCIL. ANJ effectively combines angular and norm information, fully utilizing the information contained within the limited samples. 3) Experiments demonstrate that SAAN can achieve performance close to SOTA and further improve SOTA models’ performance. Specifically, SAAN improves the final round accuracy by over 3% across three datasets and two methods."
https://arxiv.org/html/2411.09219v1,"Harnessing Vision Foundation Models for High-Performance, Training-Free Open Vocabulary Segmentation","While Contrastive Language-Image Pre-training (CLIP) has advanced open-vocabulary predictions, its performance on semantic segmentation remains suboptimal. This shortfall primarily stems from its spatial-invariant semantic features and constrained resolution. While previous adaptations addressed spatial invariance semantic by modifying the self-attention in CLIP’s image encoder, the issue of limited resolution remains unexplored. Different from previous segment-then-splice methods that segment sub-images via a sliding window and splice the results, we introduce a splice-then-segment paradigm that incorporates Segment-Anything Model (SAM) to tackle the resolution issue since SAM excels at extracting fine-grained semantic correlations from high-resolution images. Specifically, we introduce Trident, a training-free framework that first splices features extracted by CLIP and DINO from sub-images, then leverages SAM’s encoder to create a correlation matrix for global aggregation, enabling a broadened receptive field for effective segmentation. Besides, we propose a refinement strategy for CLIP’s coarse segmentation outputs by transforming them into prompts for SAM, further enhancing the segmentation performance. Trident achieves a significant improvement in the mIoU across eight benchmarks compared with the current SOTA, increasing from 44.4 to 48.6. Code is available at https://github.com/YuHengsss/Trident.","Figure 1: Comparison with previous SOTA performance of open-vocabulary semantic segmentation under training-free setting. Semantic segmentation is a foundational vision task that aims to segment images according to different semantics, where deep learning shows impressive performance [42, 53, 2, 9, 25, 66, 11]. However, these methods are trained on a close set, limiting their application in open-vocabulary scenarios. More recently, Vision Language Models (VLMs) [49, 12, 57, 38, 31, 32], trained on web-scale data to align textual descriptions with image semantics, have demonstrated remarkable capabilities in open-vocabulary recognition. As a pioneering and representative work, CLIP [49] has inspired numerous efforts to leverage its robust open-vocabulary capabilities for dense prediction tasks, such as semantic segmentation [74, 60, 36]. However, CLIP only receives image-level supervision without correlating text and region-level features. It compromises the semantic integrity of dense feature maps due to spatial invariance semantic, indicating local features tend to be invariant to their spatial positions [59], which leads to suboptimal performance of segmentation that leverages pixel-level features. In contrast to CLIP, Vision Foundation Models (VFMs) [5, 24, 29] show a close relationship between feature semantics and their positions, but they lack explicit understanding. For example, SAM segments visible objects within an image but fails to give corresponding semantics. To migrate the spatial invariance semantic for CLIP, different adaptations are proposed and these works can be categorized into three types according to their training paradigm, i.e., full tuning [36, 51, 13, 60], partial tuning with additional learnable parameters [65, 75, 43, 70], and training-free paradigm [74, 59, 28]. Compared to tuning-based methods, training-free paradigm offers an attractive alternative, characterized by its low cost, free of annotations, and preservation of generalization capabilities. MaskCLIP [74], a pioneering work in this field, attributes the spatial invariance semantic of CLIP’s image features to the self-attention mechanism [58]. It proposes to replace the QK attention of the last transformer layer in CLIP’s image encoder with a simple convolution, which brings significant improvements. Inspired by this innovation, a series of studies [34, 65, 54] have proposed more effective aggregation methods. However, these methods are constrained by CLIP’s inherent limitation of operating at low resolutions. To address this issue, a sliding window strategy is widely adopted to split the source image into multiple sub-images, segment them separately, and then splice the results together, which we denote as Segment-then-Splice. Although it alleviates this issue to some degree, it is difficult to generalize to scenarios with higher resolutions. As demonstrated in Tab. 1, the performance significantly declines when the resolution increases. We mainly attribute it to the limited receptive field of the sub-images given higher resolutions. In this work, we propose to reformulate the sliding window strategy that improves CLIP’s segmentation ability on high-resolution images. Motivated by the fine-grained semantic correlations of pixel-level features from SAM [29], we utilize a correlation matrix sourced from its encoder to harmonize the superiority of both SAM and CLIP. Specifically, we splice feature maps extracted from CLIP’s image encoder during the sliding window procedure and incorporate the correlation matrix from SAM to further aggregate this spliced feature map for segmentation, which we denote as Splice-then-Segment. While feature extraction operates locally on sub-images, the correlation matrix enables global attention, extending the receptive field beyond individual windows to encompass the entire image. To further improve the segmentation, we developed a refinement strategy. By converting CLIP’s segmentation results to points, boxes, and mask prompts for SAM refinement, the results using mask prompts only can be significantly improved. During feature extraction of sub-images, we adopt the DINO to provide spatially covariant semantic guidance, as introduced in [30]. Considering our method integrates three foundational models, i.e., CLIP, DINO, and SAM, we named it Trident. Trident achieves SOTA performance among training-free methods, and shows competitive results even compared to weakly supervised methods [26, 7] across eight widely used benchmarks, as shown in Fig. 1. Our contributions can be summarized as: (1) We analyze the limitations of existing Segment-then-Splice paradigm when adapting CLIP for high-resolution semantic segmentation. (2) Motivated by this analysis, we introduce a Splice-then-Segment paradigm to harmonize different vision foundation models for semantic segmentation. (3) Our framework Trident is validated across 8 popular benchmarks, surpassing previous SOTA results by a significant margin."
https://arxiv.org/html/2411.09209v1,JoyVASA: Portrait and Animal Image Animation with Diffusion-Based Audio-Driven Facial Dynamics and Head Motion Generation,"Audio-driven portrait animation has made significant advances with diffusion-based models, improving video quality and lip-sync accuracy. However, the increasing complexity of these models has led to inefficiencies in training and inference, as well as constraints on video length and inter-frame continuity. In this paper, we propose JoyVASA, a diffusion-based method for generating facial dynamics and head motion in audio-driven facial animation. Specifically, in the first stage, we introduce a decoupled facial representation framework that separates dynamic facial expressions from static 3D facial representations. This decoupling allows the system to generate longer videos by combining any static 3D facial representation with dynamic motion sequences. Then, in the second stage, a diffusion transformer is trained to generate motion sequences directly from audio cues, independent of character identity. Finally, a generator trained in the first stage uses the 3D facial representation and the generated motion sequences as inputs to render high-quality animations. With the decoupled facial representation and the identity-independent motion generation process, JoyVASA extends beyond human portraits to animate animal faces seamlessly. The model is trained on a hybrid dataset of private Chinese and public English data, enabling multilingual support. Experimental results validate the effectiveness of our approach. Future work will focus on improving real-time performance and refining expression control, further expanding the framework’s applications in portrait animation. The code will be available at: https://jdhalgo.github.io/JoyVASA.","In recent years, the domain of audio-driven portrait animation has achieved remarkable progress, largely driven by the emergence of diffusion-based generative models [1, 2, 3, 4, 5, 6, 7]. These innovative approaches have significantly improved both the quality of generated videos and the accuracy of lip synchronization, contributing to more lifelike and engaging animated characters. This technology finds application across diverse fields such as digital avatars [8], virtual assistants [9, 10], and entertainment [11, 12], where realistic animation plays a pivotal role in enhancing user engagement. Nevertheless, the growing complexity of these models introduces challenges, leading to inefficiencies in the training and inference phases. This added complexity also restricts the length of videos that can be generated in a single batch, which may negatively impact the continuity of motion between frames. For instance, a lot of existing methods predict approximately 10 frames of images, which corresponds to about 0.4 seconds of video when considering a standard frame rate of 25 frames per second [13, 14, 6, 15, 16, 2]. This limitation poses challenges for models in capturing motion information from preceding frames, potentially leading to unstable outcomes, such as sudden temporal jitter and significant deformation of the character’s appearance. To address these issues, some studies have proposed simply increasing the number of predicted frames as well as the number of preceding frames [4]. However, this approach introduces a substantial computational burden, complicating the task. Besides, numerous existing methods require the provision of motion guidance or reference actions, such as action sequence images or facial key points. However, the reliance on such additional reference information reduces the flexibility of the model, as the reference actions typically need to be extracted from supplementary videos. To alleviate these limitations, we introduce a diffusion-based framework for facial dynamics and head motion generation in audio-driven portrait animation. Our proposed approach harnesses the power of disentangled facial representation to generate compact dynamic sequences directly from audio inputs, without any other conditions. This enables the production of longer animated videos, surpassing the capabilities of current end-to-end diffusion models. Specifically, our framework utilizes a decoupled facial representation model, Liveportrait [17], which distinctly separates dynamic facial expressions from static 3D facial representations. This separation allows for the flexible amalgamation of static representations with dynamically generated sequences, yielding more accurate and adaptable animations. Furthermore, we develop a diffusion transformer model that synthesizes motion sequences, including dynamic facial expressions and head motions. This model operates independently of character identity, drawing solely on audio cues. This feature enhances the versatility of our method, allowing it to be applied to a broad range of character types, including both human and animal figures. A renderer, trained within the decoupled representation framework, then integrates static 3D facial representations with the generated motion sequences to create high-quality animated outputs. Our framework is trained on a hybrid dataset that combines our private Chinese dataset with two publicly available English datasets, ensuring better multilingual support. The results of our experiments validate the effectiveness of our approach. Subsequent advancements will focus on enhancing real-time processing capabilities and refining the control over character expressions, thus expanding the applicability of our framework within the field of portrait animation. Figure 1: Inference Pipeline of the proposed JoyVASA."
https://arxiv.org/html/2411.09180v1,LEAP:D - A Novel Prompt-based Approach for Domain-Generalized Aerial Object Detection,"Drone-captured images present significant challenges in object detection due to varying shooting conditions, which can alter object appearance and shape. Factors such as drone altitude, angle, and weather cause these variations, influencing the performance of object detection algorithms. To tackle these challenges, we introduce an innovative vision-language approach using learnable prompts. This shift from conventional manual prompts aims to reduce domain-specific knowledge interference, ultimately improving object detection capabilities. Furthermore, we streamline the training process with a one-step approach, updating the learnable prompt concurrently with model training, enhancing efficiency without compromising performance. Our study contributes to domain-generalized object detection by leveraging learnable prompts and optimizing training processes. This enhances model robustness and adaptability across diverse environments, leading to more effective aerial object detection.","Object detection in aerial imagery is rapidly progressing in tandem with advancements in deep learning. However, the unique characteristics of drone imagery pose significant challenges, impacting object appearance and shape due to varying shooting conditions. As a result, there is considerable interest in employing domain generalization techniques to address these challenges. Factors such as drone altitude, angle, and weather conditions contribute to variability in shooting conditions. Variations in drone altitude can affect object sizes, while camera viewing direction can alter object shapes. Moreover, weather conditions and the time of day when images are captured cause significant differences in lighting, further influencing the imagery. To simply enhance the object detection performance in drone captured images, [1, 2] conducted research on methods for processing high-resolution images. The Nuisance Disentangled Feature Transform (NDFT) is a technique designed to disentangle and separate nuisance features from important features. Wu et al. collected shooting condition information as metadata, which defines each environment as a ”domain” and allows for classification specific to each domain [3]. Adversarial training during backpropagation is used by introducing negative values into the backbone network, ensuring the model does not overly respond to domain-specific features but instead leverages domain-invariant features for object detection. Lee et al. proposed an improved version of NDFT, called A-NDFT, by introducing feature replay and a slow learner strategy to address the learning speed limitations observed in NDFT, significantly enhancing training speed while maintaining comparable performance [4]. Jin et al. proposed the Fine-Grained Feature Disentanglement (FGFD) module, which uses the one-stage detector YOLOv5 [5] as the backbone network to differentiate between domain-specific and domain-invariant feature maps [6]. There has been a significant surge in interest around vision-language representation learning tasks, largely due to training on large-scale image-text pairs, which leads to notable generalization performance. This advantage has been effectively harnessed to deliver remarkable results in image classification. Specifically, active research is underway in domains that demand high generalization capabilities, such as few-shot learning, zero-shot learning, and open-vocabulary learning. Zhou et al. proposed the Context Optimization (CoOp) approach, which addresses concerns about the time-consuming nature of using manually crafted prompts for labeling [7], similar to CLIP [8]. To address this issue, they replaced manual prompts with learnable prompts, reducing labeling time while maintaining performance comparable to CLIP. Building on this concept, Zhou et al. introduced Conditional Context Optimization (CoCoOP), where they incorporated a subnetwork called Meta-Net to conditionally pass information extracted from the image encoder to the learnable prompt, leading to high performance in vision-language tasks [9]. In the field of domain-generalized object detection, a noticeable trend has emerged toward incorporating large-scale vision-language models. Vidit et al. introduced CLIP-the-Gap, which leverages data augmentation in the feature space through text prompts. This approach enhances generalization across diverse environments for models trained on single domains with various known domain labels [10]. Liu et al. built on this concept with LGNet, a pioneering work utilizing large-scale vision-language models for aerial image object detection [11]. Their approach employs a two-step training process: First, text prompt embeddings are fine-tuned using manual prompts, and then the object detection model is trained with the fine-tuned CLIP model. The manual prompts incorporate domain-specific information like drone altitude, angle, and weather. However, this approach has a limitation in that it confines drone-specific information to only altitude, angle, and weather, neglecting other potential conditions that might impact detection performance. Our proposed approach diverges from the traditional reliance on manual prompts by employing a learnable prompt. This adjustment enables us to utilize a broader spectrum of domain-specific knowledge beyond the fixed information used in previous methods. Moreover, we shift from the conventional two-step training process, where text prompt embeddings are fine-tuned separately before training the object detection model. Instead, by updating the learnable prompt concurrently with the object detection model training, we establish a more efficient, streamlined one-stage training process. The proposed LEAP:D approach, illustrated in Figure 1, streamlines the training process by adopting a learnable prompt mechanism that updates concurrently with the object detection model, enabling efficient one-step training compared to traditional two-step methods."
https://arxiv.org/html/2411.09174v1,Advancing Diffusion Models: Alias-Free Resampling and Enhanced Rotational Equivariance,"Recent advances in image generation, particularly via diffusion models, have led to impressive improvements in image synthesis quality. Despite this, diffusion models are still challenged by model-induced artifacts and limited stability in image fidelity. In this work, we hypothesize that the primary cause of this issue is the improper resampling operation that introduces aliasing in the diffusion model and a careful alias-free resampling dictated by image processing theory can improve the model’s performance in image synthesis. We propose the integration of alias-free resampling layers into the UNet architecture of diffusion models without adding extra trainable parameters, thereby maintaining computational efficiency. We then assess whether these theory-driven modifications enhance image quality and rotational equivariance. Our experimental results on benchmark datasets, including CIFAR-10, MNIST, and MNIST-M, reveal consistent gains in image quality, particularly in terms of FID and KID scores. Furthermore, we propose a modified diffusion process that enables user-controlled rotation of generated images without requiring additional training. Our findings highlight the potential of theory-driven enhancements such as alias-free resampling in generative models to improve image quality while maintaining model efficiency and pioneer future research directions to incorporate them into video-generating diffusion models, enabling deeper exploration of the applications of alias-free resampling in generative modeling.","Recent advancements in generative modeling, particularly in diffusion models [6], have pushed the boundaries of what is possible in high-quality image synthesis. Among these, the Stable Diffusion model has gained prominence for its ability to generate realistic images by iteratively refining noise into coherent visual outputs [4]. Despite its success, there remains a challenge in further enhancing the model’s performance, particularly in terms of stability and image fidelity [3]. In this paper, we hypothesize that the existing resampling operations (upsampling/downsampling) in the architecture of current diffusion models introduce aliasing which leads to a reduction of image quality. We also propose that proper theory-driven alias-free resampling can improve the model’s performance in image synthesis. Improving the performance of image synthesis via alias-free resampling techniques has recently been explored in generative adversarial networks (GANs). Indeed, StyleGAN3 [9], the latest iteration in the StyleGAN series, has demonstrated significant improvements over its predecessors by incorporating carefully designed alias-free resampling layers via anti-aliasing filtering techniques that prevent high-frequency artifacts and improve the overall visual coherence of generated images. Earlier versions of StyleGAN networks, including StyleGAN2 [10], fail to rigorously implement the alias-free resampling during the up or downsampling stages and this careless signal processing can be a root cause of aliasing in the generator network [9]. Their advancements have shown that even small architectural modifications, when grounded in image processing principles, can lead to substantial gains in model performance. Alias-free resampling ensures that when images are upsampled or transformed between scales, no high-frequency components are artificially introduced, which could corrupt the learned texture or details. Thus, incorporating these principles allows the model to avoid aliasing artifacts and enhance its rotational equivariance and overall output fidelity. In addition to GAN networks, we hypothesize that the principle of alias-free resampling is especially crucial for diffusion models, where sequential resampling occurs across multiple scales, making it essential to preserve fine details and avoid introducing spurious artifacts. However, to date, proper integration of alias-free resampling into diffusion models remains largely unexplored. Current diffusion models typically apply standard downsampling and upsampling operations [6], which can introduce aliasing and degrade the quality of generated images, especially at finer scales. By integrating alias-free resampling techniques, diffusion models could achieve more stable and artifact-free outputs, enhancing both image fidelity and rotational consistency across generated samples. This work investigates the theory-driven integration of alias-free resampling techniques into the UNet structure of Diffusion models. Importantly, our approach focuses on enhancing model performance without introducing any new trainable parameters, thereby maintaining the model’s efficiency and simplicity. By strategically incorporating alias-free resampling layers, we aim to leverage the principles of image processing to improve the stability and output quality of the diffusion model. Furthermore, we propose a modified diffusion process to incorporate user-controlled rotation of the generated image without any additional training. Our experiments show that incorporating our proposed modifications significantly enhances the image quality across various configurations of the UNet structure. Specifically, our modified configurations outperformed the standard UNet model on benchmark datasets such as MNIST, CIFAR-10, and MNIST-M [11, 12, 5]. This highlights the potential of using appropriate alias-free resampling layers, as guided by image processing principles, to achieve better results in generative models. Our modified diffusion process also showed promising results in rotational consistency despite being trained on images without rotations. The key takeaway from our findings is that careful architectural re-design governed by signal and image processing theories can enhance model performance without the need for additional trainable parameters, thereby offering a path forward for further innovations in generative modeling."
https://arxiv.org/html/2411.09156v1,DyGASR: Dynamic Generalized Exponential Splatting with Surface Alignment for Accelerated 3D Mesh Reconstruction,"Recent advancements in 3D Gaussian Splatting (3DGS), which lead to high-quality novel view synthesis and accelerated rendering, have remarkably improved the quality of radiance field reconstruction. However, the extraction of mesh from a massive number of minute 3D Gaussian points remains great challenge due to the large volume of Gaussians and difficulty of representation of sharp signals caused by their inherent low-pass characteristics. To address this issue, we propose DyGASR, which utilizes generalized exponential function instead of traditional 3D Gaussian to decrease the number of particles and dynamically optimize the representation of the captured signal. In addition, it is observed that reconstructing mesh with Generalized Exponential Splatting(GES) without modifications frequently leads to failures since the generalized exponential distribution centroids may not precisely align with the scene surface. To overcome this, we adopt Sugar’s approach and introduce Generalized Surface Regularization (GSR), which reduces the smallest scaling vector of each point cloud to zero and ensures normal alignment perpendicular to the surface, facilitating subsequent Poisson surface mesh reconstruction. Additionally, we propose a dynamic resolution adjustment strategy that utilizes a cosine schedule to gradually increase image resolution from low to high during the training stage, thus avoiding constant full resolution, which significantly boosts the reconstruction speed. Our approach surpasses existing 3DGS-based mesh reconstruction methods, as evidenced by extensive evaluations on various scene datasets, demonstrating a 25% increase in speed, and a 30% reduction in memory usage.","In 3D computer vision, reconstructing surface mesh from multiple calibrated views is a foundational task. Initially, point clouds are typically derived from image collections using traditional Multi-View Stereo (MVS) techniques, and triangular meshes are subsequently constructed from them [1]. Recently, Neural Radiance Fields (NeRF) [2] represent neural implicit surface reconstruction techniques and have emerged as formidable competitors. This technique commonly employs Multi-Layer Perceptrons or hash encoding technologies [3] to attribute geometric properties like density [4] or the signed distance functions to the nearest surface (SDF) [5, 6, 7] to spatial points. In the domain of neural implicit mesh reconstruction, SDF-based methods are particularly prominent because they facilitate volumetric rendering and enable the learning of implicit surface representations through density functions derived from SDF. Impressive results are attained in small scenes through rendering supervision methods based on neural implicit techniques, however, these methods struggle in complex or large-scale scenes, particularly in those with extensive untextured areas [8, 9]. To address these challenges, structural priors such as depth [10], normal regularization [8], point clouds [9], and semantic information have been incorporated into the optimization process in previous studies, alongside refined sampling strategies such as voxel keypoint guidance [11] and hierarchical sampling [9]. While these strategies enhance the accuracy of surface mesh reconstruction, they significantly increase computational demands and extend training duration. Although some methods [9] use MVS-predicted point clouds as priors for mesh reconstruction, these clouds are sparse and noisy, failing to capture the scene’s detailed features. Following NeRF, the 3DGS method was recently introduced and has gained popularity [12]. This method excels in generating dense geometric point clouds and explicitly storing the scene’s structure in parameter space, enabling direct edits to 3D scenes. Each Gaussian’s parameters include position in 3D space, covariance matrix, opacity, and spherical harmonics coefficients. However, Gaussians optimized through 3DGS are not directly usable as priors for mesh reconstruction due to their large numbers, slow training speeds, and predominantly internal scene positioning, which may produce noisy outcomes. Figure 1: Illustrates that our method, excels in both training time and reconstruction quality, achieving the highest performance. Our method aims to reduce training times and storage costs while surpassing state-of-the-art reconstruction quality. We noted the inherent assumption of low-pass characteristics in 3DGS signal modeling, as illustrated in Figure 2(a, b). Given the high-frequency discontinuities in most scenes and the memory burden from numerous tiny Gaussians in 3DGS, we chose a more appropriate basis. Inspired by the work presented in the GES[13], we have incorporated the Generalized Exponential Splatting(GES), which represents various signal points with fewer particles and greater precision, over ordinary Gaussians. However, since GES-generated geometric point clouds’ centers do not align with actual scene surfaces, we adopt Sugar’s approach and introduce Generalized Surface Regularization(GSR)[14] to align these point clouds with the surface, optimizing parameters to control the shape of generalized exponential splatting. concurrently. Ideally, when generalized exponential splatting distributions are flat and uniformly distributed over the surface, the SDF calculated by the density function minimizes discrepancies with the SDF from actual generalized exponential distribution, allowing precise representation of surface attributes. A level set, extracted from the density function, undergoes point sampling to facilitate surface mesh creation via Poisson reconstruction [1]. Furthermore, we abandoned the original training methods and introduced a strategy that smoothly transitions resolution from small to large, significantly speeding up training convergence and stability while enhancing reconstruction quality. Fig.1 displays the high-quality results of our reconstruction and rendering. This work makes the following contributions: (1) We propose DyGASR, employing GES over 3DGS to generate fewer, more effective point cloud priors and introducing GSR to align these priors with scene surfaces, thus accelerating the reconstruction of high-quality surface mesh. (2) Additionally, we apply a dynamic resolution training strategy that smoothly transitions from low to high resolution, effectively shortening training durations and reducing memory consumption. (3) The effectiveness of our method for 3D mesh reconstruction is demonstrated through ablation studies. Evaluations across multiple datasets show a 25% reduction in training time, a 30% decrease in memory usage, and a 0.29 dB improvement in PSNR over the SOTA method. Figure 2: We verify the low-pass characteristics of 3D Gaussians. In (a), the GT image is shown on the left, and a rendering after 500 iterations of 3DGS on the right, both analyzed in the frequency domain via Fourier transform along the same horizontal line. The rendering appears in green and the GT image in blue, highlighting that the low-pass characteristics of 3DGS do not perfectly align with the scene’s signal features. In (c), generalized exponential functions are displayed, where \epsilon=1 represents a scaled Laplace distribution, \epsilon=2, a scaled Gaussian distribution, along with other signal shapes such as triangles and squares. Here, \epsilon serves as a parameter for each component in our method."
https://arxiv.org/html/2411.09153v1,VidMan: Exploiting Implicit Dynamics from Video Diffusion Model for Effective Robot Manipulation,"Recent advancements utilizing large-scale video data for learning video generation models demonstrate significant potential in understanding complex physical dynamics. It suggests the feasibility of leveraging diverse robot trajectory data to develop a unified, dynamics-aware model to enhance robot manipulation. However, given the relatively small amount of available robot data, directly fitting data without considering the relationship between visual observations and actions could lead to suboptimal data utilization. To this end, we propose VidMan (Video Diffusion for Robot Manipulation), a novel framework that employs a two-stage training mechanism inspired by dual-process theory from neuroscience to enhance stability and improve data utilization efficiency. Specifically, in the first stage, VidMan is pre-trained on the Open X-Embodiment dataset (OXE) for predicting future visual trajectories in a video denoising diffusion manner, enabling the model to develop a long horizontal awareness of the environment’s dynamics. In the second stage, a flexible yet effective layer-wise self-attention adapter is introduced to transform VidMan into an efficient inverse dynamics model that predicts action modulated by the implicit dynamics knowledge via parameter sharing. Our VidMan framework outperforms state-of-the-art baseline model GR-1 on the CALVIN benchmark, achieving a 11.7% relative improvement, and demonstrates over 9% precision gains on the OXE small-scale dataset. These results provide compelling evidence that world models can significantly enhance the precision of robot action prediction. Codes and models will be public.","In the rapidly advancing field of robotics, accurately predicting and executing precise actions based on sensory inputs is crucial. While traditional approaches [1, 2, 3, 4, 5] for robot manipulation often rely on labor-intensive hand-engineered features and models prone to errors, data-driven methods [6, 7, 8] offer promising solutions. However, the challenge lies in the difficulty and cost of acquiring high-quality robotic data. Recent advancements [9, 10, 11, 12], particularly those utilizing large-scale online video data for learning a video generator, demonstrate significant potential in comprehending complex physical dynamics of the real world. These models, trained on diverse datasets [13, 8], possess a nuanced understanding of the world, suggesting the feasibility of amalgamating and leveraging varied robot visual trajectory data [14, 15, 16] to develop a unified dynamics-aware model for enhanced robot manipulation. Yet, achieving this unification poses challenges; merely fitting data without considering the relationship between visual observations and actions could lead to suboptimal utilization of the data. Hence, there is a pressing need to develop efficient training mechanism and model architecture that can effectively leverage existing cross-robot and cross-scene data to enhance action prediction accuracy. As shown in Fig. 1, to optimize the utilization of diverse robot data [8], we draw upon insights from neuroscience’s dual process theory [17, 18, 19], which unveils the complex mechanisms of information processing and decision-making in the human brain. This theory distinguishes between two cognitive processes: System 1, responsible for rapid, intuitive responses based on immediate sensory inputs, and System 2, which involves slower, long-horizon planning grounded in abstract concepts and understanding of world dynamics [20]. Inspired by these insights, we adopt a two-stage paradigm for robot learning, exemplified by our innovative framework, terms as VidMan (Video diffusion for robot Manipulation). VidMan leverages the power of the video diffusion generation method Open-Sora [21] for robot imitation learning, tapping into the awareness of long-horizon dynamics inherent in video diffusion models to achieve more nuanced and dynamics-modulated robot action prediction. By learning different facets of data at distinct stages, VidMan acquires an inductive bias of inverse dynamics of robot control, wherein actions are the outcomes of state sequences, significantly enhancing the method’s generalization performance, especially under scenarios with limited data. Figure 1: VidMan’s two-stage training paradigm mirrors dual process theory: its first stage (like System 2) pre-trains on understanding environment dynamics through video diffusion, forming a foundation for accurate action prediction, while its second stage (like System 1) was adapted from the first stage to leverage the learned dynamics knowledge for rapid, low-level action inference. Specifically, our VidMan employs a two-stage training mechanism, akin to the principles of dual process theory, to enhance stability and significantly improve data utilization: 1) In the first stage, namely the Dynamics-aware Visionary Stage, VidMan undergoes pre-training on the Open X-Embodiment [8] dataset (OXE) using a video diffusion model to predict future trajectories based on historical observations and language instructions. This stage involves the robot learning the dynamics of state transitions from data and accurately perceiving the current environmental state, enabling the model to develop a deep understanding of the environment’s dynamics, forming a robust foundation for accurate action prediction; 2) In the second stage, dubbed the Dynamics-modulated Action Stage, VidMan incorporates a flexible yet powerful layer-wise self-attention adapter [22] to seamlessly integrate the pre-trained knowledge from the first stage into action prediction. Through shared neural architecture and parameters with the dynamics-aware visionary stage, this phase transforms VidMan into an implicit inverse dynamics model that infers dynamics-modulated actions without explicitly generating future visual trajectories, rendering it suitable for real-world robot control scenarios. The performance of VidMan has been evaluated against SOTA baselines on the CALVIN [14] benchmark, where it achieves a 11.7% relative improvement. In addition, VidMan has shown notable effectiveness on the OXE small-scale dataset, achieving over 9% precision gains. This improvement is particularly evident when the data from the target robot is small, underscoring the effective data utilization of our method. Extensive ablation studies have been conducted to analyze the effects of various design decisions within our method. These experimental results suggest that VidMan represents a meaningful advancement in robotics, providing a valuable tool for developing more capable and responsive robotic systems."
https://arxiv.org/html/2411.09151v1,Mono2Stereo: Monocular Knowledge Transfer for Enhanced Stereo Matching,"The generalization and performance of stereo matching networks are limited due to the domain gap of the existing synthetic datasets and the sparseness of GT labels in the real datasets. In contrast, monocular depth estimation has achieved significant advancements, benefiting from large-scale depth datasets and self-supervised strategies. To bridge the performance gap between monocular depth estimation and stereo matching, we propose leveraging monocular knowledge transfer to enhance stereo matching, namely Mono2Stereo. We introduce knowledge transfer with a two-stage training process, comprising synthetic data pre-training and real-world data fine-tuning. In the pre-training stage, we design a data generation pipeline that synthesizes stereo training data from monocular images. This pipeline utilizes monocular depth for warping and novel view synthesis and employs our proposed Edge-Aware (EA) inpainting module to fill in missing contents in the generated images. In the fine-tuning stage, we introduce a Sparse-to-Dense Knowledge Distillation (S2DKD) strategy encouraging the distributions of predictions to align with dense monocular depths. This strategy mitigates issues with edge blurring in sparse real-world labels and enhances overall consistency. Experimental results demonstrate that our pre-trained model exhibits strong zero-shot generalization capabilities. Furthermore, domain-specific fine-tuning using our pre-trained model and S2DKD strategy significantly increments in-domain performance. The code will be made available soon.","Stereo matching is one of the fundamental problems in computer vision, which aims to get the disparity of two input images. It is crucial for many downstream problems, such as robotics (Zhang et al. 2015), autonomous driving (Orb 2017; Guo et al. 2024a), and augmented reality (Yang et al. 2019a). With the development of deep learning methods, learning-based methods (Chang and Chen 2018; Shen, Dai, and Rao 2021; Xu et al. 2023) have shown dominant performance, but heavily rely on the quality and diversity of the training data. Existing methods mainly use synthetic data and real data for training. However, due to the limitation of the blending engine, most of the synthetic data (Mayer et al. 2016) only contains indoor scenes causing domain gaps between the synthetic data and real-world data, since most of the stereo matching scenarios are outdoor scenes. Besides, the mainstream outdoor stereo datasets (Geiger, Lenz, and Urtasun 2012; Menze and Geiger 2015) are acquired using LiDAR, which is both expensive and time-consuming, sometimes with alignment problems. Because of the certain camera principle and limited view range of the LiDAR camera, the real-world ground-truth labels are usually sparse and incomplete (Guo et al. 2024b), causing insufficient supervising signal during fine-tuning limiting the further improvement of the stereo matching model. Some studies (Yang et al. 2019b; Yao et al. 2020; Liang et al. 2023; Guo et al. 2024b) have tried to boost the generalization and performance of the model by adding more simulated data and real data. Others (Xu et al. 2023; Lipson, Teed, and Deng 2021) improve networks by introducing advanced modules. However, these methods do not solve the most fundamental problems, which are synthetic data domain gap and real-world label sparseness. To further solve these problems of stereo matching models, we turn our attention to monocular depth estimation, as shown in Figure 1. Recently, lots of researchers have proposed powerful fundamental models in monocular depth estimation like MiDaS (Ranftl et al. 2022), Megadepth (Li and Snavely 2018), and DepthAnything (Yang et al. 2024), which perform outstanding details and generalization in various datasets. Since their depth estimation output is inverse depth, which has a strong connection with disparities, the great performance of those depth estimation models inspires us to migrate their knowledge to the stereo-matching models to boost their generalization and performance. To achieve enhanced generalization and performance, we introduce the Mono2Stereo framework, leveraging the strengths of a powerful monocular depth estimation model (MDM). It is utilized during the two-stage training process of stereo matching, comprising synthetic data pre-training and real-world data fine-tuning. Specifically, we first build a stereo training pair generation pipeline to get synthetic training data of real-world scenarios as a pre-training dataset. During generation, we use monocular depth estimation for forward warping for novel view synthesis, then we introduce an Edge-Aware (EA) inpainting module to generate the missing parts in the novel view image. In the second stage, we introduce a Sparse-to-Dense Knowledge Distillation (S2DKD) strategy to further transfer knowledge from the monocular model. Leveraging the powerful relative depth estimation capability of the monocular depth estimation network, our knowledge distillation strategy focuses on supplementing the missing information in sparse labels, such as detail and out-of-view information. To sum up, we make the following contributions: • We propose a stereo data generation framework that utilizes monocular images with an edge-aware inpainting module, which generates highly realistic stereo data for training deep stereo networks. • We propose a Sparse-to-Dense Knowledge Distillation (S2DKD) strategy, which enhances edge detail from monocular depth when fine-tuning on sparse labels. • Our method emphasizes the importance of monocular depth for deep stereo networks through extensive experiments. Models trained using our approach achieve state-of-the-art results across various datasets."
https://arxiv.org/html/2411.09145v2,"UniHOI: Learning Fast, Dense and Generalizable 4D Reconstruction for Egocentric Hand Object Interaction Videos","Egocentric Hand Object Interaction (HOI) videos provide valuable insights into human interactions with the physical world, attracting growing interest from the computer vision and robotics communities. A key task in fully understanding the geometry and dynamics of HOI scenes is dense pointclouds sequence reconstruction. However, the inherent motion of both hands and the camera makes this challenging. Current methods often rely on time-consuming test-time optimization, making them impractical for reconstructing internet-scale videos. To address this, we introduce UniHOI, a model that unifies the estimation of all variables necessary for dense 4D reconstruction, including camera intrinsic, camera poses, and video depth, for egocentric HOI scene in a fast feed-forward manner. We end-to-end optimize all these variables to improve their consistency in 3D space. Furthermore, our model could be trained solely on large-scale monocular video datasets, overcoming the limitation of scarce labeled HOI data. We evaluate UniHOI with both in-domain and zero-shot generalization setting, surpassing all baselines in pointclouds sequence reconstruction and long-term 3D scene flow recovery. UniHOI is the first approach to offer fast, dense, and generalizable monocular egocentric HOI scene reconstruction in the presence of motion. Code and trained model will be released in the future.","Egocentric Hand Object Interaction (HOI) videos capture a vast amount of knowledge about human interaction with the physical world, particularly in tool usage. Due to this rich source of interaction knowledge, egocentric human videos have gained increasing interest from both the research community (e.g., computer vision [91, 51] and robotics [44, 87]) and industry (e.g., virtual reality [47]). Figure 1: We propose UniHOI, a generalizable model that unifies camera intrinsic, procrustes-alignment confidence maps (for camera poses estimation), camera poses, and video depth estimation for fast and dense 4D reconstruction of egocentric HOI scene. To better understand the geometry and dynamics in these activities, a crucial task is the dense 4D reconstruction from internet-scale egocentric HOI video datasets [15, 21]. Here, dense 4D reconstruction is represented as a pointclouds sequence which captures the 3D position of every pixel in each frame within a global coordinate system [77, 90], preserving maximal geometry details. This task requires: (1) fast processing speeds to support large-scale reconstruction, (2) dense per-pixel reconstruction, (3) strong generalization to adapt to unseen HOI scenes, and (4) the ability to handle dynamic motions in HOI videos. However, current methods do not meet these demands. Traditional methods [57, 70], such as Structure-from-Motion (SfM) or SLAM system combined with dense depth estimation [49, 86], face significant challenges in reconstructing dynamic scenes. These multi-step approaches also suffer from accumulated errors and inconsistencies between modules [77, 74]. Recent methods have focused on time-consuming test-time optimization techniques [89, 29, 32, 40], like Gaussian Splatting [26], but their speed is impractical for reconstructing large-scale egocentric datasets [21, 38]. To address these challenges, we aim to explore fast, dense and generalizable reconstruction of highly dynamic egocentric HOI scenes. Our key insight is to simultaneously predict all variables necessary for dense 4D reconstruction with a unified model (Figure 1), including camera intrinsic, camera poses and video depth [64, 37]. This paradigm enables fast, streaming feed-forward inference, achieving speeds 30x faster than the current test-time optimization methods [68, 75, 88]. We optimize the prediction of all variables together by directly supervising the reconstructed pointclouds sequence in 3D space. This end-to-end optimization enhances scale and geometry consistency across all variables, mitigating the accumulated errors and inconsistencies in previous multi-step methods, resulting in improved reconstruction outcomes [77]. One challenge in training HOI scene reconstruction model is the limited availability of datasets with pointclouds sequence labels [31, 76]. To overcome this, we train our model purely on large-scale monocular video datasets by leveraging priors from foundation vision models. Specifically, we (1) regularize the pointclouds sequence with pre-computed per-frame monocular scene reconstruction results [49]; (2) ensure inter-frame consistency via 3D flow [83] and tracking [24] with a designed scale-agnostic loss. We transform camera pose estimation into a dense procrustes-alignment confidence map prediction problem [63, 64] and further (3) regularize the confidence map with predicted HOI masks from off-the-shelf model [91]. By training on large-scale, unlabeled mixed datasets [15, 20, 38, 31, 34], our model achieves zero-shot generalization ability. We evaluate our model on both in-domain and zero-shot unseen scenes. UniHOI successfully recovers the 3D structure of scenes and the motion of dynamic parts, even in challenging synthetic surgery HOI videos [76]. We also perform a quantitative comparison of our model with other methods that have near-linear time complexity (to the number of frames), focusing on two fundamental 4D tasks: dense pointclouds sequence reconstruction [64, 77] and long-term 3D scene flow recovery [30, 87, 3]. UniHOI demonstrates strong performance on evaluation metrics and outperforms all other baseline methods. In conclusion, our main contributions are as follows: • We propose UniHOI, a model that unifies camera intrinsic, camera poses and video depth estimation for fast, dense and generalizable 4D reconstruction of egocentric HOI scene. • We train our model solely on large-scale, unlabeled monocular HOI datasets, which helps alleviate the issue of labeled data scarcity. • UniHOI demonstrates promising performance in both in-domain and zero-shot unseen scenes, surpassing all baselines in pointclouds sequence reconstruction and long-term 3D scene flow recovery tasks. Figure 2: The overview of our UniHOI framework. The model first simultaneously predicts camera intrinsic, video depth, and procrustes-alignment confidence maps (for camera pose estimation). Camera poses are then calculated by aligning unprojected pointclouds from different frames with predicted confidence maps. The final dense pointclouds sequence reconstruction is assembled using all the predicted variables. We train our model purely on unlabeled HOI video datasets, leveraging foundation vision priors from pretrained EgoHOS [91], UniDepth [49], GMFlow [83], and CoTracker [24] as training supervision."
https://arxiv.org/html/2411.09140v1,Adversarial Vessel-Unveiling Semi-Supervised Segmentation for Retinopathy of Prematurity Diagnosis,"Accurate segmentation of retinal images plays a crucial role in aiding ophthalmologists in diagnosing retinopathy of prematurity (ROP) and assessing its severity. However, due to their underdeveloped, thinner vessels, manual annotation in infant fundus images is very complex, and this presents challenges for fully-supervised learning. To address the scarcity of annotations, we propose a semi-supervised segmentation framework designed to advance ROP studies without the need for extensive manual vessel annotation. Unlike previous methods that rely solely on limited labeled data, our approach leverages teacher-student learning by integrating two powerful components: an uncertainty-weighted vessel-unveiling module and domain adversarial learning. The vessel-unveiling module helps the model effectively reveal obscured and hard-to-detect vessel structures, while adversarial training aligns feature representations across different domains, ensuring robust and generalizable vessel segmentations. We validate our approach on public datasets (CHASEDB, STARE) and an in-house ROP dataset, demonstrating its superior performance across multiple evaluation metrics. Additionally, we extend the model’s utility to a downstream task of ROP multi-stage classification, where vessel masks extracted by our segmentation model improve diagnostic accuracy. The promising results in classification underscore the model’s potential for clinical application, particularly in early-stage ROP diagnosis and intervention. Overall, our work offers a scalable solution for leveraging unlabeled data in pediatric ophthalmology, opening new avenues for biomarker discovery and clinical research.","Retinopathy of prematurity (ROP) is one of the leading causes of vision impairment and blindness in preterm infants. It occurs when abnormal blood vessels develop in the retina, which is the layer of tissue at the back of the eye that is sensitive to light [34]. Fundus images are often used to diagnose the presence and severity of ROP since they provide information about the structure and function of capillaries [1, 2]. Being a vascular disease, ROP diagnosis heavily depends on vessel morphology and distribution, in particular, vessel tortuosity, vessel coverage, and the progression of neovascularization, which refer to the twisting and turning of blood vessels, the extent to which blood vessels cover the retina, and the growth of abnormal blood vessels, respectively. Therefore, vessel extraction is a crucial step for ROP study. Manual annotation of vessels from ROP fundus images is extremely challenging; images of preterm babies are characterized by their underdeveloped retinal vessels and choroidal vessels underneath retinal pigment epithelium. Previous studies trained automatic vessel segmentation models for ROP with proprietary datasets [35, 1, 9] with high-quality annotation, but such models are not available to the public. Therefore, leveraging publicly available labeled data remains essential for effectively segmenting unlabeled ROP data. Figure 1: The domain difference among publicly available datasets (CHASEDB [3] & STARE [24]) and our ROP dataset. CHASEDB primarily consists of images from adolescents and healthy populations, while STARE comprises images from adults and emphasizes heterogeneous pathology cases. In contrast, our ROP dataset is curated for infants’ retinopathy of prematurity detection. We introduce a semi-supervised learning (SSL) approach to address the ROP vessel segmentation problem, effectively leveraging both publicly labeled datasets and unlabeled ROP data. SSL has shown promises in various medical imaging applications [26, 12, 14], including retinal vessel segmentation [13, 11]. Compared to other SSL techniques [47, 32, 31, 28], we used a teacher-student network, where the student learns more aggressively from both labeled and unlabeled data to explore new features, while the teacher focuses solely on the unlabeled target data, evolving conservatively through an Exponential Moving Average (EMA) of the student’s weights. This targeted knowledge transfer enhances ROP vessel extraction. Despite advancements in teacher-student networks, segmenting ROP images remains challenging due to both external and internal factors. The external challenge stems from the significant distribution gap between public datasets and ROP images. Public datasets [3, 24] differ significantly from ROP images in terms of age population and disease diversity. These datasets mostly consist of images from adolescents and adults with prevalent conditions like diabetic retinopathy and glaucoma [3, 24, 8, 4, 5, 6, 7], where the vessels are thicker and have better contrast with the background. In contrast, ROP images from preterm infants have underdeveloped retinas, and exhibit a more diffuse, white-ish tissue appearance with thinner, less developed vessels, as shown in Figure 1. The internal challenge arises from the high variability in ROP imaging conditions. While vessels near the optic disc (OD) are clearer, image quality deteriorates further from the OD, and there is substantial variability in ROP images due to the diverse developmental stages of preterm infants. Considering these challenges and the lack of ROP image annotations, applying the conventional SSL approach would struggle to exploit the available labeled data from public datasets fully. The unique characteristics of ROP images, such as the underdeveloped retinal structures, already complicate the segmentation process. Unsupervised methods, like domain adaptation, may also overlook valuable segmentation cues from labeled public data, which are crucial in guiding the model without ROP labels. Therefore, to mitigate the external challenge, we incorporated a domain adversarial framework into our semi-supervised learning. Domain adversarial techniques have demonstrated promising improvements [37], [38], including in medical imaging [39], and particularly in retinal images [40], [41]. We use an adversarial discriminator to align the latent feature representations from both domains, ensuring they remain informative for vessel segmentation while reducing the domain gap by aligning feature distributions between the two domains through adversarial training. This feature alignment helps reduce irrelevant attributes such as color and background variations, improving the vessel extraction performance on the unlabeled ROP domain. This allows us to balance domain generalization and specificity, extracting meaningful vessel patterns across both domains. For the internal challenge, we introduce an uncertainty-weighted vessel-unveiling module that applies an uncertainty map to enhance hidden or obscured retinal vessels in ROP images, particularly in areas with variable imaging conditions. This module maximizes vessel coverage, which is critical for accurate ROP diagnosis. This paper proposes an adversarial feature alignment-based uncertainty-aware vessel-unveiling SSL method. To the best of our knowledge, this is the first attempt to combine adversarial feature alignment in SSL with a vessel-uncertainty map to enhance vessel visibility and ensure comprehensive vessel extraction for ROP segmentation. In our work, we make four three contributions: • We propose the first to apply semi-supervised learning to ROP vessel segmentation, utilizing publicly available labeled data alongside unlabeled ROP images to enhance segmentation accuracy. • We propose a teacher-student network combined with an adversarial feature alignment discriminator to address the domain gap between public datasets and ROP images for vessel segmentation. • We introduce an uncertainty-aware vessel-unveiling module that enhances underdeveloped and obscured vessels, improving segmentation performance in ROP images. • This is the first study to segment all four stages of ROP images without ground-truth vessel annotations and to use the segmented masks in a downstream task, emphasizing adaptability to real-world clinical tasks."
https://arxiv.org/html/2411.09126v1,SCAN: Bootstrapping Contrastive Pre-training for Data Efficiency,"While contrastive pre-training is widely employed, its data efficiency problem has remained relatively under-explored thus far. Existing methods often rely on static coreset selection algorithms to pre-identify important data for training. However, this static nature renders them unable to dynamically track the data usefulness throughout pre-training, leading to subpar pre-trained models. To address this challenge, our paper introduces a novel dynamic bootstrapping dataset pruning method. It involves pruning data preparation followed by dataset mutation operations, both of which undergo iterative and dynamic updates. We apply this method to two prevalent contrastive pre-training frameworks: CLIP and MoCo, representing vision-language and vision-centric domains, respectively. In particular, we individually pre-train seven CLIP models on two large-scale image-text pair datasets, and two MoCo models on the ImageNet dataset, resulting in a total of 16 pre-trained models. With a data pruning rate of 30-35% across all 16 models, our method exhibits only marginal performance degradation (less than 1% on average) compared to corresponding models trained on the full dataset counterparts across various downstream datasets, and also surpasses several baselines with a large performance margin. Additionally, the byproduct from our method, i.e. coresets derived from the original datasets after pre-training, also demonstrates significant superiority in terms of downstream performance over other static coreset selection approaches. Code is released at https://github.com/guoyang9/SCAN.","Large models are heavily data-driven, particularly in the realm of pre-training [10, 11, 49]. This paradigm has been widely underpinned by the scaling law [25, 28], which suggests that more data often lead to reduced generalization errors. However, using large quantities of data frequently results in a notable increase in carbon footprints. Addressing this pressing issue requires substantial efforts to optimize the data training efficiency. Figure 1: The interplay between training data size and model downstream performance of base model CLIP, our method SCAN, and two SoTA baselines. This paper delves into the data efficiency problem for contrastive pre-training. Despite the pervasiveness of contrastive pre-training across both vision-centric [10, 11] and vision-language [27, 49] domains, nevertheless, the data efficiency issue has received scant attention in the existing literature. We attribute the reason for this fact to two challenges. I- Absence of reliable labels for self-supervised learning objectives. Unlike in supervised learning, where explicit labels aid in class prediction, self-supervised learning in contrastive pre-training operates without such guidance, making it unable to estimate the class probability of data samples such as EL2N [47]. II- Extensive data scale due to easy accessibility, e.g. the interleaved image-text data from the web [49]. Current datasets usually comprise millions [7, 54] or even billions of samples [52]. It is thus intractable in time for methods employing gradients [47] or second derivative (Influence Functions) [29] to evaluate data usefulness individually. Recent approaches in the vision-language area have resorted to coreset selection algorithms [42] for a reduced pre-training dataset beforehand [1, 40, 41, 63, 66]. The crux of these methods lies in the semantic match/duplication that is quantified by some proxy metrics like CLIP matching score [49]. Consequently, a subset, namely a coreset, of the original dataset is filtered for pre-training from scratch. Our motivation for this work is inspired by the advancement in dynamic sparse training (DST) [45, 71, 72], which dynamically prunes less influential learnable weights from models. Compared to its static sparse training counterparts [32, 59], DST demonstrates notable strengths in performance, robustness, and model compression without the need for over-parameterization [37, 45]. Intriguingly, we recognize that recent coreset selection algorithms predominantly adhere to the static approach, akin to the fixed weight masks employed in static sparse networks [32]. As a result, we argue that these coreset-based dataset pruning methods are subject to similar limitations as previous static sparse training ones, albeit with a shift in application scope from learnable weights to individual data samples. Given this context, approaching the dataset pruning challenge can be easily decomposed into two sub-problems: 1)- metric identification and 2)- pruning strategy design. Specifically, the proxy metric should meet several conditions: dynamic adaptability, quick obtainability (with minimal additional cost), and reflecting the learning status of each sample. Regarding the pruning strategy, we introduce a novel data bootstrapping algorithm named SCAN. Instead of employing a consistent pruning ratio throughout training, our SCAN approach identifies and eliminates data from less important subsets in a bootstrapping manner. These two operations from our SCAN method are performed iteratively for stable pre-training. We validate the effectiveness of the proposed method with widely used contrastive pre-training frameworks in both vision-language (CLIP [26, 49]) and vision-centric (MoCo [10, 11]) domains. The pre-training datasets for CLIP include CC3M [54], MSCOCO [36], SBU-Captions [46], and CC12M [7], forming two groups of datasets with different scales. On the other hand, we pre-train MoCo models using the ImageNet Dataset [13]. Moreover, we employ various downstream datasets, including ImageNet [13], CIFAR-10, and CIFAR-100 [30], along with out-of-distribution datasets such as ImageNet-R [23] and ImageNet V2 [50]. Our evaluation protocols encompass full fine-tuning, linear probing, and zero-shot testing on ImageNet. Within the CLIP framework, we evaluate seven models covering ResNet [21], ViT [14], and Swin Transformer [38]. As for MoCo, due to resource constraints, we use two popular ViT models [60] in the experiments. Our experimental results, partially depicted in Fig. 1, exhibit that SCAN achieves a significant trade-off between training data size and downstream model performance as compared to several baselines [1, 48, 69]. We include the code in the supplementary material for the reproduction of our results. To the best of our knowledge, we are the first to comprehensively study the data efficiency problem within the context of contrastive pre-training. Our work not only introduces an effective bootstrapping approach but also is able to produce a static coreset (a smaller dataset) that outperforms other static coreset selection methods [1, 69] by a large performance margin on diverse downstream image classification datasets. These contributions enable our work to hold a positive promise for the efficient utilization of data in contrastive pre-training, thereby potentially reducing more computational overhead and carbon footprints."
https://arxiv.org/html/2411.09105v1,VCBench: A Controllable Benchmark for Symbolic and Abstract Challenges in Video Cognition,"Recent advancements in Large Video-Language Models (LVLMs) have led to the development of benchmarks aimed at assessing cognitive abilities. However, most existing benchmarks heavily rely on web-collected videos paired with human annotations or model-generated questions, which limit control over the video content and fall short in evaluating advanced cognitive abilities involving symbolic and abstract concepts. To address these limitations, we introduce VCBench, a controllable benchmark to assess LVLMs’ cognitive abilities, involving symbolic and abstract elements at varying difficulty levels. By generating video data with the Python-based engine, VCBench allows for precise control over the video content, creating dynamic, task-oriented videos that feature complex scenes and abstract concepts. Each task is paired with customized question templates tailored to specific cognitive challenges, providing a rigorous evaluation test. Our evaluation reveals that even state-of-the-art (SOTA) models, such as Qwen2-VL-72B, struggle with simple video cognition tasks involving abstract concepts, with performance sharply dropping by 19% as video complexity rises. These findings reveal the current limitations of LVLMs in advanced cognitive tasks and highlight the critical role of VCBench in driving research toward more robust and generalized LVLMs for complex video cognition challenges.","With the rapid advancement of artificial intelligence (AI), large video-language models (LVLMs) have emerged as essential tools for video understanding [24, 63, 29, 23, 60, 45]. To unlock their full potential, LVLMs must exhibit cognitive abilities that approximate human-like perception and reasoning [47, 18]. While recent evaluations of large language models (LLMs) and image-based models have increasingly focused on advanced cognition [41, 9, 7], current benchmarks for video-based models [62, 36, 5, 11, 26, 14, 25] continue to overlook advanced cognitive abilities fundamental to both humans and AI. Furthermore, existing benchmarks largely focus on video domain and duration [14, 51, 25, 26], without precise control over video content and difficulty, resulting in limited assessment of cognitive depth required for advanced video understanding. Constructing benchmarks from real-world videos also poses challenges, including intensive prompt engineering, manual annotation, and data filtering [16, 5, 36, 25, 32, 38, 53], along with the risk of data leakage where video content may inadvertently be included in model training [58]. To address these limitations, we introduce VCBench, a controllable Video Cognitive Benchmark designed to evaluate the cognitive capabilities of LVLMs. VCBench offers a fully programmatic approach to synthetic videos, allowing precise control over both content and complexity to evaluate the cognitive abilities of LVLMs. The video scenes integrate symbolic elements and abstract concepts, containing symbolic objects, abstract attributes (color, shape, and size), abstract actions (action type, action speed, and direction), and spatial (2D and 3D) and temporal relationships across easy-to-difficult video scenarios. For each video scene, we carefully design specific question templates with GPT4 to evaluate multiple key dimensions of video cognition. These dimensions include Object Perception [42], Action Perception [22], Spatial Reasoning [34, 44], Temporal Reasoning [35], and comprehension within gaming and full-modal environments [37, 43, 10]. Table 1 presents a comparative analysis of VCbench alongside existing benchmarks. Through extensive testing on LVLMs fine-tuned with video question-answer tasks, VCBench provides a detailed evaluation that reveals previously overlooked insights. While some models perform similarly on simple videos, a clear performance gap becomes evident as video difficulty rises to medium and difficult levels. Notably, even advanced LVLMs, such as Qwen2-VL-72B, struggle with advanced cognition of simple videos with symbolic elements, sharply declining performance as complexity rises. Specifically, Qwen2-VL-72B shows a 7% performance drop in the ‘Sky Battle’ scene with five added enemy planes and a further 10% decline at the highest difficulty. These findings highlight the urgent need for developing LVLMs with advanced cognition, especially for handling abstract and complex video scenarios, thereby setting a foundation for future research. Figure 1: In the Sky Battle scene, symbolic icons represent planes, bullets, and enemies, with ‘destroy’ depicted as an abstract action within the video. The difficulty level is adjusted by varying the number and speed of enemy symbols, with questions such as ‘How many enemy planes were destroyed by the player?’."
https://arxiv.org/html/2411.09101v1,Heuristical Comparison of Vision Transformers Against Convolutional Neural Networks for Semantic Segmentation on Remote Sensing Imagery,"Vision Transformers (ViT) have recently brought a new wave of research in the field of computer vision. These models have done particularly well in the field of image classification and segmentation. Research on semantic and instance segmentation has emerged to accelerate with the inception of the new architecture, with over 80% of the top 20 benchmarks for the iSAID dataset being either based on the ViT architecture or the attention mechanism behind its success. This paper focuses on the heuristic comparison of three key factors of using (or not using) ViT for semantic segmentation of remote sensing aerial images on the iSAID. The experimental results observed during the course of the research were under the scrutinization of the following objectives: 1. Use of weighted fused loss function for the maximum mean Intersection over Union (mIoU) score, Dice score, and minimization or conservation of entropy or class representation, 2. Comparison of transfer learning on Meta’s MaskFormer, a ViT-based semantic segmentation model, against generic UNet Convolutional Neural networks (CNNs) judged over mIoU, Dice scores, training efficiency, and inference time, and 3. What do we lose for what we gain? i.e., the comparison of the two models against current state-of-art segmentation models. We show the use of the novel combined weighted loss function significantly boosts the CNN model’s performance capacities as compared to transfer learning the ViT. The code for this implementation can be found on https://github.com/ashimdahal/ViT-vs-CNN-Image-Segmentation.","The introduction of Transformers [23] has changed the research landscape when it comes to attention mechanisms on Natural Language Processing (NLP) tasks. However, this potential wasn’t fully capitalized in computer vision tasks until recently when Dosovitskiy et al. implemented the attention mechanism of transformers in their seminal paper [5] ever since ViT has been one of the fundamental areas of research in computer vision. Although initially proposed for image classification tasks, like CNNs in their inception days, they soon turned out to be one of the best-performing architectures for image segmentation tasks as well. Image segmentation refers to the task of classifying each pixel of an image into a category. It can be said that image segmentation is a type of classification as well, but the role, approach and method of each subjects varies in themselves. In specific to the scope of this paper, semantic segmentation means to group certain objects in an image to a class among the given n classes in the dataset. Instance segmentation, similarly, would be to segment each object present in an image as a distinct item in itself. As with any deep learning computer vision tasks, image segmentations were best done by models that were capable of capturing, encoding, and decoding essential patterns of the input image, mainly the implementation of UNet style architectures in CNNs [31, 37, 1, 36, 19, 33]. Specific to the scope of this paper, in the past few years researchers have utilized the UNet CNNs in the iSAID dataset[32], which is built on top of the DOTA[29] dataset, to do semantic segmentations [18, 38, 25, 4]. One of the main pitfalls of such datasets is the background class[11]. If not handled properly during the training process, there is a good chance to see high evaluation metrics on mIoU since the model would easily overfit on the most common class, which is the unlabelled class in this case. Although the traditional deep learning technique of employing a UNet-based Convolutional Neural Network (CNN) remains the cornerstone of many segmentation tasks, recent trends in computer vision research indicate a significant shift towards Transformer-based architectures, particularly the Vision Transformer (ViT) and its variants. This shift is driven by the inherent ability of Transformer models to capture long-range dependencies through self-attention mechanisms, a feature that CNNs typically struggle with due to their localized receptive fields. The increasing preference for ViT models is exemplified by the fact that among the top 20 benchmark models for the iSAID dataset as listed on Papers with Code[paper_with_code], the top five employ either ViT, attention-based CNNs, or a hybrid combination of both [7, 8, 6, 9]. These models leverage the powerful self-attention mechanism to refine segmentation masks by focusing on relevant image regions. In this paper, we introduce a novel loss function that integrates maximizing mean Intersection over Union (mIoU) and Dice score, while preserving entropy to ensure robust mask predictions. This new loss function is integrated into the UNet framework to improve its ability to model complex spatial relationships in images. This unique formulation allows for better generalization to unseen data by maintaining a balance between maximizing overlap with the ground truth masks and preventing over-segmentation. In addition, we investigate various data augmentation techniques since the number of samples is relatively lower in the iSAID[32] dataset. In parallel, we provide a direct comparison between training a UNet CNN model from scratch and fine-tuning Meta AI’s widely adopted MaskFormer[2], a ViT-based model that has achieved state-of-the-art results in semantic segmentation tasks; opensourced in Hugging Face[26]. This paper focuses on three key objectives throughout its experimentation and analysis: • Propose a combined weighted loss function to maximize mIoU and Dice while preserving entropy • Analyze the impact on efficiency during training and inference in both architectures • Benchmark and test inference capabilities of both models against current state-of-the-art iSAID benchmarks on unseen data The rest of the paper is laid out in the following order, section II contains the current state of art and previous work on the field. Section III discusses our approach to the given objectives, then IV effectively binds the results with the research question. Section V would then conclude the paper with some ending thoughts and future direction for the research field."
https://arxiv.org/html/2411.09077v1,Drone Detection using Deep Neural Networks Trained on Pure Synthetic Data,"Drone detection has benefited from improvements in deep neural networks, but like many other applications, suffers from the availability of accurate data for training. Synthetic data provides a potential for low-cost data generation and has been shown to improve data availability and quality. However, models trained on synthetic datasets need to prove their ability to perform on real-world data, known as the problem of sim-to-real transferability. Here, we present a drone detection Faster-RCNN model trained on a purely synthetic dataset that transfers to real-world data. We found that it achieves an AP_{50} of 97.0% when evaluated on the MAV-Vid - a real dataset of flying drones - compared with 97.8% for an equivalent model trained on real-world data. Our results show that using synthetic data for drone detection has the potential to reduce data collection costs and improve labelling quality. These findings could be a starting point for more elaborate synthetic drone datasets. For example, realistic recreations of specific scenarios could de-risk the dataset generation of safety-critical applications such as the detection of drones at airports. Further, synthetic data may enable reliable drone detection systems, which could benefit other areas, such as unmanned traffic management systems. The code111The code is available https://github.com/mazqtpopx/cranfield-synthetic-drone-detection and datasets222The datasets are available https://huggingface.co/datasets/mazqtpopx/cranfield-synthetic-drone-detection are available.","Drone intrusions pose a risk to airports. In 2018, a reported drone sighting disrupted the operations at the Gatwick airport, UK, for 3 days. Since then, drones have been used for other illicit purposes, such as smuggling drugs into prisons [1, 2]. Although a serious disruption, such as the one in Gatwick, has not occurred since, reports of drone sightings at UK airports persist [3, 4, 5]. To prevent this, the UK Government presented a counter-unmanned aircraft strategy [6]. It sets the detection of drones as a key objective. On the other hand, UAS traffic management (UTM) is a relatively new area of research. It is concerned with integrating drones and air taxis with the current air traffic management (ATM). For UTM to work in urban environments, camera systems will likely have to work alongside RF/radar sensors due to interference from buildings. Accurate detection systems will enable the use of authorized drones for approved unmanned operations, whilst preventing unauthorized drones from flying in excluded parts of the airspace. Drones can be detected using different sensors such as radar, radio frequency (RF), acoustic, and electro-optical/infrared [7]. State-of-the-art detection methods typically fuse some of these sensors. Our research focuses on drone detection using a visible spectrum camera. Recent advances in neural networks (NNs) allow for much more accurate detection, tracking, and classification of drones in images or video feeds. However, accurate data for training the NN models remains difficult to obtain. In this article, we explore the use of structured domain randomization (SDR) within a simulated environment to generate a synthetic dataset of drones. We test whether a synthetic dataset generated using this method is generalizable to real-world datasets, by comparing the results to an established benchmark. I-A Literature We focus on the optical detection of drones using NNs trained on synthetic data. Although object detection in images has been extensively studied, drone detection poses unique challenges, usually caused by environmental, or systematic issues. For example, weather effects impact the image, the drones can be small in the video frame making them hard to detect or classify, and the movement of UAVs is complicated to predict. Classification of the objects in the sky such as drones and birds is also a key issue. We investigate the literature on NNs, the use of synthetic data for training NNs, applications of NNs for drone detection, and finally, the use of synthetic data for drone detection. I-A1 DNN advances and architectures Recent advances in deep neural network (DNN) architectures (e.g. AlexNet [8]) became the state-of-the-art method for object recognition. They were later expanded to detect object bounding boxes, segment objects, and track them across videos. Object detection architectures comprise of single-stage and two-stage detectors. Examples of one-stage detectors are YOLO [9], [10] and SSD [11]. Examples of two-stage detectors are Fast-RCNN [12], Faster-RCNN [13]. One-stage detectors do the detection/classification directly from the extracted features, without a separate region proposal. Two-stage detectors generate region proposals (i.e. regions of the image where it thinks an object might exist) and then classify each of the proposed regions. In theory, two-stage detectors should produce better accuracy at a higher computational cost than one-stage detectors. However, in practice, they tend to vary for different applications and are sensitive to training parameters. An example of this is Isaac-Medina et al. [14], where the results of different architectures vary across different datasets. More recently, transformers [15] which use an attention mechanism and an encoder-decoder architecture, showed improvements in performance over traditional RNNs and CNNs as they are more parallelizable. This led to detection architectures such as DETR [16]. DETR matches Faster R-CNN in terms of performance on the COCO object detection dataset. I-A2 Synthetic Data, Sim-to-Real, and Domain Randomization Accurate data for training neural networks is expensive and may be hard to obtain. Creating real-life images or videos has a cost associated with it, which varies depending on the application. After the data is recorded, it needs to be manually labelled with the ground truth by a human. Synthetic data offers an automated alternative. By using 3D models and rendering software, synthetic images along with accurate ground truth labels can be generated. This presents a research question: can images rendered by 3D software be used to train NN models and transferred to real-world applications? The use of synthetic data comes with advantages and disadvantages. The primary advantage is the potential reduction in the time cost required to generate the dataset. An algorithm can generate a very large dataset with no human input required. The primary disadvantage is that NN models trained on synthetic data are hard to transfer to real-world problems. Hence, the primary objective of synthetic data research is bridging the sim-to-real gap - the transfer of a NN model trained on synthetic data to perform equivalently on real-world data. However, machine learning algorithms used to train neural networks rely on the training and evaluation datasets coming from the same source distribution. We are fundamentally breaking this assumption by using synthetic data for training and evaluating on real-world data. This is referred to as the out-of-distribution generalization [17]. There exist multiple strategies for achieving out-of-distribution generalization such as transfer learning [18], domain adaptation [19], and domain generalization [20]. Each of these relies on different assumptions between the training data, access to test data, and training conditions. To achieve sim-to-real transfer, domain generalization is the most relevant to our problem, as we assume that we cannot see the real-world test data during training. For the problem of sim-to-real for images, the most popular method of domain generalization is domain randomization. Domain randomization (DR) is a method of introducing model generalizability. This is done by randomizing parameters used to synthesize the source distribution, with the goal of making the data distribution so wide and varied that the model trained on this dataset is transferable to the target distribution. Lighting, textures, poses, and other parameters are randomized. Popular approaches aim to make the scenes as unrealistic as possible. Tobin at al. [21] is an example, which employs DR by randomizing the textures of the objects to make the network invariant to changes in scene conditions. Trembley et al. [22] use DR to bridge the sim-to-reality gap for object detection of cars for an autonomous driving application. Their approach employs DR by inputting random shapes, textures, and cars in unrealistic positions, with the aim of generating more variety to focus the neural networks on the structure of the object of interest. Prakash et al. [23] propose the use of structured domain randomization (SDR) - a variant of domain randomization, which takes into account the context of the scene. Hence instead of generating random scenes with unrealistic textures and objects, it aims to generate realistic scenes, while randomizing other parameters in a structured way. They find that SDR provides performance improvements over DR. Borrego et al. [24] propose the use of synthetic DR datasets as an alternative to pre-trained networks. Alghonaim & Johns [25] investigate the parameters used to randomize the simulation such as camera position, target colour, and lighting variations to find the most important parameters for sim-to-real transfer. They found that more high-quality realistic images improved the performance of sim-to-real. This comes in contrast with some of the earlier works (Tobin et al. [21] and Trembley et al. [22]) which focused on unrealistic, low-quality generated images. They also found that the randomization of distractor objects and background textures was important for generalising to new environments. Hinterstoisser et al. [26] propose a ’trick’ to freeze the layers responsible for feature extraction of a NN trained on real-world images, and train only the remaining layers on synthetic images. They show that their approach works well on architectures such as Faster-RCNN, and Mask-RCNN. Hence, there appear two schools of thought. DR produces unrealistic images with the aim of expanding the domain to allow the NN to better learn the features in settings that might not necessarily be used during test time. SDR produces realistic images with the aim of making the training data similar to the real-world data used at test time. I-B Visual Drone Detection Visual drone detection is the process of finding the position of a drone in images or video feeds produced by visible-spectrum cameras. It has become a popular area of research, largely driven by the recent advances in machine learning. Most approaches [27, 28, 29, 30] typically use some of the detection architectures described in section I-A1, or a modification of the network for drone detection purposes, trained and tested on one of the publicly available drone datasets, or on a private dataset. Isaac-Medina et al. [14] present a benchmark of multiple DNN algorithms (SSD, YOLOv3, Faster R-CNN, and DETR) on publicly available datasets of flying drones (MAV-VID, Drone-vs-Bird, Anti-UAV). They present two benchmarks: the accuracy of detection, and the accuracy of tracking algorithms. They present the results of each of the models trained and tested on each of the datasets. Note that they create three distinct sets of weights for each of the datasets. We were able to independently reproduce the test metrics shown by Isaac-Medina et al. based on their published model weights for the Faster-RCNN model (although, we did not attempt to reproduce the training of the models). Freudenmann et al. [31] explore the effects of data augmentation strategies on the performance of their NN models. They set up their experiments by training a Faster R-CNN model on the Drone vs Bird dataset. They then test their NN model on other datasets: a subset of the Drone vs Bird dataset, Anti-UAV dataset, background test dataset (images of drones scraped from the internet), Disturbing Objects dataset (to find false positives), and New UAV Types (unseen UAV images scraped from the internet). They find that by test a model trained on the DvB dataset, on the Anti-UAV dataset, the accuracy drops significantly. The authors propose the cause to be the mosaics and artefacts found in the Anti-UAV dataset. To reduce this effect, the authors propose the use of mosaic + standard dataset transforms. This increases the mAP to 78.7% from 54.2% with no augmentations. The augmentations appear to train the model to reduce the false positive rate. Still, this falls short of the results presented by Isaac-Medina et al. where the Faster R-CNN model trained on the Anti-UAV dataset and tested on the subset of the Anti-UAV dataset achieves 97.7%. This highlights the challenge of transferring models trained on one dataset to other datasets - a drone detection model trained on one dataset is not guaranteed to perform equivalently on another dataset. Mediavilla et al. [32] present a benchmark of Swin, YOLOv4, CenterNet, CenterNet2, and Faster-RCNN architectures on CACHOW - a helicopter dataset - and Drone vs Bird datasets. However, the results are not directly comparable with Isaac-Medina et al. as the test configurations are different. Other approaches consider the temporal dimension. Thai et al. [33] present a spatio-temporal NN design, in which multiple frames of the video feed are stacked together before being input into the NN. They show that this approach is more accurate, but comes with a higher computational cost as more data needs to be processed. Craye and Ardjoune [34] use U-Net to segment the position of the drone, classify it, and then use a spatio-temporal filter. Sangam et al. [35] present a spatio-temporal transformer for drone-to-drone detection. I-C Synthetic Data for Drone Detection Synthetic data approaches have been attempted in the field of drone detection. One of the first attempts that we were able to find is Rozantsev et al. [36] which uses domain randomization techniques (without referring to them as such - the paper was published before domain randomization became a common term in literature), such as randomizing the position of the drones, varying the motion blur, adding random noise, and randomizing object material properties. They also repeat this process for aircraft and cars. Marez et al. [37] create a synthetic dataset of UAVs using DR and test it on the Drone vs Bird dataset. They create a baseline model (trained only on DvB) and compare the models trained only on the DR datasets, and models pre-trained on the DR datasets and finetuned on real-world data. They find that DR-only datasets perform the worst, while the finetuned models perform better but still fall short of the baseline. This highlights the challenge of transferring synthetically trained models to real-world datasets. Peng et al. [38] use 3D models of drones and random HDRIs to generate a photorealistic synthetic dataset, and train a Faster-RCNN model. It is tested on a real-life dataset of images downloaded from the internet. They find that using a pre-trained model improves their performance significantly, and using occluded drone images improves their performance by 1%. However, their test dataset of images downloaded from the internet might not be representative of a real-world security scenario. We were also unable to find a copy of their test dataset online which makes it hard to directly compare the results to. Dieter et al. [39] attempt to bridge the sim-to-real gap by generating a synthetic dataset in Unreal Engine. They test their models on synthetic and real data. Overall, they find that, generally, models trained on synthetic data fall short in terms of the performance of the models trained on real-world data. However, by using a small share of real-world data, they improve the results and get close to the results produced by using real-world data only. DronePose [40] uses 3D models of a DJI Mavic, and a DJI Inspire, to create a synthetic dataset using Unreal Engine. They also create a neural network based on a U-Net architecture, to segment parts of the drone, find the orientation, and identify the drone model. The use of GANs to generate a synthetic dataset was attempted by Li et al. [41]. This is done to better understand the deep feature space of how drones are represented by neural networks. Further, they use Topological Data Analysis to acquire missing data. This study looks at something that not many other studies consider - how neural networks learn to represent drones within the latent space. Carrio et al. [42] use synthetic images to predict drones using depth maps for the application of obstacle detection of avoidance. By using a synthetic environment, they are able to create accurate ground truth depth maps, which they later use to train a NN. I-D Outro - Contribution and Outline This work is largely a continuation of Wisniewski et al. [43, 44]. It expands the method of synthetic data generation to object detectors, which can detect the position of the drone in each of the frames. Isaac-Medina et al. created a UAV detection benchmark, in which they compare common object detection architectures. This benchmark is used as a baseline to compare the results from our models trained on synthetic data. We present a method of systematically generating a synthetic dataset of drones alongside pixel-accurate ground truth segmentation masks using a form of structured domain randomization. This presents an advantage over real-world datasets which are costly to annotate and may not be as accurate. Further, synthetic datasets allow flexibility over standard approaches such as being able to add new drones to the dataset using only a 3D model. They also allow for the creation of complex scenarios that may be hard to record in real life such as forest fires, or drones flying in stormy conditions. Generally, drones should not be flown in windy conditions in real life and this might make creating a research study case hard due to safety concerns. The challenge with this approach is that the synthetic dataset may not be representative of the real-world data, and a model trained on this dataset might not translate to real-world datasets - this is the sim-to-real problem of transferring domains, as explored in section I-A2. We present and compare multiple datasets which are generated by processes of SDR and DR. The reasons for focusing on the dataset are: • The publicly available drone datasets vary in quality. Because they use human-labeled ground truth boxes, these are imperfect at times. They are also limited in that they do not have the segmentation ground truth. • Some of the previously publicly available datasets are no longer available. • New real-world datasets are costly to produce. These items make researching drone detection and comparing results between studies challenging. We address these points by releasing our synthetic dataset. The contributions of this publication are: • Multiple synthetic datasets with pixel-accurate segmentation masks, generated by the process of structured domain randomization. • A study into the ability of the neural network model trained on synthetic data to generalize to real-world drone detection scenarios. This differs from SOTA in the review because the synthetic drone detection applications mixed real-world data with their synthetic data for the training to achieve sim-to-real. They also didn’t use open-access datasets such as Drone-vs-Bird or Anti-UAV. • Study of different domain randomization styles. • Generalizable Faster R-CNN model weights for drone detection; trained purely on synthetic images, tested on multiple real-life datasets. Through our literature review, we have identified several faults within some of the research in the drone detection area that we attempted to remedy during our study: • Use of private datasets. Using private datasets makes it impossible for other researchers to reproduce results. To remedy this, we open-source our dataset, and we use publicly available datasets for testing. • Lack of a common baseline to compare the results to. This is a common problem which makes it hard to directly compare the results of one method over another. To remedy this, we use the results of the Isaac-Medina et al. benchmark to compare our results. • Repeatability of results. Many publications present the results as a single value. However, due to the randomness of neural network training, retraining yields different results. To remedy this, we present our results as a mean with a 95 per cent confidence interval. • Reproducibility of results. A lot of research in this area is hard to reproduce. To remedy this, we open-source333Datasets https://huggingface.co/datasets/mazqtpopx/cranfield-synthetic-drone-detection, and source code https://github.com/mazqtpopx/cranfield-synthetic-drone-detection our synthetic dataset, as well as the code used for training and testing of the NN models. We believe that this is one of the first attempts to study the use of structured domain randomization for the generation of synthetic drone datasets. Further, we perform a study comparing different domain randomization techniques to find out how much they influence drone detection. Although we have found attempts in the literature to use domain randomization techniques for drone detection, we were unable to find systematic studies to quantify which aspects of domain randomization improve the results of drone detection. To the best of our knowledge, previous attempts to test the sim-to-real transferability used private real-world datasets. Instead, we directly compare our results to the benchmark presented by Isaac-Medina et al. [14] which uses publicly available datasets. We present a successful sim-to-real transfer for the application of drone detection. We prove that our NN model trained on a purely synthetic dataset successfully transfers to the MAV-Vid dataset. Our model achieves a mean of 97.0% compared with the Isaac-Medina et al. model, which was trained on the MAV-Vid dataset, of 97.8%. This paper is divided into five sections. In section II we explain the methodology. This includes explaining the rendering process used to generate the datasets. We explain different styles of datasets generated using different domain randomization styles. We describe the NN training process, we explore the use of data augmentations, in particular noise and JPEG compression, and finally, we explain the testing procedure, to measure the accuracy of sim-to-real transfer. In section III we explain the results. We perform tests on camera bounds randomization, effects of data augmentations, and variation in domain randomization styles, and lastly, we compare our results to the literature. In section IV we conclude the findings. In section V we discuss further work."
https://arxiv.org/html/2411.09062v1,Multimodal Object Detection using Depth and Image Data for Manufacturing Parts,"Manufacturing requires reliable object detection methods for precise picking and handling of diverse types of manufacturing parts and components. Traditional object detection methods utilize either only 2D images from cameras or 3D data from lidars or similar 3D sensors. However, each of these sensors have weaknesses and limitations. Cameras do not have depth perception and 3D sensors typically do not carry color information. These weaknesses can undermine the reliability and robustness of industrial manufacturing systems. To address these challenges, this work proposes a multi-sensor system combining an red-green-blue (RGB) camera and a 3D point cloud sensor. The two sensors are calibrated for precise alignment of the multimodal data captured from the two hardware devices. A novel multimodal object detection method is developed to process both RGB and depth data. This object detector is based on the Faster R-CNN baseline that was originally designed to process only camera images. The results show that the multimodal model significantly outperforms the depth-only and RGB-only baselines on established object detection metrics. More specifically, the multimodal model improves mAP by 13% and raises Mean Precision by 11.8% in comparison to the RGB-only baseline. Compared to the depth-only baseline, it improves mAP by 78% and raises Mean Precision by 57%. Hence, this method facilitates more reliable and robust object detection in service to smart manufacturing applications.","Рис. 1: Overview of the RGBD-Man multimodal object detection framework. Employing Artificial Intelligence (AI) for automation of manufacturing has resulted in increased efficiency, precision, and flexibility and created a pardigm shift in the design of manufacturing systems. AI has been successfully applied to a vast array of manufacturing tasks in the industry [1, 2]. While AI-based methods have improved the manufacturing process, there are still challenges in ensuring that the AI-based black box systems continue to be reliable and robust. The most fundamental layer underlying all smart manufacturing systems is object detection, which allows the system to identify the type and position of the objects that it needs to handle. Object detection is a established computer vision problem, which involves identifying and categorizing specific objects of interest within a larger image by placing a bounding box around each detected object [3]. An effective automation system also requires proper sensor design to provide adequate coverage over the environment and allow the system to properly observe the scene and the objects [4]. The past decades have seen great advancement in sensor hardware. Camera resolutions have increased and they have become more affordable at the same time. Similarly, consumer applications have facilitated mass manufacturing of 3D sensors like lidars and stereo cameras, which are great sensors for smart manufacturing. Despite these improvements, sensors have inherent limitations rooted in their physics. For example, a single image captured by camera does not carry depth information. 3D sensors capture point cloud data which addresses this issue, but these sensors are typically low resolution and do not provide color information. An effective and reliable automation system requires selecting the right sensors and an object detection system that can effectively ingest the data provided by these sensors. Prior work has shown the limitations of object detection systems that rely solely on cameras [5, 6, 7, 8]. Image distortions like blur and noise can significantly lower the detection accuracy [5]. Although cameras provide color information, there are environments where there is low contrast between objects and the background, and as a result detection accuracy may suffer [9] and cameras may not be enough for handling these environments. Moreover, camera-based object detection systems are sensitive to illumination and can be fragile if there are changes to the lighting conditions. It has also been shown that illumination can negatively affect [6] camera-based object detection systems, because they struggle to generalize to operate under lighting conditions different from what they have experienced during training. Similarly, the performance of these systems diminishes in scenarios where objects vary significantly in size or when they blend indistinguishable with the background in camera’s view [7]. While 3D sensors are less sensitive to lighting, they pose their own set of challenges when used in industrial environments [10]. As an example, object detection systems using 3D sensors often struggle when the scene contains densely arranged objects [11]. In addition, presence of objects with similar shapes and sizes, or objects with repetitive patterns cause difficulties for systems relying on point clouds for object detection [12]. Another challenge with using point cloud data is the extra complexity of modeling 3D information and the slower speed of processing 3D information. This is a significant obstacle, particularly for manufacturing environments which require real-time object detection and therefore cannot afford too much complexity and computational overhead [13, 14]. These conditions emphasize the need for advanced detection techniques that can take advantage of 3D sensors for improved performance while keeping the system simple and computationally efficient. Since different sensors can be complementary and cover each other’s blind spots and weaknesses, it makes sense to create object detectors which can leverage the strengths of multiple sensors to improve accuracy and dependability rather than relying on a single sensor. Multimodal object detection [15, 16], which utilizes data from multiple sensor modalities has the potential to address the above-mentioned limitations of single-sensor systems. Integrating information from multiple sensors can be achieved via the sensor fusion process [17], which can produce more accurate and more reliable data for the object detection model. Different types of sensor fusion have been studied [18] in the past. Early fusion methods merge sensor data at the input stage. Intermediate fusion methods combine features derived from different sensors at an intermediate layer of a model. Lastly, late fusion approaches aggregate decisions proposed from different sensors at the final stage in the model. All types of sensor fusion can contribute to overcoming the limitations of single-sensor systems. Prior work has explored multimodal object detection. One notable approach [19] added a depth branch to the Faster-RCNN architecture to process depth in parallel with the RGB data. The depth branch created feature maps from depth inputs and these feature maps were concatenated with feature maps generated from RGB images. The authors showed that using depth allowed the model to succeed in some scenarios where color information alone could not distinguish objects from their backgrounds [19]. Similarly, Zhu et al. [20] added a depth processing branch to the Faster R-CNN framework which enabled their model to handle both RGB and depth images for enhanced object detection. Their method employed two distinct CNNs to extract features from RGB and depth images independently. They also used depth information to delineate object edges more clearly and distinguish them from their backgrounds. Following feature extraction, the features were aligned and merged using a feature fusion layer, which implemented sum fusion, max fusion, and concatenation fusion strategies. Garbouge et al. [21] presented an approach that integrated RGB and depth data at an early stage by stacking them into a four-channel input for a CNN inspired by the AlexNet architecture for a classification task and showed that employing depth information improves object classification metrics. However, none of the existing methods have explored efficient four-channel RGB+D inputs in the context of object detection tasks. The processing of RGB and depth data through a single shared backbone offers several advantages. Firstly, when RGB and depth data are stacked at the input level, the model is able to efficiently extract features that merge information from both modalities, for example color and texture from RGB alongside spatial and structural details from depth. This integrated approach enables the network to learn more meaningful features early in the feature extraction process. By processing both data types simultaneously, the network can better generalize to unseen data as it learns to recognize and interpret patterns across both modalities. Additionally, employing a single backbone for processing reduces the computational overhead significantly. This computational efficiency comes from managing only one set of weights during back-propagation and inference. This approach creates a leaner model which is faster and less costly to run in a manufacturing plant. Lastly, the lower architectural complexity and lack of complex fusion mechanisms simplifies the development and tuning process for training and deploying the final model in the manufacturing environment. This work introduces a novel method that employs early sensor fusion, where data from RGB images and 3D point clouds are integrated at the data level before being used by the detection algorithm. This method utilizes the combined strengths of both modalities for a more comprehensive representation of the scene that enhances object detection capabilities. Early fusion facilitates the extraction of comprehensive features that embody both the visual and spatial attributes of objects, which addresses the challenges posed by occlusions, variable sizes, and complex background information more effectively than late fusion or single-sensor methods. Hence, this work presents a novel model tailored for processing RGB-D images for object detection tasks in manufacturing. The rest of the paper is organized as follows. Section 2 discusses the methodology of the multimodal object detection framework. Section 3 outlines experiments which compare this approach to both RBG-only and Depth-only object detection of a NIST manufacturing task board. Section 4 discusses the experiment results, and Section 5 discusses the conclusions and suggestions for future work."
https://arxiv.org/html/2411.09037v1,A Transformer-Based Visual Piano Transcription Algorithm,"Automatic music transcription (AMT) for musical performances is a long standing problem in the field of Music Information Retrieval (MIR). Visual piano transcription (VPT) is a multimodal subproblem of AMT which focuses on extracting a symbolic representation of a piano performance from visual information only (e.g., from a top-down video of the piano keyboard). Inspired by the success of Transformers for audio-based AMT, as well as their recent successes in other computer vision tasks, in this paper we present a Transformer based architecture for VPT. The proposed VPT system combines a piano bounding box detection model with an onset and pitch detection model, allowing our system to perform well in more naturalistic conditions like imperfect image crops around the piano and slightly tilted images.","Automatic music transcription (AMT) aims to generate a symbolic music representation given some input, most commonly an audio signal[4]. The generated symbolic music representation is often inspired by MIDI, with notes being represented by their start time (onset), release time (offset), pitch, and loudness (velocity). An AMT algorithm therefore aims to predict one or more of these features from the given input. Various MIR tasks such as music search (indexing, recommendation), automatic music accompaniment, and music generation benefit from the symbolic music representation given by an AMT algorithm[4, 23]. High-quality AMT systems also enable the creation of new datasets[29, 16] which can be used for a variety of MIR tasks. Although audio based methods have been the primary focus throughout past research into AMT, video based visual transcription methods have also been considered. Especially in the case of automatic piano transcription, a top-down video can provide a significant amount of information on the musical performance, with the ability to see note onset, offset and pitch directly from the key being pressed and released. Videos with this setup are a popular way to showcase piano related content on platforms such as YouTube.111See e.g., https://www.youtube.com/watch?v=xspj9uE5TQQ Research into using video for AMT has shown that it can act as a supplement to audio based methods, helping the audio based model deal with harmonics[27] and improving the performance of the overall system in cases where the audio data may have distortion[15]. It can additionally be useful for problems such as video-audio synchronization and in cases where there is no audio for a musical performance. Past research utilizing machine learning for visual piano transcription (VPT) has employed convolutional neural networks (CNNs) for the task[28, 15, 27]. Recently, the Video Transformer[3] has been shown to be competitive with state of the art CNN’s in computer vision benchmarks. This is leading to a shift in the computer vision space from CNN’s to Transformers. As the main contribution of this paper, we propose a Transformer based VPT system, utilizing a VideoMAE[26] model for onset/pitch detection, as well as a YOLOv8[13] model for automatic piano bounding box detection. We present the rational behind key choices made during the design of the system, including the results of our experimentation into the trade-off between model precision and recall, as well as what image pre-processing techniques we found worked best for our model. Lastly, we showcase the overall performance of our model. The rest of this work is structured as follows: Sec. 2 looks at related works, discussing similar uses for Transformers in audio based AMT, as well as past works in VPT and other similar multi-modal computer vision tasks where Transformers have been used. Sec. 3 explains our approach and goes into detail on the architecture of our proposed system. Sec. 4 contains information on the datasets we use, how we pre-process the data, the general experimental setup used when training, and how we evaluate our model. Sec. 5 showcases the performance of our bounding box and onset/pitch detection models. We also present experiments looking into the effect of different positive class weights on performance metrics. Sec. 6 discusses insights gained from our experiments. Finally, Sec. 7 concludes the paper."
https://arxiv.org/html/2411.09023v1,CoMiX: Cross-Modal Fusion with Deformable Convolutions for HSI-X Semantic Segmentation,"Pixel-wise semantic segmentation of hyperspectral images (HSIs) enables detailed and accurate analysis of complex scenes, benefiting various applications. Improving HSI semantic segmentation by exploiting complementary information from a supplementary data type (here denoted as X-modality) is promising but challenging due to differences in imaging sensors, image content, and resolution. Current cross-model fusion techniques have limitations in enhancing modality-specific and modality-shared information, as well as in capturing dynamic information interactions and fusion between different modalities. In response, this study proposes CoMiX, a cross-modal fusion framework with deformable convolutions (DCNs) for pixel-wise HSI-X semantic segmentation. CoMiX is an asymmetric encoder-decoder architecture designed to extract, calibrate, and fuse information from HSI and X modalities. Its pipeline includes an encoder with two parallel and interacting backbones and a lightweight all-multilayer perceptron (ALL-MLP) decoder. Within the encoder, four stages are deployed, each incorporating 2D DCN blocks for the X-model to adapt to geometric variations, and 3D DCN blocks for HSIs to adaptively aggregate spatial-spectral features. Additionally, each stage includes a Cross-Modality Feature enhancement and eXchange (CMFeX) module and a feature fusion module (FFM). CMFeX is designed to exploit spatial and spectral correlations from both modalities to recalibrate and enhance modality-specific and modality-shared features while adaptively exchanging complementary information between them. Outputs from CMFeX are fed into the FFM for fusion and passed to the next stage for further information learning. Finally, the outputs from each FFM are integrated by the ALL-MLP decoder for final prediction. Extensive experiments demonstrate that our CoMiX achieves superior performance and generalizes well to various multimodal recognition tasks. The CoMiX code will be released.","Hyperspectral images (HSIs), with continuous spectral information over a wide range of wavelengths, enable detailed analysis and discrimination of different materials [1]. HSI semantic segmentation is a fundamental task in scene understanding and supports many applications [2, 3], including land-cover analysis [4], agricultural quality assessment [5], and urban development monitoring [6]. It is closely related to HSI classification as both tasks predict the category at the pixel-level rather than at the image-level [7]. However, using single HSI data has reached its performance bottleneck, especially in complex scenes where classes exhibit similar spectral signatures. Multimodal data fusion techniques address these challenges by integrating information from a supplementary sensor type (here called X-modality), such as digital surface models (DSM), light detection and ranging (LiDAR), and synthetic aperture radar (SAR), to provide complementary information to HSIs. For example, DSM offers elevation information, LiDAR provides accurate 3D spatial information, and SAR captures structural information about the Earth’s surface [8]. Numerous multimodal fusion approaches [9, 10] have been developed, typically classified into three main types: pixel-level fusion [11], feature-level fusion [12], and decision-level fusion [13]. Pixel-level fusion combines data by averaging or concatenating along the input channels, often requiring alignment and registration of images from different modalities. Although effective in some scenarios, pixel-level fusion faces challenges in handling differences in image resolution and characteristics. Feature-level fusion employs individual encoders for each modality to capture the related features, which are then fused to enhance information representation. Decision-level fusion combines results generated independently by each modality to make a final decision. It is often used in scenarios where individual modalities excel in different aspects of a task, and their combined decisions lead to improved performance. Among them, feature-level fusion has emerged as one of the most promising approach due to its scalability and effectiveness in integrating information from multiple modalities. For example, Rasti et al. [12] adopted extinction profiles (EPs) [14] to automatically extract spatial and elevation information from HSI and LiDAR data, respectively, which was subsequently fused to generate classification maps. The shared and specific feature learning model, S2FL [8], gathers information from HSI and another modality by decomposing modality-shared components on the latent manifold subspace while separating modality-specific components. Furthermore, many follow-up studies [15, 16] have progressively introduced more sophisticated strategies [17, 18] to improve the integration of information across different modalities. Despite offering valuable solutions, these traditional fusion techniques still face inherent drawbacks, such as the requirement for expert feature engineering and limitations in feature representation and generalization. Deep learning (DL), an automatic feature learning technique, has demonstrated overwhelming superiority in vision tasks [19, 20, 21, 22, 23]. Among DL algorithms, convolutional neural networks (CNNs) are preferred for their scalability, flexibility and ease of optimization [24]. Numerous CNN-based studies have been developed for multi-source remote sensing image learning and fusion. Chen et al. [25] employed two-branch CNNs to extract features from multispectral image (MSI)/HSI and LiDAR data, which are fused through fully connected (FC) layers. An FC-dominated encoder–decoder network called EndNet [26] extracts information from different modalities and combines them by forcing the fused features to reconstruct the multimodal input. Ge et al. [27] extracted EPs and local binary patterns (LBPs) from HSI and LiDAR data and fused them using a deep residual fusion framework. The unified multimodal fusion framework presented in [28], developed as a baseline for complex scenes, includes extraction and fusion subnetworks, with the latter incorporating a novel cross-fusion strategy that outperforms concatenation-based fusion. In contrast to these approaches, Fusion-FCN, the winner of the 2018 IEEE Data Fusion Contest (DFC), was built with a fully convolutional network (FCN) [18], demonstrating the significant potential of FCNs in multimodal semantic segmentation. Despite these advances, CNNs treat input content uniformly, which is impractical given the different contributions of inputs to identification [29]. Several efforts [30, 31] have been made to address the equal treatment of convolution kernels by introducing attention mechanisms to guide models on “where” and “what” to focus. Wang et al. [32] and Zhang et al. [33] applied attention modules to enhance optical and LiDAR feature learning, respectively. Coupled adversarial learning-based classification (CALC) [34] introduced a spatial attention module for adversarial feature learning to capture high-order semantic information from HSI and LiDAR data. Some studies [35, 36, 37] developed cross-modal attention modules for multimodal information fusion. Xue et al. [38] introduced self-attention modules to simultaneously enhance feature learning and fusion of HSI and LiDAR data. Roy et al. [39] developed a cross hyperspectral and LiDAR attention, where LiDAR patch tokens serve as queries and HSI patch tokens serve as keys and values. Taking advantage of the adaptive spatial aggregation and long-range dependencies inherent in self-attention, the vision transformer (ViT) [40] was developed and further refined and scaled in subsequent studies [41, 42]. With the advent of transformers, multimodal data fusion has also witnessed a significant progress. The multimodal fusion transformer (MFT) [43] and the deep hierarchical vision transformer [44] were developed to learn heterogeneous information from HSI and LiDAR data, followed by a cross-attention module for heterogeneous feature fusion. Similarly, the local information interaction transformer network [45], a dual-branch transformer, exploits sequence information from multimodal data, which is then fused and fed into a convolutional transformer module for classification. Although transformers have shown impressive results in multimodal information representation, their low sensitivity to local features and lack of the inductive biases inherent in CNNs limit their performance. Therefore, some hybrid networks [35, 46, 47] have been developed to combine the strengths of CNNs and transformers to overcome these limitations. For instance, the local–global transformer network, i.e., GLT-Net, [46] deploys multiscale convolution blocks in shallow layers to learn local spatial features, incorporates transformer modules in deep layers to capture global spectral features, and then performs global–local information fusion for prediction. Similarly, Fusion_HCT [48] sequentially employs CNN and transformer blocks to learn HSI and LiDAR information in parallel, followed by a cross-token attention fusion module for information fusion. Alternatively, some studies [49, 50] in computer vision have attempted to introduce long-range dependencies into CNNs by using convolutions with large dense kernels (e.g., 31\times 31). However, these large kernel CNNs are still inferior in performance to state-of-the-art transformers [51]. Recently, Mamba [52] has been introduced as an efficient alternative to transformers for encoding sequential features in a linear fashion. Nonetheless, the inherent 1-D nature of its selective scanning technique poses challenges when applied to 2D or higher-dimensional visual data, potentially resulting in the loss of crucial spatial information. Furthermore, the deformable convolution network (DCN) series [51, 53, 54] innovatively combines the strengths of CNNs and transformers. DCNs achieve adaptive spatial aggregation by incorporating learnable offsets into convolutional kernels. These offsets allow the network to dynamically adjust and learn appropriate receptive fields (local- or long-range) according to the input data. Additionally, the convolutional kernels used in DCN remain small, typically 3\times 3, avoiding the optimization challenges and high computational cost [51]. Although the aforementioned studies have achieved improvements in feature extraction or multimodal data fusion by combining the advantages of CNNs and transformers from different perspectives, they still have important limitations: 1) Most of the aforementioned approaches are predominantly designed for specific modalities, such as HSI and LiDAR data fusion. However, a general and modality-agnostic multimodal architecture for HSI-X semantic segmentation is highly desirable to facilitate robust real-world scene understanding. Such an architecture streamlines research efforts by eliminating the need to optimize for specific modality combinations, while allowing seamless integration of new sensors as they emerge. 2) Existing multimodal semantic segmentation methods often process HSIs similarly to 2D X-modal, overlooking that HSIs are 3D spatial-spectral data. This limits the comprehensive use of the rich spectral information inherent in HSIs, thus compromising the integration of discriminative information. Therefore, HSI-X semantic segmentation architectures should not only accommodate various X modalities but also effectively exploit the abundant spectral information in HSIs, while maintaining computational efficiency for processing 3D data. 3) Although significant progress has been achieved in HSI-X information fusion, directly concatenating or adding the features from two modalities [26, 33, 34, 46, 55] or simply assembling information from two modalities through a cross-modality fusion module [35, 45, 56] may lead to inferior performance. Hence, it is important to design a cross-modality fusion technique that can effectively identify modality-specific, modality-shared, and complementary features, and integrate them into an efficient representation. In this work, we explore an alternative paradigm for designing a universal modality-agnostic HSI-X semantic segmentation framework, called CoMiX. The backbone of CoMiX incorporates deformable convolutions with custom block-level designs inspired by transformers. This integration facilitates the development of 3D DCN blocks and 2D DCN blocks for adaptive extraction of modality-specific features from HSI and X data. As illustrated in Fig. 1, CoMiX consists of an encoder with two parallel and interactive backbones, and a lightweight all-multilayer perceptron (ALL-MLP) decoder. Within the encoder, the 3D DCN blocks and 2D DCN blocks are tailored to adaptively extract modality-specific features from HSI and X data, respectively. Additionally, a Cross-Modality Feature enhancement and eXchange (CMFeX) module is introduced at each stage to recalibrate modality-specific feature responses and promote cross-modality feature interaction across spatial and spectral dimensions. The enhanced HSI- and X-modality features are then passed to the feature fusion module (FFM) for cross-modality information fusion, while being fed to the next stage for further information learning. Finally, the All-MLP decoder aggregates the fused features from each stage for prediction. Figure 1: Overview of the proposed CoMiX framework for HSI-X semantic segmentation. The main contributions of this study are as follows. 1) Proposing a universal modality-agnostic HSI-X semantic segmentation framework, CoMiX, that is specifically tailored to extract modality-specific, modality-shared, and complementary features from HSI and X data while advancing the cross-modality information fusion. 2) Developing two parallel and interactive backbones with deformable convolution, where 3D DCN blocks and 2D DCN blocks are designed for modality-specific feature learning from HSI and X data, respectively. 3) Designing the CMFeX module to recalibrate and highlight modality-specific and modality-shared information, while facilitating the exchange of complementary information between them in a more effective and efficient manner. The combination of CMFeX and FFM further enhances cross-modality information fusion. The remainder of this study is organized as follows. Section II details the proposed CoMiX framework. Section III presents the experimental setup and results. Section IV focuses on the network analysis. Finally, concluding remarks and future work are presented in Section V."
https://arxiv.org/html/2411.09018v1,"Bridging the Visual Gap: Fine-Tuning Multimodal Models 
with Knowledge-Adapted Captions","Recent research increasingly focuses on training vision-language models (VLMs) with long, detailed image captions. However, small-scale VLMs often struggle to balance the richness of these captions with the risk of hallucinating content during fine-tuning. In this paper, we explore how well VLMs adapt to such captions. To quantify caption quality, we propose Decomposed NLI (DNLI), an evaluation framework that breaks down generated captions into individual propositions, assessing each in isolation. This fine-grained analysis reveals a critical balance between capturing descriptive details and preventing hallucinations. Our findings show that simply reducing caption complexity or employing standard data curation techniques does not effectively resolve this issue. To tackle this challenge, we introduce Knowledge Adapted (KnowAda) fine-tuning, a data-centric approach that automatically adapts training data with the model’s existing knowledge and visual understanding. KnowAda minimizes hallucinations while preserving high descriptiveness. We validate this approach across several small-scale VLMs (up to 7B parameters) and dense caption datasets, demonstrating that KnowAda effectively balances hallucination reduction and descriptiveness. Our results show that KnowAda outperforms various baselines in both automatic metrics and human evaluations. We will release our code and models.","Fine-tuning pretrained multimodal models for generating dense image captions is common in both research and practical applications, such as assisting visually impaired individuals. Recent work has focused on creating high-quality, descriptive captions through human annotations Onoe et al. (2024); Garg et al. (2024); Deitke et al. (2024) and synthetic generations from models like GPT-4 Chen et al. (2023a) and Gemini Singla et al. (2024). These datasets help fine-tune models to produce detailed descriptions in specific styles, differing from the zero-shot captions of pretrained models. However, smaller multimodal models (e.g., up to 7 billion parameters), crucial for real-time applications, often struggle with capturing visual details during fine-tuning, leading to hallucinations. Consider a fine-tuning dataset for dense captioning, like the one in Figure 1 from DOCCI Onoe et al. (2024), alongside a pretrained vision-language model trained on tasks like captioning, VQA, and OCR. If, for instance, the model has only encountered low-resolution images during pretraining, it may struggle to identify fine details, such as the drawing on the purple van or the hotel name in the background, which require higher resolution. This issue extends beyond resolution to other visual challenges in modern VLMs Tong et al. (2024); Wu et al. (2024); Zhang et al. (2024a). We hypothesize that fine-tuning the model on overly complex captions may increase hallucinations, as the model is compelled to predict details it cannot accurately perceive or understand. Recent works on large language models (LLMs) have shown that fine-tuning primarily adapts pre-existing factual knowledge for specific tasks, with the majority of this knowledge being encoded during the pretraining phase Geva et al. (2020); Meng et al. (2022); Roberts et al. (2020). Rather than acquiring new information, fine-tuning typically activates and refines pretrained knowledge, which remains largely stable throughout the process Zhou et al. (2024b). Furthermore, Gekhman et al. (2024) demonstrated that fine-tuning on content not grounded in a model’s pre-existing factual-knowledge can lead to an increase in hallucinations. Motivated by these findings, we demonstrate that similar challenges extend beyond factual knowledge, affecting multimodal models when fine-tuned on information that is not rooted in their pretrained visual understanding. To better adapt pretrained models to dense caption datasets, we introduce KnowAda, a model-specific adaptation technique that simplifies complex details in dense captions. The KnowAda pipeline, shown in Figure 2, automatically identifies visual knowledge gaps between a pretrained VLM and an image-caption pair by generating questions related to the image’s visual content. It then modifies the captions to align with the model’s visual knowledge and capabilities, producing adapted captions that enhance fine-grained control while balancing a low hallucination rate with high descriptiveness. To evaluate dense image captions, two factors are crucial: descriptiveness, which captures the image’s details, and hallucination rate, which measures factual accuracy. Traditional metrics, which focus on similarity to reference captions Papineni et al. (2002); Banerjee and Lavie (2005); Zhang et al. (2019); Reimers (2019) or CLIP-based alignment Sarto et al. (2023); Radford et al. (2021), struggle with long captions. These metrics penalize valid variations in phrasing and miss the distinction between factual accuracy and token overlap or semantic similarity, making them inadequate for detecting hallucinations in detailed captions. Existing hallucination metrics focus on short captions or object-level errors Rohrbach et al. (2018); Ben-Kish et al. (2023). In response, we propose Decomposed NLI (DNLI) a new evaluation framework that decomposes captions into propositions and measures their entailment with the ground truth description. DNLI provides a more reliable assessment of both descriptiveness and accuracy and highly aligns with human scores. Our results show that training with KnowAda captions offers a favorable balance between descriptiveness and fidelity when fine-tuning LLaVA-7B, outperforming other data-centric baselines. To demonstrate the consistency of KnowAda across different models and datasets, we fine-tuned several multimodal models, ranging from 2 billion to 7 billion parameters, on two different dense captioning datasets. Across all models, KnowAda consistently reduced hallucinations compared to training with the original captions, as confirmed by both automatic and human evaluations. Stated explicitly, our contributions are: (I) We show that small-to-medium-scale VLMs underperform when fine-tuned directly on dense caption datasets, exhibiting increased hallucinations and reduced descriptive accuracy; (II) To address this we propose KnowAda, a model-dependent augmentation method that fills knowledge gaps in captions, reducing hallucinations while preserving high descriptive accuracy; (III) We demonstrate the effectiveness of KnowAda through extensive experiments, supported by both quantitative metrics and qualitative human evaluations; (IV) We introduce DNLI, a novel evaluation framework for dense captions that offers a more fine-grained analysis and shows strong correlation with human annotations. Figure 3: Dense captioning descriptiveness precision-recall results for LLaVA-7B fine-tuned with DOCCI captions, adapted using different methods. “Trimmed” refers to naive removal of sentences, while “Gemini” involves prompting Gemini to simplify the caption by removing difficult details of varying degrees. KnowAda consistently achieves the best precision-recall balance. The original dataset corresponds to KnowAda with a threshold of T=100\%, where no information is classified as unknown. Figure 4: DNLI Evaluation. Given a generated description by a VLM, we decompose it to atomic propositions. Then, we classify each proposition to either entailed, contradicted or neutral, conditioned on the ground-truth description. Finally, we calculate the descriptiveness and contradiction based on the number of entailed and contradicted propositions. Contradiction \downarrow Descriptiveness \uparrow Model FT Captions Precision Recall Precision Recall # Words Auto Human Auto Human Auto Human Auto Human PaliGemma Synthetic 38.9 19 30 15.5 47.9 81 39.2 67.8 72 PaliGemma Synthetic KA 32.4 18.4 20.8 13 55.2 81.6 36.4 61.2 54 TinyLLaVA Synthetic 38.1 52.1 49.1 45 35.1 47.9 26.3 40.8 71 TinyLLaVA Synthetic KA 40.2 34 40.2 22.2 48.1 66 25.4 40.9 51 LLaVA-7B Synthetic 39.1 19.3 39.1 16.2 47.3 80.7 39.7 65.3 79 LLaVA-7B Synthetic KA 31 15.8 31 9.3 58.4 84.2 34.5 47.4 54 PaliGemma Human 41.6 20.4 24.6 14.9 46.7 79.6 24.6 43.2 62 PaliGemma Human KA 38.3 18.3 22.2 13.6 49.4 81.7 28.7 38.9 65 TinyLLaVA Human 53 51.8 39.5 38.8 31.9 51.8 22.4 31.4 100 TinyLLaVA Human KA 42.6 34.5 19.6 12.5 46.9 65.5 19.1 22.5 53 LLaVA-7B Human 47.2 33.4 39.7 33.2 39.4 66.6 33.7 48.1 109 LLaVA-7B Human KA 33.7 17.1 16.7 11.2 56.9 82.9 25.8 31.8 55 Table 1: Dense captioning results over the test sets of DOCCI when fine-tuning on original human-annotated captions, synthetic captions, and KnowAda-adapted captions (denoted as KA) with a threshold of 20%. “Automatic (Auto)” refers to model-based NLI evaluation, while “Human” refers to evaluations based on human labeling."
https://arxiv.org/html/2411.09007v1,"Scale Contrastive Learning with Selective Attentions for Blind 
Image Quality Assessment","Blind image quality assessment (BIQA) serves as a fundamental task in computer vision, yet it often fails to consistently align with human subjective perception. Recent advances show that multi-scale evaluation strategies are promising due to their ability to replicate the hierarchical structure of human vision. However, the effectiveness of these strategies is limited by a lack of understanding of how different image scales influence perceived quality. This paper addresses two primary challenges: the significant redundancy of information across different scales, and the confusion caused by combining features from these scales, which may vary widely in quality. To this end, a new multi-scale BIQA framework is proposed, namely Contrast-Constrained Scale-Focused IQA Framework (CSFIQA). CSFIQA features a selective focus attention mechanism to minimize information redundancy and highlight critical quality-related information. Additionally, CSFIQA includes a scale-level contrastive learning module equipped with a noise sample matching mechanism to identify quality discrepancies across the same image content at different scales. By exploring the intrinsic relationship between image scales and the perceived quality, the proposed CSFIQA achieves leading performance on eight benchmark datasets, e.g., achieving SRCC values of 0.967 (versus 0.947 in CSIQ) and 0.905 (versus 0.876 in LIVEC).","Image quality assessment (IQA) seeks to emulate the human visual system’s ability to perceive image quality (Yang et al. 2022; Zhang et al. 2022b). It has been extensively applied in image restoration (Zhang et al. 2022a), compression (Liu et al. 2022), and generation (Wang et al. 2023), thereby enhancing the human visual experience. Based on the need for distortion-free reference images, IQA can be categorized into three types: full-reference, reduced-reference, and no-reference or blind IQA (BIQA) (Liu et al. 2024; Chahine et al. 2023; Zhang et al. 2023). Among these, BIQA methods have seen a significant increase in demand due to their broad applicability. Multi-scale methods are extensively employed in BIQA aiming at simulating the multi-scale perception characteristics of the human vision system (Guo et al. 2021). Numerous studies have validated the effectiveness of these approaches, which can be broadly classified into image-level and feature-level methods based on their strategies for extracting and fusing multi-scale features (Chen et al. 2024). Image-level methods, such as NIQE (Mittal, Soundararajan, and Bovik 2013) and MUSIQ (Ke et al. 2021a), typically create multi-scale images by directly resizing the original image, from which features are extracted, and quality scores are computed in parallel. However, it is straightforward that direct resizing can introduce additional distortions into the image. On the other hand, many existing methods focus on feature-level extraction (Golestaneh, Dadsetan, and Kitani 2022a; Xu et al. 2024), employing a pyramid structure to derive multi-scale spatial features from the same image for predicting the quality scores. Figure 1: (a) The quality-aware information for a patch at the same relative position varies significantly across different image scales. At a small scale, overexposure on the cropped image patch greatly reduces the image quality, while at a larger scale, the quality of image patch remains relatively good. (b-d) Comparison of two mainstream multi-scale paradigms with our proposal. Our method leverages cross-scale contrastive learning to distinguish the quality differences in (a). The designed feature filtering and amplification module enhances attention to quality-aware features. Despite the multi-scale BIQA methods achieving favourable results, they often struggle to accurately characterize the relationship between scale and the overall image quality. The reasons lie in two aspects: (1) Firstly, there exists a considerable degree of information redundancy across different scales, and processing this data indiscriminately can result in a significant waste of computational resources. (2) Secondly, feature-level multi-scale methods often directly concatenate features of different scales to predict quality scores. This approach can create a “visual illusio” effect, where the quality-aware information from different scales confuses the model’s ability to make an accurate assessment. This phenomenon is illustrated in Fig. 1(a) in which the same content photograph but different scales exhibit distinct quality differences. Specifically, the small-scale image contains overexposed lighting, leading to prominent low-quality areas. Consequently, the fusion of information at the scale level inevitably introduces noise into the quality perception features. To address these issues, we propose the Contrast-Constrained Scale-Focused IQA Framework (CSFIQA), an innovative methodology designed to minimize information redundancy while accurately simulating the complex relationship between scale and overall image quality. To address the redundancy of multi-scale information highlighted in Problem 1, we propose an efficient selective focus attention mechanism to filter and enhance multi-scale quality information. This mechanism includes a filtering selector and an information focuser. The filtering selector identifies and retains the most relevant self-attention values from multi-scale features, while the information focuser amplifies these retained features, thereby reducing redundancy. As for Problem 2, accurately distinguishing quality perception information at different scales is crucial for alleviating visual illusions. Correspondingly, inspired by the effectiveness of contrastive learning in differentiating samples of varying quality, we propose a contrastive learning strategy that operates on both inter-scale and intra-scale levels to enhance the model’s ability in learning from quality perception features across scales. Specifically, we construct a straightforward set of positive and negative samples based on quality labels to capture more intricate relationships between different image quality regions. At the intra-scale level, we have designed a simple yet effective adaptive noise sample matching mechanism. We identify the sample in the neighboring region of scale B with the least similar quality information as a negative class for contrastive learning for a given image at scale A, thereby effectively distinguishing those cropped patches with inconsistent quality information across different scales of the same image. We summarize our contributions: • We identify the previously overlooked phenomenon in multi-scale BIQA methods and present a comprehensive analysis of the relationship between the patch scales and the overall image quality. • We propose a selective focus attention mechanism that reduces redundancy in multi-scale information by filtering out highly correlated quality features and amplifying the retained features. • We introduce the contrastive loss metrics across multiple scales to capture intricate relationships within diverse regions of image quality. Meanwhile, we propose a straightforward yet effective adaptive noise sample matching mechanism for collecting negative samples. This mechanism efficiently distinguishes samples that exhibit inconsistent quality information across different patch scales within the same image. Extensive experiments conducted on eight IQA datasets reveal that our CSFIQA surpasses existing state-of-the-art BIQA approaches. Notably, in contrast to existing contrastive learning methods, we have identified quality inconsistencies across patch scales and introduced an adaptive noise sample matching mechanism to address these discrepancies."
https://arxiv.org/html/2411.08937v1,Dual-Head Knowledge Distillation: Enhancing Logits Utilization with an Auxiliary Head,"Traditional knowledge distillation focuses on aligning the student’s predicted probabilities with both ground-truth labels and the teacher’s predicted probabilities. However, the transition to predicted probabilities from logits would obscure certain indispensable information. To address this issue, it is intuitive to additionally introduce a logit-level loss function as a supplement to the widely used probability-level loss function, for exploiting the latent information of logits. Unfortunately, we empirically find that the amalgamation of the newly introduced logit-level loss and the previous probability-level loss will lead to performance degeneration, even trailing behind the performance of employing either loss in isolation. We attribute this phenomenon to the collapse of the classification head, which is verified by our theoretical analysis based on the neural collapse theory. Specifically, the gradients of the two loss functions exhibit contradictions in the linear classifier yet display no such conflict within the backbone. Drawing from the theoretical analysis, we propose a novel method called dual-head knowledge distillation, which partitions the linear classifier into two classification heads responsible for different losses, thereby preserving the beneficial effects of both losses on the backbone while eliminating adverse influences on the classification head. Extensive experiments validate that our method can effectively exploit the information inside the logits and achieve superior performance against state-of-the-art counterparts.","Despite the remarkable success of deep neural networks (DNNs) in various fields, it is a significant challenge to deploy these large models in lightweight terminals (e.g., mobile phones), particularly under the constraint of computational resources or the requirement of short inference time. To mitigate this problem, knowledge distillation (KD) (Hinton et al., 2015) is widely investigated, which aims to improve the performance of a small network (a.k.a. the “student”) by leveraging the expansive knowledge of a large network (a.k.a. the “teacher”) to guide the training of the student network. Traditional KD techniques focus on minimizing the disparity in the predicted probabilities between the teacher and the student, which are typically the outputs of the softmax function. Nevertheless, the transformation from logits to predictive probabilities via the softmax function may lose some underlying information. As shown in Figure 1(a), considering a 3-class classification problem, even if the teacher model outputs two different logit vectors [2,3,4] and [-2,-1,0], the softmax function renders the same probability vector [0.09,0.24,0.67]. However, different logit vectors may carry different underlying information that would be further exploited by the student, which could be lost due to the transformation process carried out by the softmax function. In order to properly leverage the information inside the logits, we introduce a logit-level KD loss. Specifically, we use the sigmoid function to formalize the pre-softmax output for each class into a range of [0,1] and deploy the Kullback-Leibler (KL) divergence to perform the binary classification of each class. By aligning the pre-softmax output of each class separately, this newly introduced loss (denoted by BinaryKL) can adequately exploit the information inside the logits. However, it is interesting to show that combining the logit-level BinaryKL loss with the probability-level cross-entropy (CE) loss results in poor performance of the student model, which is even worse than the performance of employing either loss separately. As illustrated in Figure 1(b), employing either loss separately as the training loss can induce a high-performance student model, but the model’s performance will be largely degraded when we combine them together during the training process. (a) Information loss through softmax (b) An illustration of four settings we evaluate Figure 1: The reason for introducing the BinaryKL loss and the incompatibility of the CE loss and the BinaryKL loss. Figure 1(a) shows that two different vectors may become the same through the softmax function, which means some information would be lost during the transformation process carried out by the softmax function. Figure 1(b) shows four settings we evaluate: (1) only cross-entropy (CE) loss; (2) only the BinaryKL loss; (3) CE + BinaryKL; (4) Our proposed Dual-Head Knowledge Distillation (DHKD). The red cross over the third setting means that the student model trained under this setting will collapse. The performance sorting of four settings is (4) > (1) > (2) \gg (3), as shown in Figure 2. To identify the root cause of this abnormal phenomenon, inspired by the recent research on neural collapse (Yang et al., 2022), we analyze the gradients of the model when using both losses. Specifically, by dividing the gradients of the model into two parts, we find that the gradients of the two loss functions contradict each other regarding the linear classifier head but display no such conflict regarding the backbone. As a consequence, while the BinaryKL loss can facilitate the backbone in learning from the teacher model more precisely, it prevents the linear classifier head from converging to a simplex equiangular tight frame (see a detailed definition in Definition 1), which is the ideal status that a well-trained linear classifier should converge to. Therefore, the combination of these two loss functions would cause the linear classifier to collapse, thereby degrading the performance of the student model. Based on our theoretical analysis that a single linear classifier is faced with the gradient contradiction when being trained by both losses simultaneously, we propose Dual-Head Knowledge Distillation (DHKD), which introduces an auxiliary classifier head apart from the original linear classifier to effectively circumvent the collapse of the linear classifier while retaining the positive effects of the BinaryKL loss on the backbone. Extensive experiments show the advantage of our proposed DHKD method. Our main contributions can be summarized as follows: • We disclose an interesting phenomenon in the knowledge distillation scenario: combining the probability-level CE loss and the logit-level BinaryKL loss would cause a performance drop of the student model, compared with using either loss separately. • We provide theoretical analyses to explain the discordance between the BinaryKL loss and the CE loss. While the BinaryKL loss aids in cultivating a stronger backbone, it harms the performance of the linear classifier head. • We propose a novel knowledge distillation method called Dual-Head Knowledge Distillation (DHKD). Apart from the linear classifier trained with the CE loss, DHKD specially introduces an auxiliary classifier trained with the BinaryKL loss. Extensive experiments demonstrate the effectiveness of our proposed method."
https://arxiv.org/html/2411.08935v1,Classification of Keratitis from Eye Corneal Photographs using Deep Learning,"Keratitis is an inflammatory corneal condition responsible for 10% of visual impairment in low- and middle-income countries (LMICs), with bacteria, fungi, or amoeba as the most common infection etiologies. While an accurate and timely diagnosis is crucial for the selected treatment and the patients’ sight outcomes, due to the high cost and limited availability of laboratory diagnostics in LMICs, diagnosis is often made by clinical observation alone, despite its lower accuracy. In this study, we investigate and compare different deep learning approaches to diagnose the source of infection: 1) three separate binary models for infection type predictions; 2) a multitask model with a shared backbone and three parallel classification layers (Multitask V1); and, 3) a multitask model with a shared backbone and a multi-head classification layer (Multitask V2). We used a private Brazilian cornea dataset to conduct the empirical evaluation. We achieved the best results with Multitask V2, with an area under the receiver operating characteristic curve (AUROC) confidence intervals of 0.7413-0.7740 (bacteria), 0.8395-0.8725 (fungi), and 0.9448-0.9616 (amoeba). A statistical analysis of the impact of patient features on models’ performance revealed that sex significantly affects amoeba infection prediction, and age seems to affect fungi and bacteria predictions.","According to the World Health Organization (WHO) [12], corneal blindness impacts 6M people worldwide, with keratitis as its leading cause [19, 20], with different consequences in low, middle and high-income countries [1]. Keratitis (or corneal ulcer) is an inflammation of the cornea caused by microorganisms (e.g., bacteria (BK), viruses (VK), fungi (FK), amoeba (AK), or herpes simplex keratitis (HSK)) or by non-infectious factors (e.g., trauma or chemical exposure) [11]. This disease is more common in LMICs, with its risk factors (e.g., compromised immune systems, poor ocular hygiene, contact lens wear, eye trauma, and surgery, exposure to contaminated water and soils and agricultural work) [9, 14] being correlated with the context of rural areas of these geographies [19]. The symptoms of keratitis include eye redness, eye pain, excess tears or discharge, foreign body sensation, difficulty opening the eyelid, blurred vision, light sensitivity, and watery eyes [9]. While the symptoms are well-defined, the main challenge with this disease is related to the diagnosis of the etiology, a crucial detail for the treatment of the condition and prevention of vision loss. The clinical pipeline for determining the etiology includes techniques such as corneal scrapping with gram stain, microscopy, and culture analysis. However, given their substantial costs (i.e., approximately $200 per patient in Brazil), these resources are often unavailable in LMICs. The alternative approach relies on performing a clinical examination with a slit lamp, with bright light and enough magnification to observe the infection. Nevertheless, distinguishing between the types of infection remains difficult and might lead to a misdiagnosis, worsening the symptoms and requiring an eye surgery [18, 4, 1]. In this paper, we explore the potential of deep learning to classify keratitis etiologies using eye corneal photographs, making the following scientific contributions: 1. Proposal and implementation of deep learning algorithms to classify different etiologies of keratitis from corneal eye photographs, using a new private Brazilian cornea dataset with clinical attributes and annotations; 2. Assessment of the statistical impact of correlated features (i.e., age and sex) on disease presentation. The code related to the implementation of this work is publicly available in a GitHub repository111https://github.com/mariamiguel01/Keratitis_Classifier."
https://arxiv.org/html/2411.08934v1,Predicting household socioeconomic position in Mozambique using satellite and household imagery,"Many studies have predicted SocioEconomic Position (SEP) for aggregated spatial units such as villages using satellite data, but SEP prediction at the household level and other sources of imagery have not been yet explored. We assembled a dataset of 975 households in a semi-rural district in southern Mozambique, consisting of self-reported asset, expenditure, and income SEP data, as well as multimodal imagery including satellite images and a ground-based photograph survey of 11 household elements. We fine-tuned a convolutional neural network to extract feature vectors from the images, which we then used in regression analyzes to model household SEP using different sets of image types. The best prediction performance was found when modeling asset-based SEP using random forest models with all image types, while the performance for expenditure- and income-based SEP was lower. Using SHAP, we observed clear differences between the images with the largest positive and negative effects, as well as identified the most relevant household elements in the predictions. Finally, we fitted an additional reduced model using only the identified relevant household elements, which had an only slightly lower performance compared to models using all images. Our results show how ground-based household photographs allow to zoom in from an area-level to an individual household prediction while minimizing the data collection effort by using explainable machine learning. The developed workflow can be potentially integrated into routine household surveys, where the collected household imagery could be used for other purposes, such as refined asset characterization and environmental exposure assessment.","SocioEconomic Position (SEP), defined as the set of social and economic factors that determine the position of an individual or a group within a society, is an important, complex, and broadly used concept in health research (Galobardes et al., 2006). SEP is an important health determinant across the life course through multiple pathways such as the access to healthcare, health-related behaviours, and environmental and occupational exposures that may lead to health inequalities (Adler and Stewart, 2010). Furthermore, it is also one of the strongest confounders in environmental epidemiology (e.g., Hystad et al., 2019) due to a potential relationship between SEP and many exposures. Given its multidimensional nature, there are many ways to measure SEP and the appropriate measure depends on the research question and context (Galobardes et al., 2006). In Low- and Middle-Income Countries (LMICs), one of the most used approaches are asset-based measures, which are widely available through Demographic and Health Surveys (DHS) (Filmer and Pritchett, 2001). Asset-based SEP is constructed based on information on durable assets, housing materials, and access to basic services of a given household, which are then summarized into a single indicator using statistical methods (Filmer and Scott, 2012). While collecting asset information is easier than other SEP metrics, asset-based SEP is a relative measure that may lack variability if the set of variables to be included is not carefully chosen (Howe et al., 2012) depending on the type of area (urban vs. rural) and geography (variation between and within countries) (Olivia Howland and Brockington, 2021). While income-based SEP is extensively used in high-income countries (Galobardes et al., 2006), in LMICs income can be more difficult to measure due to a higher prevalence of informal and seasonal labour, multiple income sources, home production, and income in the form of goods (Howe et al., 2012). Expenditure SEP measures have also been used: although these are more stable over time than income, they are challenging to measure due to misreporting, limitations of expenditure diaries, the monetization of home-produced goods (Howe et al., 2012), and seasonal fluctuations in food prices (Bai et al., 2020). Collecting SEP data is complex, time-consuming, and costly (Burke et al., 2021). Motivated by this, a rapidly growing body of literature focusing on predictive mapping approaches to estimate SEP based on remote sensing data has emerged, leveraging the increasing availability of satellite data (Burke et al., 2021) and advances in the fields of machine and deep learning (Gao et al., 2019). Many of the studies focused on the prediction of asset-based SEP, while others predicted expenditure, income, education, or other poverty-related measures (Hall et al., 2023). Early works focused on SEP prediction based on nighttime light satellite data (e.g., Elvidge et al., 2009), but recently works using very high-resolution (<10m) optical remote sensing data have become increasingly prevalent in LMICs (e.g., Head et al., 2017) including Mozambique (Sohnesen et al., 2022). Alternatively, freely available high- and medium-resolution optical satellite data (MODIS, Landsat, Sentinel 2) have also been used (e.g., Yeh et al., 2020; Jonathan Hersh and Mann, 2021). Many of these studies relied on deep Convolutional Neural Networks (CNN) to extract feature vectors from images (e.g., Jean et al., 2016; Zhao et al., 2019), but some studies also used hand-crafted features such as vehicle, building, or roof object detection (e.g., Ayush et al., 2020; Engstrom et al., 2022); vegetation and imperviousness spectral indices (e.g., Niu et al., 2020); or features derived from land cover classification and texture analyses (e.g., Duque et al., 2015). A recent review of SEP mapping studies found that the factors that benefited performance included the combination of deep learning and machine learning methods (as opposed to using only one of them), predicting asset-based measures (compared to expenditure and income), and the number of datasets used (Hall et al., 2023). On the other hand, the most important limitation was the scarce and noisy ground truth data (Burke et al., 2021). Data scarcity has been addressed via transfer learning of deep learning models (Burke et al., 2021), with some studies using widely available SEP proxies such as satellite nighttime lights (e.g., Jean et al., 2016; Xie et al., 2016). Despite rapid progress in the field of SEP prediction, there are still some challenges to be addressed. First, most mapping studies focused on predicting SEP for aggregated spatial units (e.g., villages (Yeh et al., 2020), grid cells (Chi et al., 2022)), while studies that predict SEP at the household level are still infrequent (Watmough et al., 2019). Targeting studies that estimate SEP for individual households are important (McBride et al., 2022) since they allow to better allocate resources to specific households during interventions (e.g., food or energy-related), address inequalities within communities, and enable the use of predictions in epidemiological analyses. Second, the use of ground imagery such as street view images could complement satellite imagery and increase model performance. However, studies using street view imagery have been mostly limited to urban areas in high-income countries (e.g., Gebru et al., 2017; Suel et al., 2019; Fan et al., 2023) due to data availability. In addition to street view images, household imagery such as the Dollar Street dataset (Rojas et al., 2022) would be particularly useful to go beyond simple binary asset indicators and capture information about their quality (Howe et al., 2012). Third, there is a need to increase the interpretability and explainability of SEP predictive models (Hall et al., 2022). As examples, SHAP values (SHapley Additive exPlanations) have been used to identify the most relevant features in SEP models (Ayush et al., 2020), whereas Grad-CAM has been used to compute activation maps in satellite images used to predict SEP (Abitbol and Karsai, 2020). Explainable machine learning methods are especially important in multimodal datasets (e.g., Chi et al., 2022), where identifying relevant and redundant modalities is key to finding opportunities to optimize data collection. This study aims to advance the current literature on SEP prediction by introducing: 1) a multimodal dataset that combines very high-resolution satellite imagery and a household photograph survey, 2) a SEP prediction at the household level, 3) a comprehensive evaluation of SEP by using multiple measures, and 4) the use of explainable machine learning to improve model explainability and optimize sampling. The objectives of the study are the following: • To predict household SEP using a multimodal satellite and household imagery dataset. • To assess the contribution of each of the images to the SEP prediction to explain the models. • To optimize future data collection efforts by identifying the most relevant image types in the models."
https://arxiv.org/html/2411.08933v2,"Confidence-aware Denoised Fine-tuning of 
Off-the-shelf Models for Certified Robustness","The remarkable advances in deep learning have led to the emergence of many off-the-shelf classifiers, e.g., large pre-trained models. However, since they are typically trained on clean data, they remain vulnerable to adversarial attacks. Despite this vulnerability, their superior performance and transferability make off-the-shelf classifiers still valuable in practice, demanding further work to provide adversarial robustness for them in a post-hoc manner. A recently proposed method, denoised smoothing, leverages a denoiser model in front of the classifier to obtain provable robustness without additional training. However, the denoiser often creates hallucination, i.e., images that have lost the semantics of their originally assigned class, leading to a drop in robustness. Furthermore, its noise-and-denoise procedure introduces a significant distribution shift from the original distribution, causing the denoised smoothing framework to achieve sub-optimal robustness. In this paper, we introduce Fine-Tuning with Confidence-Aware Denoised Image Selection (FT-CADIS), a novel fine-tuning scheme to enhance the certified robustness of off-the-shelf classifiers. FT-CADIS is inspired by the observation that the confidence of off-the-shelf classifiers can effectively identify hallucinated images during denoised smoothing. Based on this, we develop a confidence-aware training objective to handle such hallucinated images and improve the stability of fine-tuning from denoised images. In this way, the classifier can be fine-tuned using only images that are beneficial for adversarial robustness. We also find that such a fine-tuning can be done by merely updating a small fraction (i.e., 1%) of parameters of the classifier. Extensive experiments demonstrate that FT-CADIS has established the state-of-the-art certified robustness among denoised smoothing methods across all \ell_{2}-adversary radius in a variety of benchmarks, such as CIFAR-10 and ImageNet.","Despite the recent advancements in modern deep neural networks in various computer vision tasks (Radford et al., 2021; Rombach et al., 2022; Kirillov et al., 2023), they still suffer from the presence of adversarial examples (Szegedy et al., 2013) i.e., a non-recognizable perturbation (for humans) of an image often fools the image classifiers to flip the output class (Goodfellow et al., 2014). Such adversarial examples can be artificially crafted with malicious intent, i.e., adversarial attacks, which pose a significant threat to the practical deployment of deep neural networks. To alleviate this issue, various approaches have been proposed to develop robust neural networks, such as adversarial training (Madry et al., 2018; Wang et al., 2019) and certified defenses (Wong & Kolter, 2018; Cohen et al., 2019; Li et al., 2023). Among these efforts, randomized smoothing (Lecuyer et al., 2019; Cohen et al., 2019) has gained much attention as a framework to build robust classifiers. This is due to its superior provable guarantee of the non-existence of adversarial examples, i.e., certified robustness (Wong & Kolter, 2018; Xiao et al., 2018), under any perturbations confined in a \ell_{2}-norm. Specifically, it builds a smoothed classifier through taking a majority vote from a base classifier, e.g., a neural network, under Gaussian perturbations of the given input image. However, it has been practically challenging to scale the model due to a critical drawback: the base classifier should be specifically trained on noise-augmented data (Lecuyer et al., 2019; Cohen et al., 2019). Recently, Lee (2021); Carlini et al. (2023) have introduced denoised smoothing which utilizes pre-trained off-the-shelf classifiers within the randomized smoothing framework. Rather than directly predicting the label of a noise-augmented image, it first feeds the perturbed image into a denoiser, e.g., a diffusion model, and then obtains the predicted label of the denoised image using off-the-shelf pre-trained classifiers that have been trained on clean images. Intriguingly, denoised smoothing with recently developed diffusion models and pre-trained classifiers, e.g., guided diffusion (Dhariwal & Nichol, 2021) and BEiT (Bao et al., 2022), shows its superior scalability with comparable certified robustness in \ell_{2}-adversary to the current state-of-the-art methods (Horváth et al., 2022b; Jeong et al., 2023). Figure 1: Overview of FT-CADIS framework. (1) Confidence-aware denoised image selection: for a given clean image, we create denoised images and find non-hallucinated images. (2) Fine-tuning with confidence-aware denoised image selection: we propose fine-tuning objectives to improve both generalizability and robustness of the smoothed classifier based on selected non-hallucinated images. On the other hand, denoised smoothing also exhibits clear limitations. Firstly, denoised images do not follow the standard pre-training distribution of the classifiers, which results in a limited robustness of the denoised smoothing framework. Secondly, fine-tuning the pre-trained classifiers with the denoised images also yields sub-optimal classifiers due to the hallucinated images (Carlini et al., 2023), i.e., the diffusion denoiser tends to generate image semantics from an incorrect class rather than the originally assigned class (see Figure 4). Consequently, denoised smoothing with such classifiers leads to a drop of the certified accuracy, especially in the large \ell_{2}-radius regime, i.e., high Gaussian variance (see Table 3). Contribution. In this paper, we aim to address the aforementioned issues of denoised smoothing by designing a fine-tuning objective for off-the-shelf classifiers that distinguishes between hallucinated images, i.e., images that have lost the original semantics after denoising, and non-hallucinated images, i.e., images that maintain the original semantics after denoising. To this end, we propose to use the “likelihood of denoised images”, i.e., confidence, of the off-the-shelf classifier with respect to the originally assigned class as a proxy for determining whether an image is hallucinated and then fine-tune the classifier with non-hallucinated images only. Consequently, we have developed a confidence-aware training objective based on the likelihood of denoised images to effectively discriminate hallucinated images (see Figure 1). Specifically, we propose a scalable and practical framework for fine-tuning off-the-shelf classifiers, coined Fine-Tuning with Confidence-Aware Denoised Image Selection (FT-CADIS), which improves certified robustness under denoised smoothing. In order to achieve this, two new losses are defined: the Confidence-aware selective cross-entropy loss and the Confidence-aware masked adversarial loss. Two losses are selectively applied only to non-hallucinated images, thereby ensuring that the overall training process avoids over-optimizing hallucinated samples, i.e., samples that are harmful for generalization, while maximizing the robustness of smoothed classifiers. Our particular loss design is motivated by Jeong et al. (2023), who were the first to investigate training objectives for randomized smoothing depending on sample-wise confidence information. We demonstrate that our novel definition of confidence in randomized smoothing, specifically through the ratio of non-hallucinated images from a denoiser, can dramatically stabilize the confidence-aware training, overcoming its previous limitation of severe accuracy degradation (e.g., see Table 3). In our experiments, we have validated the effectiveness of our proposed method on standard benchmarks for certified \ell_{2}-robustness, i.e., CIFAR-10 (Krizhevsky, 2009) and ImageNet (Russakovsky et al., 2015). Our results show that the proposed method significantly outperforms existing state-of-the-art denoised smoothing methods in certified robustness across all \ell_{2}-norm setups, while updating only 1% of the parameters of off-the-shelf classifiers on ImageNet. In particular, FT-CADIS significantly improves the certified robustness in the high Gaussian variance regime, i.e., high certified radius. For instance, FT-CADIS outperforms the best performing baseline, i.e., diffusion denoised (Carlini et al., 2023), by 29.5% \rightarrow 39.4% at \varepsilon = 2.0 for ImageNet experiments."
https://arxiv.org/html/2411.08930v1,Structured Pattern Expansion with Diffusion Models,"Recent advances in diffusion models have significantly improved the synthesis of materials, textures, and 3D shapes. By conditioning these models via text or images, users can guide the generation, reducing the time required to create digital assets. In this paper, we address the synthesis of structured, stationary patterns, where diffusion models are generally less reliable and, more importantly, less controllable.Our approach leverages the generative capabilities of diffusion models specifically adapted for the pattern domain. It enables users to exercise direct control over the synthesis by expanding a partially hand-drawn pattern into a larger design while preserving the structure and details of the input. To enhance pattern quality, we fine-tune an image-pretrained diffusion model on structured patterns using Low-Rank Adaptation (LoRA), apply a noise rolling technique to ensure tileability, and utilize a patch-based approach to facilitate the generation of large-scale assets.We demonstrate the effectiveness of our method through a comprehensive set of experiments, showing that it outperforms existing models in generating diverse, consistent patterns that respond directly to user input.","Hand-drawn structured patterns are central to computer graphics, with applications spanning various domains in design and digital art. Creating these patterns remains a complex and time-consuming task that requires specialized expertise. AI-assisted content creation offers the potential to simplify this process. For instance, learning-based image synthesis methods have shown impressive generation capabilities for natural images [Rombach et al., 2022; Brock et al., 2019; Karras et al., 2018, 2020; Podell et al., 2023]. However, the application of these methods to pattern-like synthesis has primarily focused on unstructured, realistic materials [Zhou et al., 2018, 2023, 2024; He et al., 2023; Vecchio et al., 2024b, a; Vecchio and Deschaintre, 2024], leaving the creation of structured patterns an underexplored task. In contrast, the synthesis of highly structured vector patterns has been assessed by automatically discovering and exploiting their structure, geometry and topology [Tu et al., 2020; Reddy et al., 2020] or by optimizing the procedural parameters of differentiable vector pattern to match a sketch or a viewport edit [Riso and Pellacini, 2023; Riso et al., 2022]. Our work focuses on structured patterns with a hand-drawn appearance, characterized by the repetitions of sketch-like shapes filled with solid colors and defined by sharp, crisp edges. Formally, these structured patterns consist of stationary repetitions of recognizable shapes, each with individual variations, and are drawn with piecewise-constant colors. Examples of these patterns are shown throughout the paper, with Fig. 2 also highlighting examples of textures outside of our scope. We focus on this type of pattern for their importance in design applications and due to the general lack of learning-based methods addressesing their synthesis. In-domain samples Out-of-domain samples Figure 2. Pattern Category. In this work, we focus on structured, stationary, patterns in a hand-drawn style, characterized by repeated recognizable shapes drawn in flat colors (left). Unstructured or aperiodic patterns, as well as photorealistic textures, fall outside the scope of this paper (right). Our approach leverages Latent Diffusion Models [Rombach et al., 2022] as a foundation for the synthesis. Although these models have achieved significant advancements in natural image generation, they are not optimized to generate structured patterns. One primary limitation is that the synthesized patterns often lack quality, as these models are typically trained to generate photorealistic images with unstructured, chaotic textures and high-frequency, stochastic color variations. When applied to structured patterns, these methods often fail to maintain the inherent structure, sharpness, and cohesive visual style of the patterns. Furthermore, design applications often require precise pattern controll by the users, often lacking in generative approaches, generally focusing on text-to-image synthesis. While high-level conditioning may be sufficient for natural image synthesis, specifying the exact structure and appearance of a pattern is far more challenging. Even when using images as conditioning inputs, existing methods perform inconsistently in producing structured patterns in our domain. To address this gap in the literature and offer artists a more accessible yet controllable tool for content creation, we propose a diffusion-based model specifically designed for the synthesis and expansion of structured, stationary patterns. In particular, we leverage the extensive knowledge already available in large-scale models, such as Stable Diffusion [Rombach et al., 2022; Podell et al., 2023], and adapt it to the patterns domain by training a “lightweight” LoRA [Hu et al., 2022]. Doing so we limit the computational and data requirements of training a diffusion model from scratch, but helps to retain the expressivity of models trained on large-scale datasets like LAION [Schuhmann et al., 2021], while adapting it to our specific domain. To that end, we collect a dataset of procedurally designed patterns that we use to train our LoRA. We base our architecture on an inpainting pipeline, which supports the expansion of a partial, hand-drawn input sketch into a larger pattern while preserving its structural integrity and details. During inference, we leverage noise rolling and patch-based synthesis to produce large-scale, tileable patterns, at high quality in a reliable way. These design choices allow us to generate large-scale, tileable patterns that accurately follow the input sketch, while adding a limited degree of variation avoiding visible repetitions. We qualitatively evaluate the effectiveness of our approach across a diverse range of input patterns, demonstrating significant improvements over previous state-of-the-art methods in texture synthesis. To assess user satisfaction with generation quality, we also conduct a user study that captures preferences and perceived fidelity in the synthesized outputs. Additionally, we analyze our architecture through a comprehensive set of experiments and ablation studies to highlight the benefits of our design choices. The results show that our method consistently generates a wide variety of structure patterns, effectively preserving the structure and visual coherence of the input sketches. In summary, the contributions of our work are as follows: • we present a new diffusion-based approach for structured pattern synthesis and expansion; • we introduce a new medium-scale dataset for fine-tuning generative models on the pattern domain; • we demonstrate the generation capabilities of our model for different types of structured patterns and show its ability to control the generation precisely from input sketches; • we validate the improvements over other generative methods, non-specifically trained for patterns, underlying the need for a specifically trained model."
https://arxiv.org/html/2411.08925v1,Retrieval of sun-induced plant fluorescence in the O-A absorption band from DESIS imagery,"We provide the first method allowing to retrieve spaceborne SIF maps at 30 m ground resolution with a strong correlation (r^{2}=0.6) to high-quality airborne estimates of sun-induced fluorescence (SIF). SIF estimates can provide explanatory information for many tasks related to agricultural management and physiological studies. While SIF products from airborne platforms are accurate and spatially well resolved, the data acquisition of such products remains science-oriented and limited to temporally constrained campaigns. Spaceborne SIF products on the other hand are available globally with often sufficient revisit times. However, the spatial resolution of spaceborne SIF products is too small for agricultural applications. In view of ESA’s upcoming FLEX mission we develop a method for SIF retrieval in the O2-A band of hyperspectral DESIS imagery to provide first insights for spaceborne SIF retrieval at high spatial resolution. To this end, we train a simulation-based self-supervised network with a novel perturbation based regularizer and test performance improvements under additional supervised regularization of atmospheric variable prediction. In a validation study with corresponding HyPlant derived SIF estimates at 740 nm we find that our model reaches a mean absolute difference of 0.78\,\,\mathrm{mW\,nm^{-1}\,sr^{-1}\,m^{-2}}.","The potential of sun-induced flurorescence (SIF) for agricultural management and phenotyping tasks was recognized early in the development of retrieval algorithms [41]. Since SIF is fuelled by a residual energy flux of photosynthetically active radiation (PAR) that is not consumed by processes related to the plant’s photochemistry and thermal energy dissipation it provides a causal link between radiance measurements and the photosynthetic status of plants [42, 50, 61, 62]. Various studies have utilized this relationship as the theoretical basis for stress detection and monitoring [1, 12, 49, 14, 68], the estimation of photosynthetic activity and gross primary productivity [10, 59, 58, 69], crop monitoring and yield predictions [25, 56, 37, 48] and disease monitoring [8, 51] from SIF estimates derived from remote sensing data at various spatial scales. Quantitative estimates of SIF allow for more sensitive and causally founded physiological assessments compared to purely reflectance based indices commonly used for such tasks. Different studies have shown the increased explanatory power of SIF estimates measured at canopy level in a range of tasks [12, 45, 65, 39]. SIF retrieval methods for a variety of sensors have been developed as the number of airborne and spaceborne sensors with sufficient spectral resolution has increased [43]. However, no spaceborne sensor designed specifically for fluorescence retrieval has yet been operationalized. ESA’s Earth Explorer Mission FLEX [16], planned to be launched in 2025, will be the first such instrument. Spaceborne SIF estimates to this day are derived from data acquired by satellite missions for atmospheric characterization (e.g., GOSAT [34], GOME [33, 27], SCIAMACHY [35], OCO-2/3 [57, 17], TROPOMI [26, 28], TanSAT [67]) as their spectral resolution (SR) and signal-to-noise ratio (SNR) allow for SIF retrieval from Fraunhofer lines [23, 24, 16]. However, the spatial resolution of these atmospheric sensors (> 4 km2) is insufficient for most agricultural applications. FLEX, on the other hand, will provide radiance data with a pixel size of 300 m which still imposes severe limits on its usability for a wide range of applications in heterogeneous agricultural landscapes. As an exploratory step towards spaceborne SIF retrieval at high spatial resolution, we therefore propose a deep learning architecture and a loss formulation for the first SIF retrieval from hyperspectral imagery of the DLR Earth Sensing Imaging Spectrometer (DESIS). SIF retrieval from DESIS imagery has the benefit of providing spaceborne SIF products at an unprecedented spatial resolution of 30 m which principally allows for the targeted acquisition of auxiliary validation data at high spatial resolution for the upcoming FLEX mission. However, the SR and SNR of DESIS are insufficient for consistent SIF retrieval with current traditional retrieval methods leveraging data in the O2-A absorption band [22, 13, 40] where the fluorescence signal contribution to the at-sensor signal has a local maximum. Airborne SIF retrieval with similar methods applied to radiance data at lower SR has however been shown to yield consistent relative SIF estimates [3]. As a solution, we extend the simulation-based self-supervised deep learning approach of [5, 7], called Spectral Fitting Method Neural Network (SFMNN), originally developed with airborne hyperspectral imagery. As in other self-supervised simulation-based learning schemes, this approach leverages the implicit constraints of a differentiable simulator of the physical image generation in the loss [30, 32] and primarily does not rely on labels for training. Further regularization terms that enforce physical and physiological domain constraints allow this encoder-decoder architecture to decompose and reconstruct hyperspectral data in the spectral range around the O2-A absorption band. In this contribution we introduce regularization terms in the SFMNN framework allowing consistent SIF retrieval in DESIS imagery despite its lower SNR and SR. Firstly, we propose a perturbation based augmentation scheme to promote the decorrelation of the predicted SIF from other confounding variables affecting the at-sensor signal. Secondly, we show that including ancillary atmospheric data from DESIS L2A products by means of a secondary supervised downstream learning task improves the performance of our model."
https://arxiv.org/html/2411.08923v1,"Aligning Visual Contrastive learning models 
via Preference Optimization","Contrastive learning models have demonstrated impressive abilities to capture semantic similarities by aligning representations in the embedding space. However, their performance can be limited by the quality of the training data and its inherent biases. While Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have been applied to generative models to align them with human preferences, their use in contrastive learning has yet to be explored. This paper introduces a novel method for training contrastive learning models using Preference Optimization (PO) to break down complex concepts. Our method systematically aligns model behavior with desired preferences, enhancing performance on the targeted task. In particular, we focus on enhancing model robustness against typographic attacks, commonly seen in contrastive models like CLIP. We further apply our method to disentangle gender understanding and mitigate gender biases, offering a more nuanced control over these sensitive attributes. Our experiments demonstrate that models trained using PO outperform standard contrastive learning techniques while retaining their ability to handle adversarial challenges and maintain accuracy on other downstream tasks. This makes our method well-suited for tasks requiring fairness, robustness, and alignment with specific preferences. We evaluate our method on several vision-language tasks, tackling challenges such as typographic attacks. Additionally, we explore the model’s ability to disentangle gender concepts and mitigate gender bias, showcasing the versatility of our approach.","1 Introduction In recent years, Vision-Language Models (VLMs) such as CLIP (Radford et al., 2021) have revolutionized downstream vision-language tasks like classification (Conde & Turgutlu, 2021), object detection (Zhong et al., 2021), segmentation (Xu et al., 2022), and image generation (Saharia et al., 2022). These models are trained on large-scale web data, such as the 400 million text-image pairs used for training CLIP. However, despite their success, VLMs exhibit several vulnerabilities. For instance, adversarial attacks (Goodfellow et al., 2015; Moosavi-Dezfooli et al., 2016) and backdoor vulnerabilities (Bai et al., 2024; Liang et al., 2024) can lead to erroneous classifications with high confidence. CLIP has also been shown to be vulnerable to typographic attacks, where text within images causes misclassification (Goh et al., 2021). Additionally, biases such as gender and racial bias (Ruggeri & Nozza, 2023; Zhang et al., 2022b; Bolukbasi et al., 2016; Jialu Wang et al., 2021) can be amplified by these models due to biases in their training datasets. Another dispreferred behavior occurs when models focus on tasks unrelated to the intended objective, as highlighted by (Menon et al., 2022). These challenges underscore the need for alignment methods in large pre-trained models. Generative models, including Large Language Models (LLMs), have also been widely adopted for solving complex problems across various domains. Examples in the literature include GPT-4 (Achiam et al., 2023), Mistral (Albert Q. Jiang et al., 2023), and LLaMA (Touvron et al., 2023). These models are trained on massive datasets of unlabeled text, learning general language representations. Some of these LLMs require additional Supervised Fine-Tuning (SFT) to specialize for specific tasks using labeled data. However, even with fine-tuning, LLMs can produce outputs that misalign with human values or safety expectations, such as the unreliability of LLMs in autonomous driving (Chen et al., 2024). To mitigate this, alignment techniques are applied, where models are further trained using a reward model that captures human preferences (Stiennon et al., 2022; Ouyang et al., 2022b; Bai et al., 2022). Popular alignment approaches include Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022b; Christiano et al., 2023) and Direct Preference Optimization (DPO) (Rafailov et al., 2023), both of which extend the capabilities of these models beyond what SFT alone can achieve. These alignment techniques have improved the safety and relevance of LLM outputs by making them more aligned with human preferences. Alignment methods have been explored in VLMs (Sun et al., 2023; Gallego, 2023; Clark et al., 2023), but to the best of our knowledge, they have not been extensively applied to contrastive learning models like CLIP. In this work, we extend the preference optimization paradigm to non-generative VLMs. Our approach builds on the contrastive learning framework while simultaneously aiming to preserve the pretrained knowledge—a concept known as continual learning (Garg et al., 2024; Wang et al., 2023). In this work, we extend the preference optimization paradigm to contrastive learning models, focusing on enhancing robustness against typographic attacks and mitigating gender biases. By systematically aligning the model’s behavior with human preferences, we aim to preserve its pretrained knowledge while improving its performance in sensitive areas such as fairness and robustness. The core tasks addressed in this work—mitigating typographic attacks and disentangling gender biases—underscore the versatility of our approach and its potential for improving model alignment in real-world applications. Our contributions are as the following: • We extend preference optimization methods, originally developed for generative models, to non-generative models, particularly contrastive learning models. This expansion opens new opportunities for improving model alignment in non-generative tasks, preserving the pretrained knowledge of models while optimizing for desired behaviors. • We propose controlling model behavior by adjusting the singular values of a learnable linear transformation. This allows fine-tuning of specific concepts, like gender, by modulating critical directions in the representation space while preserving overall performance. This method provides precise control and interpretability, enabling flexible post-training adjustments. • Our experiments demonstrate the usefulness of applying preference optimization in contrastive learning tasks, such as training for robustness against typographic attacks and disentangling gender information from the embedding space, mitigating gender bias, and showcasing its effectiveness in improving model behavior in real-world scenarios."
https://arxiv.org/html/2411.09702v1,On the Surprising Effectiveness of Attention Transfer for Vision Transformers,"Conventional wisdom suggests that pre-training Vision Transformers (ViT) improves downstream performance by learning useful representations. Is this actually true? We investigate this question and find that the features and representations learned during pre-training are not essential. Surprisingly, using only the attention patterns from pre-training (i.e., guiding how information flows between tokens) is sufficient for models to learn high quality features from scratch and achieve comparable downstream performance. We show this by introducing a simple method called attention transfer, where only the attention patterns from a pre-trained teacher ViT are transferred to a student, either by copying or distilling the attention maps. Since attention transfer lets the student learn its own features, ensembling it with a fine-tuned teacher also further improves accuracy on ImageNet. We systematically study various aspects of our findings on the sufficiency of attention maps, including distribution shift settings where they underperform fine-tuning. We hope our exploration provides a better understanding of what pre-training accomplishes and leads to a useful alternative to the standard practice of fine-tuning. Code to reproduce our results is at https://github.com/alexlioralexli/attention-transfer.","Pre-training has emerged as a dominant paradigm in machine learning and has significantly improved performance on a variety of tasks [27, 11, 2, 22]. In computer vision in particular, self-supervised representation learning methods [21, 6, 4, 22] and weakly supervised methods [40, 45] have enabled learning from large amounts of images. It is widely accepted that these methods work because they teach models useful features that are relevant for downstream tasks. But is this story actually true? Perhaps there is another capability learned during pre-training that is sufficient to explain its benefits. In this paper, we present an alternative explanation: pre-training teaches the model how information should be routed between tokens. We specifically focus on Vision Transformers (ViT) [12], not only because they are the most popular architecture for scaling, but also because Transformers explicitly decouple this information flow. Inter-token communication is solely fulfilled by attention, while the remaining bulk of computation are intra-token operations that are applied to each token independently. In contrast, other architectures such as ConvNets [33, 20] simultaneously expand the receptive fields and extract the features, making it difficult to isolate the effect of information flow. We hypothesize that the features computed by the intra-token operations are not essential to explain the benefits of pre-training, and that the pre-trained attention maps are typically sufficient for downstream tasks. We test our hypothesis by introducing a new set of methods called attention transfer. Concretely, we treat a pre-trained ViT as the teacher and train a student model for downstream tasks while transferring only the attention patterns from the teacher. In contrast to the common fine-tuning paradigm of transferring all the weights (which mixes the effect of features and attention maps), only the inter-token flow is transferred. In this way, the student must learn features from scratch, while isolating the benefits of the attention maps learned during pre-training. Figure 1: Using only attention is sufficient for full performance. By copying the attention maps (top) from a MAE [22] pre-trained ViT-L [12], a ViT-L can reach a top-1 accuracy of 85.1 on ImageNet-1K [10] – recovering 77.8% of the gap between no transfer (training from scratch, 83.0) and full transfer (fine-tuning all the weights, 85.7). Distilling attention maps (bottom) can even fully match MAE weight tuning while only transferring the inter-token flow. We study two types of attention transfer. The first is Attention Copy, which directly “copy-and-pastes” the attention maps. The learning is fully decoupled, as inter-token computation is entirely from the teacher, and the student only learns intra-token patterns routed by the teacher’s attention maps. This is well-suited as a scientific probe, but is less practical since both networks need to be forwarded during the inference. The second is Attention Distillation, where the student simply distills attention patterns from the teacher, whose attention maps are no longer used after training. This is practical, but also helps identify the importance of the teacher’s inter-token information flow. While both attention transfer variants are straightforward, we find them highly effective. Figure 1 illustrates this with a ViT-L [12] pre-trained using Masked Autoencoding (MAE) [22]. Compared to no transfer (training from scratch) and full transfer (fine-tuning all the MAE weights), Attention Copy can close most of the gap in performance, whereas Attention Distillation can match the fine-tuning accuracy on ImageNet-1K classification [10]. This is achieved by only transferring the inter-token flow from the same model. Furthermore, since attention transfer requires the student to learn features from scratch, those features are significantly different from the teachers’ (Figure 5) and improve ImageNet-1K accuracy score to 86.3 (+0.6) when ensembled with the teacher (Figure 6). To summarize, we make the following contributions: • Detailed analysis on the sufficiency of attention maps. We find that solely using the pre-trained attention patterns is typically sufficient to achieve the same downstream accuracy as fine-tuning on ImageNet-1K. Furthermore, we observe practical benefits, as ensembling with attention transfer significantly improves ImageNet performance. This calls into question the commonly-believed story that pre-training is only about feature learning. While our main observation is robust w.r.t. different models and pre-training methods, we do find settings where pre-trained features are indeed necessary to realize the full gains from pre-training. Our bare-minimum solution for attention transfer is more affected by data distribution shifts compared to weight tuning. Section 4 presents extensive analyses to better understand the behaviors of attention transfer. They are i) partial transfer with a subset of layers or heads; ii) variants of our method that transfer other attention-related activations; and importantly, iii) various ways to verify that the student is not just re-learning the teacher model. Section 5 systematically tests how well our findings apply across a variety of pre-training and fine-tuning datasets, pre-training methods, model sizes, and tasks. • Attention transfer methods. We introduce Attention Copy and Attention Distillation, which are methods to train a ViT on a downstream task while utilizing only the attention maps of a pre-trained teacher ViT. These methods help us understand the role of the features versus the attention patterns learned during pre-training. With further research, attention transfer could offer a potential alternative to the decade-long practice of fine-tuning pre-trained vision models [16, 12, 22]. Nearly all aspects of the fine-tuning pipeline have been thoroughly examined, suggesting a probable saturation of recipes. Weight sharing can also face security risks (e.g., white-box attacks [17]). We hope our systematic examination of attention transfer sheds new light on how to leverage pre-trained ViTs, and will help establish this approach as an effective alternative when weight transfer is less applicable. Figure 2: Two types of Attention transfer for Vision Transformers. Attention Copy (left): We simply “copy-and-paste” the attention maps from a pre-trained teacher model to a randomly initialized student one. Other weights of the student are then trained via supervised learning. This fully decouples inter-token learning (from the teacher) and intra-token learning (in the student); but is less practical. Attention Distillation (right): The student computes its own attention maps, with an additional cross-entropy loss to distill patterns from the teacher during training. The teacher is no longer used during inference. H: number of heads; L: number of Transformer layers."
https://arxiv.org/html/2411.09627v1,"One-Shot Manipulation Strategy Learning
by Making Contact Analogies","We present a novel approach, magic (manipulation analogies for generalizable intelligent contacts), for one-shot learning of manipulation strategies with fast and extensive generalization to novel objects. By leveraging a reference action trajectory, magic effectively identifies similar contact points and sequences of actions on novel objects to replicate a demonstrated strategy, such as using different hooks to retrieve distant objects of different shapes and sizes. Our method is based on a two-stage contact-point matching process that combines global shape matching using pretrained neural features with local curvature analysis to ensure precise and physically plausible contact points. We experiment with three tasks including scooping, hanging, and hooking objects. magic demonstrates superior performance over existing methods, achieving significant improvements in runtime speed and generalization to different object categories. Website: https://magic-2024.github.io/.","A hallmark of human intelligence is flexible tool use: humans can quickly acquire new manipulation “strategies” from just a handful of demonstrations and apply these strategies across various scenarios, including generalization to novel objects of unseen categories. For example, as illustrated in Fig. 1, even from a single demonstration of using a hook to reach distant objects or putting hangers on a rod, we can generalize to different object positions, sizes, and diverse categories, such as hangers and mugs. Traditionally, two main approaches have been widely studied to build machines that can flexibly use tools: model-based and analytic approaches which take novel scenarios and goals and use built-in physical models to compute plans [1, 2, 3, 4], and policy learning, which leverages various types of priors (e.g., object-based and part-based models) and pretrained neural features for generalization [5, 6, 7, 8]. However, both approaches have their limitations. Model-based planning generalizes well given accurate object and physical models. However, it is slow and usually does not benefit from learning. Policy learning approaches, on the other hand, are very efficient at performance time but usually exhibit limited generalization to novel objects and scenarios, particularly when the shape of the novel objects differs significantly from objects seen during training, such as generalizing from hangers to mugs. In this paper, we present a novel approach, magic (manipulation analogies for generalizable intelligent contacts), for one-shot manipulation strategy learning. Shown in Fig. 1, given a single reference action trajectory (e.g., using a hook to reach for a distant object) and a novel scenario (e.g., with different tools and different objects), the goal of the algorithm is to generate a sequence of robot actions that apply a “similar” strategy to the test objects specified by users: in this example, having the target object moving along a certain direction for a given distance. magic extends two critical insights into a broad class of manipulation strategies. First, many strategies such as hooking, hanging, hammering, pushing, reaching [9, 10], stacking, pouring [11, 12, 13], and cutting [13] can be characterized by a sequence of contact waypoints (i.e., the order in which contacts between objects and robot bodies are made); second, these contacts are characterized by forceful affordances between object pairs: a specific pair of contact points on two objects would enable the application of forces along certain directions. However, searching for contact points that would enable the specific affordance is generally challenging due to complex constraints on reachability, collision avoidance, and motion stability. Figure 1: We introduce magic (Manipulation Analogies for Generalizable Intelligent Contacts), a pipeline that is capable of learning manipulation strategies from single demonstrations and applying them to novel objects. magic tackles these challenges by combining data-driven and analytic approaches to generate contact waypoints in novel scenarios. In particular, it first extracts the sequence of contacts among objects in the reference trajectory and then proposes (pairs of) contact points that have similar global and local shape properties as the contact points in the reference, which can be used as guidance for motion planning or motion retargeting. Finally, it utilizes a physical simulator to discard trajectories that fail to achieve the goal due to collisions, unstable physical contact, or violations of joint and torque limits. Our key innovation lies in a novel global-to-local matching algorithm to find functional correspondences between the target objects and reference objects. Intuitively, a “good” contact point would satisfy both a global and a local matching property. First, the points on two objects should be on similar parts of the global shape (e.g., in the hook-using example, we need a contact point on the tool that is at the end of a long rod). Second, the hooking contact point should have a matched local curvature with the target object being hooked so that we can execute the actions stably. Therefore, we propose to use a pretrained visual feature-based correspondence matching to resolve the global matching property. This enables us to quickly search over different parts of the objects but the resulting contact point is usually not precise. Next, we use a local curvature-based matching algorithm to find the best contact point within a local region of the previously proposed contact point, which gives us precise and physically plausible (e.g., collision-free and physically stable) contacts. Overall, magic tackles the problem of one-shot manipulation strategy learning by making analogies in contact waypoints. We validate the effectiveness of our approach on three challenging tasks: scooping a ball against a concave arc with a spoon, hanging a mug onto a mug tree, and using tools to hook objects of varying sizes. Compared to global shape-matching algorithms, our framework achieves significant improvements when the reference objects are from different categories than the test objects. Compared to local shape-matching and simulation-based approaches, our framework is orders of magnitude faster — for most test objects, we need to run simulations on fewer than three candidate contact points to find a solution. Finally, compared to pretrained feature-matching-based approaches, our method finds more precise and physically plausible solutions."
https://arxiv.org/html/2411.09623v1,Vision-based Manipulation of Transparent Plastic Bags in Industrial Setups,"This paper addresses the challenges of vision-based manipulation for autonomous cutting and unpacking of transparent plastic bags in industrial setups, aligning with the Industry 4.0 paradigm. Industry 4.0, driven by data, connectivity, analytics, and robotics, promises enhanced accessibility and sustainability throughout the value chain. The integration of autonomous systems, including collaborative robots (cobots), into industrial processes is pivotal for efficiency and safety. The proposed solution employs advanced Machine Learning algorithms, particularly Convolutional Neural Networks (CNNs), to identify transparent plastic bags under varying lighting and background conditions. Tracking algorithms and depth sensing technologies are utilized for 3D spatial awareness during pick and placement. The system addresses challenges in grasping and manipulation, considering optimal points, compliance control with vacuum gripping technology, and real-time automation for safe interaction in dynamic environments. The system’s successful testing and validation in the lab with the FRANKA robot arm, showcases its potential for widespread industrial applications, while demonstrating effectiveness in automating the unpacking and cutting of transparent plastic bags for an 8-stack bulk-loader based on specific requirements and rigorous testing.","I INTRODUCTION Industry 4.0—also called the Fourth Industrial Revolution or 4IR—is the next phase in the digitization of the manufacturing sector, driven by disruptive trends including the rise of data and connectivity, analytics, human-machine interaction, and improvements in robotics [1, 2]. This could make products and services more easily accessible and transmissible for businesses, consumers, and stakeholders all along the value chain [3]. Preliminary data indicate that successfully scaling 4IR technology makes supply chains more efficient and sustainable [4], creates a safer and more productive environment for the employees, reduces occupational accidents and factory waste, and has countless other benefits. Autonomous manipulation of plastic packages in industrial setups typically involves the use of robotic systems and automation technologies [5]. These systems are designed to handle, move, and manipulate plastic packages in a variety of industrial processes, such as packaging, recycling and sorting, food processing, and quality control [6, 7]. Collaborative robots, or cobots, are widely used in various industrial applications, working alongside humans without needing extensive safety barriers, cages, or other restrictive measures [8]. These robots use different sensors to identify their environment, recognise objects and are programmed for better accessibility, flexibility and repeatability. Example cases can be found in the textile industry as described in [9], where the authors proposed a dual arm collaborative system for textile material identification. By imitating human behavior, in this work the robots use actions such as pulling and twisting to identify and learn more about textile properties. In recent years, the recycling and waste management industry has begun to use vision-based robotic systems for the classification and accurate sorting of waste materials [10]. Indicative examples can be found in different recycling industries for the management of construction waste [11, 12], recyclable materials [13, 14] or electronic parts [15, 16]. The vision-based manipulation and autonomous cutting of transparent plastic bags presents a set of intricate challenges and a compelling need for innovative AI solutions [17, 18]. The inherent transparency of the bags poses difficulties in accurate detection due to the reflection and refraction of light, demanding sophisticated computer vision algorithms for reliable identification [19]. The deformable nature of plastic bags adds complexity to the grasping and manipulation process, necessitating advanced robotic control strategies to handle their variability [20]. Additionally, autonomous cutting requires well-considered mechanical design and precise vision-guided tools to discern optimal cutting points while avoiding unintended damage. Ensuring the safety and efficiency of these systems in real-time, dynamic environments further amplifies the challenge. The pressing need for such technologies arises from the increasing demand for automated waste management, recycling, and packaging processes, where vision-based systems can enhance efficiency, reduce human intervention, and contribute to sustainable practices by facilitating the effective processing of transparent plastic bags [21]. In this work, through the use of advanced Machine Learning algorithms, based on Convolutional Neural Networks (CNNs), the system can identify transparent plastic bags within its visual field, taking into account variations in lighting and background. Once the bags are detected, the system utilizes tracking algorithms to follow the pick and placement of the bags, and, integrate depth sensing technologies for 3D spatial awareness. The next steps involve developing algorithms for robotic grasping and manipulation, accounting for the challenges posed by the deformable and transparent nature of plastic bags. This includes considerations for optimal grasping points, compliance control using vacuum gripping technology, and real-time automation and processing to ensure effective and safe interaction with the bags in dynamic environments. The rest of the paper is organized as follows. Section II describes the mechanical design of the proposed system. Section III presents the object detection and manipulation approach based on deep-learning and Section IV presents the autonomous cutting mechanism and the automation process. The testing of the pilot proof-of-concept prototype is presented in Section V. Finally, the last section discusses the obtained results and highlights directions for future work."
https://arxiv.org/html/2411.09598v1,Assessing the Performance of the DINOv2 Self-supervised Learning Vision Transformer Model for the Segmentation of the Left Atrium from MRI Images,"Accurate segmentation of the left atrium from pre-operative scans is required for diagnosing atrial fibrillation, treatment planning, intraoperative guidance, and supporting computer-assisted surgical interventions. While deep learning models are pivotal in medical image segmentation, they often require extensive manually annotated datasets. However, the emergence of foundation models trained on larger datasets has helped to reduce this dependency, enhancing generalizability and robustness through transfer learning capabilities. In this work, we explore the out-of-the-box potential of DINOv2, a self-supervised learning vision transformer-based foundation model trained on natural images, by evaluating its performance in the left atrium (LA) segmentation task using MRI images. The challenges include the left atrium’s complex anatomical structures, thin myocardial walls, and limited annotated data, making it difficult to accurately segment the desired LA structures both prior to or during the image-guided intervention. We aim to demonstrate DINOv2’s ability to provide accurate and consistent segmentation in this specific context. We comprehensively evaluated the performance of DINOv2 in LA segmentation, utilizing end-to-end fine-tuning, and achieved a mean Dice score of 87.1% and an Intersection over Union (IoU) of 79.2%. Our study included data-level few-shot learning across different dataset sizes and patient counts, consistently finding that DINOv2 outperforms all baseline models. Furthermore, these comparisons suggest that DINOv2 can perform well out-of-the-box to match the above instances in the medical domain and effectively adapt and generalize to MRI data, even with minimal fine-tuning and limited data. These findings highlight DINOv2’s potential as a competitive tool for cardiac segmentation, providing accurate results essential for pre-procedural planning and pre-operative applications. Our study aims to inform medical researchers about DINOv2’s potential for broader implementation in other medical imaging modalities.","Atrial Fibrillation (AFib), a condition with irregular heart rhythm, is expected to affect 12 million people in the U.S. by 2030 [1]. Accurate segmentation of the left atrium (LA) is important for diagnosing and treating AFib, as it helps identify the condition and guide interventional procedures like catheter ablation and the Maze procedure, both aiming to restore normal heart rhythm [2]. Precise LA segmentation provides vital anatomical details, assisting surgeons in accurately targeting treatment areas, thereby enhancing the effectiveness of interventions and reducing complications. Additionally, it is essential for post-operative assessments, confirming the procedure’s success, and monitoring for potential recurrences, ultimately ensuring better patient outcomes. Recent advances in SSL have led to the development of powerful open-source AI models like DINOv2 [3], which have shown exceptional capabilities in zero-shot segmentation of natural images. Yet, significant differences exist between natural and medical image data, including variations in color, intensity, scaling, and anatomical structures [4]. Medical domain data often present unique characteristics depending on the imaging modality (CT, X-ray, or MRI). While experts can identify subtle changes and annotate these images accurately, deep learning models trained on natural images may perform less effectively in this domain. Given the challenges associated with collecting large annotated datasets comparable in size to those used in training DINOv2, it is worthwhile to investigate the potential of leveraging pre-trained DINOv2 models for medical image analysis, especially for segmentation tasks. This exploration could provide valuable insights into adapting such foundation models for specialized applications, bridging the gap between general-purpose AI and domain-specific demands. The LA is adjacent to other anatomical structures with similar intensities to the blood pool, and its thin myocardial wall (2-3 mm) challenges imaging, even with high-resolution techniques. The scarcity of annotated data further complicates the segmentation process. [5]. We specifically chose the LA as a focus area in light of the segmentation challenges it poses caused by its variable behavior and the scarcity of extensive annotated datasets. Our study explores the potential of using DINOv2 to obtain a sufficiently accurate segmentation of the LA from MRI images, driven by the challenges posed by the complex and dynamic anatomical structure of the LA. By evaluating DINOv2 in this context, we aim to demonstrate its capability to deliver precise and consistent segmentation outcomes, even in complex and data-constrained scenarios. We compared the performance of DINOv2 with state-of-the-art (SOTA) models such as Attention UNet (Att. UNet) [6], UNet [7], and pre-trained ResNet50 backbone with UNet (Res50-UNet) [8]. Additionally, we examine the performance of the model on data-level few-shot learning across various data percentages and patient counts. We chose to compare DINOv2 against Re50-UNet, UNet, and Attention UNet, despite their lower parameter counts and fully supervised training, to highlight how DINOv2, even with supervised fine-tuning on all data, excels in complex, data-constrained scenarios and successfully adapts from a natural domain to medical imaging. By leveraging DINOv2’s capability to focus on relevant features and adapt to new data, we aim to show its effectiveness in achieving accurate segmentation with minimal supervision."
https://arxiv.org/html/2411.09595v1,LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models,"This work explores expanding the capabilities of large language models (LLMs) pretrained on text to generate 3D meshes within a unified model. This offers key advantages of (1) leveraging spatial knowledge already embedded in LLMs, derived from textual sources like 3D tutorials, and (2) enabling conversational 3D generation and mesh understanding. A primary challenge is effectively tokenizing 3D mesh data into discrete tokens that LLMs can process seamlessly. To address this, we introduce Llama-Mesh, a novel approach that represents the vertex coordinates and face definitions of 3D meshes as plain text, allowing direct integration with LLMs without expanding the vocabulary. We construct a supervised fine-tuning (SFT) dataset enabling pretrained LLMs to (1) generate 3D meshes from text prompts, (2) produce interleaved text and 3D mesh outputs as required, and (3) understand and interpret 3D meshes. Our work is the first to demonstrate that LLMs can be fine-tuned to acquire complex spatial knowledge for 3D mesh generation in a text-based format, effectively unifying the 3D and text modalities. Llama-Mesh achieves mesh generation quality on par with models trained from scratch while maintaining strong text generation performance. ††footnotemark: 11footnotetext: Work completed during NVIDIA internship.","Large Language Models (LLMs) [52, 5] have demonstrated remarkable capabilities in understanding and generating human-like text, achieving success in applications such as conversational agents, code generation, and visual content reasoning [1, 16, 30]. Despite these advances, their generative abilities have primarily been limited to the textual content, restricting their utility for broader tasks. Our work seeks to extend LLMs into a new modality—3D mesh generation—unlocking significant potential for fields like computer graphics, engineering, robotics, and virtual/augmented reality. By enabling LLMs to generate 3D meshes from textual descriptions, we unify language understanding with 3D content creation, expanding the functional scope of LLMs. This approach paves the way for more intuitive and efficient workflows in 3D content creation driven by language-based instructions. However, integrating a new modality into an LLM is challenging, particularly in the tokenization process for processing the new modality. To the best of our knowledge, there have been no attempts to unify 3D mesh and text generation in a single framework. Some studies explored unifying image and text generation. Among these works [37, 55], a common approach is to train a new tokenizer such as a vector-quantized variational autoencoder (VQ-VAE) [51, 17] to encode the new modality into discrete tokens, which are used in training. However, this requires vocabulary expansion, increasing the adaptation’s learning cost. Additionally, this method introduces information loss during the auto-encoding process. To tackle these challenges, we introduce Llama-Mesh, a novel framework that enables large language models (LLMs) to generate 3D meshes by representing them as plain text. Our approach uses the OBJ file format, a widely adopted text-based standard for 3D models comprising vertex coordinates and face definitions, as shown in Figure 4. By treating these numerical values as a sequence of text, we convert 3D meshes into a format that LLMs can process directly, avoiding modifications to the tokenizer or the vocabulary, thus minimizing additional training overhead. This design capitalizes on the extensive knowledge embedded in pretrained LLMs. Figure 6 shows pretrained LLMs demonstrate a native ability to represent 3D structures in text — a capability our framework harnesses. We construct a supervised fine-tuning (SFT) dataset that includes text-3D pairs and interleaved text-3D dialogues. We fine-tune a pretrained LLaMA-3.1-8B-Instruct [16] model on our curated dataset. We find that LLMs can acquire complex spatial knowledge by learning the numerical values of meshes in textual format. After fine-tuning, our model demonstrates the ability to (1) generate 3D meshes given text prompts, (2) produce interleaved outputs of text and 3D meshes in a conversational setup, and (3) describe meshes in natural language. Llama-Mesh is the first successful effort to empower an LLM to generate 3D content with language, unifying the 3D and text modalities in a single large model. It achieves mesh generation quality comparable to models trained from scratch while maintaining strong text generation abilities. Figure 3: Gallery of generations from Llama-Mesh. We can generate high-quality and diverse meshes with artist-like created topology."
https://arxiv.org/html/2411.09593v1,SMILE-UHURA Challenge - Small Vessel Segmentation at Mesoscopic Scale from Ultra-High Resolution 7T Magnetic Resonance Angiograms,"The human brain receives nutrients and oxygen through an intricate network of blood vessels. Pathology affecting small vessels, at the mesoscopic scale, represents a critical vulnerability within the cerebral blood supply and can lead to severe conditions, such as Cerebral Small Vessel Diseases. The advent of 7 Tesla MRI systems has enabled the acquisition of higher spatial resolution images, making it possible to visualise such vessels in the brain. However, the lack of publicly available annotated datasets has impeded the development of robust, machine learning-driven segmentation algorithms. To address the complexities of mesoscopic vessel segmentation and to highlight the need for advanced techniques to manage the high noise levels and poor vessel-to-background contrast inherent in ”ultra-high-resolution” data, the SMILE-UHURA challenge was organised. This challenge, held in conjunction with the ISBI 2023, in Cartagena de Indias, Colombia, aimed to provide a platform for researchers working on related topics. The SMILE-UHURA challenge addresses the gap in publicly available annotated datasets by providing an annotated dataset of Time-of-Flight angiography acquired with 7T MRI. This dataset was created through a combination of automated pre-segmentation and extensive manual refinement. In this manuscript, sixteen submitted methods and two baseline methods are compared both quantitatively and qualitatively on two different datasets: held-out test MRAs from the same dataset as the training data (with labels kept secret) and a separate 7T ToF MRA dataset where both input volumes and labels are kept secret. The results demonstrate that most of the submitted deep learning methods, trained on the provided training dataset, achieved reliable segmentation performance. Dice scores reached up to 0.838 ± 0.066 and 0.716 ± 0.125 on the respective datasets, with an average performance of up to 0.804 ± 0.15.","Brain function relies on the cerebral vasculature to supply nutrients and oxygen. Any impairment of the vasculature can damage brain tissue, potentially leading to cognitive decline. The cerebral vasculature is organised as a hierarchical, tree-like network, where vessel diameter decreases while the number of branches increases with higher branch order. For major cerebral vessels at the macroscopic scale and for capillaries, arterioles, and venules at the microscopic scale, in vivo and ex vivo imaging modalities are available, respectively. However, assessing the mesoscopic scale (vessel diameters of 100–500 µm) remains challenging. Pathologies at the mesoscopic scale are potentially linked to ageing, dementia, and Alzheimer’s disease [1, 2]. Segmentation and quantification of these vessels are crucial steps in the investigation of Cerebral Small Vessel Disease (CSVD) [3, 4]. Recently, ultra-high field (UHF) magnetic resonance imaging (MRI) has emerged as a means of bridging the gap between macroscopic and microscopic assessments of the human cerebral vasculature. Following pioneering work on magnetic resonance angiography (MRA) at 7 Tesla (7T) [5, 6], the field has advanced significantly, achieving the highest resolutions to date [7, 8] — as high as 150 µm and 140 µm, respectively. These advancements enable imaging of mesoscopic vessels, which are highly relevant to understanding cerebral small vessel diseases, neurodegeneration, and the origins of the functional fMRI signal. However, automatic segmentation of vessels at this scale has yet to be established. To address this need within the neurological and neuroscientific community, this challenge was initiated, focusing on the segmentation of vasculature at the mesoscopic scale. While vessel segmentation challenges have a long tradition, using UHF MRI for mesoscopic vessels presents unique difficulties compared to 2D microscopic or 3D macroscopic vessel imaging and segmentation: (I) instead of a single 2D image per sample, a 3D volume is acquired, significantly increasing computational demands and making manual segmentation highly time-consuming, and (II) compared to macroscopic segmentation, ultra-high-resolution data is noisier and exhibits poorer vessel-to-background contrast, complicating both automatic and manual segmentation. These challenges have hindered the establishment of openly accessible data repositories and the development of high-performance mesoscopic vessel segmentation algorithms. Currently, no high-resolution 7T dataset with annotations is available for training machine learning-based segmentation methods or benchmarking performance. To address this gap, an annotated dataset of Time-of-Flight (ToF) angiography acquired with a 7T MRI was created for this challenge. This dataset was generated using a combination of automatic pre-segmentation and extensive manual refinement. It serves as the foundation of this challenge and provides a benchmark for quantitative performance assessment, facilitating future advancements in mesoscopic vessel segmentation."
https://arxiv.org/html/2411.09502v1,Golden Noise for Diffusion Models: A Learning Framework,"Text-to-image diffusion model is a popular paradigm that synthesizes personalized images by providing a text prompt and a random Gaussian noise. While people observe that some noises are “golden noises” that can achieve better text-image alignment and higher human preference than others, we still lack a machine learning framework to obtain those golden noises. To learn golden noises for diffusion sampling, we mainly make three contributions in this paper. First, we identify a new concept termed the noise prompt, which aims at turning a random Gaussian noise into a golden noise by adding a small desirable perturbation derived from the text prompt. Following the concept, we first formulate the noise prompt learning framework that systematically learns “prompted” golden noise associated with a text prompt for diffusion models. Second, we design a noise prompt data collection pipeline and collect a large-scale noise prompt dataset (NPD) that contains 100k pairs of random noises and golden noises with the associated text prompts. With the prepared NPD as the training dataset, we trained a small noise prompt network (NPNet) that can directly learn to transform a random noise into a golden noise. The learned golden noise perturbation can be considered as a kind of prompt for noise, as it is rich in semantic information and tailored to the given text prompt. Third, our extensive experiments demonstrate the impressive effectiveness and generalization of NPNet on improving the quality of synthesized images across various diffusion models, including SDXL, DreamShaper-xl-v2-turbo, and Hunyuan-DiT. Moreover, NPNet is a small and efficient controller that acts as a plug-and-play module with very limited additional inference and computational costs, as it just provides a golden noise instead of a random noise without accessing the original pipeline.","Image synthesis that are precisely aligned with given text prompts remains a significant challenge for text-to-image (T2I) diffusion models [2, 4, 31, 24, 25]. Previous studies [43, 38, 37, 15, 18] have investigated the influence of text embeddings on the synthesized images and leveraged these embeddings for training-free image synthesis. It is well known that text prompts significantly matter to the quality and fidelity of the synthesized images. However, image synthesis is induced by both the text prompts and the noise. Variations in the noise can lead to substantial changes in the synthesized images, as even minor alterations in the noise input can dramatically influence the output [42, 27]. This sensitivity underscores the critical role that noise plays in shaping the final visual representation, affecting both the overall aesthetics and the semantic faithfulness between the synthesized images and the provided text prompt. Recent studies [22, 7, 5, 27, 3] observe that some selected or optimized noises are golden noises that can help the T2I diffusion models to produce images of better semantic faithfulness with text prompts, and can also improve the overall quality of the synthesized images. These methods [3, 7] incorporate extra modules like attention to reduce the truncate errors during the sampling process, showing promising results on the compositional generalization task. However, they are often not widely adopted in practice for several reasons. First, they often struggle to generally transfer to various benchmark datasets or diffusion models but only work for some specific tasks. Second, these methods often introduce significant time delays in order to optimize the noises during the reverse process. Third, they require in-depth modifications to the original pipelines when applied to different T2I diffusion models with varying architectures. Fourth, they need specific subject tokens for each prompt to calculate the loss of certain areas, which are unrealistic requirements for real users. These not only significantly complicate the original inference pipeline but also raise concerns regarding the generalization ability across various T2I diffusion models and datasets. In light of the aforementioned research, we pose several critical questions: 1) Can we formulate obtaining the golden noises as a machine learning problem so that we can predict them efficiently with only one model forward inference? 2) Can such a machine learning framework generalize well to various noises, prompts, and even diffusion models? Fortunately, the answers are affirmative. In this paper, we mainly make three contributions: First, we identify a new concept termed noise prompt, which aims at turning a random noise into a golden noise by adding a small desirable perturbation derived from the text prompt. The golden noise perturbation can be considered as a kind of prompt for noise, as it is rich in semantic information and tailored to the given text prompt. Building upon this concept, we formulate a noise prompt learning framework that learns “prompted” golden noises associated with text prompts for diffusion models. Second, to implement the formulated noise prompt learning framework, we propose the training dataset, namely the noise prompt dataset (NPD), and the learning model, namely the noise prompt network (NPNet). Specifically, we design a noise prompt data collection pipeline via re-denoise sampling, a way to produce noise pairs. We also incorporate AI-driven feedback mechanisms to ensure that the noise pairs are highly valuable. This pipeline enables us to collect a large-scale training dataset for noise prompt learning, so the trained NPNet can directly transform a random Gaussian noise into a golden noise to boost the performance of the T2I diffusion model. Figure 2: Our workflow diagram consists of three main stages. Stage I: We begin by denoising the original random Gaussian noise \mathbf{x}_{T} to obtain \mathbf{x}_{T-1}, and then use \operatorname{DDIM-Inversion}(\cdot) to obtain inverse \mathbf{x}^{\prime}_{T} with more semantic information. Both synthesized images \mathbf{x}_{0} and \mathbf{x}^{\prime}_{0} are filtered by the human preference model, such as HPSv2, to ensure the dataset is both diverse and representative. Stage II: After collecting NPD, we input the original noise (source noise) \mathbf{x}_{T}, inverse noise (target noise) \mathbf{x}^{\prime}_{T} and text prompt \mathbf{c} into the NPNet, where the noises are processed by the singular value predictor and the residual predictor, and text prompt \mathbf{c} is encoded by the text encoder \mathcal{E}(\cdot) of the T2I diffusion model, resulting in the golden noise. Stage III: Once trained, our NPNet can directly convert the random Gaussian noise into a golden noise before inputting T2I diffusion models, boosting the performance of these models. Third, we conduct extensive experiments across various mainstream diffusion models, including StableDiffusion-xl (SDXL) [26], DreamShaper-xl-v2-turbo and Hunyuan-DiT [16], with 7 different samplers on 4 different datasets. We evaluate our model by utilizing 6 human preference metrics including Human Preference Score v2 (HPSv2) [39], PickScore [14], Aesthetic Score (AES) [34], ImageReward [41], CLIPScore [8] and Multi-dimensional Preference Score (MPS) [45]. As illustrated in Fig. 1, by leveraging the learned golden noises, not only is the overall quality and aesthetic style of the synthesized images visually enhanced, but all metrics also show significant improvements, demonstrating the effectiveness and generalization ability of our NPNet. For instance, on GenEval, our NPNet let SDXL improve the classical evaluation metric HPSv2 by 18% (24.04\rightarrow28.41), which even surpasses a recent much stronger DiT-based diffusion model Hunyuan-DiT (27.78). Furthermore, the NPNet is a compact and efficient neural network that functions as a plug-and-play module, introducing only a 3% extra inference time per image compared to the standard pipeline, while requiring approximately 3% of the memory required by the standard pipeline. This efficiency underscores the practical applicability of NPNet in real-world scenarios."
https://arxiv.org/html/2411.09402v2,Automated Segmentation of Ischemic Stroke Lesions in Non-Contrast Computed Tomography Images for Enhanced Treatment and Prognosis,"Stroke is the second leading cause of death worldwide, and is increasingly prevalent in low- and middle-income countries (LMICs). Timely interventions can significantly influence stroke survivability and the quality of life after treatment. However, the standard and most widely available imaging method for confirming strokes and their sub-types, the NCCT, is more challenging and time-consuming to employ in cases of ischemic stroke. For this reason, we developed an automated method for ischemic stroke lesion segmentation in NCCTs using the nnU-Net frame-work, aimed at enhancing early treatment and improving the prognosis of ischemic stroke patients. We achieved Dice scores of 0.596 and Inter-section over Union (IoU) scores of 0.501 on the sampled dataset. After adjusting for outliers, these scores improved to 0.752 for the Dice score and 0.643 for the IoU. Proper delineation of the region of infarction can help clinicians better assess the potential impact of the infarction, and guide treatment procedures.","There is an estimated 15 million cases of stroke annually, with one-third of these cases leading to death, and another one-third leading to some form of disability [3]. The rate of prevalence is more apparent in Low- and Middle-Income Countries (LMICs), where existing healthcare infrastructures remain unprepared to deal with the surge in cases [16, 9]. Timely intervention is a critical factor in the survivability and post-treatment quality of life of patients [15]. Expert radiologist interpretations of the standard Non-Contrast Computed Tomography (NCCT) images are far more accurate for hemorrhagic stroke (95%), when compared to ischemic stroke where a diagnostic ruling is given for major onsets only two-thirds of the time [10, 15]. CT imaging is the most accessible modality for brain diagnostic imaging in LMICs [2]. The process of diagnosing stroke, and subsequently segmenting the lesion region using radiological images can guide treatment and predict rehabilitation outcomes [12, 4]. Manual segmentation can be highly time-consuming and error-prone, with significant inter-observer variability depending on the observer’s level of expertise [17]. The process of segmenting lesions in NCCT images is a relatively difficult task (due to low tissue contrast) as compared to MRI [14], especially when done manually. It is important that robust automated lesion segmentation processes for varying modalities (in this case, NCCT) be made available, especially in regions with limited imaging modality accessibility. Several works explore the segmentation of stroke lesions, employing a variety of approaches. Liang et. al [11] proposed the Symmetry-Enhanced Attention Network (SEAN) to automatically segment acute ischemic infarcts in NCCTs. The input image is first transformed into the standard space in an unsupervised manner by an ’Alignment Network’. In the transformed image with its bilateral symmetry, long range dependencies are better captured by SEAN, which has proven to outperform other existing symmetry based methods, with a dice score of 0.5784. The nnU-Net was developed by Fabian et. al [6], and is described as an out-of-the-box tool for biomedical image segmentation. It is a well-validated segmentation method, and has achieved state-of-the-art (SOTA) results in various biomedical image segmentation benchmark datasets such as the Medical Segmentation Decathlon (MSD) [1], Brain Tumor Segmentation (BraTS) Challenge [13], and the Ischemic Stroke Lesion Segmentation Challenge (ISLES) [5]. In this paper, we achieve promising results in ischemic lesion segmentation in NCCTs, by using out-of-the-box, well-validated methods. Initial image processing involved normalizing the input NCCT scans to ensure uniformity in image quality and resolution. We employed a Residual Encoder U-Net architecture from the nnU-Net framework, known for its robust performance in medical image segmentation tasks. The network was trained using a curated dataset consisting of NCCT images which were annotated using DWI MRI scans of the subjects as reference. The results obtained reiterate the potential of using validated methods to automate the difficult task of ischemic lesion segmentation in NCCT scans. Comparisons with existing techniques highlight the improvements and underscore areas for future enhancements."
https://arxiv.org/html/2411.09373v1,Are nuclear masks all you need for improved out-of-domain generalisation? A closer look at cancer classification in histopathology,"Domain generalisation in computational histopathology is challenging because the images are substantially affected by differences among hospitals due to factors like fixation and staining of tissue and imaging equipment. We hypothesise that focusing on nuclei can improve the out-of-domain (OOD) generalisation in cancer detection. We propose a simple approach to improve OOD generalisation for cancer detection by focusing on nuclear morphology and organisation, as these are domain-invariant features critical in cancer detection. Our approach integrates original images with nuclear segmentation masks during training, encouraging the model to prioritise nuclei and their spatial arrangement. Going beyond mere data augmentation, we introduce a regularisation technique that aligns the representations of masks and original images. We show, using multiple datasets, that our method improves OOD generalisation and also leads to increased robustness to image corruptions and adversarial attacks. The source code is available at https://github.com/undercutspiky/SFL/","Domain generalisation in histopathology is a crucial challenge because domain shifts naturally occur among hospitals and even within a single hospital or laboratory, e.g., temporally or among human operators and observers such as pathologists. Non-biological factors that substantially alter the images include differences in scanners, staining protocols, fixation of tissue, and even minor aspects like the manufacturer and storage conditions of stains [1]. Collecting data from numerous hospitals to address these domain shifts is often impractical and may not adequately reflect the full variability present in routine clinical practice, thus making it difficult to build computational histopathology models that generalise well. This leads us to focus on single-domain generalisation (S-DG) in this paper, specifically on how to train a model using data from only one hospital (considered a domain here) that generalises well to data from other hospitals. Popular S-GD methods in histopathology apply data augmentation and stain normalisation [2, 3]. The effectiveness of S-GD methods developed for natural images remains underexplored in histopathology [3]. Here, we compare these methods to a new, simple approach that we propose. Research has shown that Convolutional Neural Networks (CNNs) tend to focus on texture over shape [4, 5]. However, in histopathology, the texture and colour of cell nuclei vary much more across domains than the shape and organisation of cell nuclei. As a result, focusing on shape features could improve a computational histopathology model’s ability to generalise to unseen data because it may rely less on domain-specific features that vary across hospitals and more. Nuclei in cancerous tissue exhibit distinct changes in shape, size, and overall organisation compared to nuclei in normal tissue [6, 7, 8]. Pathologists rely on these and other visual cues [9] for cancer diagnosis and grading, underscoring the biological importance and the consistency of nuclear morphology and organisation across domains. We hypothesise that focusing on nuclear morphology and organisation may be sufficient for cancer detection and that exploiting this during training could result in models with good generalisation. We propose a method that encourages CNNs to focus more on nuclear morphology and organisation by using additional loss terms that prioritise shape-based features. Specifically, our method leverages nuclear segmentation masks during training to steer the learning towards nuclei. Through extensive experimentation, we demonstrate that this method improves performance on out-of-domain data without requiring nuclear segmentation masks at inference time, thus offering a promising and attractive solution for addressing domain generalisation in histopathology. Our contributions include: • We propose a novel training method that incentivises the model to focus on nuclei. • We evaluate our method on three datasets comprising hundreds of WSIs in total from various hospitals and organs. Our results show accuracy improvements over all other approaches. • We evaluate the sensitivity of our method to image corruptions and adversarial attacks. Our results show performance improvements over the baseline. • We conduct extensive ablation studies to show that models trained with our method focus on nuclei."
https://arxiv.org/html/2411.09308v1,DT-JRD: Deep Transformer based Just Recognizable Difference Prediction Model for Video Coding for Machines,"Just Recognizable Difference (JRD) represents the minimum visual difference that is detectable by machine vision, which can be exploited to promote machine vision oriented visual signal processing. In this paper, we propose a Deep Transformer based JRD (DT-JRD) prediction model for Video Coding for Machines (VCM), where the accurately predicted JRD can be used reduce the coding bit rate while maintaining the accuracy of machine tasks. Firstly, we model the JRD prediction as a multi-class classification and propose a DT-JRD prediction model that integrates an improved embedding, a content and distortion feature extraction, a multi-class classification and a novel learning strategy. Secondly, inspired by the perception property that machine vision exhibits a similar response to distortions near JRD, we propose an asymptotic JRD loss by using Gaussian Distribution-based Soft Labels (GDSL), which significantly extends the number of training labels and relaxes classification boundaries. Finally, we propose a DT-JRD based VCM to reduce the coding bits while maintaining the accuracy of object detection. Extensive experimental results demonstrate that the mean absolute error of the predicted JRD by the DT-JRD is 5.574, outperforming the state-of-the-art JRD prediction model by 13.1%. Coding experiments shows that comparing with the VVC, the DT-JRD based VCM achieves an average of 29.58% bit rate reduction while maintaining the object detection accuracy.","With the rapid development of Artificial Intelligence (AI) and multimedia communication, remote intelligent visual analytics [1] is highly demanded for smart city, privacy preserving, robotics and remote control, etc.. Video/Image Coding for Machine (VCM/ICM) [2] that aims to encode visual content more efficiently for remote visual analysis is highly demanded for machine-to-machine visual communication. The main challenge in the VCM field is to reduce coding bit rate while maintaining high performance in machine vision tasks. Conventionally, there are a number of visual coding standards, such as JPEG[3] and JPEG2000[4] for image compression and High Efficiency Video Coding (HEVC)[5] and Versatile Video Coding (VVC)[6] for video compression, which exploited spatial-temporal correlation, symbol redundancies as well as the visual redundancies in Human Visual System (HVS). With the vigorous development of deep learning, a number of Learned Image Compression (LIC) methods have been developed, such as Non-Local Attention optimization and Improved Context modeling-based image compression (NLAIC)[7], Efficient Learned Image Compression (ELIC)[8] and Multi-reference entropy model based Learned Image Compression (MLIC)[9], which surpassed traditional image coding with deep network architecture and advanced entropy model. Thanks to the powerful feature extraction and representation abilities, the LIC represented the visual content well and exploited the redundancies effectively for human vision. Both traditional and learned image/video codecs mainly leveraged visual properties of HVS and aimed to minimize the image/video quality degradation at given bit rate constraints. Due to the contrast and brightness visual sensitivities, and spatial-temporal and pattern masking effects in HVS, distortion below the Just Noticeable Difference (JND) are not perceivable. Based on the understandings of the visual properties in HVS, many JND estimation models have been proposed, which mainly include pixel-domain JND model[10], transform-domain JND model, block-level JND model[11, 12], and picture/video-wise JND model[13, 14, 15, 16] . Liu et al.[15] modeled Picture-Wise JND (PW-JND) prediction as a multi-class classification which was solved by using multiple binary classifications. Moreover, a deep learning-based binary predictor was proposed to predict whether the distortion of an image was visually perceivable or not as compared to its reference image. By additionally fusing the temporal information extracted from the optical flow of the video, Zhang et al. [16] proposed a Video Wise Spatial-Temporal Satisfied User Ratio (VW-STSUR) model where the JND of the video was determined as 75% of the Satisfied User Ratio (SUR). Nami et al.[17] formulated JND prediction as a multi-task problem, where JND prediction was jointly learned with reconstructing JND-quality frames. Furthermore, three JND levels in HVS were learned simultaneously, which not only improved the prediction accuracy but also reduced the complexity. As one of the most important visual mechanisms in HVS, JND inspires perceptually optimized coding methods [18, 19] which leverage the visual redundancies and improve the perceptual quality . Zhou et al.[20] proposed a Rate-Distortion (R-D) model based on the JND factor and developed a rate control approach for HEVC. Zhu et al.[21] incorporated the pixel-wise JND model into the Rate Distortion Optimization (RDO) for the AV1 encoder and proposed a perceptual distortion measure combining Mean Squared Error (MSE) and JND to adjust the Lagrange multiplier. With breakthroughs in end-to-end image compression networks, JND was employed in their perceptual optimization as well. Ding et al.[22] designed a JND-based perceptual quality loss and a distortion-aware adjuster, outperforming the baseline end-to-end networks. Based on the JND dataset Videoset[23], Pakdaman et al.[24] proved that image-wise JND loss and feature-wise JND loss helped to achieve an improvement in rate-distortion performance for learned image compression. These works were proposed to model or exploit the JND in HVS. However, they can hardly be applicable to VCM as they did not consider the characteristics of machine vision. Recently, VCM aims to minimize the coding bit rate while maintaining the performance of downstream machine vision tasks. VCM becomes a hot research topic in academia and industry. With the development of learning based image/video compression, a number of works on end-to-end VCM networks have been developed. Le et al.[25] proposed a machine-oriented learned image codec and a dynamic loss weighting strategy to balance rate loss, distortion loss, and machine vision task loss. Gao et al.[26] proposed semantic-oriented metrics for developing LIC in multiple machine vision tasks. Choi et al.[27] designed a scalable hierarchical codec that used different levels of features in the latent space for multiple tasks, including reconstruction, detection, and segmentation. Chen et al.[28] applied visual prompts on Transformer-based LIC, which achieved remarkable transferable performance on various machine tasks. Feng et al.[29] proposed an omnipotent feature learning framework based on contrastive learning, and constrained entropy by an information filtering module to make the framework more suitable for feature compression. These approaches optimized the VCM in an end-to-end way from network architectures, optimization objectives, multi-task perspective, and so on. However, the end-to-end implicit feature learning in these VCM approaches did not effectively exploit machine vision properties. To further optimize the VCM, it is important to reveal and exploit the perceptual mechanism of machine vision. Recent studies revealed that machine vision has different wider frequency bandwidth[30], higher contrast sensitivity [31], and lower robustness [32] as compared with human vision. On contrary, machine vision also has attention mechanisms similar to the visual attention, visual masking, and non-local constraints in human vision. Moreover, it was found that there is a Just Recognizable Difference (JRD) for machine vision[33], indicating the minimum distortion that affects the performance of machine vision significantly. To investigate the JRD, Jin et al. [34] proposed a Deep Machine Vision-Just Noticeable Difference (DMV-JND) model to find visual redundant features for each pixel in an image. With the assistance of the magnitude and spatial restraint strategies from DMV-JND, distorted images with a lower average Peak Signal-to-Noise Ratio (PSNR) were generated while maintaining the classification accuracy at an acceptable level. Akan et al.[35] proposed a machine-perceived JND measurement and an adversarial image generation algorithm that added noise continuously to an image until the model output incorrect labels, deceiving the machine vision model. Motivated by the concept of SUR, Zhang et al.[36] established a large-scale Satisfied Machine Ratio (SMR) dataset, which contained 72 and 98 models for classification and detection respectively. Based on the SMR dataset, they proposed to predict the SMR for perceptual VCM. This work took into account the properties of diverse machines and was conducive to general ICM/VCM. Zhang et al.[33] proposed an ensemble-learning-based JRD prediction framework for object detection. By decomposing multi-classification into 8 binary classifiers, coarse-grained prediction results were searched to get the predicted JRD. Zhang et al.[37] built an Object-Wise JRD (OW-JRD) dataset with 29218 objects and each object image has 64 distortion levels from VVC intra coding. Then, they proposed a EfficientNet-based OW-JRD prediction model to predict the JRD from 64 classes, which was decomposed into multiple binary classifications by using binary classifiers. Nevertheless, this method required multiple times of image compression and prediction, which caused additional complexity overheads. Also, the JRD prediction accuracy could be further improved. In this paper, we propose a Deep Transformer-based JRD (DT-JRD) prediction model for VCM. The contributions are 1. We model the JRD prediction as a multi-class classification problem and propose a novel DT-JRD framework, which integrates an improved embedding, a content and distortion feature extraction, a multi-class classification, and a novel learning strategy. 2. To improve the DT-JRD model training, we propose an asymptotic JRD loss by using Gaussian Distribution-based Soft Labels (GDSL), which extends the number of training labels and relaxes classification boundaries. 3. We propose a DT-JRD based VCM optimization, which reduces the coding bits while maintaining the accuracy of object detection. The remainder of this paper is organized as follows. Section II-A describes the motivation and problem of JRD prediction. Section II-B to Section II-D presents the framework and key modules of DT-JRD. Section II-E introduces the DT-JRD based VCM method. Section III presents the experimental results and analysis. Section IV draws the conclusion."
https://arxiv.org/html/2411.09283v1,Leveraging Auxiliary Classification for Rib Fracture Segmentation,"Thoracic trauma often results in rib fractures, which demand swift and accurate diagnosis for effective treatment. However, detecting these fractures on rib CT scans poses considerable challenges, involving the analysis of many image slices in sequence. Despite notable advancements in algorithms for automated fracture segmentation, the persisting challenges stem from the diverse shapes and sizes of these fractures. To address these issues, this study introduces a sophisticated deep-learning model with an auxiliary classification task designed to enhance the accuracy of rib fracture segmentation. The auxiliary classification task is crucial in distinguishing between fractured ribs and negative regions, encompassing non-fractured ribs and surrounding tissues, from the patches obtained from CT scans. By leveraging this auxiliary task, the model aims to improve feature representation at the bottleneck layer by highlighting the regions of interest. Experimental results on the RibFrac dataset demonstrate significant improvement in segmentation performance.","Rib fractures present a critical concern in cases of traumatic chest injuries due to their pivotal role in protecting the thoracic organs (Dogrul et al., 2020). These fractures, often stemming from blunt force trauma, can vary widely in severity from minor cracks to complete breaks, each carrying significant implications for patient health. When ribs sustain fractures, especially in multiple locations, they can give rise to a range of complications, raising immediate concerns such as the potential for thoracic hemorrhage (Kuo and Kim, 2019) and pneumothorax (Kim and Moore, 2020). Additionally, the fractured ends of the ribs harbor the potential to inflict harm upon adjacent structures, including the heart, lungs, or major blood vessels (Davoodabadi et al., 2022), underscoring the critical need for prompt and thorough medical assessment and treatment. Through the precise diagnosis and management of rib fractures, healthcare teams can mitigate potential complications, minimize associated risks, and enhance overall patient care in the realm of traumatic chest injuries. Computed Tomography (CT) scans have emerged as the preferred modality for diagnosing rib fractures and assessing accompanying injuries in patients presenting with traumatic chest injuries (Sochor et al., 2003; Oikonomou and Prassopoulos, 2011; Awais et al., 2019). CT scans offer highly detailed, cross-sectional images of the chest area, enabling healthcare providers to visualize fractures precisely and determine their extent and severity. Despite the remarkable capabilities of CT scans, manually reviewing these CT slices for rib fractures can be labor-intensive and susceptible to human error. This is particularly evident in cases where fractures are subtle or distributed across multiple slices of the scan, as shown in Fig. 1. A meticulous examination is essential, as undetected or misinterpreted fractures can seriously affect patient care and treatment outcomes. Thus, there is a strong imperative to develop automated and accurate methods for identifying rib fractures in clinical applications. Figure 1. The first row of images displays fractured regions of different sizes and shapes, appearing as irregularities and disruptions in the bone structure. In the second row, the ground truth annotations are overlaid on the fractured regions in the corresponding CT images from the first row. Traditional methods to segment bone fractures (Anu and Raman, 2015; Ruikar et al., 2019) rely on handcrafted feature extraction followed by segmentation and are highly susceptible to errors. The application of deep learning (DL) in medical imaging has revolutionized medical image segmentation, offering algorithms that streamline the process by automatically extracting intricate features from imaging data. Models such as UNet (Ronneberger et al., 2015), UNet++ (Zhou et al., 2018), SegNet (Xing et al., 2022), and FCN (Roth et al., 2018) have become indispensable for semantic segmentation, offering a robust framework for medical image analysis. The encoder-decoder structure of UNet and skip connections make it particularly adept at handling the complexities of medical imaging. It is crucial to consider the unique characteristics of the rib region when segmenting fractures to avoid false positives from distant areas and gain insight into fracture patterns in healthy bone structures. Jin et al. (Jin et al., 2020b) proposed FracNet, a 3DUNet-based model for segmenting fractured ribs on the RibFrac dataset. To handle variations in fracture size, Liu et al. (Liu et al., 2021) devised a multi-scale network to integrate multiple sizes of fractures to minimize size alteration. Cao et al. (Cao et al., 2023) proposed SA-FracNet, a fracture shape-aware multi-task segmentation network to delineate the fracture and improve performance over the FracNet model. However, SA-FracNet necessitates an extra network trained through self-supervised contrastive learning for its initialization, which can distinguish between fractured and non-fractured regions. The investigation by Phan et al.(Phan et al., 2023) utilizes an additional segmentation task to improve the performance of image-level classification. Considering an alternative approach, the patch-level annotations can be used to identify the voxel-level annotations. This patch-level classification task facilitates a more thorough understanding of regions where fractures are more probable to occur, thereby serving as an auxiliary task. We propose a network that segments fracture regions aided by the auxiliary classification task, which helps identify the potential fracture sites through a classifier and increases the segmentation performance."
https://arxiv.org/html/2411.09263v1,Rethinking Weight-Averaged Model-merging,"Weight-averaged model-merging has emerged as a powerful approach in deep learning, capable of enhancing model performance without fine-tuning or retraining. However, the underlying mechanisms that explain its effectiveness remain largely unexplored. In this paper, we investigate this technique from three novel perspectives to provide deeper insights into how and why weight-averaged model-merging works: (1) we examine the intrinsic patterns captured by the learning of the model weights, through the visualizations of their patterns on several datasets, showing that these weights often encode structured and interpretable patterns; (2) we investigate model ensemble merging strategies based on averaging on weights versus averaging on features, providing detailed analyses across diverse architectures and datasets; and (3) we explore the impact on model-merging prediction stability in terms of changing the parameter magnitude, revealing insights into the way of weight averaging works as regularization by showing the robustness across different parameter scales. Our findings shed light on the “black box” of weight-averaged model-merging, offering valuable insights and practical recommendations that advance the model-merging process.","Model-merging can combine multiple independently trained models into a single, efficient model without introducing extra inference computations. This approach has been applied in contexts such as natural language processing [36], computer vision [14], and their sub-areas (e.g. federated learning [30], distillation [11], adversarial learning [42, 3] and large language model [20, 43]), where the ability to merge models effectively can lead to improved model performance, efficiency, and flexibility in deployment. Despite the practical importance of model-merging, there has been limited research on the investigation of the underlying factors that make it effective, especially weight-averaged model-merging [33] (e.g. Uniform Soups and Greedy Soups) as the mainstream techniques. Without a clear understanding of the underlying mechanisms of model-merging, researchers and practitioners may lack guidance on when and how to apply it in a principled way. This research gap motivates the need to systematically explore the underlying patterns and behaviors in weight-averaged model-merging. In this paper, we address this gap by providing a comprehensive analysis of weight-averaged model-merging from the following three key perspectives: • An examination of the patterns contained in the averaged model weights through the visualization of the patterns present in both linear layer as classifiers or inside deep learning models used on multiple datasets. Results reveal that clear and structured patterns emerge within the weights, suggesting that weight averaging can be seen as a linear vector combination, which contributes to the success of merging models via weight operations; • An investigation of averaging on weights versus averaging on features strategies allows us to evaluate the relative effectiveness and limitations of model-merging and traditional model ensembles across multiple architectures and datasets, uncovering distinct benefits and behaviors; • An exploration of the robustness of model-merging predictions with respect to different weight magnitudes/variances, which provides insights into the model-merging works as a type of of model regularization. Based on our viewpoints and findings, we conclude with useful proposals and source code that will be publicly available to provide a good reference for the model-merging research community."
https://arxiv.org/html/2411.09251v1,Cross Space and Time: A Spatio-Temporal Unitized Model for Traffic Flow Forecasting,"Predicting spatio-temporal traffic flow presents significant challenges due to complex interactions between spatial and temporal factors. Existing approaches often address these dimensions in isolation, neglecting their critical interdependencies. In this paper, we introduce the Spatio-Temporal Unitized Model (STUM), a unified framework designed to capture both spatial and temporal dependencies while addressing spatio-temporal heterogeneity through techniques such as distribution alignment and feature fusion. It also ensures both predictive accuracy and computational efficiency. Central to STUM is the Adaptive Spatio-temporal Unitized Cell (ASTUC), which utilizes low-rank matrices to seamlessly store, update, and interact with space, time, as well as their correlations. Our framework is also modular, allowing it to integrate with various spatio-temporal graph neural networks through components such as backbone models, feature extractors, residual fusion blocks, and predictive modules to collectively enhance forecasting outcomes. Experimental results across multiple real-world datasets demonstrate that STUM consistently improves prediction performance with minimal computational cost. These findings are further supported by hyperparameter optimization, pre-training analysis, and result visualization. We provide our source code for reproducibility at https://anonymous.4open.science/r/STUM-E4F0.","Rapid economic growth and the surge in vehicle numbers have intensified traffic congestion and parking challenges in urban areas globally. To address these challenges, numerous countries have been investing in the development of Intelligent Transportation Systems (ITS), harnessing advances in data collection and mobile computing technologies [1, 2, 3]. Modeling and analyzing spatio-temporal dynamic systems are applicable to various prediction scenarios, and research in this field has received sustained attention over the past few decades [4, 5]. As a crucial component of ITS, traffic flow prediction aims to optimize traffic management, enhance travel safety, and mitigate worsening traffic conditions. [6] Early research primarily focused on statistical model-based approaches, such as the Historical Average (HA) [7] and the Auto-Regressive Integrated Moving Average (ARIMA) [8, 9] model, as well as machine learning-based models [10], including Vector Auto-Regression (VAR) [11, 12] and Artificial Neural Networks (ANN) [13]. However, these methods often struggle to capture the complex nonlinear relationships present in large-scale traffic networks, especially when directly applied to spatio-temporal prediction tasks. With the rise of spatio-temporal big data, recent methods have shifted towards data-driven deep learning models that can more effectively capture the inherent spatio-temporal dependencies of dynamic systems [14]. Simple yet effective strategies include using Convolutional Neural Networks (CNNs) [15] to capture spatial dependencies, and utilizing Recurrent Neural Networks (RNNs) [16] and their variants, such as Long Short-Term Memory (LSTM) [17] networks and Gated Recurrent Units (GRUs) [18], to capture temporal dependencies, thereby improving performance [19]. Recently, numerous traffic prediction methods have combined sophisticated temporal models with Graph Neural Networks (GNNs) to capture global temporal dependencies and regional pattern features, respectively. Spatio-temporal graph neural networks (STGNNs) [20, 21] have gained significant attention due to their ability to learn robust high-level spatio-temporal representations through local information aggregation [6]. Researchers have invested considerable effort in developing complex and innovative models for traffic prediction, including novel graph convolutional methods [22, 23, 24, 25, 26, 27, 28, 29, 30, 4, 31, 32, 33], learning graph structures [5, 34, 35, 36, 37], efficient attention mechanisms [38, 39, 40, 41, 42], and other approaches [43, 44, 45, 46, 47, 48, 49], achieving performance improvements. However, despite ongoing advancements in network architectures, performance gains have begun to plateau, largely due to the following challenges: • Separation between the spatial and temporal module: The independent computation of spatio-temporal modules always limits the effectiveness and efficiency of spatio-temporal representation learning. As shown in Figure 1(c), spatio-temporal relational information influences regional predictions over time. Prediction modules that separate spatial and temporal processing fall short of efficiently propagating regional relationships across temporal intervals. • Data heterogeneity: The heterogeneity of spatio-temporal data results in varying patterns across different spatial and temporal scales. For instance, Figure 1(a) depicts one of the regions monitored by sensors in the PEMS dataset [29], where traffic flow exhibits substantial variability between regions. Figure 1(b) shows traffic flow waveforms at two points within the same region, highlighting that even within a single area, distinct periods show different traffic dynamics. Figure 1: Motivation of our proposed method. (a) shows the sensor distribution of the PEMS04 dataset. (b) is a visual result of the traffic flow of a pair of residential areas over a random period. And (c) is spatio-temporal dependencies shown in traffic flow prediction tasks. Upon revisiting existing traffic forecasting methods, we recognize the need for a unitized framework to address these challenges. To this end, we first propose the concept of Adaptive Spatio-temporal Unitized Cells (ASTUCs), which are designed to compute, update, and store spatial, temporal, and relational information within a single unit, in contrast to prior research that separates spatial and temporal modules. Meanwhile, we propose a novel block called Multi-layer Residual Fusion (MLRF) that leverages the properties of these cells to better capture complex non-linear spatio-temporal dependencies, thereby overcoming heterogeneity and improving computational efficiency and performance. Specifically, we begin by defining an adaptive spatio-temporal unitized matrix at the node level, represented by multiple trainable adaptive matrices using low-rank matrix factorization. During the training process, these cells carry node information and aggregate it into reorganized matrices containing dynamic information at each time step. The use of multi-layer fusion residual blocks mitigates redundant computations, reducing over-parameterization. Finally, all adaptive spatio-temporal unitized cells contribute to the prediction module, enabling accurate traffic flow forecasting. Our main contributions can be summarized as follows: • A unified approach that unifies spatial and temporal learning. In response to module separation, we introduce a novel framework called the Spatio-temporal Unitized Model (STUM) and a corresponding training approach that unifies spatial and temporal processing, as opposed to the traditional method of separating spatial and temporal modules. This unified treatment allows for more efficient learning and accurate representation of spatio-temporal dependencies. • Designed novel modules for spatio-temporal unitization computing. In response to data heterogeneity, we present the Adaptive Spatio-temporal Unitized Cell (ASTUC) based on low-rank adaptive matrices, and a dual feature extraction strategy based on backbone network extractor and Multi-layer Residual Fusion (MLRF), improving the model’s ability to handle complex spatio-temporal interactions. • Extensive experiments. We conduct comprehensive experiments on multiple real-world datasets, demonstrating that our proposed framework significantly outperforms existing baseline models in spatio-temporal prediction tasks while maintaining computational efficiency. Figure 2: The overview of our proposed method. (a) shows the architecture of the Spatio-Temporal Unitized Model (STUM), where MLP represents the model prototype and STGNN represents a way of enhancement. (b) shows the computing process of The Multi-Layer Residual Fusion (MLRF) blocks. (c) shows the construction of Adaptive Spatio-temporal Unitized Cells and how the information transmission Cross Space and Time."
https://arxiv.org/html/2411.09176v1,"Gazing at Rewards: Eye Movements as a Lens into
Human and AI Decision-Making in Hybrid Visual Foraging","Imagine searching a collection of coins for quarters (0.25), dimes (0.10), nickels (0.05), and pennies (0.01)—a hybrid foraging task where observers look for multiple instances of multiple target types. In such tasks, how do target values and their prevalence influence foraging and eye movement behaviors (e.g., should you prioritize rare quarters or common nickels)? To explore this, we conducted human psychophysics experiments, revealing that humans are proficient reward foragers. Their eye fixations are drawn to regions with higher average rewards, fixation durations are longer on more valuable targets, and their cumulative rewards exceed chance, approaching the upper bound of optimal foragers. To probe these decision-making processes of humans, we developed a transformer-based Visual Forager (VF) model trained via reinforcement learning. Our VF model takes a series of targets, their corresponding values, and the search image as inputs, processes the images using foveated vision, and produces a sequence of eye movements along with decisions on whether to collect each fixated item. Our model outperforms all baselines, achieves cumulative rewards comparable to those of humans, and approximates human foraging behavior in eye movements and foraging biases within time-limited environments. Furthermore, stress tests on out-of-distribution tasks with novel targets, unseen values, and varying set sizes demonstrate the VF model’s effective generalization. Our work offers valuable insights into the relationship between eye movements and decision-making, with our model serving as a powerful tool for further exploration of this connection. All data, code, and models will be made publicly available.","Figure 1: Illustrative example of eye movements and decision-making in a hybrid visual foraging task. The image depicts a real-world scenario where the goal is to search piles of coins for multiple instances of target coins with varying monetary values in order to maximize the accumulative monetary reward, within a time-limited environment. Yellow dots and arrows represent the locations and order of eye movements during the search. Red bounding boxes show the target coins that are collected. Note that humans do not always collect every item they fixate on, highlighting the selective nature of the foraging process. Hybrid visual foraging is a ubiquitous challenge in our daily life, such as grocery shopping for a list of items, simultaneously scanning for traffic lights, parking spaces, and restaurants while driving, or looking for a specific amount of change among piles of coins ( Fig. 1). These tasks involve searching for multiple instances of various target types stored in memory, where target values and prevalence can vary, and the exact number of target instances is often unknown. This raises a critical question about how to prioritize target selections during the search process. Understanding these dynamics is essential for optimizing search efficiency and decision-making in complex environments. To tackle this question, eye movements could offer a unique window into the underlying perceptual, cognitive, and evaluative processes involved in decision-making, such as sensory evidence sampling and accumulation [137, 103, 84, 143, 75], decision timing and temporal expectation [11, 117, 107, 123, 6], response inhibitions [82, 46, 62, 85, 24], and decision certainty and confidence [60, 107, 23, 10, 93], offering high temporal and spatial resolution [116, 44, 72, 47, 100]. In hybrid visual foraging, while neuroscience and psychology works [134, 136, 135, 77, 130] have primarily examined the sequence of target selections within the same environment and the timing of search transitions across different environments especially when target values and prevalence vary, there is a notable lack of studies focusing on eye movements. Here, we design and conduct human psychophysics experiments to examine how foraging strategies and eye movements are influenced by the prevalence and value of targets. Alongside studies in psychology and neuroscience, many AI models have been developed to predict eye movements during decision-making tasks, including visual search [57, 35, 2, 83, 142, 49, 125, 118], object recognition and detection [127, 8, 96, 88], and visual question answering [56, 18, 58]. Notably, existing visual search models integrate both bottom-up saliency [57, 35, 2] and top-down feature modulations [83, 142, 49]. However, these models assume idealized scenarios where either a single target type is present or multiple target types have equal values. As a result, they often overlook the need to prioritize target selections based on varying target prevalences and values during the search process. In this work, we introduce a computational model called Visual Forager (VF), a transformer-based architecture trained with reinforcement learning, designed to perform hybrid visual foraging efficiently across varying combinations of target prevalence and values. Unlike prior visual search models [26, 119, 19, 139], which often rely on human data for supervised training, our VF approximates human foraging behaviors and biases, despite zero training on human data. We highlight our key contributions: 1. Drawing from psychology, we introduce hybrid visual foraging tasks for AI models. The predicted eye movements offer a unique window into the decision-making process with high spatial and temporal resolution. 2. We propose an AI model, Visual Forager (VF), for hybrid visual foraging tasks. VF uses actor-critic networks with a vision transformer backbone and integrates feature-based and value-based modulations to guide decision-making processes, determining where to fixate next and whether to collect currently fixated items during foraging tasks. 3. To benchmark AI model performances, we design and conduct human eye-tracking experiments for hybrid visual foraging tasks. Despite no training on human data, our VF achieves cumulative rewards comparable to human participants and approximates their foraging behaviors, including eye movements and decision biases toward highly valued and prevalent targets. 4. Humans can flexibly adapt their foraging strategies to maximize total rewards under varying target values and prevalence. Remarkably, our VF also performs efficient foraging, under out-of-distribution conditions it was never trained on. This capability is attributed to our newly introduced data augmentations applied to target values."
https://arxiv.org/html/2411.09137v1,Fast probabilistic snake algorithm,"Few people use the probability theory in order to achieve image segmentation with snake models. In this article111International Conference on Image Processing (ICIP), Barcelona, Spain, September 2003, we are presenting an active contour algorithm based on a probability approach inspired by A. Blake work and P. Réfrégier’s team research in France. Our algorithm, both very fast and highly accurate as far as contour description is concerned, is easily adaptable to any specific application. Keywords: snake, energy minimization, probability, regularization.","In the last fifteen years the active contours have been successfully applied in many different ways. Snake can be modelled according to several mathematical formulation. Each one of them entails its own drawback but provides its own advantages at the same time. In 1988, Kass et al. [1] proposed an energy-based formulation, \begin{split}E(C)=\int_{0}^{1}\alpha(s)\left\|\dfrac{\partial C(s)}{\partial s% }\right\|^{2}+\beta(s)\left\|\dfrac{\partial^{2}C(s)}{\partial s^{2}}\right\|^% {2}\\ -\left\|\nabla I(C(s))\right\|^{2}ds\end{split} (1) in which C represented the curve, I the image, \alpha and \beta the parameters used for the control of the curve properties. Though it works, Kass’s model is limited in its sensitivity to initialization. What is more, it has no natural adaptability to changes in topology. As for the improvements proposed (see [2],[3]), they add to the complexity of the implementation. In 1997 geodesic active contours were proposed as a geometric alternative to snake (see [4],[5]). They included as well an energy-based term. Yet they managed to overcome the deficiencies of the classical snake formulation. Thanks to geodesic active region [6] and to levelset implementation, geodesic active contours stopped being sensitive to initialization and at the same time topology changes became implicit. Lately the question has been examined from an innovative angle by A. Black et al. [7] who resort to probabilities and B-Spline curves. And as for him C. Chesnaud [8] proposes a region-based criterion so as to minimize, J(s,\vec{C})=N_{a}(\vec{C})H(\widehat{\theta}_{a})+N_{b}(\vec{C})H(\widehat{% \theta}_{b})+K_{l} (2) In which N_{i}(\vec{C}) is the number of pixels inside the curve and H_{i}(\widehat{\theta_{i}}) represents a measure of probability wich depends on the a priori model chosen to figure the pixels distribution, see [8] for more details (we shall call this model CASP). CASP is actually interesting but its main drawback is that it cannot be used with open curves. That is why we have endeavoured to develop a new formulation of CASP so as to adapt it equally to open or closed curves. The following section gives the formulation of our model, the third one exposes the implementation of our algorithm and the last one states our results and compares them with other models."
https://arxiv.org/html/2411.09133v1,Computational metaoptics for imaging,"Metasurfaces—ultrathin structures composed of subwavelength optical elements—have revolutionized light manipulation by enabling precise control over electromagnetic waves’ amplitude, phase, polarization, and spectral properties. Concurrently, computational imaging leverages algorithms to reconstruct images from optically processed signals, overcoming limitations of traditional imaging systems. This review explores the synergistic integration of metaoptics and computational imaging, “computational metaoptics,” which combines the physical wavefront shaping ability of metasurfaces with advanced computational algorithms to enhance imaging performance beyond conventional limits. We discuss how computational metaoptics addresses the inherent limitations of single-layer metasurfaces in achieving multifunctionality without compromising efficiency. By treating metasurfaces as physical preconditioners and co-designing them with reconstruction algorithms through end-to-end (inverse) design, it is possible to jointly optimize the optical hardware and computational software. This holistic approach allows for the automatic discovery of optimal metasurface designs and reconstruction methods that significantly improve imaging capabilities. Advanced applications enabled by computational metaoptics are highlighted, including phase imaging and quantum state measurement, which benefit from the metasurfaces’ ability to manipulate complex light fields and the computational algorithms’ capacity to reconstruct high-dimensional information. We also examine performance evaluation challenges, emphasizing the need for new metrics that account for the combined optical and computational nature of these systems. Finally, we identify new frontiers in computational metaoptics which point toward a future where computational metaoptics may play a central role in advancing imaging science and technology.","All imaging systems rely on a combination of optical hardware – that routes photons from a scene onto a detector – and software – that processes the measured signal to generate an image. Both components have been the topic of constant innovation over the past few decades. On the hardware side, metasurfaces – subwavelength arrays of nanostructured optical elements – have revolutionized the field of nanophotonics in the past decade kuznetsov2024roadmap ; yu2014flat ; khorasaninejad2016metalenses ; yu2011light ; genevet2017recent ; khorasaninejad2017metalenses ; arbabi2015dielectric . Metasurfaces allow the control of virtually all properties of an incident electromagnetic wave at the nanoscale, such as its polarization, spectral, and angular distribution (Fig. 1b). The development of integrated nonlinear material platforms has also enabled frequency conversion and quantum optical state generation with metasurfaces li2017nonlinear ; Wang2022metasurfaces ; solntsev2021metasurfaces . On the software side, computational imaging is an interdisciplinary field that combines elements of computer science, optics, signal processing, and imaging technologies to enhance the quality and capabilities of imaging systems. By using algorithms to process and interpret data captured by sensors, computational imaging extends beyond the limitations of optics-only approaches, enabling applications such as high-resolution imaging, volumetric, depth imaging, and advanced object recognition. A foundational aspect of computational imaging is the use of computational methods to reconstruct images from data that may be incomplete, noisy, or otherwise imperfect (some examples shown in Fig. 1c). This approach can involve numerical methods in machine learning, inverse problems, and optimization bertero2021introduction ; donoho2006compressed ; barbastathis2019use . Recent works have explored the fruitful intersection between metaoptics and computational imaging lin2021end ; lin2022end ; huang2022fullcolor ; saragadam2024foveated ; whitehead2022fast ; colburn2018metasurface ; li2024singleshot . This Perspective aims at highlighting this novel and exciting direction for metasurface research which also represents a paradigm shifting opportunity for imaging and sensing technologies. But why is there a natural “marriage” between metaoptics and computational imaging? We focus on four key arguments to answer this question below: (1) the performance of multifunctional metasurfaces will inevitably reach a ceiling. While single-layer metasurfaces are now able to process multiple spectral, polarization, and momentum degrees of freedom, integrating multiple functionalities in a single device is usually done at the price of key performance metrics (e.g., focusing efficiency, Strehl ratio, etc.). An alternative solution is to stack multiple layers of metasurfaces to lift the limitations of single layers while conserving a compact form factor. For instance, realizing the same amount of functionality as a conventional plan achromatic objective requires a dozen such layers lin2021computational . Proof-of-concept experiments with few layers have been realized at telecom wavelengths zhou2018multilayer ; roques2022toward , and with volumetric designs at longer wavelengths camayd2020multifunctional ; roberts20233d ; ballew2023multi , but realizing nanoscale volumetric patterning at optical frequencies remains an incredible technical challenge; (2) many iterative algorithms already used in computational imaging rely on numerical preconditioners that transform the input to facilitate the task of a reconstruction problem saad2003iterative . Metasurfaces are physical preconditioners that can implement simple computational imaging tasks in the optical domain long2021isotropic ; abdollahramezani2020meta ; pors2015analog ; cordaro2023solving ; koenderink2015nanophotonics ; kwon2018nonlocal . In other words, metasurfaces preprocess the light field from a scene incident on a detector, whose signal is input to a numerical image-reconstruction task. Optimization of metasurface designs even allows the automatic discovery of optimal preconditioners that do not require human specification. Moreover, we argue that synergetic endeavors between metaoptics and computational imaging are natural and effortless: (3) any practical imaging application relies on a digital detector whose signal is processed by a computing unit. The use of software for denoising, signal processing, and segmentation is already pervasive, and significant effort has been invested in making application-specific computing units (such as graphics processing units, field programmable gate arrays, and image signal processors owens2007survey ; bailey2023design ; nakamura2017image ) to match the bandwidth and energy requirements of image processing tasks; (4) while optimization is already prevalent in image processing, automatic discovery of optimal metasurface designs (with methods such as inverse design, topology optimization, and surrogate models jensen2011topology ; molesky2018inverse ; pestourie2018inverse ) has permeated the field of metaoptics. Computational metaoptic imaging can therefore rely on the natural co-integration of optimization methods from computational imaging and metaoptics design. This Perspective aims at providing a comprehensive exploration of computational imaging empowered by metaoptics. We begin by revisiting the foundational principles of computational imaging with metasurfaces, underscoring their unique advantages over traditional lens-based systems. We then delve into the end-to-end (inverse) design of computational metaoptics, highlighting how the seamless co-design of hardware and software through gradient backpropagation can lead to superior imaging performance. Our discussion extends to advanced applications enabled by this approach, including phase imaging and quantum photonic state measurement. We also address the challenges of performance evaluation specific to computational metaoptics, proposing suitable metrics for assessing these systems that combine hardware and software. Finally, we outline new frontiers in the field, pointing toward future research directions and opportunities in this rapidly evolving domain. Figure 1: Computational imaging with metaoptics: degrees of freedom, physics, and algorithms. a. The general goal of a computational imaging device is to reconstruct various degrees of freedom of an incident light field, for instance its polarization, frequency, momentum, and complex amplitude distribution. Advanced degrees of freedom (e.g., density matrix of the quantum state of light) may also be of interest. b. Light manipulation is realized by leveraging physical properties of metaoptical devices, such as their ability to locally control the complex amplitude of an incoming wavefront, engineered spectral dispersion, non-locality (spatial dispersion), active control of physical properties (e.g., complex transmission), and nonlinear optical properties. c. Once imaged by a detector, the signal may be reconstructed using various reconstruction and estimation methods, such as least-square error minimization (which may include priors on the reconstructed degrees of freedom, such as high sparsity or low complexity). Black-box methods, such as fully connected neural networks, may also be utilized to classify detected signals and images. Other parameter estimation methods, such as maximum likelihood estimation (MLE) may also be used to estimate the degrees of freedom of the incident light field."
https://arxiv.org/html/2411.09066v1,A multidimensional measurement of photorealistic avatar quality of experience,"Photorealistic avatars are human avatars that look, move, and talk like real people. The performance of photorealistic avatars has significantly improved recently based on objective metrics such as PSNR, SSIM, LPIPS, FID, and FVD. However, recent photorealistic avatar publications do not provide subjective tests of the avatars to measure human usability factors. We provide an open source test framework to subjectively measure photorealistic avatar performance in ten dimensions: realism, trust, comfortableness using, comfortableness interacting with, appropriateness for work, creepiness, formality, affinity, resemblance to the person, and emotion accuracy. We show that the correlation of nine of these subjective metrics with PSNR, SSIM, LPIPS, FID, and FVD is weak, and moderate for emotion accuracy. The crowdsourced subjective test framework is highly reproducible and accurate when compared to a panel of experts. We analyze a wide range of avatars from photorealistic to cartoon-like and show that some photorealistic avatars are approaching real video performance based on these dimensions. We also find that for avatars above a certain level of realism, eight of these measured dimensions are strongly correlated. In particular, for photorealistic avatars there is a linear relationship between avatar affinity and realism; in other words, there is no uncanny valley effect for photorealistic avatars in the telecommunication scenario. We provide several extensions of this test framework for future work and discuss design implications for telecommunication systems. The test framework is available at https://github.com/microsoft/P.910.","Photorealistic avatars are human avatars that look, move, and talk like real people. Photorealistic avatars can be used for various applications, such as: • Telecommunication: Photorealistic avatars can be used instead of two-dimensional webcam videos to create a virtual meeting space. Users can interact while maintaining correct eye gaze, allowing participants to know who is looking at whom, which increases trust (Nguyen and Canny, 2007) while reducing video fatigue by reducing hyper-gaze (Gale et al., 1975; Fauville et al., 2021). • Health care: Photorealistic avatars can be used to provide virtual consultations, training, therapy, and education for patients and medical professionals. For example, a photorealistic avatar of a doctor can explain a diagnosis, prescribe a treatment, or demonstrate a procedure to a patient. • Education: Photorealistic avatars can be used to create immersive, personalized, and interactive learning environments for students and teachers. For example, a photorealistic avatar of a teacher can guide students through a lesson, provide feedback, or answer questions. • Retail and e-commerce: Photorealistic avatars can be used to enhance the (online) shopping experience for customers, sellers, and customer service. For customers, photorealistic avatars can enable virtual try-on experiences for clothing, accessories, or makeup, allowing customers to visualize how products will look on them before making a purchase. • Entertainment: Photorealistic avatars can be used to create realistic and engaging characters for games, movies, shows, and social media. For example, a photorealistic avatar of an actor can perform scenes, interact with fans, or promote brands. While there are many applications of photorealistic avatars, our focus is on evaluating them for the telecommunication scenario to improve trust and reduce video fatigue. Therefore the avatar test sequences we use are targeted for telecommunication, which includes people talking and expressing a wide range of emotions (happy, sad, surprised, fear, anger, and disgust). The performance of photorealistic avatars has been improving significantly recently based on objective metrics such as PSNR (Gonzalez and Woods, 2006), SSIM (Wang et al., 2004), LPIPS (Zhang et al., 2018), FID (Heusel et al., 2017), and FVD (Unterthiner et al., 2019). However, recent avatar publications do not provide subjective tests of the avatars to measure human usability factors (e.g., (Saito et al., 2024; Tian et al., 2024; Xu et al., 2024b; Huang et al., 2024; Shao et al., 2024; Zhou et al., 2024; Deng et al., 2024; Kirschstein et al., 2024; Xu et al., 2024a; Liu et al., 2024)). We believe this is because such subjective tests are challenging, but also because there is no standardized or readily available method to do so. The usability factors that have been previously suggested and studied for avatars include realism (Inkpen and Sedlins, 2011), comfortableness using (Inkpen and Sedlins, 2011), comfortableness interacting with (Inkpen and Sedlins, 2011), appropriateness for work (Inkpen and Sedlins, 2011), creepiness (Inkpen and Sedlins, 2011), formality (Inkpen and Sedlins, 2011), resemblance to the person (Inkpen and Sedlins, 2011), trust (ITU-T Recommendation P.1320, 2022), and emotion accuracy (ITU-T Recommendation P.1320, 2022). We also include affinity to study the uncanny valley effect (Mori et al., 2012). The research questions we want to answer in this work are: • RQ1: Are the objective metrics currently used to develop avatars (PSNR, SSIM, LPIPS, FID, FVD) sufficient to achieve the performance goals of avatars, especially the human usability factors for avatars? • RQ2: Which human usability factors are the most important for photorealistic avatars? • RQ3: Can we develop an accurate and reproducible test framework to measure human usability factors for avatars? • RQ4: Is there an uncanny valley effect for photorealistic avatars in the telecommunication scenario? Our contributions in this work are: • We provide an open source test framework to subjectively measure photorealistic avatar quality of experience in ten dimensions using crowdsourcing. • The crowdsourced subjective test framework is highly reproducible and accurate compared to a panel of experts. • We show that the correlation of nine of these subjective dimensions to PSNR, SSIM, LPIPS, FID, and FVD is weak and one (emotion accuracy) is moderate, motivating the need for an available subjective test framework as well as improved objective metrics to measure avatar performance. • We analyze a wide range of avatars from photorealistic to cartoon-like and show some photorealistic avatars are approaching real video quality based on these subjective metrics. • We show that for avatars with a realism > 2 (out of a 1-5 Likert scale) eight of the ten measured dimensions are strongly correlated, which leads to a dimensionality reduction of ten to three for the survey. • We show that for photorealistic avatars there is a linear relationship between avatar affinity and realism. In other words, there is no uncanny valley effect for photorealistic avatars in the telecommunication scenario; the more realistic the avatar is, the more affinity there is to the avatar. In Section 2, we review related work in this area. In Section 3, we describe the test framework design to measure avatar quality of experience, and in Section 4, we show that the test framework is both reproducible and accurate. Using the test framework, we show the results and analysis in Section 5. Finally, we provide conclusions, future extensions, system design implications, and limitations in Section 6."
https://arxiv.org/html/2411.08992v1,IDCIA: Immunocytochemistry Dataset for Cellular Image Analysis,"We present a new annotated microscopic cellular image dataset to improve the effectiveness of machine learning methods for cellular image analysis. Cell counting is an important step in cell analysis. Typically, domain experts manually count cells in a microscopic image. Automated cell counting can potentially eliminate this tedious, time-consuming process. However, a good, labeled dataset is required for training an accurate machine learning model. Our dataset includes microscopic images of cells, and for each image, the cell count and the location of individual cells. The data were collected as part of an ongoing study investigating the potential of electrical stimulation to modulate stem cell differentiation and possible applications for neural repair. Compared to existing publicly available datasets, our dataset has more images of cells stained with more variety of antibodies (protein components of immune responses against invaders) typically used for cell analysis. The experimental results on this dataset indicate that none of the five existing models under this study are able to achieve sufficiently accurate count to replace the manual methods. The dataset is available at https://figshare.com/articles/dataset/Dataset/21970604.","Cell biology is a sub-discipline of biology where the structure and physiological functioning, and interaction of cells are studied (Bradshaw and Stahl, 2016). Cells are examined under a microscope and imaged at a high resolution. In immunocytochemistry (ICC), different antibodies are used to visualize the presence of particular proteins to identify specific cell types in a given sample. Cell analysis involves a wide range of tasks, such as counting cells and measuring and evaluating cell state (e.g., shape, motility), cell health, and cell growth. Cell biology is closely intertwined with other fields, such as neuroscience, genetics, and molecular biology. One fascinating application area of cell biology is research for the potential diagnosis and treatment of diseases. The research in this area is full of potential and possibilities that could improve quality of life. Deep Neural Networks (DNNs) have been applied in the analysis of microscopic cell images, including cell counting (Paul Cohen et al., 2017; Xie et al., 2018), segmentation (Al-Kofahi et al., 2018; Ghaznavi et al., 2022; Hiramatsu et al., 2018; Morelli et al., 2021), and detection (Wang et al., 2022; Jiang et al., 2020; Fujita and Han, 2020). Given an input image, cell counting provides the number of cells in the image. In contrast, cell segmentation finds the contours of individual cells, separating them from each other and the background. On the other hand, cell detection localizes a cell by drawing the smallest rectangle around each cell in the input image. The advantages of DNNs over traditional machine learning methods are that DNNs automatically extract important properties (features) of the object of interest and use them to perform the intended task. However, the major drawback of DNNs is that it requires a large high-quality labeled dataset for accurate predictions. Existing DNN methods for cell counting can be broadly categorized into two groups: detection-based and regression-based categories. The detection-based category undertakes the counting task by first detecting individual cells (contours, bounding boxes, or centroids of the cells) in a given image and counting the detected cells to obtain the final cell count (Morelli et al., 2021; Khan et al., 2016). These methods hinge on the availability of the annotated ground truth of the bounding box or a centroid of a cell. The methods are also dependent on the characteristics of the microscopic input images. In particular, detection-based methods fail to offer good performance when there is a high occlusion in the images. The regression-based category (Paul Cohen et al., 2017; Xie et al., 2018) predicts the cell count without detecting individual cells. Some of these methods use only the ground truth cell count for each training image for training. Other methods predict a corresponding density map for a given image and obtain the final count from the predicted density map. Our team examines cellular images taken after electrical stimulation experiments on stem cells for cell differentiation. Cell differentiation is the process in which an unspecialized cell develops and matures to become a specialized cell. Electrical stimulation of stem cells is potentially useful for stem cell therapy in patients with nerve injuries. Cell counting is an important step toward determining an appropriate amount of electrical voltage and stimulation duration to be applied. To perform the electrical stimulation, cells are placed on the surface of a scaffold, which are structures providing support for cells to grow within an interdigitated electrode region. Then the voltage is applied to the electrode pads of the scaffold, which are structures providing support for cells to grow. During an electrical stimulation experiment, cells exhibit changes in size, shape, and energy requirement (Das et al., 2017; Uz et al., 2020, 2019). Following electrical stimulation, immunocytochemistry (ICC) is performed to measure the effect of the stimulation on the cells. Different antibodies are used during the ICC process to identify the potential cell types these cells could be differentiating into. A fluorescent microscope is used to examine and image the cells. Currently, cell counting and cell analysis are done manually. The challenges for developing accurate automated cell counting are a wide range of cells in an image given different antibodies, different cell sizes, low contrast, and cell occlusion. The main contributions of this work are as follows. (1) An annotated dataset for automated cell counting along with the domain knowledge to use the dataset. The annotation includes the cell locations as well as the count of cells per image. To the best of our knowledge, there is no annotated fluorescent microscopic cell image dataset that covers as many staining methods as this dataset. (2) Performance comparison of the state-of-the-art regression-based and density map estimation DNN methods. The results can be used as baseline results for future improvement. The source code and the trained models are available publicly at https://github.com/ISU-NRT-D4/cell-analysis. The rest of the paper is organized as follows. In Section 2, we provide a summary of existing datasets related to cell counting. Section 3 presents our data collection and annotation process and the details of our new dataset. Section 4 includes applicable scenarios to utilize the dataset. Section 5 details the baseline experimental results on the dataset with five DNN models. Finally, we provide a conclusion and description of the future work in Section 6."
https://arxiv.org/html/2411.08975v1,Fluoroformer: Scaling multiple instance learning to multiplexed images via attention-based channel fusion,"Though multiple instance learning (MIL) has been a foundational strategy in computational pathology for processing whole slide images (WSIs), current approaches are designed for traditional hematoxylin and eosin (H&E) slides rather than emerging multiplexed technologies. Here, we present an MIL strategy, the Fluoroformer module, that is specifically tailored to multiplexed WSIs by leveraging scaled dot-product attention (SDPA) to interpretably fuse information across disparate channels. On a cohort of 434 non-small cell lung cancer (NSCLC) samples, we show that the Fluoroformer both obtains strong prognostic performance and recapitulates immuno-oncological hallmarks of NSCLC. Our technique thereby provides a path for adapting state-of-the-art AI techniques to emerging spatial biology assays.","Multiple instance learning (MIL) has emerged as the de facto standard approach in computational pathology for generating predictions from whole slide images (WSIs) (Ilse et al., 2018; Maron and Lozano-Pérez, 1997; Carbonneau et al., 2018; Lu et al., 2021). The typical MIL pipeline consists of 1) dividing the WSI into smaller image patches, 2) extracting lower dimensional embeddings for each patch from a pre-trained neural network, 3) pooling embeddings across patches to create a slide-level summary vector, and 4) generating slide-level predictions for the particular task at hand. Compared to traditional strategies such as training patch-level predictors that rely exclusively on clinician-annotated regions of interest (ROIs), MIL enables weakly-supervised training on entire WSIs, thereby offering enhanced scalability, reduced sampling bias, and potentially superior performance (Zhou, 2018). Figure 1: Overview of the Fluoroformer strategy for multiplexed imaging. Mathematical symbols are defined in text. Thus far in computational pathology, MIL pipelines have largely been confined to traditional hematoxylin & eosin (H&E)-stained WSIs (Wilson et al., 2021; Ghahremani et al., 2022). While H&E staining can provide detailed morphological information, it fails to explicitly capture important proteins and other complex biomarkers that indicate cell phenotype and state (Lee et al., 2020; Peng et al., 2023; Muñoz-Castro et al., 2022). In contrast, emergent techniques in spatial biology such as multiplex immunofluorescence (mIF) enable the imaging of many biomarkers simultaneously in tissue samples while preserving spatial context (Figure 1). These techniques result in rich, multi-channel (\sim5-50) images that have advanced our understanding of diseases ranging from neurodegenerative disorders (Muñoz-Castro et al., 2022) to cancer (Lee et al., 2020; Peng et al., 2023). Conversely, mIF images are often analyzed using hand-engineered features, such as the counts of discrete biomarkers within clinician-defined ROIs (Wilson et al., 2021). More recent efforts have pointed to the potential of using deep learning to improve performance on downstream tasks, but these efforts have also focused on ROIs rather than expanding to WSIs (Hoebel et al., 2024; Sorin et al., 2023; Wu et al., 2022). There is therefore a pressing need to optimize MIL methods for mIF in order to yield the benefits of both weakly-supervised training and the rich information provided by spatial assays. Doing so, however, presents several challenges. The disparate channels must be somehow combined, and, moreover, ideally would be done so flexibly given that the number of channels can vary between mIF protocols. Here, we present the Fluoroformer, a Transformer-like neural network module designed to interpretably scale MIL to multiplex images. Leveraging scaled dot-product attention (SDPA) (Vaswani, 2017), it fuses the information from disparate multiplexed channels into a single summary vector for each patch, enabling the subsequent pooling of the patch embeddings via standard attention-based MIL (ABMIL) mechanisms. Importantly, the Fluoroformer produces attention matrices for each patch that may offer insights into cell-cell interactions and biological structures. Using a cohort of 434 non-small cell lung cancer (NSCLC) samples and their corresponding mIF WSIs, we find that the Fluoroformer demonstrates strong performance in predicting patient prognosis. Analysis of the channel-wise attention matrices offers insights into immune-tumor interactions that potentially associate with prognosis. Our approach therefore bridges spatial biology techniques with state-of-the-art artificial intelligence approaches to maximize the potential of this emerging field. Figure 2: Heatmaps of the most highly attended markers in each patch of selected mIF images for the Fluoroformer model with a ResNet50 embedder. Patterns are observed such as DAPI being a highly attended marker for alveolar tissue (A, B) and CD8-attended regions appearing at tumor margins and sporadically within the tumor (A, C, D)."
https://arxiv.org/html/2411.08936v1,Clustered Patch Embeddings for Permutation-Invariant Classification of Whole Slide Images,"Whole Slide Imaging (WSI) is a cornerstone of digital pathology, offering detailed insights critical for diagnosis and research. Yet, the gigapixel size of WSIs imposes significant computational challenges, limiting their practical utility. Our novel approach addresses these challenges by leveraging various encoders for intelligent data reduction and employing a different classification model to ensure robust, permutation-invariant representations of WSIs. A key innovation of our method is the ability to distill the complex information of an entire WSI into a single vector, effectively capturing the essential features needed for accurate analysis. This approach significantly enhances the computational efficiency of WSI analysis, enabling more accurate pathological assessments without the need for extensive computational resources. This breakthrough equips us with the capability to effectively address the challenges posed by large image resolutions in whole-slide imaging, paving the way for more scalable and effective utilization of WSIs in medical diagnostics and research, marking a significant advancement in the field.","In the evolving field of digital pathology, Whole Slide Imaging (WSI) has emerged as a transformative technology, enabling the digitization of histopathological slides at gigapixel resolution. This advancement has not only facilitated remote diagnostics and educational opportunities but also opened new avenues for quantitative image analysis [1, 2]. Despite its potential, the sheer size and complexity of WSIs pose significant computational challenges, limiting the practicality of large-scale analysis and the application of advanced machine learning techniques [3, 4]. Whole slide imaging (WSI) represents a significant breakthrough in digital pathology, enabling the digitization of histological slides at high resolutions. This advancement allows for improved visualization, analysis, and management of tissue samples, essential for accurate disease diagnosis and research. However, the sheer size and complexity of WSIs pose unique challenges in image processing and analysis, necessitating innovative approaches for efficient and effective feature extraction and classification. Traditional methods for analyzing WSIs often rely on supervised learning techniques, which require extensive annotated datasets prepared by expert pathologists. This process is not only time-consuming but also prone to variability due to inter-observer differences. Moreover, the high-dimensional nature of WSIs results in computational and storage challenges, limiting the scalability of conventional approaches. In response to these challenges, recent research has explored the potential of self-supervised learning [5, 6] as a promising alternative for feature extraction from WSIs. Self-supervised learning, a subset of unsupervised learning methods, involves generating labels from the data and using them as supervisory signals for learning rich, discriminative features. This technique is particularly advantageous in the context of WSIs, where labeled data are scarce and costly to obtain. Our research introduces a comprehensive framework that leverages various encoders to preprocess WSIs and extract meaningful features. This novel approach begins with an advanced preprocessing stage designed to enhance the visual quality of WSIs and prepare them for subsequent analysis. We have used a deep learning model [7] to get the WSI without artifacts. After that, we employ adaptive filtering techniques to improve contrast and highlight pertinent morphological details, which are essential for effective feature learning. A preprocessed WSI is divided into patches of size 512x512 to manage the high resolution of the image. Following preprocessing, we implement SimCLR [5], ResNet50 [8], EfficientNet [9], RegNet [10], ConvNeXT_Tiny [11] and Swin_Tiny [12] model to derive a high-dimensional feature space from the enhanced images. To address the challenge of high dimensionality in the learned feature space, we introduce an innovative clustering [13] approach. Instead of using all extracted features, our method focuses on clustering these features and uses the centroids of these clusters as new, compact representations of the original WSIs. This step significantly reduces the dimensionality of the data, making it more manageable for subsequent analysis and classification tasks. The clustered mean vectors serve as the input to our classification module, which are Swin_Tiny, Multi-layer perceptron(MLP)[14] and attention-based multiple instance learning (MIL) [15] to mitigate permute variance problem of mean feature vector obtained from clustering. Recent advances in transformer models have revolutionized natural language processing and are beginning to show significant potential in image-based tasks [16, 17]. These models, known for their ability to handle sequence data, offer an intriguing solution to the problem of permutation invariance in image patches derived from WSIs, a challenge that needs to be adequately addressed by conventional convolutional neural networks (CNNs). Due to these features of the transformer, we have probed both the techniques, such as MIL and Transformer, as classifiers. Transformer models, known for their effectiveness in handling sequential data, are adapted in our framework to process the spatial relationships among cluster centroids. These models use self-attention mechanisms to weigh the importance of different regions in a WSI, allowing the classifier to focus on areas most indicative of the pathological state. Additionally, we investigate with MLP [14] and attention-based MIL [18] component to further refine our classification strategy. MIL is particularly well-suited for tasks where labels are available at a coarse level (e.g., slide level) rather than a fine-grained level (e.g., pixel level). In our approach, each WSI is treated as a ""bag"" of instances (cluster centroids), and the MIL classifier learns to attend to those instances most relevant for predicting the slide’s label. The use of transformers and attention-based MIL as a classifier not only enhances the classification accuracy but also improves the model’s interpretability. By focusing on specific clusters within a WSI, pathologists can identify the critical areas that led to a particular diagnostic decision, thereby aligning the model’s operation with clinical reasoning processes. Figure 1: Thumbnail image samples of TCGA Lung data [19] (first row) and Camelyon17 dataset [20] (bottom row) To validate the effectiveness of our proposed methodology, we conducted extensive experiments on a diverse dataset of WSIs such as Lung Cancer [19, 21] and Camelyon17 [20] dataset. The snapshot of thumbnails of TCGA LUNG data and Camelyon17 dataset is shown in Fig. 1. Our results demonstrate significant improvements in classification accuracy compared to traditional methods, highlighting the potential of our approach to transform the landscape of digital pathology. In conclusion, our research contributes to the fields of digital pathology and machine learning by presenting a novel, effective, and efficient framework for the preprocessing, feature extraction, and classification of WSIs. This approach not only addresses the limitations of existing techniques but also paves the way for more scalable and insightful analyses of histological data, with potential applications in automated disease diagnosis and biomarker discovery."
https://arxiv.org/html/2411.08926v1,DG-PPU: Dynamical Graphs based Post-processing of Point Clouds extracted from Knee Ultrasounds,"Patients undergoing total knee arthroplasty (TKA) often experience non-specific anterior knee pain, arising from abnormal patellofemoral joint (PFJ) instability. Tracking PFJ motion is challenging since static imaging modalities like CT and MRI are limited by field of view and metal artefact interference. Ultrasounds offer an alternative modality for dynamic musculoskeletal imaging. We aim to achieve accurate visualisation of patellar tracking and PFJ motion, using 3D registration of point clouds extracted from ultrasound scans across different angles of joint flexion. Ultrasound images containing soft tissue are often mislabeled as bone during segmentation, resulting in noisy 3D point clouds that hinder accurate registration of the bony joint anatomy. Machine learning the intrinsic geometry of the knee bone may help us eliminate these false positives. As the intrinsic geometry of the knee does not change during PFJ motion, one may expect this to be robust across multiple angles of joint flexion. Our dynamical graphs based post-processing algorithm (DG-PPU) is able to achieve this, creating smoother point clouds that accurately represent bony knee anatomy across different angles. After inverting these point clouds back to their original ultrasound images, we evaluated that DG-PPU outperformed manual data cleaning done by our lab technician, deleting false positives and noise with 98.2\% precision across three different angles of joint flexion. DG-PPU is the first algorithm to automate the post-processing of 3D point clouds extracted from ultrasound scans. With DG-PPU, we contribute towards the development of a novel patellar mal-tracking assessment system with ultrasound, which currently does not exist.","Up to 30% of patients report non-specific anterior knee pain after TKA [1]. Currently, the mechanisms underlying this joint pain are not fully understood; numerous studies have previously suggested that abnormal patellofemoral instability with the trochlea of the implant is one of the main causes of this pain [2, 3]. Previous studies aimed to visualise this using static imaging methods such as CT and MRI [4]. However, consideration of the limited field view of static CT and MRI imaging, radiation exposure of CT scans, and cost of MRI imaging led to the increasing usage of ultrasound as an alternative modality for dynamic MSK imaging [5, 6, 7, 8]. Our co-authors previously developed a Computer-Aided Tracking and Motion Analysis with Ultrasound System (CATMAUS) to visualise these patellofemoral interactions [9, 10]. Their most recent work was successfully able to achieve 3D point cloud reconstruction with motion tracking from 2D ultrasound scans, using a combination of semantic segmentation and point cloud registration [11]. However, the focus of this paper was on their novel tracking method—the training data they used in this study was generated from realistic plastic bone models. The long term goal is to replicate this pipeline using point clouds generated from ultrasound scans on real human bone. Due to the nature of ultrasound data, point clouds extracted from 2D ultrasound images tend to be noisy with several false positives where soft tissue is mislabeled as bone. Previously, manual cleaning/de-noising was required in order to remove these erroneous points before reconstructing a 3D mesh of the joint that could be used for motion analysis of patellar tracking. In a clinical setting, this is far from ideal, as manual data cleaning cannot be done in real time to accurately visualise PFJ motion. Furthermore, the noise of the point clouds hindered the Iterative Closest Points (ICP) algorithm [12] from finding the rigid transformations used in point cloud registration [11]. Accurate point cloud registration is what allows us to visualise 3D PFJ motion. Detailed ultrasound scans are a necessity when reconstructing a realistic point cloud representation of the PFJ. To collect such data, it requires very thorough, time-consuming scanning of the joint that would not be feasible within the time frame of a standard clinic appointment. We use the term thorough scans to describe the ultrasound scans we achieve in the lab and partial scans for a less detailed scan one may expect to be able to use in a clinical setting. We aim to achieve accurate visualisation of PFJ motion via registration of point clouds extracted from partial scans across different angles of the knee joint. With the long term goal of obtaining point cloud registration, this project focuses on de-noising and filtering the 3D point clouds extracted from the ultrasound images by CATMAUS. For this purpose, we’ve designed our novel algorithm: Dynamical Graph-based Post-processing for Ultrasounds (DG-PPU). We elaborate more on the methodology we’ve taken below."
https://arxiv.org/html/2411.08885v1,"Enhancing Lie Detection Accuracy: A Comparative Study of Classic ML, CNN, and GCN Models using Audio-Visual Features","Inaccuracies in polygraph tests often lead to wrongful convictions, false information, and bias, which have significant consequences for both legal and political systems. Recently, analyzing facial micro-expressions has emerged as a method to detect deception; however, current models have not reached high accuracy and generalizability. The purpose of this paper is to aid in remedying these problems. The unique multimodal transformer architecture used in this paper improves upon previous approaches by using auditory input, visual facial micro-expressions, and manually transcribed gesture annotations, moving closer to a reliable non-invasive lie detection model. Visual and auditory features were extracted using Vision Transformer and OpenSmile models respectively, which were then concatenated with the transcriptions of participants’ micro-expressions and gestures. Various models were trained for classification instances of lies and truth using these processed and concatenated features. The CNN Conv1D multimodal model achieved a 95.4% average accuracy. However, further research is still required to create higher-quality datasets and even more generalized models for more diverse applications.","Lie detection has been a recurring focus of research and technological innovation in law enforcement and criminal justice. According to a survey conducted by the University of Wisconsin-La Crosse, about 75% of survey respondents reported telling zero to two lies per day; lying comprised 7% of total communication, with 79% of the lies being told face-to-face and 21% being mediated [3]. Current technologies, such as polygraphs, have focused on biological responses like blood pressure to detect lies. However, these methods are unpredictable and easily flawed. Recently, research has begun to focus on various other indicators of deception, including facial micro-expressions and audio cues [4]. Facial micro-expressions (ME) are intentional or involuntary localized and momentary movements of the face, usually lasting less than 500 milliseconds [2]. Despite advancements in lie detection techniques, traditional methods remain intrusive, subjective, and often inaccurate. Detecting deception through ME and speech analysis presents a significant challenge due to the subtle and brief nature of these cues. As shown in Table I, traditional methods have high variance and relatively low accuracy. This study aims to address these limitations by developing a non-intrusive, objective, and highly accurate method for detecting deception using both ME and audio signals. Accurate lie detection is crucial in various fields, including security, legal systems, and psychological evaluations. The primary objective of this study is to establish an AI model that can differentiate between truth and deception with high accuracy by analyzing audio, visual cues in videos, and extracted gestures. Audio dialogue, visuals, and gestures all help to distinguish between deception and truthfulness, making them important features to consider [23]. Therefore, the Real-life Deception Detection Dataset from the University of Michigan was used, which includes 121 videos of deception and truthfulness and a CSV file for gestures. Visuals and audio were extracted from the videos, and OpenSMILE and Vision Transformer (ViT) were used to extract features from audio and video, respectively. Classical machine learning models like Random Forest Classifiers and Logistic Regression can serve as accurate baseline references for a binary classification task like truth and lie. Yet to build off of that, by leveraging advanced neural network models, such as Conv1D, Graph Convolutional Networks (GCN), and CNN LSTM, the accuracy can be increased. TABLE I: Estimated accuracy of different test types in detecting deception and truthfulness Test type Detecting deception Detecting truthfulness Laboratory studies CQT – Polygraph 74%–82% 60%–83% CIT – Polygraph 76%–88% 83%–97% ERP 68% 82% fMRI 84% 81% Field studies CQT – Polygraph 84%–89% 59%–75% CIT – Polygraph 42%–76% 94%–98% This study addresses the following research questions: How effective is the proposed AI model in detecting lies compared to traditional methods and some recent AI models? Which features carry the highest weights in prediction? Deception detection technology has the potential to revolutionize various fields. In law enforcement, it could improve interrogation outcomes and border security by identifying deceptive behavior. In the legal system, it could be utilized to assess the credibility of courtroom testimonies and negotiations. Additionally, applying this technology to financial services could aid in detecting fraudulent claims and reducing the risk of financial fraud. Previous studies have experimented with various machine learning models. For instance, a study by Soldner et al. implemented the Random Forest model, achieving the best accuracy of 69%, as shown in II [5]. Insights from this paper suggest expanding our dataset and exploring additional modalities to enhance the model’s accuracy and reliability in lie detection. Furthermore, Random Forest, being a machine learning technique, cannot handle complex relations as well as multimodal data, which is a limitation of the mentioned study. Moreover, most traditional AI models fall short in reliability and accuracy, often leading to false positives or negatives [1]. A study conducted by the University of Michigan in 2015 analyzed trial videos using micro-facial expressions and achieved a rudimentary accuracy rate of 83.05% using neural networks [6]. Aligning different data types and achieving 83.05% accuracy are two main advantages of the study. TABLE II: Best Results Of Study [2]. Features Acc. Linguistic 66% Dialog 57% Non-verbal 61% All Features 69% This paper is organized as follows: analyzing previous work, discussing the paper’s methods (data collection, data analysis, feature extraction, and implementation guide for the tested models), presenting the results of different tested models, comparing the paper’s results with other studies using the same dataset, and providing a discussion including limitations and recommendations. The paper concludes with a summary of key findings and a look forward."
https://arxiv.org/html/2411.08882v1,A Novel Multimodal System to Predict Agitation in People with Dementia Within Clinical Settings: A Proof of Concept,"Dementia is a neurodegenerative condition that combines several diseases and impacts millions around the world and those around them. Although cognitive impairment is profoundly disabling, it is the noncognitive features of dementia, referred to as Neuropsychiatric Symptoms (NPS), that are most closely associated with a diminished quality of life. Agitation and aggression (AA) in people living with dementia (PwD) contribute to distress and increased healthcare demands. Current assessment methods rely on caregiver intervention and reporting of incidents, introducing subjectivity and bias. Artificial Intelligence (AI) and predictive algorithms offer a potential solution for detecting AA episodes in PwD when utilized in real-time. We present a 5-year study system that integrates a multimodal approach, utilizing the EmbracePlus wristband and a video detection system to predict AA in severe dementia patients. We conducted a pilot study with three participants at the Ontario Shores Mental Health Institute to validate the functionality of the system. The system collects and processes raw and digital biomarkers from the EmbracePlus wristband to accurately predict AA. The system also detected pre-agitation patterns at least six minutes before the AA event, which was not previously discovered from the EmbracePlus wristband. Furthermore, the privacy-preserving video system uses a masking tool to hide the features of the people in frames and employs a deep learning model for AA detection. The video system also helps identify the actual start and end time of the agitation events for labeling. The promising results of the preliminary data analysis underscore the ability of the system to predict AA events. The ability of the proposed system to run autonomously in real-time and identify AA and pre-agitation symptoms without external assistance represents a significant milestone in this research field.","Dementia is a neurodegenerative condition that leads to a progressive decline in cognition and is one of the leading causes of death, disability, and hospitalization in Canada and worldwide. Currently, dementia is the seventh cause of death worldwide [1]. Globally, over 55 million individuals are living with dementia; as the ratio of older people increases, this number will grow to 78 million by 2030 and 139 million by 2050, making dementia a major global health crisis [1]. In addition to cognitive and functional decline, people living with dementia (PwD) also experience non-cognitive neuropsychiatric symptoms (NPS) during their illness [2]. NPS commonly includes agitation, aggression, apathy, symptoms of psychosis, delusions, hallucinations, and disturbances of sleep and appetite. Among NPS, agitation and aggression (AA) occur frequently in severe cases and are a common source of distress for patients and caregivers [3]. They commonly occur during care and are believed to be manifestations of perceived or real unmet needs [3]. Behaviors of AA include pacing, rocking, gesturing, restlessness, shouting, scratching, throwing objects, and destroying property [4]. These symptoms are the leading cause of hospitalizations, extended length of stay as inpatients, and increased demand for placement in long-term care facilities [5]. AA enormously burdens PwD, their families, caregivers, and healthcare systems. In current practices, AA are commonly assessed through caregiver reports. Many observational methods have been developed, including the Neuropsychiatric Inventory (NPI) [6] and the Cohen-Mansfield Agitation Inventory (CMAI) [7]. These assessments are based on manual observations, which are subject to potential bias depending on the caregiver’s memory or emotional state. It is possible to address these limitations by using Artificial Intelligence (AI) and predictive algorithms to predict episodes of AA in PwD before they occur. By 2025, AI technologies are expected to be worth an estimated $36 billion (US) [8]. There is growing evidence that combining AI and sensory technologies to develop a solution for NPS detection will guide the provision of personalized interventions for PwD [9, 10, 11, 12]. The timely detection of critical events in PwD using digital technologies is gaining wide acceptance. For example, smartwatches are being used to help people with dementia [13, 14] and detect epileptic seizures to prevent the development of severe complications [15, 16]. Multiple attempts have been made to create predictive algorithms to detect AA in PwD using several physiological parameters and/or environmental data [17, 18]. Such studies incorporate wearable sensors to capture patient data and use it in AA prediction using machine learning algorithms. Moreover, AI has also been employed in video-based monitoring systems to monitor and detect AA behavior in PwD [19, 20]. To the best of our knowledge, there is no current video surveillance system operating in a hospital setting to detect AA in PwD in real-time due to privacy concerns. The use of multimodal sensing, including wearable sensors and camera footage, for real-time detection of AA and pre-agitation behavior in PwD has not been extensively explored to date. The combination of multimodal sensing and artificial intelligence holds great promise in effectively detecting and predicting AA in real-time. This approach could lead to the timely implementation of preventive strategies or therapies, which could reduce care costs and decrease the frequency of critical incidents among this demographic [21, 22]. This study aims to understand the complicated behaviors of PwD and predict AA in PwD. We carried out this study in the Geriatric Dementia Unit (GDU) and the Geriatric Transitional Unit (GTU) at the Ontario Shores Center for Mental Health Sciences [23] for 5 years. We integrate a multimodal approach, combining biometric data from the EmbracePlus wristband [24] and video data from CCTV cameras installed in common areas in both units. These biometric signs are analyzed to determine the possible correlation with abnormal behaviors. Data collected by these devices, along with the results of the data analysis, is compared against the nurse notes collected via custom forms to confirm the AA and pre-agitation events. The cameras deployed in the designated places automatically detect AA behavior. The developed AI model detects abnormal behavior from body activity recognition in real-time using deep learning techniques. The cameras allow us to document the exact time of the incident for further analysis. The data and analysis then determine personalized pre-agitation conditions using our proposed classification system. To assess our system, we conducted a pilot study focusing on patient acceptance of wristbands, complemented by video camera validation and multimodal sensor data for predicting AA in PwD. The EmbracePlus wristband was crucial for collecting physiological signals like Electrodermal Activity (EDA), heart rate, skin temperature, and movement data. The camera system was also a key component in detecting body movements and recording agitation events. Both systems are tested on three participants who were successfully recruited at the Ontario Shores Centre for Mental Health Sciences. We achieved high accuracy in detecting AA through comprehensive data preprocessing, feature extraction, and the ExtraTrees classification algorithm. Additionally, AA detection was enhanced by analyzing real-time video feeds with OpenPose-generated skeletal keypoints and employing RNN-based neural networks, particularly LSTM and GRU [25]. These networks, optimized for real-time processing, facilitate timely interventions. The pilot study demonstrated the system’s effectiveness through both the wristband and video detection."
https://arxiv.org/html/2411.08879v1,"4D Gaussian Splatting in the Wild 
with Uncertainty-Aware Regularization","Novel view synthesis of dynamic scenes is becoming important in various applications, including augmented and virtual reality. We propose a novel 4D Gaussian Splatting (4DGS) algorithm for dynamic scenes from casually recorded monocular videos. To overcome the overfitting problem of existing work for these real-world videos, we introduce an uncertainty-aware regularization that identifies uncertain regions with few observations and selectively imposes additional priors based on diffusion models and depth smoothness on such regions. This approach improves both the performance of novel view synthesis and the quality of training image reconstruction. We also identify the initialization problem of 4DGS in fast-moving dynamic regions, where the Structure from Motion (SfM) algorithm fails to provide reliable 3D landmarks. To initialize Gaussian primitives in such regions, we present a dynamic region densification method using the estimated depth maps and scene flow. Our experiments show that the proposed method improves the performance of 4DGS reconstruction from a video captured by a handheld monocular camera and also exhibits promising results in few-shot static scene reconstruction.","Dynamic novel View Synthesis (DVS) aims to reconstruct dynamic scenes from captured videos and generate photorealistic frames for an arbitrary new combination of a viewpoint and a time step. This task has emerged as a vital research area in the 3D vision community with rapid advancements in augmented reality and virtual reality. Early DVS research primarily relied on neural radiance fields [29, 69, 10, 13, 11, 38, 39, 41, 5, 12, 50]. In contrast, more recent methods [61, 18, 31] extend 3D Gaussian Splatting [23] to account for the additional time dimension in dynamic scenes, and these techniques are referred to as 4D Gaussian Splatting. Despite the recent success of 4D Gaussian Splatting models [61, 18, 31, 68], their applicability remains largely limited to controlled and purpose-built environments. Most existing models are developed and tested with multi-view video setups [29, 41]. While there are several methods tackling monocular video settings, these setups are still controlled and fall short of in-the-wild scenarios. For instance, [38, 69] maintain multi-view characteristics, where the camera captures a broad arc around a slow-moving object. Also, HyperNeRF [39] relies on unrealistic train-test splits, with both sets sampled from the same video trajectory, which renders the task closer to video interpolation than genuine novel view synthesis. In this paper, we focus for the first time on more natural, real-world monocular videos [14], where a single handheld camera moves around fast-moving objects. In casually recorded monocular videos, which often lack sufficient multi-view information, 4D Gaussian Splatting algorithms tend to overfit the training frames in real-world scenarios. To address overfitting, recent regularization techniques [26, 7, 58, 67, 25, 36, 20] can be applied to provide additional priors for unseen views. However, these regularization techniques often involve a balancing issue: while they effectively improve novel view synthesis performance during testing, they inherently sacrifice the reconstruction accuracy of training images. Since both the reconstruction accuracy and the novel view synthesis quality are equally important in our target task, the trade-off caused by the naïve application of the regularization techniques is not desirable. In this paper, we address this balancing issue with a simple yet effective solution: uncertainty-aware regularization. First, we quantify the uncertainty of each Gaussian primitive based on its contribution to rendering for training images. Then, a 2D uncertainty map is constructed for unseen views using an \alpha-blending method. Regularization is selectively applied to uncertain regions, guided by the diffusion and depth smoothness priors, while low-uncertainty regions, where training data already provide sufficient reconstruction detail, are left unregularized, as illustrated in Figure 1. This approach results in a better balance between training and test performance, achieving good performance. In real-world scenarios involving fast motions, especially in casually recorded videos, 4D Gaussian Splatting additionally faces considerable challenges with initialization. The algorithms based on Gaussian Splatting initialize Gaussian primitives using point clouds obtained by Structure from Motion (SfM) [47]. However, SfM struggles to reconstruct dynamic regions, particularly those with fast motion, often treating them as noise and leaving these areas without initialized primitives. Such an incomplete initialization disrupts training, causing primitives in static regions to be repeatedly cloned and split in an attempt to fill the dynamic areas. This can lead to an excessive number of primitives and, in some cases, out-of-memory issues. To address this limitation, we propose a dynamic region densification technique that initializes additional Gaussian primitives in dynamic regions. Figure 1: Concept of uncertainty-aware regularization. Existing models often use regularization techniques to introduce additional priors for unseen views, aiming to enhance novel view synthesis performance. However, these methods tend to over-regularize accurately reconstructed pixels, which degrades the reconstruction quality of training images. To address this issue, our uncertainty-aware regularization selectively focuses on uncertain regions in unseen views, preserving the quality of well-reconstructed pixels with low uncertainty. We address the challenging problem of 4D reconstruction from an in-the-wild monocular video recorded casually with a handheld camera—a scenario that has been rarely explored. The main contributions of this paper are summarized as follows: • We propose an uncertainty quantification technique based on contribution to training image rendering and introduce adaptive regularization techniques based on the uncertainty map, which balances between novel view synthesis performance and training image reconstruction quality. • We address the issue of incomplete initialization in dynamic regions, emphasizing the importance of proper initialization in the training process of 4D Gaussian Splatting. • We demonstrate the effectiveness of our algorithm on casually recorded monocular videos, showing improvements over baselines. Additionally, we validate the applicability of our method in few-shot static scene reconstruction. The rest of this paper is organized as follows. Section 2 reviews related work and Section 3 discusses the basic concepts of 4D Gaussian splatting, which builds upon 3D Gaussian Splatting by integrating deformation strategies. The details of our approach are described in Section 4, followed by the presentation of experimental results in Section 5. Finally, we conclude this paper in Section 6."
https://arxiv.org/html/2411.08878v1,A Short Note on Evaluating RepNet for Temporal Repetition Counting in Videos,"We discuss some consistent issues on how RepNet has been evaluated in various papers. As a way to mitigate these issues, we report RepNet performance results on different datasets, and release evaluation code and the RepNet checkpoint to obtain these results. Code URL: https://github.com/google-research/google-research/blob/master/repnet/","This note is related to evaluating the class-agnostic repetition counting model RepNet [1] on various video repetition counting datasets. In many papers [2, 8, 5, 6] it has been reported that performance of RepNet on some repetition counting datasets is deficient. The first time this was reported was in the TransRAC [2] paper. However, the model referred to as ‘RepNet’ in that paper is a modified version of RepNet. In Section 5.3 of [2], it is mentioned that ”… for a fair comparison, we modify the last fully connection layer of RepNet [1] to make it capable of handling those videos containing more than 32 action periods”. It is unclear what this modification is exactly but it leads to the modified RepNet’s performance being close to 0 in the Off-by-One Accuracy (OBOA) metric on the UCFRep and RepCount datasets. These results imply that the modified model is not able to count repetitions in the videos of these datasets. The papers that follow TransRAC have reused the numbers reported in TransRAC but refer to this modified model as RepNet. We would like to highlight that the original RepNet model is capable of making predictions of higher than 32 period length. This is achieved by playing the video at different speeds, rather than modifying the model. This technique is described in the original RepNet paper [1] as Multi-speed Evaluation (Section 3.5). This was used for evaluating on the Countix dataset as well and has been proposed before in [4]. When we evaluate the RepNet model using the multi-speed technique on the UCFRep and RepCount datasets, we find that it results in strong performance."
https://arxiv.org/html/2411.08840v1,Multimodal Instruction Tuning with Hybrid State Space Models,"Handling lengthy context is crucial for enhancing the recognition and understanding capabilities of multimodal large language models (MLLMs) in applications such as processing high-resolution images or high frame rate videos. The rise in image resolution and frame rate substantially increases computational demands due to the increased number of input tokens. This challenge is further exacerbated by the quadratic complexity with respect to sequence length of the self-attention mechanism. Most prior works either pre-train models with long contexts, overlooking the efficiency problem, or attempt to reduce the context length via downsampling (e.g., identify the key image patches or frames) to decrease the context length, which may result in information loss. To circumvent this issue while keeping the remarkable effectiveness of MLLMs, we propose a novel approach using a hybrid transformer-MAMBA model to efficiently handle long contexts in multimodal applications. Our multimodal model can effectively process long context input exceeding 100k tokens, outperforming existing models across various benchmarks. Remarkably, our model enhances inference efficiency for high-resolution images and high-frame-rate videos by about 4 times compared to current models, with efficiency gains increasing as image resolution or video frames rise. Furthermore, our model is the first to be trained on low-resolution images or low-frame-rate videos while being capable of inference on high-resolution images and high-frame-rate videos, offering flexibility for inference in diverse scenarios.","Recent efforts have extended large language models (LLMs) to incorporate multiple modalities, leading to breakthroughs in various multimodal tasks (Liu et al., 2024d; c; b; Luo et al., 2024a; Chen et al., 2023b). Despite these advances, existing MLLMs still struggle with tasks that require long input sequence, e.g. granular visual recognition (Tong et al., 2024), high frame rate videos and long videos. For example, many well-trained models (e.g., GPT-4V) produce hallucinations when identifying small and occluded objects in images (Tong et al., 2024) while the same also happens to video MLLMs (Ma et al., 2023), a limitation that hinders the practical application of MLLMs. Various methods have been explored to improve multimodality processing in different domains by incorporating lengthy context, including increasing the resolution of input images (Liu et al., 2024c; Tong et al., 2024; Hong et al., 2024; Li et al., 2023a; Wang et al., 2023; Luo et al., 2024b), increasing frame rate of input videos and increasing frame sampling rate for long videos. Although better performance is achieved by incorporating extra lengthy context, increasing context length directly can significantly escalate computational demands. For instance, increasing the resolution of images to 448×448 pixels raises the computational complexity of models like LLaVA by approximately 1.4 times compared to the default 336×336 resolution. Moreover, pre-trained vision encoders in current MLLMs typically do not support long contexts due to their fixed context length (Liu et al., 2024d; b; Alayrac et al., 2022), making training unstable with significantly increased resolution. Consequently, most prior works have pre-trained models with long contexts (Li et al., 2023a), overlooking efficiency issues. Besides, some works divide the high-resolution images into smaller patches and then encode them independently (Liu et al., 2024b; c), which might lose the spatial information and also lead to quadratic computational cost increase. Additionally, some methods shorten the context length (Hong et al., 2024; Luo et al., 2024b) by inject high-resolution image features into the hidden layers of LLMs, which leads to varying degrees of visual information loss and thus reduces effectiveness. To address these issues, our work proposes using a hybrid transformer-mamba model (e.g., Jamba (Lieber et al., 2024)) to overcome the quadratic complexity associated with transformer models. The hybrid architecture enables our multimodal model to efficiently process the long context resulting from high resolutions and frame rates without compromising effectiveness. For instance, it operates 4\times faster than current open-source models (e.g. LLaVA-Next-13B) when the resolution is 4368*4368. In addition, we also propose a train-on-short-infer-on-long recipe, enabling our model to be trained on inputs with a short context (e.g. low-resolution images) for better training efficiency and do inference on inputs with longer contexts (e.g. high-resolution images) for better performance. This approach addresses both the efficiency and effectiveness problems simultaneously. To summarize, our contributions include: • We introduce a hybrid transformer-mamba multimodal model, MMJamba, optimized for efficiently processing lengthy contexts from high resolutions and frame rates. Our ”train-on-short-infer-on-long” strategy allows the model to train on short-context inputs (e.g., low-resolution images) for efficiency and infer on long-context inputs (e.g., high-resolution images) for enhanced performance. • We conducte experiments on both images and videos across 18 benchmark datasets focusing on images and videos. Our results show that MMJamba achieves state-of-the-art performance compared to open-source and proprietary models, consistently outperforming LLaVA-NeXT (Liu et al., 2024c) and Gemini Pro 1.0 (Team et al., 2023), occasionally matching or surpassing proprietary models like GPT-4V (OpenAI, 2023). More importantly, we show that our model achieves the best efficiency when processing high-resolution images and high frame rate videos. • We conduct comprehensive model analyses, including ablation and case studies, to elucidate the inner workings and demonstrate the model’s performance in real-world scenarios."
https://arxiv.org/html/2411.08768v1,Sharingan: Extract User Action Sequence from Desktop Recordings,"Video recordings of user activities, particularly desktop recordings, offer a rich source of data for understanding user behaviors and automating processes. However, despite advancements in Vision-Language Models (VLMs) and their increasing use in video analysis, extracting user actions from desktop recordings remains an underexplored area. This paper addresses this gap by proposing two novel VLM-based methods for user action extraction: the Direct Frame-Based Approach (DF), which inputs sampled frames directly into VLMs, and the Differential Frame-Based Approach (DiffF), which incorporates explicit frame differences detected via computer vision techniques. We evaluate these methods using a basic self-curated dataset and an advanced benchmark adapted from prior work. Our results show that the DF approach achieves an accuracy of 70% to 80% in identifying user actions, with the extracted action sequences being re-playable though Robotic Process Automation. We find that while VLMs show potential, incorporating explicit UI changes can degrade performance, making the DF approach more reliable. This work represents the first application of VLMs for extracting user action sequences from desktop recordings, contributing new methods, benchmarks, and insights for future research.","Video recordings are increasingly favored for capturing user activities due to their ease of implementation and broad applicability. Moreover, video’s universal compatibility across platforms and devices, combined with its ability to capture detailed and context-rich data, ensures minimal information loss and facilitates thorough analysis. Recent advancements in Vision-Language Models (VLMs) [21, 20, 8, 24, 5, 14, 1] have significantly improved the utility of video recordings. These AI-driven models automate the interpretation and extraction of insights from video data, enhancing the identification of user behaviors and patterns. Combined with the increasing prevalence of AI-integrated hardware [9, 22], these technological innovations are accelerating the adoption of video as an essential tool for documenting and analyzing user activities. Despite extensive research into understanding user actions from various types of videos [7, 12], there remains a lack of focus on desktop recordings. Addressing this gap is crucial, as extracting user actions from desktop videos offers numerous benefits. For instance, it can enhance Robotic Process Automation (RPA) by utilizing demo videos as input, increasing productivity through automation [10]. Moreover, desktop video analysis facilitates the automatic creation of tutorials and guidelines, while also enabling the extraction of personalized interaction patterns, which can be leveraged elsewhere to create a more personalized user experience. We propose two VLM-based methods for extracting user action sequences from desktop recordings. In the Direct Frame-Based Approach (DF), sampled video frames are directly input into VLMs, while the Differential Frame-Based Approach (DiffF) first detects frame changes using computer vision techniques before interpreting them with VLMs. The key difference lies in whether explicit frame differences are incorporated to aid action inference. We evaluate both methods using two benchmark datasets: one crafted by us, focusing on individual action types, and the other adapted from GUI-World [3] which better reflects real-world scenarios. Experimental results reveal that current VLMs show great potential in extracting user actions from desktop recordings. For instance, using the DF approach, we achieve an accuracy of 70\%\sim 80\% in identifying operation types (e.g., click), and the extracted action sequences are replayable through RPA-like processes. Moreover, comparing the two approaches reveals that VLMs struggle to utilize UI changes derived explicitly, sometimes leading to performance degradation. Thus, we recommend the Direct Frame-Based Approach, relying on VLMs’ inherent ability to infer actions, as the current best practice for action extraction. Our contributions can be summarized as follows: • We introduce two VLM-based methods to address the gap in existing research on extracting user action sequences from desktop recordings. To the best of our knowledge, this is the first attempt to leverage VLMs for this task. • We develop two benchmark datasets to assess the performance of methods on this task. All evaluation source codes and benchmarks will be made publicly available. • We perform a comprehensive evaluation of the proposed methods using the developed benchmarks."
https://arxiv.org/html/2411.08756v2,"Masked Image Modeling Boosting
Semi-Supervised Semantic Segmentation","In view of the fact that semi- and self-supervised learning share a fundamental principle, effectively modeling knowledge from unlabeled data, various semi-supervised semantic segmentation methods have integrated representative self-supervised learning paradigms for further regularization. However, the potential of the state-of-the-art generative self-supervised paradigm, masked image modeling, has been scarcely studied. This paradigm learns the knowledge through establishing connections between the masked and visible parts of masked image, during the pixel reconstruction process. By inheriting and extending this insight, we successfully leverage masked image modeling to boost semi-supervised semantic segmentation. Specifically, we introduce a novel class-wise masked image modeling that independently reconstructs different image regions according to their respective classes. In this way, the mask-induced connections are established within each class, mitigating the semantic confusion that arises from plainly reconstructing images in basic masked image modeling. To strengthen these intra-class connections, we further develop a feature aggregation strategy that minimizes the distances between features corresponding to the masked and visible parts within the same class. Additionally, in semantic space, we explore the application of masked image modeling to enhance regularization. Extensive experiments conducted on well-known benchmarks demonstrate that our approach achieves state-of-the-art performance. The code will be available at https://github.com/haoxt/S4MIM.","Semantic segmentation, a fundamental task in computer vision, involves assigning a category label to each pixel in an image. Recently, supervised semantic segmentation using deep neural networks [1], [2], [3], [4] has achieved remarkable success. Nevertheless, this paradigm demands extensive pixel-wise manual labeling, which is both time-consuming and labor-intensive. To alleviate this issue, semi-supervised semantic segmentation [5] has been proposed. It leverages a small amount of labeled data and a substantial amount of readily accessible unlabeled data for learning. In this context, many advanced works [6], [7], [8], [9], [10] have been developed, in which it is crucial to effectively model knowledge from unlabeled data to further enhance the generalization capability, relying on the semantics acquired from labeled data. Figure 1: The illustration of the insight into MIM and the idea of our S4MIM. During the encoding-decoding (E-PiD) process, MIM learns knowledge through establishing the connections between the features corresponding to the masked and visible parts. Building upon this insight, there are three core components of our idea: Class-wise MIM, Class-wise Mask-induced Feature Aggregation, and MIM in Semantic Space. Concurrently, a paradigm that learns exclusively from unlabeled data, known as self-supervised/unsupervised learning, has received significant attention. Self-supervised learning acquires representation and generalization capabilities by pretext tasks defined on unlabeled data, enabling the trained model to serve as a pre-trained model for diverse downstream tasks including classification, semantic segmentation, and object detection. According to the difference in pretext tasks, self-supervised learning can be divided into two categories: discriminative and generative. In terms of discriminative approaches, typical works include [11], [12] and [13], which learn representations by performing pretext tasks such as predicting patch indices, image rotation angles, etc. In particular, the methods [14], [15], [16], [17], [18] built upon contrastive learning [19] have produced promising results, emerging as a prevailing trend in current development. On the generative side, typical works include [20] and [21], which perform pretext tasks such as denoising, colorization, etc. As a broader extension of denoising, masked image modeling (MIM) [22], [23] masks a portion of the image, then serves the masked image as input to reconstruct the entire image or the masked content. Over the past few years, inspired by the impressive success of masked language modeling in natural language processing (NLP) [24], further advancements have been achieved [25], [26], [27], [28]. Therefore, generative self-supervised learning has regained interest among the research community. As previously discussed, there is an inherent connection between self-supervised learning and semi-supervised semantic segmentation, i.e., effectively modeling knowledge from unlabeled data. Consequently, in the semi-supervised semantic segmentation field, numerous works have integrated the self-supervised learning paradigm to benefit from its merits, where contrastive learning is the primary focus [29], [30], [31], [32], [33]. However, the effectiveness of the state-of-the-art generative self-supervised learning paradigm, MIM, within semi-supervised semantic segmentation has not been widely investigated. Motivated by the success of MIM, this work introduces a novel approach into semi-supervised semantic segmentation. As illustrated in Fig. 1 and investigated in [34] and [35], from the perspective of mask-induced learning, MIM effectively models knowledge through connecting the masked part with the visible (unmasked) part of the masked image during the reconstruction process. In order to inherit and extend this insight, our approach consists of a shared encoder followed by parallel pixel and semantic decoders to support explorations in pixel, feature, and semantic spaces, corresponding to “Class-wise MIM”, “Class-wise Mask-induced Feature Aggregation”, and “MIM in Semantic Space”, respectively, which are shown in Fig. 1. Firstly, we observe that in the basic MIM, the connections between the masked and visible parts are derived from the entire image rather than the category objects. Consequently, simply introducing the basic MIM, which may connect features from different classes, can lead to confusion. To address this issue, we introduce a class-wise modification to the basic MIM, since, in a semi-supervised setting, the availability of limited amounts of labeled data allows category information to be provided via both ground-truth and pseudo-label forms. Specifically, in the pixel decoder, through injecting category information, the intermediate features of masked image are grouped by class, where each group’s features have active vectors only at the spatial regions corresponding to that class, with the remaining locations set to zero vectors. An independent head is then assigned for each group’s features to reconstruct the original pixels of the respective class. Finally, in the pixel space, pixels from different classes are combined to form the entire image. In such a process, the mask-induced connections are established within each class, mitigating the semantic confusion among classes caused by the basic MIM. Subsequently, to strengthen the intra-class connections, based on the class-wise grouped features, we explicitly minimize the distances between the features from different parts of the same class. Concretely, we maintain a dictionary where entries map to each class. Each entry stores a prototype representing its class, which is constructed from the features of the visible part belonging to that class. Simultaneously, for each class, the features of the masked part are constrained to move closer to the prototype, resulting in class-wise feature aggregation. In the feature space, such a design extends the idea of connecting features from the same class. On the other hand, to more comprehensively investigate the role of MIM, we also implement it in the semantic space. The semantic predictions derived from masked images are ensured to align with those from original images. As a complement to our explorations in the pixel and feature spaces, we are able to fully leverage the insight into MIM for regularization. In summary, our main contributions are as follows: • We introduce a novel class-wise MIM into the semi-supervised setting, which independently reconstructs regions corresponding to different category objects of the entire image. By inheriting and extending the insight into basic MIM, our class-wise variant establishes intra-class connections and mitigates inter-class confusion. • We develop a class-wise mask-induced feature aggregation strategy, extending the previous point. This strategy explicitly minimizes the distances between features of the visible and masked parts within the same class, thereby strengthening the intra-class connections. • We also explore MIM in the semantic space as a complement to the above two points. Under masking, our goal is to maintain semantic consistency. This exploration allows the insight into MIM to be thoroughly integrated into semi-supervised semantic segmentation. Formally, our approach is named Semi-Supervised Semantic Segmentation with MIM (S4MIM). As the experiments conducted on PASCAL VOC 2012 [36] and Cityscapes [37] demonstrate, our S4MIM achieves state-of-the-art performance. The validation of each component’s effectiveness proves that MIM can boost semi-supervised semantic segmentation."
https://arxiv.org/html/2411.08755v1,Weakly-Supervised Anomaly Detection in Surveillance Videos Based on Two-Stream I3D Convolution Network,"The widespread implementation of urban surveillance systems has necessitated more sophisticated techniques for anomaly detection to ensure enhanced public safety. This paper presents a significant advancement in the field of anomaly detection through the application of Two-Stream Inflated 3D (I3D) Convolutional Networks. These networks substantially outperform traditional 3D Convolutional Networks (C3D) by more effectively extracting spatial and temporal features from surveillance videos, thus improving the precision of anomaly detection. Our research advances the field by implementing a weakly supervised learning framework based on Multiple Instance Learning (MIL), which uniquely conceptualizes surveillance videos as collections of ’bags’ that contain instances (video clips). Each instance is innovatively processed through a ranking mechanism that prioritizes clips based on their potential to display anomalies. This novel strategy not only enhances the accuracy and precision of anomaly detection but also significantly diminishes the dependency on extensive manual annotations. Moreover, through meticulous optimization of model settings, including the choice of optimizer, our approach not only establishes new benchmarks in the performance of anomaly detection systems but also offers a scalable and efficient solution for real-world surveillance applications. This paper contributes significantly to the field of computer vision by delivering a more adaptable, efficient, and context-aware anomaly detection system, which is poised to redefine practices in urban surveillance","Anomaly detection stands as one of the most intricate challenges within the realm of computer vision Zaheer et al. (2019); Sultani et al. (2018); Basharat et al. (2008); Mohindru and Singla (2021); Liu et al. (2020); Li et al. (2014); Kratz and Nishino (2009); Zhao et al. (2011). Specifically, video anomaly detection constitutes a focal point of research, dedicated to discerning uncommon or abnormal behaviors and incidents depicted in video streams. This encompasses a broad spectrum of events, including the unexpected presence of objects or incidents, instances such as a person falling, a vehicular collision, a medical emergency, and deviations from anticipated motion patterns Li et al. (2014). Anomaly detection remains one of the most challenging and significant areas within the realm of computer vision, particularly in the context of video anomaly detection. This field is crucial for identifying uncommon or abnormal behaviors and incidents depicted in video streams, which include a broad spectrum of events such as unexpected object appearances, medical emergencies, vehicular collisions, and deviations from normal motion patterns [Zaheer et al. (2019); Sultani et al. (2018); Basharat et al. (2008); Mohindru and Singla (2021); Liu et al. (2020); Li et al. (2014); Kratz and Nishino (2009); Zhao et al. (2011)]. The applications of anomaly detection are diverse and rapidly growing, spanning from medical imaging and traffic monitoring to urban surveillance, where the stakes for timely and accurate detection are particularly high. Anomaly detection in videos is experiencing rapid growth with diverse applications, including medical imaging, traffic monitoring, and surveillance. Due to the infrequent nature of abnormal occurrences, identifying them manually can be challenging. Therefore, the development of methods to detect patterns deviating from the norm is crucial. Traditional video analysis techniques rely on human monitoring, which is susceptible to errors and time-consuming [Chandola et al. (2009)]. Anomaly detection systems are designed to identify anomalies in video footage and promptly alert users to their presence, proving invaluable in various contexts such as surveillance, industrial monitoring, and other scenarios where detecting abnormalities is critical. The rising demand for urban security has led to an increase in the use of surveillance videos in urban environments to monitor human activity and prevent abnormal events. Detecting anomalies in surveillance videos serves as a crucial tool for security and surveillance, enabling the identification of potential security threats and facilitating prompt action by security personnel. However, continuous monitoring by trained personnel for abnormal events in surveillance videos is both labor-intensive and time-consuming. Hence, research efforts in automatic video anomaly detection are imperative to streamline video monitoring processes, reduce reliance on human resources, and enhance detection accuracy. To develop a video anomaly detection system, the initial step involves extracting relevant features from the videos. Feature extraction is a critical aspect of this process, requiring the identification and selection of significant patterns and attributes from the video data to differentiate normal from abnormal behavior. This entails analyzing the video data and identifying specific characteristics, such as appearance-based and motion-based features. Appearance-based features, such as color, texture, and shape, are derived from an object’s visual appearance and can detect unusual behavior based on changes in object appearance over time. Conversely, motion-based features, derived from the speed, direction, and acceleration of an object’s motion, are utilized to detect abnormal movements or sudden changes in motion that may indicate an anomaly. In recent years, researchers have employed various techniques, including CNNs, VGG architectures, and C3D models, to extract features from visual data. However, a common challenge faced by these methods is their limited capacity to effectively capture temporal features within videos, which are crucial for tasks like anomaly detection. In our study, our primary focus was on addressing this challenge through innovative strategies. Specifically, we investigated the utilization of a two-stream Inflated 3D CNN as our chosen feature extractor. This novel approach facilitates the extraction of both appearance-based (RGB) and motion-based (flow) features from video data. We hypothesized that this integrated approach could yield improved outcomes, as the combination of appearance-based and motion-based features provides a more comprehensive representation of video content. Consequently, we anticipated enhancements in the accuracy and reliability of video anomaly detection systems. Following feature extraction from video data, these features are utilized to train machine learning models for the accurate detection of anomalous behavior within the video data. Various techniques have been developed for anomaly detection, with recent advancements in deep learning leading to the emergence of new methods tailored for surveillance video analysis. Deep learning methods, particularly, exhibit promising capabilities in outperforming conventional machine learning systems, especially when more data is utilized Zhao (2021). Many existing approaches to anomaly detection operate under the assumption that anomalies manifest as deviations from a learned normal pattern. However, this assumption may not always hold true in the context of surveillance videos, which frequently capture complex real-world anomalies that cannot be constructed from normal activities Sultani et al. (2018). Moreover, it is impractical to enumerate all conceivable normal activities, as their normality can vary depending on contextual factors Chandola et al. (2009). To address the limitations of previous works mentioned earlier, the principal accomplishment of this paper is the implementation of an anomaly detection system capable of detecting anomalies with minimal supervision and less reliance on prior information. The main contributions of this paper can be summed up as follows. • In this paper, we propose a novel approach for video anomaly detection based on a two-stream architecture. Our method employs a two-stream Inflated 3D (I3D) Convolutional Neural Network to extract both RGB and optical flow features from the input video. The RGB stream focuses on capturing information related to object appearance and scene context, while the optical flow stream delineates object motion and dynamics between frames. By integrating the insights from both streams through concatenation of the learned features, our approach achieves a holistic understanding of video content, resulting in enhanced accuracy in anomaly detection. • We extended and refined the anomaly detection model introduced in Sultani et al. (2018) by leveraging the PyTorch framework Paszke et al. (2019). Our enhanced detector is trained on videos that are weakly labeled as normal or abnormal. To tackle this task, we adopted a weakly-supervised approach based on video-level annotations, employing the Multiple Instance Learning (MIL) framework. Anomaly detection is treated as a regression problem, with the MIL framework utilized to assign elevated anomaly scores to videos anticipated to contain anomalies. • We conducted an extensive evaluation of our proposed method on the UCF-Crime dataset Sultani et al. (2018), performing experiments to assess its effectiveness in anomaly detection. Our results demonstrate the robust performance of our model, showcasing its capability to accurately detect anomalies in surveillance videos. This paper is organized as follows: In Section 1, we review the existing literature on feature extraction techniques and video anomaly detection methods, establishing the foundation for our research. In Section 2, we present our proposed anomaly detection model, outlining the methodology, including the model architecture and the feature extraction techniques employed. In Section 3, we explore the evaluation metrics and provide experimental results, critically comparing our findings with prior studies. Finally, in Section 4, we conclude with a summary of our contributions and suggest potential avenues for future research to build upon this work."
https://arxiv.org/html/2411.08753v1,"Which Viewpoint Shows it Best?
Language for Weakly Supervising View Selection in
Multi-view","Given a multi-view video, which viewpoint is most informative for a human observer? Existing methods rely on heuristics or expensive “best-view"" supervision to answer this question, limiting their applicability. We propose a weakly supervised approach that leverages language accompanying an instructional multi-view video as a means to recover its most informative viewpoint(s). Our key hypothesis is that the more accurately an individual view can predict a view-agnostic text summary, the more informative it is. To put this into action, we propose a framework that uses the relative accuracy of view-dependent caption predictions as a proxy for best view pseudo-labels. Then, those pseudo-labels are used to train a view selector, together with an auxiliary camera pose predictor that enhances view-sensitivity. During inference, our model takes as input only a multi-view video—no language or camera poses—and returns the best viewpoint to watch at each timestep. On two challenging datasets comprised of diverse multi-camera setups and how-to activities, our model consistently outperforms state-of-the-art baselines, both with quantitative metrics and human evaluation.","Figure 1: Main idea: given multi-view instructional videos, we aim to learn a view selection model that can identify the best view for learning how to perform the activity shown in the videos, in the absence of best view labels. To achieve this, we compare each estimated view-dependent caption to the view-agnostic ground-truth video narration of the human activity, and use their respective accuracies as a proxy for view quality. These quality scores then serve as pseudo-labels for learning to select the most informative view. In this example, the i^{\text{th}} view most clearly shows all entities involved in the activity—the wheel and the person’s hands, and how they interact—and hence, produces a caption that best matches the ground-truth, making it a positive pseudo-label for view selection. Videos are an essential vehicle for communicating how to perform a new skill, as evidenced by the millions of “how-to"" videos online, for everything from frosting a cake to perfecting a basketball layup. The more intricate the task, however, the more important the viewpoint used to film the instructional video. For example, a close-up view of the hands is desirable when a knitter shows how to add stitches of yarn to a needle, or when a rock-climber demonstrates a particular hold—whereas a view from afar may be preferable when the knitter shows the knitted sweater being worn, or the climber shows their selected path up the wall. In general, the information available in any given viewpoint of an activity varies. Not all views are created equal. Shooting video with multiple cameras provides a holistic view of the activity taking place in a scene, by capturing it from different locations and angles, and multi-view video is developing as a new frontier in computer vision research [85, 91, 65, 33, 44], especially in instructional settings [64, 33]. However, multi-view videos are generally not suitable for direct human consumption [21]: digesting multiple views at once imposes a high cognitive burden. Thus, in practice, the status quo is to orchestrate view selection in how-to videos manually with either active camera work or post-production video editing tools, which is time consuming and tedious. What if instead a vision model learned to automatically perform view selection, at every time step deciding which camera from the multi-view video to adopt? View selection has traditionally been studied in the context of automatic cinematography for specialized domains, e.g., 360 panoramas [88, 56], sports clips [12, 43], virtual environments [26, 40, 41, 66], or lecture videos [27, 108, 90]. Aside from their specialized domains, existing work is limited by relying on hand-coded heuristics [26, 40, 3] or assuming access to manual labels indicating the favored views for training [88, 56, 43, 99, 17, 12]. Such labels are expensive and quite special purpose. Conscious of these shortcomings, we propose to learn view selection in multi-view instructional videos in the absence of best view labels. Towards that goal, we hypothesize that view-agnostic natural language descriptions of the activity shown in the videos [33, 50, 44]—commonly referred to as “narrations""111 Narrations in multi-view datasets [33, 50, 44] are produced by human annotators who watch all views and write down a view-independent description of how the activity is performed, and in the wild they correspond to the “how-to” descriptions spoken by a person demonstrating a task [64]. can act as a source of weak supervision. Specifically, our core idea is that for any multi-view video clip, the viewpoint that is most predictive of such a narration is likely to be the most informative of the activity, and hence, can be pseudo-labeled as the best view for training a view selector. For example, given a multi-view video of a person repairing a bike (Figure 1), independent captions on each view will emphasize different visible components of the scene (the wheel, the person’s hands, other objects in the scene, etc.); the caption most aligned with the view-agnostic narration “the person removes the rear wheel with both hands"" indicates which view is most informative for the whole activity content in that clip. Unlike explicit best-view labels, the vision-language annotations that fuel today’s captioners are open-world, versatile, and widely available. To validate our hypothesis, we design a novel framework composed of two key elements: a best view pseudo-labeler, and a best view selector. See Fig. 1. The pseudo-labeler automatically generates best view pseudo-labels for a multi-view video during training, by using off-the-shelf video captioners [109, 58] to score and rank views on the basis of how well the predicted narration from a view matches the view-agnostic ground-truth narration. The selector takes a multi-view video as input, and predicts the best view labels. During training, the selector also solves an auxiliary task of predicting the relative camera pose between different views, to increase its view-sensitivity and improve its selection accuracy. At inference, our model requires as input only a multi-view video, but no language or camera poses. We evaluate our method using two challenging multi-view instructional video datasets encompassing diverse activity scenarios and multi-camera setups, Ego-Exo4D [33] and LEMMA [50]. On both, our model outperforms multiple baselines and state-of-the-art methods for view selection on several automatic and human evaluation metrics. More broadly, our work offers a novel way for language to elicit the information content of video."
https://arxiv.org/html/2411.08715v1,Retrieval Augmented Recipe Generation,"The growing interest in generating recipes from food images has drawn substantial research attention in recent years. Existing works for recipe generation primarily utilize a two-stage training method—first predicting ingredients from a food image and then generating instructions from both the image and ingredients. Large Multi-modal Models (LMMs), which have achieved notable success across a variety of vision and language tasks, shed light on generating both ingredients and instructions directly from images. Nevertheless, LMMs still face the common issue of hallucinations during recipe generation, leading to suboptimal performance. To tackle this issue, we propose a retrieval augmented large multimodal model for recipe generation. We first introduce Stochastic Diversified Retrieval Augmentation (SDRA) to retrieve recipes semantically related to the image from an existing datastore as a supplement, integrating them into the prompt to add diverse and rich context to the input image. Additionally, Self-Consistency Ensemble Voting mechanism is proposed to determine the most confident prediction recipes as the final output. It calculates the consistency among generated recipe candidates, which use different retrieval recipes as context for generation. Extensive experiments validate the effectiveness of our proposed method, which demonstrates state-of-the-art (SOTA) performance in recipe generation on the Recipe1M dataset.","With the rising focus on food and health, food computing [49] has increasingly captured attention and spurred various food related tasks, such as food recognition [12, 68, 7, 48, 3, 28, 43, 22, 75, 59, 30], cross-modal recipe retrieval [77, 51, 66, 76, 57, 6], recipe generation [56, 65, 64, 11, 46], food recommendation [60, 19, 17] and food logging [55, 13]. Previous research on food understanding has primarily focused on classifying food and ingredient recognition [54, 45, 73, 42]. However, due to the limited availability of detailed information on prepared foods, a comprehensive visual food recognition system should not only be able to identify the type of diet or its ingredients but also generate cooking instructions. Therefore, the task of recipe generation has become a significant task in the field of food computing. Figure 1: (a) The structural differences between our retrieval-augmented framework and the “two-stage"" [56, 11] and “LMMs-based"" [44, 70] approaches. “G"" refers to the generator. (b) Recipe generation results comparison. “GT"" refers to ground truth, “LLaVA-FT"" denotes the model using pre-trained LLAVA weights fine-tuned on Recipe1M, “Inverse cooking"" [56] represents a model trained with two-stage, “FoodLMM"" [70] is the LMMs-based model for recipe generation, and “Ours"" refers to our model, where yellow highlights indicate ingredients that match those in the “GT"", blue signifies cooking instructions predictions matching the “GT"", and red font denotes incorrectly predicted ingredients. Previous methods for recipe generation [56, 47, 9] typically use a two-stage approach: first extracting ingredients from images, then generating instructions based on the embeddings of those ingredients and the images, which is shown in Figure 1 (a). Due to limited training data and poor multi-modal alignment, traditional methods often yield unsatisfactory results. In contrast, Multi-modal Models (LMMs) [1, 8, 14, 2, 44] can directly generate recipes from images in one stage. FoodLMM [70] improved recipe generation performance but still suffers from hallucinations, affecting recipe quality. Figure 1 (b) compares the recipe generation results of different methods. Compared to the ground truth instructions, the results predicted by one of the state-of-the-art LMMs, LLaVA [44], exhibit clear hallucinations, as it incorrectly identifies crumbs as ‘beef’ and erroneously detects ‘tomatoes’ and ‘taco seasoning’ that are not present in the image. Although the two-stage method [56] accurately identifies the correct temperature, it fails to precisely recognize the ingredients. FoodLMM [70] manages to identify most of the correct ingredients but still hallucinated, mistakenly recognizing ‘rice’. The result arises due to inadequate multi-modal understanding and a lack of effective use of context, which prevents the models from learning sufficient information. This paper addresses the limitation by introducing the first retrieval-augmented large multimodal model to generate recipes from food images. The proposed architecture consists of a retriever and a generator. The retriever leverages an off-the-shelf cross-modal recipe retrieval model [57] to identify semantically similar recipes from the image. The generator is built upon LLaVA [44] with LoRA [26] to generate recipes based on the image and retrieved reference recipes. On the one hand, we propose Stochastic Diversified Retrieval Augmentation (SDRA) to provide a rich and diverse set of retrieved recipes as references, which could provide relevant knowledge for the generation to reduce hallucination [52, 24, 29]. Section 2 of the supplementary materials compares retrieved recipes with the ground truth, highlighting related content that helps alleviate hallucinations and improve generation. On the other hand, we propose Self-consistency Ensemble Voting strategy to improve generation quality during the inference phase. Specifically, the generator produces multiple recipe candidates using different retrieved recipes and the image. Cosine similarity scores are computed for these candidates to assess their mutual agreement. The recipe with the highest score relative to the others is selected as the final output. The consensus among different candidates is capable of further reducing the hallucination in the generated recipes, as detailed in Section 5 of the supplementary materials, which explains why the final output is superior. On Recipe1M dataset [66], our proposed model significantly outperforms existing methods and fine-tuned LLaVA. Moreover, our model demonstrates strong generalizability, surpassing the state-of-the-art (SOTA) benchmarks on the Recipe1M dataset for ingredient recognition metrics. Overall, our main contributions can be summarized as follows: • We propose the first retrieval-augmented large multimodal model tailored for recipe generation. Our model introduces a stochastic diversified retrieval-augmentation technique to enhance the diversity and richness of retrieval. Additionally, we employ a Self-consistency Ensemble Voting strategy during the inference stage, using different retrieved recipes to ensure agreement and consistency in the generated recipe. • Our proposed model achieves SOTA recipe generation performance on Recipe1M dataset. We conduct comprehensive ablation studies to validate the effectiveness of our design choices and demonstrate the contributions of each component to the overall performance. Our proposed model exhibits exceptional adaptability, outperforming current SOTA results in ingredient recognition on the Recipe1M dataset."
https://arxiv.org/html/2411.08701v1,TRACE: Transformer-based Risk Assessment for Clinical Evaluation,"We present TRACE (Transformer-based Risk Assessment for Clinical Evaluation), a novel method for clinical risk assessment based on clinical data, leveraging the self-attention mechanism for enhanced feature interaction and result interpretation. Our approach is able to handle different data modalities, including continuous, categorical and multiple-choice (checkbox) attributes. The proposed architecture features a shared representation of the clinical data obtained by integrating specialized embeddings of each data modality, enabling the detection of high-risk individuals using Transformer encoder layers. To assess the effectiveness of the proposed method, a strong baseline based on non-negative multi-layer perceptrons (MLPs) is introduced. The proposed method outperforms various baselines widely used in the domain of clinical risk assessment, while effectively handling missing values. In terms of explainability, our Transformer-based method offers easily interpretable results via attention weights, further enhancing the clinicians’ decision-making process.","1 Introduction Healthcare is an industry that can gain significantly by utilizing modern artificial intelligence (AI) and machine learning (ML) methods by assisting clinicians in diagnosis, risk assessment and pathology of diseases. By incorporating AI-driven risk assessment algorithms ([1, 2, 3]), clinicians can offer risk stratification of the patients for screening recommendations, which can help in early detection of diseases, enabling better informed decision-making from the clinicians, on the one hand, and timely intervention for patients that are considered high-risk, on the other. Healthcare data, typically collected using questionnaire-based surveys, exhibit a large diversity both in the nature, the quantity and the completeness of the attributes that are recorded (or reported) for each patient. Importantly, clinical data are multi-modal, comprising a combination of numerical (i.e., age, height, weight etc.) and categorical features (i.e., eye color, hair color, etc.) or even ‘‘checkboxes’’, where multiple values within the same feature are valid simultaneously (i.e., ancestry, doctors visited, etc.). On the other hand, missing values and other issues affecting clinical, and tabular data in general, also pose a significant challenge for maximizing data utilization. This is crucial in the case of healthcare data, as they are generally scarce, and their collection is laborious and with high cost, while AI, and deep-learning in particular, typically assume a large quantity of available data for training representative models. Finally, another characteristic of clinical data is the notable imbalance between case and control groups. These aspects, necessitate the development of task-specific AI and ML methods for the clinical domain. To address the data scarcity, effective methodologies dealing with clinical data should either enhance the data artificially (e.g., through data augmentation/imputation, generative models, etc.) or introduce ways to effectively utilize missing values, inconsistencies, and other issues of the available data. In this work, we propose a transformer-based [4] clinical risk assessment model operating across the spectrum of clinical data feature modalities that explicitly handles missing values, leading to improved performance with minimal computational overhead. Another key contribution of our work is the explainability provided through the generation of attention maps, which facilitates the interpretability of the produced results both for computer scientists and for clinicians. In summary, the contributions of this work are summarized below: • We propose a novel framework to perform clinical risk assessment using different data modalities that explicitly handles instances with missing values. • We introduce ‘‘checkbox embeddings’’ to handle features with multiple valid categories simultaneously, commonly extracted from questionnaire-based surveys. • The proposed model offers improved explainability, assisting clinicians in interpreting the model results and taking informed decision. • We establish a new baseline method by deploying a non-negative neural network, inspired by [5] and designed for risk assessment on healthcare clinical data. • We perform extensive experiments on two clinical datasets for melanoma and heart disease risk assessment, showing that the proposed model, although significantly smaller, achieves competitive performance with respect to the state-of-the-art."
https://arxiv.org/html/2411.08666v1,A Survey on Vision Autoregressive Model,"Autoregressive models have demonstrated great performance in natural language processing (NLP) with impressive scalability, adaptability and generalizability. Inspired by their notable success in NLP field, autoregressive models have been intensively investigated recently for computer vision, which perform next-token predictions by representing visual data as visual tokens and enables autoregressive modelling for a wide range of vision tasks, ranging from visual generation and visual understanding to the very recent multimodal generation that unifies visual generation and understanding with a single autoregressive model. This paper provides a systematic review of vision autoregressive models, including the development of a taxonomy of existing methods and highlighting their major contributions, strengths, and limitations, covering various vision tasks such as image generation, video generation, image editing, motion generation, medical image analysis, 3D generation, robotic manipulation, unified multimodal generation, etc. Besides, we investigate and analyze the latest advancements in autoregressive models, including thorough benchmarking and discussion of existing methods across various evaluation datasets. Finally, we outline key challenges and promising directions for future research, offering a roadmap to guide further advancements in vision autoregressive models.","Autoregressive (AR) models have recently driven significant progress in artificial intelligence, particularly through models like the GPT series [1, 2, 3, 4, 5] and other large language models (LLMs) [6, 7, 8] that excel at solving a variety of natural language processing tasks. These models employ a straightforward yet powerful “next-token prediction” strategy, allowing them to generate coherent and contextually relevant text by predicting each subsequent word in a sequence. The success of AR models can be attributed to two key characteristics: (1) scalability, as scaling laws [9, 10] enable researchers to predict the performance of larger models based on smaller ones, optimizing resource allocation and guiding model development; and (2) generalizability, as AR models can adapt to new and unseen tasks without requiring task-specific training [1, 3]. These promising characteristics enable AR models to address language tasks with unprecedented effectiveness, revealing their potential toward general-purpose AI systems. Inspired by the success of AR models in natural language processing, recent studies have extended AR models to visual generation tasks. Notable examples include models like VQVAE [11], VQGAN [12], DALL-E [13], and Parti [14], which convert continuous images into discrete tokens through image tokenizers. This conversion allows AR models to generate images through a next-token prediction approach similar to that used in language processing. Visual tokenization unifies the representation of text and images by treating both as sequences of discrete tokens, making them compatible with sequence-to-sequence modeling techniques. As a result, these models can leverage the architectures similar to the GPT series [1, 2, 3] to learn effectively from large collections of text-image pairs. Beyond visual generation, AR models have also advanced visual understanding, particularly within the area of multimodal understanding [15, 16, 17, 18, 19], where they are designed to perceive and integrate multiple modalities. In multimodal tasks, AR models are trained to interpret visual input and generate coherent textual sequences, making them powerful tools for applications requiring a deep understanding of both visual and textual information. For example, multimodal large language models (MLLMs) like LLaVA [15] utilize LLMs to interpret visual input alongside text, allowing them to answer questions about images, generate descriptive captions, and engage in dialogue with detailed visual context. Through this design, AR-based MLLMs demonstrate great potential for advancing versatile visual understanding capabilities in AI applications. Given the achievements of AR models in visual generation and visual understanding, recent works attempt to assemble the two types of capabilities into a unified AR model that can handle both visual generation and understanding. For example, Transfusion [20] demonstrates this integration by combining the next-token prediction objective commonly used in language modeling with diffusion processes for image generation. By jointly training on both text and image data, Transfusion [20] can handle both discrete text tokens and continuous image data within a single transformer architecture, enabling it to perform a wide range of multimodal tasks and bridging the gap between visual understanding and visual generation. Moreover, AR models show strong capabilities in both understanding and generation across other domains, such as video [21], where they handle tasks like video captioning, generation, and scene interpretation. Despite the significant progress and growing interest in AR models for vision research, there is currently a lack of a systematic survey to provide an overarching view of existing methods, challenges, and potential future directions. This paper aims to fill this gap by conducting a comprehensive survey of AR models across a diverse range of vision tasks, categorized by task type, including image generation, image understanding, and other areas. We approach this survey from multiple perspectives, covering the background of AR models, relevant datasets, methodologies, benchmarks, as well as current research challenges and open directions. Our goal is to offer the community a clear overview of what has been achieved, the challenges that remain, and the promising directions for future research in AR models for vision. The main contributions of this work can be summarized in three key aspects. First, we provide a systematic and comprehensive review of the applications of AR models in vision, developing a taxonomy of existing methods and highlighting their major contributions, strengths, and limitations. Second, we investigate and analyze the latest advancements in AR models, including thorough benchmarking and discussion of existing methods across various evaluation datasets. Third, we identify and discuss several challenges and promising directions for future research in AR models, aiming to guide the community in addressing open questions and advancing the field. Figure 1: A timeline of representative autoregressive models in vision. We are witnessing rapid growth in this field. More works can be found in our released GitHub page, which is updated daily. {forest} for tree= grow=east, parent anchor=east, child anchor=west, align=center, l=2cm, s sep=0.5cm, edge=draw=black, -latex, rounded corners, draw, minimum size=0.5cm, fill=blue!10, edge path= [draw, -latex] (!u.east) – +(1mm,0) —- (.child anchor)\forestoptionedge label; , font= [Datasets for Autoregressive Models in Vision, calign=edge midpoint [3D Shape and Scene Generation, calign=edge midpoint, [ ShapeNet, ModelNet ] ], [Medical Image Datasets, calign=edge midpoint, [ BraTS, LUNA16, MIMIC-CXR, RibFrac, TCIA Covid19, AMOS22, ISLES2022, AbdomenCT-1K, Totalsegmentator, Verse 2020, RSNA-2022-CSFD, RSNA-2020-PED, STOTIC, FLARE22, FLARE23, DeepLesion dataset, Task03 Liver, Task06 Lung, Task07 Pancreas, Task08 Hepatic Vessel, Task09 Spleen, Task10 Colon ] ], [Multi-modal Generation Datasets, calign=edge midpoint, [ WikiHow, WIT, DeepSeek-VL, MME, GQA, Capfusion-120M, Capfusion dataset, LAIONaesthetics-12M, LLaVA-v1.5mix-665K, LLaVA-Pretrain558K, DataComp, COYO700M, GenHowTo ] ], [Motion Generation Datasets, calign=edge midpoint, [ Human3.6M, MPII Human Pose, AMASS, HumanLong3D, HumanMusic, KIT, HumanML3D, KIT-ML ] ], [Image Editing, calign=child edge, [ ImageNet, ADE20K, COCOStuff, LAION-Aesthetics, MultiGen-20M ] ], [Image-to-Image Translation Datasets, calign=edge midpoint, [ Cityscapes, Edges2Shoes/Edges2Handbags ] ], [Text-to-Image Generation, calign=edge midpoint, [ MSCOCO, Flickr30K, Text2Art Dataset, PartiPrompts, CC12M, CC, YFCC100m, Redcaps, MJHQ-30K, GenEval, DPG-Bench, VILA1.5-13B, T2I-CompBench, ] ] [Video Generation Datasets, calign=edge midpoint, [ UCF-101, Kinetics-600, Vimeo-90K, SkyTimelapse, Text2Video Dataset, AudioSet, AVA-Action, Moments-in-Time (MiT), Something-Something v2 (SSV2) ] ], [Image Generation Datasets, calign=edge midpoint, [ ImageNet, CelebA-HQ, CIFAR-10/100, LSUN, LAION-5B, WebLI, JourneyDB, LAION-High-Resolution111https://huggingface.co/datasets/laion/laion-high-resolution, FFHQ, OpenImages ] ] ] Figure 2: Datasets for Autoregressive Model in Vision"
https://arxiv.org/html/2411.08665v1,OSMLoc: Single Image-Based Visual Localization in OpenStreetMap with Geometric and Semantic Guidances,"OpenStreetMap (OSM), an online and versatile source of volunteered geographic information (VGI), is widely used for human self-localization by matching nearby visual observations with vectorized map data. However, due to the divergence in modalities and views, image-to-OSM (I2O) matching and localization remain challenging for robots, preventing the full utilization of VGI data in the unmanned ground vehicles and logistic industry. Inspired by the fact that the human brain relies on geometric and semantic understanding of sensory information for spatial localization tasks (Hermer and Spelke, 1994; Epstein and Vass, 2014), we propose the OSMLoc in this paper. OSMLoc is a brain-inspired single-image visual localization method with semantic and geometric guidance to improve accuracy, robustness, and generalization ability. First, we equip the OSMLoc with the visual foundational model to extract powerful image features. Second, a geometry-guided depth distribution adapter is proposed to bridge the monocular depth estimation and camera-to-BEV transform. Thirdly, the semantic embeddings from the OSM data are utilized as auxiliary guidance for image-to-OSM feature matching. To validate the proposed OSMLoc, we collect a worldwide cross-area and cross-condition (CC) benchmark for extensive evaluation. Experiments on the MGL dataset, CC validation benchmark, and KITTI dataset have demonstrated the superiority of our method. Code, pre-trained models, CC validation benchmark, and additional results are available on: https://github.com/WHU-USI3DV/OSMLoc.","Map Construction and positioning are the basics of seamless navigation for logistic robots and unmanned ground vehicles (UGV). Most existing industrial solutions use the geo-referenced images (Arandjelovic et al., 2016; Cheng et al., 2018; Hausler et al., 2021; Zhu et al., 2023) or pre-built 3D point cloud maps (Yu et al., 2021; Chen et al., 2022; Shi et al., 2022; Zou et al., 2023) as the reference map. However, constructing geo-referenced images or point cloud maps is expensive and difficult to update. Since the beginning of the Internet era, volunteering geo-informatics (VGI) has emerged as a new paradigm that creates, assembles, and disseminates geographic data by crowd-sourcing users instead of specific experts (Wang et al., 2024). With over 10 million registered contributors (OSM_Foundation, 2024), the OpenStreetMap (OSM) provides detailed and up-to-date data about stationary objects throughout the world, including infrastructure and other aspects of the built environment, points of interest, land use and land cover classifications, and topography (Fan et al., 2014; Vargas-Munoz et al., 2020). With rich geographic and semantic information, humans can easily localize themselves by comparing the surrounding views with the OSM web map. However, the image-to-OSM (I2O) localization process remains challenging for robots due to differing modalities and perspectives, which limits the full potential of VGI data in the logistics industry and hinder the development of navigation with the crowd-sourcing map. Due to above mentioned difficulties, research on I2O visual localization is scarce. Early I2O localization approaches, such as Samano et al. (2020) localize along the pre-defined routes, while Zhou et al. (2021) integrates deep image features into the Monte Carlo framework to improve the representation ability of the observation model. OrienterNet (Sarlin et al., 2023) is the first end-to-end single-image I2O localization approach, which lifts the front-view image features to the birds-eye-view (BEV) and estimates the pose by dense matching. MaplocNet (Wu et al., 2024) proposed a coarse-to-fine registration pipeline for I2O localization and introduced the semantic labels from the autonomous driving datasets as auxiliary supervision. Although existing methods have achieved impressive results, we highlight that two key challenges remain unsolved: 1) effective modular transformation of visual observations for cross-modality I2O matching; 2) explainable localization and strong generalization across diverse environments worldwide. Figure 1: Image-to-OpenstreetMap localization aims to estimate the 3 degrees of freedom (3-DoF) camera pose. (a) shows the core idea of our method, which integrates geometry and semantic guidance into the framework. (b) shows the global evaluation results. Inspired by how the human brain uses geometric and semantic understanding from sensory information and long-term spatial knowledge for visual localization task (Hermer and Spelke, 1994; Vass and Epstein, 2013; Epstein and Vass, 2014), this paper introduces the OSMLoc, a single image-based visual localization framework with semantic and geometric guidance. The core idea of OSMLoc is illustrated in Fig. 1(a). We inherit the fundamental model with rich prior knowledge, specifically, the DINOv2 encoder (Oquab et al., 2023) of the Depth Anything model (Yang et al., 2024) into our framework to construct powerful image features. A compact depth distribution adapter is proposed to incorporate geometric information from Depth Anything into camera-to-BEV transformation. To our knowledge, this is the first attempt to integrate the foundation model into the I2O visual localization framework. The semantic consistency between visual and OSM data aids the model’s learning in completing cross-modular matching. Moreover, existing same-area validation sets exhibit similar styles, with consistent building and road patterns within neighborhood blocks of the training sets. This similarity arises because the data is typically collected by the same users, on similar devices, and within close timeframes. This uniformity may introduce bias into the validation results. Therefore, we collect a novel cross-area and cross-condition (CC) validation benchmark for evaluation in diverse environments, as shown in Fig. 1(b). The CC validation benchmark contains thousands of images from Detroit, Munich, Taipei, and Brisbane with vast scenarios. Experiments on the Mapillary geo-localization (MGL) dataset, CC validation benchmark, and the KITTI dataset reveal that our method outperforms existing methods, particularly in unseen areas. Overall, the main contributions include: 1. We propose a brain-inspired framework, OSMLoc, which jointly learns the geometry, semantics, and pose for I2O visual localization. Given the challenges of localization across cross-view and cross-modality sources, we fully leverage geometric and semantic cues from the foundational model and OSM data to help the framework bridge the domain gap and understand the surroundings. 2. We propose a novel depth distribution adapter (DDA) to incorporate the depth prior to the camera-to-BEV transformation. Since accurate depth is essential for viewpoint transformation and monocular depth estimation is ambiguous, we utilize the DDA to distill the prior depth knowledge from the foundation model into our framework. 3. We introduce the auxiliary semantic alignment task to improve the scene understanding capability of the image model in a self-supervision manner. As the OSM data provides detailed semantic and geographic features of the areas, roads and other objects, we leverage the semantic consistency between the image and map as an additional constraint for the visual localization task. 4. We collect the cross-area and cross-condition (CC) validation benchmark with thousands of frames in Detroit, Munich, Taipei, and Brisbane, providing an extensive testbed for the I2O visual localization models."
https://arxiv.org/html/2411.08663v1,Toward Human Understanding with Controllable Synthesis,"Training methods to perform robust 3D human pose and shape (HPS) estimation requires diverse training images with accurate ground truth. While BEDLAM demonstrates the potential of traditional procedural graphics to generate such data, the training images are clearly synthetic. In contrast, generative image models produce highly realistic images but without ground truth. Putting these methods together seems straightforward: use a generative model with the body ground truth as controlling signal. However, we find that, the more realistic the generated images, the more they deviate from the ground truth, making them inappropriate for training and evaluation. Enhancements of realistic details, such as clothing and facial expressions, can lead to subtle yet significant deviations from the ground truth, potentially misleading training models. We empirically verify that this misalignment causes the accuracy of HPS networks to decline when trained with generated images. To address this, we design a controllable synthesis method that effectively balances image realism with precise ground truth. We use this to create the Generative BEDLAM (Gen-B) dataset, which improves the realism of the existing synthetic BEDLAM dataset while preserving ground truth accuracy. We perform extensive experiments, with various noise-conditioning strategies, to evaluate the tradeoff between visual realism and HPS accuracy. We show, for the first time, that generative image models can be controlled by traditional graphics methods to produce training data that increases the accuracy of HPS methods. The code and dataset will be available for research purposes.","Training neural networks to accurately estimate 3D human pose and shape requires large amounts of accurate training data. Ideally, this data includes paired images with ground truth (GT) pose and shape parameters, e.g. parameters of a parametric body model like SMPL [25]. The diversity of the images and poses are critical for methods to generalize. Capturing real images with ground truth data is hard, particularly with complex poses, so many methods rely on pseudo ground truth estimated from 2D data. This can introduce errors and bias. An alternative is synthetic data but, until recently, the domain gap between real and synthetic images meant that such data was insufficient to train networks without also using real data. The BEDLAM dataset [4] demonstrated that traditional graphics methods can be used to produce images with ground truth data of sufficient complexity and realism that training only on the synthetic data produces state-of-the-art (SOTA) performance. Despite this, images in the BEDLAM dataset are still very obviously synthetic. In contrast, recent generative image models produce highly realistic images of people that are often indistinguishable from real images. Generative models can synthesize an effectively infinite variation in clothing, hair, lighting, skin tone, etc. To date, however, such imagery [35, 36] could not be directly use for HPS methods because it lacks aligned ground truth 3D body shape and pose parameters. The question then arises whether generative image methods can be leveraged to make synthetic training datasets with accurate ground truth and realistic imagery. There have been several attempts [43, 12] to obtain a pseudo-ground truth label from the generated images using ControlNet [46] and depth information to provide some control over the human body. However, they still do not manage to beat the BEDLAM dataset. This shows that controllable synthesis has the potential to produce highly realistic images, however, there is something missing to constrain them to retain accurate ground truth information. In this work, we combine both, generative models with traditional rendered images to enhance their photorealism while maintaining the accuracy of the ground truth pose and shape data. This accomplishes what is desired in terms of reducing the visual domain gap between synthetic data and real images. Unfortunately, it introduces a new kind of “gap” between the generated images and the ground truth pose and shape. Making the images realistic changes them in ways that then deviate from the ground truth—the more realistic the imagery, the greater this gap, see Fig. 1 and Fig. 7. The changes can be subtle to the human eye, though clothing often changes in ways that imply a different body shape underneath. We verify this effect quantitatively by evaluating HPS methods trained on the synthetic imagery and observe that as visual realism of the training data increases, the performance of networks trained on the data decreases. What we seek at the end is a method to produce a strong alignment between visually realistic synthetic images and the ground truth. Here we explore several methods to improve this alignment. In particular, we condition the generative process on 2D keypoints, rendered depth maps, rendered surface normals, and image edges. We use Stable Diffusion model [35] and add varying amounts of noise to the original BEDLAM images. We explore using different noise levels for different parts of the body. Our pipeline changes things like clothing, without impacting the body shape. We increase the realism of the faces and add hair and shoes, which are missing or often missing in BEDLAM. Through these experiments, we find our pipeline increases realism while staying aligned with the ground truth, see Fig. 5. The result is a new Generative BEDLAM (Gen-B) dataset that can be used to train HPS methods. In addition to our pipeline and dataset, a key contribution of our work is that it explores and exposes the issues involved in using generative image models to create training data for HPS. We expose the alignment issue as equally important as the well-known visual domain gap. While it may seem obvious a priori that improving image realism is a useful goal, our work suggests caution. Many HPS methods today use small image crops to analyze human pose and shape. At low resolution, improvements in image realism may not be significant. In the end, one wants an HPS method to be invariant to many things including unrealistic images. Finally, we extensively evaluate state-of-the-art HPS networks to test how well our dataset generalizes to real-world images compared to BEDLAM. We train HMR [21] with the original BEDLAM dataset and Gen-B. Our results show that training on Gen-B improves accuracy on real test datasets—3DPW, HMDB, and RICH—by 2.37\%, 4.66\%, and 1.95\%, respectively. Similarly, when training CLIFF [24], Gen-B consistently outperforms BEDLAM, achieving lower error across all datasets. This consistent advantage extends to newer architectures like transformer-based ones, where we observe that HMR2.0 [13] trained on Gen-B show a reduction of error of 2.26\% on the RICH dataset and keep being marginally better on the 3DPW dataset. In summary, we (1) provide a process to upgrade synthetic data like BEDLAM using generative models while preserving alignment with the ground truth body shape and pose; (2) provide a new dataset, Gen-B, that can be used to train HPS regressors to achieve SOTA accuracy; (3) extensively evaluate existing HPS methods trained on Gen-B, (4) explore and discuss how narrowing the domain gap is not enough and that alignment between the images and the ground truth is key. The dataset, code to generate the dataset, trained models, and model training code will be available for research purposes."
https://arxiv.org/html/2411.08656v2,MikuDance: Animating Character Art with Mixed Motion Dynamics,"We propose MikuDance, a diffusion-based pipeline incorporating mixed motion dynamics to animate stylized character art. MikuDance consists of two key techniques: Mixed Motion Modeling and Mixed-Control Diffusion, to address the challenges of high-dynamic motion and reference-guidance misalignment in character art animation. Specifically, a Scene Motion Tracking strategy is presented to explicitly model the dynamic camera in pixel-wise space, enabling unified character-scene motion modeling. Building on this, the Mixed-Control Diffusion implicitly aligns the scale and body shape of diverse characters with motion guidance, allowing flexible control of local character motion. Subsequently, a Motion-Adaptive Normalization module is incorporated to effectively inject global scene motion, paving the way for comprehensive character art animation. Through extensive experiments, we demonstrate the effectiveness and generalizability of MikuDance across various character art and motion guidance, consistently producing high-quality animations with remarkable motion dynamics.","Character art plays a crucial role in the film, game, and digital design industries. Animating character art, which brings static character images to life, has been an increasingly prominent challenge in computer vision and graphics. Traditional animation software, such as MMD [11], and Live2D [22], requires professional skills, posing significant barriers for non-experts. Recently, image-to-video generation methods [40, 21, 7, 19, 50] have emerged as a promising solution for animation. However, these methods are primarily designed for animating real-world humans and cannot be directly applied to character art due to the following two key challenges. Figure 1: We propose MikuDance, a Diffusion-based pipeline for animating complex and stylized character art with high-dynamic motion guidance. The core insight of MikuDance lies in its Mixed Motion Modeling and Mixed-Control Diffusion capabilities. The first challenge arises from the high-dynamic motion guidance for both the complex foreground and background in character art, making unified control and maintaining temporal consistency difficult. For instance, in the second drawing shown in Figure 1, the girl is portrayed in an elegant dress against an artistic background, driven by large-scale dance motion and camera movements. Existing image animation methods, such as Animate Anyone [14] and DISCO [36], are primarily limited to animating humans with a static camera and a clean background. In contrast, animating character art requires the model to handle large-scale motion within complex scenes. As a result, simultaneously modeling the high-dynamic motion of both characters and the entire backgrounds becomes a critical task. The second challenge stems from the unique body shapes and diverse scales of characters, which often misalign with motion guidance. For example, anime characters exhibit a large head-to-body ratio, exaggerated poses, and varied artistic styles. As shown in Figure 1, previous methods employ separate networks to process the reference image and motion guidance, oversimplifying the task by assuming a pre-aligned human body [51, 30], or performing alignment through pre-processing [37], which often leads to unnecessarily complex motion control architectures. However, considering that art images feature distinct characters, explicit alignment becomes impractical. Thus, implicitly aligning the reference image and motion guidance within a unified structure presents a significant task. To address these challenges and leverage recent advancements in video generation for character art animation, we propose MikuDance. MikuDance animates in-the-wild character art by utilizing mixed character-scene motion guidance to generate videos with large-scale motion dynamics. It introduces two key techniques: Mixed Motion Modeling and Mixed-Control Diffusion. Mixed Motion Modeling explicitly represents character motions and 3D dynamic camera movements within a unified 2D space, enabling local and global motion guidance of both foreground and background in animations. Unlike previous methods that use camera parameters directly as the control signal [39, 42, 9], we propose a Scene Motion Tracking (SMT) strategy to model the global motion. SMT strategy projects the reference image to a scene point cloud and tracks corresponding points across consecutive camera frames, transforming camera poses into a pixel-wise scene motion representation. This scene motion, resembling keypoint-based character motion, establishes the foundational basis of our mixed motion control approach. Mixed-Control Diffusion addresses misalignments in scales and body shapes of characters by integrating all reference and motion guidance into a unified Reference UNet [29]. This design is based on our observation that the mixed and implicit alignment approach outperforms other sophisticated control networks while preserving an elegant model architecture. Moreover, as scene motion guides the global dynamics of the animation, we carefully design a Motion-Adaptive Normalization (MAN) module to flexibly inject the scene motion into the Reference UNet, effectively integrating global dynamics and maintaining local consistency in character art animation. By leveraging these two techniques and a mixed-source training approach, MikuDance animates diverse character art with mixed motion dynamics. We evaluate MikuDance using a range of reference characters and motion guidance. Both qualitative and quantitative results demonstrate that MikuDance can generate high-quality animation, particularly in maintaining consistency in character local motion and effectively handling large-scale scene motion. Contributions of our MikuDance are listed below: • Mixed Motion Modeling is proposed to explicitly model character and camera motions within a unified pixel-wise space, enabling the effective representation of high-dynamic motion. • Mixed-Control Diffusion is exploited to implicitly align character shape, pose, and scale with the motion guidance, enabling cohesive motion control for character art animation. • Extensive experiments demonstrate the effectiveness and generalizability of our MikuDance, achieving superior animation quality and high-dynamic motion control compared to state-of-the-art methods. Figure 2: Illustration of our MikuDance pipeline. Given a reference character art and a driving video, the pixel-wise scene motion is predicted using the Scene Motion Tracking (SMT) strategy, which is combined with the character poses to form the character-scene mixed motion guidance. The Mixed-Control Diffusion subsequently generates the animation in a latent space, guided by the character poses and the scene motion injected through the Motion-Adaptive Normalization (MAN) module."
https://arxiv.org/html/2411.08606v1,LG-Gaze: Learning Geometry-aware Continuous Prompts for Language-Guided Gaze Estimation,"The ability of gaze estimation models to generalize is often significantly hindered by various factors unrelated to gaze, especially when the training dataset is limited. Current strategies aim to address this challenge through different domain generalization techniques, yet they have had limited success due to the risk of overfitting when solely relying on value labels for regression. Recent progress in pre-trained vision-language models has motivated us to capitalize on the abundant semantic information available. We propose a novel approach in this paper, reframing the gaze estimation task as a vision-language alignment issue. Our proposed framework, named Language-Guided Gaze Estimation (LG-Gaze), learns continuous and geometry-sensitive features for gaze estimation benefit from the rich prior knowledges of vision-language models. Specifically, LG-Gaze aligns gaze features with continuous linguistic features through our proposed multimodal contrastive regression loss, which customizes adaptive weights for different negative samples. Furthermore, to better adapt to the labels for gaze estimation task, we propose a geometry-aware interpolation method to obtain more precise gaze embeddings. Through extensive experiments, we validate the efficacy of our framework in four different cross-domain evaluation tasks.","The gaze estimation is crucial for understanding human behavior. Precise gaze estimation offers significant assistance for many applications, such as human-computer interaction [1], augmented reality [21], and driver monitoring systems [20]. The deep learning has led to significant improvements in gaze estimation based on appearance. While these methods show impressive results in evaluations within the same domain, they tend to experience a decline in performance when evaluated on different domains. This decline is primarily attributed to overfitting rather than learning robust features from the original domain. Figure 1: (a) The traditional method of gaze generalization involves overseeing model training by means of numerical label regression. (b) In our study, we introduce a text models to steer the development of robust features in visual models. Gaze data encompass diverse elements, such as appearance, wearables, environments, and image clarity[31, 34], yet gaze annotations predominantly focus on eye direction, marginalizing other variables as noise. Such irrelevant factors exacerbate the domain disparity, hindering gaze models’ adaptability. Additionally, supervisory numeric labels might induce overfitting, influenced by these extraneous gaze components. As shown in Figure 1(a), training a gaze model typically involves utilizing a vision encoder to extract features and then employing a regressor for prediction. Traditional methods train a gaze model by using data disturbing [31] ,feature enhancement [30], and adversarial learning [6] to improve the generalization ability of the gaze model. However, these regularization techniques have limited effectiveness as they rely solely on numerical labels for supervision, making them susceptible to overfitting due to irrelevant factors. The above may indicate that it is difficult to guide the model to learn ideal robust features through these indirect regularization terms. To counter overfitting in gaze representation learning, we advocate for multimodal integration. Language, rich in semantics and knowledge, complements visual data. Each gaze label is described textually, ""the yaw/pitch degree angle of the person is {yaw/pitch}."". Leveraging visual-language models, such as CLIP, which excel in generalized latent space learning from vast image-text pairs, we align image features with a language space. This distillation of CLIP’s language expertise acts as regularization, enhancing generalization. However, these methods are only applicable to a limited set of discrete scalars and are not suitable for continuous tasks such as gaze estimation. Aligning gaze features to discrete text features can result in inaccurate feature alignment, which in turn affects the performance of continuous regression tasks. To address these issues, we propose a novel training method named LG-Gaze, which is designed to learn continuous and geometry-sensitive features for gaze estimation, as depicted in Figure 1(b). This framework aligns gaze features with continuous linguistic features extracted by a powerful language model, which not only prevents model overfitting but also enhances the generalization capabilities of gaze model. Primarily, unlike common discrete multimodal contrastive learning methods [22, 15], we introduce a loss function for continuous multimodal contrastive learning. Our Multimodal Contrastive Regression loss function (MCR) customizes adaptive weights for different negative samples according to label distance, facilitating more refined feature alignment in the feature space and benefiting the model in learning more continuous features. Next, to better adapt to the vectorial property of labels for gaze estimation task, we propose a geometry-aware interpolation method to obtain more precise gaze embeddings. The geometry-aware interpolation method combines spherical [26] and bilinear interpolation techniques to resolve the issue of creating semantic labels for gaze, ensuring accurate gaze prompts. Furthermore, MCR also resolves the limitations of previous contrastive learning functions by utilizing a more uniform distribution of global negative samples for contrast. Unlike most methods that are limited to intra-batch comparisons, our loss function benefits from extensive global contrast, resulting in a reasonable and robust feature distribution. The main contributions can be summarized as follows: • In this paper, we propose a novel training framework called LG-Gaze for gaze estimation. LG-Gaze guides the gaze model to learn generalized representations by leveraging textual features extracted from language models, resulting in more robust gaze representations. By incorporating rich language knowledge, this approach achieves robust gaze features, ultimately enhancing the cross-domain generalization capability of the gaze model. • To learn continuous feature for regression tasks, we propose a new multimodal contrastive learning loss function named MCR. The method employ adaptive weights for different negative sample, helping learn more refined features. Moreover, MCR leverages ample global negative samples for contrast, leading to a reasonable feature distribution. • We introduce a geometry-aware interpolation method that employs spherical interpolation techniques to compute accurate gaze embeddings, thereby ensuring the accuracy of semantic labels. • Experimental results and visualizations demonstrate that LG-Gaze not only achieves remarkable performance compared to the baseline but also surpasses state-of-the-art domain generalization methods for gaze estimation."
https://arxiv.org/html/2411.08603v1,Generalized Pose Embeddings for Training In-the-Wild via Analysis-by-Synthesis,"Modern pose estimation models are trained on large, manually-labelled datasets which are costly and may not cover the full extent of human poses and appearances in the real world. With advances in neural rendering, analysis-by-synthesis and the ability to not only predict, but also render the pose, is becoming an appealing framework, which could alleviate the need for large scale manual labelling efforts. While recent work have shown the feasibility of this approach, the predictions admit many flips due to a simplistic intermediate skeleton representation, resulting in low precision and inhibiting the acquisition of any downstream knowledge such as three-dimensional positioning. We solve this problem with a more expressive intermediate skeleton representation capable of capturing the semantics of the pose (left and right), which significantly reduces flips. To successfully train this new representation, we extend the analysis-by-synthesis framework with a training protocol based on synthetic data. We show that our representation results in less flips and more accurate predictions. Our approach outperforms previous models trained with analysis-by-synthesis on standard benchmarks.","Building datasets for pose estimation of humans and animals is costly and remains challenging in terms of capturing the full diversity in pose and appearance of the real world. Some works seek to utilize synthetic humans but remain at a gap from real world imagery. This inhibits the ability of labelled datasets—both synthetic and real—to generalize and motivates the need for a class of methods that can learn from un-labelled video data in-the-wild. Analysis-by-synthesis is the idea of including a depiction model in order to define a loss or cost back to the input image—enabling a training signal directly from unlabelled images. Such an approach, within a deep learning human pose estimation setting has been demonstrated in Jakab et al. (2020), where they use an intermediate pictorial skeleton representation of the pose. While their representation of a pose can be trained in an unsupervised way, it consists of only a single channel (\mathbb{R}^{1\times W\times H}) and does not represent the semantics of body parts—leading to significant flips between left and right, front and back in both the pose predictions and renderings of the person, as can be seen in Figure 3. We solve this problem with a new multi-channel pose representation. However, we found this higher capacity model sensitive to divergence during training. To address this problem, we introduce synthetic humans to pre-train our pose and rendering model, providing a better conditioning for further fine-tuning using analysis-by-synthesis in the wild. Another way to look at our approach is to consider analysis-by-synthesis a tool for bridging the reality gap of models trained with synthetic data. On the standard benchmark Human3.6M Ionescu et al. (2014), we outperform the prior work of Jakab Jakab et al. (2020) with an MSE of 10.39, compared to the MSE of 14.46 for the baseline. We also gathered specialized data of a target subject and measured whether fine-tuning on this data would improve accuracy at run-time. On the Human3.6M benchmark, this refinement step further reduces the MSE to 6.62, outperforming the baseline by a large margin. Additionally, we investigated our framework further to answer whether a 3D pose representation could be trained end-to-end. We also show that our approach can generalize to other skeletal structures such as animals, where we qualitatively improve the accuracy of the 3D pose predictions compared to the work of Borer Borer et al. (2021a)."
https://arxiv.org/html/2411.08592v1,Slender Object Scene Segmentation in Remote Sensing Image Based on Learnable Morphological Skeleton with Segment Anything Model,"Morphological methods play a crucial role in remote sensing image processing, due to their ability to capture and preserve small structural details. However, most of the existing deep learning models for semantic segmentation are based on the encoder-decoder architecture including U-net and Segment Anything Model (SAM), where the downsampling process tends to discard fine details. In this paper, we propose a new approach that integrates learnable morphological skeleton prior into deep neural networks using the variational method. To address the difficulty in backpropagation in neural networks caused by the non-differentiability presented in classical morphological operations, we provide a smooth representation of the morphological skeleton and design a variational segmentation model integrating morphological skeleton prior by employing operator splitting and dual methods. Then, we integrate this model into the network architecture of SAM, which is achieved by adding a token to mask decoder and modifying the final sigmoid layer, ensuring the final segmentation results preserve the skeleton structure as much as possible. Experimental results on remote sensing datasets, including buildings and roads, demonstrate that our method outperforms the original SAM on slender object segmentation and exhibits better generalization capability.","Semantic segmentation is a fundamental problem in remote sensing image analysis and other fields, aiming to separate objects of interest from the background at the pixel level. This technique has been widely utilized in various practical applications, including urban planning, damage detection, land cover classification, and resource management. At present, the mainstream image segmentation methods can be broadly classified into two categories: model-based methods and data-driven methods. The former typically regards the image segmentation as an optimization problem of energy minimization, such as the Potts model [1], the Snakes model [2] and Threshold Dynamic [3]. These energy functions generally consist of two primary components: a fidelity term that ensures the accuracy of segmentation and a regularization term that preserves essential spatial features within the segmentation result. However, the design of traditional variational models requires substantial expertise in related fields and often suffers from poor generalization. Data-driven methods can overcome these limitations by leveraging deep learning techniques. Some neural networks were proposed in last few years like Fully Convolutional Network (FCN)[4], U-net[5] and Vision-Transformer (ViT)[6], which significantly improved the flexibility and adaptability of image segmentation. Unlike natural images, segmentation in remote sensing images requires preserving local slender structures and overall topological properties, such as connectivity, holes, and branches, which makes this task particularly challenging. The morphological skeleton [7] is a simplified representation of the object’s structure that can accurately describe these topological features. Several studies have incorporated skeletons as prior knowledge in segmentation models. For instance, Hu et al. [8] utilized skeletons as the initial values for the Snakes model to guide road extraction. Additionally, Gaetano et al. [9] proposed a segmentation method based on the watershed transform [10], where skeletons were used as markers. Skeleton prior has also been integrated into data-driven models. Shit et al.[11] improved the skeleton extraction algorithm and designed a loss function that combines traditional segmentation loss with skeleton loss. Similarly, [12] proposed an connectivity-preserving loss based on skeleton for the detection of line-shaped objects. Unlike above methods, Liu et al. [13] designed a CNN to predict road surface, edges, and center lines, and correlated these results to enhance road extraction. However, all these methods lack reliable mathematical interpretability, and whether the subnetworks or loss functions can learn the desired features depends entirely on empirical evidence. In recent years, foundational models like GPT [14] and CLIP [15] have transformed the field of natural language processing. Recently, Meta AI introduced a visual foundational model called Segment Anything Model (SAM)[16]. After pre-training on the SA-1B dataset, SAM can segment any type of object using prompts inputs such as points, boxes, masks, or text. It demonstrates strong zero-shot generalization performance across multiple remote sensing datasets[17]. Currently, two main approaches exist for adapting SAM to downstream tasks: the first is to design one-shot or few-shot methods to enhance SAM’s performance by learning from a limited number of examples in new datasets [18, 19]. The second approach involves adding adaptors or prompters[20, 21] to SAM’s network architecture, tailoring outputs for specific tasks, but this lacks a rigorous mathematical foundation. Most segmentation networks utilize an encoder-decoder architecture to extract high-dimensional features. However, the upsampling and downsampling processes in this design often lead to the loss of fine details, limiting the network’s ability to segment small targets or structures. To address this issue, we propose integrating spatial priors, such as morphological skeletons, into the neural network to enhance segmentation accuracy and preserve overall geometric structure. There are primarily three strategies to achieve this. The first strategy is to post-process the neural network outputs using traditional models. For instance, Zhang et al.[22] utilized Conditional Random Field (CRF) to extract spatial neighbourhood information of buildings, which was fused with the network’s output to ensure clear segmentation boundaries. However, this approach does not correct inherent errors in the neural network’s output. The second strategy focuses on constructing a loss function that incorporates spatial priors to guide the model’s learning process. For example, Ma et al. [23] introduced a object consistency loss and a boundary preservation loss to fine-tune SAM, in order to avoid excessive fragmentation of segmentation results and inaccurate boundaries. However, these models may struggle when input data significantly deviates from the training data, as they cannot use learned priors knowledge during prediction. The third approach involves unrolling the variational segmentation algorithm to design network layers. By designing regularization and penalty terms, prior constraints can be integrated into the variational segmentation model. The unrolling technique combines the advantages of both model-based and data-driven methods, enabling the network to automatically minimize the energy functional during forward propagation. Monga et al. [24] described this technique in deep learning, and numerous studies have explored its potential [25, 26, 27, 28]. In contrast to unrolling methods, some engineering studies, such as [29, 30], proposed the use of subnetworks to learn geometric priors, which are then applied during the decoding process. However, these methods often lack the same level of mathematical interpretability provided by variational models. In this study, we aim to develop a mathematically interpretable segmentation model that preserves morphological skeleton priors. Unlike existing methods that typically incorporate skeleton and other structural information within the loss function, such as the cl-dice in [11], we construct an energy functional that maintains the skeleton prior as a regularization term. This functional is integrated into the Threshold Dynamic (TD) segmentation model [31, 28]. By employing operator splitting and Fenchel-Legendre duality, we transform the model into three sub-problems, for which we provide both the solution form and an iterative solving algorithm. Furthermore, we unroll the variational model into a novel network module, which we refer to as the Morphological Skeleton Prior (MorSP) module. This module can be seamlessly integrated into any neural network framework, endowing the network with the ability to preserve learnable skeleton priors during the decoding process. In this paper, we demonstrate the effectiveness of our approach by incorporating the MorSP module to SAM, showcasing improvements in segmentation accuracy and detail preservation for building and road extraction of remote sensing images. The main contributions of this paper are as follows: • We provide a smooth approximation of morphological operators, offering a differentiable representation of the morphological skeleton and its variation. This allows the integration of non-differentiable morphological operations into the backpropagation process within neural networks. • We design a variational segmentation model based on smooth morphological skeletons and Threshold Dynamics, which can be efficiently solved using the Douglas-Rachford splitting method. This facilitates the incorporation of learnable morphological skeleton priors into any neural network architecture. • We propose a novel approach to integrate the morphological skeleton prior into the Segment Anything Model (SAM). This fusion method is mathematically interpretable and can automatically learn and preserve the skeleton prior during both training and prediction across various datasets. Additionally, we demonstrate its advantages in building and road extraction tasks through ablation experiments. The paper is organized as follows: Section II reviews related works, including variational segmentation models, classical morphological operators, and the Segment Anything Model. Section III presents the proposed segmentation model that preserves morphological skeleton priors and introduces the MorSP module, detailing its integration into SAM. Section IV evaluates the performance of MorSP Module on remote sensing images. Section V presents numerical experiment results and analysis. Finally, we summarize and discuss this work."
https://arxiv.org/html/2411.08579v1,NavAgent: Multi-scale Urban Street View Fusion For UAV Embodied Vision-and-Language Navigation,"Vision-and-Language Navigation (VLN), as a widely discussed research direction in embodied intelligence, aims to enable embodied agents to navigate in complicated visual environments through natural language commands. Most existing VLN methods focus on indoor ground robot scenarios. However, when applied to UAV VLN in outdoor urban scenes, it faces two significant challenges. First, urban scenes contain numerous objects, which makes it challenging to match fine-grained landmarks in images with complex textual descriptions of these landmarks. Second, overall environmental information encompasses multiple modal dimensions, and the diversity of representations significantly increases the complexity of the encoding process. To address these challenges, we propose NavAgent, the first urban UAV embodied navigation model driven by a large Vision-Language Model. NavAgent undertakes navigation tasks by synthesizing multi-scale environmental information, including topological maps (global), panoramas (medium), and fine-grained landmarks (local). Specifically, we utilize GLIP to build a visual recognizer for landmark capable of identifying and linguisticizing fine-grained landmarks. Subsequently, we develop dynamically growing scene topology map that integrate environmental information and employ Graph Convolutional Networks to encode global environmental data. In addition, to train the visual recognizer for landmark, we develop NavAgent-Landmark2K, the first fine-grained landmark dataset for real urban street scenes. In experiments conducted on the Touchdown and Map2seq datasets, NavAgent outperforms strong baseline models. The code and dataset will be released to the community to facilitate the exploration and development of outdoor VLN.","UAV Vision-and-Language Navigation (VLN) is a specialization of embodied intelligence for navigation applications in the aerial domains[1, 2, 3, 4, 5, 6, 7]. It aims to explore how to enable UAV embodied agents to navigate in unknown urban environments based on natural language commands and environmental observations. This approach has a wide range of applications across various fields, including inspection and monitoring, search and rescue, and low-altitude logistics[8, 9, 10]. However, existing research on VLN primarily focuses on indoor scenes[11, 12, 13, 14, 15]. In contrast, the outdoor urban environments targeted by UAV VLN tasks involve a much larger spatial scale, greater complexity, and sparser landmarks, making them more challenging[16, 17, 18, 19, 20]. Recent studies have demonstrated that large Vision-Language Models, which are capable of processing multimodal inputs, exhibit strong generalization abilities and outstanding performance in embodied tasks[21, 22, 23, 24, 25, 26, 27, 28]. Given this, our objective is to develop the first embodied large Vision-Language Model specifically designed for UAV VLN tasks, enabling UAV agents to navigate autonomously in urban street scenes. However, there are two primary challenges in this process. (1) Difficulty in Matching Fine-Grained Landmarks in Panoramic Observation Images. When the agent is positioned at any observation point, it perceives the surrounding environment through a panoramic image captured at that location. The landmarks that need to be recognized are typically fine-grained targets located on both sides of the road, which comprise less than 5\% of the pixels in the panoramic image. Furthermore, the texts associated with these landmarks are often not simple nouns but rather complex phrases that include multiple modifiers, such as “a green mailbox” or “two red garbage cans”. As a result, ordinary image encoders struggle to accurately match these intricate details. (2) Difficulty in Encoding Overall Environmental Information in the Decision-Making Process. The environments in which the agents operate are complex, requiring the integration of various dimensions of overall environmental information. This includes visual data (e.g., observation images), semantic information (e.g., landmark categories and locations), and geographic data (e.g., environmental map). Not only do these data types have different representations, but they also exhibit a high degree of heterogeneity in both space and time, which complicates the encoding process. Furthermore, the dynamic nature of the environmental information necessitates real-time updates as the agent moves, significantly increasing the challenges associated with coding. To address the aforementioned challenges, we propose a multi-scale environment fusion-enhanced VLN model called NavAgent. As illustrated in Figure 1, this model integrates the global environmental topology map, the panoramic view of the current observation position, and local fine-grained landmark data. This integration facilitates accurate and stable VLN for UAV agents, specifically: Figure 1: Schematic diagram of the VLN model augmented by multi-scale environment fusion, with the environment topology map containing the overall information of the environment in the yellow box, the observation image of the agent at this point in the red box, the fine-grained landmarks extracted from the observation image in the green box, and the navigation text in the black box . Prior research has demonstrated the effectiveness of GLIP in fine-grained target recognition and matching tasks within a general-purpose domain[23]. We modify the structure of GLIP to develop the visual recognizer for landmark. The visual recognizer facilitates fine-grained matching between the images of the environment observed by the agent during navigation and the text of landmarks, enabling precise identification of landmarks present in the observation images. To train the visual recognizer for landmark, we first frame the landmarks within the street images selected from Google Street View[29]. We then use BLIP2[21] to generate descriptions for the images of the landmarks, ultimately creating a dataset with 2,000 landmark annotations. This dataset, named NavAgent-Landmark2K, is the first landmark recognition dataset designed for outdoor VLN. Comparative experiments have shown that the visual recognizer for landmark fine-tuned with this dataset improves accuracy by 9.5\% compared to the GLIP in recognizing landmark images within the context of outdoor VLN. Further, we develop a dynamically evolving scene topology map to integrate environmental information and design the topology map encoder to capture global environmental features. Specifically, we record navigable positions in the urban scene as nodes, initially capturing each node’s position and the orientation relationships between nodes. We then explore the current node and its contiguous nodes, combining them into a cohesive scene topology map. To integrate environmental information, we employ an image encoder to extract visual features from the current observation image. We then utilize a cross-attention mechanism to incorporate these visual features into the scene topology map, facilitating the fusion of multimodal information. After the UAV agent moves, the scene topology map retains historical information and updates the nodes to integrate new environmental data, effectively encoding dynamic and complex environments. In summary, our main contributions are: (1) We propose NavAgent, the first urban UAV embodied navigation model driven by a large Vision-Language Model, enabling autonomous navigation of agent in urban environments through the fusion of multi-scale environmental information. (2) We design and train the visual recognizer for landmark that recognizes fine-grained landmarks by calculating similarity scores by matching region features extracted from observed images with text features extracted from landmark descriptions. Experimental results indicate that the visual recognizer of landmark enhances accuracy by 9.5% in the fine-grained landmark recognition task when compared to the GLIP. (3) We construct a dynamically growing scene topology map and employ the topology map encoder to encode individual nodes and their spatial relationships, thereby enhancing the planning ability of agent for long-distance navigation. (4) We develop the first fine-grained landmark dataset for real urban street scenes, named NavAgent-Landmark2K. This dataset comprises 2,000 image-text pairs, where the images represent fine-grained landmarks occupying approximately 5\% of the pixel area, and the accompanying text consists of landmark phrases that include multiple modifiers. (5) In the experiments conducted on the Touchdown and Map2seq datasets, our proposed NavAgent outperforms the powerful baseline models, achieving improvements of 4.6\% and 2.2\% compared to VELMA on the development sets of two datasets."
https://arxiv.org/html/2411.08569v1,UIFormer: A Unified Transformer-based Framework for Incremental Few-Shot Object Detection and Instance Segmentation,"This paper introduces a novel framework for unified incremental few-shot object detection (iFSOD) and instance segmentation (iFSIS) using the Transformer architecture. Our goal is to create an optimal solution for situations where only a few examples of novel object classes are available, with no access to training data for base or old classes, while maintaining high performance across both base and novel classes. To achieve this, We extend Mask-DINO into a two-stage incremental learning framework. Stage 1 focuses on optimizing the model using the base dataset, while Stage 2 involves fine-tuning the model on novel classes. Besides, we incorporate a classifier selection strategy that assigns appropriate classifiers to the encoder and decoder according to their distinct functions. Empirical evidence indicates that this approach effectively mitigates the over-fitting on novel classes learning. Furthermore, we implement knowledge distillation to prevent catastrophic forgetting of base classes. Comprehensive evaluations on the COCO and LVIS datasets for both iFSIS and iFSOD tasks demonstrate that our method significantly outperforms state-of-the-art approaches.","In the field of computer vision, incremental few-shot object detection (iFSOD) and instance segmentation (iFSIS) stand as critical tasks. iFSOD focuses on detecting novel object classes incrementally with only a few examples, while simultaneously preserving knowledge of previously learned classes without needing to re-access data from these base classes. iFSIS extends this challenge to a more granular level by assigning categorical labels at the pixel level, enhancing the model’s capability for fine-grained segmentation. So far, significant advancements have been made in both iFSOD and iFSIS. The ONCE framework [1] was the first to address the iFSOD problem, but its basic learning strategy struggles to retain knowledge of base classes when novel classes are introduced. To combat catastrophic forgetting, methods such as LEAST [2] and Incremental-DETR [3] have employed transfer learning and knowledge distillation, respectively, to safeguard the knowledge of base classes. Sylph [4] further mitigates knowledge forgetting by using a base detector with class-agnostic localization. For the more challenging iFSIS tasks, iMTFA [5] calculates the class embedding averages using cosine similarity, allowing the model to extend to novel classes without additional training. Recently, a unified framework, iFS-RCNN [6], has been developed to bridge these tasks by adapting the general Mask R-CNN model. This model introduces a probit-based object classifier and an uncertainty-guided bounding box predictor, delivering robust performance in both iFSOD and iFSIS. Despite the clear distinctions between region-level and pixel-level tasks, the inherent adaptability of Mask R-CNN supports seamless task integration. However, CNN-based models generally lag behind transformer-based models in handling complex semantics, particularly in large-scale datasets. This gap underscores the pressing need for transformer-based solutions for iFSOD and iFSIS in data-intensive scenarios. 1.0.1 Motivation Currently, transformer-based models for incremental object detection and instance segmentation remain disjointed. This lack of integration restricts data sharing and cooperative learning opportunities across the tasks. Mask-DINO [7] , a transformer-based extension of DINO [8], illustrates the potential for unifying region-level and pixel-level feature representations, suggesting that detection and segmentation can mutually enhance each other within a single transformer-based architecture. Nonetheless, a significant limitation observed in Mask-DINO is the uniform classifier across its encoder and decoder components, which our empirical studies suggest degrades performance in both iFSOD and iFSIS. This observation leads us to pose two pivotal questions: • Is it feasible to unify detection and instance segmentation using a Transformer architecture in an incremental setting? • How can we effectively assign appropriate classifiers to the encoder and decoder to reflect their distinct functionalities? 1.0.2 Our Method To address these inquiries, we introduce UIFormer, a novel unified transformer-based framework designed to support both iFSOD and iFSIS. Our approach employs a dual-phase learning strategy, initially dividing the learning of base classes into pre-training and fine-tuning phases to solidify foundational knowledge. During fine-tuning, we incorporate an attention-driven pseudo ground-truth search technique to detect and label previously unidentified objects, effectively expanding the dataset’s class diversity. Additionally, to combat the loss of base class knowledge, we integrate knowledge distillation into the fine-tuning process for novel classes. Simultaneously, we propose a new class-agnostic foreground predictor for the encoder, which focuses solely on distinguishing between foreground and background, enhancing token selection for subsequent processing by the decoder, which employs a cosine-similarity classifier for precise semantic discrimination. 1.0.3 Contributions To sum up, our contributions are four-folds as follows: • We are the first to explore the extension of transformer-based models for learning iFSOD and iFSIS tasks in a unified manner. This goal is achieved through the introduction of UIFormer, a transformer-based framework that unifies object detection and instance segmentation. • We propose an classifier selection strategy to assign different classifiers to encoder and decoder based on their distinct functions. In addition, a knowledge distillation method is used in novel fine-tuning stage. Empirical evidence demonstrates that our approach effectively mitigates over-fitting and catastrophic forgetting. • We conduct extensive experiments on the commonly used COCO and LIVS datasets. The results demonstrate that the proposed method outperforms the state-of-the-arts approaches by a significant margin on both iFSOD and iFSIS task."
https://arxiv.org/html/2411.08567v1,Saliency Map-based Image Retrieval using Invariant Krawtchouk Moments,"With the widespread adoption of digital devices equipped with cameras and the rapid development of Internet technology, numerous content-based image retrieval systems and novel image feature extraction techniques have emerged in recent years. This paper introduces a saliency map-based image retrieval approach using invariant Krawtchouk moments (SM-IKM) to enhance retrieval speed and accuracy. The proposed method applies a global contrast-based salient region detection algorithm to create a saliency map that effectively isolates the foreground from the background. It then combines multiple orders of invariant Krawtchouk moments (IKM) with local binary patterns (LBPs) and color histograms to comprehensively represent the foreground and background. Additionally, it incorporates LBPs derived from the saliency map to improve discriminative power, facilitating more precise image differentiation. A bag-of-visual-words (BoVW) model is employed to generate a codebook for classification and discrimination. By using compact IKMs in the BoVW framework and integrating a range of region-based features—including color histograms, LBPs, and saliency map-enhanced LBPs—our proposed SM-IKM achieves efficient and accurate image retrieval. Extensive experiments on publicly available datasets, such as Caltech 101 and Wang, demonstrate that SM-IKM outperforms recent state-of-the-art retrieval methods. The source code for SM-IKM is available at github.com/arnejad/SMIKM.","In the past decades, there has been a need for finding the most semantically similar samples to a query image in many applications, including medicine [1], copyright protection [2] and e-commerce [3]. Since the images cannot necessarily be found by describing them in words, it is essential to design a procedure to compare and find the most similar samples to a query based on the content. This has led to the development of content-based image retrieval (CBIR) systems [4], which consider various properties such as color, texture, shape, and structure [5] to semantically compare the content of images. However, semantic analysis of images makes CBIR challenging. In addition, different images of the same object are prone to transformations (e.g., translation, rotation, and scaling). As a result, a CBIR system needs to maintain retrieval accuracy when images in the dataset are captured with various transformations [4]. Various techniques have been applied for the improvement of CBIR systems. One of the first methods is bag-of-visual-words (BoVW) [6] ], which is derived from the bag-of-words [7] concept used in natural language processing. Researchers employ the original BoVW method and its variants to generate a codebook for classification and discrimination. They further utilize the codebook together with a feature descriptor such as SIFT [8] or SURF [9] to achieve higher retrieval accuracy [10, 11, 12]. Many studies have been conducted to propose algorithms that extract robust and discriminative features. For example, moments are constructed based on continuous or discrete polynomials to resolve image transformation issues. Zernik and Legendre moments are examples of continuous orthogonal moments. They are valid in the [-1, 1] range and therefore suffer from the errors resulting from the mandatory transformation and quantization processes [13]. On the other hand, the discrete orthogonal moments like Tchebichef and Krawtchouk do not carry the digitization, and coordinate space transformation errors since the basis sets are orthogonal in the discrete domain [13]. Unlike Tchebichef moments, Krawtchouk moments can work as local descriptors [14]. To further increase discrimination among images, researchers use the fusion technique to combine multiple features [15, 16, 17, 18], where each feature provides information from different aspects, including illumination, local pattern, and color. For example, Singh et al. [19] propose an LBPC+LBPH+CH framework that acquires a fusion of local binary pattern for color images (LBPC), the local binary pattern of the hue (LBPH), and the color histogram (CH) features. The final fused features then take part in similarity measurement to compute the closest images to the query. To address the challenge of describing local texture patterns in color images, Dubey et al. [20] propose adder- and decoder-based schemas called MDLBP to combine the LBPs from more than one channel. Since not all areas in the image contribute equally to the recognition process, researchers have recently used saliency maps to take regional importance into account. The saliency map is a matrix of the same size as the input image, where each cell in the map indicates its prominence in the image, and the main objects correspond to the pixels with high prominence. Zhang et al. [21] propose an extended salient region (ESR) method to obtain the salient region with a surrounding safety margin. They then extract texture, SIFT, and HSV histogram features from each ESR to form a codebook to compare the content of the images. One recent CBIR method, which combines BoVW and salient region detection, is the saliency-based multi-feature modeling (SMFM) [22] method. It obtains foreground and background areas in images via the saliency map and then extracts fused features, including SIFT, LBP, and color histograms for both foreground and background regions. Integrating saliency maps in the image retrieval process makes retrieval results semantically closer to human perception. In this paper, we propose a CBIR framework named saliency map-based image retrieval using invariant Krawtchouk moments (SM-IKM), which uses salient regions to customize the extent of contribution for each segment of an image. SM-IKM first separates the foreground and background regions of each image using the global contrast-based salient region detection method [23]. It then exploits invariant Krawtchouk moments (IKMs) to gain transformation robustness and more robustly describe the image regions (or patches). The IKMs are further employed to build the BoVW model and fused with texture and color features to improve the semantic discriminative capability of each region and therefore achieve retrieval results semantically closer to human perception. Our extensive experiments show that SM-IKM outperforms state-of-the-art peer CBIR methods on two publicly available Caltech-101 and Wang datasets. Unlike the SMFM method [22], which combines salient region detection and BoVW involving SIFT, LBP, and CH, the proposed SM-IKM employs IKM-based BoVW and fused features of IKMs, LBP of one color channel and the saliency map, and CH to capture the semantic meaning of each segmented region. The main contributions of this paper are as follows: 1. Efficiently gaining more discriminative power by incorporating LBPs of the saliency map. 2. Effectively gaining robustness against transformation, noise, and discretization error by employing IKMs. 3. Efficiently reducing the BoVW construction time by producing compact discriminative features (e.g., IKMs). 4. Balancing the runtime and memory complexity by providing the flexibility of specifying the length of the feature descriptors (i.e., the order of IKMs). The rest of this paper is organized as follows: Section 2 presents the related work which is the foundation of the proposed SM-IKM method. Section 3 explains the SM-IKM framework in detail. Section 4 summarizes the experimental results and compares the performance of SM-IKM with several state-of-the-art peer CBIR methods. Section 5 concludes and presents directions for future work."
https://arxiv.org/html/2411.08545v1,"APDDv2: Aesthetics of Paintings and Drawings 
Dataset with Artist Labeled Scores and Comments","Datasets play a pivotal role in training visual models, facilitating the development of abstract understandings of visual features through diverse image samples and multidimensional attributes. However, in the realm of aesthetic evaluation of artistic images, datasets remain relatively scarce. Existing painting datasets are often characterized by limited scoring dimensions and insufficient annotations, thereby constraining the advancement and application of automatic aesthetic evaluation methods in the domain of painting. To bridge this gap, we introduce the Aesthetics Paintings and Drawings Dataset (APDD), the first comprehensive collection of paintings encompassing 24 distinct artistic categories and 10 aesthetic attributes. Building upon the initial release of APDDv1(Jin et al., 2024), our ongoing research has identified opportunities for enhancement in data scale and annotation precision. Consequently, APDDv2 boasts an expanded image corpus and improved annotation quality, featuring detailed language comments to better cater to the needs of both researchers and practitioners seeking high-quality painting datasets. Furthermore, we present an updated version of the Art Assessment Network for Specific Painting Styles, denoted as ArtCLIP. Experimental validation demonstrates the superior performance of this revised model in the realm of aesthetic evaluation, surpassing its predecessor in accuracy and efficacy. The dataset and model are available at https://github.com/BestiVictory/APDDv2.git.","The IAQA (Image Aesthetic Quality Assessment) task aims to automatically evaluate the aesthetic quality of images through computer vision techniques. IAQA can be summarized into five layers of tasks as illustrated in the Figure 2 (Jin et al., 2018). In the IAQA task, the preponderance of datasets is concentrated within the realm of photography. If the APDD dataset is not considered, across the initial three phases of the IAQA task, the quantity of aesthetic image datasets within the photography domain totals a minimum of 685,000 images, contrasting sharply with the scanty 60,000 images found in the painting domain. Within the Aesthetic Attributes phase, datasets sourced from the photography domain comprise no fewer than 31,000 images, while their painting counterparts consist of a mere 4,248 images. Moving to the Aesthetic Captions phase, datasets originating from the photography domain encompass no less than 198,000 images, whereas there are no publicly available datasets specifically tailored for the painting domain. Figure 2: The five-layered tasks of IAQA exhibit an overall inverted triangular distribution in terms of corresponding data volume: as the hierarchy ascends, the data volume decreases, accompanied by a decrease in annotation quality. Constructing high-quality benchmark datasets for paintings is crucial for advancing computer vision aesthetics in art, providing a framework for analyzing and understanding artistic imagery. Unlike the uniform realm of photography, the diversity and intricacies of artistic expression significantly heighten the challenge of dataset construction, requiring meticulous consideration of variations in styles, themes, and techniques. The subjective nature of aesthetic evaluations in art necessitates accounting for the artist’s intent and audience perception, demanding professional expertise. In collaboration with experts and educators, we established a robust system for assessing aesthetic components, categorizing paintings into 24 art categories and 10 aesthetic attributes, with defined scoring standards. This effort resulted in APDDv1(Jin et al., 2024), comprising 4,985 images and over 31,100 annotations from 28 art experts and 24 art students, with each image evaluated by at least six annotators, covering both overall aesthetic scores and specific attribute scores. To further enhance the dataset’s quality and richness, we assembled a more specialized labeling team and introduced aesthetic language comments, leading to the release of APDDv2. This updated version expands the image count to 10,023 and the number of annotations exceeds 90,000, including detailed aesthetic comments. Building on the foundation of APDDv1, we developed more detailed and applicable scoring standards for artistic images. These enhancements aim to create a more comprehensive, rich, and high-quality dataset for the aesthetics of paintings and sketches, thereby providing a more reliable foundation for the analysis and research of artistic images. The primary goal of constructing our dataset is to train aesthetic models. To enhance the CLIP image encoder (Radford et al., 2021) for image aesthetics, we employed a fine-tuning approach tailored for this domain. Directly applying the original CLIP to Image Aesthetic Analysis (IAA) yields suboptimal results due to the generic nature of datasets like COCO(Lin et al., 2014) and Flickr30K(Plummer et al., 2015), which lack the distinct features influencing aesthetic perception. To address this, AesCLIP (Sheng et al., 2023) uses contrastive learning to capture representations sensitive to aesthetic attributes, bridging the gap between general and aesthetic image domains. Drawing inspiration from the AesCLIP approach, we trained ArtCLIP on the APDDv2 dataset. Experimental results show that ArtCLIP surpasses state-of-the-art techniques, enhancing both image aesthetic analysis and related research tools. Overall, our contributions include: We developed a comprehensive set of evaluation criteria specifically tailored for artistic images, effectively assessing their aesthetic components. Furthermore, we provided a practical and scalable evaluation methodology for the continuous expansion of the dataset. We introduced the APDDv2 dataset (the license of the dataset: CC BY 4.0), created with the participation of over 40 experts from the painting domain, comprising 10,023 images, 85,191 scoring annotations, and 6,249 language comments. We developed ArtCLIP, an art assessment network for specific painting styles, utilizing a multi-attribute contrastive learning framework. Experimental validation demonstrated that the updated model outperforms existing techniques in aesthetic evaluation."
https://arxiv.org/html/2411.08537v1,MLV-Net: Rater-Based ajority-abel oting for Consistent eningeal ymphatic essel Segmentation,"Meningeal lymphatic vessels (MLVs) are responsible for the drainage of waste products from the human brain. An impairment in their functionality has been associated with aging as well as brain disorders like multiple sclerosis and Alzheimer’s disease. However, MLVs have only recently been described for the first time in magnetic resonance imaging (MRI), and their ramified structure renders manual segmentation particularly difficult. Further, as there is no consistent notion of their appearance, human-annotated MLV structures contain a high inter-rater variability that most automatic segmentation methods cannot take into account. In this work, we propose a new rater-aware training scheme for the popular nnU-Net model, and we explore rater-based ensembling strategies for accurate and consistent segmentation of MLVs. This enables us to boost nnU-Net’s performance while obtaining explicit predictions in different annotation styles and a rater-based uncertainty estimation. Our final model, MLV2-Net, achieves a Dice similarity coefficient of 0.806 with respect to the human reference standard. The model further matches the human inter-rater reliability and replicates age-related associations with MLV volume.","The lymphatic system — part of the immune system and responsible for the drainage of waste products — stretches across the entire human body and can often be found alongside blood vessels of the circulatory system. In the brain, the glymphatic system (Iliff et al., 2012) takes a similar role in that it clears waste products. To this end, meningeal lymphatic vessels (MLVs), located alongside the dural venous sinuses, transfer interstitial fluids and macromolecules to deep cervical lymph nodes (Louveau et al., 2015). An impairment in the MLVs’ functionality, potentially coupled with morphological changes such as thickening, has been linked to aging (Albayram et al., 2022) as well as to clinical conditions like Alzheimer’s (Goodman et al., 2018), multiple sclerosis (Louveau et al., 2018), and Parkinson’s disease (Ding et al., 2021). Yet, MLVs have only recently been described in 3D FLAIR MRI (Albayram et al., 2022), and their segmentation has only been done manually so far. However, manual annotation of MLVs is difficult and time-consuming due to their ramified structure, cf. Figure 1. Moreover, the training of automatic segmentation models on expert-annotated data is challenging due to the high inter-rater variability. Related work Deep neural networks for medical image segmentation are commonly trained to remove this variablility (Guo et al., 2024; Hatamizadeh et al., 2022; Ronneberger et al., 2015). However, this approach does not model the reality where disagreement about the true contours of a structure often exists (Warfield et al., 2004). This issue is especially problematic for newly discovered structures, such as MLVs, which bear enormous potential for innovative findings but for which a common notion of their appearance does not (yet) exist. Notably, a few dedicated methods for rater-aware segmentation were developed (Kohl et al., 2018; Mirikharaji et al., 2021; Warfield et al., 2004; Zhang et al., 2023). These approaches yielded effective results for certain standard applications, e.g., skin lesion (Mirikharaji et al., 2021) or brain tumor segmentation (Zhang et al., 2023), but transferring them to new tasks is difficult due to the large number of hyperparameters involved. These choices are non-trivial, not reproducible, and subject to the developer’s experience and preferences (Isensee et al., 2021). At the same time, the best segmentation results are typically obtained with nnU-Net (Isensee et al., 2024), which provides a versatile framework for hyperparameter selection. Unfortunately, nnU-Net cannot model the variability in segmentations provided by different raters — a functionality essential for trustworthy and comprehensible clinical predictions. We close this gap and develop a rater-based ensembling strategy for nnU-Net that keeps its architecture intact and augments it with the ability to replicate individual raters’ annotation styles. Contribution We present the first automatic method for segmentation of MLVs from 3D FLAIR MRI. To achieve accurate and reliable segmentation of the ramified structure, we made the following technical contributions. First, we developed an innovative rater-aware training scheme for the popular nnU-Net model that takes into account the different raters involved in the creation of the training set. This enables nnUNet to learn individual raters’ segmentation styles and to explicitly predict a set of plausible segmentations. In a second step, we aggregate the predictions with a weighted majority-label voting scheme for best segmentation accuracy. In addition, we obtain a rater-based uncertainty prediction from the model. Finally, since the volume of MLVs is usually of utmost importance for downstream analyses, we derive error boundaries of the model’s predicted volumes with respect to the ground-truth volume. Table 1: Composition of the annotated and raw datasets used in this study. Name Used for Joint annot. #Annotations #Images by all raters per image Training set Training & validation ✗ 1 27 IRR set Testing (inter-rater reliability) ✗ 4 2 Consensus set Testing (accuracy) ✓ 1 4 Raw set Testing (downstream analysis) N/A N/A 22"
https://arxiv.org/html/2411.08531v1,Classification and Morphological Analysis of DLBCL Subtypes in H&E-Stained Slides,"We address the challenge of automated classification of diffuse large B-cell lymphoma (DLBCL) into its two primary subtypes: activated B-cell-like (ABC) and germinal center B-cell-like (GCB). Accurate classification between these subtypes is essential for determining the appropriate therapeutic strategy, given their distinct molecular profiles and treatment responses. Our proposed deep learning model demonstrates robust performance, achieving an average area under the curve (AUC) of (87.4 ± 5.7)% during cross-validation. It shows a high positive predictive value (PPV), highlighting its potential for clinical application, such as triaging for molecular testing. To gain biological insights, we performed an analysis of morphological features of ABC and GCB subtypes. We segmented cell nuclei using a pre-trained deep neural network and compared the statistics of geometric and color features for ABC and GCB. We found that the distributions of these features were not very different for the two subtypes, which suggests that the visual differences between them are more subtle. These results underscore the potential of our method to assist in more precise subtype classification and can contribute to improved treatment management and outcomes for patients of DLBCL.","Diffuse large B-cell lymphoma (DLBCL) comprises a collection of aggressive B-cell non-Hodgkin lymphomas (NHL), characterized by significant genetic diversity and a wide range of clinical manifestations [1]. It is the most common subtype, accounting for 30-40% of all B-cell NHL cases. The median age at diagnosis is seventh decade and patients present with widespread lymph-adenopathy [2]. More than 20 years ago, gene expression profiling (GEP) identified two distinct molecular subtypes, classified according to cell of origin (COO), the germinal centre B-cell-like (GCB) subtype and the non-GCB subtype, which includes activated B-cell-like (ABC) and primary mediastinal B-cell subtypes. The COO classification categorizes subtypes that originate from B-cells at various stages of development, each characterized by unique oncogenic mechanisms, dependence on distinct survival pathways, and varying clinical outcomes [1]. These DLBCL subtypes exhibit significant differences in prognosis, with ABC subtype linked to poorer outcomes [3]. DLBCL is a potentially curable disease. Standard first-line immunochemotherapy, which typically includes rituximab along with cyclophosphamide, doxorubicin, vincristine, and prednisone (R-CHOP), produces remission rates of 60-70%. It was seen that patients with ABC subtype treated with the R-CHOP regimen tend to have worse outcomes compared to those with the GCB subtype, with a 5-year progression-free survival (PFS) of 48% versus 73% . With the advent of novel therapies i.e. immunomodulatory agents, like ibrutinib or lenalidomide, The response rate in ABC subtype has improved to 95% [3]. GEP by cDNA microarray is the gold standard for identifying the molecular subtypes of DLBCL. However, due to ease of availability and cost, traditionally, immunohistochemistry is used for classification. The Hans algorithm, uses antibodies for CD10, BCL6, and IRF4/MUM1. However the identified subgroups do not perfectly align with molecular categories [4]. Figure 1: Snapshot of H&E stained images from TMC Dataset: Top row are images of GCB subtype and bottom row, ABC subtypes of DLBCL respectively Figure 2: Patches are generated from H&E stained WSI with optimal preprocessing and fed to encoder (pretrained) to get the embedding. Figure 3: Schematic of the attention-based classification model. The features are processed through an attention block (NN), where importance weights are applied to each feature. The weighted features are then aggregated and passed to a classifier. To tackle this classification challenge between activated B-cell-like (ABC) and germinal center B-cell-like (GCB) subtypes, we implemented a weakly supervised technique such as the customized version [5] of CLAM [6] (Clustering-constrained Attention Multiple-instance learning) and MIL [7](Multiple Instance Learning) model, which is particularly effective for analyzing whole slide images. We experimented with a variety of feature extractors, including ResNet34 [8], ResNet50 [8], RegNet [9], ConvNeXT_Tiny [10], EfficientNet [11], and Swin_Tiny [12], each chosen for their ability to capture different levels of histological detail. In conjunction with these feature extractors, we applied a range of histology-specific augmentations designed to simulate realistic variations in the images, such as alterations in staining intensity, rotation, and scaling. These augmentations helped to enhance the robustness and generalizability of the model. By leveraging the CLAM model alongside these advanced feature extractors and augmentation strategies, we aimed to significantly improve the accuracy and reliability of subtype classification. To substantiate our claims and rigorously evaluate the proposed methodology, we employed the TMC Dataset, a proprietary collection comprising 115 whole slide images (WSIs). Each image in this dataset has been meticulously annotated and classified, providing a robust foundation for validating our approach. The TMC Dataset offers a diverse range of histological patterns, representative of the challenges in accurately classifying subtypes such as activated B-cell-like (ABC) and germinal center B-cell-like (GCB) lymphomas. To provide a visual context, Fig. 1 displays the thumbnails of the WSIs included in the dataset, showcasing the complex and varied histopathological features that our model must process and classify. The use of these thumbnails illustrates the dataset’s complexity, emphasizing the technical rigor required in our model’s analysis and the critical importance of accurate subtype differentiation in this challenging domain."
https://arxiv.org/html/2411.08530v1,Efficient Whole Slide Image Classification through Fisher Vector Representation,"The advancement of digital pathology, particularly through computational analysis of whole slide images (WSI), is poised to significantly enhance diagnostic precision and efficiency. However, the large size and complexity of WSIs make it difficult to analyze and classify them using computers. This study introduces a novel method for WSI classification by automating the identification and examination of the most informative patches, thus eliminating the need to process the entire slide. Our method involves two-stages: firstly, it extracts only a few patches from the WSIs based on their pathological significance; and secondly, it employs Fisher vectors (FVs) for representing features extracted from these patches, which is known for its robustness in capturing fine-grained details. This approach not only accentuates key pathological features within the WSI representation but also significantly reduces computational overhead, thus making the process more efficient and scalable. We have rigorously evaluated the proposed method across multiple datasets to benchmark its performance against comprehensive WSI analysis and contemporary weakly-supervised learning methodologies. The empirical results indicate that our focused analysis of select patches, combined with Fisher vector representation, not only aligns with, but at times surpasses, the classification accuracy of standard practices. Moreover, this strategy notably diminishes computational load and resource expenditure, thereby establishing an efficient and precise framework for WSI analysis in the realm of digital pathology.","In recent years, the field of digital pathology has experienced a significant transformation, predominantly driven by advancements in high-throughput whole slide imaging (WSI) technology. WSIs, essentially high-resolution digital scans of tissue slides, have become integral in pathology for diagnostic, educational, and research purposes. However, the comprehensive analysis of these large and complex images presents considerable computational challenges. The traditional approach of analyzing entire slides is not only resource-intensive but also inefficient, given that not all regions in a WSI are equally informative for diagnosis. Recognizing these challenges, our research focuses on a novel approach to WSI classification. We propose a method that selectively identifies and analyzes only the most pertinent patches within the slides. This approach is grounded in the hypothesis that certain regions within a WSI hold more diagnostic value than others and that their targeted analysis can yield efficient and accurate classification results. The cornerstone of our methodology is the application of fisher vector representation for feature extraction. Fisher vectors have been used in image processing and computer vision due to their effectiveness in capturing fine-grained details in image data while still offering considerable data compression compared to using all available samples [1]. By applying FV representation to the selected patches, we aim to extract rich and discriminative features that are crucial for accurate pathology classification in a data and computationally-efficient manner. Through this research, we aim to contribute to the field of digital pathology by providing a more efficient and scalable approach to WSI analysis. Our method not only has the potential to enhance diagnostic accuracy but also to significantly reduce the computational burden associated with the analysis of large-scale pathological data. To validate our claims we have used The Cancer Genome Atlas (TCGA) Lung Dataset [2, 3] and Camelyon17 dataset [4] which consist of whole slide images. Snapshots of sample images from both datasets are shown in Fig. LABEL:fig1. The Camelyon17 dataset, essential for the Camelyon17 Challenge, has facilitated the development of automated methods for the detection of breast cancer metastases in lymph node WSIs. It includes 500 high-resolution, expert-annotated images from various centers, categorized into four classes (Negative, Isolated Tumor Cell (ITC), Macro-metastases, and Micro-metastases). The TCGA Lung dataset has been used to develop automated methods to detect epidermal growth factor receptor (EGFR) mutations from H&E stained WSIs. It comprises 159 slides divided into EGFR and non-EGFR classes. Our stated goals were achieved by proposing an efficient feature representation of whole slide images to address the unique challenges of H&E stained histology images. The performance evaluation was focused on accuracy, robustness, and generalization to surpass state-of-the-art techniques on the two benchmark datasets. Furthermore, the research explored potential cross-domain applications in medical image analysis and computer vision, offering promising advancements in practical unsupervised domain adaptation with the help of effective feature space representation of whole slide images with different Fisher vectors to achieve significant improvement."
https://arxiv.org/html/2411.08508v1,"BillBoard Splatting (BBSplat): Learnable Textured Primitives 
for Novel View Synthesis","We present billboard Splatting (BBSplat) - a novel approach for 3D scene representation based on textured geometric primitives. BBSplat represents the scene as a set of optimizable textured planar primitives with learnable RGB textures and alpha-maps to control their shape. BBSplat primitives can be used in any Gaussian Splatting pipeline as drop-in replacements for Gaussians. Our method’s qualitative and quantitative improvements over 3D and 2D Gaussians are most noticeable when fewer primitives are used, when BBSplat achieves over 1200 FPS. Our novel regularization term encourages textures to have a sparser structure, unlocking an efficient compression that leads to a reduction in storage space of the model. Our experiments show the efficiency of BBSplat on standard datasets of real indoor and outdoor scenes such as Tanks&Temples, DTU, and Mip-NeRF-360. We demonstrate improvements on PSNR, SSIM, and LPIPS metrics compared to the state-of-the-art, especially for the case when fewer primitives are used, which, on the other hand, leads to up to 2\times inference speed improvement for the same rendering quality.","Novel view synthesis (NVS) is a crucial technology for a variety of applications, including virtual reality, computer gaming, and cinematography. Several efforts have been dedicated to deploy methods that are more efficient while providing a better quality of the synthesized images. Specifically, the choice of geometric primitives used to represent the scene plays a key role in defining the advantages and drawbacks of different NVS methods. Recent breakthroughs on neural representations [30, 31, 34, 4, 5, 35, 47, 8, 18, 7, 51] have been sided by recent methods based on Gaussian Splatting [19, 23, 24, 52, 10, 36, 13, 54], demonstrating most efficient way for novel view rendering. Indeed, NeRF-based methods [34, 5, 4, 35] still achieve the best NVS quality for challenging real-world captures by using implicit scene representations such as the weights of an MLP. However, image rendering with a NeRF is less efficient as it requires repeated MLP inferences to predict colors along camera rays. An alternative to neural rendering, 3D Gaussian Splatting (3DGS) [23] used faster rendering based on projecting explicit primitives on the screen surface while preserving high-quality NVS. In practice, 3DGS uses Gaussian-distributed radiance around explicit 3D scene points as primitives. Recently, 2D Gaussian Splatting (2DGS) [19] proposed the use of flat Gaussians, oriented in 3D, to represent the scene more efficiently. Since 2D Gaussians are effectively tangent to object surfaces, they allow more accurate surface extraction. Despite proving their efficiency in mesh extraction task, 2D primitives result in a downgrade of rendering metrics compared to 3D primitives such as 3D Gaussians. In this work, we aim to make 2D primitives suitable for high-quality NVS by intoducing a new primitive representation. Our proposed geometric primitives for NVS take inspiration from the classic billboards used for extreme 3D model simplification [9] by replacing mesh during a greedy optimization process. A 3D scene can be efficiently rendered by using a “billboard cloud” of given textured planar primitives with alpha channels. Using billboards, we can efficiently model planar surfaces, such as a painting on a wall or scene background, while dramatically reducing the number of geometric primitives required, up to an order of magnitude with respect to 3DGS/2DGS (check LABEL:fig:shapes for a comparison). In combination with efficient texture sampling implemented on the GPU, this leads to inference speed improvements without a drop in the rendering quality (Section 4.2). We define billboards with 2D Gaussians parameters (rotation, scaling, 3D center location, and spherical harmonics) while also introducing RGB texture and alpha map to control pixel-wise color and shape (Fig. 1(b2)). The alpha map defines the billboard silhouette and models the arbitrary shape of primitives. Similarly, the RGB texture stores color for each point of the billboard. In this way, we can use fewer primitives to represent high-frequency details (LABEL:fig:shapes). The key aspect of our approach, BillBoard Splatting (BBSplat), is a method to learn billboard parameters from a set of calibrated images. To ensure rendering efficiency, we implemented texture sampling and a back-propagation process in CUDA. In this way, the use of textures does not result in overtime compared to the 2DGS Gaussians’ rasterization process. To tackle the challenge of storing all billboard textures, we compress them by representing each texture as sparse offsets from colors predicted by spherical harmonics [2] and by further quantizing them to 8 bits. Then, we can efficiently utilize dictionary-based compression algorithms [41, 11] for quantized textures. To summarise, our contributions are as follows: • We propose BBSplat with optimizable textured primitives to learn 3D scene representation with photometric losses. BBSplat allows up to \times 2 acceleration for NVS compared to the state-of-the-art for the same rendering quality. • We developed an algorithm to efficiently represent and store textures for billboards. BBSplat demonstrates significant storage cost reduction. As a result, for some scenarios, storing the scene represented with billboards is more efficient than 3DGS or 2DGS. • We implemented BBSplat with CUDA and demonstrated its efficiency in extensive experiments on several real-scene open datasets."
https://arxiv.org/html/2411.08490v1,Impact of Iris Pigmentation on Performance Bias in Visible Iris Verification Systems: A Comparative Study,"Iris recognition technology plays a critical role in biometric identification systems, but their performance can be affected by variations in iris pigmentation. In this work, we investigate the impact of iris pigmentation on the efficacy of biometric recognition systems, focusing on a comparative analysis of blue and dark irises. Data sets were collected using multiple devices, including P1, P2, and P3 smartphones [4], to assess the robustness of the systems in different capture environments [19]. Both traditional machine learning techniques and deep learning models were used, namely Open-Iris, ViT-b, and ResNet50, to evaluate performance metrics such as Equal Error Rate (EER) and True Match Rate (TMR). Our results indicate that iris recognition systems generally exhibit higher accuracy for blue irises compared to dark irises. Furthermore, we examined the generalization capabilities of these systems across different iris colors and devices, finding that while training on diverse datasets enhances recognition performance, the degree of improvement is contingent on the specific model and device used. Our analysis also identifies inherent biases in recognition performance related to iris color and cross-device variability. These findings underscore the need for more inclusive dataset collection and model refinement to reduce bias and promote equitable biometric recognition across varying iris pigmentation and device configurations.","Iris recognition systems have become one of the most reliable and secure forms of biometric identification [13][14][24] due to unique and stable patterns in the human iris. These systems leverage high-resolution imaging and advanced algorithms [1][18] to capture, extract, and match intricate iris features, offering unmatched accuracy in identity verification. Despite their robustness, the fairness [11] of iris recognition systems has become a critical area of research. Variations in iris color, texture, and pupil dilation due to demographic differences can introduce biases, affecting the system’s performance across different populations. For instance, factors such as age, ethnicity, and even environmental conditions can lead to discrepancies in recognition accuracy, raising concerns about equitable treatment for all users. Specifically, the risk of bias based on demographic factors such as age, race, or gender can result in uneven performance across different groups. This performance disparity, often quantified through differential error rates, highlights the need to address these biases to ensure that biometric systems provide fair and equitable outcomes for all users This paper delves into the technical foundations of iris recognition systems, explores the inherent biases that can affect their performance, and discusses strategies to enhance fairness in these systems. Through comprehensive analysis and experimentation, we aim to contribute to the ongoing efforts to create more equitable biometric technologies. 1.1 Motivation and contribution Fairness in biometric systems specifically concerns differences in outcomes linked to demographic traits[17][3][21], such as age, gender, race, or iris color. However, these demographic variations[21] may overlap with other individual characteristics, leading to complex biases. Figure 1: Illustrates an iris recognition system comparing blue and dark irises, using similarity scores and analyzing fairness with DPD and EOD metrics to address demographic biases. These differences in iris pigmentation [7] might affect the system’s ability to accurately match or verify identity, leading to higher false rejection rates for some groups. Highlights the importance of developing algorithms [17][11] that account for these variations to ensure that biometric systems[10][23] perform consistently across all demographics, thereby avoiding unintended biases in high-stakes applications. The primary aim of fairness in iris biometrics[7] is to identify and understand biases in systems that determine or validate individual identities or characteristics, such as iris color. In our study, we are motivated to investigate and mitigate biases in iris biometric systems to ensure fairness and improve the accuracy and reliability of these systems, particularly in relation to iris pigmentation color as shown in Figure 1. This research aims to address the growing concerns of fairness in biometric systems by examining how different pigmentation levels, such as blue versus dark irises, impact system performance. To the best of our knowledge, this is the first study to comprehensively explore these effects across multiple devices and recognition models, highlighting the importance of unbiased and accurate biometric recognition. Our work aims to tackle the rising challenge of ensuring fairness in biometric systems, particularly focusing on how biases related to iris pigmentation can impact the accuracy and reliability of recognition models. In particular, we introduce the following critical questions: • Q1.Does iris pigmentation (Blue Iris (BI) vs. Dark Iris (DI) color) impact the accuracy of the biometric system ? • Q2.Can biometric systems effectively generalize across varying iris colors, specifically between blue and dark iris ? In addressing the research questions posed above, this study makes the following key contributions, including being the first to investigate the bias of visible iris recognition systems for blue and dark-colored irises. • To the best of author’s knowledge, this is the first detailed study examining how visible blue vs dark iris pigmentation impacts the performance of iris recognition systems. • A detailed experimental study are presented to benchmark the performance of the traditional and deep learning-based iris recognition systems to provide insights into the generalization capability of biometric systems across different smartphone devices and iris pigmentation colors ( Blue vs Dark). • A fairness analysis is conducted using Demographic Parity Difference (DPD) and Equalized Odds Difference (EoD) metrics to assess biases in the recognition systems. The structure of the paper is as follows: Section 2 reviews related work in iris recognition. Section 3 explores various methods employed in iris recognition, while Section 4 presents a quantitative and qualitative analysis of these systems, examining their impact on iris pigmentation (dark versus blue). Finally, Section 5 concludes the paper."
https://arxiv.org/html/2411.08482v1,Methodology for a Statistical Analysis of Influencing Factors on 3D Object Detection Performance,"In autonomous driving, object detection is an essential task to perceive the environment by localizing and classifying objects. Most object detection algorithms rely on deep learning for their superior performance. However, their black box nature makes it challenging to ensure safety. In this paper, we propose a first-of-its-kind methodology for statistical analysis of the influence of various factors related to the objects to detect or the environment on the detection performance of both LiDAR- and camera-based 3D object detectors. We perform a univariate analysis between each of the factors and the detection error in order to compare the strength of influence. To better identify potential sources of detection errors, we also analyze the performance in dependency of the influencing factors and examine the interdependencies between the different influencing factors. Recognizing the factors that influence detection performance helps identify robustness issues in the trained object detector and supports the safety approval of object detection systems.","The foundation of perception in autonomous driving (AD) is object detection by various sensors, such as RGB cameras, LiDAR, and radar [1]. To fully realize the potential of autonomous vehicles (AVs) in reducing accidents and improving road traffic safety [2], it is necessary, among other factors, that AVs perceive their environment accurately and robustly [3]. Object detectors based on deep neural networks (DNN) achieve high performance in localizing and classifying objects [4]. Nevertheless, DNNs exhibit several safety concerns such as brittleness respectively sensitivity to small perturbations, and unknown behavior due to their black box nature [5]. Thus, providing safety assurance for DNNs is challenging [6], which is a major problem for the applicability of DNN-based object detectors in the safety-relevant domain of AD. A step towards providing safety assurance in the perception systems of AD is to identify the weaknesses of the DNN during the design phase and to understand the causes of detection errors. Hence, this work identifies and analyzes the influence of various factors respectively meta-information from the environment and the objects on the detection performance of DNN-based object detectors. The meta-information refers to characteristics of the scene, such as weather conditions or specific object properties. By analyzing camera-based and LiDAR-based object detectors, we also investigate the differences between the two sensor modalities. Our developed methodology identifies specific weaknesses for the different object detectors and supports the development of sophisticated test catalogs. The main contributions of our work are: (i) providing a statistical comparison of detector’s dependencies on meta-information, (ii) identifying the differences of influencing factors between LiDAR-based and camera-based single-sensor detectors, and (iii) pinpointing class-specific dependencies."
https://arxiv.org/html/2411.08472v1,A survey on Graph Deep Representation Learning for Facial Expression Recognition,"This comprehensive review delves deeply into the various methodologies applied to facial expression recognition (FER) through the lens of graph representation learning (GRL). Initially, we introduce the task of FER and the concepts of graph representation and GRL. Afterward, we discuss some of the most prevalent and valuable databases for this task. We explore promising approaches for graph representation in FER, including graph diffusion, spatio-temporal graphs, and multi-stream architectures. Finally, we identify future research opportunities and provide concluding remarks.","Facial expressions transcend cultural barriers, playing a crucial role in communication and human interaction. They serve as a natural conduit for expressing emotions and sharing information, making facial expression recognition (FER) essential for deciphering emotional subtleties and anticipating reactions across various contexts. FER finds application in a multitude of sectors including healthcare, education, the automotive industry, marketing, robotics, entertainment, and customer service, underscoring its significance and potential for innovation. Historically, the development of FER technologies has been dominated by deep learning (DL) techniques, notably convolutional neural networks (CNN). Despite notable progress, these methods face significant challenges in accurately modeling the complexity of facial expressions. These challenges include locating faces in cluttered environments, variations in lighting that can obscure or distort essential facial features, and analysis of face texture, with repetitive patterns or skin peculiarities. We can also mention occlusion problems, where external elements hide parts of the face, expression differences among individuals, and varied head poses. Furthermore, efficiently encoding faces, poses a crucial challenge, necessitating robust representations. Finally, the debate between discrete and continuous emotion representation, as researchers have to choose between the precision of discrete categories and the nuance of continuous models to classify the human emotions. These challenges represent significant barriers to the generalization of FER models, highlighting the importance of approaches capable of handling the intrinsic variability of human expressions and image capture conditions. These challenges have opened the search for new approaches, among which graph representation learning (GRL) stands out as a promising solution. Leveraging relational and structural data, this method offers an innovative technique to overcome the challenges of FER and opens a new direction for advancements in the field. Overcoming all obstacles will require continuous innovations in image processing, data modeling, and machine learning, emphasizing the importance of future research to explore new methods aimed at improving the accuracy, robustness, and efficiency of FER systems. I-A Facial Expression Recognition (FER) FER is both a complex and comprehensive task, typically associated with a classification problem. Its history is also rooted in the disciplines of psychology and neuroscience. Its origins can be traced back to the pioneering work ”The Expression of the Emotions in Man and Animals” by C. Darwin [1], in which he argued for the universality of some facial expressions among human beings. Throughout the 20th century, the field had a large growth due to the introduction of the Facial Action Coding System (FACS) by P. Ekman et al. [2]. This system, which focused on classifying facial expressions through action units (AUs), marked a major turning point, significantly enriching research and applications in this area. At the end of the 20th century, its integration into computing allowed the automatization of processes. This integration allowed major innovations in machine learning [3], such as the tracking of facial landmarks. New databases specialized in emotions appeared at the end of the century, we can cite the Japanese Female Facial Expression (JAFFE) database by M. Lyons et al. [4], presented in Fig. 1, and the Cohn-Kanade dataset by T. Kanade et al. [5]. Their release made possible the development of data-driven approaches as we know them today. Figure 1: Samples from the JAFFE [4] FER database. More recently, DL marked an important stepping stone for the task of FER [6]. The large adoption of CNNs not only improved the accuracy and robustness of FER systems but also extended their fields of application, facilitating their integration into diverse areas of applications such as mental health diagnostics, or human-machine interaction. The research on the FER task has become increasingly dynamic and the results follow a rapid progression with more challenging databases to align with the recent technological advancements. I-B Graph Theory GRL originally comes from graph theory, a mathematical discipline initiated by L. Euler [7] with his solution to the Königsberg bridge problem. Figure 2: Königsberg bridge problem [8]. This breakthrough laid the foundations for graph theory. The beginnings of neural networks encouraged researchers to use graph theory for their experiments, but the computational capabilities of the early models rapidly limited the models to structured data, setting aside graph-structured data. New techniques then allowed more application of graph theory to machine learning. Techniques such as spectral clustering [9], utilized the eigenvalues of graph laplacians, offering modern tools for deeper analysis and processing of graph-structured data. The early 2000s saw the introduction of graph neural networks (GNNs) [10], thus greatly improving GRL, the combination of neural networks to graph data opened new paths of research for graph machine learning. Their development led quickly to graph convolutional networks (GCNs) [11], and graph attention mechanisms [12], greatly improving GCNs effectiveness. GRL became dynamic through the years, rapidly expanding due to its versatile applications. New methods are explored to improve the models’ efficiency and scalability. This article aims to provide a general overview of GRL approaches for FER. Following this introduction, we will discuss graphs representation in section 2, the GRL in section 3, the main databases in section 4, recent and promising approaches in section 5, the research opportunities in section 6, and finally the conclusion in section 7."
https://arxiv.org/html/2411.08470v1,HyperFace: Generating Synthetic Face Recognition Datasets by Exploring Face Embedding Hypersphere,"Face recognition datasets are often collected by crawling Internet and without individuals’ consents, raising ethical and privacy concerns. Generating synthetic datasets for training face recognition models has emerged as a promising alternative. However, the generation of synthetic datasets remains challenging as it entails adequate inter-class and intra-class variations. While advances in generative models have made it easier to increase intra-class variations in face datasets (such as pose, illumination, etc.), generating sufficient inter-class variation is still a difficult task. In this paper, we formulate the dataset generation as a packing problem on the embedding space (represented on a hypersphere) of a face recognition model and propose a new synthetic dataset generation approach, called HyperFace. We formalize our packing problem as an optimization problem and solve it with a gradient descent-based approach. Then, we use a conditional face generator model to synthesize face images from the optimized embeddings. We use our generated datasets to train face recognition models and evaluate the trained models on several benchmarking real datasets. Our experimental results show that models trained with HyperFace achieve state-of-the-art performance in training face recognition using synthetic datasets. Project page: https://www.idiap.ch/paper/hyperface","Recent advances in the development of face recognition models are mainly driven by the deep neural networks (He et al., 2016), the angular loss functions (Deng et al., 2019; Kim et al., 2022), and the availability of large-scale training datasets (Guo et al., 2016; Cao et al., 2018; Zhu et al., 2021). The large-scale training face recognition datasets are collected by crawling the Internet and without the individual’s consent, raising privacy concerns. This has created important ethical and legal challenges regarding the collecting, distribution, and use of such large-scale datasets (Nat, 2022). Considering such concerns, some popular face recognition datasets, such as MS-Celeb (Guo et al., 2016) and VGGFace2 (Cao et al., 2018), have been retracted. With the development of generative models, generating synthetic datasets has become a promising solution to address privacy concerns in large-scale datasets (Melzi et al., 2024; Shahreza et al., 2024). In spite of several face generator models in the literature (Deng et al., 2020; Karras et al., 2019, 2020; Rombach et al., 2022; Chan et al., 2022), generating a synthetic face recognition model that can replace real face recognition datasets and be used to train a new face recognition model from scratch is a challenging task. In particular, the generated synthetic face recognition datasets require adequate inter-class and intra-class variations. While conditioning the generator models on different attributes can help increasing intra-class variations, increasing inter-class variations remains a difficult task. Figure 1: Sample face images from the HyperFace dataset In this paper, we focus on the generation of synthetic face recognition datasets and formulate the dataset generation process as a packing problem on the embedding space (represented on the surface of a hypersphere) of a pretrained face recognition model. We investigate different packing strategies and show that with a simple optimization, we can find a set of reference embeddings for synthetic subjects that has a high inter-class variation. We also propose a regularization term in our optimization to keep the optimized embedding on the manifold of face embeddings. After finding optimized embeddings, we use a face generative model that can generate face images from embeddings on the hypersphere, and generate synthetic face recognition datasets. We use our generated synthetic face recognition datasets, called HyperFace, to train face recognition models. We evaluate the recognition performance of models trained using synthetic datasets, and show that our optimization and packing approach can lead to new synthetic datasets that can be used to train face recognition models. We also compare trained models with our generated dataset to models trained with previous synthetic datasets, where our generated datasets achieve competitive performance with state-of-the-art synthetic datasets in the literature. Figure 1 illustrates sample face images from our synthetic dataset. The remainder of this paper is organized as follows. In Section 2, we present our problem formulation and describe our proposed method to generate synthetic face datasets. In Section 3, we provide our experimental results and evaluate our synthetic datasets. In Section 4, we review related work in the literature. Finally, we conclude the paper in Section 5."
https://arxiv.org/html/2411.08466v1,"Can MLLMs Guide Weakly-Supervised Temporal Action
Localization Tasks?","Recent breakthroughs in Multimodal Large Language Models (MLLMs) have gained significant recognition within the deep learning community, where the fusion of the Video Foundation Models (VFMs) and Large Language Models(LLMs) has proven instrumental in constructing robust video understanding systems, effectively surmounting constraints associated with predefined visual tasks. These sophisticated MLLMs exhibit remarkable proficiency in comprehending videos, swiftly attaining unprecedented performance levels across diverse benchmarks. However, their operation demands substantial memory and computational resources, underscoring the continued importance of traditional models in video comprehension tasks. In this paper, we introduce a novel learning paradigm termed MLLM4WTAL. This paradigm harnesses the potential of MLLM to offer temporal action key semantics and complete semantic priors for conventional Weakly-supervised Temporal Action Localization (WTAL) methods. MLLM4WTAL facilitates the enhancement of WTAL by leveraging MLLM guidance. It achieves this by integrating two distinct modules: Key Semantic Matching (KSM) and Complete Semantic Reconstruction (CSR). These modules work in tandem to effectively address prevalent issues like incomplete and over-complete outcomes common in WTAL methods. Rigorous experiments are conducted to validate the efficacy of our proposed approach in augmenting the performance of various heterogeneous WTAL models.","Temporal action localization (TAL) (Paul, Roy, and Roy-Chowdhury 2018; Narayan et al. 2021; Lee et al. 2021; Alayrac et al. 2022; Dai et al. ) aims to localize action instances of interest from untrimmed videos. Although current fully-supervised TAL methods achieve significant localization results, fully-supervised methods require expensive, time-consuming, and labor-intensive frame-level annotations, and weakly-supervised temporal action localization (WTAL) methods (Paul, Roy, and Roy-Chowdhury 2018; Narayan et al. 2021; Lee et al. 2021), which require only video-level labels, have received much attention in order to alleviate the cost of annotation. Existing WTAL methods typically train a classifier(Paul, Roy, and Roy-Chowdhury 2018; Narayan et al. 2021; Lee et al. 2021) using video-level labels, generating a set of probability predictions. This process results in temporal class activation maps (T-CAM). Despite notable performance improvements, current methods still face two challenges: incomplete and over-complete localization. As illustrated in Fig.1, some sub-actions with low discriminability may be ignored, leading to incomplete localization, while certain background segments that contribute to classification might be misclassified as foreground actions, causing over-complete localization. Figure 1: Previous methods have problems of over-complete positioning and incomplete positioning. To overcome these challenges, we turn our perspective to existing multimodal large language models(MLLMs)(Alayrac et al. 2022; Dai et al. ). The state-of-the-art MLLM employs a seamless amalgamation of a video foundation model and a large language model, culminating in the development of a sophisticated system for enhanced video comprehension. This integrated framework transcends the limitations inherent in traditional predefined visual tasks. Although MLLM exhibits powerful video understanding capabilities, the resource-intensive nature of running it poses high memory and computational costs. Therefore, traditional models remain essential for video understanding tasks. Is it possible to propose a learning paradigm that leverages MLLMs to enhance existing traditional WTAL methods? Inspired by this, we introduce a paradigm for MLLM-enhanced WTAL tasks, known as MLLM4WTAL. Aiming at the incomplete and over-complete localization problems of previous WTAL methods, we propose MLLM-based key semantic matching (KSM) and complete semantic reconstruction (CSR) modules. KSM aims to utilize MLLM to generate key semantic priors for videos, matching segments in the video based on key semantic embeddings and video embeddings. This matching strategy helps locate key intervals for temporal actions. CSR aims to utilize MLLM to generate complete semantic priors for videos. By reconstructing the complete semantics of masked action words, we aim to mine the time interval of action instances as complete as possible. This reconstruction strategy helps locate complete intervals for temporal actions. Based on KSM and CSR, we obtain the key intervals and the complete intervals of the temporal action in turn. Due to the task characteristics of KSM, it may overly focus on segments most matching key semantics, neglecting fewer matching segments, leading to incomplete localization. On the other hand, due to the task characteristics of CSR, it may associate as many segments as possible with key action verbs, often introducing background segments similar to foreground actions, resulting in over-complete localization. Considering the strengths and weaknesses of the two modules, we propose a dual prior interactive enhancement optimization strategy. First, the KSM branch performs forward operations to obtain \mathcal{A}_{KSM}, and then the CSR branch uses \mathcal{A}_{KSM} as the optimization target to alleviate its over-complete localization problem. Subsequently, the CSR branch performs forward operations to obtain \mathcal{A}_{CSR}, and then the KSM branch uses \mathcal{A}_{CSR} as the optimization target to alleviate its incomplete localization problem. The two modules interactively distill each other, forming a strong joint approach. Our approach implements the state-of-the-art on two popular benchmarks, THUMOS14 and ActivityNet1.2. Furthermore, we find that our proposed approach can be applied to existing methods and improve their performance with significant advantages. Our contributions can be summarized in four aspects: • We propose a paradigm, MLLM4WTAL, that guides WTAL methods using MLLM. To the best of our knowledge, this is the first work that enhances WTAL methods using MLLM. We also show that our method can be easily extended to existing state-of-the-art methods and improve their performance without introducing additional overhead in the inference phase. • To fully leverage the prior information provided by MLLM, we design two modules, KSM and CSR, to enhance WTAL methods from the perspectives of key semantic matching and complete semantic reconstruction, respectively. • To achieve a strong collaboration between KSM and CSR, we propose a dual prior interactive distillation strategy, cleverly addressing the incomplete and over-complete issues present in each module. • Extensive experiments show that our method outperforms existing methods on two public datasets. Comprehensive ablation studies reveal the effectiveness of the proposed method."
https://arxiv.org/html/2411.08451v1,AD-DINO: Attention-Dynamic DINO for Distance-Aware Embodied Reference Understanding,"Embodied reference understanding is crucial for intelligent agents to predict referents based on human intention through gesture signals and language descriptions. This paper introduces the Attention-Dynamic DINO, a novel framework designed to mitigate misinterpretations of pointing gestures across various interaction contexts. Our approach integrates visual and textual features to simultaneously predict the target object’s bounding box and the attention source in pointing gestures. Leveraging the distance-aware nature of nonverbal communication in visual perspective taking, we extend the virtual touch line mechanism and propose an attention-dynamic touch line to represent referring gesture based on interactive distances. The combination of this distance-aware approach and independent prediction of the attention source, enhances the alignment between objects and the gesture represented line. Extensive experiments on the YouRefIt dataset demonstrate the efficacy of our gesture information understanding method in significantly improving task performance. Our model achieves 76.4% accuracy at the 0.25 IoU threshold and, notably, surpasses human performance at the 0.75 IoU threshold, marking a first in this domain. Comparative experiments with distance-unaware understanding methods from previous research further validate the superiority of the Attention-Dynamic Touch Line across diverse contexts.","Reference understanding (RU) is fundamental to interpersonal and human-robot communication, particularly when referring to shared objects within a common space [1, 2]. In the computer vision community, referring expression comprehension (REC) [3, 4, 5, 6, 7, 8, 9], a crucial task within RU, facilitates visual grounding in images by integrating visual and linguistic cues. However, accurately locating referents based solely on verbal descriptions remains challenging [10, 11, 12, 13, 14]. Non-verbal references, such as embodied pointing gestures synchronized by the describer, often provide more precise spatial indication than verbal descriptions alone. To enhance the combined interpretation of linguistic and visual cues, Chen et al.[15] introduced the embodied reference understanding (ERU) task, along with the YouRefIt dataset and benchmark. Nevertheless, the interpretation of pointing gestures continues to significantly constrain their effectiveness in ERU applications. The comprehension of pointing gestures is deeply rooted in human cognitive development and learning processes [16, 17]. Traditionally, it has been assumed that the target object’s orientation lies within the extension of the pointer’s arm-finger line. Intriguingly, recent studies have revealed that this arm-finger line represents a systematic spatial misunderstanding [18, 19]. When the referent is distant from the pointer, observers can utilize the eye-finger line — formed by aligning the eye, finger, and referent — to mitigate misinterpretation of the pointing gesture. Building on this concept, O’Madagain et al. [20] proposed the Virtual Touch Line (VTL) mechanism, which Li et al.[21] subsequently employed to connect the eye to the fingertip, significantly enhancing ERU performance. Figure 1: Example of pointing to the object that close to body. In this scenario, the girl is accurately pointing at the lipstick which is around her while the eyes deviate the connect line between fingertip and lipstick. However, as pointing is a situated interactive activity [22], the VTL mechanism can be misinterpreted in close-proximity interactions. Fig. 1 illustrates this limitation: a girl points to a nearby lipstick, yet the VTL and its extension do not intersect with the object. In this scenario, previously used reference lines such as the wrist-elbow or arm-finger lines are not clearly identifiable. The pointer’s limb may bend arbitrarily while maintaining an extended index finger, allowing the finger line (FL) to pass through the object’s spatial location, thus providing a clear embodied reference. Embodied reference understanding is predicated on visual perspective taking (VPT)[6, 23, 24], where mutual engagement requires both parties to adjust their cognitive and perceptual states for alignment. We argue that previous research has focused mechanistically on interpreting gesture appearances, neglecting the pointer-referent interaction contexts that inform gesture formation from the pointer to the observer. Specifically, pointers invariably aim to provide clear gestures by directing their finger toward the target object. In distant interactions, pointers spatially align their eye, finger, and the object to accurately indicate the referent’s position, adhering to the VTL mechanism. However, in close interactions, pointers disregard VTL-imposed alignment restrictions, allowing for more casual yet accurate pointing under gaze supervision. While O’Madagain et al.’s theory of pointing originating from touch [20] remains valid, the VTL mechanism becomes inapplicable in these scenarios. Drawing inspiration from multiple studies [25, 26, 27] and common sense, which noted that humans adjust arm positions during pointing movements through complex motion planning based on object position and distance, we introduce the concept of distance-aware VPT (DA-VPT). This approach extends gesture representation from VTL to the Attention-Dynamic Touch Line (ADTL). In the ADTL framework, while the endpoint remains fixed at the fingertip, the attention source (starting point) becomes dynamic, adapting to the interaction distance. For distant interactions, the attention source is set to the eyes, aligning with the VTL. However, in close-proximity interactions, considering DA-VPT, we shift the attention source to the metacarpophalangeal joint (MCP) of the index finger, corresponding to the FL, resulting in a clearer gesture representation compared to the VTL. To address the limitations of previous research and incorporate the ADTL concept, we propose the Attention-Dynamic DINO (AD-DINO) to enhance visual grounding accuracy. Our system processes visual and natural language inputs through initial feature extraction, cross-modal feature fusion, and language-guided query selection for the decoder. Both language and visual features are simultaneously fed into the cross-modality decoder, which directly outputs the object’s bounding box location and the attention source. Additionally, we integrate a fingertip detector to determine the fingertip’s position, enabling the construction of the ADTL corresponding to the human body pose by combining the attention source and fingertip position. The selection of the attention source is based on the interaction distance between the pointer and the object. When this distance exceeds arm’s length, we localize the attention source to the eyes. Otherwise, we shift the attention source from the eyes to the MCP. Independent prediction of the attention source also reduces model training costs and error levels compared to attention source -fingertip pair prediction. By the designed AD-DINO and ADTL, our proposed method achieves 76.3% accuracy under the 0.25 IoU threshold. Significantly, at the 0.75 IoU threshold, AD-DINO achieves 55.4% accuracy and outperforms human performance for the first time in embodied reference understanding tasks, marking a crucial milestone in narrowing the gap between computational models and human capabilities in this domain. The main contributions of this paper are fourfold: (i) We emphasize the distance-aware aspect of the visual perspective taking mechanism and incorporate it into the embodied reference understanding task. (ii) We develop an attention-dynamic touch line based on pointer-referent interaction distance and distance-aware visual perspective taking, enhancing embodied reference understanding performance. (iii) We propose a novel model that combines images with gesture information and verbal instructions, optimizing the detection mechanism for key points in pointing gestures. (iv) We achieve state-of-the-art (SOTA) performance on ERU, with our method demonstrating a 16.4% improvement over the previous SOTA method at the 0.75 IoU threshold on the YouRefIt dataset, surpassing human performance for the first time."
https://arxiv.org/html/2411.08424v1,A Heterogeneous Graph Neural Network Fusing Functional and Structural Connectivity for MCI Diagnosis,"Brain connectivity alternations associated with brain disorders have been widely reported in resting-state functional imaging (rs-fMRI) and diffusion tensor imaging (DTI). While many dual-modal fusion methods based on graph neural networks (GNNs) have been proposed, they generally follow homogenous fusion ways ignoring rich heterogeneity of dual-modal information. To address this issue, we propose a novel method that integrates functional and structural connectivity based on heterogeneous graph neural networks (HGNNs) to better leverage the rich heterogeneity in dual-modal images. We firstly use blood oxygen level dependency and whiter matter structure information provided by rs-fMRI and DTI to establish homo-meta-path, capturing node relationships within the same modality. At the same time, we propose to establish hetero-meta-path based on structure-function coupling and brain community searching to capture relations among cross-modal nodes. Secondly, we further introduce a heterogeneous graph pooling strategy that automatically balances homo- and hetero-meta-path, effectively leveraging heterogeneous information and preventing feature confusion after pooling. Thirdly, based on the flexibility of heterogeneous graphs, we propose a heterogeneous graph data augmentation approach that can conveniently address the sample imbalance issue commonly seen in clinical diagnosis. We evaluate our method on ADNI-3 dataset for mild cognitive impairment (MCI) diagnosis. Experimental results indicate the proposed method is effective and superior to other algorithms, with a mean classification accuracy of 93.3%.","\IEEEPARstart Magnetic Resonance Imaging (MRI) has emerged as a valuable tool in neuroscience, offering deeper objective insights into neurological disorders and their underlying mechanisms. Previous researches have demonstrated the valuable contributions of resting-state functional MRI (rs-fMRI) and Diffusion Tensor Imaging (DTI) among multiple MRI modalities in understanding brain structure and function [1, 2]. Specifically, brain functional connectivity (FC) constructed from rs-fMRI imaging can capture spontaneous neuronal activity and reveal intrinsic connections between brain regions [3, 4] while brain structural connectivity (SC) constructed from DTI imaging is able to provide crucial insights into the integrity of white matter structures and the identification of neural fiber abnormalities [5, 6]. In recent years, many studies find that alterations in neuronal functioning are directly related to alterations in white matter structure [7, 8], which has given rise to a lot of work on dual-modal fusion of FC and SC. Since FC and SC can be conveniently described by graphs, framework based on graph neural networks (GNNs) have become a popular choice to identify brain disorders combining dual-modal information [9, 10, 11, 12]. There are mainly two popular ways to fuse dual-modal information with GNNs, which are feature-level fusion and connectivity-level fusion. Specifically, in feature-level fusion, the same backbone is applied to different modalities to extract functional and structural features separately, then features are concatenated or weighted summed together as the fused feature for further analysis. While in connectivity-level fusion, usually a summative homogeneous graph (i.e., graph that have only one type of node and one type of edge) will be constructed based on fused structural-functional connectivity, and the fused structure-function features will be extracted from the summative graph. While these approaches are able to fuse dual-modal features, there is more information of connectivity yet to be explored. Firstly, there is no feature interaction in the above feature fusion methods, which leads to insufficient feature learning and reduced classification performance. Although connectivity-level fusion provides more consistent feature embedding than feature-level fusion, the summative graphs constructed in homogeneous way may disrupt the inherent heterogeneity between FC and SC, such as differences in feature spaces and graph topologies. These problems urge a new dual-modal fusion approach to better synthesize the information from FC and SC to identity brain disorders. Heterogeneous graph (HG) provides new ways of describing the complex connectivity in real world [13], which drives the development of heterogeneous graph neural networks (HGNNs) in fields such as social network analysis and bioinformatics [13, 14]. Considering that integrating FC and SC into HG can effectively preserve the heterogeneous information in the two modalities, we pursue a new dual-modal fusion method based on HG in present work. There are several challenges needed to be addressed yet. Specifically, i) different types of relations in HG are usually defined through meta-path which is semantic dependent [15, 16], meaning that meta-path needs to follow the inherent connectivity within the modality as well as to reveal interactive connectivity between modalities. ii) Pooling strategies for HGs need to cope with more complex relations, and directly applying pooling strategies designed for homogeneous graphs [17, 18] can lead to feature confusion among different types of nodes in HGs. iii) Differences in the incidence of various brain diseases lead to sample imbalance, which in turn affects the classification performance of graph networks. Therefore, we propose several effective mechanisms to address these challenges. Firstly, in constructing HG, in order to capture node relationships within rs-fMRI or DTI modality and relationships among cross-modal node pairs, we propose to define homo-meta-path and hetero-meta-path. Based on existing rs-fMRI and DTI studies, we naturally utilize blood oxygen level dependency information to construct FC and white matter fibers to construct SC, which serve as homo-meta-paths. On the other hand, we propose to establish hetero-meta-path from node-level and community-level based on structure-functional coupling [19] and brain community searching [20] to capture cross-modal relationships. Secondly, as existing fusion methods based on GNNs are not capable for HGs, we introduce a novel HG pooling strategy that not only comprehensively considers heterogeneous topology but also can avoid feature confusion among different types of nodes. Thirdly, to address the common issue of sample imbalance in brain disorder datasets, we propose a novel HG augmentation method leveraging the adaptability of HG. The main contributions of our present work can be summarized as follows: 1) We propose a novel HGNN to fuse rs-fMRI and DTI information for MCI diagnosis. In constructing HG, we propose to establish homo-meta-path reflecting unimodal connectivity and hetero-meta-path reflecting dual-modal inter-relations, where structure-function coupling and brain community searching are used in establishing hetero-meta-path. 2) We introduce a novel HG pooling strategy which can automatically balance homo- and hetero-meta-path, effectively leveraging heterogeneous information and preventing feature confusion after pooling. 3) We propose an HG augmentation method leveraging the adaptability of HG to address the issue of sample imbalance, which is a common factor that affects the performance of diagnostic model in classification of brain disorders. The proposed method is validated using a mild cognitive impairment (MCI) dataset sampled from ADNI-3 dataset. Experimental results indicate that our method can achieve remarkable performance for MCI identification. The structure of this paper is organized as follows. Section 2 introduces the most relevant concepts. In Section 3, we introduce materials used in this work. Section 4 introduces details of proposed method. In Section 5, we introduce experimental settings, state-of-the-art methods and present experimental results. Section 6 discusses the influence of proposed key mechanisms in our method. We conclude this letter in Section 7."
https://arxiv.org/html/2411.08402v1,"V2X-R: Cooperative LiDAR-4D Radar Fusion 
for 3D Object Detection with Denoising Diffusion","Current Vehicle-to-Everything (V2X) systems have significantly enhanced 3D object detection using LiDAR and camera data. However, these methods suffer from performance degradation in adverse weather conditions. The weather-robust 4D radar provides Doppler and additional geometric information, raising the possibility of addressing this challenge. To this end, we present V2X-R, the first simulated V2X dataset incorporating LiDAR, camera, and 4D radar. V2X-R contains 12,079 scenarios with 37,727 frames of LiDAR and 4D radar point clouds, 150,908 images, and 170,859 annotated 3D vehicle bounding boxes. Subsequently, we propose a novel cooperative LiDAR-4D radar fusion pipeline for 3D object detection and implement it with various fusion strategies. To achieve weather-robust detection, we additionally propose a Multi-modal Denoising Diffusion (MDD) module in our fusion pipeline. MDD utilizes weather-robust 4D radar feature as a condition to prompt the diffusion model to denoise noisy LiDAR features. Experiments show that our LiDAR-4D radar fusion pipeline demonstrates superior performance in the V2X-R dataset. Over and above this, our MDD module further improved the performance of basic fusion model by up to 5.73%/6.70% in foggy/snowy conditions with barely disrupting normal performance. The dataset and code will be publicly available at: https://github.com/ylwhxht/V2X-R.","In recent years, autonomous driving and other unmanned systems have garnered widespread attention. This has led to rapid advancements in 3D object detection [24, 64, 4]. Outdoor environments, however, present complex and dynamic challenges, including various occlusions and weather conditions [44, 14]. Such factors significantly impact the performance of 3D object detection. Consequently, some effort has been made to explore multi-agent cooperative perception, such as vehicle-to-vehicle (V2V), vehicle-to-infrastructure (V2I), and vehicle-to-everything (V2X) [53, 52, 56, 63]. Benefiting from the information shared between agents, in complex outdoor scenarios, cooperative 3D object detection has natural advantages, such as long detection distance and multi-view object observation. Figure 1: The advantages provided by the dense 4D radar point cloud in multi-agent view. Including weather-robustness, fewer spatial errors, Doppler information, and geometric shapes. Current research in cooperative 3D object detection mainly focuses on two strategies: LiDAR-based single modality [54, 52, 12, 59, 31] and LiDAR-camera multi-modal fusion [53, 47, 65, 13, 67]. The latter strategy provides more fine-grained information and, therefore, improves the performance of single LiDAR-based methods to some extent. However, both LiDAR point clouds and camera images are weather-sensitive. They are all prone to become noisy in adverse weather [8, 15, 9]. Aside from LiDAR and camera, 4D radar has also been widely noticed [1, 16, 10, 38, 41, 40] because it can perform all-weather sensing and provide speed measurements [66]. Fusing 4D radar and LiDAR is expected to improve the perception ability under adverse weather conditions. As shown in Fig. 1, cooperative 4D radar and LiDAR fusion perception have the following advantages: • Weather-Robustness. The millimeter-wave signals of 4D radar easily penetrates particles in adverse weather [30, 7, 50], fusing 4D radar and LiDAR will grant the model weather robust sensing capabilities. • Fewer spatial relationship errors. The error-prone operations (view-transformation or depth-estimation) [43, 42, 1] are not required in the process of fusion of 4D radar and LiDAR point cloud. There are fewer corresponding spatial relationship errors in fusing 4D radar and LiDAR. • Additional Doppler information. The Doppler information provided by 4D radar is favorable for object detection [16, 1], which can greatly help to detect motion objects. • Richer geometric and shape information. The multi-agent view can significantly improve the shortcoming of low resolution in 4D radar. This empowers 4D radar with the ability to supplement the richer geometric and shape information of objects for LiDAR. However, there is a lack of 4D radar data in the current cooperative perception dataset. Therefore, we present V2X-R, the first simulated cooperative 3D Object Detection V2X dataset that not only includes LiDAR, cameras, but also 4D radar data. V2X-R contains 12,079 scenarios with 37,727 frames of LiDAR and 4D radar point clouds, 150,908 images, and 170,859 annotated 3D vehicle bounding boxes. Based on the V2X-R dataset, we further develop a general cooperative LiDAR-4D radar fusion pipeline for 3D object detection. The entire pipeline explicitly consists of four stages: 1) Encode by each agent. 2) Agent fusion. 3) Modal fusion. 4) Box prediction. We propose a novel Multi-modal Diffusion Denoising (MDD) module in the modal fusion stage of the pipeline to accommodate the noisy LiDAR features challenge after the agent fusion stage. MDD utilizes weather-robust 4D radar feature as a condition to prompt the diffusion model to denoise noisy LiDAR features. Notably, our MDD module barely disrupts performance in normal weather, attaining high performance in both normal and adverse weather. Subsequently, we implement the cooperative LiDAR-4D radar fusion pipeline with various agent fusion and modal fusion strategies on our V2X-R dataset, establishing a comprehensive benchmark for cooperative 3D object detection. The comprehensive experiment results show that the LiDAR-4D radar fusion demonstrates superior performance based on various model architectures, as shown in Fig. 2(a). The effectiveness of our designed MDD module is also validated under noise-prone foggy and snowy weather. As shown in Fig. 2(b), by incorporating our MDD module, AttFuse [53] has significantly improved the weather-robustness ability. Our contributions can be summarized in three key points: • We present V2X-R, the first simulated V2X dataset that not only includes LiDAR, cameras, but also 4D radar data. This dataset lays the data foundation for research in V2X cooperative perception with 4D radar. • We designed a novel Multi-modal Diffusion Denoising (MDD) module to utilize weather-robust 4D radar feature to handle weather-induced noisy LiDAR features. Our MDD shows effectiveness in both single and multi-agent, simulated and real scenarios. • We construct a cooperative LiDAR-4D radar fusion pipeline for 3D object detection. We have implemented this pipeline with various fusion strategies and provided a comprehensive benchmark on our V2X-R dataset, boosting the research in cooperative LiDAR-4D radar fusion for cooperative 3D object detection. Figure 2: The performance of different methods in our V2X-R dataset. (a) Performance comparison of different modalities (L and 4DR represent LiDAR and 4D radar modality, respectively). (b) Performance comparison of Attfuse [53] model (with and without our MDD module) under different weather conditions."
https://arxiv.org/html/2411.08395v1,MambaXCTrack: Mamba-based Tracker with SSM Cross-correlation and Motion Prompt for Ultrasound Needle Tracking,"Ultrasound (US)-guided needle insertion is widely employed in percutaneous interventions. However, providing feedback on the needle tip position via US image presents challenges due to noise, artifacts, and the thin imaging plane of US, which degrades needle features and leads to intermittent tip visibility. In this paper, a Mamba-based US needle tracker MambaXCTrack utilizing structured state space models cross-correlation (SSMX-Corr) and implicit motion prompt is proposed, which is the first application of Mamba in US needle tracking. The SSMX-Corr enhances cross-correlation by long-range modeling and global searching of distant semantic features between template and search maps, benefiting the tracking under noise and artifacts by implicitly learning potential distant semantic cues. By combining with cross-map interleaved scan (CIS), local pixel-wise interaction with positional inductive bias can also be introduced to SSMX-Corr. The implicit low-level motion descriptor is proposed as a non-visual prompt to enhance tracking robustness, addressing the intermittent tip visibility problem. Extensive experiments on a dataset with motorized needle insertion in both phantom and tissue samples demonstrate that the proposed tracker outperforms other state-of-the-art trackers while ablation studies further highlight the effectiveness of each proposed tracking module.","I INTRODUCTION In various percutaneous intervention procedures, ultrasound (US)-guided needle insertion is commonly adopted in minimally invasive interventions, such as tissue biopsy, tumor ablation, regional anesthesia [1], etc. As a non-invasive, portable, safe, and cost-effective imaging modality [2], US provides real-time intraoperative imaging of the needle and tissue, thereby minimizing the risk of accidental injury to vessels or critical organs. Despite these advantages, US imaging has inherent limitations, including low resolution and susceptibility to noise and artifacts [1], which can obscure or distort the needle tip position. Furthermore, under the narrow US imaging plane [2], small structures, such as the needle tip, can disappear intermittently when they are obscured by anatomical structures or not co-planar with the US imaging plane. These challenges underscore the necessity for robust and accurate needle tracking to ensure successful insertion procedures under challenging environments. Prior to the widespread adoption of learning-based needle trackers, traditional methods, such as the statistical filter [3] and Gabor filter [4], achieved somewhat satisfactory performance but were hindered by complex workflows and sensitivity to hyper-parameters, failing to address challenging environmental factors in US imaging. A method based on discriminative correlation filter (DCF) has been proposed in [5], but correlation filter-based methods can be susceptible to background distraction and image distortion [6]. Later, DCF [5] is integrated with an optical tracking system for higher accuracy [7], but it is constrained by the cumbersome deployment of the optical tracking system. Recently, deep learning methods based on convolutional neural network (CNN) and transformer [8] have gained popularity in tracking tasks [9, 10, 11, 12], including US needle tracking. Mwikirize et. al. proposed a US needle tracker with a two-step structure based on a fully convolutional network and region-based CNN [13]. A paradigm utilizing digital subtraction is proposed in [14], which augments tip features to enhance visibility prior to tracking. However, these two methods have non-end-to-end structures that require multiple steps to localize the needle tip, potentially affecting robustness and accuracy. Since the needle tip is often obscured or distorted under artifacts and noise, some other methods perform needle shaft segmentation before localizing the needle tip [15, 16]. Although segmentation mask of the needle shaft can provide useful cues on the axial position of needle tip, accumulative error and discrepancy can be accidentally introduced by this two-stage workflow, and again constrained by the disadvantages of non-end-to-end methods. Additional segmentation mask data is also required to train these segmentation models, leading to additional obstacles for the model deployment. As an effective structure, cross-correlation (X-Corr) has been widely adopted in end-to-end learning-based trackers. It measures the similarity between a reference template and a search region by performing convolution with sliding windows. The target location is then obtained from the induced similarity score. Following this diagram, many trackers based on X-Corr have been proposed [10, 11, 17, 18]. However, the existing convolutional X-Corr has a limited modeling range constrained by the kernel size. It cannot learn distant semantic features (e.g. needle shaft) which are important for US needle tracking, since local information can unpredictably become unreliable due to degradation by noise and artifacts. In addition to the challenging environment with noise and artifacts, the intermittently visible needle tip poses another problem that hinders accurate needle tracking. This issue can be caused by deviation of US imaging plane or obstruction by anatomical structures [19]. Mwikirize et. al. proposed a single-shot needle tracker by integrating historical frames to enhance needle tip features, addressing scenarios where the tip is imperceptible or the shaft is invisible [20]. Although it integrates historical information, its feature pre-enhancement can unexpectedly shift the original latent features, causing potential inherent information loss. Integrating historical needle motion into visual tracking presents another approach, since motion information can serve as an effective non-visual prompt to prevent tracking failure when the needle tip is invisible. A motion prediction module is integrated with a visual tracker in [6], yet it is constrained by its explicit motion prediction that poses challenges on generalizability when encountering motion from unseen domains. Mamba [21, 22] has recently drawn considerable attention and is being applied in real-world tracking tasks [23, 24, 25]. Based on the structured state space models (SSMs) [26], Mamba has a computationally efficient long-range lossless modeling capability with its selective scan mechanism. This capability allows Mamba-based trackers by leveraging long-range information, such as aggregating a video-level template set [24] or integrating historical frames [23]. It should be noted that a transformer-based tracker can hardly achieve long-range modeling under similar model size and complexity since the transformer has quadratic time complexity and memory requirement. It also requires positional encoding that may not effectively capture lossless long-range dependencies compared to Mamba which is designed specifically for such tasks. Thus Mamba usually outperforms transformer-based methods under similar model size and complexity [21]. Since no Mamba-based tracker has yet been proposed for US needle tracking, developing an effective Mamba-based needle tracker under US imaging remains an open research area. To address the aforementioned challenges of noise, artifacts, and intermittent visibility in US needle tracking, in this paper, a Mamba-based US needle tracker MambaXCTrack utilizing SSM cross-correlation (SSMX-Corr) and implicit motion prompt is proposed. To the best of our knowledge, it is the first time a Mamba-based tracker has been adopted in US needle tracking. It is also the first time that cross-correlation is implemented with SSM. Leveraging the long-range modeling capability of SSMs, SSMX-Corr enables global search and long-range modeling of distant semantic features. When the tip feature is degraded by noise and artifacts, SSMX-Corr avoids tracking failure by implicitly learning distant semantic feature that potentially comes from visual cues like needle shaft, rather than by explicitly performing segmentation on the needle shaft like existing methods [15, 16]. By further integrating the proposed cross-map interleaved scan (CIS), SSMX-Corr enjoys global search without losing local pixel-wise interaction between search and template maps to keep positional inductive bias, while existing convolutional X-Corr models [9, 10] only consider local modeling. To address the intermittent visibility of the needle tip, the implicit low-level motion descriptor is introduced as a non-visual prompt in addition to visual features that can unpredictably become unreliable. Different from [6] that trains an external motion predictor to explicitly predict the future motion, the proposed workflow preprocesses motion to obtain the low-level motion descriptor, which is then implicitly integrated with visual features. This implicit low-level motion integration introduces image-agnostic raw motion and ensures an end-to-end network to enhance training stability and tracking robustness. Extensive evaluations on a dataset of motorized needle insertions in both phantom and animal tissue demonstrate MambaXCTrack’s superior performance compared to state-of-the-art methods. The main contributions are fourfold: • SSMX-Corr improves existing cross-correlation with SSM by globally searching and long-range modeling distant semantic features, thus learning potential distant visual cues. This represents the first effort to adopt Mamba effectively for US needle tracking. • CIS is proposed to provide SSMX-Corr with local pixel-wise interaction and positional inductive bias in addition to global search to enhance overall tracking performance. • Implicit low-level motion descriptor is adopted to provide a non-visual prompt, thus leveraging motion information to address the challenge of intermittent needle visibility to achieve robust and consistent tracking. • The proposed tracker achieves SOTA performance on both phantom and tissue experiments. Further ablation studies show the effectiveness of the proposed modules."
https://arxiv.org/html/2411.08380v1,"EgoVid-5M: A Large-Scale Video-Action Dataset 
for Egocentric Video Generation","Video generation has emerged as a promising tool for world simulation, leveraging visual data to replicate real-world environments. Within this context, egocentric video generation, which centers on the human perspective, holds significant potential for enhancing applications in virtual reality, augmented reality, and gaming. However, the generation of egocentric videos presents substantial challenges due to the dynamic nature of egocentric viewpoints, the intricate diversity of actions, and the complex variety of scenes encountered. Existing datasets are inadequate for addressing these challenges effectively. To bridge this gap, we present EgoVid-5M, the first high-quality dataset specifically curated for egocentric video generation. EgoVid-5M encompasses 5 million egocentric video clips and is enriched with detailed action annotations, including fine-grained kinematic control and high-level textual descriptions. To ensure the integrity and usability of the dataset, we implement a sophisticated data cleaning pipeline designed to maintain frame consistency, action coherence, and motion smoothness under egocentric conditions. Furthermore, we introduce EgoDreamer, which is capable of generating egocentric videos driven simultaneously by action descriptions and kinematic control signals. The EgoVid-5M dataset, associated action annotations, and all data cleansing metadata will be released for the advancement of research in egocentric video generation.","One of the most promising avenues in video generation is the development of world simulators. These systems utilize visual simulations and interactions to deliver applications in the physical world. Contemporary research is increasingly validating the capabilities of video generation in this realm, including applications in autonomous driving [58, 25, 79, 60, 78, 71], autonomous agents [72, 82, 64, 18, 10, 19, 5], and even in general world [4, 13]. In the context of human-centric scenarios, leveraging behavioral actions to drive egocentric video generation has emerged as a pivotal strategy. This approach greatly enhances applications in Virtual Reality (VR), Augmented Reality (AR), and gaming, offering more immersive and interactive experiences and advancing the state of the art in these fields. Video generation necessitates vast quantities of high-quality data for training. This requirement is even more stringent in egocentric video generation, which is inherently challenging due to the dynamic nature of egocentric perspectives, the richness of observed actions, and the diversity of encountered scenarios. Despite the critical need for specialized data, there is currently a scarcity of publicly available, large-scale datasets for training egocentric video generation models. To bridge this gap, we present the EgoVid-5M dataset, a pioneering high-quality dataset specifically curated for egocentric video generation (see Fig. 1). As shown in Tab. 1, EgoVid-5M is distinguished by several key features: (1) High Quality: This dataset offers 5 million egocentric videos at 1080p resolution. In contrast to Ego4D [16], which is intended for egocentric perception and includes excessive noisy camera motion that is unsuitable for generative training, EgoVid-5M undergoes a rigorous data cleaning process. The videos are curated based on stringent criteria, including the alignment between action descriptions and video content, the magnitude of motion, and the consistency between frames. (2) Comprehensive Scene Coverage: EgoVid-5M boasts a comprehensive range of scenarios including household environments, outdoor settings, office activities, sports, and skilled operations. It encompasses hundreds of action categories, thus covering the majority of scenes encountered in egocentric perspectives. (3) Detailed and Precise Annotations: The dataset includes extensive behavioral annotations, which are categorized into fine-grained kinematic control and high-level action descriptions. For kinematic information, we employ Visual Inertial Odometry (VIO) to provide precise annotations, ensuring accurate alignment with video contents. For action descriptions, a multimodal large language model combined with a large language model is utilized to generate detailed text annotations. Figure 2: Data annotation pipeline and cleansing metadata of EgoVid-5M. Leveraging the proposed EgoVid-5M dataset, we train different video generation baselines to validate the dataset’s quality and efficacy. Various architectures, such as U-Net [3, 65] and DiT [30], are employed as baseline models, and the experimental results demonstrate that EgoVid-5M significantly bolsters the training of egocentric video generation. In addition, we propose EgoDreamer, which utilizes both action descriptions and kinematic control to drive the generation of egocentric videos. To provide a comprehensive assessment of action-driven egocentric video generation, we establish an extensive set of evaluation metrics. These metrics encompass multiple dimensions, including visual quality, frame coherence, semantic compliance with actions, and kinematic accuracy. Extensive experiments show that EgoVid-5M markedly enhances the capability of various video generation models to produce high-quality egocentric videos. The main contributions of this paper can be summarized as follows: (1) We introduce EgoVid-5M, the first publicly released, high-quality dataset tailored for egocentric video generation. This dataset is proposed to advance both research and applications in the domain of egocentric visual simulation. (2) Our dataset includes detailed and precise action annotations, incorporating both fine-grained kinematic control and high-level textual descriptions. In addition, we employ robust data cleaning strategies to ensure frame consistency, action coherence, and motion smoothness within EgoVid-5M. (3) Utilizing EgoVid-5M, we conducted extensive experiments on various video generation baselines to validate the dataset’s quality and efficacy. Furthermore, to support future advancements in action-driven egocentric video generation, we propose EgoDreamer, which leverages both action descriptions and kinematic control to drive egocentric video generation."
https://arxiv.org/html/2411.08371v1,"Multiscale Graph Construction Using
Non-local Cluster Features","This paper presents a multiscale graph construction method using both graph and signal features. Multiscale graph is a hierarchical representation of the graph, where a node at each level indicates a cluster in a finer resolution. To obtain the hierarchical clusters, existing methods often use graph clustering; however, they may ignore signal variations. As a result, these methods could fail to detect the clusters having similar features on nodes. In this paper, we consider graph and node-wise features simultaneously for multiscale clustering of a graph. With given clusters of the graph, the clusters are merged hierarchically in three steps: 1) Feature vectors in the clusters are extracted. 2) Similarities among cluster features are calculated using optimal transport. 3) A variable k-nearest neighbor graph (VkNNG) is constructed and graph spectral clustering is applied to the VkNNG to obtain clusters at a coarser scale. Additionally, the multiscale graph in this paper has non-local characteristics: Nodes with similar features are merged even if they are spatially separated. In experiments on multiscale image and point cloud segmentation, we demonstrate the effectiveness of the proposed method.","Graph signal processing (GSP) is a powerful tool for analyzing signals distributed irregularly in space [1]. Graph signals are defined as signals on a network. Examples of graph signals are data measured by physical and physiological sensors, LiDAR, and radar, to name a few [2, 3]. In many cases, a graph often forms several clusters and each cluster has nodes having similar features, and therefore, graph signals exhibit a piecewise smooth variation in the clusters. For example, physical sensor networks typically form clusters because of the discontinuity and spatial distance of the measured environment. This leads to a requirement for analyzing graph signals with several spatial resolutions to detect clusters correctly. Multiscale graph is useful for representing a graph with several spatial resolutions [4]. A multiscale graph has a hierarchical structure where a node in a coarser scale graph corresponds to a cluster in a finer one. It is needed in many GSP applications, including multiresolution analysis of graphs, graph filter bank designs, and graph neural networks [5, 6]. To create a multiscale graph from a given graph, existing methods usually use spectral clustering of nodes in each level [7]. It estimates clusters so that the sum of the cut (total edge weights among clusters) is minimized based on some criteria. However, these methods ignore signal variations of the nodes/clusters. As a result, they could not detect cluster boundaries associated with signal variations appropriately. Furthermore, as shown in Fig. 1, existing methods may not connect non-local clusters because these methods only consider spatial relationships. This implies that they cannot structurally detect semantically similar clusters. To tackle the above-mentioned issues, in this paper, we propose a multiscale graph clustering method by considering graph and node-wise features simultaneously. Our method takes into account not only the spatial characteristics of the graph but also the signal variation for the multiscale graph construction. For given clusters of the graph at the finest resolution, we hierarchically merge the clusters in three steps: 1) K-means clustering of features in each cluster is performed to obtain sub-clusters, and their K centroids are extracted as representative feature vectors in the corresponding clusters. 2) Similarities/proximities among clusters are evaluated based on optimal transport [8]. 3) We construct a variable k-nearest neighbor graph (VkNNG) [9] based on the similarities among clusters. Finally, graph spectral clustering [10] is applied to the VkNNG and obtains coarser-level clusters. As illustrated in Fig. 1, our multiscale graph construction can connect non-local clusters when their features are similar to each other. In experiments on multiscale image and point cloud segmentation, the proposed method exhibits a comparable performance to existing single-scale methods specifically designed for image processing while we preserve the boundaries of the original clusters in the finest level and connect non-local regions. Notation: We summarize the important notations used throughout the paper in Table 1. (a) Conventional multiscale graph. (b) Proposed multiscale graph. Fig. 1: Comparison of multiscale graphs. Our method can connect non-local clusters, while the conventional one cannot do so."
https://arxiv.org/html/2411.08334v1,Enhancing Multimodal Query Representation via Visual Dialogues for End-to-End Knowledge Retrieval,"Existing multimodal retrieval systems often rely on disjointed models for image comprehension, such as object detectors and caption generators, leading to cumbersome implementations and training processes. To overcome this limitation, we propose an end-to-end retrieval system, Ret-XKnow, to endow a text retriever with the ability to understand multimodal queries via dynamic modality interaction. Ret-XKnow leverages a partial convolution mechanism to focus on visual information relevant to the given textual query, thereby enhancing multimodal query representations. To effectively learn multimodal interaction, we also introduce the Visual Dialogue-to-Retrieval (ViD2R) dataset automatically constructed from visual dialogue datasets. Our dataset construction process ensures that the dialogues are transformed into suitable information retrieval tasks using a text retriever. We demonstrate that our approach not only significantly improves retrieval performance in zero-shot settings but also achieves substantial improvements in fine-tuning scenarios. Our code is publicly available: https://github.com/yeongjoonJu/Ret˙XKnow.","With the growing demand for information retrieval across diverse applications, such as internet search and knowledge-based question answering, precise and efficient retrieval from multimodal queries involving pairs of images and text has emerged as a critical challenge. In such multimodal queries, each modality independently provides insufficient information to retrieve the desired passages within a knowledge base, necessitating the integrated understanding of the visual and textual queries. Existing Vision-Language (VL) retrievers (Qu et al. 2021; Luo et al. 2021; Gao et al. 2022; Lin et al. 2023) often depend on disjointed models for object detection or image captioning to provide visual information. The reliance on disjointed models complicates the training process (e.g., the models should be fine-tuned for separate tasks in domain adaptation) and increases the likelihood of propagating erroneous predictions. The utilization of the captioning model also lacks the fine-grained information embedded within the images. Previous approaches (Lin et al. 2023; Luo et al. 2023) have attempted to address these drawbacks. However, as shown in Fig. 1, they result in lower performance in a zero-shot setting than a text retriever that does not use image information despite their pre-training for the image-text alignment. Lin et al. (2023) introduce token-level embeddings and utilize two types of visual representations: textual description of the image and feature-based visual embeddings with regions of interest by an object detector. They pre-train the retriever to map token-level visual embeddings into the linguistic space of a text retriever and then fine-tune it by adding image captions to the textual queries. Such the retriever captures fine-grained features of the image by employing visual embeddings with captions. They also facilitate modality interaction between the textual query and the image by relying on textual information, but the mechanism also results in complex implementations and inefficient retrieval due to multiple steps. Figure 1: Zero-shot retrieval performance on OK-VQA (Google Search). Ret-XKnow outperforms the text-based retriever, while other multimodal retrievers fall short, relying on the textual query in the pre-training stage. Luo et al. (2023) present an end-to-end approach that projects multimodal features encoded via self-attention into linguistic space with a pre-training task called VL-ICT, to detach the dependency on the disjointed modules. They automatically construct a pre-training dataset by applying the Inverse Cloze Task (ICT) (Lee, Chang, and Toutanova 2019) to a multimodal knowledge base. However, this approach has significant limitations. First, the dataset does not adequately reflect the variety and complexity of real-world queries, as it only removes the title or caption from a sentence extracted as the query without considering the image. Second, in the constructed pairs of a multimodal query and the corresponding passage, the passage can often be matched solely with the textual content of the query. This occurs because the target passage is selected from the content following a sentence with a title or caption, thereby hindering learning rich image representations. To tackle these issues, we propose two approaches: (1) an end-to-end Retriever to eXpand visual Knowledge, Ret-XKnow, and (2) a Visual Dialogue-to-Retrieval (ViD2R) pre-training dataset constructed from visual dialogues containing distinct relevant passages for various queries related to the same image. Ret-XKnow endows a text retriever with the understanding of multimodal queries in the context of efficient information retrieval, inspired by the concept of partial convolutions (Liu et al. 2018), which fill undesired pixels with surrounding pixel information. We compress visual embeddings to focus on the visual information relevant to the textual query by leveraging the relevance scores between visual embeddings and textual query representations as an adaptive mask. We only attach a vision encoder to the text retriever with only a few layers, utilizing output embeddings of the penultimate layer in the vision model for fine-grained visual representations. Our model architecture does not allow the direct intervention of textual query features in the pre-training stage, achieving modality interaction without fusing text features with image features. Through this architecture, we introduce both the late-interaction mechanism (Khattab and Zaharia 2020) for pre-indexing documents and the modality interaction without requiring an additional document encoder and disjointed models. Recent advances in multimodal language models have produced several multimodal dialogue datasets (Zhu et al. 2023; Liu et al. 2023; Wang et al. 2023; Huang et al. 2023) for training models to perform tasks based on visual content. These datasets consist of multi-turn sessions with query-response pairs centered around a single image, providing precise and comprehensive information pertinent to the query and image. The response with detailed information can improve multimodal retrieval tasks by linking image understanding with complex textual queries. Whereas, such datasets are not appropriate for directly training retrievers due to the gap between explicit responses and broader passages. To bridge this gap, we transform the visual dialogue datasets into a format suitable for retrieval tasks through three simple steps: pre-processing, neural filtering, and response-to-passage conversion. Our construction process is applicable in diverse domains and modalities since our approach only requires multimodal dialogue datasets and sets of documents related to the target domain. Our retriever, Ret-XKnow pre-trained with the ViD2R dataset, outperforms various baselines in zero-shot retrieval performance across four multimodal datasets in an end-to-end manner. Furthermore, we demonstrate that the pre-training dataset curated via our construction method effectively mitigates the issue of overlooking visual features during the pre-training stage, leading to remarkable performance in fine-tuning settings. Our contributions are summarized as follows: • We propose Ret-XKnow, an end-to-end multimodal retriever that overcomes the limitations of disjointed models by dynamically focusing on visual features relevant to the textual query. • We introduce the ViD2R dataset, which transforms visual dialogue datasets into a format suitable for training VL retrievers, leading to significant improvements in zero-shot retrieval performance. • We demonstrate the comprehensive adaptability of Ret-XKnow by fine-tuning three downstream tasks. Our end-to-end retriever even shows comparable performance on baseline methods utilizing image captioning."
https://arxiv.org/html/2411.08333v1,SASE: A Searching Architecture for Squeeze and Excitation Operations,"In the past few years, channel-wise and spatial-wise attention blocks have been widely adopted as supplementary modules in deep neural networks, enhancing network representational abilities while introducing low complexity. Most attention modules follow a squeeze-and-excitation paradigm. However, to design such attention modules, requires a substantial amount of experiments and computational resources. Neural Architecture Search (NAS), meanwhile, is able to automate the design of neural networks and spares the numerous experiments required for an optimal architecture. This motivates us to design a search architecture that can automatically find near-optimal attention modules through NAS. We propose SASE, a Searching Architecture for Squeeze and Excitation operations, to form a plug-and-play attention block by searching within certain search space. The search space is separated into 4 different sets, each corresponds to the squeeze or excitation operation along the channel or spatial dimension. Additionally, the search sets include not only existing attention blocks but also other operations that have not been utilized in attention mechanisms before. To the best of our knowledge, SASE is the first attempt to subdivide the attention search space and search for architectures beyond currently known attention modules. The searched attention module is tested with extensive experiments across a range of visual tasks. Experimental results indicate that visual backbone networks (ResNet-50/101) using the SASE attention module achieved the best performance compared to those using the current state-of-the-art attention modules. Codes are included in the supplementary material, and they will be made public later.","Attention mechanisms are a crucial technique in computer vision, mimicking how the human visual system focuses when observing a scene. They help models process information more effectively, thereby improve performance. Various attention modules have been designed and applied to a wide range of tasks. Self-attention (Vaswani et al. 2017; Cao et al. 2019; Liu et al. 2021; Han et al. 2023) allows models to capture global information by broadening their receptive fields and has been widely adopted in convolutional neural networks (CNNs) and transformers due to its extraordinary representational capacity. Soft and hard attention (Xu et al. 2015) can help visualize where model actually ”focus on” when performing image captioning. Channel-wise and spatial-wise attention (Hu, Shen, and Sun 2018; Ruan et al. 2021; Li, Li, and Wen 2024; Jiang et al. 2024) adopt a squeeze-and-excitation paradigm that extracts features through a squeeze operation and produces an attention map through an excitation operation. Enhancing model performance with minimal additional parameters makes them common auxiliary modules for CNNs. The above attention modules are typically developed based on expert experience and require multiple experimental attempts, which can be labor-intensive. Furthermore, given that most squeeze-and-excitation-style attention blocks follow similar design patterns, we would like to explore the possibility of designing such modules in a more automatic fashion, and NAS (Lu et al. 2023) is a powerful tool to accomplish this objective. Several works also focused on developing attention blocks using NAS. Ma et al. (Ma et al. 2020) created a search space comprising existing attention blocks such as the SE block (Hu, Shen, and Sun 2018) and CBAM (Woo et al. 2018). They formed a fused attention module by applying differentiable architecture search (DARTS) (Liu, Simonyan, and Yang 2018) to combine the attention sub-blocks. Nakai et al. (Nakai, Matsubara, and Uehara 2020) adopted a similar approach by expanding a DARTS-like search space with well-known attention modules. They searched for an architecture that incorporated these attention blocks along with different convolution and pooling operations. The above two methods demonstrated that a well-searched combination of different attention modules can indeed outperform its individual components. However, the search space of existing algorithms are limited to off-the-shelf attention blocks. Even the best searched architectures are merely combinations of existing attention sub-modules. This inflexible search space restricts the capability of the entire search architecture, preventing the exploration of diverse potential solutions and thus limiting the performance of the searched attention module. To address this shortcoming, this paper proposes SASE: a Searching Architecture for Squeeze and Excitation operations. In SASE, squeeze and excitation operations in the channel and spatial dimensions are searched separately in four different sets. Operations within each set can be drawn from a diverse range of sources, including both existing attention blocks and novel techniques. These encompass squeeze or excitation operations from existing attention mechanisms (such as global average pooling from SE-Net (Hu, Shen, and Sun 2018), second-order pooling from GSoP-Net (Gao et al. 2019), and one-dimensional convolution from ECA-Net (Wang et al. 2020)), as well as operations not previously utilized in attention before (like L_{p}-pooling (Sermanet, Chintala, and LeCun 2012) and rank-based pooling (Shi, Ye, and Wu 2016; Dong et al. 2017)). Searching within our proposed search space yields possible squeeze-and-excitation combinations distinct from existing attention modules. Additionally, SASE is easily scalable by incorporating new squeeze or excitation operations into each search set. We summarize the main contributions of our work as follows: • To enable a more fine-grained searching, we designed four distinct operation sets instead of one search set: channel-wise squeezing, channel-wise excitation, spatial-wise squeezing, and spatial-wise excitation. Additionally, we modified certain operations within each set to ensure consistent output dimensions. • We constructed a Directed Acyclic Graph (DAG) tailored to the commonly used squeeze-and-excitation paradigm as the search starting point, with each edge corresponding to one of the four operation sets. And, we applied second-order DARTS to the customized DAG for an efficient search. • To validate the effectiveness of SASE, the searched attention module was integrated into ResNet-50 and ResNet-101 backbones and evaluated on various vision benchmarks: COCO benchmarks for object detection and instance segmentation, as well as the ImageNet-1K classification benchmark. Results indicate that visual backbone using the SASE attention module achieved the best overall performance compared to those using the current state-of-the-art attention modules."
https://arxiv.org/html/2411.08328v1,MVideo: Motion Control for Enhanced Complex Action Video Generation,"Existing text-to-video (T2V) models often struggle with generating videos with sufficiently pronounced or complex actions. A key limitation lies in the text prompt’s inability to precisely convey intricate motion details. To address this, we propose a novel framework, MVideo, designed to produce long-duration videos with precise, fluid actions. MVideo overcomes the limitations of text prompts by incorporating mask sequences as an additional motion condition input, providing a clearer, more accurate representation of intended actions. Leveraging foundational vision models such as GroundingDINO and SAM2, MVideo automatically generates mask sequences, enhancing both efficiency and robustness. Our results demonstrate that, after training, MVideo effectively aligns text prompts with motion conditions to produce videos that simultaneously meet both criteria. This dual control mechanism allows for more dynamic video generation by enabling alterations to either the text prompt or motion condition independently, or both in tandem. Furthermore, MVideo supports motion condition editing and composition, facilitating the generation of videos with more complex actions. MVideo thus advances T2V motion generation, setting a strong benchmark for improved action depiction in current video diffusion models. Our project page is available at https://mvideo-v1.github.io/.","Despite significant progress in the field of image [7, 45, 5, 20, 15, 14, 27, 8] and video generation [10, 29, 19, 4, 36, 38, 31, 1, 3, 25, 47, 42, 28, 49, 18, 37, 21, 34, 33], current models still face substantial challenges when tasked with generating complex action videos. One of the core difficulties lies in the inability of text-based descriptions to fully capture the nuanced details of intricate movements and actions. This limitation significantly hinders the model’s ability to accurately train and infer complex action sequences in videos. Furthermore, these action videos often span longer durations, increasing the complexity of the generation process by requiring models to maintain temporal coherence across extended periods. In this work, we introduce MVideo, a novel framework specifically designed to tackle the inherent challenges of generating complex action videos. Traditional video generation models rely heavily on textual descriptions, which are often inadequate for conveying the dynamic intricacies of complex actions. To overcome this limitation, MVideo leverages mask sequences as an additional conditioning input. Unlike text prompts, mask sequences offer a more precise and explicit representation of the desired actions, allowing the model to generate videos that more accurately capture the intended movements. Thanks to advancements in foundational vision models, such as GroundingDINO [17] and SAM2 [23], the mask sequences can be extracted automatically, enhancing MVideo’s efficiency and robustness in action video generation. Existing video diffusion models can only generate a few seconds of video, which is insufficient for creating a complete and coherent motion sequence. To address this, MVideo proposes an efficient iterative video generation method that combines image conditions with low-resolution video conditions. This method reduces the computational cost of the model while maintaining temporal consistency, ensuring that even longer videos maintain coherent content and consistent action sequences throughout their duration. MVideo is fine-tuned from the existing video diffusion model CogvideoX [42]. However, through experimentation, we observed that simply adding the mask sequence condition during fine-tuning led to a noticeable decline in the model’s ability to align text prompts with video generation, as reflected in the reduced metrics for “overall consistency” and “imaging quality” shown in Table 4. To mitigate this issue, we propose a novel consistency loss during training, which distills the text-to-video generation expertise of CogvideoX into MVideo, ensuring that the model not only learns to align mask sequences but also preserves its original text-condition capabilities. Our experiments demonstrate that MVideo effectively aligns text prompts with motion conditions and generalizes to previously unseen mask sequences. This deep alignment capability enables MVideo to generate complex video effects. For instance, given a motion condition, MVideo allows modification of foreground objects or background scenes via text prompts and supports editing or combining motion conditions to produce more intricate actions. Additional examples are provided in the case study section 4.3. In summary, our contributions of MVideo are threefold: • We introduce MVideo, a novel framework that iteratively generates long-duration action videos by integrating additional motion conditions for precise motion control. • We show that MVideo generalizes effectively, aligning with unseen motion conditions and enabling complex video generation through motion condition editing and combination. • We validate MVideo’s effectiveness through quantitative and visual comparisons with state-of-the-art video diffusion methods."
https://arxiv.org/html/2411.08293v1,Choix d’un espace de représentation image adapté à la détection de réseaux routiers,"Ces dernières années, des algorithmes permettant de décomposer une image en ses composantes structures + textures ont vu le jour. Dans cet article, nous présentons une application de ce type de décomposition au problème de la détection de réseau routier en imagerie aérienne ou satellitaire. La chaîne algorithmique met en œuvre la décomposition d’image (dont nous utilisons une propriété particulière), une détection d’alignements issus de la théorie de la Gestalt et un raffinement des routes extraites par contours actifs statistiques.","Différents travaux, plus ou moins récents, se sont attelés au problème de la détection automatique de réseaux routiers en imagerie aérienne ou satellitaire ([5, 8]). Ces travaux ont plus particulièrement concerné les méthodes de détection et la modélisation même d’une route. Dans cet article, nous étudions la possibilité de disposer d’un espace de représentation de l’image qui soit mieux adapté en vue de faire la détection. Nous proposons d’utiliser l’espace de textures basé sur les méthodes de décomposition d’image développées ces dernières années autour des travaux de Y.Meyer ([9]). Après avoir rappelé le principe de la décomposition d’image, nous montrerons que la composante texture permet de réhausser les objets filiformes. La décomposition sera alors utilisée comme prétraitement avant l’application d’un algorithme de détection (nous proposons un algorithme bas niveau basé sur la méthode de détection d’alignements issue de la théorie de la Gestalt et des contours actifs statistiques)."
https://arxiv.org/html/2411.08292v1,"Noisy image decomposition: a new structure, texture and noise model based on local adaptivity","These last few years, image decomposition algorithms have been proposed to split an image into two parts: the structures and the textures. These algorithms are not adapted to the case of noisy images because the textures are corrupted by noise. In this paper, we propose a new model which decomposes an image into three parts (structures, textures and noise) based on a local regularization scheme. We compare our results with the recent work of Aujol and Chambolle. We finish by giving another model which combines the advantages of the two previous ones. Keywords: image decomposition, BV, texture, noise, oscillating functions, Besov spaces, local adaptivity.","In 2001, in [3], Y.Meyer investigated the model of image restoration proposed by Rudin-Osher-Fatemi [5]: F^{ROF}(u)=J(u)+(2\lambda)^{-1}\|f-u\|_{L^{2}}^{2} (1) where f is the measured image which implies an estimation: \hat{u}=\arg\inf_{u\in BV}F^{ROF}(u), (2) of the restored image. The main hypothesis is that u belongs to the space BV (the space of bounded variation functions). The quantity J(u) is a semi-norm on BV. It is also knowing as the total variation of the function u and can be expressed by J(u)=\int|\nabla u|. (3) If we write v=f-u and take the point of view of image decomposition (i.e f is composed of structures (u) and textures (v)), we can rewrite equation (1) as F^{ROF}(u)=J(u)+(2\lambda)^{-1}\|v\|_{L^{2}}^{2}. (4) Y.Meyer proved that this model rejects the oscillatory component of f which is considered to be the textured component. Then, he proposed a new model where the L^{2} norm of v is replaced by a norm in a space G close to the dual space of BV and which contains oscillatory functions. The new model introduced by Y.Meyer is then given by F^{YM}(u,v)=J(u)+(2\lambda)^{-1}\|v\|_{G}, (5) where the G-norm is defined by: Definition 1 For v=\partial_{1}g_{1}+\partial_{2}g_{2} where g_{1}\in L^{\infty}(\mathbb{R}^{2}),g_{2}\in L^{\infty}(\mathbb{R}^{2}), \|v\|_{G}=\inf_{g}\left\|\left(\left|g_{1}\right|^{2}+\left|g_{2}\right|^{2}% \right)^{\frac{1}{2}}\right\|_{L^{\infty}}. (6) Due to its non-linearity, the G-norm is difficult to compute numerically (see [3]). Then two approaches can be found in the litterature to run Meyer’s algorithm. The first one is the algorithm proposed by L.Vese and S.Osher in [2]. The authors use the following approximation: \forall v\in L^{\infty}\quad\|v\|_{L^{\infty}}=\lim_{p\rightarrow+\infty}\|v\|% _{L^{p}}, (7) and the property that any function v in G can be written as v=\textup{div\,}(g) where g=(g_{1},g_{2})\in(L^{\infty}\times L^{\infty}). Then they give a new formulation of the problem: F^{VO}(u,g)=J(u)+(2\lambda)^{-1}\|f-(u+\textup{div\,}\;g)\|_{L^{2}}^{2}+\mu% \left\|\sqrt{g_{1}^{2}+g_{2}^{2}}\right\|_{L^{p}}. (8) The authors seek for (\hat{u},\hat{g_{1}},\hat{g_{2}})=\arg\inf_{(u,g_{1},g_{2})\in(BV\times L^{% \infty}\times L^{\infty}}F^{VO}(u,g_{1},g_{2})) (9) The authors deduce the related partial differential equation system and its discrete formulation (see [2] for details). The drawback of their approach is the numerical stability of the algorithm. The second approach is proposed by Aujol et al. in [1, 11]. Their model is given by F^{AU}(u,v)=J(u)+J^{*}\left(\frac{v}{\mu}\right)+(2\lambda)^{-1}\|f-u-v\|_{L^{% 2}}^{2}, (10) (\hat{u},\hat{v})=\arg\inf_{(u,v)\in(BV\times G}F^{AU}(u,v)) (11) where G_{\mu}=\{v\in G/\|v\|_{G}\leqslant\mu\}, (12) and J^{*} is the adjoint of J; therefore it is the indicator function defined by J^{*}\left(\frac{v}{\mu}\right)=\begin{cases}&0\quad\quad\text{if}\;v\in G_{% \mu},\\ &+\infty\;\;\;\text{if}\;v\in G\backslash G_{\mu}.\end{cases} (13) The authors prove that the solution of minimizing (10) can be found by an iterative algorithm based on non-linear projectors P_{G_{\mu}} proposed by A.Chambolle (see [4] for details and for a convergence theorem). This algorithm gives good results but is of limited interest in the case of noisy images. Indeed, the noise can be viewed as a very highly oscillatory function (this means that noise is in G_{\mu}). Therefore the algorithm incorporates the noise in the texture component. Then the textures are corrupted by noise (see fig.1 for an example). In this paper, we propose to extend the two components model to a three components model, f=u+v+w, which discriminates between structures (u), textures (v) and noise (w). This work was initiated in [13]. Figure 1: Two part decomposition of noisy image, from left to right: noisy image, object part, texture part corrupted by noise. An outline of the paper is given now. In section 2, we describe our new algorithm based on a local regularization principle [7, 10]. We give some numerical aspects of the algorithm and show some results. In section 3 we recall the recent model proposed by Aujol and Chambolle [6] which also decomposes an image into three parts. A comparison between our approach and their model shows the advantages and disadvantages of each algorithm. In section 4, we propose another algorithm which incorporates the advantages of each the previous ones. We conclude and give some perspectives of this work in section 5."
https://arxiv.org/html/2411.08291v1,Restoration algorithms and system performance evaluation for active imagers,"This paper deals with two fields related to active imaging system. First, we begin to explore image processing algorithms to restore the artefacts like speckle, scintillation and image dancing caused by atmospheric turbulence. Next, we examine how to evaluate the performance of this kind of systems. To do this task, we propose a modified version of the german TRM3 metric which permits to get MTF-like measures. We use the database acquired during NATO-TG40 field trials to make our tests.","1 INTRODUCTION Since few years, active imaging systems appear as new imaging solutions. The fields of application are various: target recognition and identification, the search for people in non-cooperative zone, detection of optics,\ldots. The use of laser illumination permits to increase the identification range, the image resolution but some drawbacks also appear. Images are corrupted by different phenomenons like speckle, scintillation and image dancing (due to the laser propagation through the atmosphere). In this paper, we propose to explore two different fields in relation with active imaging. In the first part of this paper, we investigate the effect of atmosphere turbulence in the case of active imaging systems. The classical filter used in the litterature is the temporal mean filter. This filter gives good results but the edges of the objects are blurred. In order to improve the performance, we propose to use a temporal median filter. We show that, in a statistical point of view, the median filter is better adapted than the mean filter. Its main advantage is that it doesn’t create new values which never appear during the acquisition process. We make different tests on the database acquired by the NATO-TG40 group. In the case of high turbulence, these filters are not efficient to correct the image dancing phenomenon. Then we present a new algorithm based on warping technics which we currently work on. The first results seems to be very promising. In the second part of the paper, we propose a new method to evaluate the performance of this kind of systems. Our method use some ideas taken from the german TRM3 model. We define a new metric which uses directly the values measured at different locations in the image. This method permits to build a Modulation Transfert Function (MTF) like the one for passive systems. We conduct some experiments on the different systems used in the NATO-TG40 field trials and evaluate their performances. We end the paper by giving some conclusions and perspectives for the future."
https://arxiv.org/html/2411.08279v1,MBA-SLAM: Motion Blur Aware Dense Visual SLAM with Radiance Fields Representation,"Emerging 3D scene representations, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have demonstrated their effectiveness in Simultaneous Localization and Mapping (SLAM) for photo-realistic rendering, particularly when using high-quality video sequences as input. However, existing methods struggle with motion-blurred frames, which are common in real-world scenarios like low-light or long-exposure conditions. This often results in a significant reduction in both camera localization accuracy and map reconstruction quality. To address this challenge, we propose a dense visual SLAM pipeline (i.e. MBA-SLAM) to handle severe motion-blurred inputs. Our approach integrates an efficient motion blur-aware tracker with either neural radiance fields or Gaussian Splatting based mapper. By accurately modeling the physical image formation process of motion-blurred images, our method simultaneously learns 3D scene representation and estimates the cameras’ local trajectory during exposure time, enabling proactive compensation for motion blur caused by camera movement. In our experiments, we demonstrate that MBA-SLAM surpasses previous state-of-the-art methods in both camera localization and map reconstruction, showcasing superior performance across a range of datasets, including synthetic and real datasets featuring sharp images as well as those affected by motion blur, highlighting the versatility and robustness of our approach. Code: https://github.com/WU-CVGL/MBA-SLAM.","Simultaneous Localization and Mapping (SLAM) is a fundamental problem in 3D vision with broad applications, including autonomous driving, robotic navigation, and virtual reality. While traditional sparse SLAM methods [1, 2] use sparse point clouds for map reconstruction, recent learning-based dense SLAM systems [3, 4, 5, 6] focus on generating dense maps, which are essential for downstream applications. Due to their ability to enable photo-realistic 3D scene representations, Neural Radiance Fields (NeRF) [7] and 3D Gaussian Splatting (3DGS) [8] have been explored in conjunction with SLAM systems [9, 10, 11, 12, 13, 14, 15, 16, 17], demonstrating significant improvements in map representation and high-fidelity surface reconstruction. However, existing methods heavily rely on high-quality, sharp RGB-D inputs, which poses challenges when dealing with motion-blurred frames, often encountered in low-light or long-exposure conditions. Such conditions can significantly degrade the localization and mapping performance of these methods. The difficulties that motion-blurred images present to dense visual SLAM systems stem from two primary factors: 1) inaccurate pose estimation during tracking: current photo-realistic dense visual SLAM algorithms depend on sharp images to estimate camera poses by maximizing photometric consistency. However, motion-blurred images, commonly occurring in real-world scenarios, violate this assumption, making it difficult to accurately recover poses from blurred frames. These inaccurately tracked poses, in turn, affect the mapping process, leading to inconsistent multi-view geometry. 2) inconsistent multi-view geometry in mapping: the mismatched features between multi-view blurry images introduce erroneous 3D geometry information, resulting in poor 3D map reconstruction. This will degrade map reconstruction quality, which subsequently affects the tracking process. Combined these two factors, existing dense virtual SLAM systems would usually exhibit performance degradation when handling motion-blurred images. To address these challenges, we introduce MBA-SLAM, a photo-realistic dense RGB-D SLAM pipeline designed to handle motion-blurred inputs effectively. Our approach integrates the physical motion blur imaging process into both the tracking and mapping stages. Specifically, we employ a continuous motion model within the \mathbf{SE}(3) space to characterize the camera motion trajectory within exposure time. Given the typically short exposure duration, the trajectory of each motion-blurred image is represented by its initial and final poses at the start and end of the exposure time respectively. During tracking, we firstly render a reference sharp image corresponding to the latest keyframe, from our learned 3D scene representation. The rendered image can then be reblurred to match the current captured blurry image based on the predicted motion trajectory from previous optimization iteration. We enforce the photo-metric consistency between the tracked blurry image and the reblurred image to further refine the camera motion trajectory within exposure time. In the mapping stage, we jointly optimize the trajectories of a set of sparsely selected frames (i.e. keyframes) and the 3D scene representation by minimizing the photo-metric consistency loss. Two commonly used scene representations are explored in our implementation, i.e. implicit neural radiance fields [12] and explicit 3D Gaussian Splatting [8]. Both representations exhibit different advantages and disadvantages. In particular, NeRF-based implementation is able to achieve higher frame rates (FPS) but exhibits lower rendering quality than 3D-GS based implementation. In contrary, 3D-GS based implementation delivers better rendering quality at the expense of lower FPS. We present both implementations to satisfy the requirements of different usage scenarios. We evaluate the performance of MBA-SLAM thoroughly by using both the sharp and blurry datasets, against prior state-of-the-art methods. In particularly, we conducted evaluations with both a public synthetic blurry dataset [18] and a self-captured blurry dataset. The real dataset is collected with a RealSense RGB-D camera under low-lighting conditions. To further evaluate the performance of MBA-SLAM on sharp images, we exploit the commonly used public datasets from Replica [19], ScanNet [20] and TUM RGB-D [21]. The experimental results demonstrate that MBA-SLAM not only delivers more robust performance with blurry images, but also has superior performance with sharp images, than prior state-of-the-art methods. In summary, our contributions are as follows: • We present a novel photometric bundle adjustment formulation specifically designed for motion blurred images, establishing an RGB-D 3DGS/NeRF-based SLAM pipeline that demonstrates robustness against motion blur. • Our SLAM pipeline is enhanced by integrating a motion blur-aware tracker, resulting in improved tracking accuracy, which in turn leads to superior mapping performance. • We illustrate how this formulation enables the acquisition of precise camera trajectories and high-fidelity 3D scene maps from motion-blurred inputs. • Our experimental results demonstrate the superior tracking and mapping performance of MBA-SLAM across various datasets, outperforming previous state-of-the-art NeRF-based and 3DGS-based SLAM methods, including both synthetic and real motion blurred datasets. • Our method also performs well and surpasses previous state-of-the-art dense visual SLAM pipelines on commonly used standard datasets with sharp images. MBA-SLAM is based on three preliminary seminar papers of the authors, i.e., MBA-VO [18], BAD-NeRF [22], and BAD-Gaussians [23], which have been accepted by ICCV 2021 (oral), CVPR 2023, and ECCV 2024, respectively. In this paper, we extend these works in several significant ways: 1) we integrate them into a comprehensive SLAM pipeline, by exploiting the motion blur aware tracker from MBA-VO [18] and the motion blur aware bundle adjustment algorithm from either BAD-NeRF [22] or BAD-Gaussians [23]; 2) we replace the vanilla NeRF representation of BAD-NeRF [22] with a more efficient tri-plane based representation, significantly improving the training efficiency by a factor of 100 times; 3) all the experimental evaluations are newly conducted to thoroughly verify the effectiveness of the pipeline, against prior state-of-the-art methods."
https://arxiv.org/html/2411.08227v1,"DPU: Dynamic Prototype Updating for Multimodal 
Out-of-Distribution Detection","Out-of-distribution (OOD) detection is crucial for ensuring the robustness of machine learning models by identifying samples that deviate from the training distribution. While traditional OOD detection has predominantly focused on single-modality inputs, such as images, recent advancements in multimodal models have shown the potential of utilizing multiple modalities (e.g., video, optical flow, audio) to improve detection performance. However, existing approaches often neglect intra-class variability within in-distribution (ID) data, assuming that samples of the same class are perfectly cohesive and consistent. This assumption can lead to performance degradation, especially when prediction discrepancies are indiscriminately amplified across all samples. To address this issue, we propose Dynamic Prototype Updating (DPU), a novel plug-and-play framework for multimodal OOD detection that accounts for intra-class variations. Our method dynamically updates class center representations for each class by measuring the variance of similar samples within each batch, enabling tailored adjustments. This approach allows us to intensify prediction discrepancies based on the updated class centers, thereby enhancing the model’s robustness and generalization across different modalities. Extensive experiments on two tasks, five datasets, and nine base OOD algorithms demonstrate that DPU significantly improves OOD detection performances, setting a new state-of-the-art in multimodal OOD detection, including improvements up to 80\% in Far-OOD detection. To improve accessibility and reproducibility, our code is released at https://github.com/lili0415/DPU-OOD-Detection.","Out-of-distribution (OOD) detection aims to identify samples that differ from the in-distribution (ID) data in ways that challenge the model’s ability to generalize [16, 36, 14, 25, 2]. It is crucial for enhancing the safety and robustness of machine learning models across various domains, such as autonomous driving [8, 26], medical imaging [19], robotics [3, 9], and other applications [4, 50, 48, 28, 15, 21, 43, 12, 42]. In recent years, numerous OOD detection algorithms have been developed, spanning classification-based and distance-based methods [49, 30, 17, 31, 40, 7]. Traditionally, OOD detection has focused on single-modality inputs, such as images or videos. With the emergence of large Vision-Language Models [37, 52], researchers are exploring OOD detection with the assistance of language modalities [33, 46, 29]. However, their evaluations remain limited to benchmarks containing only images. Effectively leveraging multimodal features (e.g., video, optical flow, and audio) remains an open challenge, requiring further research. Figure 1: Performance of our DPU applied to four base OOD methods in the Multimodal Far-OOD Detection task (§4.2), using HMDB51 as the ID dataset and Kinetics600 as the OOD dataset. Red symbols denote the OOD methods enhanced by DPU, demonstrating that DPU significantly improves their performances. Current Work. Dong et al. [11] introduced the first multimodal OOD benchmark and framework, identifying the phenomenon of modality prediction discrepancy. This phenomenon reveals that softmax prediction discrepancies across different modalities are negligible for ID data but significant for OOD data. By amplifying this prediction discrepancy during training, they observed improvements in OOD detection performance. However, a key assumption in previous multimodal OOD detection studies is that all samples within a given class are entirely in-distribution [11, 49], implying perfect cohesion among samples within the same class. This assumption rarely holds true in real-world applications, where intra-class variability is common. As a result, applying uniform discrepancy intensification across all training samples can degrade the model’s ID prediction accuracy [11]. When the discrepancy is intensified on class-center samples—typically exhibit consistent predictions across all modalities—this consistency is disrupted, causing significant model confusion and performance degradation, as shown in Appx. C.4 case study. Our Proposal. To tackle the challenge of intra-class variations in existing multimodal OOD detection methods, we introduce a novel approach called Dynamic Prototype Updating (DPU). DPU dynamically adjusts the multimodal prediction discrepancy to ensure high intra-class cohesion and clear inter-class separation, leveraging instance-level training invariance. The core idea of DPU is to update the prototype representations [24, 27] of each class at a dynamic, sample-specific rate, resulting in more precise and robust model performance. These prototype representations act as central reference points, capturing the key features of each class (see Fig. 2 for an overview). To establish a reliable representation space, we first introduce the Cohesive-Separate Contrastive Training procedure (§3.3), which applies marginal contrastive learning to strengthen intra-class cohesion while preserving distinctions between individual samples. Building on this, we design the Dynamic Prototype Approximation mechanism (§3.4), which adaptively refines prototype representations based on observed sample variances. This adaptive updating helps mitigate the negative impact of outliers on prototype evolution, stabilizing the learning process. Using these refined prototypes, we further adjust the multimodal prediction discrepancy for each sample according to its similarity to its class prototype (§3.5). Finally, OOD models make predictions by leveraging both the joint probability distribution across all modalities and the distinct information from each modality. In summary, we make the following contributions: • New Observations in Multimodal OOD Detection. We are the first to identify and explore the negative impact of intra-class variations within ID data for OOD detection. • Novel, Model-Agnostic Framework. We propose DPU, a flexible, plug-and-play method that effectively handles intra-class variations and is compatible with various existing OOD detection models. As shown in Fig. 1, DPU enhances performance across various OOD methods. • Effectiveness. Comprehensive experiments demonstrate the effectiveness of the proposed method across two tasks, five datasets, and nine base OOD methods. DPU significantly improves the performance of all benchmark models, achieving new state-of-the-art results, including improvements of around 10% across all metrics for Near-OOD detection and up to 80% for Far-OOD detection."
https://arxiv.org/html/2411.08216v1,GTA: Global Tracklet Association for Multi-Object Tracking in Sports,"Multi-object tracking in sports scenarios has become one of the focal points in computer vision, experiencing significant advancements through the integration of deep learning techniques. Despite these breakthroughs, challenges remain, such as accurately re-identifying players upon re-entry into the scene and minimizing ID switches. In this paper, we propose an appearance-based global tracklet association algorithm designed to enhance tracking performance by splitting tracklets containing multiple identities and connecting tracklets seemingly from the same identity. This method can serve as a plug-and-play refinement tool for any multi-object tracker to further boost their performance. The proposed method achieved a new state-of-the-art performance on the SportsMOT dataset with HOTA score of 81.04%. Similarly, on the SoccerNet dataset, our method enhanced multiple trackers’ performance, consistently increasing the HOTA score from 79.41% to 83.11%. These significant and consistent improvements across different trackers and datasets underscore our proposed method’s potential impact on the application of sports player tracking. We open-source our project codebase at https://github.com/sjc042/gta-link.git.","In recent years, advancements in computer vision and deep learning have revolutionized sports analytics, offering unprecedented insights into player performance and strategy. For example, sports video understanding [6, 10], sports field registration [20, 13], and 2D/3D human pose estimation for sports [21]. Among these innovations, sports player tracking systems have emerged as a cornerstone, providing coaches and analysts with valuable data on player movements, positioning, and interactions during game play [12]. These systems have become integral to modern sports, enabling data-driven decision-making and performance optimization across various disciplines. However, despite significant progress in multi-object tracking technologies, challenges persist in accurately tracking players, including the irregular movements and similar appearances of sports players, and the lack of re-identification algorithm in handling re-entry situation after leaving camera field of view after certain amount of time. Current state-of-the-art on-line tracking algorithms often fail in long-term object re-identification with the aforementioned challenges. While these trackers perform well in controlled environments or short-term scenarios, they struggle to maintain consistent player identities throughout entire matches or when players re-enter the field after substantial absences. This limitation significantly impacts the accuracy and reliability of player performance analysis, tactical evaluations, and automated game statistics. Figure 1: Our proposed Global Tracklet Association (GTA) method significantly boosts the HOTA and IDF1 score of existing trackers, such as SORT, ByteTrack, and Deep-EIoU, on sports tracking datasets, including SportsMOT and SoccerNet. To address these persistent issues, our paper proposes an effective plug-and-play post-processing algorithm named Global Tracklet Association (GTA), designed specifically for sports player tracking applications. GTA aims to refine the tracking results of on-line or off-line trackers by improving long-term re-identification capabilities and handling the unique challenges posed by sports environments. By leveraging global temporal information and advanced association techniques, GTA enhances the robustness and accuracy of player tracking, potentially bridging the gap between current tracking technologies and the demanding requirements of professional sports analytics."
https://arxiv.org/html/2411.08196v1,Latent Space Disentanglement in Diffusion Transformers Enables Precise Zero-shot Semantic Editing,"Diffusion Transformers (DiTs) have recently achieved remarkable success in text-guided image generation. In image editing, DiTs project text and image inputs to a joint latent space, from which they decode and synthesize new images. However, it remains largely unexplored how multimodal information collectively forms this joint space and how they guide the semantics of the synthesized images. In this paper, we investigate the latent space of DiT models and uncover two key properties: First, DiT’s latent space is inherently semantically disentangled, where different semantic attributes can be controlled by specific editing directions. Second, consistent semantic editing requires utilizing the entire joint latent space, as neither encoded image nor text alone contains enough semantic information. We show that these editing directions can be obtained directly from text prompts, enabling precise semantic control without additional training or mask annotations. Based on these insights, we propose a simple yet effective Encode-Identify-Manipulate (EIM) framework for zero-shot fine-grained image editing. Specifically, we first encode both the given source image and the text prompt that describes the image, to obtain the joint latent embedding. Then, using our proposed Hessian Score Distillation Sampling (HSDS) method, we identify editing directions that control specific target attributes while preserving other image features. These directions are guided by text prompts and used to manipulate the latent embeddings. Moreover, we propose a new metric to quantify the disentanglement degree of the latent space of diffusion models. Extensive experiment results on our new curated benchmark dataset and analysis demonstrate DiT’s disentanglement properties and effectiveness of the EIM framework. 111Our annotated benchmark dataset is publicly available at https://anonymous.com/anonymous/EIM-Benchmark.","Diffusion models have achieved remarkable success in text-guided generation tasks, producing diverse, high-fidelity images and videos based on text prompts (Ma et al., 2024; Esser et al., 2024). These diffusion-based models are typically built on the UNet-based latent diffusion framework (Rombach et al., 2022), where input images are projected into latent embeddings and integrated with text embeddings through cross-attention layers within the UNet, as shown in Fig. 1(a.1). Recently, Diffusion Transformers (DiT) (Fig. 1(a.2)) introduced a new architecture that combines input image and text embeddings into a joint latent space and processes them through stacked self-attention layers. While empirical results suggest that this new structure enhances T2I controllability (Esser et al., 2024), it is unclear how multimodal information collectively forms this joint latent space and how these representations guide the semantics of the synthesized images. A key to achieving controllability is the property of semantic disentanglement, where semantic attributes are encoded into distinct subspaces that can be independently controlled along specific editing directions. In other words, we can precisely manipulate a semantic attribute without affecting the others. This property has not been clearly observed in the latent space constructed in the UNet-based diffusion models (Lu et al., 2024; Kwon et al., 2022). Consequently, existing approaches alternatively rely on either loss-driven methods to identify semantic control directions (Kim et al., 2022; Kwon et al., 2022; Avrahami et al., 2022; Mokady et al., 2023; Kawar et al., 2023) or attention maps to connect image semantics with text inputs (Hertz et al., 2022; Brooks et al., 2023). Note that these methods all require additional annotations (e.g., sub-image masks) or extensive optimization to achieve precise semantic control in image editing task, limiting their interpretability and generalizability in broader applications. In contrast, DiT’s self-attention architecture projects image and text inputs into a joint latent space, potentially enabling direct link between image semantics and text prompts. Given this novel attention mechanism and DiT’s remarkable text-to-image generation performance (Esser et al., 2024), we are motivated to delve into the fundamental research question: Is the joint latent space of DiT semantically disentangled? Figure 1: (a) UNet-based models align text embeddings with image embeddings via cross-attention layers. In contrast, DiT creates a joint latent space by combining the text embedding and image embedding, then feeds them into the denoising block. (b) DiT’s has semantic disentangled latent space, where intensities of image semantics in generated images are controlled by separate directions, which can be easily identified. By modifying image semantics in the joint latent space using directions from encoded text prompts or reference images, we discover two disentanglement properties: (1) Semantic attributes can be edited through specific directions in a controllable manner; (2) Modifying semantics along one controlling direction does not affect other attributes. These properties enable precise and fine-grained modification of specific semantics while preserving others, using easily identifiable editing directions. As shown in Fig. 1(b), given an input image and its text description, we can identify the controlling direction for a specific attribute (e.g., ‘smile’) by encoding its corresponding text prompt (e.g., the word ‘smile’). This enables precise control over the semantic intensity by manipulating the joint latent embedding along this direction. These properties enable DiT to serve as a prior knowledge for precise semantic editing. Based on this, we propose a simple yet effective Encode-Identify-Manipulate (EIM) framework for zero-shot image editing with precise and fine-grained semantic control, while not requiring additional training or mask annotations. Given an input image and target semantic, we first extract the image’s text description using a multi-modal LLM, then encode both image and text into a joint latent embedding. To precisely edit the target semantic, we introduce a Hessian-Score-Distillation-Sampling (HSDS) method that identifies disentangled editing directions guided by the target semantic’s text prompt, as well as the desired editing degree. The joint latent embedding is then linearly manipulated along this direction. To evaluate our approach, we propose a Semantic Disentanglement mEtric (SDE) for measuring latent space disentanglement in text-to-image diffusion models. We also introduce ZOPIE (Zero-shot Open-source Precise Image Editing), a new benchmark for comprehensive evaluation. Both automatic and human evaluations demonstrate our framework’s effectiveness in precise image editing tasks. Our contributions are summarized as follows: • We systematically studied and modeled the latent embedding space of DiT models, and propose a new metric to quantify its disentanglement. Our study for the first time comprehensively uncover this important disentanglement property of the DiT model, which set the foundation for controllable image editing with DiT models. • We propose a simple yet effective Extract-Indentify-Manipulation (EIM) method via Hessian Score Distillation Sampling method for zero-shot image editing with precise and fine-grained semantic control, without requiring any additional training or mask annotations. • We introduce a novel Zero-shot Open-source Precise Image Editing (ZOPIE) benchmark, with both automatic and human annotations to assess the effectiveness of precise image editing tasks, as well as the disentanglement property of generative models. Figure 2: The disentanglement properties of the joint latent space of DiT enable: (a) Given a target semantic to be edited, we can identify a direction that allows fine-grained control of the intensity of this semantic; (b) With a reference image that only differs in the target semantic, we can obtain such direction in the image embedding space and achieve similar precise control."
https://arxiv.org/html/2411.08195v1,An Explainable Machine Learning Approach for Age and Gender Estimation in Living Individuals Using Dental Biometrics,"Objectives: Age and gender estimation is crucial for various applications, including forensic investigations and anthropological studies. This research aims to develop a predictive system for age and gender estimation in living individuals, leveraging dental measurements such as Coronal Height (CH), Coronal Pulp Cavity Height (CPCH), and Tooth Coronal Index (TCI). Methods: Machine learning models were employed in our study, including Cat Boost Classifier (Catboost), Gradient Boosting Machine (GBM), Ada Boost Classifier (AdaBoost), Random Forest (RF), eXtreme Gradient Boosting (XGB), Light Gradient Boosting Machine (LGB), and Extra Trees Classifier (ETC), to analyze dental data from 862 living individuals (459 males and 403 females). Specifically, periapical radiographs from six teeth per individual were utilized, including premolars and molars from both maxillary and mandibular. A novel ensemble learning technique was developed, which uses multiple models each tailored to distinct dental metrics, to estimate age and gender accurately. Furthermore, an explainable AI model has been created utilizing SHAP, enabling dental experts to make judicious decisions based on comprehensible insight. Results: The RF and XGB models were particularly effective, yielding the highest F1 score for age and gender estimation. Notably, the XGB model showed a slightly better performance in age estimation, achieving an F1 score of 73.26%. A similar trend for the RF model was also observed in gender estimation, achieving a F1 score of 77.53%. Conclusions: This study marks a significant advancement in dental forensic methods, showcasing the potential of machine learning to automate age and gender estimation processes with improved accuracy. Clinical Significance: Accurate age and gender predictions hold importance across diverse domains, including forensic investigations involving both living and deceased individuals. Moreover, beyond its forensic applications, age and gender estimation based on dental measurements holds clinical significance in dental diagnostics and treatment planning.","Accurate age and gender estimation using dental features are crucial across various fields, including forensic science and clinical dentistry [2]. Dental structures, known for their resistance to external factors, are a reliable source for age estimation [8]. Key dental measurements, such as Coronal Height (CH), Coronal Pulp Cavity Height (CPCH), and Tooth Coronal Index (TCI), are closely linked to ageing due to the reduction in dental pulp volume over time [25]. Traditional methods of age estimation in forensic sciences using dentition are commonly applied to identify unknown corpses or human remains [26]. However, these traditional methods sometimes lack precision and reliability. Since much of the research is focused on human remains, these methods typically require the extraction of teeth. Consequently, they are often deemed time-consuming and ethically unsuitable, thereby facing reluctance to adopt. Some recent advancements suggest the use of dental radiographs to measure dental pulp for age and gender determination in the living [18]. A widely employed non-destructive approach for age and gender estimation, utilized by both dentists and forensic experts, involves analyzing radiographs of the upper and lower jaw to assess the pulp area [24]. Although this method does not involve physical alteration of the teeth, it requires the expertise of experienced dental professionals for precise predictions. This study explores a range of machine learning (ML) techniques, which offer a promising approach for age and gender determination in living individuals based on dental pulp measurements due to its ability to handle complex patterns and large datasets. Several ML models, such as Cat Boost Classifier (Catboost) [19], Gradient Boosting Machine (GBM) [17], Ada Boost Classifier (AdaBoost) [14], Random Forest (RF) [5], eXtreme Gradient Boosting (XGB) [6], Light Gradient Boosting Machine (LGB)[11], and Extra Trees Classifier (ETC) [22], were explored in this study, which provide diverse capability of capturing nuanced relationships between features and target variables. Additionally, data balancing techniques were adopted to mitigate biases and enhance accuracy. Furthermore, the ML models enable continual refinement through iterative learning, potentially improving accuracy and reliability over traditional methods. This interdisciplinary approach harnesses computational power to advance dental diagnostics, offering a non-invasive and potentially more accurate means of age and gender estimation. Moreover, ensemble learning methods combined with non-trainable combiners were investigated in this study. This approach leverages multiple models’ strengths, addressing individual model limitations to enhance overall predictive performance. The primary goals of this study are two-fold: 1) to examine the effectiveness of CH, CPCH, and TCI in estimating age and gender; and 2) to evaluate the performance of ensemble ML models in improving the accuracy and reliability of age and gender estimation. By integrating diverse models, we aim to create a robust and versatile framework for age and gender estimation using dental biometrics. Furthermore, to address the issue of non-interpretable results in ensemble models, eXplainable Artificial Intelligence (XAI) techniques, specifically SHAP (SHapley Additive exPlanations), were adopted in our study, which aims to enhance the interpretability of predictions from the proposed model, empowering dental experts to make more informed decisions by gaining insights into the underlying factors influencing the predictions. The remainder of this paper is organized as follows: Section II presents related work, highlighting existing methods and their limitations. Section III details the proposed methods, including those for data collection, feature extraction, machine learning and ensemble learning for age and gender estimation, and explainable AI (XAI). Section IV provides experimental results and discussions, and finally, Section V concludes the paper, summarizing key findings and outlining avenues for future research."
https://arxiv.org/html/2411.08187v1,TractoEmbed: Modular Multi-level Embedding framework for white matter tract segmentation,"White matter tract segmentation is crucial for studying brain structural connectivity and neurosurgical planning. However, segmentation remains challenging due to issues like class imbalance between major and minor tracts, structural similarity, subject variability, symmetric streamlines between hemispheres etc. To address these challenges, we propose TractoEmbed, a modular multi-level embedding framework, that encodes localized representations through learning tasks in respective encoders. In this paper, TractoEmbed introduces a novel hierarchical streamline data representation that captures maximum spatial information at each level i.e. individual streamlines, clusters, and patches. Experiments show that TractoEmbed outperforms state-of-the-art methods in white matter tract segmentation across different datasets, and spanning various age groups. The modular framework directly allows the integration of additional embeddings in future works.","Diffusion MRI (dMRI) [2, 1] facilitates the non-invasive examination of the brain’s white matter (WM) microstructural organization. A crucial component of the dMRI analysis pipeline is fiber tractography [3, 23, 22], which tracks fibers or streamlines under anatomical constraints from the dMRI signal received from the scanner(refer to Section 3). Tract Segmentation involves dividing the streamlines into distinct, anatomically meaningful tracts, with each tract corresponding to a specific white matter pathway. These tracts can be broadly grouped into 3 types based on structural connectivity, i.e. Association, Commissural, and Projection Fibers. Each type is further subdivided based on its specific structural connectivity and function, allowing for more granular distinctions. Through the segmentation process, it becomes possible to conduct quantitative studies of white matter (WM), which is important in understanding neurological disorders such as Alzheimer’s, and Parkinson’s [16], the effect of tumors on segmenting fiber streamlines, etc. In addition, tract segmentation is also crucial for preoperative neurosurgical planning [14], as it helps identify eloquent white matter areas and determine optimal surgical approaches that minimize post-operative damage. Tract segmentation is also extensively used to visualize particular segments for focused examination by clinicians. However, this process is typically performed by expert Neuroanatomists using their knowledge of brain anatomy to divide fibers into multiple bundles. As a result, it is very time-consuming and can vary between experts, affecting the consistency and reliability of the results. Taking challenges associated with manual tract segmentation, various techniques have been developed over the years. These techniques range from classical methods to ATLAS-based and distance-based algorithms [8, 9, 21, 25] (refer to Section 2). An ATLAS refers to a standardized reference that allows spatial mapping of neuroimaging data from different studies (refer to Table 1) and modalities. They approximate the shape, location, and brain region boundaries in a common coordinate space, facilitating the comparison of brain structure and function across individuals. These methods require significant manual intervention and are prone to age-related brain changes, also their effectiveness depends on the alignment and quality of the ATLAS. Considering the limitations of manual and classical methods, as well as the importance of tract segmentation, machine learning, and deep learning-based frameworks have been proposed for automatic tract segmentation [33, 4, 28]. Deep learning algorithms can learn information from shape, structure, relative location, fiber orientations, etc. However, a notable drawback is that these models often fail in classifying streamlines that are linear in shape due to over-reliance on shape, such as striato-thalamo-pallido projection fibers, which in existing methods, require global reference along with streamlines [7]. Additionally, when neurosurgeons are concerned with segmenting only a specific set of streamlines, global tractography can become a computational overhead. Due to these complexities in streamline classification-based tract segmentation, each method inherently has a certain drawback. To address this, we propose TractoEmbed, a modular framework that combines multi-level embeddings extracted from hierarchical data representations specifically at streamline, patch, and cluster levels (refer to Fig. 1). Our approach surpasses state-of-the-art (SOTA) results in tract segmentation. In this work, we present an approach with the following major contributions: 1. We introduce TractoEmbed, a novel modular multi-embedding framework, which leverages learning task-specific encoders to embed data representations, and generate embeddings. Where the encoders and their hyperparameters are selected after rigorous experimentation. 2. We propose novel hierarchical and descriptive streamline data representations. These representations includes spatial information about regional patches, neighboring streamlines and the streamline itself, providing a comprehensive understanding of the streamline characteristics. In contrast to recent advances, our method leverages minimal neighbouring streamlines, hyperlocal streamlines, enhancing robustness to practical clinical settings 3. It is demonstrated that TractoEmbed framework, generalizes across various datasets encompassing different age groups (refer Table 1). Additionally, the framework is modular at the embedding level, allowing researchers to integrate their own learnable embeddings to achieve even richer representations of streamline data. This modularity enhances the flexibility and adaptability of the framework."
https://arxiv.org/html/2411.08171v1,Comprehensive and Comparative Analysis between Transfer Learning and Custom Built VGG and CNN-SVM Models for Wildfire Detection,"Contemporary Artificial Intelligence (AI) and Machine Learning (ML) research places a significant emphasis on transfer learning, showcasing its transformative potential in enhancing model performance across diverse domains. This paper examines the efficiency and effectiveness of transfer learning in the context of wildfire detection. Three purpose-built models – Visual Geometry Group (VGG)-7, VGG-10, and Convolutional Neural Network (CNN)-Support Vector Machine(SVM) CNN-SVM – are rigorously compared with three pretrained models – VGG-16, VGG-19, and Residual Neural Network (ResNet) ResNet101. We trained and evaluated these models using a dataset that captures the complexities of wildfires, incorporating variables such as varying lighting conditions, time of day, and diverse terrains. The objective is to discern how transfer learning performs against models trained from scratch in addressing the intricacies of the wildfire detection problem. By assessing the performance metrics, including accuracy, precision, recall, and F1 score, a comprehensive understanding of the advantages and disadvantages of transfer learning in this specific domain is obtained. This study contributes valuable insights to the ongoing discourse, guiding future directions in AI and ML research.","Transfer learning has emerged as a focal point in contemporary Artificial Intelligence (AI) and Machine Learning (ML) research due to its transformative impact on model performance across diverse domains [1]. The collective efforts of leading companies and research institutions underscore the pivotal role transfer learning plays in enhancing efficiency across various facets of human life. This surge in popularity can be attributed to the adaptability and efficiency transfer learning imparts to AI and ML applications. In the context of AI/ML, transfer learning refers to leveraging pretrained models initially trained on a source dataset and applying them to a target dataset with shared domain characteristics [2]. Notably, in computer vision [1], models are often trained on various iterations of the ImageNet dataset [3], which comprises an extensive array of object classes, including animals, everyday objects, and specific species of flora and fauna. This dataset continually evolves, incorporating new classes and images to enhance its comprehensiveness. Researchers have dedicated significant efforts to developing models capable of accurately classifying objects within the ImageNet dataset, achieving remarkable accuracies often exceeding 99\% [4]. For professionals engaged in specific classification tasks, such as identifying microorganisms or various car models, transfer learning offers a notable advantage. Rather than undertaking the laborious process of training a model from scratch, practitioners can access pretrained neural network weights and adapt them to specific domain-related tasks. Transfer learning presents broad applicability across diverse fields of study, including computer vision, Large Language Models (LLMs), language translation, and chatbots. Figure 1: Sample of images in the training dataset. These images include synthetic images created by data augmentation techniques like rotation, translation, scaling, brightness adjustment, and the introduction of Gaussian noise (for more details visit [1]). I-A Computer Vision and Language Models Computer Vision: Transfer learning significantly impacts computer vision by leveraging pretrained models on extensive datasets such as ImageNet. This approach expedites the learning process, enhancing efficiency in tasks such as object detection, image segmentation, and facial recognition [1]. The ability to transfer knowledge from one visual domain to another enables models to discern intricate patterns and features with remarkable accuracy. Large Language Models (LLMs): In natural language processing and LLMs, transfer learning has revolutionized the landscape. Models like Bidirectional Encoder Representations from Transformers (BERT) are pretrained on vast corpora, enabling them to grasp intricate language nuances and contextual relationships. These pretrained language models serve as a foundation for diverse Natural Language Processing (NLP) tasks, including sentiment analysis, text summarization, and question answering. Language Translation: Transfer learning proves instrumental in language translation tasks. By pre-training models on multilingual datasets, these models gain an understanding of language structures and semantics across different languages. Fine-tuning for specific language pairs leads to more accurate and contextually relevant translations, contributing to the development of more accessible and effective communication tools. Chatbots: The development of intelligent chatbots benefits significantly from transfer learning. Pretrained models, equipped with extensive linguistic knowledge, comprehend user queries, understand context, and generate coherent responses. This approach mitigates the need for exhaustive training on specific dialog datasets, allowing developers to create chatbots adept at natural language understanding and generation. I-B Pretrained Models vs Custom Built Models Utilizing pretrained models offers substantial benefits in terms of data efficiency, as these models are initially trained on extensive and varied datasets, enabling them to grasp general features and patterns. This proves advantageous when dealing with limited labeled data for a particular task, as the pretrained model has already acquired useful representations from a broader context. Transfer learning further streamlines the process by taking a pretrained model and fine-tuning it on a task-specific dataset, leading to faster convergence and requiring less labeled data compared to training from scratch. The efficiency gains extend to computation time and resources, as training large neural networks from the beginning is computationally intensive, whereas pretrained models save time by having completed a significant portion of the learning process. Additionally, pretrained models serve as effective feature extractors, particularly leveraging lower-level features that are transferable across different tasks. The models’ ability to generalize well to diverse inputs, especially when the pre-training dataset aligns with the target domain, contributes to improved performance on new, unseen data. Furthermore, in scenarios involving related but not identical domains, pretrained models support domain adaptation through fine-tuning, enabling effective performance on specific tasks within the target domain. The collaborative nature of the research community enhances accessibility, with many open-source pretrained models in computer vision available for practitioners, fostering the development and utilization of state-of-the-art models for diverse applications. I-C Contributions This paper applies and studies the performance measure of transfer learning in the context of wildfire detection. To assess the performance measure of transfer learning utilization, three custom-built models, namely, Visual Geometry Group (VGG) VGG-7, VGG-10, and Convolutional Neural Network (CNN)-Support Vector Machine(SVM) CNN-SVM are compared relative to three pretrained models – VGG-16, VGG-19, and Residual Neural Network (ResNet) ResNet101. The proposed study shows that transfer learning can successfully capture the complexities of wildfires, incorporating challenging variables (e.g., time of day, varying lighting conditions, and diverse terrains) as presented in Fig. 1. In this paper, we show how transfer learning can address the intricacies of the wildfire detection when compared to custom-built models trained from scratch. Performance metrics, such as accuracy, precision, recall, and F1 score, are used to provide a comprehensive understanding of transfer learning advantages and disadvantages in the wildfire detection domain. I-D Structure The rest of the paper is organized as follows: Section II describes the problem formulation and dataset preprocessing. Section III illustrates the research methodology and segmentation. Section III presents custom built CNN-SVM, VGG-7, and VGG-10, and pretrained VGG-16, VGG-19, and ResNet101. Section IV illustrates the comparative results, performance analysis, and test cases. Finally, Section V concludes the work."
https://arxiv.org/html/2411.08128v1,CameraHMR: Aligning People with Perspective,"We address the challenge of accurate 3D human pose and shape estimation from monocular images. The key to accuracy and robustness lies in high-quality training data. Existing training datasets containing real images with pseudo ground truth (pGT) use SMPLify to fit SMPL to sparse 2D joint locations, assuming a simplified camera with default intrinsics. We make two contributions that improve pGT accuracy. First, to estimate camera intrinsics, we develop a field-of-view prediction model (HumanFoV) trained on a dataset of images containing people. We use the estimated intrinsics to enhance the 4D-Humans dataset by incorporating a full perspective camera model during SMPLify fitting. Second, 2D joints provide limited constraints on 3D body shape, resulting in average-looking bodies. To address this, we use the BEDLAM dataset to train a dense surface keypoint detector. We apply this detector to the 4D-Humans dataset and modify SMPLify to fit the detected keypoints, resulting in significantly more realistic body shapes. Finally, we upgrade the HMR2.0 architecture to include the estimated camera parameters. We iterate model training and SMPLify fitting initialized with the previously trained model. This leads to more accurate pGT and a new model, CameraHMR, with state-of-the-art accuracy. Code and pGT is available for research purposes.","The field of monocular 3D human pose and shape (HPS) estimation has advanced rapidly. Updated architectures, stronger backbones, and more extensive training data have all led to improvements in robustness and accuracy. We argue that a key remaining source of error lies in the fact that many HPS methods use a simplified weak-perspective camera model. We describe how the wrong camera model introduces error and we propose a solution. Specifically, we collect a dataset of images of people with varied field of view (FoV) and train a network to directly predict FoV from pixels. We then leverage this predicted FoV in training and show how this leads to state-of-the-art accuracy. Recent HPS methods, such as HMR2.0 [15], achieve notable 2D alignment by leveraging large-scale real image datasets for training. However, this success in 2D alignment comes at the cost of reduced 3D accuracy as described in [12]. The core issue lies in the fact that these large-scale real image datasets frequently lack camera intrinsic parameters. Training involves first creating 3D pseudo ground truth (pGT) data by fitting a parametric 3D body model like SMPL [35] to 2D features such as keypoints. This fitting process typically uses a weak perspective camera model or default camera intrinsics. When the camera model is wrong, fitting 2D keypoints accurately forces the 3D pose to be wrong. Consequently, methods trained on these pGT datasets learn to replicate the 3D errors. To achieve both accurate 2D alignment and 3D poses, it is crucial to use the correct camera intrinsics in creating the pGT. Unfortunately, estimating intrinsics from a single image is challenging. While there are many state-of-the-art approaches for camera calibration from monocular images [31, 20, 53], they are trained on datasets such as Google Street View [5] and SUN360 [48]. Such datasets focus on outdoor or indoor scenes rather than people. Methods trained on datasets containing panoramic images of streets, natural landscapes, urban scenes, indoor settings, etc., do not work well on images of people. On the other hand methods trained on synthetic data like SPEC-camcalib [28] do not generalize well to in-the-wild data. This highlights the need for a robust camera calibration model for images containing people to achieve accurate 3D human pose and shape estimation. To address this problem, we collect a dataset of about 500K images predominantly comprising people, to train a field of view (FoV) prediction model. The human body provides useful information for camera estimation. While it would be going too far to call the body a “calibration object,” bodies have highly regular proportions and a limited range of heights. When projected into images, this regular structure systematically varies with focal length and perspective projection. To exploit this fact, we train a direct FoV regressor, HumanFoV, which generalizes well on benchmarks featuring humans compared to other state-of-the-art (SOTA) camera calibration methods. HumanFoV can be directly incorporated into HPS methods that use a full perspective camera model, enabling accurate 3D reconstruction. Using an accurate camera not only helps HPS regressors infer the 3D location of the people in camera space but it also improves alignment of the inferred body with image features, especially for wide angle images and extreme viewing angles. While incorporating a more accurate camera model into HPS methods is important, we still need high-quality training data that is as diverse as possible. To that end, we use HumanFoV to improve the real-image pGT in the 4DHumans dataset that is used to train HMR2.0 [15]. The original dataset uses SMPLify [7] to fit SMPL to 2D keypoints under a weak-perspective assumption. Instead, we use a full perspective camera model in SMPLify and exploit HumanFoV to estimate the FoV of the training images. Additionally, the original dataset is created by fitting SMPL to only 17 sparse 2D joints; these lack the detail necessary for accurate 3D shape reconstruction. To improve this, we train a keypoint detector (DenseKP) on the BEDLAM [6] dataset to estimate 138 dense surface keypoints. We modify SMPLify to use these together with the original 17 2D joints. This results in significantly more realistic body shapes. Qualitatively, the improved camera model and dense keypoints lead to good 2D image alignment and more plausible 3D pGT compared to original dataset (Fig. 2). With this, we generate high-quality 3D pGT for a large-scale real image dataset comprising approximately 3.2 million cropped images. Importantly, the dataset includes the camera intrinsics estimated by HumanFoV; these are crucial for HPS methods. We further modify the HMR2.0 architecture to incorporate camera parameters from HumanFoV in training. We iterate training this new CameraHMR model and refining the pGT with SMPLify initialized with the previously trained model. This significantly improves performance, with CameraHMR achieving state-of-the-art accuracy on multiple HPS benchmarks. See Fig. 1. In summary, we (1) collect a dataset of varied images of humans with known FoV, (2) using this dataset, we train HumanFoV to regress FoV from images of people, (3) update SMPLify with a full perspective model that uses the HumanFoV output, (4) introduce a dense surface keypoint regressor and incorporate these keypoints into SMPLify, (5) improve the 4DHumans training set using the new version of SMPLify, (6) incorporate the FoV estimation in HMR2.0, (7) train a new model, CameraHMR, with SOTA accuracy. Code and pGT dataset is available for research purposes."
https://arxiv.org/html/2411.08127v1,TIPO: Text to Image with Text Presampling for Prompt Optimization,"TIPO (Text to Image with text pre-sampling for Prompt Optimization) is an innovative framework designed to enhance text-to-image (T2I) generation by language model (LM) for automatic prompt engineering. By refining and extending user-provided prompts, TIPO bridges the gap between simple inputs and the detailed prompts required for high-quality image generation.Unlike previous approaches that rely on Large Language Models (LLMs) or reinforcement learning (RL), TIPO adjusts user input prompts with the distribution of a trained prompt dataset, eliminating the need for complex runtime cost via lightweight model. This pre-sampling approach enables efficient and scalable prompt optimization. Experimental results demonstrate TIPO’s effectiveness in improving aesthetic scores, reducing image corruption, and better aligning generated images with dataset distributions. These findings highlight the critical role of prompt engineering in T2I systems and open avenues for broader applications of automatic prompt refinement.","Recent advancements in Text-to-Image (T2I) generative models have revolutionized creative applications [36, 6, 19, 41, 38, 39, 43, 40, 37, 42, 12, 11, 28, 20, 7]. These models tend to perform better when provided with longer, more detailed prompts, which describes specific elements like style, composition, and context. However, this reliance on highly descriptive inputs is a significant bottleneck for generating high-quality images, especially for users who prefer or require simpler inputs. The need for intricate, nuanced prompts often results in a higher barrier to entry, limiting accessibility and ease of use for those unfamiliar with the complexities of prompt engineering. The previous works [31] tried to utilize LLMs to lenghthen prompt to solve tendency, however many ignore the root cause of why some prompts result in worse images and rely heavily on manually engineered prompts, which require substantial resources. To address these challenges, we introduce TIPO (Text to Image with text pre-sampling for Prompt Optimization), an innovative framework designed to enhance T2I generative models. TIPO introduces ‘text pre-sampling’ preprocess procedure in inference pipeline, letting text-to-image models reproduce a more accurate distribution of corresponding user prompts without additional manual engineering. Unlike previous approaches that focus on extending prompts with manually collected prompt sets [4, 15, 45, 21], directly use LLM to modify the prompts [26, 51] or utilize reinforcement learning like method which update model’s input or model itself from text-to-image feedback [22, 13, 31] without addressing underlying issues, TIPO introduces a method that optimizes prompts efficiently as shown Section 4. Our work makes the following key contributions: 1. We introduce an universal prompt optimization framework that is designed to work with any text-to-image model, providing a versatile solution for improving image generation across different architectures. 2. We provide comprehensive evaluation methodology and results including using both standard and non-standard metrics. Our experiments demonstrate that TIPO’s prompt engineering method can improve image quality in terms of mathematical measures and user preferences, validating its effectiveness from multiple perspectives. Figure 1: Comparison of prompt optimization methods. (a) uses instructions for prompt refinement but lacks understanding of T2I-specific prompts. (b) improves detail but limits variety. (c) optimizes for model feedback but are computationally expensive. (d) TIPO (ours) aligns prompts with the trained distribution of T2I models, ensuring prompt detail, consistency with training data, and controlled variety within model scope. This paper presents TIPO’s theoretical foundations, implementation details, and experimental results demonstrating its effectiveness in improving aesthetic scores [17], reducing image corruption [32, 33], and better aligning generated images with dataset distributions [24, 35]. Our findings highlight the critical role of prompt optimization in advancing T2I technology and open avenues for broader applications of automatic prompt refinement."
https://arxiv.org/html/2411.08460v1,Trap-MID: Trapdoor-based Defense against Model Inversion Attacks,"Model Inversion (MI) attacks pose a significant threat to the privacy of Deep Neural Networks by recovering training data distribution from well-trained models. While existing defenses often rely on regularization techniques to reduce information leakage, they remain vulnerable to recent attacks. In this paper, we propose the Trapdoor-based Model Inversion Defense (Trap-MID) to mislead MI attacks. A trapdoor is integrated into the model to predict a specific label when the input is injected with the corresponding trigger. Consequently, this trapdoor information serves as the ""shortcut"" for MI attacks, leading them to extract trapdoor triggers rather than private data. We provide theoretical insights into the impacts of trapdoor’s effectiveness and naturalness on deceiving MI attacks. In addition, empirical experiments demonstrate the state-of-the-art defense performance of Trap-MID against various MI attacks without the requirements for extra data or large computational overhead. Our source code is publicly available at https://github.com/ntuaislab/Trap-MID.","Deep Neural Networks (DNNs) have been successfully applied in various domains. However, training DNNs could involve sensitive data like facial recognition and medical diagnosis, which raises privacy concerns. Model Inversion (MI) stands as one of the important privacy attacks aimed at reconstructing private data within specific classes from a well-trained model. For example, an adversary may recover the training images of specific identities from a facial recognition system. MI attacks were first introduced by Fredrikson et al. [1, 2], reconstructing private attributes from low-capacity models. After that, Zhang et al. [3] proposed Generative Model-Inversion (GMI) attacks to reconstruct private images from DNNs, utilizing Generative Adversarial Network (GAN) as a general prior. This GAN-based framework has been widely adopted by later attacks [4, 5, 6, 7, 8, 9, 10, 11]. Among them, PLG-MI [8] achieves state-of-the-art attack performance. Previous works also demonstrated the efficacy of MI attacks under black-box [9, 10, 12] or label-only [11, 13] settings. In this paper, we focus on defending against white-box attacks, which pose a more challenging scenario. Most existing defenses focus on reducing the information leakage through Differential Privacy (DP) [1, 3], dependency regularization [14, 15], or manipulating the loss landscape [16]. However, these methods remain vulnerable to recent MI attacks [16]. In contrast, recent works proposed to mislead MI attacks by prompting models to classify fake samples as the protected class with high confidence [17, 18, 19]. Although effective, these misleading-based strategies face challenges, including additional data requirements and substantial computational overhead. Furthermore, they typically protect only a single or a limited set of classes, while other defenses aim to secure all classes simultaneously. Sharing a similar idea, Shan et al. [20] introduced Trapdoor-enabled Adversarial Detection (TeD) against targeted adversarial attacks, which aims to change the model behaviors by applying adversarial perturbations to the input data. Instead of training a robust model against such perturbations, TeD shows that injecting trapdoors into the models can mislead the adversarial attacks to result in samples with similar features to poisoned data, thereby empowering the adversarial detection by measuring their similarity to the trapdoor signatures. Inspired by previous misleading-based defenses [17, 18, 19] and TeD [20], we propose Trapdoor-based Model Inversion Defense (Trap-MID), which deceives MI attacks by incorporating trapdoors as the ""shortcuts"". We discuss the key properties of trapdoor triggers necessary for misleading these attacks, and experiments show that Trap-MID outperforms existing methods in defending against MI attacks. Our contributions can be summarized as follows: 1. We propose a trapdoor-based defense, Trap-MID, to preserve privacy by misleading MI attacks. Through extensive experimentation, it presents state-of-the-art defense performance against various MI attacks. 2. To the best of our knowledge, we are the first to establish the connection between MI defenses and trapdoor injection techniques. We theoretically discuss the importance of trapdoor effectiveness and naturalness in misleading MI attacks and showcase its efficacy with empirical experiments. 3. Compared to previous trapping defenses, our trapdoor-based framework is more computationally and data-efficient, without large computational overhead or additional data."
https://arxiv.org/html/2411.08443v1,Machine Unlearning on Pre-trained Models by Residual Feature Alignment Using LoRA,"Machine unlearning is new emerged technology that removes a subset of the training data from a trained model without affecting the model performance on the remaining data. This topic is becoming increasingly important in protecting user privacy and eliminating harmful or outdated data. The key challenge lies in effectively and efficiently unlearning specific information without compromising the model’s utility on the retained data. For the pre-trained models, fine-tuning is an important way to achieve the unlearning target. Previous work typically fine-tuned the entire model’s parameters, which incurs significant computation costs. In addition, the fine-tuning process may cause shifts in the intermediate layer features, affecting the model’s overall utility. In this work, we propose a novel and efficient machine unlearning method on pre-trained models. We term the method as Residual Feature Alignment Unlearning. Specifically, we leverage LoRA (Low-Rank Adaptation) to decompose the model’s intermediate features into pre-trained features and residual features. By adjusting the residual features, we align the unlearned model with the pre-trained model at the intermediate feature level to achieve both unlearning and remaining targets. The method aims to learn the zero residuals on the retained set and shifted residuals on the unlearning set. Extensive experiments on numerous datasets validate the effectiveness of our approach.","Machine unlearning has emerged as a significant research direction in the field of machine learning in recent years, receiving extensive attention for its ability to protect user privacy and remove harmful data. Machine unlearning provides an effective solution to ensure user data can be securely removed from models when requested or no longer needed. It has also been extended to remove harmful or outdated data contained in models, enhancing security and usability. Current research mainly focuses on how to implement unlearning efficiently and effectively. Deletion requests can be frequent in practice [1], including user requests to remove personal data and the removal of harmful information from models. Therefore, quickly and effectively deleting target data without affecting the model’s performance on the retained data is a key focus of current machine unlearning research [2, 3]. Current unlearning methods can be categorized into two types: exact unlearning and approximate unlearning. Exact unlearning requires that the distributions of the unlearned model are indistinguishable from that of a retrained model, while approximate unlearning only requires that the two are approximately indistinguishable [3]. Exact unlearning typically requires retraining all or part of the training data, which incurs high computational and time costs. However, since it can achieve theoretically complete unlearning, it received significant attention in the early stages of research. A typical method is SISA [4], which stores a checkpoint for each subset during training and only retrains the corresponding sub-model upon receiving a deletion request. However, it requires storing a large number of intermediate results during pre-training, which can consume significant storage space [3]. Additionally, it may be ineffective for pre-trained models, which do not partition the data or store intermediate results during pre-training. To mitigate the above issues, recent work on pre-trained models has shifted towards approximate unlearning. Approximate unlearning does not aim for complete indistinguishability between the unlearned and retrained models but rather for approximate indistinguishability [2]. It eliminates the influence of the unlearning data by modifying the model’s parameters. Compared to retraining, approximate unlearning only fine-tuning the pre-trained model, reducing computation costs and training time. A typical approach is to compute and remove the contribution of the unlearning data to the model [5, 6]. However, existing approximate unlearning methods on pre-trained models still face the following challenges. 1. Intermediate feature shift. Current unlearning methods primarily focus on handling the final output, without adequately considering the feature representations in the intermediate layers of the model. In fact, the intermediate features of a pre-trained model are abstract representations learned from a large-scale training set. Removing a small portion of the data may lead to significant shifts in these features, which can impact the model’s performance on the retained data. 2. Unlearning initialization. The final parameters of the unlearned model are often close to those of the pre-trained model. Existing methods lack a progressive initialization mechanism during the unlearning process, causing the model to deviate quickly from the pre-trained model at the beginning of unlearning. This rapid deviation may lead to performance degradation. Especially in the early stages of unlearning, the model struggles to effectively balance performance on retained and unlearning data. 3. Unlearning efficiency. Although approximate unlearning can be achieved through fine-tuning, the computational and memory costs of fine-tuning remain high for large models, making fast and efficient unlearning a significant challenge in practical applications. To address these challenges, we propose a fast and efficient unlearning method based on residual feature alignment using Low-Rank Adaptation (LoRA) [7]. LoRA is a fine-tuning method that introduces low-rank matrices into the model, allowing for efficient updates with low memory consumption and computational overhead. We address the above challenges by introducing LoRA modules in certain intermediate layers of the model: 1) To handle intermediate feature shift, we divide the intermediate features of the model into pre-trained features and residual features. We freeze the pre-trained features and achieve unlearning by adjusting the residual features to align with different objectives on the retained and the unlearning sets. 2) The residual features are initialized to zero (or near-zero), ensuring a smoother update process during the initialization of unlearning, preventing the model from deviating too quickly from the pre-trained model in the early stages of unlearning, thus maintaining proximity to the pre-trained model and reducing performance loss. 3) To improve unlearning efficiency, we introduce LoRA modules only in the necessary intermediate layers. By adding low-rank matrices, we significantly reduce memory consumption and computational overhead, enabling even large-scale models to unlearn quickly and efficiently. Our main contributions are summarized as follows: 1. We propose a LoRA-based residual feature alignment method for machine unlearning, which enables efficient unlearning while maintaining consistency between the intermediate feature of the unlearned model and the pre-trained model. 2. We mathematically reformulate the training objective for residual feature alignment, allowing the training process to be implemented via a teacher-student framework, thereby simplifying the implementation complexity. 3. We conducted extensive experiments on various datasets and models. The broad experimental results demonstrate the effectiveness and efficiency of our method."
https://arxiv.org/html/2411.08410v1,The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense,"The vulnerability of Vision Large Language Models (VLLMs) to jailbreak attacks appears as no surprise. However, recent defense mechanisms against these attacks have reached near-saturation performance on benchmarks, often with minimal effort. This simultaneous high performance in both attack and defense presents a perplexing paradox. Resolving it is critical for advancing the development of trustworthy models. To address this research gap, we first investigate why VLLMs are prone to these attacks. We then make a key observation: existing defense mechanisms suffer from an over-prudence problem, resulting in unexpected abstention even in the presence of benign inputs. Additionally, we find that the two representative evaluation methods for jailbreak often exhibit chance agreement. This limitation makes it potentially misleading when evaluating attack strategies or defense mechanisms. Beyond these empirical observations, our another contribution in this work is to repurpose the guardrails of LLMs on the shelf, as an effective alternative detector prior to VLLM response. We believe these findings offer useful insights to rethink the foundational development of VLLM safety with respect to benchmark datasets, evaluation methods, and defense strategies.","The pervasiveness of Large Language Models (LLMs) concurrently ushers varied challenges for both researchers and practitioners [26]. Among these, protecting the trustworthiness of free-form outputs, as defined by the 3H criterion [51], has grown increasingly critical in recent years [22, 55]. Beyond important considerations of Helpfulness and Honesty, the need for Harmlessness is far more urgent given its potential social impact. Jailbreak attacks, often referred to as red-teaming [41], serve as the most common method for assessing the harmlessness of LLMs and Vision-LLMs (VLLMs) [53, 32, 7]. They are designed to circumvent the built-in restrictions or safeguards within models [23], eliciting them to produce malicious outputs, such as content related to illegal activities, hate speech, and pornography. Compared to their LLM counterparts, the vulnerability of VLLMs to jailbreak attacks has garnered considerable attention until very recently [61, 38, 30]. Some initial methods inject high-risk content into images through typography or generative techniques like stable diffusion [45]. Leveraging such methods, datasets have been curated that easily expose a high Attack Success Rate (ASR) for both proprietary models [1] and publicly open-sourced models [31, 32]. On the other hand, without many bells and whistles, recent defense strategies - primarily focused on safety-aware supervised fine-tuning [71] and system prompt protection [60] - have surprisingly shown remarkable results on these benchmark datasets. In particular, VLLMs like LLaVA [31] can be fully safeguarded against the attacks involved (ASR \rightarrow 0) [60, 71, 18]. As such, these dual findings raise an intriguing question: does it suggest that defending against jailbreak attacks is easy, given that the attacks themselves have already been shown to be relatively simple? The observation above highlights a perplexing safety paradox that undermines the credibility of recent advancements, particularly in defense mechanisms. To address this challenge, we present the first comprehensive study that underlines and elucidates the dual ease of both jailbreak attacks and defenses in VLLMs. Our first finding challenges prior assumptions that the vulnerability to jailbreak attacks stems from catastrophic forgetting or fine-tuning [71, 42]. Instead, we reveal that the true cause lies in the inclusion of image inputs, which compromises the guardrails of the base LLMs. On the other hand, we observe that existing defense mechanisms [71, 60] tend to be overly prudent. One typical manifestation is that VLLMs with post-defense, are prone to abstaining from responding even to benign queries. This issue of over-prudence significantly impairs the helpfulness of VLLMs, reducing their utility in real-world applications. In addition, we point out that the two well-studied evaluation methods often show a limited correlation in identifying jailbreaks, making it less convincing to determine the effectiveness of an attack method or a defense strategy. Beyond understanding the safety paradox, we further note that the jailbreak defense can be re-framed into a detection-then-response process. In view of this, we propose to implement an evaluator prior to the final VLLM response. We opt not to utilize an additional VLLM for detection as [18], given the limited reliability of current VLLMs in providing robust safeguards. Instead, we explore a vision-free evaluator, where we repurpose the guardrails of recent advanced LLMs to judge the harmfulness of a given textual query. Interestingly, we find that this evaluator, when paired with a VLLM for safe response generation, suffers less from the over-prudence problem, achieving a balanced interplay between robust safety alignment and model helpfulness. To the best of our knowledge, we are the first to investigate the safety paradox problem of VLLMs. Through the empirical findings presented in this work, we seek to highlight this issue and raise more attention to its significance. Beyond this, we hope to provide insights that can support future advancements in this field, such as reaching a consensus on the nature of jailbreak attacks and their associated risks, facilitating the collection of comprehensive attack data, and developing more robust evaluations."
https://arxiv.org/html/2411.08305v1,Robust Divergence Learning for Missing-Modality Segmentation,"Multimodal Magnetic Resonance Imaging (MRI) provides essential complementary information for analyzing brain tumor subregions. While methods using four common MRI modalities for automatic segmentation have shown success, they often face challenges with missing modalities due to image quality issues, inconsistent protocols, allergic reactions, or cost factors. Thus, developing a segmentation paradigm that handles missing modalities is clinically valuable. A novel single-modality parallel processing network framework based on Hölder divergence and mutual information is introduced. Each modality is independently input into a shared network backbone for parallel processing, preserving unique information. Additionally, a dynamic sharing framework is introduced that adjusts network parameters based on modality availability. A Hölder divergence and mutual information-based loss functions are used for evaluating discrepancies between predictions and labels. Extensive testing on the BraTS 2018 and BraTS 2020 datasets demonstrates that our method outperforms existing techniques in handling missing modalities and validates each component’s effectiveness.","Brain tumors are aggressive diseases requiring early detection for effective treatment. MRI is widely used for evaluating brain tumors due to its superior soft tissue contrast and lack of radiation exposure. MRI segmentation is crucial for isolating healthy tissue from abnormal cells, offering various modalities for effective tumor detection, including T1-weighted, T1-weighted post-contrast- enhancement, T2-weighted, and FLAIR. Several existing methods [1, 2, 3, 4, 5, 6] achieved high accuracy in tumor segmentation when all modalities are available. However, in real-world scenarios, one or more modalities may be missing due to patient movement, hardware issues, or other factors. This “missing modality” problem arises when one or more modalities (e.g., T1w, T2, T1c, and FLAIR) are missing during inference but available during training [7]. Several approaches have been developed to address this issue, which can be categorized into two types [8]: modeling each missing case individually or using a single model to handle all cases. For the former, knowledge distillation is commonly used to transfer competencies from a well-trained teacher model to a student model designed for specific missing modalities. SMU-Net [9] employed a novel distillation strategy where a multimodal teacher network transfers knowledge to unimodal student networks at both the latent space and network output levels. ProtoKD [10] integrated prototype learning with knowledge distillation, effectively capturing the underlying data distribution. MMCFormer [11] leveraged transformers with auxiliary tokens to facilitate modality-specific representation transfer. The latter category aims to address all missing-modal situations with a single model, typically involving separate modality encoders to project each modality into a shared latent space before feature fusion. RFNet [12] integrated characteristics from various sources using a region-cognizant component. Moreover, Ting and Liu [13] used modality-specific encoders, a shared decoder, and a strategy to complement incomplete data with complete data. Furthermore, Wang et al. [14] employed specific and shared encoders to handle missing modalities for both segmentation and classification tasks. However, these approaches have some shortcomings. Using a specific model for each missing modality scenario is training-costly, for example, 2^{N}-1 models need to be trained when there are N modalities[8]. Conversely, a single model for all cases often results in performance deficiencies with few modalities available [9] and high inference costs due to numerous parameters. Inspired by Chang et al. [15] and high mutual information knowledge transfer learning [16], we process four different modalities individually to preserve unique information and enhance the model’s ability to recognize diverse data features, which can handle all cases with signal model with shared backbone. Specifically, we propose a novel mutual information-based metric with Hölder divergence[17] that evaluate discrepancies between the predictions and labels. What is more, a dynamic sharing framework is introduced, which allows the model to adapt its parameters depending on the availability of different modalities. The main contributions of this paper are summarized as follows: 1. Novel Network Architecture: We propose a new network architecture for parallel computing based on 3D U-Net. This framework combines unimodal parallel processing and dynamic network module combinations to handle missing modalities during brain tumor segmentation training. 2. New Metric Introduction: the Hölder divergence and mutual information are introduced to evaluate discrepancies between model predictions and labels. By minimizing the distances, we achieve more accurate feature alignment. 3. Extensive Validation: Extensive experiments on the BraTS 2018 and BraTS 2020 medical image datasets [18] are conducted. The results demonstrate that our method achieves state-of-the-art performance, showcasing its efficiency and practicality. The main notations used in this work is shown in Table I. TABLE I: Main Notations Used in This Work. This table provides an overview of the main notations used throughout this work, offering a concise reference for understanding the symbols and terminology employed in the algorithms discussed. Notation Definition x_{i} the i^{th} modality data of the sample. d_{i} The deepest-level feature of the i^{th} modality. d_{f} The deepest-level full-modality feature. h_{i} The generated single-modality representation of the i^{th} modality. \widehat{Y} Integrated output under missing modalities. Y real sample. p(d_{f}\mid d_{m}) The conditional distribution of the feature f given the missing modality information m. q(d_{f}\mid d_{m}) The conditional distribution approximated using variational methods. Figure 1: The Framework of Robust Divergence Learning for Missing-Modality Segmentation. This figure illustrates the overall structure of the proposed robust divergence learning approach, specifically designed to address segmentation challenges in scenarios where certain modalities are missing."
https://arxiv.org/html/2411.08164v1,EAPCR: A Universal Feature Extractor for Scientific Data without Explicit Feature Relation Patterns,"Conventional methods, including Decision Tree (DT)-based methods, have been effective in scientific tasks, such as non-image medical diagnostics, system anomaly detection, and inorganic catalysis efficiency prediction. However, most deep-learning techniques have struggled to surpass or even match this level of success as traditional machine learning methods. The primary reason is that these applications involve multi-source, heterogeneous data, where features lack explicit relationships. This contrasts with image data, where pixels exhibit spatial relationships; textual data, where words have sequential dependencies; and graph data, where nodes are connected through established associations. The absence of explicit Feature Relation Patterns (FRPs) presents a significant challenge for deep learning techniques in scientific applications that are not image, text, and graph-based. In this paper, we introduce EAPCR, a universal feature extractor designed for data without explicit FRPs. Tested across various scientific tasks, EAPCR consistently outperforms traditional methods and bridges the gap where deep learning models fall short. To further demonstrate its robustness, we synthesize a dataset without explicit FRPs. While Kolmogorov–Arnold Network (KAN) and feature extractors like Convolutional Neural Networks (CNNs), Graph Convolutional Networks (GCNs), and Transformers struggle, EAPCR excels, demonstrating its robustness and superior performance in scientific tasks without FRPs.","In various scientific applications, such as non-image medical diagnostics, system anomaly detection, inorganic catalysis efficiency prediction, and etc., traditional machine learning techniques, such as \acfdt (Ali et al., 2012) and \acdt-based method (e.g., \acrf and \acxgboost), have been reported as highly effective (Coşkun & Kuncan, 2022; Mutlu et al., 2023; Alizargar et al., 2023; Schossler et al., 2023; Mallioris et al., 2024). In contrast, few studies have reported deep learning models as the best-performing methods, indicating that more complex deep learning techniques, such as \accnns (LeCun et al., 1998), \acgcns (Kipf & Welling, 2016; Bronstein et al., 2017), and Transformers (Vaswani, 2017), have not shown the same level of success in those scientific applications. The primary reason deep learning techniques often underperform in scientific applications is that the data in these fields differ significantly from traditional tasks like images, text, and graphs. For example, in non-image medical diagnostics, patient data come from diverse sources, such as physical measurements (e.g., weight, blood pressure) and chemical tests (e.g., glucose levels) (Fig. 1-a-1). Unlike pixels in images or words in text, which have spatial or sequential relationships (LeCun et al., 1998; Bronstein et al., 2017) (Fig. 1-b-1), or nodes in graphs with known connections (Kipf & Welling, 2016; Bronstein et al., 2017) (Fig. 1-b-2), features in scientific data lack such explicit relationships. This absence of explicit feature relationships is common across various scientific tasks, such as system anomaly detection (Wang et al., 2023; Tian, 2023) (Fig. 1-a-2) and inorganic catalysis efficiency prediction (Sun et al., 2024) (Fig. 1-a-3), where features are collected from heterogeneous sources, such as electrical signals and temperature, and have different units, like pH levels and illumination time. Figure 1: Motivation for designing feature extractors for data without FRPs. The feature relationship patterns, including spatial relationships and connections, are referred to as \acfFRPs in this work. \acFRPs are critical for deep learning-based feature extractors as they contain essential information about feature associations or correlations. For instance, in image and text data, spatial relationships between pixels and words reveal correlations where nearby features have stronger associations and distant ones weaker. This inherent relational information enables feature extractors to quickly identify important combinations of strongly interacting features. For example, \accnns in image data leverage spatial relationships to focus on local feature patterns while ignoring irrelevant non-local ones (Bronstein et al., 2017; Yun et al., 2023) (Fig. 1-b-1). Similarly, \acgcns use adjacency matrices in graph data to capture meaningful node connections (Bronstein et al., 2017; Schlichtkrull et al., 2018) (Fig. 1-b-2). However, when explicit \acFRPs, like spatial relationships or known connections, are absent, deep learning methods often struggle (Fig. 1-b-3) as assumed \acFRPs may not match the actual implicit relationships. This raises a key question: How can we design universal feature extraction modules for data that lack explicit \acFRPs? In this paper, we propose a feature extraction module, EAPCR, designed as a universal feature extractor for data without explicit FRPs. Traditional feature extraction modules rely on known FRPs to distinguish between important and unimportant feature combinations. However, without the guidance of FRPs, EAPCR adopts a different approach. First, it exposes all possible \acFRPs. Second, it accelerates the sampling of these combinations to ensure a wide range of feature interactions are evaluated, allowing it to effectively identify important combinations of strongly interacting features. To evaluate EAPCR’s effectiveness, we apply it to various scientific domains, including non-image medical diagnostics (Anderies et al., 2022), system anomaly detection (Tian, 2023), and inorganic catalysis efficiency prediction (Liu et al., 2022). EAPCR consistently outperforms traditional methods in such tasks lacking explicit FRPs. To further assess its robustness, we synthesize a dataset without explicit FRPs, where models like \accnns (LeCun et al., 1998), \acgcns (Kipf & Welling, 2016; Bronstein et al., 2017), Transformers (Vaswani, 2017), and KAN (Liu et al., 2024b, a) struggle, while EAPCR excels in capturing meaningful features. In summary: • EAPCR is designed as a universal feature extractor for tasks lacking explicit FRPs, addressing a critical gap in deep learning for scientific applications. It consistently outperforms other models across various real-world scientific applications. • We synthesize a representative dataset to investigate the challenges of modeling without FRPs, revealing the limitations of traditional methods and validating the robustness and effectiveness of EAPCR. Related Works Feature engineering at the early stage: Feature engineering (Bengio et al., 2013) plays a crucial role in improving the accuracy of classification models. Early approaches primarily focus on addressing feature redundancies and nonlinear relationships. For instance, principal component analysis (PCA) (Abdi & Williams, 2010) reduces linear correlations, while nonlinear methods like nonlinear PCA (Linting et al., 2007) and autoencoders (Bank et al., 2023) handle redundancies through nonlinear transformations. Although classifiers such as \acsvms (Hearst et al., 1998), \acmlps (Rumelhart et al., 1986), and more recent models like \ackan (Liu et al., 2024b, a) can manage complex nonlinear feature relationships, their performance heavily depends on how input data is represented. For example, using large pre-trained models to encode images improves classification and retrieval accuracy by emphasizing critical features like edges and shapes (Liu et al., 2023; Holliday & Dudek, 2020; Zhou et al., 2024). Therefore, more advanced feature extraction techniques are required to go beyond capturing nonlinear relationships, further refining feature representations for better performance. Feature engineering for data with FRPs: Accurately capturing implicit correlations between features is essential for effective classification. For example, determining obesity cannot solely rely on weight; height must also be considered to provide a more accurate assessment. In more complex scenarios, classification depends on interactions between features, where their joint contribution exceeds the sum of their individual effects (Koh & Liang, 2017; Ali et al., 2012; Beraha et al., 2019; Deng et al., 2022). This is why traditional classifiers, like \acmlps (Rumelhart et al., 1986), rely on advanced feature extractors to improve performance by identifying complex feature interactions. In this vein, recent advancements, such as ConvNeXt (Woo et al., 2023), \acbert (Devlin, 2018), \acgpt (Radford et al., 2019), \acvit (Dosovitskiy, 2020), and \actft (Lim et al., 2021), efficiently capture interaction patterns of features in structured or Euclidean data like images and texts. For non-Euclidean data, techniques like manifold learning (McInnes et al., 2018; Tenenbaum et al., 2000) and \acgnns, including \acGraphSAGE (Hamilton et al., 2017) and \acDGCNN (Wang et al., 2019), address unique challenges. Regardless of the data type, these methods rely on explicit \acFRPs (e.g., spatial, sequential, or relational connections) of data, which contain the implicit feature correlations essential for effective feature extraction. Feature engineering for data without FRPs: As discussed earlier, many scientific tasks lack explicit \acFRPs. Machine learning techniques like \acfdt and \acdt-based methods perform well in these tasks by handling various data types (numerical and categorical) and automatically capturing interaction effects between features, as each decision split evaluates the relationship between a feature and the target variable (Gregorutti et al., 2017). In contrast, deep learning models, such as \accnns, \acgcns, and Transformers, struggle due to their reliance on predefined \acFRPs. For example, in heart failure and maternal health risk prediction, the best-performing models are \acrf (Coşkun & Kuncan, 2022) and \acdt (Mutlu et al., 2023). Similarly, in hepatitis C (Alizargar et al., 2023), TiO2 photocatalytic degradation (Schossler et al., 2023), and centrifugal pump health status prediction (Mallioris et al., 2024), \acxgboost, another \acdt-based method, consistently outperforms deep learning methods. Despite advancements in multimodal techniques (Lahat et al., 2015) and methods for non-Euclidean data (Bronstein et al., 2017), feature heterogeneity does not always align with distinct modalities. Even features from the same source may lack explicit \acFRPs, rendering multimodal approaches ineffective. Moreover, inconsistent feature dimensions complicate the definition of feature distances in Euclidean space. For instance, determining how a 1 kg increase in weight correlates with a change in height for a particular disease is non-trivial. Data without explicit \acFRPs differ from traditional Euclidean and non-Euclidean data (Bronstein et al., 2017), making deep learning techniques less effective in these applications."
https://arxiv.org/html/2411.08060v1,"Online Collision Risk Estimation via Monocular
Depth-Aware Object Detectors and Fuzzy Inference","This paper presents a monitoring framework that infers the level of autonomous vehicle (AV) collision risk based on its object detector’s performance using only monocular camera images. Essentially, the framework takes two sets of predictions produced by different algorithms and associates their inconsistencies with the collision risk via fuzzy inference. The first set of predictions is obtained through retrieving safety-critical 2.5D objects from a depth map, and the second set comes from the AV’s 3D object detector. We experimentally validate that, based on Intersection-over-Union (IoU) and a depth discrepancy measure, the inconsistencies between the two sets of predictions strongly correlate to the safety-related error of the 3D object detector against ground truths. This correlation allows us to construct a fuzzy inference system and map the inconsistency measures to an existing collision risk indicator. In particular, we apply various knowledge- and data-driven techniques and find using particle swarm optimization that learns general fuzzy rules gives the best mapping result. Lastly, we validate our monitor’s capability to produce relevant risk estimates with the large-scale nuScenes dataset and show it can safeguard an AV in closed-loop simulations.","Over the past decade, autonomous vehicles (AVs) have attained great development and can be seen on public roads nowadays. However, it is still possible to hear AV accidents, especially in corner cases such as severe weather conditions or the emergence of rare objects [1]. To ensure the safety of AVs and allow their wider deployment, it is important to have run-time monitors that can identify such performance-hindering situations. Correspondingly, regulations and industrial standards such as EU AI Act [2] and ISO 21448 [3] also demand the inclusion of monitoring mechanisms in safety-critical autonomous systems. In the literature, in fact, one can find various run-time monitoring techniques. Focusing on planning and control, many work derive collision risks based on ego and traffic information, e.g., driving path deviation or time-to-collision to other agents [4]. These approaches, nonetheless, often assume perfect perception, which is generally not the case. In this work, we relax such an assumption and attempt to identify hazards as early as possible in an AV software stack, such that the controller can trigger a safety maneuver in time. Holding a similar mindset, several studies have suggested to monitor the object detection function. For instance, some propose algorithms that check the spatial or temporal consistency of the set of detected objects [5, 6]. Despite effective, one crucial limitation of the existing work is the relevance between the identified anomalies and the actual safety risk of the AV. For example, these monitors may raise a warning for an object that is far from the AV, which actually poses a low risk. Likewise, they only examine the set of detected objects and ignore potential misses (i.e., false negatives of the object detector), which are more likely risk-relevant. Figure 1: The overall monitoring framework using alignment measures between two sets of predictions to infer ego vehicle’s collision risks. The dashed lines mark an offline optimization process using a previously presented risk-correlating metric, USC [7]. Hence, in this work, we aim to find safety-related errors of the object detector and use them to characterize an AV’s collision risks. Fig. 1 depicts the overall framework. In particular, our previous work presented a design-time safety-focused metric, called uncompromising spatial constraints (USC), which highly correlates with AV collision rates [7]. This work’s objective, therefore, is to reproduce it and generate a relevant risk level during operation. To achieve this, there are two challenges: • The first and main challenge lies in the lack of ground-truth labels at run time. To overcome it, we ask the critical question whether employing a separate object retrieval pipeline and measuring the inconsistencies from the original object detector’s predictions can serve as a proxy to the ground truths. Specifically, considering cost factors and the recent breakthrough in monocular depth estimation, we employ the state-of-the-art ZoeDepth [8] and implement image processing techniques to retrieve safety-critical objects. Our key insight, thereby, is an experimental validation that confirms the inconsistencies between the two sets of predictions, measured by Intersection-over-Union (IoU) and depth discrepancy, are closely linked to safety-related errors in the 3D object detector when compared to ground-truth data. • With the confirmed correlation, the second challenge is how to associate them with the desired risk quantifier, USC. Our solution is to use fuzzy logic, which can flexibly model non-linear functions while tolerating potential imprecision [9]. Concretely, we build a fuzzy inference system (FIS) using three distinct knowledge- and data-driven approaches and finally find the one adopting the global particle swarm optimization algorithm gives the best result within a separate testing dataset. To demonstrate the efficacy of our framework, we run it with a state-of-the-art 3D object detector, PGD [10], across the large-scale nuScenes dataset [11] in six conditions, including nominal scenes, night scenes, scenes with rare objects, and scenes augmented with rain, snow, or Gaussian noises. In addition, we implement our monitor on a baseline AV in simulation and showcase that it can indeed infer a useful risk indicator that helps protect the AV. Altogether, our work can be seen as a novel attempt to derive collision risk estimates from the object detection function using a single data source, i.e., images from one monocular camera. Moreover, as we shall show in later sections, the framework offers good interpretability and adaptability, allowing developers to continuously analyze and improve the monitor."
https://arxiv.org/html/2411.08037v1,Material transforms from disentangled NeRF representations,"In this paper, we first propose a novel method for transferring material transformations across different scenes. Building on disentangled Neural Radiance Field (NeRF) representations, our approach learns to map Bidirectional Reflectance Distribution Functions (BRDF) from pairs of scenes observed in varying conditions, such as dry and wet. The learned transformations can then be applied to unseen scenes with similar materials, therefore effectively rendering the transformation learned with an arbitrary level of intensity. Extensive experiments on synthetic scenes and real-world objects validate the effectiveness of our approach, showing that it can learn various transformations such as wetness, painting, coating, etc. Our results highlight not only the versatility of our method but also its potential for practical applications in computer graphics. We publish our method implementation, along with our synthetic/real datasets on https://github.com/astra-vision/BRDFTransform","In computer graphics and vision, inverse rendering is key to extracting material information and allowing re-rendering under novel conditions (viewpoint, lighting, materials, etc.). While neural representations have largely taken over the traditional Physically-Based Rendering (PBR) techniques, recent works have demonstrated that the two representations can be combined [JLX∗23], thus preserving the editability and expressivity of PBR representations along with the flexibility of neural representations. When considering the appearance of a scene, certain transformations (such as applying a coat of varnish) can alter the material properties significantly, causing the scene’s appearance to change drastically. Currently, estimating the PBR characteristics of a known material after such a transformation requires capturing the scene again in the desired target condition. This process is both complex and laborious due to the variety of possible transformations, such as wetness, dust, varnish, painting, etc. In this work, we aim to learn a BRDF transformation from a source scene and apply it to different scenes. Assuming we have paired observations of the same scene under two different conditions, say original and varnished, we propose a method to learn the transformation of materials. This transformation can then be applied to another scene composed of similar materials. This allows us to predict the appearance of that scene under this effect, effectively transferring the material transformation. Material transforms from disentangled NeRF representations illustrates that several material transformations can be learned from multiple pairs of scenes (left) and later applied on novel scenes (right), whether synthetic or real. Technically, our method relies on the joint optimization of a radiance field corresponding to a first scene captured in original and transformed (e.g., varnished) conditions, possibly with varying lighting conditions. We rely here on the disentangled NeRF representation of TensoIR [JLX∗23]—that optimizes appearance, geometry, and parametric BRDF simultaneously—while introducing two novel key components. First, we condition the transformed scene BRDF on the original scene and approximate its transformation with a Multi-Layer Perceptron (MLP). Second, we expose a limitation of TensoIR showing it fails at decomposing highly reflective materials and propose an improved light estimation scheme that better estimates low roughness components while preserving high frequencies in the illumination. As a result, our framework allows capturing a collection of transformations which can then be applied on new scenes, while controlling the intensity of the transformation. We demonstrate the performance of our method on two new datasets: a synthetic dataset with a series of custom shader transformations and a real-world dataset of figurines with varying material conditions (e.g., original, painted, varnished, etc.). On both datasets, our approach produces faithful transformations. Our method and datasets will be released publicly."
https://arxiv.org/html/2411.08034v2,"Scaling Properties of Diffusion Models 
for Perceptual Tasks","In this paper, we argue that iterative computation with diffusion models offers a powerful paradigm for not only generation but also visual perception tasks. We unify tasks such as depth estimation, optical flow, and amodal segmentation under the framework of image-to-image translation, and show how diffusion models benefit from scaling training and test-time compute for these perceptual tasks. Through a careful analysis of these scaling properties, we formulate compute-optimal training and inference recipes to scale diffusion models for visual perception tasks. Our models achieve competitive performance to state-of-the-art methods using significantly less data and compute. We release code and models at scaling-diffusion-perception.github.io.","Diffusion models have emerged as powerful techniques for generating images and videos, while showing excellent scaling behaviors. However, diffusion models can also be utilized for downstream prediction for inverse vision problems. In this paper, we present a unified framework to perform depth estimation, optical flow estimation, and amodal segmentation with a single diffusion model, as illustrated in Figure 1. Previous works such as Marigold (Ke et al., 2024), FlowDiffuser (Luo et al., 2024), and pix2gestalt (Ozguroglu et al., 2024) demonstrate the potential of repurposing image diffusion models for various inverse vision tasks individually. Building on this foundation, we perform an extensive empirical study, establishing scaling power laws for depth estimation, and display their transferability to other perceptual tasks. Using insights from these scaling laws, we formulate compute-optimal recipes for diffusion training and inference. We are the first to show that efficiently scaling compute for diffusion models leads to significant performance gains in downstream perceptual tasks. Recent works have also focused on scaling test-time compute to enhance the capabilities of modern LLMs, as demonstrated by OpenAI’s o1 model (OpenAI, 2024). As Noam Brown noted, “It turned out that having a bot think for just 20 seconds in a hand of poker got the same boosting performance as scaling up the model by 100,000x and training it for 100,000 times longer.” In our experiments, we realize a similar trade off between allocating more compute during training versus test-time for diffusion models. We scale test-time compute by exploiting the iterative and stochastic nature of diffusion to increase the number of denoising steps. By allocating more compute to early denoising steps, and ensembling multiple denoised predictions, we consistently achieve higher accuracy on these perceptual tasks. Our results provide evidence of the benefits of scaling test-time compute for inverse vision problems under constrained compute budgets, bringing a new perspective to the conventional paradigm of training-centric scaling for generative models. Figure 1: A Unified Framework: We fine-tune a pre-trained Diffusion Model (DM), for visual perception tasks. We take a RGB image, and a conditional image (i.e. next video frame, occlusion mask, etc.), along with the noised image of the ground truth prediction. Our model generates predictions for visual tasks such as depth estimation, optical flow prediction, and amodal segmentation, based on the conditional task embedding. We train a generalist model that can perform all three tasks with exceptional performance."
https://arxiv.org/html/2411.08033v1,: Interactive Point Cloud Latent Diffusion for 3D Generation,"While 3D content generation has advanced significantly, existing methods still face challenges with input formats, latent space design, and output representations. This paper introduces a novel 3D generation framework that addresses these challenges, offering scalable, high-quality 3D generation with an interactive Point Cloud-structured Latent space. Our framework employs a Variational Autoencoder (VAE) with multi-view posed RGB-D(epth)-N(ormal) renderings as input, using a unique latent space design that preserves 3D shape information, and incorporates a cascaded latent diffusion model for improved shape-texture disentanglement. The proposed method, GaussianAnything, supports multi-modal conditional 3D generation, allowing for point cloud, caption, and single/multi-view image inputs. Notably, the newly proposed latent space naturally enables geometry-texture disentanglement, thus allowing 3D-aware editing. Experimental results demonstrate the effectiveness of our approach on multiple datasets, outperforming existing methods in both text- and image-conditioned 3D generation.","3D content generation holds great potential for transforming the virtual reality, film, and gaming industries. Current approaches typically follow one of two paths: either a 2D-lifting method or the design of native 3D diffusion models. While the 2D-lifting approach (Shi et al., 2023b; Liu et al., 2023b) benefits from leveraging 2D diffusion model priors, it is often hindered by expensive optimization, the Janus problem, and inconsistencies between views. In contrast, native 3D diffusion models (Jun & Nichol, 2023; Lan et al., 2024; Zhang et al., 2024) are trained from scratch for 3D generation, offering improved generality, efficiency, and control. Despite the progress in native 3D diffusion models, several design challenges still persist: (1) Input format to the 3D VAE. Most methods (Zhang et al., 2024; Li et al., 2024) directly adopt point cloud as input. However, it fails to encode the high-frequency details from textures. Besides, this limits the available training dataset to artist-created 3D assets, which are challenging to collect on a large scale. LN3Diff (Lan et al., 2024) adopt multi-view images as input. Though straightforward, it lacks direct 3D information input and cannot comprehensively encode the given object. (2) 3D latent space structure. Since 3D objects are diverse in geometry, color, and size, most 3D VAE models adopt the permutation-invariant set latent (Zhang et al., 2023a; Sajjadi et al., 2022; Zhang et al., 2024) to encode incoming 3D objects. Though flexible, this design lacks the image-latent correspondence as in Stable Diffusion VAE (Rombach et al., 2022), where the VAE latent code can directly serve as the proxy for editing input image (Mou et al., 2023b; a). Other methods adopt latent tri-plane (Wu et al., 2024; Lan et al., 2024) as the 3D latent representation. However, the latent tri-plane is still unsuitable for interactive 3D editing as changes in one plane may not map to the exact part of the objects that need editing. (3) Choice of 3D output representations. Existing solutions either output texture-less SDF (Wu et al., 2024; Zhang et al., 2024), which requires additional shading model for post-processing; or volumetric tri-plane (Lan et al., 2024), which struggles with high-resolution rendering due to extensive memory required by volumetric rendering (Mildenhall et al., 2020). In this study, we propose a novel 3D generation framework that resolves the problems above and enables scalable, high-quality 3D generation with an interactive Point Cloud-structured Latent space. The resulting method, dubbed GaussianAnything, supports multi-modal conditional 3D generation, including point cloud, caption, and image. Specifically, we propose a 3D VAE that adopts multi-view posed RGB-D(epth)-N(ormal) renderings as the input, which are easy to render and contain comprehensive 3D attributes corresponding to the input 3D object. The information of each input view is channel-wise concatenated and efficiently encoded with the scene representation transformer (Sajjadi et al., 2022), yielding a set latent that compactly encodes the given 3D input. Instead of directly applying it for diffusion learning (Zhang et al., 2024; Li et al., 2024), our novel design concretizes the unordered tokens into the shape of the 3D input. Specifically, this is achieved by cross-attending (Huang et al., 2024b) the set latent via a sparse point cloud sampled from the input 3D shape, as visualized in Fig. 2. The resulting point-cloud structured latent space significantly facilitate shape-texture disentanglement and 3D editing. Afterward, a DiT-based 3D decoder (Peebles & Xie, 2023; Lan et al., 2024) gradually decodes and upsamples the latent point cloud into a set of dense surfel Gaussians (Huang et al., 2024a), which are rasterized to high-resolution renderings to supervise 3D VAE training. After the 3D VAE is trained, we conduct cascaded latent diffusion modeling on the latent space through flow matching (Albergo et al., 2023; Lipman et al., 2023; Liu et al., 2023c) using the DiT (Peebles & Xie, 2023) framework. To encourage better shape-texture disentanglement, a point cloud diffusion model is first trained to carve the overall layout of the input shape. Then, a point-cloud feature diffusion model is cascaded to output the corresponding feature conditioned on the generated point cloud. The generated featured point cloud is then decoded into surfel Gaussians via pre-trained VAE for downstream applications. In summary, we contribute a comprehensive 3D generation framework with a point cloud-structured 3D latent space. The redesigned 3D VAE efficiently encodes the 3D input into an interactive latent space, which is further decoded into high-quality surfel Gaussians. The diffusion models trained on the compressed latent space have shown superior performance in text-conditioned 3D generation and editing, as well as impressive image-conditioned 3D generation on general real world data."
https://arxiv.org/html/2411.08017v1,Wavelet Latent Diffusion (WaLa): Billion-Parameter 3D Generative Model with Compact Wavelet Encodings,"Large-scale 3D generative models require substantial computational resources yet often fall short in capturing fine details and complex geometries at high resolutions. We attribute this limitation to the inefficiency of current representations, which lack the compactness required to model the generative models effectively. To address this, we introduce a novel approach called Wavelet Latent Diffusion, or WaLa, that encodes 3D shapes into a wavelet-based, compact latent encodings. Specifically, we compress a 256^{3} signed distance field into a 12^{3}\times 4 latent grid, achieving an impressive 2,427× compression ratio with minimal loss of detail. This high level of compression allows our method to efficiently train large-scale generative networks without increasing the inference time. Our models, both conditional and unconditional, contain approximately one billion parameters and successfully generate high-quality 3D shapes at 256^{3} resolution. Moreover, WaLa offers rapid inference, producing shapes within two to four seconds depending on the condition, despite the model’s scale. We demonstrate state-of-the-art performance across multiple datasets, with significant improvements in generation quality, diversity, and computational efficiency. We open-source our code and, to the best of our knowledge, release the largest pretrained 3D generative models across different modalities: https://github.com/AutodeskAILab/WaLa.","Training generative models on large-scale 3D data presents significant challenges. The cubic nature of 3D data drastically increases the number of input variables the model must handle, far exceeding the complexity found in image and natural language tasks. This complexity is further compounded by storage and streaming issues. Training such large models often requires cloud services, which makes the process expensive for high-resolution 3D datasets as these datasets take up considerable space and are slow to stream during training. Additionally, unlike other data types, 3D shapes can be represented in various ways, such as voxels, point clouds, meshes, and implicit functions. Each representation presents different trade-offs between quality and compactness. Determining which representation best balances high fidelity with compactness for efficient training and generation remains an open challenge. Finally, 3D representations often exhibit complex hierarchical structures with details at multiple scales, making it challenging for a generative model to capture both global structure and fine-grained details simultaneously. Figure 2: WaLa generates 3D shapes across various input modalities (see appendix for more). To address these challenges, current state-of-the-art methods for large generative models typically employ three main strategies. The first strategy involves using low-resolution representations, such as sparse point clouds (Nichol et al., 2022c; Jun & Nichol, 2023b), low-polygon meshes (Chen et al., 2024b), or coarse grids (Cheng et al., 2023; Sanghi et al., 2023b). While these approaches reduce computational complexity, they are limited in their ability to model the full distribution of 3D shapes, struggle to capture intricate details, and often lead to lossy representations. The second approach represents 3D shapes through a collection of 2D images (Yan et al., 2024a) or incorporates images (Hong et al., 2023; Li et al., 2023a; Liu et al., 2024; Xu et al., 2023b; Siddiqui et al., 2024; Bensadoun et al., 2024) into the training loss. However, this method suffers from long training times due to the need for rendering and can fail to capture internal details of 3D shapes, as it primarily focuses on external appearances. The third strategy introduces more compactness into the input representations (Hui et al., 2024; Zhou et al., 2024; Ren et al., 2024; Yariv et al., 2024; Xiong et al., 2024; Zhang et al., 2024) to reduce the number of variables the generative model must handle. While these representations can be sparse (Ren et al., 2024; Yariv et al., 2024; Xiong et al., 2024), they are often irregular or discrete in nature making it challenging to be modeled via neural networks and can still be relatively large compared to image or natural language data (Hui et al., 2024; Zhou et al., 2024), thus making it difficult to scale the model parameters efficiently. One prominent compact input representation is wavelet-based representation, which includes Neural Wavelet (Hui et al., 2022), UDiFF (Zhou et al., 2024), and wavelet-tree frameworks (Hui et al., 2024). These methods utilize wavelet transforms and their inverses to seamlessly convert between wavelet spaces and high-resolution truncated signed distance function (TSDF) representations. They offer several key advantages: data can be easily compressed by discarding selected coefficients with minimal loss of detail, and the interrelationships between coefficients facilitate efficient storage, streaming, and processing of large-scale 3D datasets compared to directly using TSDFs (Hui et al., 2024). However, despite these benefits, wavelet-based representations remain substantially large, especially when scaling up for large-scale generative models. For example, a 256^{3} TSDF can be represented as a wavelet-tree of size 46^{3}\times 64 (Hui et al., 2024), which is equivalent to a 1440\times 1440 RGB image. Scaling within this space continues to pose significant challenges. In this work, we build upon the wavelet representation described above and introduce the Wavelet Latent Diffusion (WaLa) framework. This framework further compresses the wavelet representation to obtain compact latent encodings without significant information loss, thereby efficiently enabling us to scale a diffusion-based generative model within this space. Starting with a truncated signed distance function (TSDF) of a shape, we first convert it into 3D wavelet tree representation as in Hui et al. (2024). Then, we train a convolution-based VQ-VAE model with adaptive sampling loss and balanced fine-tuning to compress a 256^{3} TSDF into a 12^{3}\times 4 grid, achieving a remarkable 2,427\times compression ratio while maintaining an impressive reconstruction without a significant loss of detail. For example, as shown in Table 1, an Intersection over Union (IOU) of 0.978 is achieved on the GSO dataset. Compared to other representations, this approach requires fewer input variables for the generative model while retaining high reconstruction accuracy. Consequently, the generative model does not need to model local details and can focus on capturing the global structure. Moreover, by significantly reducing the number of input variables that the generative model must handle due to this compression, we enable the training of large-scale 3D generative models with up to a billion parameters, producing highly detailed and diverse shapes. WaLa also supports controlled generation through multiple input modalities without adding significant inductive biases, making the framework flexible and adaptable beyond single-view 3D reconstruction tasks. As a result, our model generates 3D shapes with complex geometry, plausible structures, intricate topologies, and smooth surfaces. This is demonstrated in Figures 1 and 2, where high-quality 3D meshes can be obtained by applying marching cubes to the SDF generated from different input modalities such as text, sketch, low-resolution voxel, point cloud, single-view, and multi-view images. In summary, we make the following contributions: • We introduce a Wavelet Latent Diffusion (WaLa) framework that tackles the dimensional and computational challenges of 3D generation with impressive compression while maximizing fidelity. • Our large billion-parameter model generates high-quality 3D shapes within two to four seconds, significantly outperforming state-of-the-art benchmarks in 3D shape generation. • Our model demonstrates exceptional versatility, accepting diverse input modalities such as single/multi-view images, voxels, point clouds, depth data, sketches, and textual descriptions (see Figure 1 and 2), making it applicable to a wide range of 3D modeling tasks. • To foster reproducibility and stimulate further research in this domain, we release what we believe is, to the best of our knowledge, the largest 3D generative model to date that works across various input modalities, comprising approximately one billion parameters. The model is available at https://github.com/AutodeskAILab/WaLa. Table 1: 3D representations compared on GSO dataset (Downs et al., 2022): Intersection over Union (IoU) for accuracy & number of input variables for generative models to evaluate complexity. Representation IoU Number of Input Variables Ground-truth SDF (256^{3}) 1.0 16,777,216 (\sim 64\text{MB}) Point Cloud (Nichol et al., 2022a) 0.8642 12,288 (\sim 0.05\text{MB}) Latent Vectors (Jun & Nichol, 2023a) 0.8576 1,048,576 (\sim 4\text{MB}) Coarse Component (Hui et al., 2022) 0.9531 97,336 (\sim 0.4\text{MB}) Wavelet tree (Hui et al., 2024) 0.9956 1,129,528 (\sim 4.3\text{MB}) WaLa 0.9780 6,912 (\sim 0.03\text{MB})"
https://arxiv.org/html/2411.07975v1,JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation,"We present JanusFlow, a powerful framework that unifies image understanding and generation in a single model. JanusFlow introduces a minimalist architecture that integrates autoregressive language models with rectified flow, a state-of-the-art method in generative modeling. Our key finding demonstrates that rectified flow can be straightforwardly trained within the large language model framework, eliminating the need for complex architectural modifications. To further improve the performance of our unified model, we adopt two key strategies: (i) decoupling the understanding and generation encoders, and (ii) aligning their representations during unified training. Extensive experiments show that JanusFlow achieves comparable or superior performance to specialized models in their respective domains, while significantly outperforming existing unified approaches across standard benchmarks. This work represents a step toward more efficient and versatile vision-language models.","(a) Benchmark Performances. (b) Visual Generation Results. Figure 1: Multimodal understanding and image generation with JanusFlow. JanusFlow surpasses the state-of-the-art unified multimodal models and several task-specific understanding models on visual understanding benchmarks. It is also capable of generating high-quality images. The resolution of the images is 384\times 384. Large language models (LLMs) have demonstrated remarkable capabilities in learning diverse knowledge and generalizing to new scenarios [88, 68, 1, 8, 7]. Leveraging these capabilities, researchers have developed sophisticated models specialized in image comprehension [57, 56, 47, 2, 15, 49] and text-to-image generation [77, 74, 71, 23]. The field has recently shifted toward creating unified systems capable of handling both tasks simultaneously. One prominent direction involves utilizing pre-trained text-to-image models for high-quality generation while training LLMs to generate conditions for these models [25, 26, 84, 27, 19]. However, this approach introduces architectural complexity and potentially constrains the model’s capabilities through maintaining separate LLM and generative components. Alternative approaches [85, 95, 96, 103, 93] propose training a single LLM for both tasks, typically incorporating either diffusion models [32, 80] or vector-quantized autoregressive models [22, 83]. Our approach builds upon recent breakthroughs in rectified flow models [60, 55, 3, 23, 61], which provide a simple framework for generative modeling while delivering exceptional empirical performance [23, 45, 36]. Building on these advances, we propose JanusFlow, a powerful unified multimodal model that seamlessly integrates rectified flow with LLM architecture. Following a minimalist design principle, our architecture requires only a lightweight encoder and decoder to adapt the LLM for rectified flow operations. To optimize JanusFlow’s performance, we implement two key strategies: First, we maintain separate vision encoders for understanding and generation tasks, preventing task interference and thus enhancing comprehension capabilities. Second, we align the intermediate representations between generation and understanding modules during training, strengthening semantic coherence in the generation process. JanusFlow shows state-of-the-art performances in both multimodal comprehension and text-to-image generation compared to existing unified approaches, and even outperforms several specialized methods. Specifically, on text-to-image generation benchmarks, MJHQ FID-30k [48], GenEval [28] and DPG-Bench [34], JanusFlow achieves scores of 9.51, 0.63 and 80.09\%, surpassing established text-to-image models including SDv1.5 [75] and SDXL [71]. In multimodal comprehension benchmarks, JanusFlow attains scores of 74.9, 70.5 and 60.3 on MMBench [62], SeedBench [46], and GQA [35], respectively, exceeding specialized models such as LLaVA-v1.5 [56] and Qwen-VL-Chat [4]. Notably, these results are achieved with a compact LLM architecture with only 1.3B parameters."
https://arxiv.org/html/2411.07945v1,"SimBase: A Simple Baseline
for Temporal Video Grounding","This paper presents SimBase, a simple yet effective baseline for temporal video grounding. While recent advances in temporal grounding have led to impressive performance, they have also driven network architectures toward greater complexity, with a range of methods to (1) capture temporal relationships and (2) achieve effective multimodal fusion. In contrast, this paper explores the question: How effective can a simplified approach be? To investigate, we design SimBase, a network that leverages lightweight, one-dimensional temporal convolutional layers instead of complex temporal structures. For cross-modal interaction, SimBase only employs an element-wise product instead of intricate multimodal fusion. Remarkably, SimBase achieves state-of-the-art results on two large-scale datasets. As a simple yet powerful baseline, we hope SimBase will spark new ideas and streamline future evaluations in temporal video grounding.","Given a natural language query and an untrimmed video, as shown in Fig 1, the task of temporal video grounding [1, 2] aims to predict the start and end time points of the video moment described by the sentence. It is an important yet challenging task with a wide spectrum of applications in video understanding and analysis [3, 4, 5, 6]. Recent years have witnessed remarkable progress in temporal video grounding, driven by deep learning techniques [7, 8, 9, 10, 11, 12, 13, 14, 15]. Despite these advancements, the network architectures for video grounding have progressively become more complex, often involving intricate designs and a large number of parameters. Most of these designs focus on two primary objectives: • Facilitating cross-modal fusion: These approaches aim to integrate visual and textual modalities at different levels of granularity for precise grounding. Existing methods often devise variants of cross-attention mechanisms to effectively combine video frames and textual cues [7, 8, 16, 10, 11, 15]. enabling models to capture fine-grained interactions between modalities. • Modeling temporal relationships: Understanding the temporal dynamics within videos is essential. Techniques often leverage graph neural networks [17] and various self-attention mechanisms [7, 8, 10, 9] to model temporal dependencies across different video frames or temporal proposals. While these sophisticated methods lead to performance improvements on several benchmarks, they also introduce challenges in algorithm analysis and comparison. For instance, the increasing complexity makes it difficult to pinpoint which components contribute most to the performance gains. This work addresses this issue by asking an opposite question: How effective can a simplified approach be? To explore this, we introduce SimBase, a simple yet powerful baseline for video grounding. SimBase leverages lightweight convolutional layers, relying on simple one-dimensional convolution instead of complex temporal modeling. For cross-modal interaction, it employs the basic element-wise product operations, avoiding intricate attention designs to fuse visual and textual modalities. Figure 1: Examples of temporal video grounding. The goal of temporal video grounding is to localize the temporal moment in an untrimmed video based on a given sentence query. Surprisingly, SimBase achieves strong results on two large-scale benchmarks, notably surpassing state-of-the-art method by a significant margin on the Charades-STA benchmark. As a straightforward yet effective baseline, we hope SimBase will inspire innovative approaches and streamline the evaluation of new ideas for video grounding. Our contributions can be summarized as: • We present SimBase, a simple baseline for temporal video grounding that primarily relies on temporal convolution for temporal modeling and element-wise product for multimodal fusion. • Experiments demonstrate that SimBase achieves state-of-the-art results on two widely used large-scale datasets. Figure 2: Overview of Simple Baseline (SimBase) for temporal video grounding. SimBase leverages lightweight, one-dimensional temporal convolutional layers instead of complex temporal structures. For cross-modal interaction, SimBase exploits Hadamard product rather than complex interaction of language and video."
https://arxiv.org/html/2411.07936v1,"Learning Disentangled Representations for 
Perceptual Point Cloud Quality Assessment via 
Mutual Information Minimization","No-Reference Point Cloud Quality Assessment (NR-PCQA) aims to objectively assess the human perceptual quality of point clouds without relying on pristine-quality point clouds for reference. It is becoming increasingly significant with the rapid advancement of immersive media applications such as virtual reality (VR) and augmented reality (AR). However, current NR-PCQA models attempt to indiscriminately learn point cloud content and distortion representations within a single network, overlooking their distinct contributions to quality information. To address this issue, we propose DisPA, a novel disentangled representation learning framework for NR-PCQA. The framework trains a dual-branch disentanglement network to minimize mutual information (MI) between representations of point cloud content and distortion. Specifically, to fully disentangle representations, the two branches adopt different philosophies: the content-aware encoder is pretrained by a masked auto-encoding strategy, which can allow the encoder to capture semantic information from rendered images of distorted point clouds; the distortion-aware encoder takes a mini-patch map as input, which forces the encoder to focus on low-level distortion patterns. Furthermore, we utilize an MI estimator to estimate the tight upper bound of the actual MI and further minimize it to achieve explicit representation disentanglement. Extensive experimental results demonstrate that DisPA outperforms state-of-the-art methods on multiple PCQA datasets.","With recent advances in 3D capture devices, point clouds have become a prominent media format to represent 3D visual content in various immersive applications, such as autonomous driving and virtual reality [4, 43]. These extensive applications stem from the rich information provided by point clouds (e.g., geometric coordinates, color). Nevertheless, before reaching the user-client, point clouds inevitably undergo various distortions at multiple stages, including acquisition, compression, transmission and rendering, leading to undesired perceptual quality degradation. Accordingly, it is necessary to develop an effective metric that introduces human perception into the research of point cloud quality assessment (PCQA), especially in the common no-reference (NR) situation where pristine reference point clouds are unavailable. In recent years, many deep learning-based NR-PCQA methods [21, 47, 51, 32, 3] have shown remarkable performance on multiple benchmarks, which can be applied directly to 3D point cloud data or 2D rendered images. Most of these methods [47, 22, 51, 3] tend to learn a unified representation for quality prediction, ignoring the fact that perceptual quality is determined by both point cloud content information and distortion pattern. Although some other models [21, 33] alternately learn content and distortion representations through different training objectives, they are still based on a single-branch network and thus may lead to highly entangled features in the representation space. From the perspective of visual rules, insufficient disentanglement between representations of point cloud content and distortion disobeys the perception mechanisms of the human vision system (HVS), further limiting performance improvement. In fact, many studies [35, 16] highlight the distinct visual processing of high-level (e.g., semantics) and low-level information (e.g., distortions) in different areas of the brain. Concretely, the left and right hemispheres of the brain are specialized in processing high-level and low-level information, respectively. These findings suggest a relatively disentangled processing mechanism in our brain, challenging existing methods that seek to learn these conflicting representations using a single network indiscriminately. The difficulty of disentangled feature learning is relatively great for NR-PCQA due to data imbalance. Specifically, although a wide range of distortion types and intensities in current PCQA datasets can enable the learning of robust low-level distortion representations, it is non-trivial to learn the representations of point cloud content that lies in a considerable high dimensional space because these PCQA datasets are extremely limited in terms of content (e.g., up to 104 contents in LS-PCQA [23]). This data limitation can lead to overfitting of NR-PCQA models regarding point cloud content, that is, when the content changes, the prediction score changes in the undesired manner, even with the same distortion pattern. As illustrated in Figure 1 (b) and (c), the NR-PCQA models PQA-Net [21] and GPA-Net [32] correctly predict the trend of quality degradation with increasing distortion intensity, but their predicted score spans deviate a lot from the ground truth in Figure 1 (a), where the content varies but the distortion pattern remains intact. Based on these observations, we expect a new disentangled representation learning framework that can obey the separate information processing mechanism of HVS, and alleviate the difficulty of content and distortion representation learning introduced by data imbalance. Figure 1: Statistics of SJTU-PCQA (part) [46] and predicted quality scores of NR-PCQA models (PQA-Net [21] and GPA-Net [32]). Quality scores of different distortion types are in lines of different colors. Red circles are to highlight the score span of different contents with the same distortion. In this paper, we propose a new Disentangled representation learning framework tailored for NR-PCQA, named DisPA. Motivated by the HVS perception mechanism, DisPA employs a dual-branch structure to learn representations of point cloud content and distortion (called content-aware and distortion-aware branches). DisPA has three steps to achieve disentanglement: 1) To address the problem introduced by data imbalance, we pretrain a content-aware encoder based on masked autoencoding strategy. Specifically, in this pretraining process, the distorted point cloud is rendered into multi-view images whose patches will be partially masked. The partially masked images are then fed into the content-aware encoder to reconstruct the rendered images of the corresponding reference point cloud. 2) To facilitate learning of distortion-aware representations, we decompose the distorted multi-view images into a mini-patch map through grid mini-patch sampling [40], which can prominently present local distortions and forces the distortion-aware encoder to ignore the global content. 3) Inspired by the utilization of mutual information (MI) in disentangled representation learning [8], we propose an MI-based regularization to explicitly disentangle the latent representations. Compared to simple linear correlation coefficients (e.g., cosine similarity), mutual information can capture the nonlinear statistical dependence between representations [15]. To achieve this, we utilize an MI estimator to estimate a tight upper bound of the MI and further minimize it to achieve straightforward disentanglement. We summarize the main contributions as follows: • We propose a novel disentangled representation learning method for NR-PCQA called DisPA, which obeys the particular HVS perception mechanism. To the best of our knowledge, DisPA is the first framework to explore representation disentanglement in PCQA. • We propose the key MI-based regularization that can explicitly disentangle the representations of point cloud content and distortion through MI minimization. • We conduct comprehensive experiments on three datasets (SJTU-PCQA [46], WPC [20], LS-PCQA [23]), and achieve superior performance over the state-of-the-art methods on all of these datasets."
https://arxiv.org/html/2411.07918v1,Isometric Transformations for Image Augmentation in Mueller Matrix Polarimetry,"Mueller matrix polarimetry captures essential information about polarized light interactions with a sample, presenting unique challenges for data augmentation in deep learning due to its distinct structure. While augmentations are an effective and affordable way to enhance dataset diversity and reduce overfitting, standard transformations like rotations and flips do not preserve the polarization properties in Mueller matrix images. To this end, we introduce a versatile simulation framework that applies physically consistent rotations and flips to Mueller matrices, tailored to maintain polarization fidelity. Our experimental results across multiple datasets reveal that conventional augmentations can lead to misleading results when applied to polarimetric data, underscoring the necessity of our physics-based approach. In our experiments, we first compare our polarization-specific augmentations against real-world captures to validate their physical consistency. We then apply these augmentations in a semantic segmentation task, achieving substantial improvements in model generalization and performance. This study underscores the necessity of physics-informed data augmentation for polarimetric imaging in deep learning (DL), paving the way for broader adoption and more robust applications across diverse research in the field. In particular, our framework unlocks the potential of DL models for polarimetric datasets with limited sample sizes. Our code implementation is available at github.com/hahnec/polar_augment.","Image data augmentation plays a crucial role in training deep neural networks by enhancing dataset diversity and mitigating the risk of overfitting [1]. Without sufficient data variety, larger deep learning models may memorize training examples rather than learning generalizable features, which results in high training accuracy but reduced performance on unseen data. This makes data augmentation essential for achieving robust, generalizable models [2]. In scenarios with limited training data, such as in polarimetric imaging, data augmentation becomes even more essential. For example, medical datasets are often constrained by the difficulty of acquiring large, annotated samples, making robust augmentation strategies a cornerstone for improving DL model generalization and performance [2]. Typical image augmentations involve pseudo-random modifications of image data including cropping [1], intensity and color variations [3], additive noise or geometric transformations [2, 4, 5]. Geometric augmentation techniques, such as rotations and image flips, are extensively used in natural images but face limitations when applied to polarimetric images. This is because polarimetric imaging captures not only the spectral intensity but also the polarization state of scattered light, which is defined by the orientation of the electric field’s oscillation plane [6, 7, 8]. As a result, standard color image rotation algorithms fail to account for changes in polarization properties. In polarimetric imaging, these properties are represented using the Mueller matrix, a 4\times4 real-valued transfer matrix that relates the Stokes vectors of the input and output light beams after interaction with a sample [9, 10, 11]. While the Mueller matrix captures full polarimetric information, conventional augmentations ignore its unique structure, highlighting the need for tailored transformation methods to prevent misleading results and support generalization. Our research addresses these limitations by developing novel augmentation techniques uniquely designed to consider the structural intricacies of the Mueller matrix. This framework introduces transformations that are both broadly applicable across polarimetric learning scenarios and rigorously grounded in physical principles. By directly accommodating the specific properties of Mueller matrix data, our method provides a crucial resource for training in applications such as semantic segmentation [12, 13], denoising models [14, 15], polarimetric demosaicking [16, 17, 18], or Mueller matrix data recovery [19]. Our proposed augmentation techniques are particularly valuable in fields constrained by limited and diverse polarimetric datasets, enabling more precise and resilient model training in data-limited environments [20, 21]. Tailoring physics-informed data augmentation specific to imaging modalities has been explored in other fields such as ultrasound [22], magnetic resonance imaging (MRI)[23], and light-fields [24]. Existing augmentation techniques for polarimetric data are predominantly tailored to radar systems in astronomy and remote sensing, where physical modeling is mainly limited to rotation and mirror transformations [25, 26, 27]. However, these methods fail to account for the unique structure of the Mueller matrix, making them unsuitable for optical polarimetric imaging applications. Previous studies on rotation-invariant parameters of the Mueller matrix and photon coordinate transformations in backscattering polarimetry provide a valuable groundwork for incorporating physical properties into augmentation methods [28, 29, 30]. Building on this foundation, our work translates physical polarimetry models directly into deep learning-based augmentations, paving the way for more representative and effective training datasets—crucial for advancing optical polarimetric imaging applications that are constrained by data limitations. Although prior research has acknowledged the challenges of augmenting optical polarimetric images [31, 32], it lacks a rigorous mathematical framework for handling Mueller matrix transformations. Additionally, evaluations in existing work rely primarily on segmentation scores without quantitative comparisons to measured data, underscoring the need for validated augmentation methods that fully account for the unique structure of Mueller matrix images. To this end, we present a simulation framework that extends traditional spatial transformations to polarimetric imaging by systematic modeling of changes in the Mueller matrix elements. Our approach ensures that augmentations follow the physical laws of polarization, closely simulating how light would interact with materials and surfaces in real-world conditions. Specifically, our findings reveal that rotating Mueller matrix images without modifying the polarization states results in erroneous polarization characteristics. By accounting for both spatial and polarization-specific variations, we can improve the generalization and accuracy of DL models in polarimetric imaging tasks. The paper is structured as follows: the Methods section reviews standard spatial transformations and introduces isometric polarimetric mappings. The Results section presents key polarimetry features, metrics, and experimental validations of our framework. We then demonstrate the effectiveness of the proposed augmentations in training a segmentation network and conclude with a discussion."
https://arxiv.org/html/2411.07901v1,TLDR: Traffic Light Detection using Fourier Domain Adaptation in Hostile WeatheR,"The scarcity of comprehensive datasets in the traffic light detection and recognition domain and the poor performance of state-of-the-art models under hostile weather conditions present significant challenges. To address these issues, this paper proposes a novel approach by merging two widely used datasets, LISA and S2TLD. The merged dataset is further processed to tackle class imbalance, a common problem in this domain. This merged dataset becomes our source domain. Synthetic rain and fog are added to the dataset to create our target domain. We employ Fourier Domain Adaptation (FDA) to create a final dataset with a minimized domain gap between the two datasets, helping the model trained on this final dataset adapt to rainy and foggy weather conditions. Additionally, we explore Semi-Supervised Learning (SSL) techniques to leverage the available data more effectively. Experimental results demonstrate that models trained on FDA-augmented images outperform those trained without FDA across confidence-dependent and independent metrics, like mAP50, mAP50-95, Precision, and Recall. The best-performing model, YOLOv8, achieved a Precision increase of 5.1860%, Recall increase of 14.8009%, mAP50 increase of 9.5074%, and mAP50-95 increase of 19.5035%. On average, percentage increases of 7.6892% in Precision, 19.9069% in Recall, 15.8506% in mAP50, and 23.8099% in mAP50-95 were observed across all models, highlighting the effectiveness of FDA in mitigating the impact of adverse weather conditions on model performance. These improvements pave the way for real-world applications where reliable performance in challenging environmental conditions is critical.","\IEEEPARstart Environmental perception, along with behaviour decision and motion control, is an essential task in the domain of autonomous driving. With artificial intelligence and computer vision techniques playing integral roles in autonomous driving, object detection plays a crucial role in understanding surrounding environmental scenarios [1, 2]. Adverse weather conditions compromise sensor accuracy and real-time performance, resulting in image degradation issues. This ultimately affects the object detection performance of autonomous vehicles [3, 4]. Detecting and recognizing traffic lights is important for ensuring safety in the field of autonomous driving. Vision-based traffic light detection and recognition in traffic scenes is still a challenge [5, 6] to be successfully overcome by the autonomous driving industry, especially when there are disturbances due to weather conditions that contribute to the degradation in the performance of object detection models. State-of-the-art (SOTA) object detection algorithms, like YOLOv5 [7], YOLOv6 [8], YOLOv8 [9] and YOLOv10 [10], perform well in clear weather conditions. However, when used in detrimental weather conditions, such as environments with rain or fog, a significant deterioration in their performance is observed, as discussed later in our work. Unsupervised Domain Adaptation (UDA) is a form of transfer learning that involves adapting a model trained on one domain that has an abundance of annotations (source domain) to demonstrate results in a related yet distinct domain (target domain) characterized by a different data distribution that lacks annotations. Training the model solely within the source domain fails to produce satisfactory outcomes owing to the occurrence of covariate shift as mentioned in [11]. The primary objective of domain adaptation techniques is to facilitate knowledge transfer from a well-labelled source domain to a target domain that lacks labels. This process is essential, as minor alterations in low-level statistics notably impact the model’s effectiveness. This is particularly relevant for leveraging existing datasets with annotations to enhance performance on related tasks that lack such annotations, like Advanced Driver Assistance Systems (ADAS), security and surveillance systems, etc. Domain adaptation thus plays a pivotal role in addressing the challenge of data scarcity in supervised learning scenarios by bridging the discrepancy between different but related domains. In our work, we combine widely used datasets LISA [12] and SJTU: Small Traffic Light Dataset (S2TLD) [13] while using various augmentations to address the class imbalance against images containing yellow traffic lights to form our source domain. Leveraging pre-existing techniques for fog addition [14] and rain addition 111https://github.com/ShenZheng2000/Rain-Generation-Python, we create a new dataset of rainy and foggy images using the images from our source dataset. This dataset serves as our target domain. We use Fourier Domain Adaptation (FDA) [11], where the Fast Fourier Transform (FFT) of each input image (clear weather) is computed, and the low-level frequencies of the target image (rainy or foggy) replace the low-level frequencies of the source image before the images are reconstituted for training using Inverse FFT (iFFT). A particular range of beta was chosen based on [11], and various values taken from that range were tested using various objection detection models. This leads to the model learning domain invariant features, i.e., traffic lights, which allows it to perform traffic light detection better than a model that was trained on a dataset with clear weather conditions in a rainy/foggy environment as observed in Fig 1 and Fig 6. The source dataset was benchmarked using various state-of-the-art methods, and their performance was qualitatively and quantitatively evaluated across different metrics (confidence dependent and independent). To address the lack of traffic light detection datasets, we used semi-supervised learning (SSL). YOLOv6 was first trained on 50% of the training data, generating pseudo-labels for the other 50%. These pseudo-labels were combined with the original labels to create a new training set. Several models were then trained on this augmented dataset to show the effectiveness of SSL. The results from inference on the target domain dataset are discussed in the next section. The contributions of our work are: • Combining widely used datasets and addressing class imbalance to synthetically generate realistic rainy and foggy images by means of effective pre-existing methods. • Proposing and demonstrating the efficacy of Fourier Domain Adaptation for Traffic Light detection in adverse weather conditions. • Implementing Semi-Supervised Learning in the realm of Traffic Light Detection as a potential solution to the lack of datasets in the domain. The paper is structured as follows, Section 2 discusses prior works adjacent to the domains relevant to this paper, Section 3 details the proposed methodology employed for the aforementioned problem statement of traffic light detection in hostile weather conditions. Later, section 4 describes the experimental setup to justify the choice of hyperparameters. Further, in Section 5, we discuss the results of our methodology by benchmarking with various unique SOTA models and explore the effect of FDA and SSL through qualitative and quantitative evaluation. Figure 1: The first row of images has rain added to them, and the second row has fog added to them. As evident, applying FDA to the images increases visibility and ability to detect."
https://arxiv.org/html/2411.07893v1,Joint multi-dimensional dynamic attention and transformer for general image restoration,"Outdoor images often suffer from severe degradation due to rain, haze, and noise, impairing image quality and challenging high-level tasks. Current image restoration methods struggle to handle complex degradation while maintaining efficiency. This paper introduces a novel image restoration architecture that combines multi-dimensional dynamic attention and self-attention within a U-Net framework. To leverage the global modeling capabilities of transformers and the local modeling capabilities of convolutions, we integrate sole CNNs in the encoder-decoder and sole transformers in the latent layer. Additionally, we design convolutional kernels with selected multi-dimensional dynamic attention to capture diverse degraded inputs efficiently. A transformer block with transposed self-attention further enhances global feature extraction while maintaining efficiency. Extensive experiments demonstrate that our method achieves a better balance between performance and computational complexity across five image restoration tasks: deraining, deblurring, denoising, dehazing, and enhancement, as well as superior performance for high-level vision tasks. The source code will be available at https://github.com/House-yuyu/MDDA-former.","In recent years, the development of artificial intelligence and deep learning has led to significant advances in image restoration. However, captured images often suffer from various degradations due to changing weather conditions and sensor noise, such as haze, rain, sensor noise, blur, or low light. These degradations can affect the precise perception of the surroundings and impact subsequent applications like object detection [1] and tracking. To prompt more precise image processing tasks for safety, image restoration is highly desired. During the past decades, for physical causes of various degradations are different, specific and strong image priors have been tailored for respective image restoration. With the development of deep learning, which could directly learn complex mapping rules between degraded and clean images from large-scale data, the performance is significantly improved compared with traditional restoration approaches. Recently, convolutional neural networks (CNN) has been successfully employed for handling different degradations due to its ability of learning generalizability, such as residual learning [2, 3, 4], attention mechanisms [5, 6]. However, intricate degradation patterns of rain, haze, etc. require for large capacity of CNN, which leads to the increasing complexity that grows with the size of the network. Recently, dynamic convolution [7, 8] has been proposed to mitigate this problem by learning kernel-wise attention for convolutional filters. In addition, CNN-based methods have inherent flaws in capturing long-range dependencies in images. Figure 1: The performance/latency comparisons among our method and the representative state-of-the-art methods on six datasets across five image restoration tasks. Since high-level vision tasks have seen great progress propelled by the self-attention (SA) [9] which could capture long-range dependency within images, Transformer-based methods have shown great potential in the image restoration (IR) field. Despite of transformer’s good performance in IR, its computational complexity increases quadratically with image resolution. To alleviate this problem, Restormer [10] introduces a novel channel-wise self-attention and gated-Dconv feed-forward network, achieving approximately linear computational complexity. Uformer decreases the computational complexity within the window by leveraging a new locally-enhanced window attention mechanism [11]. Although the above Transformer-based methods utilize different schemes to decrease computational complexities, they still cost a large number of FLOPs and has slow inference speed due to the transformer architecture, such as 140.99G-81.76ms for Restormer and 89.46G-49.08ms for Uformer with regards to FLOPs and latency tested on images with 256\times256 resolution. Compared with the CNN architecture, it is of lower efficiency to extract local information via the transformer architecture, which is performed at shallower layers [12]. Recently, Transformer blocks combined with CNNs have been designed in the U-Net [13] architecture to seek a good balance between performance and computational complexity in various tasks. Zhang et al. [14] introduced hybrid swin-conv blocks and embedded them into all the layers of U-shaped architecture for image denoising. Zhang et al. [15] proposed AFD-former by plugging asymmetric flow division unit into CNN-Transformer block for synthesized view quality enhancement. These works utilize the hybrid CNN-Transformer block in all the layers of U-shaped architectures to exploit CNN’s local modeling ability and transformer’s global modeling ability while achieving relatively low complexity. However, the effective and efficient way of where and how CNNs and transformers integrate into the U-shaped architecture needs further exploration. In the U-shaped architecture, the latent layer (the deepest layer) has the most compact representation which is beneficial for SA to extract global features while the other layers present multi-scale early feature representation suitable for CNN to extract local information. In view of this, there have been some recent works attempting to use CNNs for the encoder-decoder and transformers for the latent layer within UNet structure in other vision tasks, such as in image generation [16] and remote sensing image detection [17]. Inspired by these, we propose a general image restoration architecture termed MDDA-former by equipping transformer blocks in the latent layer and CNN-based modules in the encoder-decoder explicitly, which can fully exploit multi-scale structural differences of the U-shaped architecture. Furthermore, to improve the capacity of CNN blocks without sacrificing much efficiency, a CNN-based Multi-Dimensional Dynamic Attention Block (MDAB) is designed in the encoder-decoder to learn dynamic complementary attentions of convolutional kernels and to perform local modeling effectively, which integrates spatial-wise, channel-wise, and filter-wise attentions. The Effective Transformer Block (ETB) in the latent layer is designed to capture global contextual information, which cascades a transposed attention [10] with a linear complexity and depth-wise convolution to improve the feature representation ability. The contributions of this work can be summarized as follows: 1) A general image restoration method termed MDDA-former is designed, which can fully exploit multi-scale structural differences of U-Net architecture by plugging CNN-based modules into the encoder-decoder and transformer blocks into the latent layer to balance between performance and efficiency. 2) The Multi-dimensional Dynamic Attention Block (MDAB) is meticulously designed to learn three complimentary attentions for convolutional kernels at a price of acceptable computational complexity, which can extract rich local contextual cues of diverse degradation information effectively. 3) An efficient and effective transformer block is proposed to capture global contextual information at the latent layer which consumes a relatively smaller number of model parameters and FLOPs while not compromising on global modeling ability. 4) Substantial experiments demonstrate that the proposed method achieves a better trade-off between performance and complexity compared with most representative SOTA methods for five image restoration tasks on 18 benchmark datasets, and also obtains superior performance for high-level vision tasks."
https://arxiv.org/html/2411.07885v1,"INTRABENCH: 
Interactive Radiological Benchmark","Current interactive segmentation approaches, inspired by the success of META’s Segment Anything model, have achieved notable advancements, however they come with substantial limitations that hinder their practical application in real clinical scenarios. These include unrealistic human interaction requirements, such as slice-by-slice operations for 2D models on 3D data, a lack of iterative refinement, and insufficient evaluation experiments. These shortcomings prevent accurate assessment of model performance and lead to inconsistent outcomes across studies.IntRaBench overcomes these challenges by offering a comprehensive and reproducible framework for evaluating interactive segmentation methods in realistic, clinically relevant scenarios. It includes diverse datasets, target structures, and segmentation models, and provides a flexible codebase that allows seamless integration of new models and prompting strategies. Additionally, we introduce advanced techniques to minimize clinician interaction, ensuring fair comparisons between 2D and 3D models. By open-sourcing IntRaBench, we invite the research community to integrate their models and prompting techniques, ensuring continuous and transparent evaluation of interactive segmentation models in 3D medical imaging.","Accurate segmentation of anatomical structures or pathological areas is crucial in fields like radiology, oncology, and surgery to isolate affected regions, monitor disease progression, treatment planning and guide therapeutic procedures. Traditional supervised medical segmentation models have demonstrated strong performance across a range of anatomies and pathologies (Isensee et al., 2020; 2023; Huang et al., 2023; Ulrich et al., 2023). However, their effectiveness remains heavily constrained by the amount and diversity of available training data, with the quality of human label annotations serving as a critical limiting factor. Consequently, fully autonomous AI solutions have not yet reached performance needed for widespread autonomous clinical applications. On the other hand, numerous semi-automatic segmentation techniques, not reliant on AI, are already in clinical practice to expedite manual annotation processes Hemalatha et al. (2018). These current ad hoc methods do not tap into the potential of AI-based automation to drastically reduce annotation time. A method that allows clinicians to segment any target with just a single click within the image could greatly enhance the efficiency of clinical workflows. The release of META’s Segment Anything (SAM) model represents a big leap towards making this potential a reality (Kirillov et al., 2023). ”SAM” is designed to segment any target through different user interaction methods, including point-based and bounding box prompts. This allows users to easily specify the area of interest by clicking on it or drawing a bounding box around it, making the segmentation process both flexible and intuitive. A particularly powerful feature is the ability for users to iteratively refine initial predictions by adding more positive or negative prompts. This advanced functionality, in contrast to traditional supervised segmentation methods, has attracted a lot of attention in the medical domain, and led to many studies evaluating and adapting SAM for 3D medical image segmentation (Roy et al., 2023; Deng et al., 2023; Hu et al., 2023; Zhou et al., 2023; Mohapatra et al., 2023; Cheng et al., 2023; Ma et al., 2024; Gong et al., 2023). Moreover, several researchers have been inspired by SAM’s capabilities to develop their own methods, often specifically designed for the 3D nature of radiological data (Du et al., 2024; He et al., 2024; Li et al., 2024; Wang et al., 2024). Although these domain-specific adaptations on medical data have shown promising progress, many published methods are plagued by pitfalls which obfuscate the efficacy of the models and prevent clinicians and researchers from determining the best methods for their use-cases: Figure 1: a) Current approaches require clinicians to interact with radiological images slice by slice, leading to increased workload. b) Some models operate natively in 3D and enable full 3D interaction. Only models that accept mask prompts allow iterative refinement of initial predictions with human guidance. Applying interactive 2D models to 3D data on a slice-by-slice basis (P1): Assuming clinicians will interact with each slice individually is unrealistic and undermines the efficiency improvements these methods aim for. Moreover, a slice-by-slice approach introduces an unfair bias when comparing 2D and 3D models, as 3D models typically require only a few interaction per image, leading to significantly fewer interactions and less supervision Cheng et al. (2023); Ma et al. (2024); Zhang & Liu (2023); Wu et al. (2024); Wong et al. (2024). Neglecting refinement (P2): Many studies assess interactive segmentation methods based on a single interaction step, overlooking the inherent ambiguities in radiological images (Ma et al., 2024; Du et al., 2024; Gong et al., 2023; Bui et al., 2024). Often, a second interaction may be necessary to specify which specific substructure the clinician wants to segment. This could be, e.g. a vessel within the liver, or the necrosis within a tumor, as exemplified in the well-known BraTs segmentation challenge (de Verdier et al., 2024). Furthermore, clinicians often want to adapt the segmentations to their clinic’s local protocol or refine them, particularly for targets with high inter-rater variability, like pathological structures (Fu et al., 2014; Benchoufi et al., 2020; Hesamian et al., 2019). Overall, there is a notable lack of research exploring realistic, iterative interaction methods for 2D models applied to 3D volumes. Obfuscated and insufficient evaluation (P3): With promptable models only recently garnering great attention, there is a lack of a standardized approach to evaluation, which has led to disparate and incomparable methods, which are at times even obfuscated or insufficient. We observed the following shortcomings: (i) Not specifying whether predictions were interactively refined or based on a single prompt with multiple points (Cheng et al., 2023; Wang et al., 2024). (ii) Being intransparent on the number of initial prompts given (Du et al., 2024). (iii) Using the best mask rather than the final mask after interactive refinement (Wang et al., 2024). (iv) Evaluating predictions slice-by-slice or on sub-patches of a 3D volume instead of evaluating on the full image (Roy et al., 2023; Ma et al., 2024; Cheng et al., 2023; He et al., 2024; Li et al., 2024). (v) Excluding targets considered ’too small’, hence neglecting valid targets such as small lesions that are neither tested nor trained on Ma et al. (2024); Cheng et al. (2023); Wang et al. (2024). (vi) Comparing against non-promptable models and SAM, rather than any other promptable model trained on medical data (Cheng et al., 2023; Ma et al., 2024; Gong et al., 2023; He et al., 2024). (vii) Lastly, overemphasizing segmenting healthy structures, such as organs, where existing supervised public models already perform well (Wasserthal et al., 2023; Ulrich et al., 2023), instead of focusing on pathologies, where interactive refinement could provide the greatest benefits (Wang et al., 2024; Zhang & Liu, 2023). To address these pitfalls, a benchmark is needed, aligning with the recent review paper from Marinov et al. (2024). To this end, we introduce IntRaBench, a reproducible and extendable Interactive Radiological Benchmark. Through it, we highlight the most performant 2D and 3D interactive segmentation and the best prompting methods in the radiological domain. In this paper, we present experiments carefully designed to replicate a clinical workflow as closely as possible, with the following key contributions: 1. IntRaBench, for the first time, enables a fair comparison of the most influential 2D and 3D interactive segmentation methods. By measuring the number of simulated interactions, a proxy for the ’Human Effort’, we test different prompting strategies that do not require a slice-wise interaction (P1). 2. We propose effective interaction strategies for refinement of predictions in a 3D volume, without requiring clinicians to interact with each individual slice (P2). 3. We provide a standardized evaluation protocol to generate prompts, select model outputs and compute the segmentation metrics on the entire image across ten datasets, covering various modalities and target structures, including small lesions (P3). Our benchmarking efforts include a performance comparison against leading interactive segmentation methods in the medical domain. 4. The extendable IntRaBench framework allows developers to a) easily evaluate a new method in a fair manner against established methods and b) easily develop and investigate new prompting strategies. Through open-sourcing IntRaBench, we invite researchers to integrate their methods into our framework, promoting continuous and equitable assessment that allows us to track the overall progress in the field of interactive 3D medical image segmentation reproducibly and transparently. Figure 2: IntRaBench overview. Although our evaluation is performed on entire 3D volumes, the benchmark accommodates both 3D and 2D interactive segmentation methods. While 3D model prompting is relatively straightforward, we introduce prompting and refinement strategies for 2D models that minimize the effort required from human interaction. The benchmark is designed to be extensible, and researchers are encouraged to propose and integrate additional methods seamlessly using our codebase particularly for areas marked by three dots."
https://arxiv.org/html/2411.07863v1,CDXFormer: Boosting Remote Sensing Change Detection with Extended Long Short-Term Memory,"In complex scenes and varied conditions, effectively integrating spatial-temporal context is crucial for accurately identifying changes. However, current RS-CD methods lack a balanced consideration of performance and efficiency. CNNs lack global context, Transformers have quadratic computational complexity, and Mambas are restricted by CUDA acceleration. In this paper, we propose CDXFormer, with a core component that is a powerful XLSTM-based feature enhancement layer, integrating the advantages of linear computational complexity, global context perception, and strong interpret-ability. Specifically, we introduce a scale-specific Feature Enhancer layer, incorporating a Cross-Temporal Global Perceptron customized for semantic-accurate deep features, and a Cross-Temporal Spatial Refiner customized for detail-rich shallow features. Additionally, we propose a Cross-Scale Interactive Fusion module to progressively interact global change representations with spatial responses. Extensive experimental results demonstrate that CDXFormer achieves state-of-the-art performance across three benchmark datasets, offering a compelling balance between efficiency and accuracy. Code is available at https://github.com/xwmaxwma/rschange.","The rapid progress in Earth observation technology, including advancements in remote sensing platforms and sensors, has expanded the ability to monitor surface activities. Remote Sensing Change Detection (RS-CD) focuses on identifying changes in objects of interest by comparing images of the same area taken at different times. This enables both quantitative and qualitative assessments of geographical and environmental changes, with applications in urban planning [1], disaster assessment [2], and environmental monitoring [3]. RS-CD tasks are inherently multi-scale and multi-temporal, with effective change detection relying on the aggregation of spatial and temporal context. Traditional methods—algebraic-based [4, 5], transformation-based [6, 7, 8], and classification-based [9, 10]—depend heavily on handcrafted features, which struggle with complex environments and limited information aggregation. CNN-based methods introduced deep learning to RS-CD by designing multi-scale feature fusion structures for improved spatio-temporal modeling [11, 12]. Techniques like deeper CNNs [3, 13, 14], dilated convolutions [15], attention mechanisms [16, 17, 18], multi-scale convolutions [19], and 3D convolution’s inner fusion properties [20] have been explored, but challenges remain in modeling long-range dependencies. After that, Transformer-based methods have gained traction for RS-CD due to their global self-attention mechanism, which models dependencies across spatio-temporal feature maps [21, 22]. These approaches focus on cross-scale [23] and cross-temporal fusion [24], achieving strong results in global spatio-temporal modeling. However, these methods often suffer from quadratic computational complexity caused by the self-attention calculation. Recently, Mamba-based methods [25] have gained rapid popularity for their linear complexity and global perception capabilities. While they offer a competitive solution, their reliance on CUDA acceleration and suboptimal performance remain limitations.In response, XLSTM has emerged with its exponential gating mechanism and matrix-parallel memory [26, 27], combining the advantages of Mamba with parallel acceleration and enhanced interpret-ability. We aim to explore the application of XLSTM in RS-CD for the first time, enabling models to capture change representations more intuitively and efficiently. Figure 1: Overall architecture of CDXFormer, which consists of a Siamese backbone, a Feature Enhancer (FE) layer, and a Cross-scale Interactive Fusion (CSIF) module. We introduce Cross-Temporal Spatial Refiner (CTSR) blocks and Cross-Temporal Global Perceptron (CTGP) blocks in the FE layer to extract bi-temporal features, focusing on spatial details and global context, respectively. Here, Bi-mLSTM refers to an mLSTM module that performs bidirectional scanning. Then, the CSIF module facilitates sufficient interaction between cross-scale features. In this paper, we evaluate the strengths and limitations of CNNs, Transformers, and Mambas, while highlighting the potential of XLSTM for RS-CD tasks. First, bi-temporal images generate multi-scale feature maps through a Siamese backbone. The core component of CDXFormer is a powerful XLSTM-based feature enhancement (FE) layer that specifically incorporates scale-specific scanning strategies. Specifically, Cross-Temporal Global Perceptron (CTGP) is introduced at the deeper layer aiming to enhance the semantic differences of the objects of interest in the bi-temporal image based on global perception. Considering that shallow features are relatively semantically inaccurate and rich in spatial details, Cross-Temporal Spatial Refiner (CTSR) is introduced at the shallow layer, which reduces the interference of background pixels and complements the spatial response to variations through axial scanning. Specifically, we employed a shared-parameter bidirectional scanning mLSTM block [27] (Bi-mLSTM). Finally, we recognize that the largest-scale branch, with its comprehensive spatial information, is crucial for accurate change detection. We propose a Cross-scale Interactive Fusion module (CSIF), which uses the largest-scale branch as a foundation to progressively integrate spatial information and global semantics from smaller-scale branches. Overall, our contributions can be summarized as follows: 1) We analyze the strengths and potential of XLSTM in comparison to CNNs, Transformers, and Mambas, and applied it to RS-CD tasks for the first time. With its linear complexity, global context awareness, parallel acceleration, and enhanced interpret-ability, XLSTM enables more intuitive and efficient differentiation of changes of interest. 2) We customize a scale-specific XLSTM scanning strategy for RS-CD tasks to reduce redundancy. Specifically, we propose a Cross-Temporal Global Perceptron (CTGP) and a Cross-Temporal Spatial Refiner (CTSR) to effectively capture both local spatial details and global contextual changes. 3) We propose a Cross-scale Interactive Fusion module (CSIF) to progressively integrate spatial information and global semantics. 4) Our experiments on three RS-CD benchmark datasets demonstrate that CDXFormer surpasses previous SOTA approaches, achieving a superior balance between accuracy and efficiency."
https://arxiv.org/html/2411.07834v1,Towards Vision Mixture of Experts for Wildlife Monitoring on the Edge,"The explosion of IoT sensors in industrial, consumer and remote sensing use cases has come with unprecedented demand for computing infrastructure to transmit and to analyze petabytes of data. Concurrently, the world is slowly shifting its focus towards more sustainable computing. For these reasons, there has been a recent effort to reduce the footprint of related computing infrastructure, especially by deep learning algorithms, for advanced insight generation. The ‘TinyML’ community is actively proposing methods to save communication bandwidth and excessive cloud storage costs while reducing algorithm inference latency and promoting data privacy. Such proposed approaches should ideally process multiple types of data, including time series, audio, satellite images, and video, near the network edge as multiple data streams has been shown to improve the discriminative ability of learning algorithms, especially for generating fine grained results. Incidentally, there has been recent work on data driven conditional computation of subnetworks that has shown real progress in using a single model to share parameters among very different types of inputs such as images and text, reducing the computation requirement of multi-tower multimodal networks. Inspired by such line of work, we explore similar per patch conditional computation for the first time for mobile vision transformers (vision only case), that will eventually be used for single-tower multimodal edge models. We evaluate the model on Cornell Sap Sucker Woods 60, a fine grained bird species discrimination dataset. Our initial experiments uses 4X fewer parameters compared to MobileViTV2-1.0 with a 1% accuracy drop on the iNaturalist ’21 birds test data provided as part of the SSW60 dataset.","Tuia et al. (2022) recently presented a call for capacity building from the (tiny) deep learning community to support ecological studies as insights from environmental monitoring data can help us stay up to date with species populations that are intricately tied to our food safety, disease transport patterns, and biodiversity sustenance Mora et al. (2022); Owino et al. (2022). Current remote sensing technologies, however, generate massive amounts of data and often struggle in limited connectivity situations. For instance, drone flights generate hundreds of Terabytes of footage in one flight Mechan et al. (2023); Kong et al. (2022); Guo et al. (2022). In fact, in some cases, it is more practical to fly with collected data across continents on hard drives in order to process it in the cloud Beery (2023), making it nearly impossible for analysis in near real time applications Kay et al. (2022); Winkler et al. (2023). In spite of the documented growing need for processing multimodal data on the network edge Speaker et al. (2022) with state of the art deep learning algorithms, current methods aren’t reliable and energy efficient enough for the expected workload. Recent years have seen an increase in energy efficient models for machine perception, led by efficient convolution networks Sandler et al. (2018); Hu et al. (2018); Lin et al. (2020), with insights from Vision Transformers quickly becoming more competitive in this domain as well Chen et al. (2022); Mehta & Rastegari (2021). Given that deep learning algorithms tend to perform better with more parameters and with more varied data, advances in efficient models either use compact representations of feature space (eg spiking networks) Cordone et al. (2022), clever ways to encode similar feature spaces of larger models (e.g. learned sparsity, low bit representation) Cai et al. (2022); Nayak et al. (2019) and more recently, multimodal data Xu et al. (2022). Due to the inherently multimodal and time aligned nature of the data generated on the edge (drone audio-visual footage, infrared sensors, passive acoustic recorders, etc.) we believe more research for multimodal edge models without incurring excessive extra computation cost is of particular importance and that mixture of experts is a promising candidate solution. 1.1 Choice of problem domain We choose ecological monitoring as our problem space because it not only provides an interesting study ground for our investigations for edge deep learning but also addresses pressing societal issues. Species Classification. Species classification is well represented in the fine grain visual categorization research area due to the inherent difficulty in discriminating between closely related species, sometimes even for experts. This is evident in the long list of datasets such as Van Horn et al. (2018); Khosla et al. (2011); Kumar et al. (2012); Wah et al. (2011). The challenge to discriminate such species near the monitoring site is further exacerbated by susceptibility to common sensor noise, and factors such as weather variability might not be observed in well curated datasets. This makes ecological monitoring data very relevant to modern edge deep learning research. Connected devices on the network edge. The concept of collaborative edge deep learning has been explored in depth in the federated learning community. However, it usually assumes replicas of the same model or different small models that independently look at different local data (data parallelism) Reisser et al. (2021); Zec et al. (2020); Parsaeefard et al. (2021); Guo et al. (2020). In our work, we seek to explore how subnetworks of “a single” conventional deep model can be optimized for easy adaptation in a multi-device setting (much like training a billion/trillion parameter model using multiple GPU machines in cloud clusters because the model cannot fit on one device). This fully network-edge model parallelism approach is relatively recent, with current proposals mostly focusing on reinforcement learning approaches of splitting the model Sen & Shen (2023, 2022), while we focus on learning semantically meaningful subnetworks that can be split across devices. 1.2 Contributions Our work is heavily inspired by Language Image Mixture of Experts Mustafa et al. (2022), a single tower multimodal architecture with many attractive benefits for the edge use case including conditional execution of structured subnetworks based on input data, easy dropping of redundant tokens, and co-learning of multimodal embeddings, that fits the “super model” concept we are interested in. To the best of our knowledge, this patch level conditional transformer architecture for tiny deep models has not been explored yet, with the exception of a very recent work Daxberger et al. (2023) which used image level mixture of experts. Our main contributions are (1) a modified model architecture of MobileViT Mehta & Rastegari (2021) to support patch level mixture of expert computation towards per-patch single tower models (2) an evaluation of the proposed model on the iNaturalist ’21 birds data in the Cornell Sapsucker Woods 60 dataset Van Horn et al. (2022) for fine-grained classification of highly confused bird species. As in Tuia et al. (2022), we hope this work inspires more researchers and enthusiasts in the efficient machine learning community to participate in the high resolution ecological monitoring space towards a sustainable future for species on the planet. Figure 1: Our proposed system based on the MobileViT Mehta & Rastegari (2021) model. The expert assignment router for any transformer mixture of expert layer is initialized with hierarchical clustering of sample training data embeddings collected after the attention computation of a pretrained network."
https://arxiv.org/html/2411.07802v1,Large-scale Remote Sensing Image Target Recognition and Automatic Annotation,"This paper presents a method for object recognition and automatic labeling in large-area remote sensing images called LRSAA. The method integrates YOLOv11 and MobileNetV3-SSD object detection algorithms through ensemble learning to enhance model performance. Furthermore, it employs Poisson disk sampling segmentation techniques and the EIOU metric to optimize the training and inference processes of segmented images, followed by the integration of results. This approach not only reduces the demand for computational resources but also achieves a good balance between accuracy and speed. The source code for this project has been made publicly available on https://github.com/anaerovane/LRSAA.","Remote sensing target recognition technology refers to the technique of utilizing image data acquired from satellites or aircraft, and through computer processing and analysis, achieving automatic detection and classification of specific targets on the ground. This technology plays a crucial role in environmental protection, resource management, disaster monitoring, and national defense security, providing timely and accurate information support for decision-making. With the application of deep learning-based target recognition methods, the accuracy and efficiency of remote sensing target recognition have been significantly improved, making it possible to detect small targets and recognize targets under complex backgrounds, thus further expanding its application scope. Nonetheless, remote sensing object recognition technology still faces several challenges and holds potential for further optimization. First, since the inception of object recognition algorithms, this field has undergone significant technological evolution and developmental phases. Recent advancements have primarily centered around Google’s MobileNetV3-SSD and Ultralytics’ YOLOv11 models, which have notably enhanced the accuracy and computational efficiency of object detection. Despite the widespread application of earlier object recognition methods in remote sensing image analysis, the adoption of the latest generation models, such as YOLOv11 and MobileNetV3-SSD, remains relatively limited. Furthermore, current remote sensing object recognition technologies predominantly rely on single-model training and prediction, a strategy that simplifies the prediction architecture and reduces computational costs but may also compromise the model’s generalization ability and robustness. At present, there is a lack of an effective framework to integrate the outputs from various remote sensing algorithms, which could lead to improved recognition performance. Lastly, most existing remote sensing object recognition schemes are designed for object identification and inference tasks on small-scale remote sensing images, exhibiting deficiencies in recognition accuracy and excessive demands on computational resources when applied to large-scale images. This limitation hinders their applicability in automated annotation systems for large-scale remote sensing imagery. Figure 1: Illustration of LRSAA’s Effect on Large-Scale Images( Tianjin city remote sensing image) To address these challenges, we propose an innovative framework aimed at improving the performance of object recognition and automatic labeling in large-scale remote sensing images. This framework not only addresses the applicability and efficiency issues of existing technologies on large datasets but also seeks to minimize hardware resource dependencies through optimized algorithm design, thereby facilitating broader practical applications. Our approach introduces several key technical innovations: • Ensemble Learning with MobileNetV3-SSD and YOLOv11: We integrate the MobileNetV3-SSD and YOLOv11 models to leverage ensemble learning, combining the strengths of both architectures to achieve superior performance. • Enhanced Non-Maximum Suppression (NMS) via EIOU Metric: We substitute the conventional Intersection over Union (IoU) metric with the Enhanced IoU (EIOU) metric for NMS, which improves the suppression process by considering additional geometric information. • Poisson Disk Sampling for Dataset Partitioning and Recognition: Utilizing Poisson disk sampling techniques, we effectively partition and recognize training and application datasets, ensuring a balanced distribution of samples and enhancing the representativeness of the data. Through these technological advancements, we have trained and evaluated the Large-Scale Remote Sensing Automatic Annotation (LRSAA) model using the XView dataset. Subsequently, the model was applied to remote sensing images of Tianjin for automatic annotation. To further validate our approach, we incorporated a certain proportion of synthetic data, generated from automatically annotated images, into real remote sensing images. The results confirmed that the inclusion of synthetically annotated data can significantly enhance the recognition capabilities of the automatic annotation model. We anticipate that this work will contribute to the advancement of remote sensing image analysis, fostering more efficient and accurate acquisition and utilization of geographic information."
https://arxiv.org/html/2411.07799v1,"Horticultural Temporal Fruit Monitoring via
3D Instance Segmentation and Re-Identification using Point Clouds","Robotic fruit monitoring is a key step toward automated agricultural production systems. Robots can significantly enhance plant and temporal fruit monitoring by providing precise, high-throughput assessments that overcome the limitations of traditional manual methods. Fruit monitoring is a challenging task due to the significant variation in size, shape, orientation, and occlusion of fruits. Also, fruits may be harvested or newly grown between recording sessions. Most methods are 2D image-based and they lack the 3D structure, depth, and spatial information, which represent key aspects of fruit monitoring. 3D colored point clouds, instead, can offer this information but they introduce challenges such as their sparsity and irregularity. In this paper, we present a novel approach for temporal fruit monitoring that addresses point clouds collected in a greenhouse over time. Our method segments fruits using a learning-based instance segmentation approach directly on the point cloud. Each segmented fruit is processed by a 3D sparse convolutional neural network to extract descriptors, which are used in an attention-based matching network to associate fruits with their instances from previous data collections. Experimental results on a real dataset of strawberries demonstrate that our approach outperforms other methods for fruits re-identification over time, allowing for precise temporal fruit monitoring in real and complex scenarios.","The challenge of meeting a growing demand for food requires advances in agricultural practices, with a focus on efficiency and sustainability. Autonomous robots offer new possibilities to automate labor-intensive tasks such as crop monitoring and management. Such systems have the potential to enhance agricultural production systems, and can enable continuous and large-scale monitoring [8, 38, 33]. In particular, these technologies support phenotyping, the process of evaluating plant characteristics by providing precise, high-throughput assessments and surpass the limitations of traditional manual methods [9, 40]. This shift towards automated phenotyping represents a critical step forward in optimizing crop selection and improving crop yield. Temporal matching, or fruit re-identification, on top of accurate instance segmentation, allows for tracking the development of single fruits over time, enabling the analysis of growth patterns and the estimation of maturation rates. Figure 1: Fruit re-identification on three point clouds acquired at three different points in time. Fruit instances are first segmented using an instance segmentation method, then they are temporally matched with fruit instances of a previous data collection (e.g., matching fruits in \mathcal{P}^{t} with fruits in \mathcal{P}^{t-1}). Green lines indicate matching between fruits, while fruits inscribed in a circle do not have a matching fruit in the previous data collection. In this paper, we aim to track fruits in a greenhouse using colored point clouds acquired by a robot equipped with a high-resolution LiDAR scanner. The goal is to identify, segment, and temporally match individual fruits at different points in time within the 3D space. 2D image-based approaches [30, 10, 13, 11, 19] lack the 3D structure, depth, and spatial information offered by point clouds. Conversely, point clouds lack a regular grid structure, making it difficult to apply traditional image processing techniques. The challenge is to process this sparse and irregular data to detect and associate individual fruits, which may vary in size, shape, orientation, and occlusion levels and may be harvested or newly grown. Once fruit instances have been segmented, the re-identification task involves recognizing and matching the same fruit instances across different point clouds captured at different points in time or from different viewpoints. Similar to loop closures in SLAM [35] and visual place recognition [37] systems, this task is usually performed assuming unique landmarks. In the context of fruit re-identification, there are no unique traits that make fruits easily distinguishable. As depicted in Fig. 1, fruits can be very similar, tightly packed, and their pose can change significantly over time, causing trivial solutions based on relative position to fail. The main contribution of this paper is a novel method for accurately performing fruit instance segmentation and re-identification on point clouds captured by an agricultural robot at different points in time, based on a learned descriptor encoder and an attentive matcher. We exploit dense high-precision point clouds recorded with a high precision Faro laser scanner. While these sensors are not commonly employed in robotic applications, their capability to scan detailed environments has recently garnered attention, leading to their integration into robotic systems [33]. We segment fruits using a learning-based instance segmentation, which infers fruit instances directly from the point cloud. Each fruit is then processed by a 3D sparse convolutional neural network to obtain a per instance descriptor. Then, we match each fruit with its corresponding instance from a previous data collection by using their descriptors and an attention-based matching neural network. To handle the possibility of a no-match scenario, where a query fruit is identified as a new instance, we represent it using a specific descriptor. We then predict a probability distribution over the candidate fruits from a previous data collection, including the no-match option. Each query fruit is subsequently matched with the previous instance that has the highest predicted probability, using a greedy assignment to determine associations. In sum, we make two key claims: (i) our approach is able to identify fruits in point clouds using an instance segmentation method; (ii) it outperforms baseline approaches on the re-identification task using real-world data collected in a greenhouse. These claims are backed up by the paper and our experimental evaluation. By harnessing 3D data and sparse convolution networks, our method significantly enhances the effectiveness and scalability of fruit segmentation and tracking, outperforming baseline approaches and offering new capabilities in automated crop monitoring. The implementation of our method is available at https://github.com/PRBonn/IRIS3D."
https://arxiv.org/html/2411.07794v1,Feature Fusion Transferability Aware Transformer for Unsupervised Domain Adaptation,"Unsupervised domain adaptation (UDA) aims to leverage the knowledge learned from labeled source domains to improve performance on the unlabeled target domains. While Convolutional Neural Networks (CNNs) have been dominant in previous UDA methods, recent research has shown promise in applying Vision Transformers (ViTs) to this task. In this study, we propose a novel Feature Fusion Transferability Aware Transformer (FFTAT) to enhance ViT performance in UDA tasks. Our method introduces two key innovations: First, we introduce a patch discriminator to evaluate the transferability of patches, generating a transferability matrix. We integrate this matrix into self-attention, directing the model to focus on transferable patches. Second, we propose a feature fusion technique to fuse embeddings in the latent space, enabling each embedding to incorporate information from all others, thereby improving generalization. These two components work in synergy to enhance feature representation learning. Extensive experiments on widely used benchmarks demonstrate that our method significantly improves UDA performance, achieving state-of-the-art (SOTA) results.","Deep neural networks (DNNs) have achieved remarkable breakthroughs across various application fields owing to their impressive automatic feature extraction capabilities. However, such success often relies on the availability of large labeled datasets, which can be challenging to acquire in many real-world scenarios due to the significant time and labor required. Fortunately, unsupervised domain adaptation (UDA) techniques [40] offer a promising solution by harnessing rich labeled data from a source domain and transferring knowledge to target domains with limited or no labeled examples. The essence of UDA lies in identifying discriminant and domain-invariant features shared between the source domain and target domain within a common latent space [44]. Over the past decade, as interests in domain adaptation research have grown, numerous UDA methods have emerged and evolved [50, 20, 24], such as adversarial adaptation, which focuses on discriminating domain-invariant and domain-variant features and acquiring domain-invariant feature representations through adversarial learning [52, 24]. Besides, deep unsupervised domain adaptation techniques usually employ a pre-trained Convolutional Neural Network (CNN) backbone [19]. Recently, the self-attention mechanism and vision transformer (ViT) [7, 48, 41] have received growing interest in the vision community. Unlike convolutional neural networks that gather information from local receptive fields of the given image, ViTs leverage the self-attention mechanism to capture long-range dependencies among patch features through a global view. In ViT and many of its variants, each image is partitioned into a series of non-overlapping fixed-size patches, which are then projected into a latent space as patch tokens and combined with position embeddings. A class token, representing the entire image, is prepended to the patch tokens. All tokens are then fed into a specific number of transformer layers to learn visual representations of the input image. Leveraging the superior global content capture capability of the self-attention mechanism, ViTs have demonstrated impressive performance across various vision tasks, including image classification [7], video understanding [11], and object detection [1]. Despite increasing interest, only a few studies have explored the application of ViTs for unsupervised domain adaptation tasks [44, 34, 43, 50]. In this work, we introduce a novel Feature Fusion Transferability Aware Transformer, designed for unsupervised domain adaptation. FFTAT builds upon TVT [44], the first ViT-based UDA model, by introducing two key components: (1) a transferability graph-guided self-attention (TG-SA) mechanism that enhances information from highly transferable features while suppressing information from less transferable features, and (2) a carefully designed features fusion (FF) operation that makes each embedding incorporate information from other embeddings in the same batch. Fig. 1 illustrates the transferability graph guided self-attention and feature fusion. From a graph view, vanilla self-attention among patches can be seen as an unweighted graph, where the patches are considered as nodes, and the attention between nodes is regarded as the edge connecting them. Unlike vanilla self-attention, our proposed transferability graph guided self-attention is controlled by a weighted graph, where the information communication between highly transferable patches is emphasized via a large-weight edge, and the information communication between less transferable patches is attenuated by a small-weight edge [48, 47]. The transferability graph is automatically learned and updated through learning iterations in the transferability-aware layer, where we design a patch discriminator to evaluate the transferability of each patch. The TG-SA allows for integrative information processing, facilitating the model to focus on domain-invariant features shared between domains and gather important information for domain adaptation. The Feature Fusion (FF) operation enables each embedding to integrate information from other embeddings. Different from recent work PMTrans [53] for unsupervised domain adaptation, our feature fusion occurs in the latent space rather than on the image level. These two new components synergistically enhance robust feature representation learning and generalization in UDA tasks. Extensive experiments on widely used UDA benchmarks demonstrate that FFTAT significantly improves UDA performance, achieving new state-of-the-art results. In summary, our contributions are as follows: • We introduce a novel transferability graph-guided attention mechanism in ViT architecture for UDA, enhancing performance by promoting attention between highly transferable features while suppressing attention between less transferable ones. • We propose a feature fusion technique that enhances feature learning and generalization capabilities for UDA. • Our proposed model, FFTAT, integrates transferability graph-guided attention and feature fusion mechanisms, resulting in notable advancements and state-of-the-art performance on widely used UDA benchmarks. Figure 2: The overview of the FFTAT framework. In FFTAT, source and target images are divided into non-overlapping fixed-size patches which are linearly projected into the latent space and concatenated with positional information. A class token is prepended to the image tokens. The tokens are subsequently processed by a transformer encoder. The Feature Fusion Layer mixes the features as illustrated in Fig. 1. The patch discriminator assesses the transferability of each patch and generates a transferability graph, which is used to guide the attention mechanism in the transformer layers. The classifier head and self-clustering module operate on source domain images and target domain images, respectively. The Domain Discriminator predicts whether an image belongs to the source or target domain."
https://arxiv.org/html/2411.07765v1,Novel View Synthesis with Pixel-Space Diffusion Models,"Synthesizing a novel view from a single input image is a challenging task. Traditionally, this task was approached by estimating scene depth, warping, and inpainting, with machine learning models enabling parts of the pipeline. More recently, generative models are being increasingly employed in novel view synthesis (NVS), often encompassing the entire end-to-end system. In this work, we adapt a modern diffusion model architecture for end-to-end NVS in the pixel space, substantially outperforming previous state-of-the-art (SOTA) techniques. We explore different ways to encode geometric information into the network. Our experiments show that while these methods may enhance performance, their impact is minor compared to utilizing improved generative models. Moreover, we introduce a novel NVS training scheme that utilizes single-view datasets, capitalizing on their relative abundance compared to their multi-view counterparts. This leads to improved generalization capabilities to scenes with out-of-domain content.","Figure 1: Novel view synthesis results from our diffusion model. Source views are taken from RealEstate10K [57], and fed into our base and SR models to produce a 256\times 256-pixel prediction. Our end-to-end system implicitly learns to preserve the features in the source view, transform their position along with the camera movement, and generate realistic details in unseen areas. In novel view synthesis (NVS), we aim to recreate a snapshot of a given scene from an unseen perspective. A successful algorithm must consider both the 3D geometry of the given scene and the underlying distribution of real-world images. NVS algorithms have been researched for many years, covering several tasks which differ in the expected number of input and output views and the relative distance between inputs and outputs, among other things. In this work, we focus on the simplest form of NVS: single-image to single-image, hoping that it will serve as a foundation for more general NVS settings. In previous works, this task has been decomposed into a pipeline of several computer vision components, namely depth estimation, warping, and inpainting [48, 36, 19, 3]. However, modern advances in generative modeling offer a simpler and more robust end-to-end approach [27, 42, 52, 35]. Generative diffusion models [12, 38, 37] have emerged as a top class of image generators, excelling at many conditional generation tasks [16, 30], including NVS [52, 42, 35]. In diffusion modeling, we train a neural network for removing Gaussian noise, and iteratively apply it in several steps to generate an image. These models are relatively simple to train, owing their stability to a simple denoising regression loss. Modern diffusion architectures notably rely on transformer layers [44] with self- and cross- attention blocks. These attention mechanisms are highly effective for the conditional generation task. For instance, in the widely used text-to-image generators [30, 28], cross-attention is used to condition the denoising network’s features on the textual tokens. In our work, we effectively harness cross-attention in diffusion models for the NVS task. Some existing diffusion-based NVS works [7, 35] operate in the latent space of an auto-encoder, following [28]. This can result in an unnecessary loss of high-frequency details due to the auto-encoder’s reconstruction error, leading to texture mismatches between source and target views. Therefore, we opt to apply our method directly in the pixel space using a cascaded diffusion model design [13], avoiding this issue altogether. We demonstrate the superior texture transfer capabilities of our model compared to latent space NVS models. Furthermore, many recent generative model-based NVS works involve conditional diffusion training with some form of intricate 3D geometry encoding [52, 42, 35]. However, in many cases, the benefit of such complex encoding methods is unclear, with some researchers claiming they may not be needed at all [27]. Inspired by previous work, we explore geometry encoding methods and perform an ablation study. In our experiments, we show that that the impact of these methods is minimal, especially when harnessing a powerful diffusion model architecture. Building on our conclusions, we train an NVS diffusion model, reaching state-of-the-art (SOTA) NVS capabilities for the commonly accepted RealEstate10K dataset [57]. Our model, which we call VIVID (View Inference Via Image Diffusion), excels in both image quality and fidelity to the ground-truth novel view, measured by FID [10] and PSNR, respectively. Finally, we experiment with the generalization capabilities of our method, and attempt to overcome the limited availability of multi-view data. Using the insight that camera rotations can be accurately simulated with a simple 2D homography transform, we propose a novel data augmentation scheme that enables the use of single-view datasets in NVS training. Our proposed scheme unlocks the possibility of training NVS models on far richer image content distributions, without introducing data scale mismatches that commonly occur in multi-dataset NVS training [31]. We show the effectiveness of this scheme in generalizing to unseen scenes with out-of-distribution content relative to the multi-view training dataset. To summarize, we consider the task of novel view synthesis (NVS), characterize its different flavors, and focus on single-image to single-image NVS. We make use of a powerful diffusion model backbone [15], adapt it for NVS using the cross-attention layers, and apply it in the pixel space rather than a latent one. We ablate on different geometry encoding methods, and conclude that they offer minimal improvement over a simple scalar embedding of the camera poses. Our resulting model, called VIVID, achieves state-of-the-art NVS performance on the widely accepted RealEstate10K [57] benchmark. However, our method has some limitations that we discuss in section 5, which we hope to address in future work."
https://arxiv.org/html/2411.07758v1,AdaSemiCD: An Adaptive Semi-Supervised Change Detection Method Based on Pseudo-Label Evaluation,"Change Detection (CD) is an essential field in remote sensing, with a primary focus on identifying areas of change in bi-temporal image pairs captured at varying intervals of the same region by a satellite. The data annotation process for the CD task is both time-consuming and labor-intensive. To make better use of the scarce labeled data and abundant unlabeled data, we present an adaptive dynamic semi-supervised learning method, AdaSemiCD, to improve the use of pseudo-labels and optimize the training process. Initially, due to the extreme class imbalance inherent in CD, the model is more inclined to focus on the background class, and it is easy to confuse the boundary of the target object. Considering these two points, we develop a measurable evaluation metric for pseudo-labels that enhances the representation of information entropy by class rebalancing and amplification of confusing areas to give a larger weight to prospects change objects. Subsequently, to enhance the reliability of sample-wise pseudo-labels, we introduce the AdaFusion module, which is capable of dynamically identifying the most uncertain region and substituting it with more trustworthy content. Lastly, to ensure better training stability, we introduce the AdaEMA module, which updates the teacher model using only batches of trusted samples. Experimental results from LEVIR-CD, WHU-CD, and CDD datasets validate the efficacy and universality of our proposed adaptive training framework.","Change detection (CD) has emerged as a significant research focus within the field of remote sensing in recent years. Its objective is to identify regions of interest that have experienced alterations in bi-temporal image pairs captured at varying times of the same geographical area using satellite technology. This method plays a crucial role in remote sensing data analysis and is of particular importance in various civilian sectors such as urban planning [1, 2], rural land management [3, 4], and disaster assessment [5, 6]. Figure 1: Comparison between SOTA semi-supervised methods and our AdaSemiCD on the CDD Dataset. Given that the process of accurately annotating masks for change detection tasks is notably labor intensive, direct application of traditional supervised learning approaches, such as CNN [7, 3, 8, 9, 4] and Transformers [10, 11, 12], to a limited set of labeled data often results in limited performance. In response to these challenges, researchers have investigated a range of approaches such as Self-supervised learning(SSL)[13, 14],Unsupervised Change Detection (USCD) [15, 16, 17], Weakly Supervised Change Detection (WSCD) [18, 19, 20], and sample generation strategies [21, 22, 23, 24]. Although WSCD is cost-efficient, it is reliant on incomplete or inaccurate labels, which can introduce substantial errors and unpredictably noisy data. USCD, on the other hand, does not require labeled data and utilizes the intrinsic patterns present in the data, but it often faces challenges when tackling specific tasks like classification or detection. Sample generation strategies, which include data augmentation [23], generative adversarial networks (GANs) [22], and diffusion models [24], frequently necessitate the simulation or synthesis of additional data. However, when dealing with limited available samples, these methods may encounter constraints due to insufficient diversity in the generated data, which can diminish the model’s ability to generalize. As a result, semi-supervised change detection (SSCD) [25, 26, 27] emerges as a potentially more effective solution. The paradigm of semi-supervised learning [28, 29, 30] is to enhance CD performance by leveraging the limited available labels and the large volume of unlabeled samples. Usually, researchers generate pseudo-labels for the unlabeled data to act as guidance during training. These pseudo-labels are often temporary predictions with higher probabilities. The most prevalent approach is the Mean-Teacher [31] framework, which employ a teacher model to generate pseudo-labels that serve as guidance for the student model during the training process. The teacher model is subsequently updated using the exponential moving average (EMA) [32] of the student model. By training on a mix of limited data with actual labels and abundant data with pseudo-labels, the student model can learn more significant features, leading to noticeable enhancements in performance. While these methods produce acceptable outcomes, significant problems persist: the model indiscriminately treats all samples, irrespective of their quality, and the training process lacks flexibility. Firstly, it’s evident that unlabeled samples may not always function as efficient ‘teachers’. Models frequently encounter difficulties in generating reliable high-quality pseudo-labels for intricate samples, which in turn introduces extra noise that can misguide the model’s training. Subsequently, the EMA updating process does not take into account the quality of samples. Given that training batches can be biased or contain noise, dynamically determining the training update timing could contribute to the stability of the training process. These factors underscore the need for a more precise supervisory approach, failing which it could negatively impact the model’s training. In this study, we introduce an adaptive dynamic learning strategy, AdaSemiCD, designed to improve the accuracy of pseudo-labels and streamline the training process. Our framework incorporates the traditional semi-supervised training approach, augmented by two innovative functional modules, AdaFusion and AdaEMA. Initially, we utilize AdaFusion to suppress noise at the individual sample level, thereby enhancing the accuracy of pseudo-labels. Contrary to previous methods like Augseg [33] or CutMix [34] that relied on entirely random fusion regions, our AdaFusion technique proactively identifies the most uncertain region and substitutes it with reliable content from either labeled datasets or unlabeled datasets of superior quality. Following this, we dynamically adjust the rate of parameter updates in the teacher-student model via AdaEMA to ensure improved stability. Although the traditional EMA effectively mitigates fluctuations in model parameters, thereby boosting stability, it persists in uniformly updating after each training iteration, overlooking the model’s varying learning outcomes across different iterations when handling a range of training samples. If unlabeled samples contain an abundance of erroneous information, it can misdirect the model’s training. Therefore, our AdaEMA introduces an adaptive selection process for model-level parameter updates, enabling the model to fully integrate superior parameters. In summary, the main contributions of this paper are as follows. • We propose an adaptive SSCD framework named AdaSemiCD, which dynamically improves the pseudo-labels as well as adjusts the training procedure with pseudo label quality assessment. • We propose an AdaFusion strategy to enhance unreliable unlabeled samples. The fusion region and the trusted contents are selectively chosen with the uncertainty map. • We propose an AdaEMA parameter update strategy, which updates the teacher model with a batch-wise pseudo-labels improving assessment. • Experimental results on publicly available datasets LEVIR-CD, WHU-CD, and CDD demonstrate the effectiveness of our method."
https://arxiv.org/html/2411.07747v2,Constraint Learning for Parametric Point Cloud,"Parametric point clouds are sampled from CAD shapes, and have become increasingly prevalent in industrial manufacturing. However, most existing point cloud learning methods focus on the geometric features, such as developing efficient convolution operations, overlooking the important attribute of constraints inherent in CAD shapes, which limits these methods’ ability to comprehend CAD shapes fully. To address this issue, we analyzed the effect of constraints, and proposed its deep learning-friendly representation, after that, the Constraint Feature Learning Network (CstNet) was developed to extract and leverage constraints. Our CstNet includes two stages. Stage 1 extracts constraints from B-Rep data or point cloud. Stage 2 leverages coordinates and constraints to enhance the comprehension of CAD shapes. Additionally, we built up the Parametric 20,000 Multi-modal Dataset for the scarcity of labeled B-Rep datasets. Experiments demonstrate that our CstNet achieved state-of-the-art performance on both public and proposed CAD shape datasets. To the best of our knowledge, CstNet is the first constraint-based learning method tailored for CAD shape analysis.","Parametric point clouds are derived from parametric templates, which consist of primitives, constraints, and dimensional parameters. By specifying all parameters, the templates can be instantiated into CAD shapes, as illustrated in Fig. 1. Parametric point clouds provide detailed information that is essential for various engineering applications, such as model design and product machining. In recent years, deep learning methods have emerged as powerful tools for point cloud analysis, with models such as PointNet [32], PointNet++ [33], and PointCNN [24] demonstrating significant success across various tasks. These models are primarily designed and evaluated on graphic objects, such as animals, trees, and other free-form shapes, where shape comprehension relies on geometric features visible in contours. However, some CAD shapes, despite exhibiting minimal visual differences, serve distinct purposes due to variations in their functional regions. This similarity in appearance poses challenges in distinguishing them based on geometric features. Notably, variations in the functional regions often introduce differing constraints, as these regions need to fit closely with other parts to perform functions such as transmission and positioning. Thus, distinguishing them from a constraint-based perspective becomes more feasible, as illustrated in Fig. 1. Figure 1: Motivation. Some similar CAD shapes serve different functions. The eccentric wheel has a rotation axis that is offset from its outer cylindrical surface, facilitating precise positioning or tension adjustment. In contrast, the flywheel has a coaxial rotation axis with its outer cylindrical surface and is used to store rotational kinetic energy. Although these shapes are challenging to distinguish visually, they can be easily differentiated from the perspective of constraints. Additionally, the limited availability of B-Rep datasets poses barriers to deep learning on CAD shapes. Most existing CAD shape datasets consist of mesh files, such as MCB [16] and ESB [12]. While mesh files could approximate the appearance of CAD shapes, lack crucial boundary information. In contrast, B-Rep data [19, 56] serves as the native representation of CAD shapes and is therefore more suitable for dataset construction. However, labeled B-Rep datasets remain relatively scarce, for example, FabWave [1] includes only 2,133 B-Rep files. Although the ABC [18] contains a large number of B-Rep files, it remains unlabeled. To facilitate the constraint feature learning of CAD shapes, we conducted studies on deep learning methods and dataset development. Traditional constraint definitions are not well-suited for deep learning, as they vary across constraint types and involve multiple possible combinations to represent primitives’ relations. To address this challenge, we proposed a novel constraint representation as point-wise local attributes. Afterward, we developed the CstNet for constraint extraction and feature learning, which consist of two stages. Stage 1 is built for constraint acquisition, if B-Rep data is available, the openCASCADE Technology (OCCT) is used to extract constraints, otherwise, the constraint prediction from point cloud (CST-PCD) module is adopted. The CST-PCD is designed to rely on local features only, enabling it to generalize to unseen datasets after pre-training. Stage 2 leverages both point cloud and constraints for constraint feature learning, which employs both point-level and feature-level attention for effective feature extraction, enhancing the comprehension of CAD shapes. Finally, we built up a multi-modal classification dataset Parametric 20000, which contains B-Rep, Mesh, and point cloud. Given the current scarcity of labeled CAD shape datasets, Parametric 20000 offers researchers an expanded resource for their work. Our contributions are threefold: • We introduced the deep learning-friendly expression of constraints. • We designed the CstNet for constraint extraction and constraint feature learning. • We collected and curated the Parametric 20000 dataset, which consists of mesh, point cloud, and B-Rep data. Our CstNet has been validated across various tasks, on the mechanical classification dataset MCB [16], our model achieved an overall accuracy improvement of 2.98 % compared to the state-of-the-art, and for mechanical segmentation dataset 360 Gallery [19], that is 6.74 %."
https://arxiv.org/html/2411.07742v1,"Efficient 3D Perception on Multi-Sweep Point Cloud 
with Gumbel Spatial Pruning","This paper studies point cloud perception within outdoor environments. Existing methods face limitations in recognizing objects located at a distance or occluded, due to the sparse nature of outdoor point clouds. In this work, we observe a significant mitigation of this problem by accumulating multiple temporally consecutive LiDAR sweeps, resulting in a remarkable improvement in perception accuracy. However, the computation cost also increases, hindering previous approaches from utilizing a large number of LiDAR sweeps. To tackle this challenge, we find that a considerable portion of points in the accumulated point cloud is redundant, and discarding these points has minimal impact on perception accuracy. We introduce a simple yet effective Gumbel Spatial Pruning (GSP) layer that dynamically prunes points based on a learned end-to-end sampling. The GSP layer is decoupled from other network components and thus can be seamlessly integrated into existing point cloud network architectures. Without incurring additional computational overhead, we increase the number of LiDAR sweeps from 10, a common practice, to as many as 40. Consequently, there is a significant enhancement in perception performance. For instance, in nuScenes 3D object detection and BEV map segmentation tasks, our pruning strategy improves several 3D perception baseline methods.","I INTRODUCTION Figure 1: Sparse or missing outdoor point cloud data poses significant challenges for perception tasks, particularly at a distance (a1) or when objects are occluded (b1). Accumulating multiple temporally consecutive LiDAR sweeps enhances the density of the point cloud (a2) (b2), thereby facilitating perception. However, this improvement comes at the expense of increased computational cost. This work explores cost-effective strategies for leveraging multiple LiDAR sweeps. The past several years have witnessed exponential growth in 3D perception techniques [1, 2, 3], including considerable point cloud-based methods [4, 5], primarily attributed to the rapid advancement and significant cost reduction of LiDAR sensors. In outdoor applications, such as autonomous driving, the utilization of LiDAR point cloud is essential for precise scene geometry measurement and dynamic object localization [6]. Typically, a LiDAR sensor is positioned atop the ego-car and performs a comprehensive [7] or partial [8] panoramic scan of the surrounding environment. However, this operational mechanism presents two inherent limitations. Firstly, the density of captured points exhibits a gradient, with higher density near the ego-car and sparser distribution at greater distances, thereby posing challenges for distant perception (Fig. 1 a1). Secondly, occluded objects remain undetected by the LiDAR sensor due to their limited or absent beam reflections (Fig. 1 b1). Consequently, these limitations significantly hinder the accuracy and reliability of LiDAR-based perception. To overcome these challenges, previous endeavors have introduced vision cues and pursued the fusion of data from multiple sensor modalities [9, 10, 11]. Nevertheless, even when considering point cloud data as the sole input, there exist approaches to mitigate the aforementioned issues. Autonomous vehicles are typically equipped with Inertial Measurement Units (IMUs) that can measure the pose of the ego-car. Leveraging this information, we can align and accumulate multiple temporally adjacent LiDAR sweeps into the current frame (Figure 1 a2 & b2). As a result, the sparse regions of the point cloud become denser, thereby facilitating the subsequent perception step. Despite the lack of a comprehensive investigation, this technique has been widely adopted as a common practice in point cloud-based tasks, such as object detection [12, 9, 13, 14] and semantic segmentation [15, 16, 17, 18] due to its consistent advantages. The flip side of the coin is the additional time consumption and computation cost that grow linearly with the rising number of sweeps. To strike a balance between network efficiency and performance gains, a typical trade-off is made by setting the number of sweeps to 10 in the nuScenes dataset [7, 19, 20, 21, 22, 23] and 4 in the Waymo Open datase [8, 24, 25, 26, 27, 28]. Notably, our observations reveal that augmenting the number of sweeps beyond the common practice further enhances perception performance. However, the increasing computation cost hinders the exploration of this possibility. In this work, our objective is to enhance point cloud perception by leveraging a greater number of temporal sweeps while minimizing any additional computational overhead. To achieve this, we make use of an observation that the accumulated point cloud data exhibits high redundancy ( Fig. 1). For instance, the road surface in close proximity to the ego-car often contains a significant number of densely packed points. Intuitively, if we can identify and selectively discard such regions, computational costs can be dramatically reduced without significantly impacting the performance of the tasks at hand. In light of the above consideration, we propose a simple yet efficient Gumbel Spatial Pruning (GSP) Layer to fulfill the dynamic pruning of LiDAR points. The GSP layer is a plug-and-play component that can be readily inserted after each computation layer. It learns a binary classifier through Gumbel Softmax to decide which points should be pruned. Our key finding is that it is crucial to use hard Gumbel Softmax which outputs discrete binary values rather than continuous logits. This way, we can use the discrete prediction for indexing and avoid thresholding the probability map during inference, keeping the data distribution consistent with training, and thus minimizing the performance loss brought by pruning. GSP layer uses neither additional supervision like pre-computed pruning label, nor prior cues such as feature magnitude [29]. The pruning is learned solely from the supervision of the task loss and a sparse regularization, in an end-to-end manner. Extensive experiments show that, by introducing the GSP layer, the sweep number can be enlarged to 4\times without incurring additional computational overhead, either in terms of FLOPS or latency, and perception accuracy remarkably improves. In the nuScenes dataset, we improve several 3D perception networks, including the vanilla TransL [13] and LargeKernel3D [14], on both 3D object detection tasks and BEV map segmentation tasks. Compared with existing spatial pruning methods [29], we achieve a better trade-off between task performance and network efficiency. To summarize, our contribution is three-fold. • We present that more temporally accumulated sweeps beyond the common practice still bring considerable gains in perception accuracy. • We propose a novel Gumbel Spatial Pruning Layer, which reduces the computational cost by 4\times with a neglectable performance drop. • Equipped with the GSP layer, we train 3D detection and segmentation models with 4\times sweeps with no additional computation cost. The accuracy for each task significantly improves, even on the basis of strong state-of-the-art models."
https://arxiv.org/html/2411.07740v1,3D Focusing-and-Matching Network for Multi-Instance Point Cloud Registration,"Multi-instance point cloud registration aims to estimate the pose of all instances of a model point cloud in the whole scene. Existing methods all adopt the strategy of first obtaining the global correspondence and then clustering to obtain the pose of each instance. However, due to the cluttered and occluded objects in the scene, it is difficult to obtain an accurate correspondence between the model point cloud and all instances in the scene. To this end, we propose a simple yet powerful 3D focusing-and-matching network for multi-instance point cloud registration by learning the multiple pair-wise point cloud registration. Specifically, we first present a 3D multi-object focusing module to locate the center of each object and generate object proposals. By using self-attention and cross-attention to associate the model point cloud with structurally similar objects, we can locate potential matching instances by regressing object centers. Then, we propose a 3D dual-masking instance matching module to estimate the pose between the model point cloud and each object proposal. It performs instance mask and overlap mask masks to accurately predict the pair-wise correspondence. Extensive experiments on two public benchmarks, Scan2CAD and ROBI, show that our method achieves a new state-of-the-art performance on the multi-instance point cloud registration task. Code is available at https://github.com/zlynpu/3DFMNet.","Point cloud registration, a fundamental process in computer vision, involves aligning two point clouds through estimating a rigid transformation. In practical applications like robotic bin picking, multi-instance registration emerges as a critical need, demanding the alignment of a model’s point cloud with multiple instances within the scene. This task presents heightened complexity compared to single-point cloud registration, primarily due to challenges such as the uncertain number of instances and inter-instance occlusions. These complexities are particularly pronounced in cluttered environments, where precise alignment becomes pivotal for effective robotic operations. Therefore, how to improve the accuracy of multi-instance point cloud registration is still a challenging issue. Figure 1: Comparison between our method and existing methods in multi-instance point cloud registration. Our method decomposes the multi-instance point cloud registration into multiple pair-wise point cloud registration. There are a few efforts for tackling multi-instance point cloud registration. Existing pipelines can be roughly divided into two types: two-stage and one-stage. For the two-stage process, we first extract point correspondences between the model point cloud and scene point clouds, and then recover per-instance transformations through multi-model fitting kluger2020consac ; tang2022multi ; yuan2022pointclm . Although two-stage methods are simple and feasible, the success of these methods largely depends on the quality of the correspondence. Furthermore, due to cluttered and occluded objects, it is still difficult to accurately cluster the correspondences into individual instances for subsequent pair-wise registration. For the one-stage process, it takes the model point cloud and scene point cloud as inputs, and directly outputs pose. As a representative one-stage work, Yu et al.yu2024learning proposed a coarse-to-fine framework, which learns to extract instance-aware correspondences for estimating transformations without multi-model fitting. Due to the consideration of instance-level information in correspondence, it can obtain fine-grained features, thereby boosting the performance. However, for the scene with multiple objects, obtaining accurate instance-level correspondence is very difficult, especially for the cluttered and occluded objects. Therefore, to alleviate the difficulty of learning correspondence between the model point cloud and multiple objects in the scene, as shown in Figure 1, we consider first focusing on the object centers, and then learning the matching between the object proposal and the model point cloud. In this paper, we propose a simple yet powerful 3D focusing-and-matching network for multi-instance point cloud registration. The core idea of our method is to decompose the multi-instance point cloud registration into multiple pair-wise point cloud registrations. Specifically, we propose a 3D multi-object focusing module to localize the potential object centers and generate object proposals. To associate the object with the input CAD model, we use self-attention and cross-attention to learn the structurally similar features, thereby improving the accuracy of prediction for object centers. Based on the learned object center, we incorporate the radius of the CAD model to generate object proposals through ball query operation. After that, we propose a 3D dual-masking instance matching module to learn accurate pair-wise registration between the CAD model and object proposal. It adopts an instance mask to filter the background points in the object proposal and uses an overlap mask to improve the pair-wise partial registration of incomplete objects. In summary, our contributions lie in three aspects: 1. Our primary contribution does not lie in the network architecture but rather in proposing a new pipeline to address the multi-instance point cloud registration problem. Existing methods (such as PointCLM yuan2022pointclm and MIRETR yu2024learning ) mainly learn correspondence between the one CAD model and multiple objects (one-to-many paradigm), while our method decompose the one-to-many paradigm into multiple pair-wise point cloud registration (multiple one-to-one paradigm) by first detecting the object centers and then learning the matching between the CAD model and each object proposal. 2. Our new pipeline is simple yet powerful, achieving the new state-of-the-art on both Scan2CAD avetisyan2019scan2cad and ROBI yang2021robi datasets. Especially on the challenging ROBI dataset, our method significantly outperforms the previous SOTA MIRETR by about 7% in terms of MR, MP, and MF. 3. The progressive decomposition approach of transforming multi-instance point cloud registration into multiple pair-wise registrations, as proposed in our paper, also holds significant insights for other tasks, such as multi-target tracking and map construction."
https://arxiv.org/html/2411.07728v1,No-Reference Point Cloud Quality Assessment via Graph Convolutional Network,"Three-dimensional (3D) point cloud, as an emerging visual media format, is increasingly favored by consumers as it can provide more realistic visual information than two-dimensional (2D) data. Similar to 2D plane images and videos, point clouds inevitably suffer from quality degradation and information loss through multimedia communication systems. Therefore, automatic point cloud quality assessment (PCQA) is of critical importance. In this work, we propose a novel no-reference PCQA method by using a graph convolutional network (GCN) to characterize the mutual dependencies of multi-view 2D projected image contents. The proposed GCN-based PCQA (GC-PCQA) method contains three modules, i.e., multi-view projection, graph construction, and GCN-based quality prediction. First, multi-view projection is performed on the test point cloud to obtain a set of horizontally and vertically projected images. Then, a perception-consistent graph is constructed based on the spatial relations among different projected images. Finally, reasoning on the constructed graph is performed by GCN to characterize the mutual dependencies and interactions between different projected images, and aggregate feature information of multi-view projected images for final quality prediction. Experimental results on two publicly available benchmark databases show that our proposed GC-PCQA can achieve superior performance than state-of-the-art quality assessment metrics. The code will be available at: https://github.com/chenwuwq/GC-PCQA.","In recent years, the development of three-dimensional (3D) visual information acquisition technology makes point clouds easier to obtain and gradually becomes a popular type of visual data. A 3D Point cloud is mainly used to describe a complete 3D scene or object, including geometric attributes (position of each point in 3D space), color attributes (RGB attributes of each point), and others (normal vector, opacity, reflectivity, time, etc.)[1]. Point clouds have been widely studied and used in a wide range of application scenarios such as 3D reconstruction[2, 3], classification and segmentation[4, 5], facial expression representation[6], autonomous driving[7, 8], and virtual reality[9], etc. Although point cloud can realistically record 3D objects through a large set of points, it also consumes a lot of memory, and it is difficult to achieve data transmission under limited network bandwidth[10, 11]. This new and effective data representation presents a challenge to the current hardware storage and network transmission. Therefore, in order to achieve efficient storage and transmission, compression of point clouds is necessary[12, 13, 14, 15]. However, point cloud compression may introduce artifacts, resulting in the degradation of point cloud visual quality. Point cloud visual quality is an important way to compare the performance of various point cloud processing algorithms. Effective point cloud quality assessment (PCQA) methods can not only help people evaluate the distortion degree of point clouds and the performance of compression algorithms but also be beneficial to optimize the visual quality of distorted point clouds. Thus, how to accurately assess the perceptual quality of point clouds has become a critical issue. Similar to image quality assessment (IQA), PCQA can also be divided into subjective and objective methods. The subjective method is mainly based on the perception of the human visual system (HVS). It is difficult to be widely applied because this kind of assessment requires a large number of participants to ensure the rationality and accuracy of the assessment results in a statistical sense. Currently, the results obtained from subjective assessment experiments are generally served as the ground-truth data for benchmarking different objective methods [16]. According to the participation of original point clouds, objective PCQA methods can have three categories: full reference (FR), reduced reference (RR), and no reference (NR). Since the original point clouds are not always available, NR-PCQA methods that do not rely on any original information as a reference are more suitable in practical applications. The traditional NR-PCQA methods [17, 18] generally predict the quality score by extracting quality-aware features based on the analysis of point cloud attributes such as geometry and color. Recently, the great success of deep learning in the field of NR-IQA has promoted the development of deep learning-based NR-PCQA metrics [19, 20, 21, 22, 23, 24]. The common practice of these deep NR-PCQA metrics is to directly apply the ordinary convolution operation on the point cloud for automatic feature learning in a data-driven manner. Nonetheless, point cloud is a typical kind of non-Euclidean data which is sparsely distributed over the 3D space, and a large number of useless pixels are also involved with pixel-by-pixel convolution, thus resulting in a huge waste of resources and inefficient data processing. In order to solve this problem, some related works try to represent non-Euclidean data with the graph which includes node information and complex adjacency relations between nodes. With the graph-based non-Euclidean data as input, the current works then introduce to use graph convolutional network (GCN) rather than traditional convolutional neural network (CNN) for more effective feature representation learning [25]. For instance, Thomas et al.[26] proposed to convert non-Euclidean data into a graph based on which the GCN is used to realize graph feature extraction. As a typical kind of non-Euclidean data, GCN has also been applied to many point cloud-based vision tasks, such as point cloud classification[27, 28], point cloud segmentation[29], point cloud data analysis[30], action recognition[31], etc. Moreover, it has also been applied to infer the perceptual quality of various multimedia data, e.g., traditional 2D images [32, 33], 360-degree images [34, 35], and meshes [36, 37]. Figure 1: We simulate the perceptual process of HVS to perform multi-view projection on the 3D point cloud and build the graph based on the projected images. The red node in graph convolution represents the central node of the current convolution process, and the red line represents the adjacency relationship. The central node will constantly exchange information with neighboring nodes to aggregate feature information from neighbors. Due to the strong capability of GCN in handling non-Euclidean data including 3D point cloud, this paper presents a novel GCN-based NR-PCQA method (GC-PCQA). One of the most critical issues is to effectively create a graph of the point cloud so that the GCN can be applied for feature learning. Since the goal of PCQA is to predict the quality of the test point cloud consistent with human perception, how to construct a highly perception-consistent graph of point clouds is the key to its success. It is known that the HVS reconstructs 3D objects in their mind based on multiple two-dimensional (2D) plane images observed from different viewpoints. In order to imitate the process of the HVS to perceive 3D objects, it is natural to perform multi-view projection on the point cloud to obtain a set of projected images with each corresponding to a specific viewpoint. Although these projected images are independent individuals, there is a certain extent of correlation between each other. Therefore, we regard all projected images as a set of non-Euclidean data and then establish a graph according to the dependencies between each individual projected image. Finally, GCN is applied to realize feature extraction from the constructed graph for quality prediction. The entire process is simply illustrated in Fig. 1. Experimental results demonstrate that our proposed GC-PCQA method outperforms state-of-the-art reference and non-reference PCQA methods on two public PCQA databases. Overall, the main contributions of this paper are as follows: 1. We perform multi-view projection on the point cloud to obtain a set of projected images based on which a highly perception-consistent graph is constructed to model the mutual dependencies of multi-view projected images. The graph nodes are defined with the projected images and connected by spatial relations between each other. 2. We perform GCN on the proposed graph to characterize the interactions between different projected images and aggregate the feature information of multi-view projected images for final quality prediction. The ablation study validates the effectiveness of the GCN architecture and the source code is available for public research usage. 3. We fuse the horizontally and vertically projected image features extracted by two GCNs that do not share weights to boost the performance. Experimental results show that the proposed GC-PCQA can predict subjective scores more accurately than the existing state-of-the-art PCQA metrics. The rest of this paper is organized as follows. In Section II, we introduce the related works. In Section III, we illustrate the proposed GC-PCQA with technical details. We conduct experiments and analyze the results in Section IV, and finally draw conclusions in Section V."
https://arxiv.org/html/2411.07725v1,"ALOcc: Adaptive Lifting-based 3D Semantic Occupancy and 
Cost Volume-based Flow Prediction","Vision-based semantic occupancy and flow prediction plays a crucial role in providing spatiotemporal cues for real-world tasks, such as autonomous driving. Existing methods prioritize higher accuracy to cater to the demands of these tasks. In this work, we strive to improve performance by introducing a series of targeted improvements for 3D semantic occupancy prediction and flow estimation. First, we introduce an occlusion-aware adaptive lifting mechanism with a depth denoising technique to improve the robustness of 2D-to-3D feature transformation and reduce the reliance on depth priors. Second, we strengthen the semantic consistency between 3D features and their original 2D modalities by utilizing shared semantic prototypes to jointly constrain both 2D and 3D features. This is complemented by confidence- and category-based sampling strategies to tackle long-tail challenges in 3D space. To alleviate the feature encoding burden in the joint prediction of semantics and flow, we propose a BEV cost volume-based prediction method that links flow and semantic features through a cost volume and employs a classification-regression supervision scheme to address the varying flow scales in dynamic scenes. Our purely convolutional architecture framework, named ALOcc, achieves an optimal tradeoff between speed and accuracy achieving state-of-the-art results on multiple benchmarks. On Occ3D and training without the camera visible mask, our ALOcc achieves an absolute gain of 2.5% in terms of RayIoU while operating at a comparable speed compared to the state-of-the-art, using the same input size (256\times704) and ResNet-50 backbone. Our method also achieves 2nd place in the CVPR24 Occupancy and Flow Prediction Competition.","Figure 1: Semantic occupancy and flow prediction results of our ALOcc. ALOcc outperforms previous SOTAs across various benchmarks and metrics, demonstrating strong performance. The input image size and backbone are standardized to 256\times 704 and ResNet50, respectively. Accurate prediction of 3D semantic occupancy is vital for the effectiveness of perception systems, with widespread applications in autonomous driving, robotics, and virtual reality [45, 39, 11, 41, 38, 31]. Modeling both static and dynamic objects as 3D semantic grids offers multiple advantages. It provides a regularized representation of scene semantics, a finer granularity, and a more generalized form than traditional bounding boxes [19, 17, 10]. In recent years, the prediction of semantic occupancy and flow grid maps solely from surround-view camera sensor data has received notable attention [39, 11, 38]. Vision-based 3D semantic occupancy prediction strives to infer regularized 3D scene semantics from 2D inputs. The task introduces an additional 2D-to-3D reconstruction, which is inherently an ill-posed problem. The challenges are two-fold. Firstly, the challenge is to accurately propagate 2D semantic signals to their correct 3D positions while predicting semantics accurately. Secondly, conducting computations on 3D data consumes substantial computational resources, which is particularly challenging for real-time tasks, such as autonomous driving. We are highly focused on the 2D-to-3D view transformation and the decoding of occupancy and flow. We examine two existing 2D-to-3D view transformation methods: depth-based lift-splat-shoot (LSS) [36, 17, 10] and 2D-3D cross attention [19, 21]. The former actively transmits 2D information to 3D by explicitly predicting 2.5D structural information (i.e., depth), while the latter passively and adaptively transfers 2D information to 3D through cross attention with 3D queries. However, the depth-based LSS approach has several inherent problems, such as the strong depth inductive bias in the network. This will cause the network to get stuck in local optima due to early inaccurate depth estimations. it restricts signal propagation to occluded areas. Additionally, ray-based mapping in LSS tends to produce features with low spatial densities at far distances [21], impeding 3D feature learning for distant objects. Conversely, cross-attention does not evolve explicit structural information. To address these challenges, we design an occlusion-aware adaptive lifting method that constructs transfer probabilities from surface to occluded and sparse locations based on explicit depth probabilities. This is inspired by human perception, where we can infer the shape and extent of occluded objects from partial views. In the occupancy decoder, we introduce a semantic enhancing method that correlates 3D and 2D features using a shared semantic prototype set. We predefine per-class semantic prototypes to aggregate class semantics from both 3D and 2D features. It helps transfer inter-class semantic relationships from original 2D signals to 3D and improves 3D semantic representation learning. We then calculate per-class segmentation masks using these prototypes. To address the long-tail problem in scenes, each prototype is trained only when the corresponding class is present in the scene. Additionally, we introduce a sampling technique based on uncertainty and class priors. We sample hard voxels based on class statistics and per-voxel prediction uncertainty. The training is only conducted on the sampled voxels. Furthermore, we propose a new BEV-based cost volume method for occupancy flow prediction. A key challenge in this task is that jointly predicting semantics and flow imposes two types of representational pressure on the features. To alleviate this pressure, we aggregate voxel features from multiple height levels above the ground into a BEV representation, creating a cost volume referenced to historical BEV features. Utilizing both the cost volume and voxel features for flow estimation allows voxel features to prioritize occupancy semantic information, thus mitigating multi-task prediction constraints. Additionally, we propose a hybrid classification-regression technique for flow prediction, enhancing the model’s adaptability to varied flow scales. As shown in Fig. 1, our method consistently outperforms existing approaches, establishing new state-of-the-art performance on multiple semantic occupancy and flow prediction benchmarks. Moreover, we propose ALOcc with multiple model variants through techniques like spatial compression. Our approach achieves superior performance while maintaining competitive computational efficiency. This establishes a favorable balance between model performance and computational cost, which is particularly beneficial for real-time applications such as autonomous driving. In summary, our main contributions are: • We introduce a 2D-to-3D adaptive lifting method that transforms 2D signals to occluded and sparse areas via adaptive weight adjustment while incorporating depth denoising to prevent convergence to local optima. • We are the first to present a BEV-based cost volume method for occupancy flow prediction, alleviating feature coding burden in multi-task settings and enhancing flow prediction with combined classification and regression. • We proposed shared semantic prototypes to transfer inter-class relationships from 2D to 3D, mitigating the class imbalance problem through selective prototype training and uncertainty-aware sampling. • Comprehensive evaluations across multiple semantic occupancy and flow prediction benchmarks demonstrate consistent improvements over current SOTAs. We provide multiple model variants, with our real-time version outperforming existing real-time approaches. Figure 2: Overall Framework of the proposed ALOcc. The image features are converted to 3D space by adaptive lifting and then encoded by a 3D encoder along with the history frames. The resulting volume features are then decoded by different task heads."
https://arxiv.org/html/2411.07708v1,Emotion Classification of Children Expressions,"This paper proposes a process for a classification model for the facial expressions. The proposed process would aid in specific categorisation of children’s emotions from 2 emotions namely ’Happy’ and ’Sad’. Since the existing emotion recognition systems algorithms primarily train on adult faces, the model developed is achieved by using advanced concepts of models with Squeeze-andExcitation blocks, Convolutional Block Attention modules, and robust data augmentation. Stable Diffusion image synthesis was used for expanding and diversifying the data set generating realistic and various training samples. The model designed using Batch Normalisation, Dropout, and SE Attention mechanisms for the classification of children’s emotions achieved an accuracy rate of 89% due to these methods improving the precision of emotion recognition in children. The relative importance of this issue is raised in this study with an emphasis on the call for a more specific model in emotion detection systems for the young generation with specific direction on how the young people can be assisted to manage emotions while online.","Children in the current world for instance have several opportunities to access any video content online irrespective of its nature being appropriate or otherwise [1]. Features that in themselves may not be restricted in any way due to other conventional blockage measures such as the keyword blocking or age-blocking will occupy the young people emotionally and psychologically. More recently, there have emerged technologies such as facial recognition systems that directly explore children and their ability to control content heard or watched in real time. These systems use facial recognition to detect signs of distress, discomfort or fear and prevent the harm by either stopping the video or alerting the caregivers. One of the measures that will enable the achievement is the formulation of the model that will propose a proper classification of children’s facial expressions since these signs differ from adult’s emotional expression both in terms of look and perception. Currently, most of the facial recognition systems are developed based on adult faces, therefore high percentage of wrong results are expected for children’s faces. Therefore, there is a need to develop a specialized model focused on identification and categorization of children’s emotions. In this paper, some important facial feature differences between children and adults are described and child facial expression recognition model is presented. It offers the review of the literature regarding current models that were derived from adult based and child focused approaches, along with their strengths and weaknesses. The presented methodology describes the design of a new CNN to distinguish between the two considered expressions “HAPPY” and “SAD” with high efficiency. Eight experiments are performed to analyse the model’s performance by varying the regularisation and the type of attention. Evaluation methods involves key indicators like accuracy, training and validation loss, confusion matrices, as well models’ interpretability by Grad-CAM visualizations. 1.1 Need for children’s expression focused model The facial expression patterns significantly differ with different age groups. Howard et al [2] state that there are clear expression differences between young people and older people. These variations create some difficulties for current emotion recognition models, where many of them are trained on datasets that contain a vast number of adult faces. This age-related gap contributes a lot in lowering the model’s accuracy. In support of this, Sayin and Aksoy [3] point out a significant gap in research on the classification of facial expressions in younger children and older adults, It might be challenging to use traditional emotion identification techniques meant for adults on children because of their very diverse emotional manifestations resulting from developmental changes, mental progress, and individual character development. Among some studies, Guodong Guo et al [4] focused on the effects of aging on FER, indicating the drastic difference between children and adults. The researchers also mentioned that children mainly communicate by making rigid, clear movements of their face compared to adult people who demonstrate subtle, tonned movements of muscles because of the elasticity of the skin and presence of wrinkles. Likewise, Houstis and Kiliaridis [5] pointed out that adult especially male show greater amount of vertical movement in facial expressions, especially when compared with children who have immature muscle hence resulting to differences in feature in expressions between different age groups. In support of this argument, Guo brings a cultural perspective into the equation and argues that the variations must be taken into consideration to enhance FER systems’ effectiveness throughout the developmental period. Rao et al [6] compared the expression detection in both children and adults, and found that the children’s expression was always weaker and not very distinguishable as compared to adults, which reflected the lower accuracy in the recognition models irrespective of whether the less number of facial landmarks were used. In addition to this, another influence to emotional display is cultural and ethnic practices. For example, Lewis et al [7] found that Japanese children report lower levels of expression for shame, pride and sadness but higher levels for embarrassment than American children. Camras et al [8] also noted that Chinese babies are less expressive than European American and Japanese babies while smiling and crying. These studies demonstrate that cultural aspects are important in emotional display, while further complicating the models of emotion recognition."
https://arxiv.org/html/2411.07688v1,Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG,"Ultra High Resolution (UHR) remote sensing imagery (RSI) (e.g. 100,000 \times 100,000 pixels or more) poses a significant challenge for current Remote Sensing Multimodal Large Language Models (RSMLLMs). If choose to resize the UHR image to standard input image size, the extensive spatial and contextual information that UHR images contain will be neglected. Otherwise, the original size of these images often exceeds the token limits of standard RSMLLMs, making it difficult to process the entire image and capture long-range dependencies to answer the query based on the abundant visual context. In this paper, we introduce ImageRAG for RS, a training-free framework to address the complexities of analyzing UHR remote sensing imagery. By transforming UHR remote sensing image analysis task to image’s long context selection task, we design an innovative image contextual retrieval mechanism based on the Retrieval-Augmented Generation (RAG) technique, denoted as ImageRAG. ImageRAG’s core innovation lies in its ability to selectively retrieve and focus on the most relevant portions of the UHR image as visual contexts that pertain to a given query. Fast path and slow path are proposed in this framework to handle this task efficiently and effectively. ImageRAG allows RSMLLMs to manage extensive context and spatial information from UHR RSI, ensuring the analysis is both accurate and efficient.","In the field of remote sensing (RS), ultra-high-resolution (UHR) images often cover vast areas, encompassing diverse landscapes and a wide range of geospatial features. For deep learning applications such as semantic segmentation, object detection, and change detection, processing these large-scale images directly poses significant challenges. The high spatial resolution results in massive image sizes (e.g. 100,000\times 100,000 pixels), making it difficult to directly train neural networks with such images due to the limitation in GPU memory. Additionally, the variability in scale, class distribution, and object sizes within these large images can lead to suboptimal performance if not handled properly. To address these, a common preprocessing step is to cut the original UHR images into smaller patches (e.g. 224\times 224 or 512\times 512) [1] [2] that can fit in regular deep learning workflows Multimodal Large Language Models (MLLMs, in this paper we specifically refer to Vision-Language Models using LLM as base model) have demonstrated remarkable potential in RS tasks, including image captioning [3], visual grounding [4], relation reasoning [5], object detection [6], and visual question answering (VQA) [7]. However, the input image resolutions for these Remote Sensing Multimodal Large Language Models (RSMLLMs) are often limited and relatively small compared with the original satellite image. For example, models like LLaVA 1.5 [8] and H2RSVLM [7] utilize image inputs of 336\times 336 pixels, while Geochat [3] and SkysenseGPT [5] process images at 504\times 504 pixels. Vision-language models (VLMs) specifically trained for RS, such as GeoRSCLIP [9] and RemoteCLIP [10], work with even smaller inputs, typically at 224\times 224 pixels. Figure 1: An example of a challenging question that requires analyzing small targets in a high-resolution image. Models such as GeoChat, LLaVA1.5, and V^{*} failed to answer. LLaVA1.5 with ImageRAG can answer correctly. In Figure 1, we illustrate how current MLLMs struggle to answer a challenging question that requires identifying small objects in a high-resolution (2086\times 2086) image. The model’s limitations in handling fine details and distinguishing small features become evident, leading to inaccurate responses when tasked with analyzing such intricate visual information (the model can answer correctly when the zoom-in image is provided). We identify four types of approaches for applying MLLMs to UHR RSI, each with its own set of limitations. The first approach involves resizing UHR images to a smaller size in order to be compatible with current MLLMs. However, this significantly reduces the visibility of small objects in the images, making them challenging to detect, even for humans. For instance, H2RSVLM [7] claims its difficulty in handling small objects, likely due to limitations in input image resolution of 336\times 336. The second approach divides UHR images into smaller patches that can be sequentially processed by MLLMs. While this allows for compatibility with existing model architectures, it results in the loss of global and relative information and relationships present in the original large-scale image, as only portions of the image are considered at a time. The third approach references techniques from general LLMs for managing long context, such as Positional Interpolation [11] and LongROPE [12]. Or adopting architecture from video MLLMs like LongVILA [13], which can extend the context window effectively. This approach could potentially enable the integration of entire UHR images while maintaining global information. However, it would necessitate retraining the models from scratch. The fourth approach employs guided visual search methods that focus on relevant patches, such as V^{*} [14], or hybrid architectures like LongLLaVA [15], which enable the processing of very large input images, and not neglect the small targets. Similar to the drawbacks of the third approach, this method also requires retraining the model and demands task-specific annotations, adding to the complexity and effort needed for implementation. Three crucial aspects for MLLMs to effectively handle UHR RSI are: (1) managing small targets, ensuring that the model can accurately aware and analyze fine details within images; (2) processing the UHR image in a way that integrates with MLLMs without significantly increasing the number of image tokens, which would lead to high computational costs; and (3) achieving these goals while minimizing the need for additional training or specialized annotation. To address these problems, we contribute the ImageRAG framework for RSMLLMs, which offers several key advantages. First, it retrieves and emphasizes relevant visual context from the UHR image based on the text query, allowing the MLLM to focus on important details, even tiny ones. Second, it integrates various external knowledge sources to guide the model, enhancing the understanding of the query and the UHR RSI. Lastly, ImageRAG is training-free, and requires no additional annotations, making it a practical solution for efficiently handling UHR RSI. ImageRAG is initially designed for RS, but it is suitable for general tasks and other specific domains, as long as building modules with corresponding data."
https://arxiv.org/html/2411.07685v1,Fast Disentangled Slim Tensor Learning for Multi-view Clustering,"Tensor-based multi-view clustering has recently received significant attention due to its exceptional ability to explore cross-view high-order correlations. However, most existing methods still encounter some limitations. (1) Most of them explore the correlations among different affinity matrices, making them unscalable to large-scale data. (2) Although some methods address it by introducing bipartite graphs, they may result in sub-optimal solutions caused by an unstable anchor selection process. (3) They generally ignore the negative impact of latent semantic-unrelated information in each view. To tackle these issues, we propose a new approach termed fast Disentangled Slim Tensor Learning (DSTL) for multi-view clustering . Instead of focusing on the multi-view graph structures, DSTL directly explores the high-order correlations among multi-view latent semantic representations based on matrix factorization. To alleviate the negative influence of feature redundancy, inspired by robust PCA, DSTL disentangles the latent low-dimensional representation into a semantic-unrelated part and a semantic-related part for each view. Subsequently, two slim tensors are constructed with tensor-based regularization. To further enhance the quality of feature disentanglement, the semantic-related representations are aligned across views through a consensus alignment indicator. Our proposed model is computationally efficient and can be solved effectively. Extensive experiments demonstrate the superiority and efficiency of DSTL over state-of-the-art approaches. The code of DSTL is available at https://github.com/dengxu-nju/DSTL.","In many real-world applications, data can originate from different sources and feature collectors. For instance, we can utilize text posts, images, videos, user profiles, and social network connections to depict user behavior and interactions on social media platforms. To analyze sensor signals, we can decompose them into time and frequency domains. These are known as multi-view data, which often detail an object from various perspectives and provides a richer and more comprehensive understanding compared to single-view data. In recent years, the surge in multi-view data has sparked considerable interest in multi-view learning (MVL). MVL seeks to leverage the potential consistent and complementary information across multiple views to enhance generalization performance in downstream machine learning activities such as classification and clustering [1, 2, 3, 4, 5, 6, 7, 8]. Single-View Clustering (SVC) refers to the clustering of data comprising of a single view, while Multi-View Clustering (MVC) is to partition the multi-view data, leading to superior results than those obtained with SVC [9, 10, 11]. Most existing methods for MVC have demonstrated success, such as those employing matrix factorization [12, 13, 14], graph learning [15, 16, 17], and subspace learning [18, 19, 20]. Multi-view matrix factorization (MultiMF) effectively reduces the dimensionality of high-dimensional data and captures diverse underlying representations of multiple views. For example, Ma et al. [13] integrated multi-view linear discriminant analysis with MultiMF to leverage the intrinsic low-dimensional structure within the projection subspace. Liu et al. [14] unified MultiMF with partition generation to improve the clustering performance and efficiency for large-scale datasets. Graph-based methods strive to learn a unified graph from multiple views to delineate the pairwise similarities among data points. Huang et al. [16] proposed to formulate both the multi-view consistent graph and diverse graph in a unified framework. In [17], Wang et al. performed both anchor learning and graph construction to acquire an anchor graph to promote clustering efficiency. Another category is subspace-based MVC methods, which aim to infer latent representations from different views in a shared subspace. One representative example is the work of LMSC [18]. It integrated multiple views into a comprehensive latent representation subspace that encodes complementary information across different views. Unlike LMSC, Sun et al. [19] combined anchor learning and graph construction into a unified subspace to get a more discriminative clustering structure. Recently, several deep MVC approaches [21, 22, 23, 24] have also emerged to bolster clustering performance by harnessing the robust feature learning capabilities of deep neural networks. Although these MVC methods have achieved good results, they fail to fully explore the high-order correlations between each view. To solve this problem, tensor-based MVC methods have developed recently [25, 26, 27, 28, 29, 30], which usually stack the similarity graphs from all views into a three-order tensor to capture the cross-view correlations with tensor-based regularization. For instance, Zhang et al. [25] integrated self-expression based subspace representations from various views into a low-rank tensor, capturing the global structure and exploring correlations across multiple views. To obtain a more effective tensor for clustering, in addition to just considering the global structure and high-order correlations, Qin et al. made further strides by proposing two works: they investigated the local structures of similarity matrices from different views using the Markov chain[28], and explored pairwise correlations based on the reconstruction of a shared similarity matrix [29]. Additionally, To exploit the relationship between low-rank tensors and label indicator matrices, Fu et al. [30] unified low-rank tensor learning with spectral embedding into a framework. However, while these methods have shown success in capturing high-order correlations, their practical application is hindered by the stacking of large similarity graphs learned from multiple views into a tensor. This process incurs significant storage and computational complexity, making it unscalable for large datasets. Although some methods [31, 32] tried to address this problem by introducing bipartite graphs, they may result in sub-optimal solutions caused by an unstable anchor selection process. Furthermore, they often overlook the adverse effects of non-semantic information in each view, leading to the entanglement of latent semantic-unrelated and semantic-related features. As a consequence, the clustering performance may be compromised. To address the aforementioned challenges, we propose a novel approach called fast Disentangled Slim Tensor Learning (DSTL) for multi-view clustering. Specifically, Instead of concentrating on the multi-view graph structures, the DSTL approach directly leverages the matrix factorization technique to obtain low-dimensional features. Inspired by Robust Principal Component Analysis (RPCA) [33], we disentangle the features of each view to learn both semantic-unrelated and semantic-related representations. Subsequently, we construct two slim tensors that not only mitigate the adverse effects of latent semantic-unrelated information but also capture high-order consistency among multiple views. The semantic-unrelated slim tensor is assumed sparse with \ell_{1}-norm regularization, while the semantic-related slim tensor is assumed low-rank with tensor nuclear norm regularization. Additionally, we incorporate a consensus alignment indicator matrix to align semantic-related representations across views, guiding the disentanglement of latent features. In summary, we present the contributions of our model as follows: • We propose a novel fast MVC approach named DSTL, which combines latent multi-view features disentanglement for learning semantic representations and slim tensor learning for capturing cross-view correlations in a unified framework. • DSTL directly explores the high-order correlations among multi-view semantic-related representations via slim tensor learning. Latent feature disentanglement is adopted to mitigate the adverse effects of view-specific semantic-unrelated information. • The semantic-related representations are aligned across views to guide the disentanglement through a consensus alignment indicator. Experiments demonstrate the effectiveness and efficiency of DSTL compared to various state-of-the-art MVC methods. Figure 1: The overall framework of DSTL."
https://arxiv.org/html/2411.07664v1,Evaluating the Generation of Spatial Relations in Text and Image Generative Models,"Understanding spatial relations is a crucial cognitive ability for both humans and AI. While current research has predominantly focused on the benchmarking of text-to-image (T2I) models, we propose a more comprehensive evaluation 111https://github.com/trolommonm/SpatialRelBench that includes both T2I and Large Language Models (LLMs). As spatial relations are naturally understood in a visuo-spatial manner, we develop an approach to convert LLM outputs into an image, thereby allowing us to evaluate both T2I models and LLMs visually. We examined the spatial relation understanding of 8 prominent generative models (3 T2I models and 5 LLMs) on a set of 10 common prepositions, as well as assess the feasibility of automatic evaluation methods. Surprisingly, we found that T2I models only achieve subpar performance despite their impressive general image-generation abilities. Even more surprisingly, our results show that LLMs are significantly more accurate than T2I models in generating spatial relations, despite being primarily trained on textual data. We examined reasons for model failures and highlight gaps that can be filled to enable more spatially faithful generations.","Understanding spatial relations, which emerges early in human development [17, 40], is critical for human cognition [46], and is required for a wide range of tasks such as robot navigation [43] and concept learning [24]. Spatial relation understanding involves the usage of spatial prepositions – words that establish relationships between nouns [42] – to articulate complex spatial dynamics between objects. These prepositions are not only crucial for human communication, but also serve to gauge the proficiency of generative models in understanding spatial relations. (“What I cannot create, I do not understand.” – Richard Feynman). Existing text-to-image (T2I) models have demonstrated impressive capabilities in rendering images from textual prompts. For example, T2I models are able to generate realistic images from complex prompts such as “a cute corgi lives in a house made out of sushi”. Such impressive empirical results lead us to believe that T2I models are capable of understanding and reasoning about spatial relations. Despite these advancements, critical analyses of T2I models have uncovered inherent limitations. Insights from research focused on model development [38, 5] and various benchmarking studies [15, 10, 20, 7, 25, 13, 41] reveal that T2I models encounter difficulties with accurate prompt interpretation and spatial comprehension, hindering the ability to generate complex scenes precisely. While the above studies have been informative, analyses of spatial relations have been limited. Many studies which included spatial relations only cover a small number of such relations. Furthermore, many studies combine spatial relations with other factors, such as object counting and attribute binding, leading to confounds when assessing models’ understanding of spatial relations. Although spatial relations in T2I models have been somewhat investigated, fewer papers have explored in detail spatial relations in LLMs. This is detrimental for a few reasons. Many T2I models rely on strong text encoders such as frozen LLMs for prompt understanding [5, 3]. Thus, the spatial relation understanding of T2I models will be affected by those LLMs. Previous research has also leveraged LLMs to enhance spatial understanding in diffusion models [26, 30], highlighting interest in transferring knowledge from LLMs to improve image generation. Better diagnosis of challenges faced by both T2I models and LLMs will enable us to disambiguate challenges which arise from either language understanding or the image generation process, allowing us to be more effective in leveraging LLMs to improve T2I performance. However, the inherent visual nature of spatial relations poses a challenge in evaluating text-based LLMs’ understanding of them. Hence, we developed a method to convert LLM outputs into an image, allowing both T2I models and LLMs to be evaluated visually, as image generators. We avoided the use of real-world objects, as this may invoke biases learnt during training. For example, “dog under table” occurs much more frequently than “dog on table” [10]. Thus, if we prompt the model to generate “dog on table”, we may just get “dog under table” simply due to data biases [22]. The use of simple visual scenes involving 3 basic geometric shapes and 10 prepositions allows more precise assessment of whether a model truly understands spatial relations. Overall, our contributions are as follows: 1. We developed the first visually-based investigation of spatial relation understanding, in LLMs, providing the full prompt set; 2. We comprehensively benchmarked the generation of 10 important spatial relations in LLMs and T2I models; 3. We pushed the boundaries of spatial relation understanding in generative models by systematically evaluating the composition of multiple spatial relations in the best performing generative models, thoroughly examining 90 distinct permutations. Figure 1: Images generated using both T2I and LLMs for some example prompts."
https://arxiv.org/html/2411.07660v1,HMIL: Hierarchical Multi-Instance Learning for Fine-Grained Whole Slide Image Classification,"Fine-grained classification of whole slide images (WSIs) is essential in precision oncology, enabling precise cancer diagnosis and personalized treatment strategies. The core of this task involves distinguishing subtle morphological variations within the same broad category of gigapixel-resolution images, which presents a significant challenge. While the multi-instance learning (MIL) paradigm alleviates the computational burden of WSIs, existing MIL methods often overlook hierarchical label correlations, treating fine-grained classification as a flat multi-class classification task. To overcome these limitations, we introduce a novel hierarchical multi-instance learning (HMIL) framework. By facilitating on the hierarchical alignment of inherent relationships between different hierarchy of labels at instance and bag level, our approach provides a more structured and informative learning process. Specifically, HMIL incorporates a class-wise attention mechanism that aligns hierarchical information at both the instance and bag levels. Furthermore, we introduce supervised contrastive learning to enhance the discriminative capability for fine-grained classification and a curriculum-based dynamic weighting module to adaptively balance the hierarchical feature during training. Extensive experiments on our large-scale cytology cervical cancer (CCC) dataset and two public histology datasets, BRACS and PANDA, demonstrate the state-of-the-art class-wise and overall performance of our HMIL framework. Our source code is available at https://github.com/ChengJin-git/HMIL.","\IEEEPARstart Whole-slide images (WSIs) have been acknowledged as the gold standard for diagnosis [1, 2]. In precision oncology, fine-grained classification of WSIs is essential for accurate diagnosis and treatment planning. Unlike merely distinguishing between benign and malignant cases or simple categorization into two or three broad classes, fine-grained classification involves observing subtle morphological differences among cancer subtypes by examining different cell types and tissue structures within WSIs. This detailed classification provides doctors with more information to make accurate diagnoses and personalized treatment decisions, which is essential for recommending precise treatments such as surgery, radiation, and hormonal therapy [3]. Significant challenges are presented in fine-grained WSI classification due to the need to differentiate subtle variations under the gigapixel resolutions inherent in WSIs, setting it apart from natural image classification tasks [4]. To this end, multi-instance learning (MIL) has emerged as a leading approach for WSI classification. In this method, each slide is treated as a “bag” containing multiple image patches (instances), and only the bag-level labels are required for training. Despite advancements in MIL, there has been limited progress in addressing fine-grained classification tasks within WSI. Hierarchical classification incorporates hierarchical labels and corresponding network designs to tackle fine-grained classification challenges [5, 6]. In contrast to prior methods that address the problem in the setting of flat multi-class classification, hierarchical classification leverages the underlying structure of cancer subtypes. Several studies have attempted to address the challenges of fine-grained WSI classification within this context [7, 8, 9]. Specifically, Mercan et al. [7] conceptualized this as a multi-instance, multi-label learning problem. They utilized a conventional max-pooling MIL method constrained by a multi-label loss, where the instances were regions of interest identified by pathologists. However, their approach did not incorporate the hierarchical mapping among cancer subtypes across different hierarchies, which has been empirically shown to enhance the performance of fine-grained image recognition in natural images [10, 11, 12]. Introducing hierarchical mapping could provide valuable prior knowledge, aiding in distinguishing subtle differences between closely related subtypes. Recognizing this potential, Lin et al. [8] proposed DPNet, which utilizes instance-level annotations along with a hierarchical grouping loss in the instance detector and a rule-based classifier for slide-level predictions. Gao et al. [9] leverage information bottleneck theory to model pathologist-selected instances with hierarchical features within a multi-task framework, which employs an auxiliary instance-level classifier to enrich the feature representation for slide-level classification. While these approaches have advanced fine-grained WSI classification, their reliance on instance-level annotations limits broader applicability and fails to fully exploit hierarchical information for semantic guidance at both the instance and bag levels in MIL models. Figure 1: Comparison among prior works and our proposed HMIL framework in fine-grained WSI analysis. Left: Conventional flat classification methods, which form fine-grained classification as a multi-class classification task. Middle: Prior hierarchical classification methods, which typically leverage detector-enriched instance feature for hierarchical classification. Right: Our HMIL framework relaxed the need for detectors, introducing hierarchical alignment at both instance and bag level to improve fine-grained classification. To this end, we propose a novel hierarchical multi-instance learning (HMIL) framework. As illustrated in Figure 1, our HMIL framework adopts a dual-branch structure: a coarse branch for coarse-grained classification and a fine branch for fine-grained classification. Between this dual-branch structure, we introduce hierarchical alignment at both instance and bag levels to better guide the learning process. At the instance level, both branches utilize class-wise attention-based MIL to introduce the foundation of hierarchical information, and the hierarchical instance matching module aligns the fine branch’s class-wise attention with the coarse branch’s class-wise attention through a fine-to-coarse similarity constrain. At the bag level, the hierarchical bag alignment module ensures fine-to-coarse prediction consistency by aligning the predictions of both branches. Moreover, we incorporate supervised contrastive learning [13] to strengthen the discriminative capability of the fine branch by maximizing inter-class distances and minimizing intra-class variations. Recognizing that the broad knowledge provided by the coarse branch may not sufficiently guide fine-grained classification, we introduce a dynamic weighting strategy to balance the influence between the coarse and fine branches during training. The contributions of this paper are twofold. First, we formulate and explore hierarchical classification under the MIL settings and propose a novel framework termed HMIL. This framework leverages holistic hierarchical guidance at both the instance and bag levels to optimize the learning of feature embeddings and refine predictions, thereby enhancing the model’s ability to differentiate closely related cancer subtypes. Second, we evaluated our HMIL framework extensively on multiple fine-grained classification WSI datasets across various imaging modalities, including our private large-scale cytology cervical cancer (CCC) WSI classification dataset, which comprises 33,528 cytology WSIs, as well as two public histology WSI datasets, specifically BRACS [14] and PANDA [15]. Our findings indicate that HMIL achieves state-of-the-art performance compared to baseline models and enhances class-wise performance, revealing the importance of incorporating label hierarchy into the model."
https://arxiv.org/html/2411.07650v1,"Understanding Audiovisual Deepfake Detection:
Techniques, Challenges, Human Factors
and Perceptual Insights","Deep Learning has been successfully applied in diverse fields, and its impact on deepfake detection is no exception. Deepfakes are fake yet realistic synthetic content that can be used deceitfully for political impersonation, phishing, slandering, or spreading misinformation. Despite extensive research on unimodal deepfake detection, identifying complex deepfakes through joint analysis of audio and visual streams remains relatively unexplored. To fill this gap, this survey first provides an overview of audiovisual deepfake generation techniques, applications, and their consequences, and then provides a comprehensive review of state-of-the-art methods that combine audio and visual modalities to enhance detection accuracy, summarizing and critically analyzing their strengths and limitations. Furthermore, we discuss existing open source datasets for a deeper understanding, which can contribute to the research community and provide necessary information to beginners who want to analyze deep learning-based audiovisual methods for video forensics. By bridging the gap between unimodal and multimodal approaches, this paper aims to improve the effectiveness of deepfake detection strategies and guide future research in cybersecurity and media integrity.","The proliferation of smart digital devices such as mobile phones, laptops, tablets, and other digital gadgets, coupled with the accessibility of social media platforms, has promoted the exponential growth of multimedia content (images, videos, and audio) on the internet. This growth is further fueled by technological advances [1], including various deep generative networks [2] [3]. However, this accessibility heightens the need for caution because it can lead to the prevalence of disinformation. Despite this, many people still stick to the trend of the antiquated phrase “seeing is believing” and share multimedia content without considering its authenticity or verifying its digital integrity. Deepfake technology, or sophisticated Artificial Intelligence (AI) models, enable deep learning (DL) tools to manipulate media (images, videos, and audio) to generate hyper-realistic fake content that deceives viewers. Deepfake is AI-generated media that has been deceptively altered by superimposing a source face in a video onto a target face, manipulating the speech in an audio clip, or both. The vast amount of data available online in the form of images, videos, and audio to train such models makes detecting such forgeries increasingly challenging. The impact of deepfakes is critical because we still trust photographic and audio recording evidence. The emergence of realistic and subtle production tools makes fake content incredibly believable and harder to distinguish from genuine content [4]. The rapid spread of harmful and uncontrolled content from fake media has serious imminent impacts and reduces trust in journalism and news providers [5] [6]. Deepfake media content can be exploited to fuel political or religious tensions between countries [7], spread misleading information or rumors between political parties [5] [8], deceive the public [5], engaging in revenge porn [8], defame celebrities [8], promote fraud and identity theft [9], and create political chaos or publicity in a campaign [10]. Generative Adversarial Networks (GAN) [2] and Variational Autoencoders (VAE) [3] are sophisticated DL models for generating counterfeit content. In GAN, the generator network and the discriminator network are the two main components, and these two networks are opposed to each other. The generator aims to generate plausible data, while the discriminator determines the real data from the fake data generated by the generator. Similarly, VAE is an unsupervised learning method consisting of encoder and decoder architectures. VAE is used to create high-quality, hyper-realistic fake content by merging and/or superimposing existing media (images or videos) onto source media for the purpose of deception. Currently, AI-synthesized videos are mainly divided into three different generation types [11] [12]. (1) Head puppetry/puppet master is a counterfeit video generation technique based on the target person animating like a puppet. (2) Face swap aims to generate a video of the target person by swapping the target person’s face with that of the source person while retaining the same facial expression as the target person. (3) Lip-sync is another deepfake video generation method whose main goal is to transform a person’s lips to be synchronized or consistent with the target audio. This technique tends to manipulate the lip region in such a way that the target of the attack appears to be saying things they never said in reality. In the past few years, immense progress in the field of automatic video editing and a great interest in face manipulation techniques have been noticed. Advances in manipulation tools and open-source codes allow even naive users to use deepfake technology like an expert in a few simple steps. This technological advancement has a wide range of positive applications in the fields of visual effects, photography, education, film industry, virtual reality, video games, cinema, and entertainment. However, it also poses significant challenges in terms of authenticity verification and prevention of malicious use. To overcome these challenges, researchers have made many attempts and proposed DL-based unimodal forgery detection methods [13, 14, 15, 16, 17]. Figure 1: Volume of research into audiovisual deepfakes between 2017 and 2023. The detection of visual manipulation in videos has long been the focus of researchers, while the identification of audio forgeries has often been overlooked. Recently, however, the trend of sound manipulation has grown rapidly along with visual alterations, leading to bimodal fabrication that enhances the authenticity of fake content and makes detection difficult [18, 19, 20, 21]. Fig. 1 highlights the research community’s growing interest and concern in audiovisual deepfakes. The number of publications on audiovisual deepfakes has increased significantly in recent years, demonstrating both beneficial progress and growing concerns. The integration of multimodality is proven to be beneficial in various research fields [22, 23, 24]. Consequently, researchers have used various DL techniques that exploit audio and visual features for video forgery detection. Nonetheless, existing media forensics research is lacking in investigations that analyze methods for generating and detecting video deepfakes using audio and visual modalities. Table I lists an overview of relevant studies. Our study was strongly motivated by the lack of attention paid to audiovisual deepfakes in surveys, highlighting the urgent need to focus research on audiovisual deepfakes, including their generation, how to mitigate their harmful effects, and a summary of existing audiovisual deepfake detection methods. TABLE I: Comparison of survey studies related to deepfake detection. Reference Year Contribution Verdoliva [25] 2020 A discussion of video deepfakes from a forensic perspective, with an emphasis on the limitations of current forensic detection methods. Mirsky et al. [26] 2021 An in-depth analysis of field-specific generation techniques and a brief discussion of detection methods. Yu et al. [27] 2021 A detailed analysis of forged video synthesis and detection techniques, with a focus on face manipulation. Rana et al. [28] 2022 A comprehensive review of deepfake detection methods proposed during 2018-2020. Nguyen et al. [29] 2022 A comprehensive overview of deepfake generation and detection techniques and a discussion of challenges and future research directions in the field. Masood et al. [30] 2023 An analysis of the generation and detection of audio and visual deepfakes and a discussion of datasets. Mubarak et al. [31] 2023 An analysis of audio, visual, and text-based deepfakes, with a focus on detection methods. Figure 2: Taxonomy of deepfakes. Generally speaking, as shown in Fig. 2, there are four types of deepfakes, namely text deepfakes, audio deepfakes, visual deepfakes, and audiovisual deepfakes. Audiovisual deepfakes are a combination of acoustic and visual manipulation that can enhance manipulated videos to make them look more believable, and have received a lot of attention in recent years. This study specifically provides an in-depth review of the latest audiovisual deep learning solutions to improve the detection of challenging video deepfakes. To the best of our knowledge, we are the first to perform a comprehensive analysis of existing DL-based methods that exploit audio and visual manipulations in videos for automatic deepfake detection. Our important contribution also includes a comprehensive discussion of publicly available datasets relevant to this task. The main contributions of our work are as follows: • We provide an unprecedented survey that systematically analyzes key detection and generation methods for audiovisual deepfakes, with a special emphasis on automatic video deepfake detection methods. • We highlight the challenges, limitations, and human perception in the field of audiovisual deepfake detection. Furthermore, we outline research directions for future developments in this field. • We summarize and present publicly available datasets that can be used to train multimodal/audiovisual deepfake detectors. The remainder of this paper is organized as follows. Section II introduces different types of deepfakes. Section III discusses video deepfake detection methods. Section IV classifies detection methods that exploit visual and acoustic streams. In Section V, we review publicly available datasets for audiovisual deepfake detection. Section VI presents performance metrics and evaluation. Section VII examines human perception of audiovisual deepfakes. Section VIII discusses several aspects of the deepfake challenge and potential research directions. Finally, Section IX concludes this survey."
https://arxiv.org/html/2411.07649v1,Maritime Search and Rescue Missions with Aerial Images: A Survey,"The speed of response by search and rescue teams at sea is of vital importance, as survival may depend on it. Recent technological advancements have led to the development of more efficient systems for locating individuals involved in a maritime incident, such as the use of Unmanned Aerial Vehicles (UAVs) equipped with cameras and other integrated sensors. Over the past decade, several researchers have contributed to the development of automatic systems capable of detecting people using aerial images, particularly by leveraging the advantages of deep learning. In this article, we provide a comprehensive review of the existing literature on this topic. We analyze the methods proposed to date, including both traditional techniques and more advanced approaches based on machine learning and neural networks. Additionally, we take into account the use of synthetic data to cover a wider range of scenarios without the need to deploy a team to collect data, which is one of the major obstacles for these systems. Overall, this paper situates the reader in the field of detecting people at sea using aerial images by quickly identifying the most suitable methodology for each scenario, as well as providing an in-depth discussion and direction for future trends.","Maritime Search and Rescue (SAR) operations are critical for saving lives during emergencies at sea [11]. The nature of these operations can vary significantly depending on the environment and circumstances but generally involves responding to incidents such as shipwrecks, capsized vessels, or collisions. Additionally, the increasing number of perilous maritime crossings, often related to migration, has led to a rise in drownings, further complicating SAR efforts. These missions encompass three main tasks: searching, assisting, and rescuing affected people. While the approach to each mission may differ based on the situation, the speed of response is a crucial factor in all scenarios, particularly when lives are at stake [100]. However, accelerating rescue operations often requires greater deployment of resources, which in turn raises costs and can compromise the mission’s success. Furthermore, the availability of specialized equipment, which is often scarce, adds another layer of complexity and expense. Addressing these challenges demands urgent global attention, alongside technological advancements to improve the surveillance, detection, and rescue of individuals at sea. In Figure 1, we present a summary of the main aspects discussed in this section: a) A graphical representation of how the task is performed, b) some of the most significant and recurring maritime accidents, c) the equipment used for these operations, d) the types of objects that can be found on the sea surface, and e) varying visual appearances of the sea surface. Given these challenges, the development of more efficient systems, such as Unmanned Aerial Vehicles (UAVs), commonly known as drones (see Figure 1c), has the potential to greatly enhance the success of rescue missions. UAVs represent a paradigm shift in locating survivors. Equipped with sensors and high-resolution cameras, these drones can capture various types of images—thermal, multispectral, and visible spectrum—to provide aerial views of the environment. This enables the detection of victims’ positions in maritime accidents, assisting in their rescue, and transmitting their location and condition to rescue teams in real-time. UAVs can swiftly cover large sea areas and access locations that are difficult or impossible for traditional search vehicles, making them versatile and efficient tools [143]. Compared to helicopters, UAVs offer significant advantages, such as the ability to fly at lower altitudes, faster response times, and lower costs [113], thus optimizing rescue operations [95]. UAVs can be remotely controlled by trained pilots or operate autonomously, blending technological innovation with the critical need for timely humanitarian aid. Nevertheless, the effective use of UAVs requires integration with robust, reliable systems capable of accurately extracting essential information from the images, as the sea surface often contains other elements that complicate the rescue process. These include (see Figure 1d): ships, debris, buoys, marine animals, rocks, waves, or foam, which create visual noise and make it more difficult to identify people in need of assistance. Figure 1: Graphical overview of the task, concerns, UAV equipment, and background limitations. Recent advancements in artificial intelligence have created opportunities for more efficient methodologies across various tasks, and the SAR field is no exception. The literature includes studies that introduce new methods to facilitate these operations [122, 79]. Although the combination of UAVs with machine learning techniques has the potential to improve SAR efforts, it remains challenging due to the nature of the task. Utilizing aerial images to detect people at sea is particularly difficult, as the small size of humans in contrast to the vast ocean makes detection arduous. In addition to speed, precision is also a crucial factor in determining the success rate of rescue operations [88]. Various solutions and architectures based on neural networks address challenges such as classification, segmentation, tracking, and object detection in maritime environments [68]. However, external factors like uncontrollable water reflections [83], rapid changes in sea state caused by waves [30], adverse weather conditions, and variations in illumination at different times of the day can negatively impact performance (see Figure 1e). Consequently, recent research has focused on mitigating these issues to improve detection accuracy and build more robust models [21, 147]. A critical challenge for these algorithms is the requirement for large amounts of labeled data [65, 1], which is often difficult to obtain in real-world settings. Addressing data scarcity, such as through the generation of synthetic data, can help enhance the performance and effectiveness of machine learning algorithms not only in this field [167], but also in a wide range of applications [3, 6]. The existing literature includes several surveys on closely related areas, such as object detection from images captured from boats, covering topics like bridges, buoys, or ships [164], marine object detection [144], and the use of electro-optical sensors for object detection at sea [84]. Additionally, some reviews describe the operation of unmanned vehicles, including UAVs, unmanned surface vessels, and underwater unmanned vehicles, which emphasize the importance of these technologies for automatic monitoring [70]. A survey on datasets for general object detection in maritime contexts also exists [130]. These reviews highlight the relevance of various aspects within the SAR field. In contrast, our paper focuses on the detection of people at sea for rescue operations using UAVs. To the best of our knowledge, this is the first comprehensive review with such a focus. We provide an in-depth analysis of materials, existing methodologies, and experimental results in this domain. By addressing this gap in the literature, we aim to contextualize the reader and offer a reference point for future work in this area, which is particularly important today due to the increasing number of maritime accidents each year [29]. The main contributions of our survey, compared to the existing literature, are as follows: 1. An exhaustive study of current methodologies for SAR operations at sea. 2. A detailed review of real and synthetic datasets used in maritime rescue contexts. 3. An overview of tools and techniques for generating synthetic data. 4. A comparison of results obtained from methods applied to the most widely used benchmarks. 5. A discussion of the current research landscape, including limitations, future directions, and open challenges. The remainder of this paper is organized as follows: Section 2 presents a review of state-of-the-art SAR methods for detecting individuals at sea, from traditional approaches to the latest advancements. Section 3 introduces the most widely used public materials. Section 4 outlines the evaluation metrics typically employed in the field. Section 5 summarizes experimental results from existing contributions. Section 6 discusses the methods, results, open challenges, and trends identified. Finally, Section 7 presents concluding remarks and directions for future research."
https://arxiv.org/html/2411.07643v1,xCG: Explainable Cell Graphs for Survival Prediction in Non-Small Cell Lung Cancer,"Understanding how deep learning models predict oncology patient risk can provide critical insights into disease progression, support clinical decision-making, and pave the way for trustworthy and data-driven precision medicine. Building on recent advances in the spatial modeling of the tumor microenvironment using graph neural networks, we present an explainable cell graph (xCG) approach for survival prediction. We validate our model on a public cohort of imaging mass cytometry (IMC) data for 416 cases of lung adenocarcinoma. We explain survival predictions in terms of known phenotypes on the cell level by computing risk attributions over cell graphs, for which we propose an efficient grid-based layer-wise relevance propagation (LRP) method. Our ablation studies highlight the importance of incorporating the cancer stage and model ensembling to improve the quality of risk estimates. Our xCG method, together with the IMC data, is made publicly available to support further research.","Lung cancer remains the leading cause of cancer-related death, accounting for over 20% of all cancer cases (Siegel et al., 2021). To improve patient outcomes, it is crucial to further advance our understanding of the disease mechanisms and identify more precise risk factors. Established risk factors such as the UICC8’s TNM classification of malignant tumors (Brierley et al., 2017) represent the standard of care but lack granularity for personalized treatment decisions, e.g. not covering immune system, molecular, or metabolic parameters, often resulting in unnecessary side effects and rendering treatment insufficient. Advances in spatially resolved single-cell technologies now allow us to explore the tumor microenvironment (TME) in unprecedented detail (Sorin et al., 2023). Leveraging these technologies, graph neural networks (GNNs) have shown promise in modeling the TME in several cancer types, including lung cancer (Zhou et al., 2019; Wang et al., 2022; Nakhli et al., 2023; Zhang et al., 2024). However, while recent studies have begun to incorporate explainability into graph-based models (Sureka et al., 2020; Jaume et al., 2020; Hu et al., 2024; Zhang et al., 2024), explaining risk factors remains a significant challenge in the medical field due to the large scale of relevant graphs. In this paper, we present the following key contributions to address this challenge: • Modality-Agnostic Survival Prediction: We propose a versatile GNN framework that can (a) handle multiple tissue samples and graphs per patient, (b) incorporate multiple cell-level feature domains such as marker expression, tumor region-segmentation and patient-level clinical metadata, and (c) is capable of survival regression and classification. Our implementation in PyTorch is publicly available. • Scalable XAI for Cell Graphs: We introduce a novel efficient grid-based GNN-LRP method for cell graphs that enables high-resolution risk attribution at the cell level. • Enhanced Risk Assessment: Our ablation studies show that combining cancer stage fusion and model ensembling significantly improves the accuracy and reliability of risk assessments."
https://arxiv.org/html/2411.07635v2,Breaking the Low-Rank Dilemma of Linear Attention,"The Softmax attention mechanism in Transformer models is notoriously computationally expensive, particularly due to its quadratic complexity, posing significant challenges in vision applications. In contrast, linear attention provides a far more efficient solution by reducing the complexity to linear levels. However, compared to Softmax attention, linear attention often experiences significant performance degradation. Our experiments indicate that this performance drop is due to the low-rank nature of linear attention’s feature map, which hinders its ability to adequately model complex spatial information. In this paper, to break the low-rank dilemma of linear attention, we conduct rank analysis from two perspectives: the KV buffer and the output features. Consequently, we introduce Rank-Augmented Linear Attention (RALA), which rivals the performance of Softmax attention while maintaining linear complexity and high efficiency. Based on RALA, we construct the Rank-Augmented Vision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT achieves excellent performance across various vision tasks. Specifically, without using any additional labels, data, or supervision during training, RAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters and 4.6G FLOPs. This result significantly surpasses previous linear attention mechanisms, fully illustrating the potential of RALA. Code will be available at https://github.com/qhfan/RALA.","Figure 1: Comparison of Softmax attention and linear attention. Linear attention has linear complexity and high efficiency, but its spatial modeling capability is inferior to Softmax attention. In recent years, the Transformer [55] has garnered increasing attention as a powerful foundational architecture. In the field of computer vision, Vision Transformers [12] have made significant breakthroughs in image classification, object detection, instance segmentation, and semantic segmentation, further demonstrating the vast potential of Transformers. Figure 2: Comparison of feature maps output by Softmax attention and different linear attentions. All experiments are conducted based on the DeiT-T architecture, with N=196 and d=64. The full rank of matrices in the fig is 64. Compared to Softmax attention, the output features of various linear attentions exhibit significantly low-rank properties. This indicates that the diversity of features learned by linear attention is inferior to that learned by Softmax attention. Figure 3: Comparison among models based on linear attention and Softmax attention. Our RAVLT achieves state-of-the-art results across all scales and significantly outperforms existing vision models based on linear attention. However, applying Transformers to vision tasks is not straightforward. Self-attention, as the core mechanism of Transformers, has quadratic complexity. As the number of tokens increases, the computational load of self-attention grows significantly, making Transformers challenging to apply to high-resolution images. Many works attempt to address this issue, with some approaches grouping vision tokens to limit the number of tokens each token can interact with [43, 11, 70, 42, 16, 58, 54]. While this method effectively reduces the model’s complexity, it sacrifices the Transformer’s crucial ability to perceive information globally. Another line of work attempts to downsample the keys and values in Softmax attention to capture global features with lower complexity [56, 57, 15, 26]. However, this approach sacrifices the model’s fine-grained perception capabilities. Unlike the methods mentioned above, linear attention [23, 24, 45, 50, 32, 2] takes a different approach by replacing the Softmax with kernel functions and altering the computation order of Q, K, and V, which is shown in the Fig. 1. This reduces the quadratic complexity of Softmax attention to linear complexity. Additionally, because its computation closely resembles Softmax attention and only uses kernel functions to approximate Softmax, linear attention possesses excellent global modeling capability. Furthermore, since linear attention does not involve downsampling, it retains the model’s fine-grained perception ability. Despite having a form and advantages very similar to Softmax attention, the actual performance of linear attention is less than satisfactory. Although it is highly efficient, its performance shows a significant gap compared to Softmax attention. To identify the cause of this performance gap, we conduct a comparison based on the DeiT-T architecture, merely replacing different attention mechanisms (vanilla linear attention, Enhanced Linear Attention [2], Efficient Attention [50], Focused Linear Attention [23]), with results shown in Fig. 2. We perform rank analysis on the features of single heads from different feature maps (N=196, d=64, with the full rank of the matrix being 64). We find that the rank of output features from existing linear attentions are significantly lower than those from Softmax attention, indicating that the diversity of features they learn is comparatively poorer. Based on the above considerations, we aim to increase the rank of linear attention to achieve a trade-off between performance and efficiency in vision models. Specifically, we find that two computational steps in linear attention—the calculation of the KV buffer and the calculation of the output features—both affect its low-rank characteristics. Based on this, we propose Rank-Augmented Linear Attention (RALA). In RALA, to address the low-rank issue of the KV buffer, we use a set of context-aware rearrangement coefficients to restructure the weights of each token in the KV buffer. This approach enhances the richness and diversity of information in the KV buffer, which in turn increases the rank of the matrix. For the low-rank issue of the output features, we introduce a feature interaction strategy in the channel dimension, ensuring the output features reach a full-rank state. RALA effectively enhances the rank of the feature matrices in linear attention, enabling it to model complex spatial features with linear complexity. As shown in the Fig. 2, the output feature matrix of RALA reaches the full rank of 64. Utilizing RALA, we construct the Rank-Augmented Vision Linear Transformer (RAVLT). We conduct extensive experiments with RAVLT on image classification, object detection, instance segmentation, and semantic segmentation, and RAVLT achieves results comparable to state-of-the-art Vision Transformers. RAVLT also exhibits great trade-off between performance and efficiency. As shown in Fig. 3, without using any additional training data, labels, or supervision, our model achieves an accuracy of 84.4% on ImageNet with only 26M parameters and 4.6G FLOPs. As far as we konw, this surpasses all existing vision models."
https://arxiv.org/html/2411.07627v1,Leveraging Previous Steps: A Training-free Fast Solver for Flow Diffusion,"Flow diffusion models (FDMs) have recently shown potential in generation tasks due to the high generation quality. However, the current ordinary differential equation (ODE) solver for FDMs, e.g., the Euler solver, still suffers from slow generation since ODE solvers need many number function evaluations (NFE) to keep high-quality generation. In this paper, we propose a novel training-free flow-solver to reduce NFE while maintaining high-quality generation. The key insight for the flow-solver is to leverage the previous steps to reduce the NFE, where a cache is created to reuse these results from the previous steps. Specifically, the Taylor expansion is first used to approximate the ODE. To calculate the high-order derivatives of Taylor expansion, the flow-solver proposes to use the previous steps and a polynomial interpolation to approximate it, where the number of orders we could approximate equals the number of previous steps we cached. We also prove that the flow-solver has a more minor approximation error and faster generation speed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom, LSUN-Church, ImageNet, and real text-to-image generation prove the efficiency of the flow-solver. Specifically, the flow-solver improves the FID-30K from 13.79 to 6.75, from 46.64 to 19.49 with \text{NFE}=10 on CIFAR-10 and LSUN-Church, respectively.","Flow diffusion models (FDMs) [1, 2], which learn ordinary differential equation (ODE) to transport between noise and data distributions, are one of the generative models that could generate high-quality images. However, diffusion-based models always need many function evaluations (NFE) [3], which causes slow generation speed. Previous accelerating works [4, 5] include training-based and training-free methods. Training-based methods rely on distillation [4, 6], which uses a student model with smaller time steps to distill the original diffusion models. Training-free methods [7, 8, 9] aim to decrease the sampling steps by proposing a fast solver. This paper focuses on the training-free methods to accelerate FDMs [1, 2]. On the one hand, since the diffusion models [3, 10, 2] can be mainly categorized as FDMs and diffusion probabilistic models (DPMs), several training-free accelerated methods [8, 7, 9, 11, 12] have been proposed for DPMs. DPMs [3, 10] leverage a score function modeled by a neural network to formulate the probabilistic path. To reduce the NFE, DPM-solver [7] first proposed an acceleration method by proposing a unique general solution for the ODE. UniPC [9] improved the DPM-solver, leveraging a unified predictor-corrector framework. DC-solver [8] further improved the UniPC by introducing a dynamic compensation strategy. On the other hand, unlike the DPMs, FDMs are based on the conditional normalization flow [1], which unifies a specific ODE to improve the training paradigm. Recently, the accelerating methods for FDMs have mainly focused on training-based methods. For example, the self-distillation [2] has proposed to achieve one-step generation. The latest work is the bespoke Non-Stationary Solvers [13], which proposed a light-aware training-based accelerating method. A few works have been proposed for training-free methods. Euler solver [2] is a classic approach for ODE. Further, the Heun solver [14] is proposed to improve the Euler solver to reduce the approximation errors. Both Euler and Heun solvers need many NFEs. For example, the Heun solver needs an additional function evaluation, which increases the NFE. In this work, we propose the flow-solver, the novel, and training-free fast solver, to accelerate the sampling process of FDMs. The key insight is to reuse the previous steps to reduce the approximation errors. Since we can create a cache to collect the previous function evaluation results, thus we can also reduce the NFE. Concretely, the flow-solver first uses the previous steps to approximate the high-order derivatives of Taylor expansion of ODE, where the distances between the results of the previous steps and those of the current step are calculated. Then, a polynomial interpolation is used to solve the mismatch coefficients. Leveraging these allows us to make the previous steps to approximate the Taylor series of the velocity function in the next step. Our proposed flow-solver could more precisely approximate the continuous integral in the large time interval. The experimental results in CIFAR-10, CelebA-HQ, LSUN-Church, LSUN-Bedroom, ImageNet, and text-to-image conditional generation tasks show that our flow-solver makes a significant improvement for FDMs compared to the existing solvers, which prove the validity of our fast solver. To sum up, our main contributions are: • We propose a novel flow-solver for FDMs, which can generate high-quality images compared to the existing solvers in fewer NFE. • The flow-solver explores the connection between the previous steps and the high-order derivatives of Taylor expansion, which ensures the training-free acceleration for FDMs. • We conduct extensive experiments on various generation tasks with three different FDMs. The experiments show that our flow-solver achieves the best performance."
https://arxiv.org/html/2411.07625v1,Unraveling the Connections between Flow Matching and Diffusion Probabilistic Models in Training-free Conditional Generation,"Training-free conditional generation aims to leverage the unconditional diffusion models to implement the conditional generation, where flow-matching (FM) and diffusion probabilistic models (DPMs) are two mature unconditional diffusion models that achieve high-quality generation. Two questions were asked in this paper: What are the underlying connections between FM and DPMs in training-free conditional generation? Can we leverage DPMs to improve the training-free conditional generation for FM? We first show that a probabilistic diffusion path can be associated with the FM and DPMs. Then, we reformulate the ordinary differential equation (ODE) of FM based on the score function of DPMs, and thus, the conditions in FM can be incorporated as those in DPMs. Finally, we propose two posterior sampling methods to estimate the conditional term and achieve a training-free conditional generation of FM. Experimental results show that our proposed method could be implemented for various conditional generation tasks. Our method can generate higher-quality results than the state-of-the-art methods.","Diffusion-based conditional generation methods [1, 2, 3] aims at generating desired images according to the given conditions, such as text prompts. Conditional generation is the key component for various computer vision tasks, such as linear/non-linear inverse problems [2, 1], image edit [4], etc. These methods can be roughly categorized into training-based [5, 6] and training-free [7, 3] approaches. In this paper, we mainly focus on the training-free method. Training-free conditional generation methods [1, 3, 8] rely on a pre-trained unconditional diffusion model and do not require retraining. Based on the way to build the probabilistic path from Gaussian noise to data distribution, the unconditional diffusion models can be divided into two categories: diffusion probabilistic models (DPMs) [9, 10, 11], and flow diffusion models [12]. DPMs introduced the score function [9] to model the probabilistic path as a stochastic path. Flow diffusion models used Continuous Normalizing Flow (CNF) [13] to model the probabilistic path via flow-matching (FM), where CNF enables an interpolation way to define the probabilistic path from two distribution points. To simplify the notation, we abbreviated flow diffusion models as FMs to emphasize the flow-match. When the unconditional diffusion model is a DPM, one of the training-free conditional DPMs is based on the posterior sampling [2]. DPS [2] first proposed the posterior sampling to solve the linear inverse problems. FreeDom [2] improved DPS by introducing the energy-based function to extend posterior sampling to solve non-linear inverse problems. MPGD [3] further introduced the manifold theory to improve FreeDom. The posterior sampling methods provide a flexible solution for various conditional generation tasks. Unlike DPMs, there was very little research on training-free conditional FMs. The latest works include D-flow [14] and \pi-flow [15]. D-flow guided the generation process by fine-tuning the start point of FMs. \pi-flow [15] only focuses on the linear inverse problems for conditional generation. The existing training-free methods for FMs have certain limitations, e.g., the \pi-flow is hidden on the linear inverse assumption. While the posterior sampling [2] in DPMs can incorporate the conditions into the unconditional diffusion model easily. Unlike DPMs, FM does not have a direct way of introducing the conditions. Therefore, two natural questions arise: 1) Is there any connections between FM and DPMs? and 2) Can we leverage posterior sampling in DPMs to improve the training-free conditional generation for FM? In this paper, we propose a novel training-free conditional generation method for FMs called flow-match based posterior sampling (FMPS). The key insight is first to unravel the connection, where there is a probabilistic path between FMs and DPMs. This enables FMPS to define the score function explicitly. In this condition, FMPS could achieve the training-free conditional generation following a similar road of DPMs via posterior sampling. Concretely, posterior sampling depends on the score function in DPMs. We first explicitly connect the ODE of FM and the score function in DPMs (Detailed in Sec. 4). Then, to make the posterior sampling available for FMs, we redefine the ODE of the FMs as the score function in DPMs. This new definition could introduce conditions into FMs via posterior sampling, where FMPS can also easily change the marginal distribution (i.e., the unconditional FMs) to the conditional distribution (i.e., the conditional FMs). Hence, an unconditional FM and a distance metric could be used for the training-free conditional FMs. Moreover, we also propose two posterior sampling methods: gradient-aware FMPS and gradient-free FMPS, where the difference is whether we need to calculate the gradient of the FMs to perform the posterior sampling. The experimental results on linear inverse problems, non-linear inverse problems, and text-to-image generation tasks show that FMPS achieves better generation quality in various downstream tasks and verify the validity and efficiency of FMPS. To sum up, the main contributions of the FMPS are: • We propose a novel training-free conditional generation method, called FMPS, for the flow diffusion models. • We unravel the connections between flow-matching and diffusion probabilistic models. • The experimental results show that FMPS could solve board-range conditional generation tasks while maintaining high-quality generation."
https://arxiv.org/html/2411.07621v1,"Mix from Failure: 
Confusion-Pairing Mixup for Long-Tailed Recognition","Long-tailed image recognition is a computer vision problem considering a real-world class distribution rather than an artificial uniform. Existing methods typically detour the problem by i) adjusting a loss function, ii) decoupling classifier learning, or iii) proposing a new multi-head architecture called experts. In this paper, we tackle the problem from a different perspective to augment a training dataset to enhance the sample diversity of minority classes. Specifically, our method, namely Confusion-Pairing Mixup (CP-Mix), estimates the confusion distribution of the model and handles the data deficiency problem by augmenting samples from confusion pairs in real-time. In this way, CP-Mix trains the model to mitigate its weakness and distinguish a pair of classes it frequently misclassifies. In addition, CP-Mix utilizes a novel mixup formulation to handle the bias in decision boundaries that originated from the imbalanced dataset. Extensive experiments demonstrate that CP-Mix outperforms existing methods for long-tailed image recognition and successfully relieves the confusion of the classifier.","Large-scale datasets [1, 2], algorithmic innovations [3, 4], and powerful network architectures [5, 6] have driven remarkable achievements in computer vision over recent decades. However, these advancements primarily rely on clean, well-curated datasets like CIFAR-100 or ImageNet, which are balanced with a comparable amount of samples for each image class. In contrast, real-world data collection processes often result in biased and imbalanced datasets with a long-tail distribution. This distribution is characterized by a large number of minority classes containing very few samples, while the majority of samples belong to a small number of majority classes. Consequently, the standard training strategy based on empirical risk minimization struggles to generalize effectively to minority classes, leading to poor performance in these categories. Given the prevalence of long-tailed data in practical scenarios, numerous research efforts have been directed toward addressing long-tailed problems. An intuitive approach to addressing this issue is data augmentation, as the key challenge is the lack of samples in minority classes. However, implementing data augmentation is not straightforward in such contexts due to the extreme scarcity of minority samples. Often, the majority classes benefit more from the augmentation method, further enlarging the discrepancy between majority and minority classes. To address these challenges, researchers have explored simper methods by synthesizing minority data points, using techniques such as SMOTE and its variations [7, 8, 9]. One such approach is ‘Mixup’, which linearly interpolates two existing data points to create a new data point [10]. Despite its conceptual simplicity, Mixup has proven effective in improving generalization [11, 12, 13, 14, 15, 16, 17, 18] and long-tailed learning [19, 20, 21, 22, 23, 24]. In this paper, we focus on effectively applying Mixup in scenarios with a large number of severely imbalanced classes. We first demonstrate that Mixup does not improve prediction accuracy for minority groups when extreme imbalances exist in the data distribution. Instead, it amplifies the performance gap between majority and minority groups because Mixup does not consider class imbalance or relationships among different classes. Vanilla Mixup randomly samples two data points from the training set, which becomes increasingly ineffective for imbalanced and complex datasets due to the difficulty of sampling minority classes and pairing them with confusing classes. To address this, we developed the confusion-pairing mixup (CP-Mix) method, which specifically samples from ‘confusion pairs’ that are prone to misclassification. With this modification, we significantly improve minority accuracy, even when the imbalance factor is as high as 996, e.g., for Places-LT. In vanilla Mixup, samples from minority groups are rarely used and often paired with unrelated classes. Our method focuses on minority classes, pairing them with useful samples to enhance model generalization. Through an ablation study, we demonstrate that strategic sampling from confusion pairs is the primary contributor to our method’s improved performance. While many strategies address the long-tailed problem, we believe that our proposed CP-Mix is uniquely strong as a simple yet powerful tool. First, it provides better generalization than approaches based on re-weighting or re-sampling [25, 26, 27, 28, 29, 30] or modifying the objective function [31, 32, 33, 34, 35, 36], which often lead to overfitting and memorization of minority samples. Secondly, CP-Mix requires significantly less computation than solutions based on training multiple-experts [37, 38, 39, 40, 41, 42]. Finally, it is easy to implement on top of any existing learning algorithm, being model-agnostic. (a) Class distribution of an imbalanced training set CIFAR10-LT-200 and the model’s misclassification tendency. (b) Confusion matrix of the classifier trained using CIFAR10-LT-200 dataset. Figure 1: (a) The model’s tendency to misclassify the examples in the minorities (truck, ship, and dog) to their similar majorities (car, plane, and cat). The model captures the similarities between classes and wrongly exploits its capacity. (b) The confusion matrix of the model trained using the imbalanced dataset. x and y axis denote true labels and predictions, respectively, and each number in a cell denotes the number of predictions. There are clear relationships between semantically similar classes. To summarize our contributions: • We identify a novel problem that vanilla mixup strategies exhibit a bias-amplifying phenomenon under very-long-tailed regimes where we have a large number of classes and the imbalance is significant between majority and minority classes. • We propose CP-Mix algorithm that combines ideas of confusion pair sampling and class-imbalance-aware mixing to achieve more targeted intervention on the most vulnerable pairs while achieving generalization. • We provide through experimental evaluation that CP-Mix achieves improvements over baselines in various long-tailed datasets. This paper is organized as follows. Section 2 introduces the background of long-tailed recognition and mixup augmentation. Then, we discuss how imbalance in the training set affects the models and how mixup cannot improve them in Section 3. CP-Mix is described in Section 4, and its superiority is demonstrated in Section 5 through extensive experiments."
https://arxiv.org/html/2411.07619v1,Artificial Intelligence for Biomedical Video Generation,"As a prominent subfield of Artificial Intelligence Generated Content (AIGC), video generation has achieved notable advancements in recent years. The introduction of Sora-alike models represents a pivotal breakthrough in video generation technologies, significantly enhancing the quality of synthesized videos. Particularly in the realm of biomedicine, video generation technology has shown immense potential such as medical concept explanation, disease simulation, and biomedical data augmentation. In this article, we thoroughly examine the latest developments in video generation models and explore their applications, challenges, and future opportunities in the biomedical sector. We have conducted an extensive review and compiled a comprehensive list of datasets from various sources to facilitate the development and evaluation of video generative models in biomedicine. Given the rapid progress in this field, we have also created a github repository to regularly update the advances of biomedical video generation at: https://github.com/Lee728243228/Biomedical-Video-Generation","Artificial Intelligence Generated Content (AIGC) stands as a cornerstone in contemporary computer vision research, bolstering significant accomplishments across numerous sectors including economy [li2023Ecoagent], healthcare [qiu2023large], and transportation [zhang2023learning]. Notably within the biomedical field, image generation technology has been effectively applied to various imaging modalities, including Computed Tomography (CT) [mr-ct, diffusion-mri-ct], Magnetic Resonance Imaging (MRI) [dai2020multimodal, jiang2023cola, qiu2023visionfm], fundus photography [bellemo2019generative, go2024fundusgeneration], and pathological imaging [moghadam2023morphology, yellapragada2024pathldm]. The fidelity of these generated images has seen progressive enhancements through technological evolution: from early generative models such as generative adversarial network (GAN)-based models [dai2020multimodal, bellemo2019generative], to auto-regressive (AR) models [zhou2024conditional, vqgan], then to cutting-edge diffusion models [diffusion-mri-ct, yellapragada2024pathldm], and now moving towards the combination of auto-regressive and diffusion (AR + Diffusion) models [show-o, zhou2024transfusion]. Compared to static images that contain solely spatial information, videos encompass additional temporal and motion features. For instance, cardiac ultrasound videos present not only the cardiac structure and pathological conditions but also the dynamics of cardiac valves and pumping functions. Hence, richer information content poses a more complex challenge for video generation. Thus far, predominant techniques in video generation are also based on GANs [saito2017temporal, li2018video, tulyakov2018mocogan], AR models [kondratyuk2023videopoet, hong2022cogvideo], and diffusion models [stable-video-diffusion, bar2024lumiere]. Following the robust generative capabilities demonstrated by Sora [sora] and Movie Gen [MovieGen], diffusion models based on transformer architectures [diffusion-transformer, dubey2024llama3] have been increasingly employed for video generation, achieving state-of-the-art results. The AR + Diffusion models, by combining the strengths of AR models and diffusion models [zhou2024transfusion, show-o], and featuring a more lightweight design [show-o], have the potential to become the next-gen video generative models. Despite the current video generation models’ capability to simulate medical scenarios [li2024endora, zhao2024largeDSA, cho2024surgen, fundus2video, sun2024bora], in order to achieve a more realistic and reliable simulation, considerations should be taken from three aspects: understanding principles in physics, effective evaluation metrics for generated medical content, and controllability and explainability of generation. Understanding principles in physics is critical to enhancing the realism and precision of synthesizing biomedical videos. Taking surgical operations as an example for physics understanding, surgical procedures involve manipulating deformable tissues and organs using articulated instruments to achieve desired outcomes. While existing video generation models [li2024endora, sun2024bora] can create surgical scenes, they fail to model the surgical operations cohesively. The recent Movie Gen [MovieGen] has demonstrated certain abilities to simulate physical behaviors, but this ability has not been proven in the generation of medical videos. To achieve a more precise understanding of the motion characteristics in biomedicine, knowledge from physiology and pathology also needs to be further learned by generative models. In addition, it is crucial to understand the significance of evaluation criteria for biomedical video synthesis. In addition to considering the coherence and authenticity of the generated content [ji2024t2vbench, fan2023aigcbench, t2v-compbench], it is also necessary to ensure the medical utility and applicability of the generated content and its value added to the existing biomedical data. Therefore, in the design of evaluation criteria, it is necessary to additionally consider whether the biomedical knowledge contained in the generated content is meaningful and meets the needs of medical practice. Besides, generated videos could serve various medical purposes, such as aiding in diagnosis or education. Information to be generated in videos should be able to be precisely controlled, necessitating both controllability and explainability. Although controlling mechanisms such as proposed in ControlNet [controlNet] has proved its applicability in medical generation [guo2024maisi, sharma2024controlpolypnet], many issues remain unresolved. Despite facing enormous challenges, the field of biomedical video generation is poised for exciting advancements. Addressing these challenges will open up numerous opportunities for technological innovation. With a foundation of existing video generation models and a wealth of biomedical video datasets, this article will explore potential application scenarios. By analyzing current technologies and challenges, we can pave the way for innovative developments in video generation that will greatly benefit the biomedical and healthcare communities. Figure 1: The existing workflow of biomedical video generation, including large-scale pretraining, adaptation in the biomedical domain, and deployment in biomedical scenarios. In the process of development and deployment of video generation technology, we highlight prominent challenges, such as learning medical physical laws, as well as risks related to hallucinations and bias. The main contributions of this work are summarized as follows: • We summarized the three main challenges in medical video generation, including learning physical laws, establishing evaluation metrics and benchmarks, and enhancing controllability and explainability, and analyzed the corresponding potential solutions. • We conducted a comprehensive investigation of existing video generation models in the general domain and surveyed models in biomedical and healthcare domains. • We curated existing biomedical video generation datasets, including open-source datasets, video libraries, and biomedical videos on various multimedia platforms. • We discussed the potential applications of video generation including medical education, patient-facing applications, and public health promotion, and analyzed their feasibility. Article Pipeline: Section 2 analyzes the challenges faced by biomedical video generation and the potential solutions to address these challenges. Section 3 introduces video generation workflow including data pre-processing, model architecture, training, inference, and evaluation. Section 4 focuses on medical video datasets and generation techniques applied in biomedicine. Section 5 discusses future directions of biomedical video generation and analyzes their feasibility and challenges. In section 6, we discuss noteworthy risks in biomedical video generation, and finally, we conclude in section 7."
https://arxiv.org/html/2411.07608v1,"Quantum Information-Empowered Graph Neural
    Network for Hyperspectral Change Detection","Change detection (CD) is a critical remote sensing technique for identifying changes in the Earth’s surface over time. The outstanding substance identifiability of hyperspectral images (HSIs) has significantly enhanced the detection accuracy, making hyperspectral change detection (HCD) an essential technology. The detection accuracy can be further upgraded by leveraging the graph structure of HSIs, motivating us to adopt the graph neural networks (GNNs) in solving HCD. For the first time, this work introduces quantum deep network (QUEEN) into HCD. Unlike GNN and CNN, both extracting the affine-computing features, QUEEN provides fundamentally different unitary-computing features. We demonstrate that through the unitary feature extraction procedure, QUEEN provides radically new information for deciding whether there is a change or not. Hierarchically, a graph feature learning (GFL) module exploits the graph structure of the bitemporal HSIs at the superpixel level, while a quantum feature learning (QFL) module learns the quantum features at the pixel level, as a complementary to GFL by preserving pixel-level detailed spatial information not retained in the superpixels. In the final classification stage, a quantum classifier is designed to cooperate with a traditional fully connected classifier. The superior HCD performance of the proposed QUEEN-empowered GNN (i.e., QUEEN-\mathcal{G}) will be experimentally demonstrated on real hyperspectral datasets.Index Terms— Change detection, hyperspectral image, quantum computing, quantum deep learning, graph neural network.","Change detection (CD) aims to detect the changed areas of the same region between two different dates using remote sensing images, making this technique crucial for monitoring the earth’s surface. CD has had a profound impact on various substantial applications such as agriculture investigation, disaster management, urban planning, and land cover/use monitoring [1, 2, 3]. The evolution of hyperspectral imaging (HSI) technology has revolutionized various remote sensing applications [4, 5, 6, 7, 8]. Traditional CD datasets mainly include very high resolution (VHR) satellite images, synthetic aperture radar (SAR) images, and multispectral images (MSIs) [9, 10, 11]. Leveraging the rich spectral information, HSI data further enables outstanding substance identifiability [12, 13], thereby significantly enhancing the detection accuracy. In light of this advantage, hyperspectral change detection (HCD) has gained significant traction in recent years. Conventional CD methods can be divided into three categories: arithmetic-based, transformation-based, and classification-based. Arithmetic-based methods primarily utilize mathematical operations to detect the changed pixels between the bitemporal HSIs. For example, change vector analysis (CVA) detects the changes in multispectral images by analyzing the change vectors, with the possibility of change being the magnitude of the change vectors [14]. For transformation-based methods, bitemporal images are transformed into specialized feature domains. This process aims to suppress the unchanged pixels while highlighting the changed pixels [15, 16]. For instance, multivariate alteration detection (MAD) leverages a transformation based on correlation analysis to perform a pixel-wise change detection of bitemporal images [17]. Classification-based methods treat the CD task as a classification problem and employ classifiers to categorize the pixels into changed and unchanged classes. A method called support vector machines (SVMs) integrates multiple SVM classifiers into a framework by combining the classification output of the classifiers [18]. Besides the three main conventional categories, the CD problem has also been formulated as a convex optimization (CO) problem and solved by the convex optimizer alternating direction method of multipliers (ADMM) with convergence guaranteed [11]. Despite the contributions of traditional change detection methods, their reliance on manually designed features becomes a critical limitation in HCD analysis. With the development of deep learning (DL), convolutional neural networks (CNNs) have demonstrated remarkable success across diverse domains, revolutionizing numerous fields of study and application [19, 20]. This widespread adoption has naturally extended to the field of remote sensing, where CNNs are increasingly being leveraged to address complex challenges in HSI processing [21]. In recent years, CNN-based approaches for HCD have drawn attention due to their ability to capture spectral-spatial features and overcome limitations existing in traditional methods. A single-stream CNN framework has been proposed to detect the bitemporal changed areas using remote sensing images [22]. We have to mention GETNET, an end-to-end 2-D CNN architecture, which employs a mixed-affinity matrix to extract cross-channel gradient information [23]. Moreover, Bilinear Convolutional Neural Network (BCNN), a siamese-structured architecture, captures inter-temporal relationships in bitemporal images by employing a symmetric CNN and fusing the output feature maps [24]. In multilevel encoder-decoder attention network (ML-EDAN) [25], a two-stream encoder-decoder framework with a contextual-information-guided attention module is developed to obtain the multilevel hierarchical features, and the long short-term memory (LSTM) subnetwork is incorporated to analyze temporal correlations. In addition to conventional CNNs, transformers, a novel neural network architecture that leverages self-attention mechanisms to process sequential data in parallel, offering a superior ability to capture long-range dependencies and contextual information, has successfully been adopted to solve challenging hyperspectral problems [26, 27]. The concept of self-attention mechanisms has recently been utilized to design neural networks for HCD. CSANet employs a Siamese 2-D convolutional neural network architecture incorporating attention mechanisms to effectively extract the spatial-spectral-temporal features from bitemporal HSI pairs [28]. Furthermore, the spectral–spatial-temporal transformer (SST-Former) integrates spectral, spatial, and temporal transformers in a sequential manner, employing position encoding, spectral and spatial transformer encoders, class tokens, and a temporal transformer to process multi-dimensional HSI data [29]. In multiscale diff-changed feature fusion network (MSDFFN) [30], a temporal feature encoder–decoder (TFED) subnetwork with a bidirectional diffchanged feature representation (BDFR) module and a multiscale attention fusion (MSAF) module is proposed to extract multiscale features. However, Transformer-based models typically have a complex network structure, which leads to the potential overfitting on limited labeled samples when solving the HCD problem. Therefore, a gated spectral–spatial–temporal attention network with spectral similarity filtering (HyGSTAN) [31] employs a spectral similarity filtering module (SSFM) to reduce spectral redundancy within patches, followed by a lightweight gated spectral-spatial attention module (GS2AM) for extracting long-range spectral dependencies. Finally, HyGSTAN integrates temporal information through a gated spectral-spatial-temporal attention module (GS2TAM), ultimately reconstructing change information through an inverse spectrum merging operation. Although CNN-based methods have achieved promising performance, they have not fully exploited the characteristics of HSIs, such as the graph structure, to attain better results. Recently, graph neural networks (GNN) have been adopted to tackle hyperspectral remote sensing tasks due to the ability to learn the long-range information of the HSIs [32, 33, 34]. GNNs have also been designed for the CD problem; for example, an unsupervised structural relationship graph representation learning framework is proposed for multimodal change detection (SRGRL-CD) [35]. Moreover, dual-branch difference amplification graph convolutional network (\textnormal{D}^{2}AGCN) innovatively integrates a dual-branch structure with a difference amplification module (DAM), leveraging graph convolutional networks (GCNs) to extract and amplify distinctive features from bitemporal HSIs, thereby enhancing the discrimination between changed and unchanged areas even with limited training samples [36]. By introducing the concept of graph attention network (GAT) [37], a two-branch framework with a novel temporal-spatial joint graph attention (TSJGAT) module in superpixel-level branch and a CNN-based pixel-level branch is proposed in complement strategy dual-branch framework (CSDBF), enabling parallel extraction and complementary fusion of multi-scale features from bitemporal HSIs, thus enhancing discrimination of changed regions while mitigating uncertainties introduced by superpixel segmentation [38]. To address the issue that graph structure constructed by superpixels ignores the multi-order difference information among graph nodes and the local difference information within superpixels, an efficient multi-order GCN (MGCN) with a channel attention module (CAM) is proposed, iteratively refining neighborhood feature representations and enhancing bitemporal difference features [39]. Synergizing the strengths of GNN and transformer, a dual-branch local information-enhanced graph-transformer (D-LIEG) network combing cascaded LIEG blocks with a novel graph-transformer is proposed, effectively extracting and integrating local-global spectral-spatial features from bitemporal HSIs, thus enabling robust change recognition even with limited training samples while preserving spectral information and accommodating diverse change areas [40]. We have to mention a convex deep learning algorithm for HCD, termed CODE-HCD [41]. CODE-HCD is designed based on a framework called CODE [42], which combines the advantages of CO and DL, alleviating the mathematical burden in the data-fitting design in the CO part and tolerating the small-data situation for training the GNN CD network in the DL part. Although GNN-based methods have exploited the graph structure of HSIs, these conventional DL approaches, including CNN-based and GNN-based ones, focus on learning affine-mapping features, which may limit their ability to capture more diverse information. We believe that introducing radically new information sources is critical for any decision-making procedure (e.g., the final detection procedure in HCD). This motivates us to introduce the novel quantum unitary-computing information for obtaining more effective HCD solutions, though integrating the quantum information with the traditional feature information has posed a great challenge (to be solved in this work). In recent years, quantum computing has emerged as a promising field, garnering significant attention from researchers. Quantum neural networks (QNNs), designed based on parameterized quantum circuits, have been developed to address complex problems and have demonstrated exceptional performance in diverse domains, including drug response prediction, health detection, and image classification [43, 44, 45]. These problems are mostly classification tasks, and those related to images are mainly RGB image problems. However, due to the limitations of modern quantum computers, complementary techniques are essential to achieve advanced quantum computing capabilities for more sophisticated applications [46]. In 2023, an innovative quantum deep network (QUEEN) for addressing the challenging hyperspectral inpainting problem was developed as the first quantum algorithm for image restoration [47]. By incorporating CNN to assist QNN, it can address the constraints imposed by limited quantum bits (qubits) resources and the barren plateaus (BP) effect to restore HSIs. The QUEEN theory demonstrates the efficacy of quantum unitary-computing features extracted by quantum networks in addressing diverse imaging inverse problems. Very recently, QUEEN has been further applied to critical remote sensing missions, such as the challenging super-resolution task and the highly challenging multispectral unmixing (MU) problem [48, 49]. Figure 1: Overall network framework of the proposed QUEEN-\mathcal{G}. The bitemporal HSIs are first dimension-reduced by a 1\times 1 convolutional layer. Subsequently, the graph feature learning (GFL) and quantum feature learning (QFL) modules, further detailed in Figs. 2 and 3, are adopted to learn the graph-structured and quantum unitary-computing features at the superpixel level and pixel level, respectively. The extracted quantum feature and graph feature are then fused, and weighted by the spectral similarity matrix {\bm{Z}} (obtained from {\bm{X}}_{1} and {\bm{X}}_{2}). Then, the extracted feature map, the dimension-reduced input HSIs, and the spectral similarity map are further fused by a feature fusion module. Finally, the quantum-enhanced classifier (QEC), detailed in Fig. 4, is employed to detect the changed areas. With the additional quantum branch, new feature information yields upgraded decision-making in the final classification/detection. In this paper, we propose a QUEEN-empowered graph neural network (QUEEN-\mathcal{G}) for the HCD problem, integrating GNN and QNN to extract features at the superpixel level and pixel level, respectively. We remark that the design of QUEEN-based architecture is challenging because some powerful traditional architectures cannot be implemented under the quantum framework; for example, the quantum no-cloning theorem limits the possibility of implementing the quantum version of the powerful residual network (ResNet) [50]. Compared to GNN, which extracts affine mapping features, QUEEN provides unitary-computing capabilities during feature extractions [47], thus obtaining features from a radically new perspective for decision-making in the final detection. Moreover, the proposed algorithm incorporates a quantum-enhanced classifier (QEC) to improve the detection of changed areas. We propose a quantum architecture with full expressibility (FE) for the QUEEN-based modules used in QUEEN-\mathcal{G}, meaning that the quantum network in the quantum modules can generate any possible quantum state or perform any unitary transform [47]. This property allows the QFL and QEC module to effectively extract useful pixel-level entangled quantum features using the quantum unitary operators. The superpixel-level affine-mapping features are learned by a graph feature learning (GFL) module constructed based on [41], leveraging the tendency of changed pixels to form small regions. At the same time, a quantum feature learning (QFL) module extracts pixel-level entangled unitary-computing features, serving as a complementary mechanism to the GFL by preserving detailed spatial information not retained in the superpixels. In light of the outstanding classification ability of QNN that has been proven in the existing literature, the final classification stage employs the quantum-enhanced classifier, QEC, that fuses a quantum classifier with a traditional fully connected layer, utilizing a customized loss function to enhance detection performance. For the first time, QUEEN has been introduced for solving the highly challenging HCD problem. The main contributions of this article are summarized as follows: 1. In this paper, we propose a QUEEN-based quantum HCD algorithm, termed as QUEEN-\mathcal{G}, by incorporating GNN and QNN, where the GFL and QFL modules extract the global superpixel-level and local pixel-level features, respectively. The entangled unitary-computing QNN features are fused with the traditional affine-computing GNN features, providing radically new quantum information for better decision-making in the final detection stage of QUEEN-\mathcal{G}. 2. The QUEEN has the mathematically provable FE, so that the quantum network in the QUEEN-\mathcal{G} can generate any possible quantum state or perform any valid quantum transform. This property allows the QFL module to effectively extract useful pixel-level entangled quantum features using the quantum unitary operators. QFL, serving as a complementary role to the superpixel-level GFL module, is designed to work under the pixel-level mechanism for preserving the detailed spatial information not retained in the superpixels. As will be seen, the new quantum information yields the state-of-the-art HCD performances, as evidenced by the ablation study (cf. Section III-D1). 3. The QEC, which integrates a quantum classifier with a traditional fully connected layer, is constructed for the final classification stage (decision-making stage). Meanwhile, a customized loss function is proposed to individually enhance the classification ability of the quantum classifier and the traditional classifier, thereby improving the detection performance. The effectiveness of the QEC is also proved by the ablation study in Section III-D1. 4. The proposed QUEEN-\mathcal{G} significantly surpasses existing benchmark HCD methods, achieving state-of-the-art performance on real hyperspectral datasets. As detailed in Section III, both the quantitative analysis and the qualitative analysis demonstrate the superiority of QUEEN-\mathcal{G} in accurately detecting the changed areas. The remaining parts of the paper are organized as follows. In Section II, the design of the proposed QUEEN-\mathcal{G} for the HCD problem is illustrated. In Section III, experiments on real hyperspectral datasets of the comparisons with benchmark HCD methods and the analysis of the proposed algorithm are summarized and discussed. Finally, conclusions are drawn in Section IV."
https://arxiv.org/html/2411.07584v1,Grounded video caption generation,"We propose a new task, dataset and model for grounded video caption generation. This task unifies captioning and object grounding in video, where the objects in the caption are grounded in the video via temporally consistent bounding boxes. We introduce the following contributions. First, we present a task definition and a manually annotated test dataset for this task, referred to as GROunded Video Caption Generation (GROC). Second, we introduce a large-scale automatic annotation method leveraging an existing model for grounded still image captioning together with an LLM for summarising frame-level captions into temporally consistent captions in video. Furthermore, we prompt the LLM to track by language – classifying noun phrases from the frame-level captions into noun phrases of the video-level generated caption. We apply this approach to videos from the HowTo100M dataset, which results in a new large-scale training dataset, called HowToGround, with automatically annotated captions and spatio-temporally consistent bounding boxes with coherent natural language labels. Third, we introduce a new grounded video caption generation model, called VideoGround, and train the model on the new automatically annotated HowToGround dataset. Finally, results of our VideoGround model set the state of the art for the new task of grounded video caption generation. We perform extensive ablations and demonstrate the importance of key technical contributions of our model.","In recent years, we have witnessed tremendous progress in multimodal video understanding thanks to Large Language Models and advanced designs that exploit the synergy of video and language. Current efforts have focused on a variety of tasks that require reasoning about (possibly long) videos together with a certain level of comprehension of natural language. Examples include natural language video captioning (Chen & Jiang, 2021; Fujita et al., 2020; Huang et al., 2020; Mun et al., 2019; Tang et al., 2021a; Wang et al., 2018; 2021; Zhou et al., 2018), temporal alignment of video with language (Han et al., 2022; Ko et al., 2022; Sigurdsson et al., 2020; Yang et al., 2021b), finding relevant moments in videos given a language query (Gao et al., 2017; Hendricks et al., 2018; Lei et al., 2020b; Zhang et al., 2020b; a; Zhu et al., 2022), or video question answering (Engin et al., 2021; Kim et al., 2020; Le et al., 2020; Lei et al., 2018; Li et al., 2019; Park et al., 2021; Yang et al., 2021a; Yu et al., 2018; Yang et al., 2022a; Zeng et al., 2017). Others have looked at producing bounding boxes of events in video given natural language descriptions (Tang et al., 2021b; Zhang et al., 2020d; Huang et al., 2018) or given natural language questions (Lei et al., 2020a). Overall, these efforts have focused on producing video-level or moment-level outputs, such as (temporally localized) captions, or producing single event-level bounding boxes in video. At the same time, producing natural language video descriptions where the described objects are spatio-temporally grounded with bounding boxes in videos has received much less attention. Progress on this problem is, however, important as spatio-temporal grounding of natural language on a large scale is an important step to advance areas such as human-robot interaction and embodied perception (Li et al., 2022; McCarthy et al., 2024; Patel et al., 2022; Sermanet et al., 2018; Zorina et al., 2021). Key factors limiting progress on this problem are the lack of annotated testing data, dedicated grounded video caption generation models and appropriate large-scale training datasets, which are costly to manually annotate. In this work, we aim to narrow this gap by the following four contributions. First, we introduce the grounded video caption generation task together with a manually annotated test dataset of 1000 videos, which we name GROunded Video Caption Generation (GROC). This allows to measure progress on this challenging problem. Second, to address the issue of limited training data, we introduce a large-scale automatic annotation method leveraging an existing model for grounded still image captioning together with an LLM to summarize frame-level captions into video-level captions. The LLM is also tasked to track by language, associating frame-level phrases that correspond to objects with video-level phrases, resulting in object tubes with a consistent label. We apply this approach to videos from the HowTo100M dataset, which results in a new large-scale training dataset, called HowToGround, with automatically annotated captions and spatio-temporally consistent bounding boxes with coherent natural language labels. Third, we introduce a new grounded video caption generation model, called VideoGround. The key technical contributions of this model include: (i) spatio-temporal adapters, which enable efficient modeling of spatio-temporal information in video; (ii) bounding box decoder and that outputs temporally coherent bounding boxes in video and (iii) temporal objectness head that explicitly models objects that temporally leave the frame or are occluded. We train the VideoGround model on the automatically annotated HowToGround dataset. Fourth, we perform extensive ablations and demonstrate the importance of key technical contributions of our model. Results of our VideoGround model set the state of the art for the new task of grounded video caption generation."
https://arxiv.org/html/2411.07579v3,Projecting Gaussian Ellipsoids While Avoiding Affine Projection Approximation,"Recently, 3D Gaussian Splatting has dominated novel-view synthesis with its real-time rendering speed and state-of-the-art rendering quality. However, during the rendering process, the use of the Jacobian of the affine approximation of the projection transformation leads to inevitable errors, resulting in blurriness, artifacts and a lack of scene consistency in the final rendered images. To address this issue, we introduce an ellipsoid-based projection method to calculate the projection of Gaussian ellipsoid onto the image plane, which is the primitive of 3D Gaussian Splatting. As our proposed ellipsoid-based projection method cannot handle Gaussian ellipsoids with camera origins inside them or parts lying below z=0 plane in the camera space, we designed a pre-filtering strategy. Experiments over multiple widely adopted benchmark datasets show that our ellipsoid-based projection method can enhance the rendering quality of 3D Gaussian Splatting and its extensions.","Novel View Synthesis (NVS) plays a crucial role in computer vision and computer graphics, with numerous applications, including robotics, virtual reality, and 3D gaming. One of the most influential works in this field is the Neural Radiance Field (NeRF) [36], proposed by Mildenhall et al. in 2020. NeRF utilizes a Multilayer Perceptron (MLP) to store geometric and appearance information of a scene and employs differentiable volume rendering [9, 29, 35, 22]. Although NeRF and its extensions can render high-quality images, their training time is excessively long, and the rendering speed is far from achieving real-time rendering (\geq 30 fps). Recently, 3D Gaussian Splatting (3DGS) [24] has made a significant impact in NVS due to its real-time rendering speed, high-quality rendering results and competitive training times. Unlike NeRF, 3DGS represents scenes with a set of Gaussian ellipsoids explicitly. By projecting each Gaussian ellipsoid onto the image plane and using \alpha-blending for rendering, the properties of each Gaussian ellipsoid, position, pose, scale, transparency, and color, are optimized based on a multi-view photometric loss. Although 3D Gaussian Splatting has shown impressive results, due to the local affine approximation of the projection transformation [58] at the center of each Gaussian ellipsoid during rendering, errors are inevitably introduced, negatively affecting rendering quality and scene consistency. We observed blurriness and artifacts in distant objects in the scene, which we attribute to larger errors in the approximated projection transformation as the distance from the Gaussian ellipsoid center grows, particularly when distant Gaussian ellipsoids are generally large. Additionally, while the details rendered in the training set are of high quality, the results in the test set show a decline, likely due to the lack of scene consistency on account of the approximated projection transformation. To solve this problem, we propose an ellipsoid-based projection method. Our core idea is to calculate the ellipse equation projected onto the image plane based on the work of David Eberly [10], given the equation of the ellipsoid and the image plane. We first derive the gaussian ellipsoid equation from the covariance matrix of the 3D Gaussian function. Then we find the equation of the cone formed by lines that pass through the camera’s origin and are tangent to the ellipsoid surface. Finally, we determine the intersection line of this cone with the image plane, which gives us the projected ellipse equation. The following experiments demonstrate that there are two types of Gaussian ellipsoids that can cause the training process to diverge and negatively impact the system. The first type has the camera’s origin inside the ellipsoid, where no lines through the camera’s origin can be tangent to it. The second type consists of Gaussian ellipsoids that have a portion below z=0 plane in the camera space, the projection of these ellipsoids results in hyperbola or parabola [10] rather than an ellipse. To avoid negatively impacting the system, we designed filtering algorithms specifically for these two types of Gaussian ellipsoids. Extensive experiments demonstrated that our method not only improves rendering quality compared to 3DGS but also further accelerates rendering speed. In summary, we make the following contributions: • We proposed an ellipsoid-based projection method to eliminate the negative impact on rendering quality and scene consistency caused by approximating the projection transformation using the Jacobian of its affine approximation in 3DGS. • We design a pre-filtering strategy for Gaussian ellipsoids that cannot be projected before the rendering process, enhancing the system’s robustness and contributing to faster rendering speed. • Experiments conducted on challenging benchmark datasets demonstrated that our method surpasses 3DGS in both rendering quality and speed. • Our ellipsoid-based projection method shows improvement results on 3DGS and its extensions, and can be easily applied to them, requiring only few changes to the original code."
https://arxiv.org/html/2411.07578v1,Atmospheric turbulence restoration by diffeomorphic image registration and blind deconvolution,"A novel approach is presented in this paper to improve images which are altered by atmospheric turbulence. Two new algorithms are presented based on two combinations of a blind deconvolution block, an elastic registration block and a temporal filter block. The algorithms are tested on real images acquired in the desert in New Mexico by the NATO RTG40 group.","The image atmospheric turbulence distortion phenomenon remains a hard problem specially for horizontal ground imagers. Many works were done for astronomical images but the models used are not adapted for ground imaging systems. If these effects do not really affect an human observer in the case of weak turbulence, it can be very awkward for an automatic target recognition algorithm because the objects may be very different from the shapes learned by the algorithms. Some examples of different levels of turbulence are given in figure 1. We can clearly see that the distortion affect the image comprehension (look at the letters on the right image). For a few years, a NATO Task Group (RTG40) focus its work on the validation of atmospheric turbulence models for infrared passive and active imaging systems. In order to evaluate the models, an acquisition campaign was done in the desert in New Mexico where different levels of turbulence were accessible. More generally the image distortion appears for different phenomena (for example underwater imaging systems are subject to the scattering effect which can be viewed as an image distortion). Figure 1. Example of images acquired by passive imagers during the NATO RTG40 campaign In this paper, we adress some work in the field of image distortion restoration. The technique we propose modelize the alteration by a combination of image warping and blurring. Then we try to inverse the process by blind deconvolution and image registration. First, in section 2 we begin to describe the related phenomena and propose to use the model of Frakes et al. [2, 9]. In section 3, we remind the temporal filters which will be used in the global restoration algorithm. In section 4, we describe the blind deconvolution algorithm we use which comes from the work of Chan et al. (see [7, 11]). In section 5, we give details about the image registration which is based on the diffeomorphism approach described in the work of Younès ([12, 13]) and Beg ([1]). Then, we present the architecture of the image distortion restoration algorithm in section 6 and section 7 show many experiments on images on which turbulence is present. Then we will conclude and give some perspectives of our work."
https://arxiv.org/html/2411.07556v1,Multi-task Feature Enhancement Network for No-Reference Image Quality Assessment,"Due to the scarcity of labeled samples in Image Quality Assessment (IQA) datasets, numerous recent studies have proposed multi-task based strategies, which explore feature information from other tasks or domains to boost the IQA task. Nevertheless, multi-task strategies based No-Reference Image Quality Assessment (NR-IQA) methods encounter several challenges. First, existing methods have not explicitly exploited texture details, which significantly influence the image quality. Second, multi-task methods conventionally integrate features through simple operations such as addition or concatenation, thereby diminishing the network’s capacity to accurately represent distorted features. To tackle these challenges, we introduce a novel multi-task NR-IQA framework. Our framework consists of three key components: a high-frequency extraction network, a quality estimation network, and a distortion-aware network. The high-frequency extraction network is designed to guide the model’s focus towards high-frequency information, which is highly related to the texture details. Meanwhile, the distortion-aware network extracts distortion-related features to distinguish different distortion types. To effectively integrate features from different tasks, a feature fusion module is developed based on an attention mechanism. Empirical results from five standard IQA databases confirm that our method not only achieves high performance but also exhibits robust generalization ability.","In recent years, with the advent of the digital age and the rapid development of technology, image data has shown explosive growth. However, the diversity of images and equipment has caused the problem of uneven image quality. At the same time, the image will be affected by a variety of factors in the process of acquisition, transmission, etc., which leads to the distortion and quality change of the image. This ultimately affects people’s perception of images and the effectiveness of downstream visual tasks. In such a context, it is particularly important to develop an algorithm that can automatically evaluate the image quality. According to whether the reference information of the original image is needed or not, objective image quality assessment methods can be categorized into full-reference IQA (FR-IQA), reduced-reference IQA (RR-IQA), and no-reference IQA (NR-IQA). In real scenarios, it is almost impossible to obtain the original reference image. Therefore, NR-IQA has become a popular topic in the field of computer vision and image processing. Traditional NR-IQA methods map distorted images to objective quality scores by extracting hand-crafted features such as natural scene statistics (NSS) features[1, 2, 3]. The limitation of the traditional method is that it does not perform well in practice when faced with complex and diverse distorted images. In recent years, deep learning-based image quality assessment methods[4, 5, 6, 7, 8, 9] have attracted widespread attention due to their effectiveness. However, deep learning-based IQA methods are affected by the size of the dataset and data distribution. Current NR-IQA datasets are small in size and lack sufficient training samples. IQA models trained on small-scale datasets are inevitably biased, which limits their generalization and application in practical scenarios. Some established methods (e.g., CaHDC[4]) expand the dataset by annotating the images using the FR-IQA model. However, the prediction results of the FR-IQA model may deviate from the subjective human perception scores, and there is still room for further improvement in the generalization ability of the trained model. To further address the above problems and to efficiently improve the generalization of IQA models, many recent studies have explored multi-task strategy[10, 11, 12, 13, 14, 15]. This strategy aims to enhance the IQA task by integrating multiple networks and utilizing feature information from other tasks or domains. In this way, the model will be able to obtain richer feature representations from other networks and understand the features of the image more comprehensively. The MEON[10] method uses the distortion type identification subtask to extract features related to image distortion, thus providing effective crucial information for the quality assessment task. However, the method classifies specific distortion types from a set of predefined categories, limiting its generalization ability, especially for authentic distortion type images in wild scenes. To overcome this problem, we use a contrastive learning based approach to pre-train the distortion recognition sub-network. This approach enhances the network’s ability to generalise to unknown distortion types, including authentic distortions. Li et al.[14] proposed MMMNet, which jointly optimises the IQA subtask and the saliency subtask, thus improving the saliency-guided IQA performance. Pan et al.[15] proposed VCRNet, which is based on the task of visual restoration network. The performance of IQA is improved by combining the generated multilevel restoration features with the multilevel features generated by the quality assessment network. However, the limitation of these methods is that they usually perform multi-task feature fusion through simple operations (e.g., addition or concatenation) and provide only simple linear aggregation operations. To solve this problem, we adopt a feature fusion method based on the attention mechanism and propose a feature fusion module. The method effectively combines global contextual features with local contextual features and assigns appropriate weights to different features to better capture various details and distortions in the image. In addition, the human visual system (HVS) has different perceptual sensitivities to information of different spatial frequencies in an image. Since the high frequency information reflects the texture and details of the image, HVS pays more attention to the high frequency content of the image. Existing IQA methods do not explicitly model the high frequency information of an image. Thus, we construct a specialized high-frequency extraction network to guide the IQA network to pay attention to the texture detail information of the image. In summary, the main contributions of this paper are as follows: \bullet We introduce a new multi-task based NR-IQA framework, consisting of a quality estimation, high-frequency extraction, and distortion-aware network for enhanced quality assessment. \bullet We enhance the network’s texture and detail perception by incorporating HVS-based high-frequency feature extraction, guiding the quality assessment to focus on important image details. \bullet In order to better share and integrate information between multiple task networks and improve the network’s ability to recognize distorted features, we adopt a feature fusion method based on the attention mechanism and propose a feature fusion module. The remainder of this paper is organized as follows. Section II describes the related work on the NR-IQA approach. Section III describes our proposed end-to-end multi-task NR-IQA approach. Section IV gives the experimental results with related analysis. The paper is summarized in Section V. Figure 1: The framework of the proposed method. Our proposed framework consists of three branches: the Quality Estimation Network (QEN), the Distortion Aware Network (DAN), and the High Frequency Extraction Network (HFEN). The HFEN consists of vanilla convolution and high-frequency modules, with VAN as the backbone of QEN and ResNet-50 as the backbone of DAN. The QEN is the primary task, while the HFEN and the DAN are the auxiliary tasks. The DAN is pre-trained using Contrastive Learning to enhance the robustness of the distortion feature representation. The HFEN utilizes the high-frequency module to extract the high-frequency details, which helps the QEN to focus on key image components. In addition, an attention mechanism-based feature fusion method (AFF) is integrated for fusing distortion-aware features, and a feature fusion module (FFM) is proposed for adaptively fusing high-frequency features."
https://arxiv.org/html/2411.07555v1,"GaussianCut: Interactive Segmentation 
via Graph Cut for 3D Gaussian Splatting","We introduce GaussianCut, a new method for interactive multiview segmentation of scenes represented as 3D Gaussians. Our approach allows for selecting the objects to be segmented by interacting with a single view. It accepts intuitive user input, such as point clicks, coarse scribbles, or text. Using 3D Gaussian Splatting (3DGS) as the underlying scene representation simplifies the extraction of objects of interest which are considered to be a subset of the scene’s Gaussians. Our key idea is to represent the scene as a graph and use the graph-cut algorithm to minimize an energy function to effectively partition the Gaussians into foreground and background. To achieve this, we construct a graph based on scene Gaussians and devise a segmentation-aligned energy function on the graph to combine user inputs with scene properties. To obtain an initial coarse segmentation, we leverage 2D image/video segmentation models and further refine these coarse estimates using our graph construction. Our empirical evaluations show the adaptability of GaussianCut across a diverse set of scenes. GaussianCut achieves competitive performance with state-of-the-art approaches for 3D segmentation without requiring any additional segmentation-aware training.","Recent advances in 3D scene representation have enabled unprecedented quality in 3D view synthesis without requiring specialized equipment or an excessively high computational budget. Fully leveraging these advances requires tools for scene understanding and manipulation specifically designed to operate on such representations. Object selection and segmentation often serve as a crucial first step in both scene understanding and editing tasks. While 2D image segmentation has been widely studied, developing analogous techniques for 3D remains challenging. One key challenge is accounting for the choice of underlying 3D scene representation in the segmentation method. 3D Gaussian Splatting (3DGS) [22] offers an explicit representation of a scene using a set of Gaussians, each characterized by its own properties. The nature of this representation motivates the idea that Gaussians corresponding to the segmented object and the background can be isolated separately. Prior works in 3DGS segmentation involve augmenting each Gaussian with a low-dimensional feature, that is jointly optimized with the parameters of the Gaussians [54, 43, 6]. This is supervised by 2D features, which provide semantic information that can be used for segmentation. While this enables a 3D consistent segmentation, it significantly increases the fitting time and the already high memory footprint of the method. Thus, enabling 3DGS segmentation without modifying the optimization process is an important research challenge. We address this challenge by proposing GaussianCut, a novel method for selecting and segmenting objects of interest in 3D Gaussian scenes. Our work taps directly into the representation created by 3DGS and maps each Gaussian to either the foreground or background. The proposed process mirrors the interactive nature of 2D segmentation tools, where users can engage through clicks, prompts, or scribbles. We require such user input on a single image and perform the object selection process in two steps. First, we obtain dense multiview segmentation masks from the user inputs using a video segmentation model. Subsequently, we construct a weighted graph, where each node represents a Gaussian. Graph cut then partitions the graph into two disjoint subsets by minimizing an energy function, which quantifies the cost of cutting the edges connecting the subsets. This approach effectively segments the selected foreground object from the background by using the energy function as a measure of dissimilarity between the nodes. An overview of the process is provided in Figure 1. Our main contribution is a novel approach for segmentation in scenes obtained from 3DGS. Its main technical novelties are twofold: 1) we propose a method for graph construction from a 3DGS model that utilizes the properties of the corresponding Gaussians to obtain edge weights, and 2) based on this graph, we propose and minimize an energy function (Equation 3) that combines the user inputs with the inherent representation of the scene. Our experimental evaluations show that GaussianCut obtains high-fidelity segmentation outperforming previous segmentation baselines."
https://arxiv.org/html/2411.07546v1,"Contrastive Language Prompting to Ease 
False Positives in Medical Anomaly Detection","A pre-trained visual-language model, contrastive language-image pre-training (CLIP), successfully accomplishes various downstream tasks with text prompts, such as finding images or localizing regions within the image. Despite CLIP’s strong multi-modal data capabilities, it remains limited in specialized environments, such as medical applications. For this purpose, many CLIP variants—i.e., BioMedCLIP, and MedCLIP-SAMv2—have emerged, but false positives related to normal regions persist. Thus, we aim to present a simple yet important goal of reducing false positives in medical anomaly detection. We introduce a Contrastive LAnguage Prompting (CLAP) method that leverages both positive and negative text prompts. This straightforward approach identifies potential lesion regions by visual attention to the positive prompts in the given image. To reduce false positives, we attenuate attention on normal regions using negative prompts. Extensive experiments with the BMAD dataset, including six biomedical benchmarks, demonstrate that CLAP method enhances anomaly detection performance. Our future plans include developing an automated fine prompting method for more practical usage.","Input Ground-truth A_{\textit{positive}} A_{\textit{negative}} A_{\textit{CLAP}} (Ours) Normal Brain MRI False positives True negatives Chest X-ray False positives True negatives Abnormal Brain MRI False positives True negatives Chest X-ray False positives True negatives Fig. 1: Generated attention maps by leveraging BiomedCLIP [1]. A_{\textit{positive}} and A_{\textit{negative}} are the attention maps obtained using positive or negative prompts only. A_{\textit{CLAP}} shows results of our proposal, dubbed Contrastive LAnguage Prompting (CLAP). CLAP leverages both positive and negative prompts. The negative prompts are used to attenuate false positive attention of normal regions. Fig. 2: Schematic diagram of our method. Existing positive prompt methods only utilize positive prompts. In this situation, the false positive attention issue remains. In comparison, our method CLAP successfully suppresses false positives by additionally exploiting negative prompts, shown in (a). After getting the attention map of CLAP, we employ the existing UAD model EAR [2]. We only replace the saliency map for mosaic obfuscation with an attention map from CLAP, shown in (b). Recent advancements in multi-modal models, particularly visual-language models (VLMs), have revolutionized various downstream tasks, such as image retrieval, captioning, and object localization. Among these, Contrastive Language–Image Pre-training (CLIP) [3] has demonstrated remarkable performance by leveraging natural language prompts to interpret visual data. This capability enables CLIP to handle a wide array of tasks without specialized fine-tuning. However, its application to highly specialized domains, such as medical imaging, has uncovered limitations. In the medical field, accurate anomaly detection is crucial for early diagnosis and treatment. Nevertheless, general-purpose models like CLIP often struggle with the intricacies of medical images, which contain subtle and features unique to medical imaging essential for identifying pathological regions. To improve performance in biomedical domains, various adaptations of CLIP, such as BiomedCLIP [1] and MedCLIP-SAMv2 [4], have been proposed to improve performance in biomedical domains. They shows surprising enhancement of medical reasoning compared to ordinary models, but the issue of false positives—incorrectly identifying normal regions as anomalous—remains prevalent. These false positives can lead to unnecessary medical procedures, increasing the burden on healthcare systems and potentially harming patients. To address this issue, we propose a novel method called Contrastive LAnguage Prompting (CLAP), which introduces a more refined way of leveraging natural language prompts for medical anomaly detection. By leveraging both positive and negative prompts, our method aims to find out lesions accurately with CLIP attention. Positive prompts guide the CLIP attention toward potential lesion regions, while negative prompts help attenuate the attention on normal regions, thereby reducing the occurrence of strong attention to false positives. This approach not only provides a more improved understanding of the medical image but also aligns with the demands for reliability in medical diagnostics by artificial intelligence. We can just determine whether disease or not based on the CLIP attention. Toward a more accurate diagnosis, we employ an unsupervised anomaly detection (UAD) method that features a reconstruction-by-inpainting strategy [2]. For this, we obfuscate strong attention regions, over \mu+0.674\sigma (Q3) [5] valued regions, by considering suspected disease regions. Then, we attempt to reconstruct obfuscated regions into normal patterns by U-Net which is trained with normal samples only. Finally, we determine the final disease based on the reconstruction error obtained. To evaluate the legitimacy of our proposal, we perform extensive experiments using BMAD dataset [6]. This dataset provides six benchmarks for five anatomies. Visual comparisons demonstrate that CLAP successfully overcomes issues of strong attention in non-lesion regions. In addition, we improved UAD performance compared to existing methods. Through this work, we aim to bridge the gap between general-purpose VLMs and the specific needs of medical anomaly detection. We conclude by discussing the potential for automating language prompt construction to further improve the usability of this approach in real-world clinical settings."
https://arxiv.org/html/2411.07541v1,HiCoM: Hierarchical Coherent Motion for Streamable Dynamic Scene with 3D Gaussian Splatting,"The online reconstruction of dynamic scenes from multi-view streaming videos faces significant challenges in training, rendering and storage efficiency. Harnessing superior learning speed and real-time rendering capabilities, 3D Gaussian Splatting (3DGS) has recently demonstrated considerable potential in this field. However, 3DGS can be inefficient in terms of storage and prone to overfitting by excessively growing Gaussians, particularly with limited views. This paper proposes an efficient framework, dubbed HiCoM, with three key components. First, we construct a compact and robust initial 3DGS representation using a perturbation smoothing strategy. Next, we introduce a Hierarchical Coherent Motion mechanism that leverages the inherent non-uniform distribution and local consistency of 3D Gaussians to swiftly and accurately learn motions across frames. Finally, we continually refine the 3DGS with additional Gaussians, which are later merged into the initial 3DGS to maintain consistency with the evolving scene. To preserve a compact representation, an equivalent number of low-opacity Gaussians that minimally impact the representation are removed before processing subsequent frames. Extensive experiments conducted on two widely used datasets show that our framework improves learning efficiency of the state-of-the-art methods by about 20\% and reduces the data storage by 85\%, achieving competitive free-viewpoint video synthesis quality but with higher robustness and stability. Moreover, by parallel learning multiple frames simultaneously, our HiCoM decreases the average training wall time to <2 seconds per frame with negligible performance degradation, substantially boosting real-world applicability and responsiveness. Code is avaliable at https://github.com/gqk/HiCoM.","The online reconstruction of dynamic scene from multi-view video streams is essential for advancing applications such as real-time free-viewpoint video (FVV) and virtual reality (VR), which are revolutionizing entertainment, education, and industry by providing immersive and interactive experiences. However, achieving high fidelity streamable dynamic scene poses significant challenges in training time, rendering speed, data storage and transmission efficiency. Traditional methods for modeling and representing dynamic scenes, such as surface estimation [3], multi-sphere imaging [4], and depth mapping [5, 6] via multi-view stereo technologies, struggle with the complex geometries and varied appearances of real-world scenarios. These techniques also face limitations in capturing the detailed and dynamic nature of practical environments. Neural Radiance Fields (NeRFs) [7, 8, 9, 10, 11, 12, 13, 14, 15, 16] have made significant breakthroughs in 3D reconstruction and novel view synthesis by mapping spatial coordinates and viewing directions to color and density using a neural network. Dynamic NeRFs [1, 17, 18, 19, 20, 21, 22, 23] integrate temporal components to capture scene changes over time. However, the high computational demands of NeRF’s volume rendering framework limit their feasibility in real-time applications. Acknowledging NeRF’s limitations, researchers have introduced 3D Gaussian Splatting (3DGS) [24], an innovative method for fast 3D reconstruction and real-time rendering. In contrust to NeRF, 3DGS explicitly employs anisotropic 3D Gaussians as primitive elements to represent 3D scenes, then rasterizes these Gaussians to render images from specific viewpoints, bypassing the complex and slow volume rendering pipeline. This not only speeds up the rendering process but also enhances the quality of the synthesized views. In a similar vein to NeRF, 3DGS has been adapted for dynamic scene reconstruction: several Dynamic Gaussian Splatting works [25, 26, 27] directly expand attributes of Gaussian primitives; other efforts [28, 29, 30, 31, 32, 33, 34] focus on decoupling the dynamic scene into a base scene representation in canonical space and time-varying motion fields. Harnessing its efficiency, 3DGS naturally supports online learning of dynamic scenes. A recent development, 3DGStream [35], introduces a pioneering framework that uses a Neural Transformation Cache derived from InstantNGP [36] to model the scene changes from previous frame to current frame, significantly reducing training time and storage requirements. However, this frame-by-frame online learning pipeline heavily relies on the quality of the initial 3DGS representation. Given that dynamic scenes are generally captured with a limited number of cameras, 3DGS can be prone to instability and overfitting when faced with sparse views. Furthermore, the number of Gaussian primitives in the initial 3DGS representation impacts the learning efficiency of subsequent frames. Despite employing multi-resolution hash encoding and lightweight MLP to alleviate inefficiency and speed up convergence, implicitly modeling the motion field still cannot fully capture critical aspects of the explicit and discrete nature of 3DGS. Although 3DGStream introduces new Gaussians to adapt to the appearance of new objects, these added Gaussians are not carried over to subsequent frames to maintain 3DGS compact. As real-world scenes evolve gradually, the discrepancies from the initial scene accumulate, necessitating more Gaussians to accurately capture these changes. In this paper, we introduce the Hierarchical Coherent Motion (HiCoM) framework, a novel approach designed to enhance the efficiency and stability of streamable dynamic scene online reconstruction. Our HiCoM framework begins with the learning of a compact and robust initial 3DGS representation through a perturbation smoothing strategy. This ensures a reduced number of Gaussians, alleviates overfitting and establishes a foundation for consistent quality across frames. Then, we leverage the inherent non-uniform distribution and local consistency of 3D Gaussians to implement a hierarchical coherent motion mechanism. Specifically, we partition the scene into regions and recognize that only a few regions actually contain Gaussian primitives due to the non-uniform distribution of Gaussians. We explicitly model the motion within these non-empty regions, allowing Gaussians in the same region to share identical motion patterns. These regions can be further divided into smaller areas, enabling the motion of each Gaussian to be determined by the combined motions of all levels of regions it inhabits. This hierarchical coherent motion mechanism captures motions from coarse to fine granularity and requires only a minimal set of parameters, facilitates rapid convergence. The inherent structure and consistency within and between regions thus support swift learning of scene changes across frames. We also introduce additional Gaussians to better accommodate significant updates in scene content. These new Gaussians are carefully integrated into the initial 3DGS representation to ensure continuous consistency with the evolving scene. To maintain the compactness of the 3DGS, an equivalent number of low-opacity Gaussians, which no longer significantly contribute to the scene representation, will be removed before the learning of the next frame. This continual refinement to the initial 3DGS representation ensures it remains as close as possible to the evolving scene, facilitating better subsequent learning. In addition, we introduce a parallel training strategy that enables simultaneous learning of multiple frames, significantly enhancing training efficiency with minimal impact on performance. The contributions of this paper are summarized as follows: • We introduce the HiCoM framework for online learning of dynamic scene from multi-view video streams, featuring a perturbation smoothing strategy for robust initial 3DGS representation learning, hierarchical coherent motion mechanism for efficient motion capture, and continual refinement to adapt evolving scene updates. • We devise a novel and concise hierarchical coherent motion mechanism that capitalizes on the inherent non-uniform distribution and local consistency of 3D Gaussians, efficiently capturing and modeling scene motions for swift and precise frame-to-frame adaptation. • Extensive experiments demonstrate that our HiCoM framework improves learning efficiency by about 20\% and reduces data storage by 85\% compared to state-of-the-art methods. Our parallel training strategy enables learning multiple frames simultaneously, substantially decreasing the training wall time with negligible effects on overall performance, further enhancing the practicality and responsiveness of our HiCoM for real-world applications."
https://arxiv.org/html/2411.07516v1,SparrowVQE: Visual Question Explanation for Course Content Understanding,"Visual Question Answering (VQA) research seeks to create AI systems to answer natural language questions in images, yet VQA methods often yield overly simplistic and short answers. This paper aims to advance the field by introducing Visual Question Explanation (VQE), which enhances the ability of VQA to provide detailed explanations rather than brief responses and address the need for more complex interaction with visual content. We first created an MLVQE dataset from a 14-week streamed video machine learning course, including 885 slide images, 110,407 words of transcripts, and 9,416 designed question-answer (QA) pairs. Next, we proposed a novel SparrowVQE, a small 3 billion parameters multimodal model. We trained our model with a three-stage training mechanism consisting of multimodal pre-training (slide images and transcripts feature alignment), instruction tuning (tuning the pre-trained model with transcripts and QA pairs), and domain fine-tuning (fine-tuning slide image and QA pairs). Eventually, our SparrowVQE can understand and connect visual information using the SigLIP model with transcripts using the Phi-2 language model with an MLP adapter. Experimental results demonstrate that our SparrowVQE achieves better performance in our developed MLVQE dataset and outperforms state-of-the-art methods in the other five benchmark VQA datasets. The source code is available at https://github.com/YoushanZhang/SparrowVQE.","Visual Question Answering (VQA) is an interdisciplinary problem that combines computer vision with natural language processing in answering questions regarding images, aiming to recognize and localize objects and information presented in a context. It makes positive differences in various applications, such as aiding visually impaired people, supporting educational tools, and developing user interfaces of human-computer interaction [1]. Figure 1: Our SparrowVQE matches the performance of 7B models in numerous visual language tasks, standing out from general-purpose text and visual language models. A major challenge for the development of VQA is the large diversity of questions that can be asked, from simple identifying tasks, such as “What is in the picture?”, to complex queries requiring sophisticated comprehension and inferential reasoning of the relationships and stories in the visual content. Applying VQA in educational settings, specifically machine learning (ML) lectures, can be more complex, as ML lectures often include complex diagrams, mathematical formulas, and dense textual information. Traditional educational resources often fail to provide the engagement and context-aware assistance necessary for effective learning. This gap results in difficulties in bridging theoretical knowledge with practical understanding in the educational context. Recently, several education chatbots or VQA systems were developed to improve students’ learning experience in different education levels [2, 3, 4, 5, 6, 7]. However, the models’ performance varies with preliminary evaluation and mostly produce short answers instead of detailed explanation of questions. The educational VQAs still face the problem of limited training data and over-simplistic answers produced. In this paper, we propose an MLVQE dataset for model training in the machine learning setting, specifically to achieve automatic teaching. We also propose a SparrowVQE model to enrich the VQA application in education. It caters to ML learners by allowing them to ask questions about visual content directly. Our work directly contributes to the improvement of effectiveness and personalizing of learning experiences. We improve our model’s efficiency in interpreting slide-text/transcript pairs, making it outperform state-of-the-art models in VQA tasks in different contents, as shown in Fig. 1. Our model has the potential to be applied in other educational domains that require training with more datasets."
https://arxiv.org/html/2411.07478v1,GUS-IR: Gaussian Splatting with Unified Shading for Inverse Rendering,"Recovering the intrinsic physical attributes of a scene from images, generally termed as the inverse rendering problem, has been a central and challenging task in computer vision and computer graphics. In this paper, we present GUS-IR, a novel framework designed to address the inverse rendering problem for complicated scenes featuring rough and glossy surfaces. This paper starts by analyzing and comparing two prominent shading techniques popularly used for inverse rendering, forward shading and deferred shading, effectiveness in handling complex materials. More importantly, we propose a unified shading solution that combines the advantages of both techniques for better decomposition. In addition, we analyze the normal modeling in 3D Gaussian Splatting (3DGS) and utilize the shortest axis as normal for each particle in GUS-IR, along with a depth-related regularization, resulting in improved geometric representation and better shape reconstruction. Furthermore, we enhance the probe-based baking scheme proposed by GS-IR to achieve more accurate ambient occlusion modeling to better handle indirect illumination. Extensive experiments have demonstrated the superior performance of GUS-IR in achieving precise intrinsic decomposition and geometric representation, supporting many downstream tasks (such as relighting, retouching) in computer vision, graphics, and extended reality.","Inverse rendering is a long-standing and challenging task in computer vision and computer graphics, aiming to recover intrinsic physical attributes (e.g. scene geometry, surface material, and environment lighting) of a 3D scene from multiple observations. The main challenges arise from the optimization in uncontrolled environments with unknown illumination, which leads to ill-posedness in decomposing multiple strongly coupled physical attributes. Traditional methods [1, 2, 3, 4, 5, 6, 7] struggled to simultaneously and accurately estimate the geometry, material, and illumination of a complex scene. Recently, neural rendering techniques such as Neural Radiance Fields (NeRF) [8] have shown remarkable promise in scene geometric reconstruction tasks [9, 10, 11, 12, 13]. However, most existing methods have overlooked the interaction between intrinsic physical attributes and illumination. Their implicit representations, which adopt an overly simplified appearance model as a function of view direction only, limit their applicability to many downstream tasks like relighting. Although recent methods [14, 15, 16, 17] introduce differentiable physical-based rendering to decompose physical attributes (e.g. geometry, surface materials, and environmental lighting), they still face challenges related to low rendering speed and explicit editing, especially when it can not be rendered at interactive rates. 3D Gaussian Splatting (3DGS) [18] has recently emerged as a promising technique to model 3D scenes and significantly boost the rendering speed to a real-time level. It combines explicit and more compact scene representation with differentiable rasterization [19, 20], resulting in a fast and remarkable performance for novel view synthesis. It is natural and essential to introduce physical-based rendering to 3DGS to achieve efficient inverse rendering tasks [21, 22, 23]. Despite their impressive relighting performance, these approaches still face challenges in geometric estimation and shading schemes. Firstly, during the 3DGS optimization, the adaptive control of the Gaussian density may lead to loose geometry, making it difficult to estimate accurate scene’s normal. One key step in using 3DGS for inverse rendering is accurately representing the geometry to describe the transportation of illumination. Although GS-IR [22] introduces a depth-related regularization, it ignores the geometric attributes of the Gaussian ellipsoid by attaching a learnable vector as the normal for each particle, struggling to compactly associate the normal optimization with the geometry. Consequently, it is necessary to model normal and better reconstruct geometry via the geometric attributes of the ellipsoid. Besides, another critical aspect of employing 3DGS for inverse rendering involves the shading scheme. Depending on where physical-based rendering is incorporated within the 3DGS-based framework, the shading scheme can be divided into forward shading [23] (i.e. shading each particle before ‘splatting’) and deferred shading [22] (i.e. shading each pixel after ‘splatting’). These different shading schemes have a substantial impact on material decomposition and overall rendering quality. Specifically, forward shading emphasizes the representation of diffuse colors, making it well-suited for complex scenes featuring rough surfaces, while deferred shading is more adept at capturing glossy objects with high specular regions. More details can be found in Fig. 3. Thus, it is critical to define a suitable shading scheme to make the inverse rendering feasible for different cases, particularly for complex scenes with rough surfaces and specular regions. Figure 1: Given multi-view captured images of a complex scene featuring rough and glossy surfaces, we propose GUS-IR (Gaussian Splatting with Unified Shading for Inverse Rendering), which utilizes 3D Gaussians to recover high-quality physical properties (e.g., normal, material, illumination) under unknown illumination. This enables us to perform advanced applications (e.g. relighting), resulting in outstanding inverse rendering results. Better viewed on screen with zoom-in, we successfully recovered the glossy surfaces of the marble balls placed in the center of the garden. In this paper, we are motivated to present a powerful 3DGS-based inverse rendering framework, GUS-IR (Gaussian Splatting with Unified Shading for Inverse Rendering), to achieve high-quality intrinsic decomposition under unknown illumination. To represent the normal in 3DGS, we treat each particle as a surfel [24] and utilize its shortest axis as the orientation (i.e. normal) for each particle. Then we find that forward and deferred shading schemes are beneficial for reconstructing rough and glossy surface materials, respectively. In GUS-IR, we thus unify both shading schemes to facilitate the intrinsic physical attributes decomposition of complex scenes containing various surface materials. By leveraging the benefits of unification, GUS-IR addresses the challenges posed by glossy surfaces in complex scenes. Additionally, we improve the baking scheme proposed in GS-IR, which leverages dense binary cubemaps to cache the occlusion during baking, achieving better ambient occlusion modeling. The contributions of this work can be summarized as follows: • We present GUS-IR that unifies several shading mechanisms to achieve intrinsic decomposition of glossy objects and complex scenes; • We revisit forward and deferred shading schemes, and propose a unified shading scheme to handle complex scenes and better capture highlight details on glossy surfaces; • We propose the shortest-axis normal modeling to represent reliable geometry, along with the depth-related regularization for better reconstruction. • We improve the probe-based baking solution proposed in GS-IR to better model ambient occlusion and handle indirect illumination. We demonstrate the superiority of our method to baseline methods qualitatively and quantitatively on various challenging scenes, including the TensoIR Synthesis [16], Shiny Blender [25], Glossy Blender [26], Mip-NeRF 360 [27], and Ref-NeRF Real [25] datasets. This work extends our preliminary work as described in [22]. In this journal version, we first give a formal formulation of the rendering equation for Gaussians and analyze the advantages of two shading schemes (forward shading and deferred shading). We then propose a unified shading framework that systematically consolidates two shading mechanisms, which outperforms other shading methods on both diffuse and specular regions. Finally, we replace the SH coefficients used in the baking solution of GS-IR with dense binary cubemaps to better handle ambient occlusion. To validate this framework, we re-conducted all our experiments including new tests on glossy objects, achieving superior performance on view synthesis, relighting, and geometry reconstruction."
https://arxiv.org/html/2411.07472v1,Semi-Truths: A Large-Scale Dataset of AI-Augmented Images for Evaluating Robustness of AI-Generated Image detectors,"Text-to-image diffusion models have impactful applications in art, design, and entertainment, yet these technologies also pose significant risks by enabling the creation and dissemination of misinformation. Although recent advancements have produced AI-generated image detectors that claim robustness against various augmentations, their true effectiveness remains uncertain. Do these detectors reliably identify images with different levels of augmentation? Are they biased toward specific scenes or data distributions? To investigate, we introduce Semi-Truths, featuring 27,600 real images, 223,400 masks, and 1,472,700 AI-augmented images that feature targeted and localized perturbations produced using diverse augmentation techniques, diffusion models, and data distributions. Each augmented image is accompanied by metadata for standardized and targeted evaluation of detector robustness. Our findings suggest that state-of-the-art detectors exhibit varying sensitivities to the types and degrees of perturbations, data distributions, and augmentation methods used, offering new insights into their performance and limitations. The code for the augmentation and evaluation pipeline is available at https://github.com/J-Kruk/SemiTruths.","The rise of text-to-image generative models has democratized automated image creation for machine learning practitioners and the general public alike. While existing architectures like Variational Autoencoders vahdat2020nvae ; huang2018introvae and GANs bao2017cvae ; Zhu_2017 ; hjelm2017boundaryseeking ; Isola_2017 ; Karras_2019 have produced realistic images for several years, diffusion models dhariwal2021diffusion ; rombach2022high ; DiffSurvey2023 have enhanced image quality, diversity, and usability, driving their rapid adoption. However, this technology presents a double-edged sword: despite its applications in fields like art, design, marketing, and entertainment hughes2021generative ; DiffApp2023 , its growing ubiquity brings a heightened risk of misuse for spreading misinformation MisinfDiff2023 ; monteith2024artificial . Recent incidents reveal an alarming rise in AI-modified images being used to perpetrate harmful acts such as misinformation campaigns PopeFrancis ; Wendling_2024 ; MetGala2024 , fraud, defamation, and identity theft cnnFinanceWorker ; globalinitiativeCriminalExploitation ; Fry_2024 ; Chang_2024 . One concerning capability of these models is their ability to alter small attributes of an original image. Rather than creating images from scratch, individuals can alter only specific parts or attributes of an image to drastically change the narrative while decreasing the likelihood of detection. An example of one such ""semi-truth"" is the “Sleepy Joe” samuels-2020 video circulated on Twitter in 2020, where President Joe Biden’s face was edited to appear as if he fell asleep during an interview. The implications of such subtle perturbations and their potential to spread misinformation PopeFrancis ; Wendling_2024 ; MetGala2024 underscore the critical need for automated detection of such attacks. Dataset Magnitude of Change Targeted Perturb. Saliency Check Data Collection Generation Data Dist. Scale GANs Diffusion #Methods Scene #Real Bench. Real Fake 1 DFDC DFDC2020 ✗ ✗ ✗ Generated ✓ ✗ 8 Face 1 488.4k \sim1.7M 2 FaceForensics++ roessler2019faceforensicspp ✗ ✗ ✗ Generated ✓ ✗ 4 Face 1 509.9k \sim1.8M 3 Celeb-DF Celeb_DF_cvpr20 ✗ ✗ ✓ Generated ✓ ✗ 1 Face 1 225.4k \sim2.1M 4 DeepFakeFace song2023robustness ✗ ✗ ✗ Generated ✗ ✓ 3 Face 1 30k 90k 5 CIFAKE bird2023cifake ✗ ✗ ✗ Generated ✗ ✓ 1 General 1 60k 60k 6 DiffusionDB wangDiffusionDBLargescalePrompt2022 ✗ ✗ ✓ Sourced ✗ ✓ 1 General 0 0 14M 7 MidJourney prompts iulia_turc_gaurav_nemade_2022 ✗ ✗ ✗ Sourced ✗ ✓ 1 General 0 0 248k 8 TWIGMA chen2023twigma ✗ ✗ ✗ Sourced ✗ ✓ unknown General 0 0 800k 9 GenImage zhu2023genimage ✗ ✗ ✗ Generated ✓ ✓ 8 General 1 1.33M 1.35M 10 Semi-Truths ✓ ✓ ✓ Generated ✗ ✓ 8 General 6 27,635 \sim1.47M Table 1: Semi-Truths vs other AI-generated image datasets. We compare Semi-Truths with other AI-generated image datasets across multiple categories: (1) Magnitude of Change: provides metadata on the magnitude of perturbations; (2) Targeted Perturb.: performs targeted perturbation of images; (3) Saliency Check: saliency assessment of fake images; (4) Data Collection: data collection strategy, Generated or Sourced from publicly available portals; (5) Generation: generator category and number of methods used (TWIGMA’s method was unknown since its images were sourced from Twitter); (6) Data Distribution: scene variation and diversity of real benchmarks; (7) Scale: number of real and fake images. However, existing datasets for training and evaluating AI-generated image detectors primarily consist of fully synthesized images, often limited to human faces DFDC2020 ; roessler2019faceforensicspp ; Celeb_DF_cvpr20 ; khalid2022fakeavceleb ; dang2020detection . This narrow focus fails to capture the diversity of real-world perturbations and does not reveal model biases toward different degrees of change. To address this, we introduce Semi-Truths, which includes AI-augmented images with varying levels of perturbation (detailed comparison in Tab. 1), enabling the evaluation of detectors against more realistic and diverse attacks like the “Sleepy Joe” video samuels-2020 . To develop a resource for stress-testing specific biases in AI-generated image detectors, precise control over the nature and extent of changes is essential. To achieve this, we source images from 6 popular semantic segmentation datasets, which contain salient images and labeled entity masks across a range of subjects. We categorize the magnitude of change in Semi-Truths based on two criteria: (1) the size of the augmented region, and (2) the semantic change achieved. The size of the augmented region is captured by surface area of the mask, structural similarity index measure (SSIM), mean squared error (MSE), and a custom metric derived from MSE (see Algo. 2). Semantic changes are quantified using metrics such as Semantic Magnitude, Learned Perceptual Image Patch Similarity (LPIPS) zhang2018perceptual , DreamSim fu2023dreamsim , and Sentence Similarity sun2022sentence (see Sec. 3.1). Perturbed images in Semi-Truths are generated through diffusion inpainting and prompt-based-editing hertz2022prompttoprompt ; mokady2022nulltext techniques using 5 different diffusion algorithms Openjourney ; Kandinsky ; podell2023sdxl ; rombach2022highresolution . Our approach to curating Semi-Truths employs a flexible, plug-and-play framework for human-guidance-free image editing. This pipeline is designed to ensure reusability and adaptability to new data distributions, large language models for prompt perturbation, diffusion models, and various image synthesis techniques. By releasing this framework, we aim to empower the community to create customized stress tests to evaluate AI-generated image detectors for specific use cases. Finally, we demonstrate how the knowledge abstractions in Semi-Truths can be used to identify the sensitivities of existing detectors. By stress-testing 6 models, we reveal unique sensitivities to different data distributions, diffusion models, and perturbation degrees. Our goal is to offer a resource for targeted, interpretable, and standardized evaluation of AI-generated image detection systems, and to provide a customizable evaluation pipeline for the community."
https://arxiv.org/html/2411.07463v3,": ultimodal mentation with Enhanced ision Foundation Models, onvolutional Neural Networks, and ncertainty uantification for High-Speed Video Phase Detection Data","Purpose: High-speed video (HSV) phase detection (PD) segmentation is vital in nuclear reactors, chemical processing, and electronics cooling for detecting vapor, liquid, and microlayer phases. Traditional segmentation models face pixel-level accuracy and generalization issues in multimodal data. MSEG-VCUQ introduces VideoSAM, a hybrid framework leveraging convolutional neural networks (CNNs) and transformer-based vision models to enhance segmentation accuracy and generalizability across complex multimodal PD tasks.Methods: VideoSAM combines U-Net CNN and the Segment Anything Model (SAM) for advanced feature extraction and segmentation across diverse HSV PD modalities, spanning fluids like water, FC-72, nitrogen, and argon under varied heat flux conditions. The framework also incorporates uncertainty quantification (UQ) to assess pixel-based discretization errors, delivering reliable metrics such as contact line density and dry area fraction under versatile modalities.Results: VideoSAM outperforms SAM and modality-specific CNN models in segmentation accuracy, excelling in environments with complex phase boundaries, overlapping bubbles, and dynamic liquid-vapor interactions. Its hybrid architecture supports cross-dataset generalization, adapting effectively to varying modalities. The UQ module provides accurate error estimates, enhancing the reliability of segmentation outputs for advanced HSV PD research.Conclusion: MSEG-VCUQ, via VideoSAM, offers a robust solution for HSV PD segmentation, addressing previous limitations with advanced deep learning and UQ techniques. The open-source datasets and tools introduced enable scalable, precise, and adaptable segmentation for multimodal PD datasets, supporting advancements in HSV analysis and autonomous experimentation. The codes and data used for this paper are publicly available at: https://github.com/chikap421/mseg_vcuq.","Boiling heat transfer is a complex phenomenon integral to various industrial and engineering applications, including nuclear reactors, electronics cooling, and chemical processing systems, where efficient thermal management is paramount [1, 2]. This process is governed by intricate mechanisms such as evaporation, quenching, and single-phase convection, all contributing to the overall heat flux [3]. Predictive mechanistic models aim to quantify these contributions using key parameters like nucleation site density, bubble departure diameter, contact line density, and dry area fraction. These metrics are crucial for understanding boiling dynamics but depend heavily on high-resolution data obtained through advanced diagnostics such as infrared thermography [2, 4] and high-speed video (HSV) phase detection (PD) imaging [5, 6]. PD images, such as those shown in Figure 1, provide crucial insights by differentiating between liquid, vapor, and microlayer phases on a boiling surface [5]. Yet, traditional methods for analyzing these images, whether manual or semi-automated, require significant time and expertise, limiting scalability for larger multimodal datasets. Convolutional neural networks (CNNs), especially U-Net [7, 8], have successfully segmented two-phase flow (TPF) images by learning complex features in data, thus providing a promising solution to automate HSV PD segmentation. Unlike the shadowgraphy images typically used for TPF studies (Figure 1), PD images feature complex bubble structures and diverse liquid-vapor interactions. These unique attributes create challenges for accurate segmentation, particularly in identifying the bubble footprints and contact lines. Figure 1: (a) Sample of Front-Lit Shadowgraphy images from two-phase flow. (b) Sample Phase-Detection Images used in this study. HSV PD data offers a valuable lens into the boiling process by delineating liquid, vapor, and microlayer phases. However, these HSV datasets’ manual or semi-automated analysis can be labor-intensive and subject to subjectivity, limiting scalability for large multimodal datasets. Traditional segmentation techniques, including edge detection and thresholding, struggle with overlapping bubbles and dynamic flow patterns, often requiring manual adjustments across experiments. Recently, CNN architectures like U-Net have become the standard for HSV tasks due to their ability to learn intricate visual patterns and deliver precise segmentation results [9, 10, 11]. Nevertheless, these CNN architectures, though powerful, exhibit limited generalizability when applied to multimodal datasets comprising varying experimental conditions, posing a significant barrier to their adoption in scientific PD tasks [12, 13, 14]. This limitation is pronounced in HSV PD analysis, where dynamic physical processes and domain-specific visual characteristics demand flexible yet robust segmentation models. The emergence of vision foundation models, like the Segment Anything Model (SAM), has demonstrated promising generalization capabilities across various segmentation tasks by leveraging transformer-based architectures [15]. Despite their potential, adapting these models to scientific HSV PD tasks remains underexplored, with most applications focused on natural images rather than PD in boiling experiments [16]. To overcome the limitations of traditional CNN-based models in HSV PD segmentation, this work presents MSEG-VCUQ. At the core of MSEG-VCUQ is VideoSAM, a hybrid segmentation model that integrates CNNs with the transformer-based architecture of SAM to enhance PD segmentation in multimodal HSV data. VideoSAM utilizes U-Net for preliminary mask generation, effectively capturing primary liquid-vapor boundaries and refines these masks through SAM’s attention-driven feature extraction. This combined approach enables VideoSAM to segment complex fluid behaviors and dynamic boiling conditions with higher precision across diverse multimodal datasets, including water, FC-72, nitrogen, and argon. The open-source dataset curated for this study encompasses a broad spectrum of PD modalities and dynamic fluid behaviors, establishing a strong foundation for training and evaluating VideoSAM. Additionally, MSEG-VCUQ incorporates an uncertainty quantification (UQ) module to assess the reliability of boiling metrics such as dry area fraction and contact line density, which are essential for accurately modeling boiling dynamics. Experimental results demonstrate that VideoSAM consistently outperforms traditional custom models, including U-Net, especially in complex scenarios characterized by varied bubble formations and intricate boundary delineation. These findings highlight the model’s versatility and scalability, positioning MSEG-VCUQ as a comprehensive solution for HSV PD segmentation in multimodal boiling data for advanced scientific and engineering applications in autonomous experiments."
https://arxiv.org/html/2411.07462v1,MureObjectStitch: Multi-reference Image Composition,"Generative image composition aims to regenerate the given foreground object in the background image to produce a realistic composite image. In this work, we propose an effective finetuning strategy for generative image composition model, in which we finetune a pretrained model using one or more images containing the same foreground object. Moreover, we propose a multi-reference strategy, which allows the model to take in multiple reference images of the foreground object. The experiments on MureCOM dataset verify the effectiveness of our method. The code and model have been released here .","The goal of generative image composition is inserting the foreground object into the background to produce a high-quality composite image. Typically, the model takes in a background image with specified foreground placement and a reference image of foreground object. In the generated image, the foreground is seamlessly blended into the background, with compatible illumination and geometry. The existing generative image composition methods can be roughly divided into high-authenticity methods and high-fidelity methods. As an example high-authenticity method, ObjectStitch [2] can generate realistic composite images, but the foreground details cannot be well preserved for those uncommonly seen objects or the objects with complex details. As an example high-fidelity method, ControlCom [3] designed a local enhancement module [3] to enrich the detail information. Such modification can greatly promote the foreground fidelity, but the model may inappropriately maintain the pose/viewpoint of foreground object, or generate images with distorted content structure and notable artifacts when attempting to adjust the pose/viewpoint. Therefore, based on the performance of previous methods [2, 3], high authenticity and high fidelity can hardly be achieved at the same time. In this work, we propose a finetuning strategy when one or more images containing the same foreground object are available. Following [2, 3], for each image, we segment the foreground object and perform illumination/geometry transformation to make it a reference image. The image after removing the foreground object becomes a background image. Based on pairs of background image and reference image, we can finetune the generative image composition model, in a similar way to [2, 3]. Furthermore, we propose a multi-reference strategy, which allows the model to take in multiple reference images of foreground object. This is natural when we have more than one images containing the target object. Using multiple reference images can enhance the model generation ability for various poses and viewpoints. The combination of finetuning and multi-reference leads to our multi-reference finetuning strategy. We apply multi-reference finetuning to a high-authenticity method ObjectStitch [2] and a high-fidelity method ControlCom [3], with experiments conducted on MureCom [1] dataset. We observe that for the high-authenticity method, the ability of detail preservation is dramatically improved. However, for the high-fidelity method, the ability of pose/viewpoint adjustment is still very weak. Therefore, we opt for applying multi-reference finetuning to high-authenticity method. In particular, we choose ObjectStitch as base network and refer to its extension with multi-reference finetuning strategy as MureObjectStitch. Figure 1: Illustration of ground-truth images, background images, and reference images. Figure 2: Illustration of our MureObjectStitch model."
https://arxiv.org/html/2411.07461v1,BLIP3-KALE: Knowledge Augmented Large-Scale Dense Captions,"We introduce BLIP3-KALE, a dataset of 218 million image-text pairs that bridges the gap between descriptive synthetic captions and factual web-scale alt-text. KALE augments synthetic dense image captions with web-scale alt-text to generate factually grounded image captions. Our two-stage approach leverages large vision-language models and language models to create knowledge-augmented captions, which are then used to train a specialized VLM for scaling up the dataset. We train vision-language models on KALE and demonstrate improvements on vision-language tasks. Our experiments show the utility of KALE for training more capable and knowledgeable multimodal models. We release the KALE dataset at https://huggingface.co/datasets/Salesforce/blip3-kale.","Dataset Scale (# of samples) Density (avg. words/caption) Knowledge-augmented? Captioner size (params) LAION-COCO1 600M 8.99 ✗ 0.5B ReCap-Datacomp-1B [15] 1.28B 49.43 ✗ 7B CapsFusion [28] 120M 22.74 ✓ 0.5B KALE 218M 67.26 ✓ 17B (stage 1) \rightarrow 2B (stage 2) Table 1: Comparison of open-source synthetic image-text datasets: We compare various datasets in terms of scale (number of samples), density (average number of words per sample), whether they are knowledge-augmented (meaning that the caption includes information found in image’s web scraped alt-text), and the size of the captioning model used to generate the descriptions. For KALE, we create an initial pool of 100M captions from a 17B parameter model and use it to distill a 2B parameter model that matches the performance of the larger 17B model. We introduce BLIP3-KALE, a dataset of 218 million image-text pairs that advances the state of knowledge-augmented image captioning. KALE builds upon recent work in this area, particularly CapsFusion [28], which pioneered the use of large language models to fuse synthetically generated captions with alt-text to incorporate real-world knowledge. KALE makes two key contributions beyond CapsFusion: Scale and Density: While CapsFusion produced 120M samples with an average of 22.74 words per caption, KALE is significantly larger and denser. It contains 218M samples with an average of 67.26 words per caption - 1.82x the scale and nearly 3x the density of CapsFusion. Efficient Generation: We distill the knowledge augmentation process into a compact 2B parameter captioning model. This enables generation of high-quality captions comparable to much larger models like CogVLM-17B [25], but at a fraction of the cost. This efficiency allows us to scale up the dataset creation process. Our approach combines synthetic captions from VLMs with factual information from web-scale alt-text, creating rich image descriptions. We demonstrate that training on KALE improves performance across multimodal tasks compared to many previous purely synthetic or web-scraped datasets."
https://arxiv.org/html/2411.07449v1,"Tracing the Roots: Leveraging Temporal Dynamics in 
Diffusion Trajectories for Origin Attribution","Diffusion models have revolutionized image synthesis, garnering significant research interest in recent years. Diffusion is an iterative algorithm in which samples are generated step-by-step, starting from pure noise. This process introduces the notion of diffusion trajectories, i.e., paths from the standard Gaussian distribution to the target image distribution. In this context, we study discriminative algorithms operating on these trajectories. Specifically, given a pre-trained diffusion model, we consider the problem of classifying images as part of the training dataset, generated by the model or originating from an external source. Our approach demonstrates the presence of patterns across steps that can be leveraged for classification. We also conduct ablation studies, which reveal that using higher-order gradient features to characterize the trajectories leads to significant performance gains and more robust algorithms.","Generative modeling has seen major breakthroughs due to advances in Denoising Diffusion Probabilistic Models (DDPMs) (Sohl-Dickstein et al. 2015; Ho, Jain, and Abbeel 2020; Rombach et al. 2022). These models have become the go-to method for image synthesis, achieving state-of-the-art performance (Dhariwal and Nichol 2021; Karras et al. 2024). However, as diffusion models become more prevalent, concerns about privacy and security arise. With no one to hold the systems’ developers responsible, copyrighted or confidential data may be used in the training process and regurgitated during inference (Carlini et al. 2023; Somepalli et al. 2023; Wen et al. 2024; Gu et al. 2024). Moreover, the lack of accountability mechanisms enables the creation and dissemination of harmful or malicious content. Towards more responsible image generation, we develop algorithms to trace the origin of images. In particular, we are interested in whether a given image is part of a model’s member (training) set, belonging set (i.e., sampled from the model) or if it is from an external dataset. Such problems are typically studied as two separate binary classification tasks in the literature. One task, membership inference, predicts whether a given image is part of a model’s member set (Shokri et al. 2017; Matsumoto, Miura, and Yanai 2023; Duan et al. 2023; Wang et al. 2024). The second, Model Attribution (MA), aims to classify images as generated by a particular model (Wang et al. 2023; Laszkiewicz et al. 2024) (i.e., belonging to the model). We introduce the three-class problem of distinguishing member, belonging and external data as Origin Attribution (OA)111Note that Wang et al. (2023) use this term to describe what we call MA. However, we reserve the OA label for our more general setting and adopt MA to describe their setup following Laszkiewicz et al. (2024).. Focusing on the more common Membership Inference Attack (MIA) setting, existing methods typically train additional surrogate, or shadow, models. In doing so, shadow models imitate the target model and can be used to estimate the classification boundary between members and non-members without access to the target’s data. However, a limitation of such attacks is that it may not be feasible to produce these surrogates due to high computational costs and undocumented training processes (e.g., unknown training iterations, dataset size and regularization) (Dubiński et al. 2024). Also, shadow model training implicitly assumes knowledge of the member distribution, potentially leaking information, and fails to address distribution shifts (Das, Zhang, and Tramèr 2024; Dubiński et al. 2024). With these points in mind, we aim to develop practical, surrogate-free, attacks that make minimal assumptions about the data availability and training process of the diffusion model. Our contributions are three-fold: • We simulate realistic attack environments for membership inference and evaluate existing attacks under this setting. Our results reveal two key problems with current approaches. Namely, threshold-based MIAs break down in the presence of belonging data and distribution shifts. • We address the above issues and produce more robust algorithms. Specifically, we tailor our methods to diffusion models and design attacks that leverage their multi-step nature. We present results for membership inference and extend our analysis to MA and OA tasks. • To better capture the multi-step dynamics of diffusion, we consider higher-order gradient features in combination with the commonly used loss features found in the literature. This feature space greatly improves the performance of our MIAs, MA attacks and OA attacks."
https://arxiv.org/html/2411.07445v1,All-in-one Weather-degraded Image Restoration via Adaptive Degradation-aware Self-prompting Model,"Existing approaches for all-in-one weather-degraded image restoration suffer from inefficiencies in leveraging degradation-aware priors, resulting in sub-optimal performance in adapting to different weather conditions. To this end, we develop an adaptive degradation-aware self-prompting model (ADSM) for all-in-one weather-degraded image restoration. Specifically, our model employs the contrastive language-image pre-training model (CLIP) to facilitate the training of our proposed latent prompt generators (LPGs), which represent three types of latent prompts to characterize the degradation type, degradation property and image caption. Moreover, we integrate the acquired degradation-aware prompts into the time embedding of diffusion model to improve degradation perception. Meanwhile, we employ the latent caption prompt to guide the reverse sampling process using the cross-attention mechanism, thereby guiding the accurate image reconstruction. Furthermore, to accelerate the reverse sampling procedure of diffusion model and address the limitations of frequency perception, we introduce a wavelet-oriented noise estimating network (WNE-Net). Extensive experiments conducted on eight publicly available datasets demonstrate the effectiveness of our proposed approach in both task-specific and all-in-one applications.","Restoring images degraded by adverse weather degradations represents significance within the domain of intelligent transportation [1, 2, 3, 4], where the weather degradations can be rain, haze, raindrop, snow, low-lightness, rain-by-snow, rain-by-haze, rain-by-raindrop and other degradations. Existing methods for weather-degraded image restoration can be categorized into task-specific, task-aligned and all-in-one settings. The task-specific methods [5, 6, 7, 8, 9] can only eliminate one type of degradations after the training, necessitating different models and parameters. The task-aligned ones [10, 11, 12, 13] can also remove one type of degradations, using the same model with varying parameters. Both strategies exhibit limitations in adapting to diverse weather conditions. In contrast, all-in-one methods [3, 14] accommodate various weather degradations using a single model with consistent parameters. Figure 1: Intuitive comparisons of the existing all-in-one approaches and our proposed method between the supervised and un-supervised metrical scores of reconstructed images. Our model notably enhances the naturalness and image quality of generated images, surpassing the capabilities of existing methods in both aspects. Figure 2: Overview of our proposed adaptive degradation-aware self-prompting model (ADSM) for all-in-one weather-degraded image restoration. Initially, we train three latent prompt generators (LPGs) to create the corresponding prompts for degradation type, degradation property and image caption. We then integrate these latent prompts into the diffusion model to perform iterative prompt learning, facilitating the weather degradation elimination and image content reconstruction. Recently, there has been a growing interest in integrating the prompt learning methodologies to improve the perceptual capabilities of image restoration models for addressing specific weather degradations. For instance, MAE-VQGAN [15] strategically utilizes the grid-like configurations during the inference phase to perform effective inpainting within regions lacking visual content. Similarly, Painter [16] generates desired images based on specified paired instance images. Given the nature of conditional generation, solutions that rely on discrete prompt engineering [17, 18, 19] are sensitive to the quality of given fixed prompts. To address this limitation, ProRes [20] introduces the trainable degradation-aware visual prompts designed to model specific types of weather degradations. However, it also relies heavily on the high-quality learnable visual prompts and lacks the information related to specific degradation and image content, resulting in limited effectiveness for processing complex weather-degraded images. Therefore, there is a need for prompt learning approachees that integrates additional prompts to characterize both the degradation specifics and contextual details. In this work, we propose an effective adaptive degradation-aware self-prompting model (ADSM) for all-in-one weather-degraded image restoration (as shown in Figure 2), which enhances the naturalness of the reconstructed images while achieving the improved supervised metrical scores. Our approach leverages the extensive vision-language priors in CLIP model [21, 22], enabling us to generate latent prompts specifically tailored to the weather-degraded images being processed. To align the latent features of weather-degraded images with their corresponding ground truths, we train three latent prompt generators (LPGs), which are designed to produce the latent prompts of degradation type, degradation property and image caption, respectively. By using the pre-trained and learnable image encoders in CLIP, our LPGs align the generated latent prompts with the output latent features from the text encoder in CLIP space. LPGs can also help solve the issue where texts with similar meanings may appear significantly distant in CLIP space [23]. In our proposed LPGs, the degradation prompts for type and property identify the potential features of both individual and combined weather conditions, whereas the existing similar method [22] fails to address combined weather degradations and to classify the specific degradation properties. Additionally, the caption prompt is essential for verifying the clarity of content in reconstructed images. After setting up the latent prompt generators, we utilize the image-conditioned diffusion model [24, 25] as the core restoration framework. Here, we combine the prompts indicating degradation type and property, which are then incorporated into the time embedding, improving the degradation perception. Meanwhile, we introduce the cross-attention mechanism to blend the latent caption prompt into the diffusion model, guiding the reconstruction of image content. To improve the computational efficiency in sampling generated images and enhance the accuracy of noise estimation, we devise a wavelet-oriented noise estimating network (WNE-Net). The traditional residual block in diffusion model [24] struggles to fully capture the crucial global information needed for effective noise perception [26]. In our WNE-Net, we introduce a wavelet self-attention representation block (WSRB) designed to effectively separate out different frequency components and capture a deep understanding of long-range relationships. Meanwhile, the inherent down-sampling feature of discrete wavelet transform (DWT) [27] reduces the computational burden. In addition, we utilize the frequency partitioning in cross-stage sampling operations and introduce a wavelet feature sampling block (WFSB) that includes both down-sampling and up-sampling operators. WFSB utilizes two branches to capture the sampling features that are sensitive to frequency, refining these characteristics by identifying the specific degradation properties. Our WNE-Net brings two key advancements. Firstly, it allows our model to better distinguish the inherent frequency characteristics of input signals. Secondly, it utilizes wavelet transform to adjust the feature resolutions, significantly reducing the computational load and accelerating the reverse sampling process. Figure 2 illustrates the overview of our proposed model. We summarize the main contributions as follows. • We achieve all-in-one weather-degraded image restoration via an adaptive degradation-aware self-prompting model, which is prompted by the latent prompts of degradation type, degradation property and image caption. • We utilize the latent degradation prompts of type and property to improve the understanding of weather conditions. Meanwhile, we employ the cross-attention mechanism to integrate the caption prompt into diffusion model, enhancing the accuracy of image reconstruction. • We develop a wavelet-oriented noise estimating network to leverage the frequency separation principles for efficient and accurate noise estimation."
https://arxiv.org/html/2411.07430v1,XPoint: A Self-Supervised Visual-State-Space based Architecture for Multispectral Image Registration,"Accurate multispectral image matching presents significant challenges due to non-linear intensity variations across spectral modalities, extreme viewpoint changes, and the scarcity of labeled datasets. Current state-of-the-art methods are typically specialized for a single spectral difference, such as visible-infrared, and struggle to adapt to other modalities due to their reliance on expensive supervision, such as depth maps or camera poses. To address the need for rapid adaptation across modalities, we introduce XPoint, a self-supervised, modular image-matching framework designed for adaptive training and fine-tuning on aligned multispectral datasets, allowing users to customize key components based on their specific tasks. XPoint employs modularity and self-supervision to allow for the adjustment of elements such as the base detector, which generates pseudo-ground truth keypoints invariant to viewpoint and spectrum variations. The framework integrates a VMamba encoder, pre-trained on segmentation tasks, for robust feature extraction, and includes three joint decoder heads: two are dedicated to interest point and descriptor extraction; and a task-specific homography regression head imposes geometric constraints for superior performance in tasks like image registration. This flexible architecture enables quick adaptation to a wide range of modalities, demonstrated by training on Optical-Thermal data and fine-tuning on settings such as visual-near infrared(0.75–1.4 \mum), visual-infrared(3-8 \mum), visual-longwave infrared(0.8–15 \mum), and visual-synthetic aperture radar. Experimental results show that XPoint consistently outperforms or matches state-of-the-art methods in feature matching and image registration tasks across five distinct multispectral datasets. Our source code is available at https://github.com/canyagmur/XPoint.","Image matching [1] identifies and correlates similar structures across multiple images, often by geometrically aligning a moving image with a fixed reference image in a process known as image registration. This technique plays a crucial role in applications such as remote sensing [2], visual localization [3], homography estimation [4], and structure from motion (SfM) [5]. Multimodal image matching (MMIM) extends this concept by integrating data from different imaging modalities, allowing systems to interpret complex environments more effectively. This integration combines images captured by various sensors or under diverse conditions (e.g., different times of day [6], weather [7], or seasons [8]). MMIM also merges data types, such as combining images with textual information [9], capitalizing each modality’s unique strengths to provide a richer, more comprehensive understanding of the scene [10]. Figure 1: Architecture of XPoint. (A) Self-supervision stage uses improved multispectral homographic adaptation with spectrum-aware keypoint acceptance and the RIFT2 detector to create multispectral pseudo-ground truth keypoints. (B) Training stage combines pretrained VMamba encoders with SS2D for enhanced feature extraction, incorporating interest point and descriptor decoders and a homography regression head for improved matching and homography estimation. (C) In inference stage, shared encoders extract features from multispectral pairs, enabling joint keypoint detection, descriptor extraction, and robust outlier removal for accurate correspondences and homography estimation. State-of-the-art image matching methods include hand-crafted and learning-based techniques. Hand-crafted methods like SIFT [11] and SURF [12] detect keypoints robust to scale, rotation, and illumination changes but require tuning, are computationally heavy, and struggle with extreme viewpoint and texture variations in multimodal scenarios. The advent of deep learning has led to substantial improvements in addressing these challenges, particularly within the visible spectrum, through learning-based approaches [13, 14, 15, 16, 17]. Despite these advancements, many learning-based methods encounter difficulties in multimodal settings due to non-linear intensity variations across spectral pairs. To address this, various hand-crafted [18, 19, 20] and learning-based [21, 22, 23] multispectral image matching frameworks have emerged. While these methods show promise on specific benchmarks, they typically target single-modality scenarios and often struggle to generalize across diverse multimodal tasks. For instance, models like RoMa [24] that utilize foundational models to enhance generalization still experience issues with overfitting and remain computationally expensive. This paper introduces XPoint, a modular architecture that enables adaptive training and fine-tuning on new datasets through self-supervision, achieving high performance in multimodal image matching where labeled data is limited. XPoint’s modularity allows rapid adaptability to novel tasks by using aligned image pairs, providing a scalable, self-supervised solution for diverse datasets. Existing methods such as detector-free frameworks [15, 16, 24, 25] often rely on labeled data (e.g., camera poses, depth maps) unavailable in multimodal contexts, limiting their generalization. XPoint overcomes this by adopting SuperPoint’s self-supervised approach [13] to generate pseudo-ground truth keypoints from aligned pairs, bypassing costly labeling and enhancing cross-modal generalization. Aligned image pairs are considerably easier to obtain than depth maps or precise camera poses, particularly in multimodal settings where additional annotations are scarce or difficult to collect. This self-supervised strategy reduces data collection requirements and enhances generalizability across various modalities. In contrast to similar work, such as ReDFeat [23], a self-supervised geometry-informed network that struggles with large viewpoint changes, XPoint effectively manages both viewpoint and spectrum variations. To handle viewpoint variation and enable self-supervision, SuperPoint [13] uses Homographic Adaptation to create pseudo-ground truth keypoints by simulating different viewpoints of the same scene, allowing a fully convolutional network to recognize scenes from multiple angles. Building on this, MultiPoint [21] adapts Homographic Adaptation for multispectral alignment, combining keypoints from both spectral domains into a unified superset. A key challenge is forming a common feature set across multispectral pairs, which MultiPoint addresses by merging keypoint maps through pixel-wise multiplication and Gaussian smoothing, though some keypoints may be missed. Our previous work, the windowing technique [26] refines this approach by allowing cross-spectral keypoints within a defined window around corresponding points in other spectra, allowing small localization errors and enhancing keypoint superset generation in MultiPoint. This work adopts a self-supervised approach, enhancing pseudo-ground truth keypoint generation with an improved multispectral homographic adaptation stage and introducing a modular architecture with a pre-trained encoder and three joint heads: two dedicated to keypoint detection and description, and a task-specific head—homography regression. The main contributions are summarized as follows (see Fig. 1 for the complete framework): Multispectral Homographic Adaptation: A spectrum-aware keypoint acceptance criteria introduces a proposed windowing technique using the RIFT2 multispectral keypoint extractor [27]. This component generates viewpoint-invariant pseudo-ground truth keypoint supersets across spectra. Pretrained VMamba Encoders: Incorporating 2D-Selective-Scan (SS2D) mechanisms, VMamba encoders provide enhanced semantic awareness, extracting features more effectively than CNNs and more efficiently than visual transformers. Homography Regression Head: A dedicated task-specific head in the decoder refines keypoint detection and description by guiding homography estimation. This constraint improves matching accuracy, enhances metric performance, and enables flexible model adaptation. Revised Detector Loss: To address imbalances in challenging datasets like VIS-SAR and VIS-NIR [23], a weighted cross-entropy loss is used, boosting confidence and performance under significant spectral differences. Modular XPoint Framework: Adaptable to various modalities, XPoint achieves state-of-the-art results across four multimodal settings (VIS-NIR, VIS-IR, VIS-LWIR, VIS-SAR) on five datasets [21, 28, 23]. Using a self-supervised approach with aligned image pairs, XPoint reduces data requirements, enhancing generalization across spectrum shifts and viewpoint changes."
https://arxiv.org/html/2411.07392v1,Feature-Space Semantic Invariance: Enhanced OOD Detection for Open-Set Domain Generalization,"Open-set domain generalization addresses a real-world challenge: training a model to generalize across unseen domains (domain generalization) while also detecting samples from unknown classes not encountered during training (open-set recognition). However, most existing approaches tackle these issues separately, limiting their practical applicability. To overcome this limitation, we propose a unified framework for open-set domain generalization by introducing Feature-space Semantic Invariance (FSI). FSI maintains semantic consistency across different domains within the feature space, enabling more accurate detection of OOD instances in unseen domains. Additionally, we adapt a generative model to produce synthetic data with novel domain styles or class labels, enhancing model robustness. Initial experiments show that our method improves AUROC by 9.1% to 18.9% on ColoredMNIST, while also significantly increasing in-distribution classification accuracy.","Open-set domain generalization addresses the dual challenge of domain generalization (DG) and open-set recognition (OSR) by aiming to classify in-distribution (ID) instances under domain shifts while simultaneously detecting samples from unknown classes in the unseen target domain. Despite this setting better reflecting real-world scenarios, current methods often treat it as two separate problems due to its complexities. For instance, OOD samples with styles similar to IDs are difficult to detect, while ID samples in a shifted target domain are prone to being misclassified as OOD. Research in open-set domain generalization is relatively sparse, with only a few works [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] making progress compared to conventional approaches that handle either DG or OSR in isolation. However, these existing methods typically rely heavily on OOD training data [11], which is rarely available in training, or utilize meta-learning strategies and complex network architectures [12, 13, 14] that lack compatibility with state-of-the-art OOD detection techniques, such as Energy [15] and DDU [16]. To address these limitations, we propose an advanced technique that enforces feature-space semantic invariance to learn high-quality domain-invariant features, and leverages synthetic OOD data to increase the separability between ID and OOD. Our main contributions are as follows: • We introduce Feature-space Semantic Invariance (FSI) to enforce semantic consistency across domains within the feature space. By aligning semantic features across augmented samples, FSI enables the model to learn high-quality domain-invariant features, enhancing its generalizability to unseen domains. • We incorporate synthetic OODs generated from ID samples to establish clearer decision boundaries between ID and OOD instances, significantly enhancing the model’s robustness against OOD. • Preliminary experiments show our approach improves AUROC by 9.1% to 18.9% on the ColoredMNIST dataset, with a notable increase in ID classification accuracy. These results validate the model’s potential in open-set domain generalization, positioning it as a viable solution for applications with novel domains and classes. Figure 1: An illustration of the proposed framework. The framework samples instances from different domains, each characterized by unique variations (e.g., colors), aiming to learn a domain-invariant feature extractor that can be combined with state-of-the-art semantic OOD detectors to effectively address both domain generalization and open-set recognition challenges."
https://arxiv.org/html/2411.07351v1,Generalization of Brady-Yong Algorithm for Fast Hough Transform to Arbitrary Image Size,"Nowadays, the Hough (discrete Radon) transform (HT/DRT) has proved to be an extremely powerful and widespread tool harnessed in a number of application areas, ranging from general image processing to X-ray computed tomography. Efficient utilization of the HT to solve applied problems demands its acceleration and increased accuracy. Along with this, most fast algorithms for computing the HT, especially the pioneering Brady-Yong algorithm, operate on power-of-two size input images and are not adapted for arbitrary size images. This paper presents a new algorithm for calculating the HT for images of arbitrary size. It generalizes the Brady-Yong algorithm from which it inherits the optimal computational complexity. Moreover, the algorithm allows to compute the HT with considerably higher accuracy compared to the existing algorithm. Herewith, the paper provides a theoretical analysis of the computational complexity and accuracy of the proposed algorithm. The conclusions of the performed experiments conform with the theoretical results.","The Hough transform (HT) is a widely used tool in image processing and machine vision. It is usually understood as a method for robust estimation of the parameters of one or more lines in a discrete image by calculating the number of points lying on each line of a set of discrete lines. The method is named after Paul Hough, who first proposed it in 1959 as a way to identify straight-line tracks in a bubble chamber [1]. The core idea of the HT is to count “votes” along discrete, parameterized lines, and accumulate a value for each of them. The higher this value is, the greater the probability that the line with corresponding parameter values is present in the image. Even though the best-known application of the HT is to find contrasting straight lines or their segments in an image [2, 3], over the years, the HT has found numerous applications in a variety of fields, including image binarization [4], segmentation [5], computed tomography [6], etc. The HT requires fast algorithms for its calculation. In 1992, M. Brady and W. Yong published a paper on the fast HT/DRT [7] for images of size n\times n,\,n=2^{q},\,q\in\mathbb{N}. The method they proposed is based on dynamic programming, which is used to skip the repeated calculation of the sums of already processed segments when computing the sum along the line they constitute. This allows for the calculation of the HT in \Theta(n^{2}\log_{2}n) summations. This method has become the de facto standard for practical applications of the Hough transform, as it offers a computationally efficient approach while maintaining the core functionality of the original method. This allowed the HT to be used in systems that have strict hardware constraints with a requirement to perform in real-time [8, 9]. In this paper, we propose a novel fast HT computation algorithm generalizing the Brady-Yong algorithm to arbitrary image size. The proposed algorithm, described in section 3, has the same complexity as the Brady-Yong algorithm while being more accurate than the previously known algorithm of the same kind [10]. In section 4, we analyze the computational complexity and accuracy of the proposed algorithm, which align with the experimentally derived estimates outlined in section 5 of the paper."
https://arxiv.org/html/2411.07326v1,"SE(3) Equivariant Ray Embeddings for 
Implicit Multi-View Depth Estimation","Incorporating inductive bias by embedding geometric entities (such as rays) as input has proven successful in multi-view learning. However, the methods adopting this technique typically lack equivariance, which is crucial for effective 3D learning. Equivariance serves as a valuable inductive prior, aiding in the generation of robust multi-view features for 3D scene understanding. In this paper, we explore the application of equivariant multi-view learning to depth estimation, not only recognizing its significance for computer vision and robotics but also addressing the limitations of previous research. Most prior studies have either overlooked equivariance in this setting or achieved only approximate equivariance through data augmentation, which often leads to inconsistencies across different reference frames. To address this issue, we propose to embed SE(3) equivariance into the Perceiver IO architecture. We employ Spherical Harmonics for positional encoding to ensure 3D rotation equivariance, and develop a specialized equivariant encoder and decoder within the Perceiver IO architecture. To validate our model, we applied it to the task of stereo depth estimation, achieving state of the art results on real-world datasets without explicit geometric constraints or extensive data augmentation.","Equivariance is a valuable property in computer vision, leveraging various symmetries to reduce sample and model complexity while boosting generalization. It has seen broad application in fields such as 3D shape analysis [48, 52, 13], panoramic image prediction [10, 54, 19], and robotics [46, 25, 42, 2]. In particular, there is an increasing interest in equivariant scene representation from multiple viewpoints [43, 55], as the multi-view setting is a fundamental challenge in the field and equivariant representations are desirable for their robustness and efficiency. Meanwhile, multi-view depth estimation has always been a core topic in computer vision. Previous works [27, 31, 26] leverage the explicit geometric constraint to construct the feature cost-volume for depth prediction. Recently, the paradigm of combining implicit representations with generalist architectures has been widely adopted and gaining success. Inserting inductive bias via the embedding of geometric entities (rays) in the multi-view setting [58, 47, 44] has become popular. Notably, in multi-view depth estimation, Yifan et al. [57] effectively combined geometric epipolar embeddings with image features for stereo depth estimation, outperforming traditional methods that depend on explicit geometric constraints. State-of-the-art work by [23] integrated multi-view geometric embeddings with image features for video depth estimation. These methods show that the implicit multi-view geometry learned by the Perceiver IO architecture, which is a more efficient general architecture compared to the vision transformer [14], can improve upon approaches that rely on traditional explicit geometric constraints, such as cost volumes, bundle adjustment, and projective geometry. However, the implicit multi-view geometry promoted by the Perceiver IO architecture lacks equivariance. This limitation becomes apparent when transforming the coordinate frame representing input geometry, such as camera poses, ray directions, or 3D coordinates. These transformations change the input in such a way that non-equivariant architectures are unable to achieve the same results, as shown in Figure 1. Figure 1: Given a sparse set of posed images (red), the task is to estimate depth for a novel viewpoint (blue). The Perceiver IO struggles to accurately predict depth when the reference frame (gray) changes, equivalent to an inverse transformation applied to the object and cameras. In contrast, our model delivers the consistent result due to its equivariant design. Although [23] have tried to approximate equivariance through extensive data augmentation, achieving true equivariance at an architectural level remains an ongoing challenge. In this paper, we propose to embed equivariance with respect to SE(3) transformation of the global coordinate frame, i.e., gauge equivariance, to the Perceiver IO model. We substitute traditional Fourier positional encodings for the ray embedding with Spherical Harmonics, which are more suitable to represent 3D rotations. We custom-develop a SE(3) equivariant attention module to seamlessly interact with different types of equivariant features. This is achieved using a combination of invariant attention weights and equivariant fundamental layers. During the decoding stage, this equivariant latent space is disentangled into the equivariant frame and invariant global features. Our approach not only simplifies the integration of existing modules without requiring a specialized design, but also allows the network to focus on effective scene analysis via an invariant latent space, reducing the effects of global transformations. The equivariant frame is used to “standardize” the query ray, serving as an invariant input for the decoder. This method ensures that both sets of inputs for the decoder are invariant, leading to an invariant output regardless of the decoder used. Consequently, we can employ the conventional Perceiver IO decoder in our equivariant framework. In summary, our key contributions are as follows: • We integrate SE(3) equivariance into a multi-view depth estimation model by design, using spherical harmonics as positional encodings for ray embeddings, as well as a specialized equivariant encoder. • By leveraging the equivariant learned latent space, we introduce a novel scene representation scheme for multi-view settings, featuring a disentangled equivariance frame and an invariant scene representation. • We assess our model’s ability to learn 3D structures through wide-baseline stereo depth estimation. Our model delivers state-of-the-art results in this task, significantly surpassing the non-equivariant baseline. Figure 2: Our proposed Equivariant Perceiver IO (EPIO) architecture. (a) We take as input the concatenation of per-pixel image, ray, and camera embeddings, the latter two calculated using spherical harmonics. (b) The output of our equivariant encoder is a global latent code, including both global invariant and equivariant components. From those, we extract an equivariant reference frame through an equivariant MLP, while simultaneously obtaining invariant latents through inner product. (c) When a query camera is positioned in this equivariant reference frame, its pose becomes invariant, which enables the use of conventional Fourier basis to encode it. (d) Given an invariant latent and invariant pose, we use a conventional Perceiver IO decoder to generate predictions for each query ray."
https://arxiv.org/html/2411.07311v1,GPU-Accelerated Inverse Lithography Towards High Quality Curvy Mask Generation,"Inverse Lithography Technology (ILT) has emerged as a promising solution for photo mask design and optimization. Relying on multi-beam mask writers, ILT enables the creation of free-form curvilinear mask shapes that enhance printed wafer image quality and process window. However, a major challenge in implementing curvilinear ILT for large-scale production is mask rule checking, an area currently under development by foundries and EDA vendors. Although recent research has incorporated mask complexity into the optimization process, much of it focuses on reducing e-beam shots, which does not align with the goals of curvilinear ILT. In this paper, we introduce a GPU-accelerated ILT algorithm that improves not only contour quality and process window but also the precision of curvilinear mask shapes. Our experiments on open benchmarks demonstrate a significant advantage of our algorithm over leading academic ILT engines. Source code will be available at https://github.com/phdyang007/curvyILT.","Lithography plays a crucial role in semiconductor manufacturing. However, a mismatch between lithography technology and the critical dimensions of chips leads to the optical proximity effect, posing challenges to technological advancement. To mitigate this issue, chip design photomasks must be optimized to correct for lithography proximity distortion, a process known as mask optimization. Optical proximity correction (OPC) is the most widely used approach for mask optimization (OPC-DATE2015-Kuang, ; OPC-JM3-2016-Matsunawa, ; MEEF-TSM2000-Wong, ). It involves dividing the edges of chip design polygons into segments, which are then adjusted using heuristic rules to counteract optical proximity effects. However, as chip feature sizes continue to shrink, the limited robustness of heuristic optimization demands extensive engineering effort, jeopardizing design turnaround time and production yield. Inverse lithography technologies (ILT), with their gradient-based free-form optimization, provide a broader solution space that can effectively address critical patterns where traditional OPC falls short. Despite this advantage, ILT has long faced a dilemma: while free-form optimization leads to better convergence, it also presents a significant manufacturing challenge, as mask shops struggle to produce these complex free-form masks efficiently. A common workaround is to approximate the ILT-generated mask with rectangles, aligning the ILT output with OPC shape rules. However, this approach sacrifices some of the optimality that ILT offers. Recently, a multi-beam mask writer was introduced for advanced lithography mask manufacturing (mbmw, ). This innovation maintains a consistent mask production time, enabling the direct fabrication of freeform or curvilinear masks. Though curvilinear ILT faces mask writing challenges to enforce clearance of mask rule check (curvyMRC, ), it is now feasible to apply curvilinear inverse lithography technology (ILT) across a significant portion of chip design layers, leading to improved quality of results (QoR). The comparison among OPC, ILT, and Curvy ILT are listed in Table 1, and the development of Curvy ILT is our focus. Table 1. Mask optimization solution. Our efforts focus on the direct generation of curvy ILT, with a specifically designed algorithm for better curvature and reduced mask artifacts. Solution OPC ILT Curvy ILT Mask Writer Variable Shaped Beam Variable Shaped Beam Multibeam Mask Rule Manhattan Geometry Constraints Manhattan Geometry Constraints Width, Area, Curvature Efficiency Fast Slow Slow Solution Space Small Medium Large Optimizer Heurestic Gradient Gradient Example ILT has garnered significant attention in academic research due to its promising advantages. Much of this research has centered on enhancing algorithmic efficiency and optimizing the quality of the final simulated wafer. For example, Wang et al. (OPC-DAC2022-Wang, ) developed A2-ILT, introducing a spatial attention layer to regulate mask gradients, which however fails the growth of sub-resolution assist features (SRAFs)—a critical aspect for process window optimization. Additionally, Yu et al. (OPC-DATE2021-Yu, ) proposed using a level-set function to model the mask, improving smoothness during ILT procedures. More recently, an efficient ILT implementation was presented by Sun et al. (OPC-DAC2023-Sun, ), which has become the state-of-the-art by utilizing multi-level lithography simulations at different resolutions to achieve faster convergence. A similar implementation is also introduced in OpenILT (openilt, ). However, these efforts primarily address the mask manufacturing challenges associated with VSB technology and are not directly applicable to curvilinear mask optimization. For instance, there has been little focus on eliminating isolated artifacts that breach shape area constraints, and the balance between quality of results (QoR) and mask smoothness has not been adequately managed (OPC-DAC2023-Sun, ; OPC-DATE2021-Yu, ). To overcome the limitations of previous work and encourage further research into curvilinear ILT solutions, we introduce a new GPU-accelerated ILT algorithm that: 1) improves upon existing algorithms to achieve better optimality, and 2) addresses the challenges of curvilinear mask writing using differentiable morphological operators. Our major contributions include: • We thoroughly analyze the limitations of existing academic ILT algorithms and have developed a new algorithm that improves optimization convergence and enhances mask quality. • We develope the idea of curvilinear design retargeting to allow ILT solvers to optimize toward corner-smoothed targets leading to faster and better convergence. • We introduce a differentiable morphological operator that can be seamlessly integrated into legacy ILT algorithms to control mask curvature and shape without compromising the final quality of results (QoR). • We conduct experiments on layers from both real-world and synthetic designs, demonstrating the superior performance of our algorithm. Reminder of the manuscript is organized as follows: Section 2 introduces related works and fundamental terminologies associated with mask optimization and ILT.; Section 3 provides a detailed description of the proposed ILT algorithm.; Section 4 presents a comprehensive analysis of the experimental results for our algorithm across various design layers; and Section 5 discusses future work and concludes the paper."
https://arxiv.org/html/2411.08027v1,LLMPhy: Complex Physical Reasoning Using Large Language Models and World Models,"Physical reasoning is an important skill needed for robotic agents when operating in the real world. However, solving such reasoning problems often involves hypothesizing and reflecting over complex multi-body interactions under the effect of a multitude of physical forces and thus learning all such interactions poses a significant hurdle for state-of-the-art machine learning frameworks, including large language models (LLMs). To study this problem, we propose a new physical reasoning task and a dataset, dubbed TraySim. Our task involves predicting the dynamics of several objects on a tray that is given an external impact – the domino effect of the ensued object interactions and their dynamics thus offering a challenging yet controlled setup, with the goal of reasoning being to infer the stability of the objects after the impact. To solve this complex physical reasoning task, we present \operatorname{LLMPhy}, a zero-shot black-box optimization framework that leverages the physics knowledge and program synthesis abilities of LLMs, and synergizes these abilities with the world models built into modern physics engines. Specifically, \operatorname{LLMPhy} uses an LLM to generate code to iteratively estimate the physical hyperparameters of the system (friction, damping, layout, etc.) via an implicit analysis-by-synthesis approach using a (non-differentiable) simulator in the loop and uses the inferred parameters to imagine the dynamics of the scene towards solving the reasoning task. To show the effectiveness of \operatorname{LLMPhy}, we present experiments on our TraySim dataset to predict the steady-state poses of the objects. Our results show that the combination of the LLM and the physics engine leads to state-of-the-art zero-shot physical reasoning performance, while demonstrating superior convergence against standard black-box optimization methods and better estimation of the physical parameters. Further, we show that \operatorname{LLMPhy} is capable of solving both continuous and discrete black-box optimization problems.","Many recent Large Language models (LLMs) appear to demonstrate the capacity to effectively capture knowledge from vast amounts of multimodal training data and their generative capabilities allow humans to naturally interact with them towards extracting this knowledge for solving challenging real-world problems. This powerful paradigm of LLM-powered problem solving has manifested in a dramatic shift in the manner of scientific pursuit towards modeling research problems attuned to a form that can leverage this condensed knowledge of the LLMs. A few notable such efforts include, but not limited to the use of LLMs for robotic planning (Song et al., 2023; Kim et al., 2024), complex code generation (Tang et al., 2024; Jin et al., 2023), solving optimization problems (Yang et al., 2024; Hao et al., 2024), conduct sophisticated mathematical reasoning (Trinh et al., 2024; Cherian et al., 2024), or even making scientific discoveries (Romera-Paredes et al., 2024). Figure 1: Frames from an example dynamical sequence in our TraySim dataset. The left-most frame shows the first frame of the scene with many objects on the tray and is going to be impacted by a black pusher (right-bottom). The subsequent frames show the state of the system at the 25-th, 50-th, and the 200-th time step (each step is 0.01s). Our task is for a model to reason through the dynamics of the system and predict the stability of each object on the tray at the end of the episode in a zero-shot manner, when provided as input only the first frame of the sequence. While current LLMs seem to possess the knowledge of the physical world and may be able to provide a plan for solving a physical reasoning task (Singh et al., 2023; Kim et al., 2024) when crafted in a suitable multimodal format (prompt), their inability to interact with the real world or measure unobservable attributes of the world model, hinders their capability in solving complex physical reasoning problems (Wang et al., 2023; Bakhtin et al., 2019; Riochet et al., 2021; Harter et al., 2020; Xue et al., 2021). Consider for example the scene in Figure 1. Suppose a reasoning model is provided as input only the first image (left-most) and is asked to answer the question: Which of the objects will remain upright when the tray is impacted by the black pusher with a velocity of 4.8 m/s?. To answer this question, the model must know the various physical attributes of the system, including the masses of the objects, friction coefficients between the contacts and their respective contact forces, stiffness of the objects, among others. While, a large sophisticated machine learning model (such as an LLM) may be able to provide an educated guess based on the intuitive physics of the system, a useful solution would demand a more intricate reasoning strategy in estimating the real-world physics and dynamics; such complex dynamics may be difficult or even impossible to be correctly learned solely from training data. Conversely, advancements in graphics hardware and software have led to the development of advanced physics engines capable of simulating realistic world models Makoviychuk et al. (2021); Bear et al. (2021); Todorov et al. (2012); Gan et al. (2020). Thus, rather than having the LLM to learn the world physics, our key idea is to consider using a physics engine in tandem with the LLM, where the LLM may use its world knowledge for generating scene-based reasoning hypotheses while the simulator is used to verify them within the physical world model. To study this problem, we consider the novel task of predicting the dynamics of objects and their stability under the influence of an impact – an important problem for a variety of robotic applications (Gasparetto et al., 2015; Ahmed et al., 2020). In this paper, we consider this problem in a challenging setting using our new dataset, TraySim, in which the impact is caused by a pusher colliding to a tray that holds several objects of varied sizes, masses, and centers of gravity, with the goal of predicting the dynamics of each of the object instances. We cast this task as that of answering physical reasoning questions. Specifically, as illustrated in Figure 1, TraySim includes simulated video sequences consisting of a tray with an arbitrary number of objects on it and given the first video frame of a given scene, the task of the reasoning model is to infer which of the objects on the tray will remain upright after the impact when the system has stabilized. As is clear from Figure 1, solving this task will require the model to derive details regarding the physical properties of each of the objects and their contacts, as well as have the ability to imagine the system’s dynamics through multi-body interactions influenced by the various internal and external forces from the impact. Our task presents a challenging reasoning setup for current machine learning models, including LLMs. To solve this task, we propose \operatorname{LLMPhy}, a black-box optimization setup combining an LLM with a physics engine that leverages the program synthesis abilities of the LLM to communicate with the engine for solving our task. \operatorname{LLMPhy} operates in two phases: i) a parameter estimation phase, where \operatorname{LLMPhy} is used as a continuous black-box optimization module towards inferring the physical parameters of the objects, including the friction, stiffness, damping, etc. from a given example video sequence, and ii) a scene understanding phase, where the LLM-simulator combination is used as a discrete black-box optimizer to reconstruct the problem layout for synthesizing the setup within the simulator for execution. Our framework builds a feedback loop between the LLM and the physics engine, where the LLM generates programs using its estimates of physical attributes; the programs are executed in the simulator, and the error from the simulations are fed back to the LLM as prompts to refine its estimates until a suitable convergence criteria is met. Note that we do not assume any differentiablity properties of the simulator, which makes our setup highly general. This allows the approach to function as a black-box optimization framework, enabling its use with a wide range of simulators without the need for gradient-based methods. While we may generate unlimited data using our simulation program, given the zero-shot nature of our setup, we synthesized 100 sequences in our TraySim dataset to demonstrate the effectiveness of \operatorname{LLMPhy}. Each sample in TraySim has two video sequences: i) the task sequence of which only the first frame is given to a reasoning agent, and ii) a parameter-estimation video sequence which has a lesser number of instances of each of the object types appearing in the task sequence; the latter sequence has an entirely different layout and dynamics of objects. To objectively evaluate the performance, we cast the task as a physical question answering problem, where the LLM is required to select the correct subset of answers from the given candidate answers. Our results on TraySim show that \operatorname{LLMPhy} leads to promising improvements in performance against alternatives, including using Bayesian optimization, CMA-ES, and solely using an LLM for physical reasoning. Interestingly, we also find that \operatorname{LLMPhy} estimates the physical parameters better and powerful recent LLMs – such as OpenAI o1-preview – show trends of superior optimization convergence. Before moving forward, we summarize below our main contributions: • We consider the novel task of reasoning over complex physics of a highly dynamical system by combining LLMs with non-differentiable physics engines. • We propose a zero-shot reasoning framework \operatorname{LLMPhy}, that combines the reasoning and program synthesis abilities of an LLM with the realistic simulation abilities of a physics engine. This approach is used to estimate the physical parameters of the model, the scene layout, and synthesizing the dynamical scene for inferring the solution. • We introduce a novel synthetic multi-view dataset: TraySim, to study this task. The dataset consists of 100 scenes for zero-shot evaluation. • Our experiments using \operatorname{LLMPhy} on the TraySim dataset demonstrate promising results against related baselines, highlighting its potential for tackling complex physics-based reasoning tasks that involves both discrete and continuous optimization sub-tasks."
https://arxiv.org/html/2411.07976v2,DINO-LG: A Task-Specific DINO Model for Coronary Calcium Scoring,"Coronary artery disease (CAD), one of the most common cause of mortality in the world. Coronary artery calcium (CAC) scoring using computed tomography (CT) is key for risk assessment to prevent coronary disease. Previous studies on risk assessment and calcification detection in CT scans primarily use approaches based on UNET architecture, frequently implemented on pre-built models. However, these models are limited by the availability of annotated CT scans containing CAC and suffering from imbalanced dataset, decreasing performance of CAC segmentation and scoring. In this study, we extend this approach by incorporating the self-supervised learning (SSL) technique of DINO (self-distillation with no labels) to eliminate limitations of scarce annotated data in CT scans. The DINO model’s ability to train without requiring CAC area annotations enhances its robustness in generating distinct features. The DINO model is trained on to focus specifically on calcified areas by using labels, aiming to generate features that effectively capture and highlight key characteristics. The label-guided DINO (DINO-LG) enhances classification by distinguishing CT slices that contain calcification from those that do not, performing 57% better than the standard DINO model in this task. CAC scoring and segmentation tasks are performed by a basic U-NET architecture, fed specifically with CT slices containing calcified areas as identified by the DINO-LG model. This targeted identification performed by DINO-LG model improves CAC segmentation performance by approximately 10% and significant increase in CAC scoring accuracy.","Cardiovascular disease (CVD) is the leading cause of death globally, responsible for approximately 17.9 million fatalities in 2019, which constitutes 32% of all deaths worldwide Organization (2021). Coronary artery disease (CAD), a major cardiovascular disease affecting the blood vessels that feed the heart muscle, caused 371,506 deaths in the United States in 2022 for Health Statistics (2024). According to the most recent heart disease and stroke statistics report, approximately 5% of adults over the age of 20 have CAD in the United States Tsao et al. (2023). Early detection of CAD allows for timely interventions that can prevent progression of the disease and reduce the risk of life-threatening heart attacks. It can also lead to a wider range of treatment options, including lifestyle changes, medications or surgical procedures. The earlier CAD is detected, the more effective these treatments can be. Furthermore, overall treatment costs can be reduced by preventing more serious complications requiring intensive care or extensive procedures. Coronary artery calcium (CAC) scoring is considered a reliable tool for assessing cardiovascular disease tool and is generally recommended for use by various guidelines Knuuti et al. (2020). CAC scoring helps identify the presence and extent of calcified plaque in the coronary arteries, which is strongly associated with the risk of CAD and future cardiovascular events. The test is non-invasive and relatively simple, using computer tomography (CT) scans to measure calcium deposits without the need for invasive procedures. The risk categorized calcium scores reflect different risk categories for cardiovascular events Oudkerk et al. (2008). Higher CAC scores can lead to more aggressive management of risk factor, while lower scores might support a more conservative approach. It is crucial for the radiographer to evaluate the position of high-density voxels in order to detect coronary calcification. CAC is typically measured using the Agatston method, assessing calcium deposits in the coronary arteries by measuring calcium density and volume to calculate a total calcium score Agatston et al. (1990). The Agatston score interval is a system for assessing coronary artery calcium deposition and is usually categorized as follows: 0 (no calcium, low cardiovascular risk), 1-10 (minimal calcium, low risk), 11-100 (low levels of calcium, moderate risk), 101-400 (moderate calcium, high risk), and over 400 (high levels of calcium, very high risk) Hecht (2015). This range of scores helps determine the risk of heart disease and plays an important role in clinical decision-making. Currently, the clinical analysis of calcium scores is performed semi-automatically by a software tool used by the radiologist to identify calcium regions by individually checking the slide images of each patient. This CAC measurement can be attention-demanding, labor-intensive, and time-consuming. To address these issues, automated CAC scoring methods are being developed, which can help enhance accuracy, consistency, and efficiency in measurements Eng et al. (2021a); van Assen et al. (2021); Takahashi et al. (2023a). Clinically, contrast-enhanced coronary CT angiography is a powerful imaging technique that uses contrast agent to provide detailed images of the coronary arteries to detect obstructive lesions and other vascular abnormalities, but involves higher radiation exposure due to the need for additional imaging sequences and the use of contrast Wolterink et al. (2016). On the other hand, non-contrast ECG-gated CT scans focus on quantifying coronary artery calcium, synchronizing image acquisition with the cardiac cycle to minimize motion artifacts and accurately assess calcified plaque for cardiovascular risk evaluation. Traditionally, when the CAC score has been calculated using non-contrast ECG-gated CT scans, a significant association with clinical outcomes has been observed Takx et al. (2015); Chiles et al. (2015). However, other studies have shown that CAC calculation using non-contrast ECG non-gated CT data has excellent agreement with gated data Raygor et al. (2023); Liu et al. (2022); Zeleznik et al. (2021a); Kerndt et al. (2023a). In this study, we propose a self-supervised learning based CAC scoring method that uses both gated and non-gated CT images together. Most efforts in deep learning for image analysis have been using supervised learning. However, existing supervised learning methods face challenges when there is a lack of labeled data. In public datasets of CT scans, for instance, less than 10% of the total slices contain calcified areas, despite the overall abundance of scans. This scarcity of labeled, relevant data poses a significant challenge for supervised approaches. Additionally, considering that calcifications typically cover only 3-5 millimeters, it becomes evident how challenging this task is. This small size makes accurate detection even more difficult, underscoring the need for advanced methods capable of identifying such minute details in CT scans. Another learning paradigm that effectively addresses the mentioned data scarcity challenges is self-supervised learning (SSL) Nielsen et al. (2023); Truong et al. (2021); Huang et al. (2023b). In traditional computer vision tasks, there is typically a large amount of unlabelled data available for SSL methods. SSL holds significant promise for enhancing coronary artery calcium scoring, particularly in the context of limited labeled data. In traditional supervised learning approaches, obtaining sufficient labeled datasets can be challenging and time-consuming, especially in the clinical domain where calcified medical images are often rare. SSL allows models to learn from vast amounts of unlabeled data by extracting features and patterns without the need for explicit annotations Huang et al. (2023b). This capability is particularly valuable for CAC scoring, as it enables more robust training of algorithms to identify calcium deposits in coronary arteries. Furthermore, while several studies have successfully developed automated CAC scoring using supervised learning methods, there is currently no reported method using self-supervised learning model on both gated and non-gated non-contrast ECG CTs. In this study, we have implemented one of the most popular SSL training technique using vision transformers (ViT) and commonly known as DINO (self-distillation with no labels) Caron et al. (2021). Regarding the architectural design of ViT models, generated features by ViT model in DINO model make possible to use these features in different tasks such as classification, segmentation or detection. In our approach, ViT models trained with DINO technique are utilized to classify CT slices whether containing calcified area. When it is considered that the area covered by calcification in CT slices and the ratio of CT slices containing calcification areas to across all CT slices in dataset, generating the features highlighting and capturing calcification areas is a challenging task. To overcome this issue and generate the features capturing targeted areas’ specifications, we introduce a novel training method, Label-Guided DINO, aimed at enhancing SSL approaches by incorporating label guidance to capture more specific features such as calcified areas in the model’s training process. This new technique is designed to contribute to SSL methodologies and expand upon existing training frameworks. Additionally, it has been demonstrated that vision foundational models can be directed toward specific areas, enabling them to generate features that effectively capture desired characteristics. The contributions of this paper can be summarized as follows: • We propose a novel training technique, developed to train a DINO model for targeted tasks by guiding it to capture highly specific features in its generated representations. This approach enhances the model’s ability to focus on relevant characteristics within the data by leveraging label guidance to direct its attention. • We introduce the DINO-LG model, a self-supervised learning (SSL) framework adaptable to various CT scan types. Designed as a foundational model, DINO-LG is capable of highlighting specific features in CT scans, making it versatile for a wide range of applications and tasks across medical imaging domains. • Our proposed system seamlessly integrates self-supervised learning (DINO-LG), classification, and segmentation models to deliver a fully automated coronary calcium scoring solution, which reduces manual analysis requirements and enables consistent and efficient CAC assessment."
https://arxiv.org/html/2411.07941v1,"DuoLift-GAN:Reconstructing CT from Single-view and Biplanar X-Rays 
with Generative Adversarial Networks","Computed tomography (CT) provides highly detailed three-dimensional (3D) medical images but is costly, time-consuming, and often inaccessible in intraoperative settings (Organization et al. 2011). Recent advancements have explored reconstructing 3D chest volumes from sparse 2D X-rays, such as single-view or orthogonal double-view images. However, current models tend to process 2D images in a planar manner, prioritizing visual realism over structural accuracy. In this work, we introduce DuoLift Generative Adversarial Networks (DuoLift-GAN), a novel architecture with dual branches that independently elevate 2D images and their features into 3D representations. These 3D outputs are merged into a unified 3D feature map and decoded into a complete 3D chest volume, enabling richer 3D information capture. We also present a masked loss function that directs reconstruction towards critical anatomical regions, improving structural accuracy and visual quality. This paper demonstrates that DuoLift-GAN significantly enhances reconstruction accuracy while achieving superior visual realism compared to existing methods.","Radiography, or X-ray imaging, is a medical imaging procedure that produces 2D projection images of a scanned patient with relatively low radiation doses and fast acquisition speeds. In contrast, computed tomography (CT)(Buzug 2011) is an imaging modality that uses a series of X-rays (typically between 100 and 250) taken at different angles around 360 degrees from which a three-dimensional volume is reconstructed. A key advantage of CT imaging is its ability to provide precise spatial information about anatomical structures, such as the lungs, in 3D space. However, CT procedures are time-consuming and expensive. X-rays, by comparison, are quicker, less costly, and result in significantly less radiation exposure (Organization et al. 2011). 2D X-ray images lack the 3D spatial information necessary for medical problems requiring quantitative morphological analysis. Thus, reconstructing a 3D CT volume from orthogonal double-view X-rays or a single-view X-ray could provide approximate 3D information and combine X-ray and CT imaging benefits. Although full 3D precision cannot be expected in such an approach, a significant 3D context may be inferred by learning from data. The primary challenge in reconstructing a 3D CT image from two orthogonal X-ray images or a single-view X-ray image is the need for more spatial information, particularly along the axis perpendicular to the 2D X-ray image. X-ray measurements capture the attenuation of X-rays as they pass through various tissues along their path. As an inverse problem, CT reconstruction typically requires projections densely sampled over 180 degrees. Fewer projections increase the ambiguity in solving this inverse problem using conventional CT reconstruction methods, such as Filtered Backprojection (Chetih and Messali 2015). However, with recent advancements in deep learning and the availability of large-scale medical image datasets, researchers have developed CT reconstruction models (Ying et al. 2019; Kasten, Doktofsky, and Kovler 2020; Ge et al. 2022; Gao et al. 2023; Song et al. 2021; Kyung et al. 2023; Sun et al. 2023) that can reconstruct 3D volumes from orthogonal double-view X-rays or even single-view X-ray images by leveraging learned priors from datasets. Some studies concentrate on dense anatomical structures, such as the knees (Kasten, Doktofsky, and Kovler 2020), cervical vertebrae (Ge et al. 2022), and spine (Gao et al. 2023), which contain bones with high attenuation coefficients, resulting in high contrast in radiographic images. Other research focuses on soft tissue reconstruction, particularly the lung (Ying et al. 2019; Kyung et al. 2023; Sun et al. 2023), an air-filled organ with a low attenuation coefficient inside. However, existing methods primarily target the reconstruction of the lung’s overall shape and surrounding soft tissues, often neglecting the intricate internal structures, such as the numerous thin, tree-like pulmonary vessels. These delicate structures are relatively smaller than the lung, resulting in subtle and complex features in 2D radiographic images. To address the challenges in 2D-to-3D reconstruction, we considered a method of lifting 2D images and features to 3D in advance to mitigate the difficulty associated with learning the mapping relationship between different dimensions. So we introduce the Duo Lift Generative Adversarial Networks (DuoLift-GAN), using a masked loss function; our model enhances detail capture by aligning the reconstruction with the target chest volume. By replicating the original 2D images and their corresponding feature maps along a specific axis multiple times, we elevate 2D images and their features into 3D representations that preserve spatial relationships and spatial coherence, resulting in more accurate 3D feature maps and improved reconstruction quality. Additionally, for precise shape and contour reconstruction, we present DuoLift-CNN, which accurately reconstructs larger anatomical structures, such as the lungs, which are suitable for capturing the overall structure rather than fine and thin structures, such as the vessels. Moreover, we observed an intriguing phenomenon during our evaluation of the reconstruction results: while GANs tend to produce volumes with richer textural details, CNN models outperform them in numerical metrics. To further explore this discrepancy, we conducted an in-depth analysis of the evaluation metrics used for CT reconstruction, focusing on the Structural Similarity Index Measure (SSIM)(Wang et al. 2004), Peak Signal-to-Noise Ratio (PSNR)(Hore and Ziou 2010), and the Learned Perceptual Image Patch Similarity (LPIPS)(Zhang et al. 2018) metric. This trend is also evident in the quantitative results of X2CT(Ying et al. 2019). Moreover, we conduct the anatomical structure level quantitative analysis of the reconstruction quality via the Dice Coefficient (DICE)(Dice 1945) of the lung and vessel segmentation maps. With the broad spectrum of evaluation metrics from pixel level, anatomical structure level, to perceptual level, we conduct a comprehensive quantitative and qualitative analysis of our methods and the state-of-the-art method on one lung CT dataset, the LIDC-IDRI dataset. Our analysis evaluates the reconstruction performance and provides insightful observations for the chest CT reconstruction problem from orthogonal X-rays. In conclusion, our contributions include: 1. We introduce a novel 2D-to-3D reconstruction architecture with dual branches that independently elevate 2D images and their feature maps into 3D representations. These branches preserve spatial relationships and 3D structure, with their outputs merged into a unified 3D feature map. Additionally, we employ a masked loss to train the generator in conjunction with a discriminator, which improves the accuracy of generated textural details. 2. Our models, DuoLift-GAN and DuoLift-CNN, achieve the best performance on benchmarks compared to methods with available code. 3. We conduct an in-depth evaluation of the reconstruction quality on the anatomical structure level. 4. We will release our code and pre-trained model weights on GitHub, promoting further research and development."
https://arxiv.org/html/2411.07940v2,"Automatic dataset shift identification to support
root cause analysis of AI performance drift","Shifts in data distribution can substantially harm the performance of clinical AI models. Hence, various methods have been developed to detect the presence of such shifts at deployment time. However, root causes of dataset shifts are varied, and the choice of shift mitigation strategies is highly dependent on the precise type of shift encountered at test time. As such, detecting test-time dataset shift is not sufficient: precisely identifying which type of shift has occurred is critical. In this work, we propose the first unsupervised dataset shift identification framework, effectively distinguishing between prevalence shift (caused by a change in the label distribution), covariate shift (caused by a change in input characteristics) and mixed shifts (simultaneous prevalence and covariate shifts). We discuss the importance of self-supervised encoders for detecting subtle covariate shifts and propose a novel shift detector leveraging both self-supervised encoders and task model outputs for improved shift detection. We report promising results for the proposed shift identification framework across three different imaging modalities (chest radiography, digital mammography, and retinal fundus images) on five types of real-world dataset shifts, using four large publicly available datasets.","Machine learning models are notoriously sensitive to changes in the input data distribution, a phenomenon commonly referred to as dataset shift [1, 2, 3, 4]. This is particularly problematic in clinical settings, where dataset shift is a common occurrence and may arise from various factors [5]. Changes in the frequency of disease positives over time or across geographical regions cause prevalence shift [6, 7] (also known as label shift). The use of different acquisition protocols or scanners [8, 9, 10], or a change in patient demographics [11, 12] can induce shifts in image characteristics, known as covariate shift. We illustrate examples of real-world shifts in Fig. 1. Dataset shift can dramatically affect the performance of AI and lead to clinical errors such as misdiagnosis [13, 4, 14]. It is recognised as the fundamental barrier hindering AI adoption [15, 16]. It is hence crucial to implement safeguards allowing not only effective detection of the presence of shifts, but importantly, reliable identification of the root causes. Comprehensive shift detection and identification frameworks are key for the safe deployment and continuous monitoring of AI in clinical practice. Dataset shifts can be detected at deployment time by using statistical testing to compare the distributions of incoming test data to the distribution of the reference data (representative of the data used to validate the deployed AI model). Significant progress has been made in this field where state-of-the-art methods can detect various types of real-world shifts [17, 18, 19, 20]. Shifts between test and reference data can either be detected at the output level by comparing distributions of model outputs, or at the input level by comparing low-dimensional feature representations of input images [19] (see Methods, Dataset shift detection methods for more details). In this study, we show that different types of shifts require different shift detection approaches. On the one hand, comparing model output distributions allows for the reliable detection of shifts directly related to the downstream task, such as changes in prevalence. On the other hand, we show that for shifts orthogonal to the downstream task, such as changes in image acquisition protocols, comparing output distributions is not sufficient. For such shifts, test and reference data need to be compared at the input level using rich feature representations. We demonstrate that self-supervised neural network image encoders [21], trained without using any task-specific annotations, yield excellent low-dimensional feature representations for shift detection. Figure 1: Examples of dataset shifts in medical imaging. Reliably detecting and identifying the nature of the shift is crucial to enable the safe deployment of machine learning systems applications. In this work, we propose the first shift identification framework able to reliably detect and classify any detected shift as (i) prevalence, (ii) covariate or (iii) mixed prevalence and covariate shift. While detecting dataset shifts is important, it is insufficient for the safe deployment of AI. Besides knowing that there is a problem, we need to be able to identify its root cause to take the necessary actions, implement preventive measures, and safeguard against harm caused by AI errors. To our knowledge, no solution yet exists to identify the root cause of dataset shifts. The precise identification of the type of shifts is critical for selecting appropriate mitigation strategies. For example, prevalence shifts can often be mitigated with lightweight output recalibration techniques [14, 22, 23, 24], but these rely on the assumption that no other types of shift are present. In contrast, covariate shifts require more advanced domain adaptation techniques or model fine-tuning [25, 26, 27, 28, 4, 29]. The difficulty is that a change in image characteristics may cause similar changes in the distribution over model outputs as a change in disease prevalence [4], and determining the cause of an observed shift can be challenging. Applying the wrong mitigation technique may, in the best case, be ineffective in resolving the shift or, in the worst case, severely harm model performance or calibration. In Appendix A, we show an example of the consequences of applying a prevalence shift correction algorithm to the wrong type of shift on model calibration, further motivating the need for dataset shift identification methods. Despite its paramount importance, automatic dataset shift identification has remained an open problem. Figure 2: Overview of the dataset shift identification pipeline proposed in this work. We leverage both task model outputs and features from self-supervised encoders for detecting and identifying dataset shifts. Contrarily to previous works, we do not simply detect the presence of shifts but also add a second step able to identify the nature of the shift. Our method effectively separates cases of (i) prevalence shift (a change in label distribution); (ii) covariate shift (a change in image characteristics) and (iii) covariate and prevalence shift (both). Our shift identification pipeline can be divided into two stages: (A) shift detection, followed by (B) shift identification. For shift detection, we combine signals from model outputs and generic features (low-dimensional representations of images) to detect whether a shift is present in the test set (2). If a shift is detected we proceed to shift identification. Identification starts with estimating the prevalence in the test set (3), then we resample the reference set to match the estimated test set prevalence (4). We then first compare feature distributions between prevalence-adjusted reference and test set (5): if differences are no longer significant after adjusting the prevalence, the shift is attributed to prevalence shift. Conversely, if differences persist after adjusting the prevalence, then covariate shift is necessarily present. In this case, we compare model output distributions to determine whether prevalence shift is also present (6). Precisely, if there were significant differences in model output distributions before adjusting the prevalence, but this shift disappears after adjusting the prevalence, we know that prevalence shift is also responsible for the observed shift, in this case we conclude that the observed shift is a case of mixed shift (prevalence + covariate shifts). Else, we conclude that the shift is attributed to covariate shift only. In this work, we address this issue by proposing the first dataset shift identification framework capable of identifying the root cause of the underlying shift, effectively separating (i) prevalence shift, (ii) covariate shift and (iii) mixed shift (both prevalence and covariate shifts). Our shift identification framework consists of two stages and is summarised in Fig. 2. First, the ‘shift detection’ module identifies whether a shift is present. Secondly, given that a shift has been detected, our ‘shift identification’ module characterises the type of shift. An in-depth evaluation across three different clinical applications (chest radiography, digital mammography, and retinal fundus images) on five types of real-world dataset shifts demonstrates that our framework accurately distinguishes between prevalence shifts, covariate shifts, and mixed shifts across various scenarios. We conclude that the proposed method may play an important role towards safe deployment and clinical adoption of medical imaging AI."
https://arxiv.org/html/2411.07899v1,Rendering-Oriented 3D Point Cloud Attribute Compression using Sparse Tensor-based Transformer,"The evolution of 3D visualization techniques has fundamentally transformed how we interact with digital content. At the forefront of this change is point cloud technology, offering an immersive experience that surpasses traditional 2D representations. However, the massive data size of point clouds presents significant challenges in data compression. Current methods for lossy point cloud attribute compression (PCAC) generally focus on reconstructing the original point clouds with minimal error. However, for point cloud visualization scenarios, the reconstructed point clouds with distortion still need to undergo a complex rendering process, which affects the final user-perceived quality. In this paper, we propose an end-to-end deep learning framework that seamlessly integrates PCAC with differentiable rendering, denoted as rendering-oriented PCAC (RO-PCAC), directly targeting the quality of rendered multiview images for viewing. In a differentiable manner, the impact of the rendering process on the reconstructed point clouds is taken into account. Moreover, we characterize point clouds as sparse tensors and propose a sparse tensor-based transformer, called SP-Trans. By aligning with the local density of the point cloud and utilizing an enhanced local attention mechanism, SP-Trans captures the intricate relationships within the point cloud, further improving feature analysis and synthesis within the framework. Extensive experiments demonstrate that the proposed RO-PCAC achieves state-of-the-art compression performance, compared to existing reconstruction-oriented methods, including traditional, learning-based, and hybrid methods.","Nowdays, the advent of 3D visualization techniques is revolutionizing the way we interact with digital content, offering a more immersive experience compared to traditional 2D formats. This technological shift has unlocked new possibilities for multimedia applications, including immersive communication [1], virtual and augmented reality experiences [2], and the preservation of cultural artifacts [3]. A key representation in 3D visualization is the point cloud, which consists of points in 3D space, often accompanied by attributes such as color and surface properties. The fidelity of a visual scene is closely tied to the number of points in the point cloud, which can range from thousands to billions, posing significant challenges for data management. To address this, various compression techniques [4, 5, 6, 7, 8, 9] have been developed to reduce point cloud data size while maintaining quality. In line with the push of the industry for efficient data handling, standardization bodies like the Joint Photographic Experts Group (JPEG) and the Moving Picture Experts Group (MPEG) have recognized the importance of point cloud data formats and have embarked on creating standards for their compression. This has led to the development of the Geometry-based Point Cloud Compression (G-PCC) and Video-based Point Cloud Compression (V-PCC) standards [10, 11, 12], both designed to meet the specific compression requirements of point cloud data. In addition, numerous deep learning-based methods [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23] have been developed for point cloud compression, showing promising performance compared to these traditional approaches. In point cloud visualization scenarios, the content rendered on the screen represents the final output seen by the end-user. Considering the compression process, the transformation of raw point clouds into visually coherent images is a complex process. Critical factors include the accuracy of point positioning, the richness of attribute information, and the effectiveness of rendering algorithms. Javaheri et al. investigated the impact of rendering on multiple MPEG point cloud coding solutions, motivated by the consideration that the perceived quality of point cloud data highly depends on the rendering solution [24]. Despite this, current point cloud compression methods focus on preserving the accuracy of point locations and associated attributes after reconstruction, often overlooking the effect of the rendering process. In this paper, we propose to integrate the compression and rendering modules within a single deep-learning framework to achieve an optimal trade-off between compression efficiency and the quality of rendered multiview images. The key feature of our framework, namely RO-PCAC, is the differentiability of the rendering module. After the rate-distortion loss is calculated, the rendering module generates necessary gradients that guide the network to jointly minimize bitrate and optimize image quality. The rate-distortion loss includes the estimated bitrate and the image error between the original and rendered multiview images. Additionally, inspired by the success of point cloud transformers [25, 26, 27] and sparse tensor-based convolution [28, 29, 30, 31], we propose a sparse tensor-based transformer, termed SP-Trans, to enhance feature analysis and synthesis. It adaptively constructs local neighborhoods within a fixed-size 3D window, aligning with local point density and ensuring a consistent number of neighbors for the enhanced local self-attention mechanism. Cosine similarity is employed within the local self-attention layer to handle varying point numbers and ensure its functionality in sparse regions. This design reduces computational complexity while leveraging the ability of the transformer to capture complex relationships and long-range dependencies between points. RO-PCAC outperforms state-of-the-art compression methods, including the traditional, learning-based, and hybrid approaches, on benchmark datasets such as 8i Voxelized Full Bodies (8iVFB) [32] and Owlii dynamic human mesh (Owlii) [33]. In summary, the main contributions of this paper are: 1. we make the first attempt to construct a general rendering-oriented compression framework for the color attribute of 3D point cloud data, termed RO-PCAC. RO-PCAC consists of a learning-based point cloud attribute compression module and a differentiable rendering module, enabling end-to-end training to jointly minimize the bitrate and optimize the quality of rendered multiview images. Furthermore, RO-PCAC supports the modular design for both the compression and rendering components. 2. we propose a sparse tensor-based transformer, SP-Trans, which captures the attribute relationships within local regions of point clouds and enhances the feature representation. SP-Trans adaptively constructs local neighborhoods and leverages a local self-attention mechanism, supported by cosine similarity, to process the density-varied regions, thereby improving compression efficiency. The remainder of this paper is organized as follows: Section II provides a review of related works. Section III describes the motivation of the proposed RO-PCAC and details the framework. Section IV first presents the training and testing setup, followed by objective and subjective quality comparison results, along with ablation studies. Finally, concluding remarks are offered in Section V."
https://arxiv.org/html/2411.07873v1,Diverse capability and scaling of diffusion and auto-regressive models when learning abstract rules,"Humans excel at discovering regular structures from limited samples and applying inferred rules to novel settings. We investigate whether modern generative AI systems can similarly learn underlying rules from finite samples and perform reasoning through conditional sampling. Inspired by Raven’s Progressive Matrices task, we designed GenRAVEN dataset, where each sample consists of three rows, and one of the 40 relational rules governing the object position, number, or attributes applies to all three rows. We trained generative models to learn the data distribution, where samples are encoded as 3×9×9 integer arrays to focus on rule learning. We compared two major families of generative models: diffusion models (EDM, DiT, SiT) and autoregressive models (GPT2, Mamba). We evaluated their ability to generate structurally consistent samples and perform panel completion via unconditional and conditional sampling. We found that diffusion models excel at unconditional generation, producing novel and more consistent samples from scratch and memorize less, but perform less well in panel completion, even with advanced conditional sampling methods like Twisted Diffusion Sampler. Conversely, autoregressive models excel at completing missing panels in a rule-consistent manner but generate less consistent samples unconditionally. We observe diverse data scaling behaviors: for both model families, rule learning emerges at a certain dataset size – around thousands examples per rule. With more training data, diffusion models improve both their unconditional and conditional generation capabilities. However, for autoregressive models, while panel completion improves with more training data, unconditional generation consistency declines. Our findings highlight complementary capabilities and limitations of diffusion and autoregressive models in rule learning and reasoning tasks, suggesting avenues for further research into their mechanisms and potential for human-like reasoning.","1 Background Figure 1: Design of the study A. Example Raven’s progression matrix, and its encoding as a 3\times9\times9 integer array. The underlying rule is constant shape. B.C. Two families of generative models: Diffusion and autoregressive model, and their training method: denoising and predicting the next token. D. The 40 relational rules, with 5 rules held out during training. Human excels at discovering regular structure from a small number of samples, and they can further apply such rule to novel settings to generate new samples or complete missing parts based on the same rule. The Raven’s progressive matrix (RPM) [27] is a famous task in human reasoning literature. In the generative version of this task (GenRAVEN), the subject observes two complete rows of panels and is tasked to complete the third row in a manner that is consistent with the first two rows (Fig.1A). Ideally, the subjects need to infer the underlying rule consistent with the first two rows and apply it correctly to the third row. How can we train a general learning system to solve such reasoning task? If we conceptualize all rule-conforming samples as a joint distribution, then rule learning can be framed as a generative modeling problem or learning the correct joint. Further, reasoning about the missing panel can be framed as sampling from the conditional probability [25]. One conceptual problem is, that given finite training samples, the rule governing them, or the ‘true‘ joint distribution is under-specified. The rules or the distribution learned by the system should be affected by its inductive bias, be it human or AI. Given this ambiguity, we asked whether modern generative AI systems could learn the correct ""joint distribution"" given finite samples. If so, can they reason and fill in the missing parts in a sample through conditional sampling? In the current age of Generative AI, there are two prominent families of generative models: autoregressive models and diffusion models. The autoregressive model generally dominates discrete sequence data, such as language, music, genomics [4, 14, 13], while the diffusion model excels at continuous data, such as image, video, audio, molecule structure, robot trajectories [28, 34, 29, 5, 3]. Both of them are capable of unconditional generation and conditional sampling based on partial observation: e.g. prompting for autoregressive model or inpainting for diffusion models. Given their similar capability, it’s interesting to compare them back-to-back on the same reasoning task and see how their different modeling method might lead to different learning and scaling behaviors. In this work, we trained a diverse set of generative models from both the diffusion family (EDM, DiT, SiT) and autoregressive family (GPT, Mamba) to learn the data distribution and then use conditional sampling techniques to perform inference, i.e., reasoning about the missing panel. We studied their performance as a function of data scale and model scale. We found that generally, diffusion models excel at unconditional sampling from the joint, creating structurally consistent samples from scratch, but perform less well for sampling conditioned on given panels. Conversely, autoregressive models excel at completing missing panels in a rule-consistent manner, but they perform less well in generating consistent samples from scratch with unconditional sampling. They also exhibit diverse scaling behavior when the size of datasets is varied. Our results call for further investigation into the seemingly complementary capabilities and limitations of diffusion and autoregressive models."
https://arxiv.org/html/2411.07848v1,"NL-SLAM for OC-VLN: 
Natural Language Grounded SLAM for Object-Centric VLN","Landmark-based navigation (e.g., go to the wooden desk) and relative positional navigation (e.g., move 5 meters forward) are distinct navigation challenges solved very differently in existing robotics navigation methodology. We present a new dataset, OC-VLN, in order to distinctly evaluate grounding object-centric natural language navigation instructions in a method for performing landmark-based navigation. We also propose Natural Language grounded SLAM (NL-SLAM), a method to ground natural language instructions to robot observations and poses. We actively perform NL-SLAM in order to follow object-centric natural language navigation instructions. Our methods leverage pre-trained vision and language foundation models and require no task-specific training. We construct two strong baselines from state-of-the-art methods on related tasks, Object Goal Navigation and Vision Language Navigation, and we show that our approach, NL-SLAM, outperforms these baselines across all our metrics of success on OC-VLN. Finally, we successfully demonstrate the effectiveness of NL-SLAM for performing navigation instruction following in the real world on a Boston Dynamics Spot robot. The OC-VLN dataset, code, and videos are available at sonia-raychaudhuri.github.io/nlslam.","Humans perform visual navigation by using landmarks to determine where they are in the world. Landmarks are distinctive sites in a scene such as objects or topological features (e.g., a particular tree or building). While we can understand relative movement from our current location (e.g., move 5 meters forward), grounding this spatial understanding in landmarks is required to understand our placement in larger regions (e.g., now I am by the front door) [1, 2, 3, 4]. To follow navigation instructions, humans associate landmarks described in natural language with real landmarks observed in the environment to simultaneously understand where they are in the instruction and in the world. Roboticists studying mapping and navigation have developed methods for creating landmark maps from visual observations and using those maps for autonomous navigation [5, 6, 7, 8, 9, 10, 11]. In fact, work in this area has matured far enough to move beyond the research community. Landmark-based mapping and navigation systems are widely used in autonomous robot operation in many industry applications, including home robot vacuum cleaners [12, 13] and drones for surveying and inspection [14]. With recent advances in natural language understanding due to large language and vision-language models [15, 16, 17, 18, 19, 20], the next frontier is to determine how to accurately align natural language navigation instructions with observation-based autonomous decision making capabilities in these mature robotic systems. To support research in this direction, researchers have developed benchmarks for evaluating an autonomous agent’s ability to follow natural language navigation instructions in simulation, specifically R2R [21], RxR [22], and VLN-CE [23]. These benchmarks consist of datasets of natural language instructions and associated paths through scanned Matterport 3D scenes from inside residential homes [24]. These datasets contain two distinct navigation challenges. The language instructions contain both object-centric references to landmarks and relative positional references to the current agent position. For example, ‘go to the blue chair, then turn left and stop at the wooden table’ versus ‘move forward and turn left and stop in the hallway’ respectively. In this work, we investigate the capability of object-centric instruction following alone. There is substantial value in disentangling these two challenges. This value can be understood from both a cognitive science and robotics perspective. Neuroscience researchers have found that distinct regions of the brain enable people to perform landmark-based spatial navigation using observed scene objects [1]. While successful artificial intelligence algorithms are frequently not a direct mimic of human neural function, roboticists also have a long history of managing landmark-based navigation [25] and relative positional navigation [26, 27, 28] with clearly distinct methodological approaches. This distinction in both biological systems and autonomous navigation algorithms motivates us to consider that successful language grounding for these different types of navigation may also be achieved with distinct methodologies. Contributions. In this work, we present a vision-language navigation dataset of exclusively object-centric navigation instruction following commands (OC-VLN). We also propose natural language grounded SLAM (NL-SLAM) for grounding natural language navigation instructions to robot observations and poses. We present a novel navigation policy to actively perform NL-SLAM in order to follow natural language instructions on OC-VLN. Our novel methods, leveraging pre-trained vision and language foundation models, require no task-specific training. Finally, we demonstrate the viability of NL-SLAM to perform real-world navigation instruction following on a Boston Dynamics Spot robot."
https://arxiv.org/html/2411.07815v1,Reliable-loc: Robust sequential LiDAR global localization in large-scale street scenes based on verifiable cues,"Wearable laser scanning (WLS) system has the advantages of flexibility and portability. It can be used for determining the user’s path within a prior map, which is a huge demand for applications in pedestrian navigation, collaborative mapping, augmented reality, and emergency rescue. However, existing LiDAR-based global localization methods suffer from insufficient robustness, especially in complex large-scale outdoor scenes with insufficient features and incomplete coverage of the prior map. To address such challenges, we propose LiDAR-based reliable global localization (Reliable-loc) exploiting the verifiable cues in the sequential LiDAR data. First, we propose a Monte Carlo Localization (MCL) based on spatially verifiable cues, utilizing the rich information embedded in local features to adjust the particles’ weights hence avoiding the particles converging to erroneous regions. Second, we propose a localization status monitoring mechanism guided by the sequential pose uncertainties and adaptively switching the localization mode using the temporal verifiable cues to avoid the crash of the localization system. To validate the proposed Reliable-loc, comprehensive experiments have been conducted on a large-scale heterogeneous point cloud dataset consisting of high-precision vehicle-mounted mobile laser scanning (MLS) point clouds and helmet-mounted WLS point clouds, which cover various street scenes with a length of over 20km. The experimental results indicate that Reliable-loc exhibits high robustness, accuracy, and efficiency in large-scale, complex street scenes, with a position accuracy of 1.66m, yaw accuracy of 3.09 degrees, and achieves real-time performance. For the code and detailed experimental results, please refer to https://github.com/zouxianghong/Reliable-loc.","Wearable laser scanning (WLS) systems have the advantages of flexibility and portability by integrating LiDAR (Light Detection And Ranging), IMU (inertial measurement unit), and other sensors in a portable device (Li et al., 2023, 2024). It can be used for finding the user’s path, i.e. global localization, which is a huge demand for pedestrian navigation (Baglietto et al., 2011), collaborative localization and mapping (Yuan et al., 2017; Kachurka et al., 2021), augmented reality (Chi et al., 2022), counter-terrorism and emergency rescue (bin Shamsudin et al., 2017). As LiDAR is not sensitive to changes in lighting, works in different weather conditions, and has high measurement accuracy (Goran, 2010; Sharif, 2021), LiDAR-based global localization is practical and can work in GNSS-denied complex environments such as urban canyons, indoors, and underground (Wang et al., 2021). LiDAR-based global localization has been long studied. According to whether sequential LiDAR data is used, LiDAR-based global localization can be divided into two categories: single-shot and sequential global localization. The former usually achieves localization based on place recognition using a single LiDAR frame and is not suitable for large-scale repetitive scenes (Du et al., 2020; Komorowski, 2021; Cattaneo et al., 2022; Zou et al., 2023). The latter usually realizes localization by utilizing place recognition with techniques like MCL and sequential matching, which is more applicable to industry applications in large-scale outdoor scenes (Liu et al., 2019a; Yin et al., 2019b; Chen et al., 2020; Ma et al., 2022). Hence, this paper focuses on the sequential global localization for the wearable device. In real applications, the WLS-based localization system needs to be operated in various scenes, posing many challenges to the reliability of the global localization algorithm. First, the localization system faces challenges from feature-insufficient environments. Take the typical urban street scene in southern China as an example: the road is wide and full of viaducts, and many road sections are only flanked by street trees and flat facades, lacking sufficient features (Li et al., 2023; Lin et al., 2023). Second, the prior map may not fully cover the area, and blank data holes exist. For example, the prior point cloud map is usually collected along roads by the vehicle-mounted MLS systems and can not fully cover roadside areas due to occlusion and accessibility (Serna and Marcotegui, 2013; Mi et al., 2021). In recent years, some LiDAR-based sequential global localization methods have integrated place recognition techniques into MCL and are applicable to large-scale outdoor scenes (Yin et al., 2023a). However, existing methods still have limitations facing the above challenges. On the one hand, most of them solely rely on the global feature (the aggregation of local features) extracted from local maps to construct the observation model in MCL. In feature-insufficient scenes where global features lack descriptive adequacy, particles in MCL are prone to converging towards erroneous regions, ultimately resulting in the localization system crashing. While many place recognition methods extract local features simultaneously to aggregate the global feature, the rich information embedded in local features is ignored by the downstream MCL task. On the other hand, existing methods only rely on point cloud registration for localization once MCL converges, thus leading to localization failure due to continuous unreliable pose estimation in the scenes with insufficient features and incomplete map coverage. To tackle the challenges of feature insufficiency and incomplete coverage of prior maps, we propose Reliable-loc, a reliable sequential global localization method based on verifiable cues from both spatial and temporal aspects. Reliable-loc enhances MCL by exploiting the rich information embedded in local features from place recognition (the spatial cues) and improving localization robustness by monitoring the sequential localization status (the temporal cues). The main contributions of this paper are as follows: 1. A novel MCL incorporating spatially verifiable cues is proposed to adjust particle weights using the rich information embedded in local features. It improves localization robustness in feature-insufficient scenes by avoiding particles converging to erroneous regions. 2. A localization status monitoring mechanism guided by the temporal sequential pose uncertainties is proposed thus the localization mode can be adaptively switched according to the temporal verifiable cues. It improves localization robustness in scenes with insufficient features and incomplete map coverage by exploiting the exploratory capability of particles in MCL. The remainder of the paper is organized as follows. Section 2 reviews the related works on LiDAR-based sequential global localization. Section 3 presents the important preliminary. Section 4 elaborates on the proposed approach. Section 5 presents the datasets and quantitative evaluation. Section 6 presents the ablation studies and discusses the deficiencies and future work. The conclusion is outlined in Section 7."
https://arxiv.org/html/2411.07784v1,Interaction Asymmetry: A General Principle for Learning Composable Abstractions,"Learning disentangled representations of concepts and re-composing them in unseen ways is crucial for generalizing to out-of-domain situations. However, the underlying properties of concepts that enable such disentanglement and compositional generalization remain poorly understood. In this work, we propose the principle of interaction asymmetry which states: “Parts of the same concept have more complex interactions than parts of different concepts”. We formalize this via block diagonality conditions on the (n+1)th order derivatives of the generator mapping concepts to observed data, where different orders of “complexity” correspond to different n. Using this formalism, we prove that interaction asymmetry enables both disentanglement and compositional generalization. Our results unify recent theoretical results for learning concepts of objects, which we show are recovered as special cases with n\!=\!0 or 1. We provide results for up to n\!=\!2, thus extending these prior works to more flexible generator functions, and conjecture that the same proof strategies generalize to larger n. Practically, our theory suggests that, to disentangle concepts, an autoencoder should penalize its latent capacity and the interactions between concepts during decoding. We propose an implementation of these criteria using a flexible Transformer-based VAE, with a novel regularizer on the attention weights of the decoder. On synthetic image datasets consisting of objects, we provide evidence that this model can achieve comparable object disentanglement to existing models that use more explicit object-centric priors.","1 Introduction A core feature of human cognition is the ability to use abstract conceptual knowledge to generalize far beyond direct experience (Tenenbaum et al., 2011, Behrens et al., 2018, Mitchell, 2021, Murphy, 2004). For example, by applying abstract knowledge of the concept “chair”, we can easily infer how to use a “chair on a beach”, even if we have not yet observed this combination of concepts. This feat is non-trivial and requires solving two key problems. Firstly, one must acquire an abstract, internal model of different concepts in the world. This implies learning a separate internal representation of each concept from sensory observations. Secondly, these representations must remain valid when observations consist of novel compositions of concepts, e.g., “chair” and “beach”. In machine learning, these two problems are commonly referred to as learning disentangled representations (Bengio et al., 2013, Higgins et al., 2018, Schölkopf et al., 2021) and compositional generalization (Fodor and Pylyshyn, 1988, Lake et al., 2017, Greff et al., 2020, Goyal and Bengio, 2022). Both problems are known to be challenging due to the issue of non-identifiability (Hyvärinen et al., 2023). Namely, many models can explain the same data equally well, but only some will learn representations of concepts which are disentangled and generalize compositionally. To guarantee identifiability with respect to (w.r.t.) these criteria, it is necessary to incorporate suitable inductive biases into a model (Hyvärinen and Pajunen, 1999, Locatello et al., 2019, Lachapelle et al., 2023b). These inductive biases, in turn, must reflect some underlying properties of the concepts which give rise to observed data. This raises a fundamental question: What properties of concepts enable learning models which provably achieve disentanglement and compositional generalization? Many works aim to answer this question by studying properties enabling either disentanglement or compositional generalization in isolation. This is insufficient, however, as disentanglement alone does not imply compositional generalization (Montero et al., 2021, Schott et al., 2022, Montero et al., 2022), while compositional generalization requires first disentangling the concepts to be composed. Only a few studies investigate properties enabling both disentanglement and compositional generalization (Lachapelle et al., 2023b, Brady et al., 2023, Wiedemer et al., 2024a). Yet, the properties proposed in these works are rather restrictive and specific to objects in simple visual scenes. There is growing evidence, however, that the principles humans use to learn conceptual knowledge are not concept-specific, but shared across different concepts (objects, attributes, events, etc.) (Constantinescu et al., 2016, Behrens et al., 2018, Hawkins et al., 2018). This suggests there exist some general properties of concepts which enable both disentanglement and compositional generalization. Figure 1: Illustration of Interaction Asymmetry. (Left) Observations \mathbf{x} result from a generator \mathbf{f} applied to latent slots \mathbf{z}_{B_{k}} that represent separate concepts. As indicated by the reflection of the cylinder upon the cube, slots can interact during generation. Our key assumption, interaction asymmetry, states that these interactions across slots must be less complex than interactions within the same slot. (Right) This is formalized by assuming block-diagonality across but not within slots for the (n\!+\!1)th order derivatives of the generator, i.e., D^{n+1}\mathbf{f}. In this work, we seek to formulate such a general property for disentangling and composing concepts. We begin by aiming to deduce, from first principles, properties which are fundamental to concepts (§ 3). From this, we arrive at the guiding principle of interaction asymmetry (Principle 3.1) stating: “Parts of the same concept have more complex interactions than parts of different concepts”. As illustrated in Fig. 1 (left), we define concepts as distinct groups, or slots, of latent variables which generate the observed data (§ 2). Interaction asymmetry is then formalized as a block-diagonality condition across but not within slots for the tensor of (n+1)th order partial derivatives of the generator function (Asm. 3.5), where n determines the complexity of interactions, see Fig. 1 (right). Theory. Using this formulation, we prove that interaction asymmetry dually enables both disentanglement (Thm. 4.3) and compositional generalization (Thm. 4.4). We also show that our formalism provides a unifying framework for prior results of Brady et al. (2023) and Lachapelle et al. (2023b), by proving that the properties studied in these works for visual objects are special cases of our assumptions for n\!=\!0 and 1, respectively. We provide results for up to n\!=\!2, thus extending these prior works to more general function classes, and conjecture that our results generalize to arbitrary n\!\geq\!0. Method. Our theory suggests that to disentangle concepts, a model should (i) enforce invertibility, without using more latent dimensions than necessary, and (ii) penalize interactions across slots during decoding. To translate these insights into a practical method, we leverage a VAE loss (Kingma and Welling, 2014) for (i), and observe that the Transformer architecture (Vaswani et al., 2017) offers an approximate means to achieve (ii) since interactions are determined by the attention weights of the model. To this end, we introduce an inexpensive interaction regularizer for a cross-attention mechanism, which we incorporate, with the VAE loss, into a flexible Transformer-based model (§ 5). Empirical Results. We test this model’s ability to disentangle concepts of visual objects on a Sprites dataset (Watters et al., 2019a) and on CLEVR6 (Johnson et al., 2017). We find that the model reliably learns disentangled representations of objects, improving performance over an unregularized Transformer (§ 6). Furthermore, we provide preliminary evidence that our regularized Transformer can achieve comparable performance to models with more explicit object-centric priors such as Slot Attention (Locatello et al., 2020b) and Spatial Broadcast Decoders (Watters et al., 2019b). Notation. We write scalars in lowercase (z), vectors in lowercase bold (\mathbf{z}), and matrices in capital bold (\mathbf{M}). [K] stands for \{1,2,...,K\}. D_{i} and D^{2}_{i,j} stand for the first- and second-order partial derivatives with respect to (w.r.t.) z_{i} and (z_{i},z_{j}), respectively. If B\subseteq[n] and {\bm{z}}\in{\mathbb{R}}^{n}, \mathbf{z}_{B} denotes the subvector (z_{i})_{i\in B} indexed by B. A function is C^{n} if it is n-times continuously differentiable."
https://arxiv.org/html/2411.07751v1,SAV-SE: Scene-aware Audio-Visual Speech Enhancement with Selective State Space Model,"Speech enhancement plays an essential role in various applications, and the integration of visual information has been demonstrated to bring substantial advantages. However, the majority of current research concentrates on the examination of facial and lip movements, which can be compromised or entirely inaccessible in scenarios where occlusions occur or when the camera view is distant. Whereas contextual visual cues from the surrounding environment have been overlooked: for example, when we see a dog bark, our brain has the innate ability to discern and filter out the barking noise. To this end, in this paper, we introduce a novel task, i.e. Scene-aware Audio-Visual Speech Enhancement (SAV-SE). To our best knowledge, this is the first proposal to use rich contextual information from synchronized video as auxiliary cues to indicate the type of noise, which eventually improves the speech enhancement performance. Specifically, we propose the VC-S2E method, which incorporates the Conformer and Mamba modules for their complementary strengths. Extensive experiments are conducted on public MUSIC, AVSpeech and AudioSet datasets, where the results demonstrate the superiority of VC-S2E over other competitive methods. We will make the source code publicly available. Project demo page: https://AVSEPage.github.io/","In our daily living environments, speech signals are often distorted by various environmental background noises during their propagation. Speech enhancement (SE) is a task aiming at isolating the clean speech in the presence of noise interference, resulting in improved speech intelligibility and perceptual quality [1, 2, 3, 4]. It enables natural and effective Human-Robot Interaction (HRI) and plays a crucial role in various applications, such as hearing aids, mobile communication, automatic speech recognition [5, 6, 7], speaker verification [8], and speaker tracking [9, 10, 11]. These applications underscore the importance of SE in realistic scenarios. Traditional signal processing-based SE approaches, which are derived from the assumed properties on speech and noise, are incapable of suppressing highly non-stationary noise sources [12, 13, 14]. In the past decade, with the advent of deep learning technology and increased computational resources, supervised speech enhancement solutions has achieved great success [2]. Figure 1: Our proposed SAV-SE task where outputs from the audio and visual encoder are fused to refine and generate the enhanced audio. By incorporating the visual context from noise environments, it significantly enhances speech quality, particularly in situations where traditional audio-only techniques falter. Despite the significant strides made in the field, the challenge of noise reduction without inflicting artifacts on the speech signal persists, particularly in dynamic environments characterized by non-stationary and multi-source noise [15]. This difficulty is further compounded by the need to maintain the integrity of the speech signal, ensuring that the naturalness of the human voice is preserved. To address this challenge, researchers have been exploring cutting-edge signal processing methodologies and sophisticated machine learning paradigms. One promising solution involves the use of neural networks, which has demonstrated great capabilities in extracting features and separating signals from complex acoustic environments. A variety of network architectures are trained to learn the underlying patterns in noisy audio data, thus accomplishing the objective of speech enhancement [16]. Each of these models contributes unique strengths to the task of learning and generalizing from noisy audio data. For example, Multi-Layer Perceptrons (MLPs) are proficient in detecting intricate, non-linear data patterns, whereas Recurrent Neural Network (RNN) effectively manage the sequential dependencies in audio signals. Temporal Convolutional Network (TCN) excel in capturing long-range dependencies without suffering from the vanishing gradient problem that plagues standard RNN. The Transformer architecture, featuring self-attention, has transformed the field by allowing models to process any part of the input sequence, which is crucial for tasks involving widespread noise-speech relationships. The Mamba architecture [17], as the latest advancement, further extends the capabilities of noise reduction and speech enhancement. Researchers have increasingly acknowledged the importance of maintaining semantic, temporal, and spatial coherence between audio and video sources [18, 19]. This motivates attempts to use video information as a complement of audio input to recover details that are lost in audio-only scenarios. Existing Audio-Visual Speech Enhancement (AVSE) schemes often exploit temporal synchronized facial and lip movements to improve the clarity and perception of enhanced speech [20, 21, 22]. Despite outperforming audio-only SE systems, they are infeasible in many practical scenarios (e.g., outdoors or pandemic period) where human visual cues are not available. Moreover, inaccurate face or lip detection (e.g., in low-quality videos) may also result in degraded performance. In contrast, visual cues of environmental information, such as noise scenes or background objects emitting the noise, are easier to capture. It is more practical to use visual environmental cues to provide a valuable complement to speech enhancement. Thus, to fully leverage audio-visual information to enhance uni-modal learning, it is essential to consider these modality-specific attributes. In this paper, we introduce a novel AVSE framework, as illustrated in Figure 1, which uses visual information of the surrounding scenes as an auxiliary prompt to improve SE performance. Specifically, it addresses the limitations of current technologies, particularly in scenarios where an accurate capture of facial or lip information is not available. The contributions of this paper are summarized as follows: 1. We introduce a novel and more practical scene-aware AVSE task, namely SAV-SE. Unlike existing AVSE studies that rely primarily on visual facial and lip movements, this paper explores auxiliary visual contextual cues from the surrounding scenes to mitigate environmental background noise. 2. We are the first to explore selective State Space Model (SSM) for audio-visual speech enhancement. Specifically, we propose a Visual-prompting ConMamba for Scene-aware Speech Enhancement (VC-S2E), a novel approach that leverages audio-visual modalities to improve speech quality and intelligibility. Built upon innovative hybrid convolution-SSM architecture, ConMamba can capture both long-range global interactions and localized fine-grained feature patterns. 3. We comprehensively evaluate our proposed method across three widely used AV datasets. The results consistently confirm the superiority of our \text{VC-}\text{S}^{2}\text{E} over other competing methods in speech quality and intelligibility. Meanwhile, the visualization analysis illustrates that visual focal areas locate at the sounding object, demonstrating the contribution of visual scene information."
https://arxiv.org/html/2411.07750v1,LapGSR: Laplacian Reconstructive Network for Guided Thermal Super-Resolution,"In the last few years, the fusion of multi-modal data has been widely studied for various applications such as robotics, gesture recognition, and autonomous navigation. Indeed, high-quality visual sensors are expensive, and consumer-grade sensors produce low-resolution images. Researchers have developed methods to combine RGB color images with non-visual data, such as thermal, to overcome this limitation to improve resolution. Fusing multiple modalities to produce visually appealing, high-resolution images often requires dense models with millions of parameters and a heavy computational load, which is commonly attributed to the intricate architecture of the model.We propose LapGSR, a multimodal, lightweight, generative model incorporating Laplacian image pyramids for guided thermal super-resolution. This approach uses a Laplacian Pyramid on RGB color images to extract vital edge information, which is then used to bypass heavy feature map computation in the higher layers of the model in tandem with a combined pixel and adversarial loss. LapGSR preserves the spatial and structural details of the image while also being efficient and compact. This results in a model with significantly fewer parameters than other SOTA models while demonstrating excellent results on two cross-domain datasets viz. ULB17-VT and VGTSR datasets.","Super-resolution methods have applications in various domains, including digital imagery, photography, robotics, autonomous navigation, surveillance, and security [1, 2, 3]. These techniques aim to enhance the resolution of low-quality images, enabling the extraction of finer details and improved visual quality from lower-resolution source data. Recently, non-visual imagery sensors have gained prominence in various applications due to their ability to capture information beyond the visible spectrum. These sensors, such as thermal cameras, provide crucial data in challenging environments, allowing the detection of heat signatures and other phenomena not perceptible to the human eye. However, thermal images typically have low resolution, and acquiring high-resolution non-visual sensors is expensive. To address this challenge, researchers have delved into Guided Thermal Super Resolution, a field focused on enhancing the resolution of thermal images using RGB images as a guide. With the advent of deep learning, guided super-resolution methods have become increasingly effective, reducing the reliance on expensive, high-quality sensors. Figure 1: Laplacian Pyramid Visualization. The first row is the Laplacian pyramid of a grayscaled RGB image with two levels and the residual at the end. The second row contains the Laplacian pyramid of a thermal image with two levels and a residual at the end. These images have been taken from the ULB17-VT dataset [4]. Integrating data from multiple sensors, such as RGB and thermal, can gain more comprehensive insights and enhance performance. The task of enhancing resolution in non-visual data while preserving content remains a persistent challenge [5]. We experiment with cross-domain datasets, namely the ULB17-VT [4] dataset, which is captured by a handheld camera, and the VGTSR dataset [6], captured by a UAV (Unmanned Aerial Vision) platform. To address these issues with a streamlined approach, we introduce LapGSR. LapGSR addresses the challenges of enhancing the resolution of low-quality thermal images by leveraging information from high-resolution RGB images. Our model combines traditional computer vision techniques like Laplacian pyramids with modern deep learning methods for feature extraction. Our framework introduces key components for handling texture, illumination, and other critical features extracted from RGB inputs, ultimately enhancing the resolution of low-quality thermal images. Its architecture incorporates a lightweight, generative neural network with cascading residual blocks, providing robustness against misalignment while ensuring computational efficiency. Our inspiration for employing Laplacian pyramids stems from their effectiveness in capturing similarities in edge maps between RGB and high-resolution thermal images. As visualized in Figure 1, the structural features, including texture, intensity, and spatial patterns, are notably congruent within the Laplacian pyramid layers. This observation motivates our use of Laplacian pyramids for feature extraction. We utilize the Laplacian pyramid of our guiding image (RGB) to reconstruct the Laplacian pyramid of the target (thermal) image. We draw upon the Laplacian pyramid technique introduced in prior work [7], initially devised for addressing image-to-image translation tasks encompassing variations in seasons and lighting conditions. Our proposed model allows for Guided Thermal Super Resolution in scenarios involving alignment and misalignment via the employment of residual blocks in the High Transformation Branch. Our proposed model carries out major feature map transformation in higher layers compared to their model’s lowest layers. Through our novel architecture, we achieve adaptive refinement of image structures, resulting in superior image quality. Our contributions extend across various domains, as our model preserves fine details in generated images and has fewer trainable parameters than other SOTA models, making it suitable for real-time applications where image quality is critical. Moreover, the model’s adaptability to misaligned thermal images is a significant advantage, ensuring its effectiveness in diverse scenarios. Our approach combines traditional computer vision with modern deep learning techniques, achieving state-of-the-art outcomes with fewer parameters compared to recent models in both aligned and misaligned scenarios. This highlights its effectiveness and potential influence in the super-resolution domain. The main contributions of our proposed method are: • Incorporating the Laplacian image pyramid, a classical computer vision technique, alongside contemporary deep learning approaches for feature extraction, our contribution effectively restores high-resolution images with high visual fidelity and accuracy. This approach allows us to reduce the reliance on a large number of convolutional layers. • A lightweight model architecture that delivers consistent performance on the VGTSR dataset and state-of-the-art performance on the ULB17-VT dataset, with 90% and 50% fewer trainable parameters than the SOTA models. • A Multi-Objective Loss that incorporates adversarial loss to address and mitigate the trade-off between PSNR and SSIM • Robustness against misaligned RGB-Thermal pairs, a situation often encountered in real-world scenarios, making our model suitable for deployment."
https://arxiv.org/html/2411.07719v1,": A Flexible Generative Perception Error Model 
for Probing Self-Driving Planners","To handle the complexities of real-world traffic, learning planners for self-driving from data is a promising direction. While recent approaches have shown great progress, they typically assume a setting in which the ground-truth world state is available as input. However, when deployed, planning needs to be robust to the long-tail of errors incurred by a noisy perception system, which is often neglected in evaluation. To address this, previous work has proposed drawing adversarial samples from a perception error model (PEM) mimicking the noise characteristics of a target object detector. However, these methods use simple PEMs that fail to accurately capture all failure modes of detection. In this paper, we present Emperror, a novel transformer-based generative PEM, apply it to stress-test an imitation learning (IL)-based planner and show that it imitates modern detectors more faithfully than previous work. Furthermore, it is able to produce realistic noisy inputs that increase the planner’s collision rate by up to 85\text{\,}\mathrm{\char 37\relax}, demonstrating its utility as a valuable tool for a more complete evaluation of self-driving planners.","After years of progress, autonomous driving systems are able to handle increasingly complex situations [1]. This is enabled, in part, by solving several aspects of driving with learned modules, such as perception [2, 3, 4] and motion forecasting [5, 6]. Recently, there has been increased interest in managing the complexity of human behavior in traffic by also learning the planning task [7, 8, 9], which has been accelerated through the emergence of motion forecasting- and planning-centric benchmarks and datasets [10, 11]. Most of this work assumes a simplified setting where the ground-truth world state is available as input and focuses on accuracy of the planned trajectory with respect to human driving. As a result, robustness to the residual risk of errors in the perception system, which is ultimately just an imperfect model operating on incomplete observations of the world, remains underexplored. Figure 1: Method Overview. We propose Emperror, a generative model that imitates a given detector by modeling the distribution of its perception errors conditioned on a ground-truth state and BEV map as scene context. Adversarial optimization in the model’s latent space can then produce challenging yet plausible proxy detections from that distribution which stress-test the robustness of a given planner, e.g. by inducing collisions. In this work, we aim to illuminate the susceptibility of learned planning, which is often brittle in the face of o.o.d inputs [12], to these errors. Recent seminal studies [13, 14] have approached this problem from the lens of adversarial attacks. To this end, these methods first construct a perception error model (PEM) [15, 16], which allows sampling multiple noisy estimates imitating a target 3D object detector given a ground-truth scene representation as context. Then, by leveraging the PEM as a proxy of the detector, challenging samples that stress-test the target planner can be found by employing an adversarial search strategy. While promising, these works consider simple, synthetic scenes that do not capture the complexities of real-world data. Moreover, they employ simple PEMs that phrase the error modeling task as an isolated, per-object perturbation of the ground-truth state, rather than jointly reasoning over the entire scene context. Hence, they cannot faithfully model the intricacies of the error patterns exhibited by modern 3D object detectors, such as duplicate detections resulting in false-positives and correlations in errors for groups of objects. Motivated by this, we propose Emperror, a novel generative empirical error model based on the transformer architecture, that can more faithfully capture the error characteristics of a target detector. Our key idea is to leverage the attention mechanism and a flexible set of latent queries to model the full range of failure modes, including false-positives, in a scene-consistent manner. Furthermore, Emperror provides a prior over different error patterns for a given scene context, enabling us to draw adversarial, yet plausible samples to probe the robustness of a target planner. Building on these advantages, we design a framework to probe the robustness of learned planners to noisy perception inputs, which is visualized in Fig. 1. We then apply the proposed framework to an imitation learning (IL)-based planner, modeling three different modern camera-based 3D object detectors, and show learned planning is indeed vulnerable to plausible noise from the long-tail of perception errors. We believe Emperror can serve as a valuable tool for data-driven evaluation of self-driving planners. Contributions: (1) We propose Emperror, a novel transformer-based, generative PEM for probing planning, that can more faithfully imitate modern object detectors than previous work. (2) We integrate Emperror into a framework for probing the robustness of a planner to noise in its perception system. (3) We show that our proposed PEM can imitate modern 3D object detectors and enables the generation of challenging yet plausible errors that can be used to evaluate planning in a data-driven manner."
https://arxiv.org/html/2411.07567v1,Uncertainty-Aware Test-Time Adaptation for Inverse Consistent Diffeomorphic Lung Image Registration,"Diffeomorphic deformable image registration ensures smooth invertible transformations across inspiratory and expiratory chest CT scans. Yet, in practice, deep learning-based diffeomorphic methods struggle to capture large deformations between inspiratory and expiratory volumes, and therefore lack inverse consistency. Existing methods also fail to account for model uncertainty, which can be useful for improving performance. We propose an uncertainty-aware test-time adaptation framework for inverse consistent diffeomorphic lung registration. Our method uses Monte Carlo (MC) dropout to estimate spatial uncertainty that is used to improve model performance. We train and evaluate our method for inspiratory-to-expiratory CT registration on a large cohort of 675 subjects from the COPDGene study, achieving a higher Dice similarity coefficient (DSC) between the lung boundaries (0.966) compared to both VoxelMorph (0.953) and TransMorph (0.953). Our method demonstrates consistent improvements in the inverse registration direction as well with an overall DSC of 0.966, higher than VoxelMorph (0.958) and TransMorph (0.956). Paired t-tests indicate statistically significant improvements.","Deformable image registration (DIR) establishes a dense correspondence between two medical image volumes, playing a critical role in tasks such as disease phenotyping [1] and surgical guidance [2]. While DIR is widely used for image matching, its displacement fields lack guarantees of smoothness and invertibility, especially when handling large deformations. For instance, when registering chest computed tomography (CT) volumes at end-inspiration and end-expiration, large deformations could lead to inadequate displacements. This is further exacerbated in cases where inverse consistency between transformations is also required. Fig. 1: Inverse consistent test-time adaptation framework. During training, the network \mathcal{G}_{\theta} learns to predict SVF \boldsymbol{v}, from a fixed image \boldsymbol{I}_{\mathrm{F}}=\boldsymbol{I}_{\mathrm{FRC}} and a moving image \boldsymbol{I}_{\mathrm{M}}=\boldsymbol{I}_{\mathrm{TLC}}. The SVF is then integrated using the scaling and squaring (SS) method to get the final displacement field \mathbf{\Phi}_{1} [3]. During inference, two different pathways (forward and inverse) are defined for uncertainty estimation and adaptation, details of which are described below. The LDDMM framework allows for adaptation in inverse direction by simply negating the SVF (-\boldsymbol{v}) and then integrating it through SS. Large deformation diffeomorphic metric mapping (LDDMM) offers a framework for estimating transformations with theoretical guarantees of smoothness, differentiability, and invertibility, enabling consistent, one-to-one mappings even for images with large deformations [4]. This one-to-one mapping also preserves topological consistency across images. While several iterative optimization methods have been developed to estimate these diffeomorphisms, they remain computationally intensive and time-consuming in practice [5]. Recently, deep neural networks have been used to estimate diffeomorphic transformations that are faster and less computationally expensive at inference [6, 7, 8]. These methods use convolutional neural networks (CNNs) or vision transformer backbones to directly predict a stationary velocity field (SVF), which is integrated to estimate the displacement field [6]. While these methods utilize a theoretically informed framework (LDDMM) to estimate diffeomorphic transformations, they still lead to inadequate displacements in regions with large deformations which may not be readily invertible. Most state-of-the-art registration methods also overlook model uncertainty, which is an important measure of confidence associated with predictions. We believe that incorporating uncertainty estimation could identify low and high confidence regions within the predicted displacement fields that could be used for rapid model adaptation at test time. We propose an uncertainty-aware test-time adaptation framework for inverse consistent diffeomorphic lung image registration that can be used to enhance model performance in both forward and inverse registration directions. Using Monte Carlo (MC) dropout, we generate spatial uncertainty maps at inference and use them to adapt and optimize the model at test-time [9]. Our framework demonstrates robust adaptation for bidirectional registration from total lung capacity (TLC) CT scans to functional residual capacity (FRC) scans and vice versa."
https://arxiv.org/html/2411.07501v2,: Learned Augmented Residual Layer,"One of the core pillars of efficient deep learning methods is architectural improvements such as the residual/skip connection, which has led to significantly better model convergence and quality. Since then the residual connection has become ubiquitous in not just convolutional neural networks but also transformer-based architectures, the backbone of LLMs.In this paper we introduce Learned Augmented Residual Layer (LAuReL)—a novel generalization of the canonical residual connection—with the goal to be an in-situ replacement of the latter while outperforming on both model quality and footprint metrics. Our experiments show that using LAuReL can help boost performance for both vision and language models. For example, on the ResNet-50, ImageNet 1K task, it achieves 60\% of the gains from adding an extra layer, while only adding 0.003\% more parameters, and matches it while adding 2.6\times fewer parameters.","Model efficiency is of critical importance in the age of extremely large language and vision models. Even if a given model’s quality is good, its footprint metrics such as train-time compute required, inference latency, resident memory size, etc. dictate if it can be experimented with and/or deployed in real-world settings. These metrics are directly tied to the financial costs of deploying the model in production and user-perceived responsiveness of systems dependent on these models. Consequently, improving the Pareto-frontier of model quality vs footprint, via efficient deep learning methods has been an area of active research in the past few years. Areas of interests span from algorithmic techniques (Menghani, 2023), to efficient hardware (Sze et al., 2017), to best practices around model efficiency (Dehghani et al., 2022), etc. One of the core pillars of efficient deep learning methods is architectural improvements such as the residual/skip connection, which had led to significantly better model convergence and quality (He et al., ). Since then the residual connection has become ubiquitous in not just convolutional neural networks but also transformer-based architectures (Vaswani et al., 2017), the backbone of LLMs. In this paper we introduce learned augmented residual layer, LAuReL, which generalizes the canonical residual connection. Recall that deep-learning models with residual connections have a ‘block’ structure, with many blocks chained together between the input and final output; these could be convolution/identity blocks within a ResNet, a transformer block in a transformer encoder/decoder, etc. Within a block, a typical residual connection is given by: x_{i+1}=f(x_{i})+x_{i}. (1) Here, f(\cdot) can be any non-linear function such as attention, MLP, multiple non-linear layers, etc., x_{i} is the input to the said non-linear function, and x_{i+1} is the combined output of the non-linear function and the residual component. Refer to Figure 1 for an illustration. To simplify exposition, we ignore pre-processing functions such as layer norm, which can be folded into f(\cdot) without loss of generality. Figure 1: A standard residual connection. We assume the model to be divided into logical ‘blocks’, which is true for most modern architectures including transformers. The residual connection combines the output of a non-linear function f and the input to the said non-linear function. Here, f can be attention, MLP, or any other combination of non-linear layers."
https://arxiv.org/html/2411.07416v1,T2-only prostate cancer prediction by meta-learning from bi-parametric MR imaging,"Current imaging-based prostate cancer diagnosis requires both MR T2-weighted (T2w) and diffusion-weighted imaging (DWI) sequences, with additional sequences for potentially greater accuracy improvement. However, measuring diffusion patterns in DWI sequences can be time-consuming, prone to artifacts and sensitive to imaging parameters. While machine learning (ML) models have demonstrated radiologist-level accuracy in detecting prostate cancer from these two sequences, this study investigates the potential of ML-enabled methods using only the T2w sequence as input during inference time. We first discuss the technical feasibility of such a T2-only approach, and then propose a novel ML formulation, where DWI sequences - readily available for training purposes - are only used to train a meta-learning model, which subsequently only uses T2w sequences at inference. Using multiple datasets from more than 3,000 prostate cancer patients, we report superior or comparable performance in localising radiologist-identified prostate cancer using our proposed T2-only models, compared with alternative models using T2-only or both sequences as input. Real patient cases are presented and discussed to demonstrate, for the first time, the exclusively true-positive cases from models with different input sequences. Open-source code is available at https://github.com/wxyi057/MetaT2.","Various MR sequences are used in the image-based diagnosis of prostate cancer. Among these sequences, T2-weighted (T2w) images typically have shorter acquisition times, compared to other commonly used sequences such as diffusion-weighted imaging (DWI) with multiple b-values and dynamic contrast enhanced sequences. This fast acquisition is partly because efficient and robust T2w protocols are widely established and relatively standardised. For comparison, DWI protocols for prostate cancer diagnosis rely on specialised experience and, currently, exhibit higher variability in terms of sensitivity to prostate cancer, potentially because of variable factors including signal-to-noise ratio, susceptibility and rectal air artifacts [1]. The adoption of a T2-only diagnostic approach could substantially reduce the cost both in required expertise and imaging time, considerably streamlining prostate cancer patient care, compared with the recently-proposed machine learning (ML)-based approaches that require two or more sequences, e.g. [2, 3]. We argue that enabling T2-only prostate cancer diagnosis is not only desirable but also feasible, especially with modern ML models. First, DWI-visible tumours that are invisible (or challenging to identify by radiologists) on T2w were previously estimated to be \sim30% of all clinically significant cases [4, 5]. However, since no correlation has been established between radiologist false positives and DWI lesion invisibility (and such correlation is unlikely to be perfect), the actual number of cases missed by omitting DWI should be lower. This estimate is further complicated by the different yet unknown number of cases in radiologist false positive and false negative, which may be corrected by ML models (e.g. by learning underutilised inter-sequence and/or T2-to-cancer correlations). Furthermore, It is conceivable that the use of a single T2-only modality circumvents a number of challenges in multiparametric MR diagnosis, such as ground-truth label ambiguity due to inter-sequence spatial misalignment [6], variable DWI protocols and quality discussed above. Therefore, it is worth investigating ML models to localise prostate cancer on T2w images, but maintaining competitive accuracy over models using multiple sequences. This work focuses on developing a system that can best localize radiologist-identified cancer using only T2w images, in contrast to conventional diagnostic approaches that rely on multiple sequences. This is considered the first step to understand the feasibility of the T2-only approach, before developing models that are trained on and predict for histopathology labels. This step enables us to directly assess the model’s capability to identify and localize cancerous regions from T2w images using radiologist annotations, without the additional complexity introduced by histopathological ground-truth sampling uncertainty [7]. Such understanding is crucial for localization tasks requiring pixel-level annotations and may inform future development of histopathology-predicting models. Models developed in this work to predict radiologist labels can, on their own, enable a number of clinically useful applications including automating radiologist readings, and providing second opinions or assisting consensus during a radiology examination. To maximise the potential of predicting prostate cancer on T2w images, we propose 1) utilising existing DWI sequences during training - a form of learning using privileged information [8], 2) maximising the utilisation of the inference-omitted DWI sequences using conditional denoising diffusion models [9], and 3) developing a new meta-learning framework to effectively train and predict prostate cancer using only T2w as input during inference. This concludes our contributions in a) proposing a new clinical application, b) developing a new technical ML algorithm, and c) a set of rigorous evaluation experiments, with open-source code, using three cohorts from more than 3,000 prostate cancer patients, tested on both a publicly available PROMIS dataset [10] and an internal multicentre MR-Targeted data set from multiple clinical trials [11, 12, 13, 14]."
https://arxiv.org/html/2411.07391v1,Federated Learning Client Pruning for Noisy Labels,"Federated Learning (FL) enables collaborative model training across decentralized edge devices while preserving data privacy. However, existing FL methods often assume clean annotated datasets, impractical for resource-constrained edge devices. In reality, noisy labels are prevalent, posing significant challenges to FL performance. Prior approaches attempt label correction and robust training techniques but exhibit limited efficacy, particularly under high noise levels. This paper introduces ClipFL (Federated Learning Client Pruning), a novel framework addressing noisy labels from a fresh perspective. ClipFL identifies and excludes noisy clients based on their performance on a clean validation dataset, tracked using a Noise Candidacy Score (NCS). The framework comprises three phases: pre-client pruning to identify potential noisy clients and calculate their NCS, client pruning to exclude a percentage of clients with the highest NCS, and post-client pruning for fine-tuning the global model with standard FL on clean clients. Empirical evaluation demonstrates ClipFL’s efficacy across diverse datasets and noise levels, achieving accurate noisy client identification, superior performance, faster convergence, and reduced communication costs compared to state-of-the-art FL methods. Our code is available at https://github.com/MMorafah/ClipFL.","Edge devices such as IoTs and mobile devices are increasingly ubiquitous, constituting a new computational platform for machine learning. Despite holding vast real-world data, these billions of devices often withhold their data due to privacy concerns. Federated Learning (FL) emerges as a decentralized machine learning paradigm, enabling model training with the collaboration of multiple clients while preserving privacy [30, 16]. FL shows promise in enhancing model performance without necessitating data sharing and finds applications across diverse domains [16, 45, 34, 6, 13]. However, FL encounters significant performance degradation in the presence of data heterogeneity [11, 25, 61]. Recent advancements introduce FL optimizers tailored to address data heterogeneity, achieving faster convergence [24, 18, 50, 38]. Despite these advancements, the majority of prior works operate under the assumption of accurately labeled and clean client data. In practice, however, acquiring precisely annotated clean datasets is arduous and resource-intensive, especially for edge devices lacking ample resources. Consequently, the labels in their datasets often contain noise. Unfortunately, FL experiences substantial performance degradation when confronted with noisy clients [56]. Thus, developing FL frameworks resilient to noisy labels is imperative. Several prior works propose methods to identify and rectify noisy samples using the global model’s predictions [53, 56]. For instance, FedCorr [53] presents a multi-stage FL framework that identifies noisy clients by measuring the local intrinsic dimensionality (LID) and corrects noisy labels using global model predictions. In order to control the negative impact of noisy client in achieving a well-trained reliable global model prior to label correction stage, they incorporate several techniques including client fraction scheduling scheme and local proximal regularization with mix-up. However, this approach heavily relies on a well-performing global model, which is challenging to obtain in the presence of data heterogeneity and noisy clients, leading to inaccurate label correction and suboptimal performance. Other approaches aim to mitigate the impact of noisy clients through client weighting strategies and robust local training methods [9, 57, 15]. For example, RHFL [9] utilizes symmetric cross-entropy loss during local training and introduces a client confidence re-weighting scheme to counteract the adverse effects of noisy labels during collaborative learning. However, these methods demonstrate limited efficacy and poor convergence, especially under high noise levels. In this paper, we address the challenge of noisy labels in FL by adopting an alternative approach compared to prior works. Rather than mitigating or correcting noisy clients, we propose a novel “Federated Learning Client Pruning” framework called ClipFL. ClipFL identifies noisy clients and excludes them from the FL training process. Our framework comprises of three phases. In the initial phase, we identify noisy clients based on their performance on a clean validation dataset, tracking the frequency of noisy identifications as Noise Candidacy Score (NCS) of each client. To mitigate the negative impact of noisy clients during server-side model aggregation, we only aggregate the top-m clients with the highest validation accuracy. Subsequently, in the second phase, we prune p\% of the clients with the highest noise candidacy score (NCS) in a one-shot manner. Finally, in the third stage, we conduct standard FL on the remaining clean clients to further refine the global model. (a) Prior works typically involve incorporating noisy clients into FL and addressing their negative impact through methods such as correcting noisy labels, re-weighting noisy clients, or designing robust local training methods. (b) In contrast to prior works, ClipFL introduces the Noise Candidacy Score (NCS) for each client, enabling robust identification of noisy clients from the FL process. Figure 1. Comparison between our approach and prior works: Prior works often face challenges related to poor convergence and reliance on a well-performing global model as a pseudo-labeler that corrects noisy labels, which can be difficult to obtain, especially in the presence of high noise levels and data heterogeneity (see Section 5.3). In contrast, ClipFL offers consistent performance improvements and reduces communication costs by robustly identifying noisy clients and excluding them from the FL process. We validate ClipFL on diverse datasets with varying noise levels for both IID and Non-IID data partitions, yielding several key observations: (1) ClipFL accurately identifies noisy clients with at least 80% accuracy on most of the cases. (2) ClipFL outperforms state-of-the-art (SOTA) FL optimizers without excluding noisy clients. (3) ClipFL surpasses existing FL methods addressing noisy labels, underscoring the effectiveness of our excluding approach. (4) ClipFL demonstrates faster convergence and reduced communication costs compared to both SOTA vanilla FL optimizers and FL methods designed to counteract noisy labels. We make our code publicly available at https://github.com/MMorafah/ClipFL. Contribution. Our contributions are threefold: • We introduce ClipFL, a novel federated learning client excluding method to address the challenge of noisy labels in FL. • ClipFL distinguishes clean clients from noisy ones through evaluation on a clean validation dataset and aggregates only the top-performing clients to mitigate the impact of noisy clients. • We empirically evaluate ClipFL across different datasets with varying noise levels for both IID and Non-IID data partitions, demonstrating significant performance improvements and reduced communication costs over SOTA FL methods. Organization. The rest of the paper is organized as follows: In Section 2, we discuss related works. Section 3 presents the background and problem formulation. Our proposed method is introduced in Section 4. Section 5 details our experimental results. We discuss our implementation and hyperparameters in Section 6. Finally, we conclude our work in Section 7."
https://arxiv.org/html/2411.07335v1,"Multimodal Fusion Balancing Through
Game-Theoretic Regularization","Multimodal learning can complete the picture of information extraction by uncovering key dependencies between data sources. However, current systems fail to fully leverage multiple modalities for optimal performance. This has been attributed to modality competition, where modalities strive for training resources, leaving some underoptimized. We show that current balancing methods struggle to train multimodal models that surpass even simple baselines, such as ensembles. This raises the question: how can we ensure that all modalities in multimodal training are sufficiently trained, and that learning from new modalities consistently improves performance? This paper proposes the Multimodal Competition Regularizer (MCR), a new loss component inspired by mutual information (MI) decomposition designed to prevent the adverse effects of competition in multimodal training. Our key contributions are: 1) Introducing game-theoretic principles in multimodal learning, where each modality acts as a player competing to maximize its influence on the final outcome, enabling automatic balancing of the MI terms. 2) Refining lower and upper bounds for each MI term to enhance the extraction of task-relevant unique and shared information across modalities. 3) Suggesting latent space permutations for conditional MI estimation, significantly improving computational efficiency. MCR outperforms all previously suggested training strategies and is the first to consistently improve multimodal learning beyond the ensemble baseline, clearly demonstrating that combining modalities leads to significant performance gains on both synthetic and large real-world datasets.","Exploiting multimodal data has made significant progress, with advances in generalizable representations and larger datasets enabling solutions to previously unattainable tasks [27; 29; 37; 43; 42; 44; 50; 53; 62]. However, studies indicate that multimodal data is often utilized suboptimally, underperforming compared to ensemble unimodal models or even the best single modality [55; 60]. The expectation that adding a modality should improve performance, assuming independent errors and above-chance predictive power [17], is frequently contradicted in practice. Huang et al. [20] attribute this issue to modality competition, where one modality quickly minimizes training error, misdirecting and suppressing the learning of others. Factors like noise levels, relationship complexity with the target, feature dimensionality, and data quality can cause one modality to fit faster than another. This imply that adding task-relevant information doesn’t guarantee better performance, primarily due to complications during training. To address these issues, it’s crucial to monitor each modality’s contribution during training and apply corrective measures. Several balancing strategies have been proposed to tackle this issue [5; 6; 9; 10; 21; 26; 28; 40; 41; 54; 55; 57; 60]. A central aspect of these methods is estimating each modality’s contribution to the output. Most assume distributional independence between modalities on predicting the target, measuring contribution via unimodal performance [60; 26; 40; 6; 54]. Some methods bypass this assumption by estimating influence based on prediction differences between original and perturbed inputs [28; 21; 10]. Perturbations can take various forms, such as zeroing values [28], adding Gaussian noise [10], or using task-specific augmentations [21; 31]. These methods aim to amplify a modality’s influence by increasing the impact of perturbations on the output. However, this also makes the network more sensitive to these changes (e.g., noise), risks becoming overly reliant on the perturbations, and struggles to scale when multiple perturbations are required. Additionally, increasing the contribution of one modality can be achieved by overshadowing others, leading to an imbalance that undermines overall performance and makes the objective counterproductive. Given these challenges, how can we design an efficient regularization method that addresses multimodal competition, ensuring balanced and effective learning across all modalities? Figure 1: (Left) Illustration of the conditional mutual information (\operatorname{CMI}) terms, \operatorname{CMI}_{1}:I(X_{1};Y\mid X_{2}) and \operatorname{CMI}_{2}:I(X_{2};Y\mid X_{1}), representing the unique contributions (U_{1} and U_{2}) of each modality to the target. The shared task-relevant information (S) between the modalities is defined as I(X_{1};X_{2})-I(X_{1};X_{2}\mid Y). (Right) Accuracy as a function of the ratio between the unique information (U_{1}) from modality X_{1} and the shared information (S) between the modalities. Synthetic data are generated as X_{1}=N_{1}+Y,X_{2}=N_{1}+Y where N_{1},N_{2} are independent noise for each modality. We consider S the percentage of the datapoints that both modalities have information about the label Y, U_{1} and U_{2} when only one has with the other modality equating to noise for those datapoints. In the experiment we keep U_{2} constant while changing U_{1} and S. As U_{1} increases and S decreases, accuracy deteriorates, reflecting intensified multimodal competition. Among the various methods, including Singleloss, Multiloss, Ensemble, unimodally pretrained and finetuned encoders (Uni-Pre Fine), OGM [40], AGM [28], and MLB [26]our regularization method MCR demonstrates a slower decline in accuracy. For further detals prease refer to Section 4.1 In this paper, we introduce the Multimodal Competition Regularizer (\operatorname{MCR}), a loss function designed to promote the exploration of task-relevant information across all available modalities. By decomposing the joint mutual information (\operatorname{MI}), we separately model shared and unique task-relevant information within the modalities. To efficiently capture the unique information from each modality, we employ a computationally inexpensive permutation-based approach. Our method maximizes the lower bounds of each MI term to encourage the network to learn both shared and unique information, while minimizing upper bounds on terms to suppress task-irrelevant information. We frame the problem in a game-theoretic setting, exploring strategies that involve both collaboration and competition among modalities to address the conflicting objectives that arise when increasing all modalities’ contributions simultaneously. This approach allows their contributions to adapt dynamically, achieving balance during training. We extensively evaluate \operatorname{MCR} on synthetic datasets and several established real-world multimodal benchmarks, including action recognition on AVE [49] and UCF [45], emotion recognition on CREMA-D [4], human sentiment on CMU-MOSI [61], human emotions on CMU-MOSEI [63] and egocentric action recognition on Something-Something [15]. Our results demonstrate that \operatorname{MCR} is the first balancing method to significantly improve supervised multimodal training over the ensemble baseline across a variety of datasets and models. Our key contributions are summarized as follows: 1. An analysis of multimodal competition, defining the error increase caused in multimodal training, while demonstrating in our results that most previous methods do no outperform simple baselines, such as unimodal ensembles. 2. A novel multimodal training strategy, \operatorname{MCR}, designed to regularize multimodal competition, which includes: • Defining lower and upper bounds of the \operatorname{MI} terms, encouraging the exploration of information across all modalities. • Introducing a game-theoretic perspective where modalities form the players that compete for training resources, assisting the regularization through balancing the corresponding MI terms. • Suggesting latent-space perturbations as an efficient way to estimate the lower bound of the \operatorname{CMI} reducing the computational cost of multiple forward passes."
https://arxiv.org/html/2411.07232v2,Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion Models,"Adding Object into images based on text instructions is a challenging task in semantic image editing, requiring a balance between preserving the original scene and seamlessly integrating the new object in a fitting location. Despite extensive efforts, existing models often struggle with this balance, particularly with finding a natural location for adding an object in complex scenes. We introduce Add-it, a training-free approach that extends diffusion models’ attention mechanisms to incorporate information from three key sources: the scene image, the text prompt, and the generated image itself. Our weighted extended-attention mechanism maintains structural consistency and fine details while ensuring natural object placement. Without task-specific fine-tuning, Add-it achieves state-of-the-art results on both real and generated image insertion benchmarks, including our newly constructed ”Additing Affordance Benchmark” for evaluating object placement plausibility, outperforming supervised methods. Human evaluations show that Add-it is preferred in over 80% of cases, and it also demonstrates improvements in various automated metrics. Our code and data will be available at: https://research.nvidia.com/labs/par/addit/","Adding objects to images based on textual instructions is a challenging task in image editing, with numerous applications in computer graphics, content creation and synthetic data generation. A creator may want to use text-to-image models to iteratively build a complex visual scene, while autonomous driving researchers may wish to draw pedestrians in new scenarios for training their car-perception system. Despite considerable recent research efforts on text-based editing, this particular task remains a challenge . When adding objects, one needs to preserve the appearance and structure of the original scene as closely as possible, while inserting the novel objects in a way that appears natural. To do so, one must first understand affordance—the deep semantic knowledge of how people and objects interact, in order to position an object in a reasonable location. For brevity, we call this task Image Additing. Several studies (Hertz et al., 2022; Meng et al., 2022) tried addressing this task by leveraging modern text-to-image diffusion models. This is a natural choice since these models embody substantial knowledge about arrangements of objects in scenes and support open-world conditioning on text. While these methods perform well for various editing tasks, their success rate for adding objects is disappointingly low, failing to align with both the source image and the text prompt. In response, another set of methods took a more direct learning approach (Brooks et al., 2023; Zhang et al., 2023; Canberk et al., 2024). They trained deep models on large image editing datasets, pairing images with and without an object to add. However, these often struggle with generalization beyond their training data, falling short of the general nature of the original diffusion model itself. This typically manifests as a failure to insert the new object, the creation of visual artifacts, or more commonly – failing to insert the object in the correct place, i.e. struggling with affordances. Indeed, we remain far from achieving open-world object insertions from text instructions. Here we describe an open-world, training-free method that can successfully leverage the knowledge stored in text-to-image foundation models, to naturally add objects into images. As a guiding principle, we propose that addressing the affordance challenge requires methods to carefully balance between the context of the existing scene and the instructions provided in the prompt. We achieve this by: first, extending the multi-modal attention mechanism (Esser et al., 2024) of recent T2I diffusion models to also consider tokens from a source image; and second, controlling the influence of each multi-modal attention component: the source image, the target image and the text prompt. A main contribution of this paper is a mechanism to balance these three sources of attention during generation. We also apply a structure transfer step and introduce a novel subject-guided latent blending mechanism to preserve the fine details of the source image while enabling necessary adjustments, such as shadows or reflections. Our full pipeline is shown at fig. 2. We name our method Add-it. Image Additing methods typically face three main failure modes: neglect, appearance, and affordance. While current CLIP-based evaluation protocols can partially assess neglect and appearance, there is a lack of reliable methods for evaluating affordance. To address this gap, we introduce the “Additing Affordance Benchmark,” where we manually annotate suitable areas for object insertion in images and propose a new protocol specifically designed to evaluate the plausibility of object placement. Additionally, we introduce a metric to capture object neglect. Add-it outperforms all baselines, improving affordance from 47% to 83%. We also evaluate our method on an existing benchmark (Sheynin et al., 2023) with real images, as well as our newly proposed Additing Benchmark for generated images. Add-it consistently surpasses previous methods, as reflected by CLIP-based metrics, our object inclusion metric, and human preference, where our method is favored in over 80% of cases, even against methods specifically trained for this task. Our contributions are as follows: (i) We propose a training-free method that achieves state-of-the-art results on the task of object insertion, significantly outperforming previous methods, including supervised ones trained for this task. (ii) We analyze the components of attention in a modern diffusion model and introduce a novel mechanism to control their contribution, along with novel Subject Guided Latent Blending and a noise structure transfer. (iii) We introduce an affordance benchmark and a new evaluation protocol to assess the plausibility of object insertion, addressing a critical gap in current Image Additing evaluation methods."
https://arxiv.org/html/2411.07231v1,Watermark Anything with Localized Messages,"Image watermarking methods are not tailored to handle small watermarked areas. This restricts applications in real-world scenarios where parts of the image may come from different sources or have been edited. We introduce a deep-learning model for localized image watermarking, dubbed the Watermark Anything Model (WAM). The WAM embedder imperceptibly modifies the input image, while the extractor segments the received image into watermarked and non-watermarked areas and recovers one or several hidden messages from the areas found to be watermarked. The models are jointly trained at low resolution and without perceptual constraints, then post-trained for imperceptibility and multiple watermarks. Experiments show that WAM is competitive with state-of-the art methods in terms of imperceptibility and robustness, especially against inpainting and splicing, even on high-resolution images. Moreover, it offers new capabilities: WAM can locate watermarked areas in spliced images and extract distinct 32-bit messages with less than 1 bit error from multiple small regions – no larger than 10% of the image surface – even for small 256\times 256 images.","Invisible image watermarking embeds information into image pixels in a way that is imperceptible to the human eye and yet robust. It was initially developed for intellectual property and copy protection, such as by Hollywood studios for DVDs. However, the applications of watermarking are evolving, particularly in light of the recent development of generative AI models (Kušen and Strembeck, 2018). Regulatory acts such as the White House executive order (USA, 2023), the Californian bill, the EU AI Act (Parliament and Council, 2024), and Chinese AI governance rules (of the People’s Republic of China, 2023) require AI-generated content to be easily identifiable. They all cite watermarking as either compulsory or a recommended measure to detect and label AI-generated images. Image splicing is one of the most common manipulations, whether applied for benign or malicious purposes (Christlein et al., 2012; Tralic et al., 2013). Splicing involves adding text or memes on a large portion of the image or extracting parts of images and overlaying them on others (Douze et al., 2021). It can bypass the state-of-the-art watermarking techniques, which take one global decision per image under scrutiny. Indeed, in traditional watermarking, the watermark signal fades away and is no longer detected as the surface of the watermarked area decreases. Besides, these techniques poorly answer the paradoxical question of deciding whether an image should be considered watermarked if only a small part carries the watermark. A positive decision triggered by a small area might be unfair to artists who use AI models for inpainting or outpainting. On the other hand, not being robust enough to splicing opens the door to easy removal. Figure 1: Overview. (a) The embedder creates an imperceptible image modification. (b) Traditional transformations (cropping, JPEG compression, etc.) and/or advanced manipulations (mixing watermarked and non-watermarked images, inpainting, etc.) may be applied to the image. (c) The extraction creates a segmentation map of watermarked parts and retrieves one or several messages. To address these issues, this paper redefines watermarking as a segmentation task, giving birth to the Watermark Anything Models (WAM). Our motivation is to disentangle the strength of the watermark signal from its pixel surface, in contrast to traditional watermarking. More precisely, the WAM extractor detects if the watermark is present and extracts a binary string for every pixel rather than predicting a message for the whole image. These outputs are post-processed according to the final task. For global detection, the image is deemed watermarked if the proportion of watermarked pixels exceeds a user-defined threshold. For global decoding, a majority vote recovers the hidden message. A new application, out of the reach of traditional robust watermarking, is the localization of watermarked areas and the extraction of multiple hidden messages. For that purpose, we choose to apply the DBSCAN clustering algorithm over the pixel-level binary strings because it does not require any prior on the number of watermarks (or centroids). This is detailed in Sec. 3. These new functionalities require a training with new objectives, that is split into two phases. The first phase pre-trains the embedder and extractor models for low-resolution images. It essentially targets the robustness criterion. The embedder encodes a n_{\text{bits}}-bit message into a watermark signal that is added to the original image. The augmenter randomly masks the watermark in parts of the image and augments the result with common processing techniques (e.g., cropping, resizing, compression). The extractor then outputs a (1+n_{\text{bits}})-dimensional vector per pixel to predict the parts of the image that are watermarked and decode the corresponding messages. Detection and decoding losses are used as training objectives. The second training phase targets the following new objectives: (1) minimize the watermark’s visibility in alignment with the human visual system, (2) allow for multiple messages within the same image. This two-stage training is less prone to instability, compared to previous use of adversarial networks and divergent objectives (Zhu et al., 2018). It also trains the extractor on both watermarked and non-watermarked images, for the first time in the literature. This increases the performance and the robustness of the detection. We first compare WAM with state-of-the-art methods for regular tasks of watermark detection and decoding on low and high-resolution images. Our results show that WAM achieves competitive performance in terms of imperceptibility and robustness. To further highlight the advantages of WAM, we then evaluate its performance on tasks that are not considered in the literature. Namely, we evaluate the localization accuracy between the predicted watermarked areas and the original mask and assess the ability to detect and decode multiple watermarks in a single image. For instance, when hiding five 32-bit messages, each in a 10% area of the image, detection of watermarked areas achieves more than 85% mIoU, even after images are horizontally flipped and the contrast adjusted, and bit accuracy (for a total of 160 bits) achieves more than 95% under the same augmentation (Sec. 5.5). In summary, our contributions are: • the definition of watermarking as a segmentation task; • a two-stage training able to strike a good trade-off between invisibility and robustness even for multiple watermarks and high-resolution images; • WAM, an embedder/extractor model competitive with state-of-the-art methods; • the highlight of new capabilities, localization of watermarks and extraction of multiple messages as depicted in Fig. 1, together with specially designed evaluations."
https://arxiv.org/html/2411.07229v1,Learning from Limited and Imperfect Data,"Deep Neural Networks have demonstrated orders of magnitude improvement in capabilities over the years after AlexNet won the ImageNet challenge in 2012. One of the major reasons for this success is the availability of large-scale, well-curated datasets. These datasets (e.g., ImageNet, MSCOCO, etc.) are often manually balanced across categories (classes) to facilitate learning of all the categories. This curation process is often expensive and requires throwing away precious annotated data to balance the frequency across classes. This is because the distribution of data in the world (e.g., internet, etc.) significantly differs from the well-curated datasets and is often over-populated with samples from common categories. The algorithms designed for well- curated datasets perform suboptimally when used to learn from imperfect datasets with long-tailed imbalances and distribution shifts. For deep models to be widely used, getting away with the costly curation process by developing robust algorithms that can learn from real-world data distribution is necessary. Toward this goal, we develop practical algorithms for Deep Neural Networks that can learn from limited and imperfect data present in the real world. These works are divided into four segments, each covering a scenario of learning from limited or imperfect data. The first part of the works focuses on Learning Generative Models for Long-Tail Data, where we mitigate the mode-collapse for tail (minority) classes and enable diverse aesthetic image generations as head (majority) classes. In the second part, we enable effective generalization on tail classes through Inductive Regularization schemes, which allow tail classes to generalize as the head classes without enforcing explicit generation of images. In the third part, we develop algorithms for Optimizing Relevant Metrics compared to the average accuracy for learning from long-tailed data with limited annotation (semi-supervised), followed by the fourth part, which focuses on the effective domain adaptation of the model to various domains with zero to very few labeled samples.","Extended Abstract Generative Models for Long-Tail Data. We first evaluate generative models’ performance, specifically variants of Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) on long-tailed datasets. The GAN variants suffer from either mode-collapse or miss-class modes during generation. To mitigate this, we propose Class Balancing GAN with a Classifier in the Loop (Rangwani et al., 2021b), which uses a classifier to asses the modes in generated images and regularizes GAN to produce all classes equally. To alleviate the dependence on the classifier, we take a closer look at model behavior at mode collapse and observe that spectral norm explosion of Batch Norm parameters correlates with mode collapse. We develop an inexpensive group Spectral Regularizer (gSR) (Rangwani et al., 2022) which mitigates the spectral collapse and significantly improves the SotA conditional GANs (SNGAN and BigGAN) performance on long-tailed data. However, we observed that class confusion is present in the generated images due to gSR norm regularization for large datasets. For this, in our latest work NoisyTwins (Rangwani∗ et al., 2023), we factor the latent space as distinct Gaussian by design for each class, enforcing class consistency and intra-class diversity using a contrastive approach (BarlowTwins). This helps us to scale high-resolution StyleGANs for thousand class long-tailed datasets of ImageNet-LT and iNaturalist2019, achieving State-of-the-Art (SotA) results while maintaining class-consistency. Inducting Regularization Schemes for Long-Tailed Data. While Data Generation is promising for improving classification models on tail classes, it often comes with the cost of training an auxiliary generative model. Hence, lightweight techniques such as higher loss weights for tail classes (loss re-weighting) while training CNNs, is practical to improve performance on the minority classes. However, we observe that the model converges to saddle point instead of minima for tail classes, hindering generalization. We show that inducing inductive bias of escaping saddles and converging to minima for tail classes, using Sharpness Aware Minimization (SAM) significantly improves performance on tail classes (Rangwani∗ et al., 2022a). Despite inductive regularizations, training Vision Transformers (ViTs) for long-tail recognition is still challenging due to the complete lack of inductive biases such as locality of features, making their training data intensive. We propose DeiT-LT (Rangwani et al., 2024), which introduces OOD and low-rank distillation from CNNs, to induce CNN-like robustness into scalable ViTs. Semi-Supervised Learning by Optimizing Practical Metrics. The above methods work in supervised long-tail learning, where they avoid throwing off the annotated data. However, the real benefit of long-tailed methods could be leveraged when they utilize the extensive unlabeled data present (i.e. semi-supervised setting). For this, we introduce a paradigm where we measure the performance using relevant metrics such as worst-case recall and recall H-mean on a held-out set, and we use their feedback to learn in a semi-supervised long-tailed setting. We introduce Cost-Sensitive Self Training (CSST) (Rangwani∗ et al., 2022), which generalizes self-training based semi-supervised learning (e.g. FixMatch, etc.) to the long-tail setting with strong guarantees and empirical performance. The general trend these days is to use self-supervised pre-training to obtain a robust model and then fine-tune it. In this setup, we introduce SelMix (Ramasubramanian∗ et al., 2023), an inexpensive fine-tuning technique to optimize the relevant metrics using pre-trained models. In SelMix, we relax the assumption that unlabeled distribution is similar to the labeled one, making models robust to distribution shifts. Efficient Domain Adaptation. The long-tail learning algorithms focus on the limited data setup and improving in-distribution generalization. However for practical usage, the model must learn from imperfect data and perform well across various domains. Towards this goal, we develop Submodular Subset Selection for Adversarial Domain Adaptation (Rangwani et al., 2021a), which carefully selects a few samples to be labeled for maximally improving model performance in the target domain. To further improve the efficiency of the Adaptation procedure, we introduce Smooth Domain Adversarial Training (SDAT) (Rangwani∗ et al., 2022b), which converges to generalizable smooth minima. The smooth minimum enables efficient and effective model adaptation across domains and tasks."
https://arxiv.org/html/2411.07205v1,DLCR: A Generative Data Expansion Framework via Diffusion for Clothes-Changing Person Re-ID,"With the recent exhibited strength of generative diffusion models, an open research question is if images generated by these models can be used to learn better visual representations. While this generative data expansion may suffice for easier visual tasks, we explore its efficacy on a more difficult discriminative task: clothes-changing person re-identification (CC-ReID). CC-ReID aims to match people appearing in non-overlapping cameras, even when they change their clothes across cameras. Not only are current CC-ReID models constrained by the limited diversity of clothing in current CC-ReID datasets, but generating additional data that retains important personal features for accurate identification is a current challenge. To address this issue we propose DLCR, a novel data expansion framework that leverages pre-trained diffusion and large language models (LLMs) to accurately generate diverse images of individuals in varied attire. We generate additional data for five benchmark CC-ReID datasets (PRCC, CCVID, LaST, VC-Clothes, and LTCC) and increase their clothing diversity by 10x, totaling over 2.1M images generated. DLCR employs diffusion-based text-guided inpainting, conditioned on clothing prompts constructed using LLMs, to generate synthetic data that only modifies a subject’s clothes while preserving their personally identifiable features. With this massive increase in data, we introduce two novel strategies - progressive learning and test-time prediction refinement - that respectively reduce training time and further boosts CC-ReID performance. We validate our method through extensive ablations and experiments and show massive improvements when training previous CC-ReID methods with our generated data. On the PRCC dataset, we obtain a large top-1 accuracy improvement of 11.3\% by training CAL, a previous state of the art (SOTA) method, with DLCR-generated data. We publicly release our code and generated data for each dataset here: https://github.com/CroitoruAlin/dlcr.","A majority of computer vision research can be categorized as discriminative methods, wherein improved or novel model architectures are proposed and shown to learn better visual representations than their predecessors. These methods operate under the assumption that their training datasets are sufficiently large and diverse, with the limiting factor being the model’s ability to learn the desired objective. However, when there is a lack in diversity in the commonly used datasets for some task, the limiting factor in this case becomes the training data itself. Collecting additional in-domain data can be both difficult and expensive, and only recently have generative models become viable enough to generate additional, high-fidelity training data (which we refer to as generative data expansion). Generative data expansion has been previously explored for supervised or self-supervised learning on easier discriminative tasks, such as ImageNet classification [3, 50]. While there are other unexplored, more difficult discriminative visual tasks that may benefit even more so from generative data expansion, the difficulty lies in generating accurate, in-domain data. In this work, we tackle this problem in the context of CC-ReID, which is difficult in nature and, due to its relative infancy, a majority of datasets suffer from only containing a small number (2-5) of clothing outfits per subject [52, 58, 39, 15, 45]. In Person Re-identification [60, 61], the primary objective is to match an individual across images or videos captured by cameras at different times and locations. This task becomes notably challenging due to disparities in the body poses, lighting, backgrounds, viewpoints, and potential occlusions present across cameras. Traditional Re-ID approaches [5, 8, 38], also known as short-term Re-ID, assume that individuals are observed for relatively brief durations and that their attire remains constant throughout videos. However, these assumptions often fall short of practical real-world scenarios. For instance, people’s clothes can change when captured across different camera feeds at different times and days. Consequently, a new branch of Re-ID, known as clothes-changing Re-ID [39, 15, 24, 17], has recently emerged to reliably identify people even when their clothing constantly change over extended periods of time. Recent CC-ReID works commonly make discriminative modifications to traditional Re-ID model architectures and loss formulations to disentangle structural (clothes-independent) and appearance information (clothes-dependent) in the learned feature space. However, these discriminative approaches are inherently constrained by the aforementioned low clothing variations present in current CC-ReID datasets. Figure 1: Best viewed with zoom. (Left) Comparing alternative data expansion approaches to our method. Simply changing the color of the clothes using our clothing masks preserves the subject’s ID information, but lacks diversity. Using standard diffusion without inpainting can sometimes introduce diversity, but the ID-related information in an image is destroyed. Our method strongly increases clothing diversity, even with specific and difficult prompts, and preserves the subject’s ID-related information for effective CC-ReID. (Right) DLCR significantly increases the size of training data for five benchmark CC-ReID datasets, generating over 2.1M diverse, clothes-changed images. To address these issues, in this work we propose an effective data expansion framework for CC-ReID (named DLCR) that increases the number of clothing variations in CC-ReID datasets by intelligently harnessing generative AI and foundation models (Diffusion and large Language models for CC-ReID). DLCR consists of two stages: (1) Data Generation and (2) Re-ID Training and Prediction Refinement. In the first stage, our objective is to enrich a given CC-ReID training dataset by generating additional samples where clothing items are artificially changed through inpainting. With more diverse training images of a particular person in different clothes, a CC-ReID model will learn better clothes-invariant person features during training. To achieve this objective, we utilize diffusion models [20, 42, 6, 36], which are capable of generating realistic images and inpainting for image editing. However, diffusion models often struggle with preserving the intricate details of a person’s identity during image generation (Fig. 1). We overcome this by leveraging a human parsing method [26] to produce binary masks that mark only the clothing regions in an image. We then use this binary mask to retain ID-specific portions in an image during diffusion inpainting, such as the face, hair, and body shape, thereby only augmenting the subject’s clothes. Furthermore, existing CC-ReID datasets only use scalar values for each unique clothes ID and lack textual descriptions of specific clothing items, preventing the use of text-guided diffusion models for data generation. Thus, to overcome this issue, we generate explicit clothing descriptions by jointly leveraging a large visual-language model (LLaVA [32]) and a large language model (LLaMA [51]). In the second stage of DLCR, we propose two effective strategies in which our generated data can be sophisticatedly utilized during CC-ReID training and testing. Firstly, we train various CC-ReID models using our generated data from stage 1 and show how our enriched training data improves performance in clothes-changing settings. To accommodate both the large increase in data and the higher clothing variety of the generated samples, we employ a progressive learning strategy during training, where we gradually introduce the generated clothing variations of a person. This allows for training on 10x more data without drastically increasing training time, as well as further assists the model in learning better clothes-invariant person features. During inference, we apply our prediction refinement strategy which is specifically tailored for a CC-ReID retrieval task. We create different variations of a given query by artificially changing the subject’s attire, then ensemble their retrieved similarity scores from the gallery to obtain a refined final prediction. We summarize our contributions as follows: • DLCR is the first to implement a text-guided diffusion approach, in conjunction with foundational language models, to synthesize multiple images of a person with different clothes in a CC-ReID dataset (Sec. 3.1). • We propose a progressive learning strategy during training that judiciously combines our generated data with the original training set, improving performance and heavily reducing training time (Sec. 3.2.1). • We employ a novel prediction refinement strategy at test-time, where variants of a given query image are created by changing the subject’s clothes, and their retrieval scores are ensembled to further boost performance (Sec. 3.2.2). • We demonstrate the effectiveness of our method on four benchmark CC-ReID datasets (Sec. 4.1). Our method also yields a significant performance boost when existing approaches (both standard Re-ID and CC-ReID) utilize our generated data during training (Sec. 4.2). • We publicly release our code and generated CC-ReID data, totaling over 2.1M generated images, for future use."
https://arxiv.org/html/2411.07199v1,OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision,"Instruction-guided image editing methods have demonstrated significant potential by training diffusion models on automatically synthesized or manually annotated image editing pairs. However, these methods remain far from practical, real-life applications. We identify three primary challenges contributing to this gap. Firstly, existing models have limited editing skills due to the biased synthesis process. Secondly, these methods are trained with datasets with a high volume of noise and artifacts. This is due to the application of simple filtering methods like CLIP-score. Thirdly, all these datasets are restricted to a single low resolution and fixed aspect ratio, limiting the versatility to handle real-world use cases. In this paper, we present Omni-Edit, which is an omnipotent editor to handle seven different image editing tasks with any aspect ratio seamlessly. Our contribution is in four folds: (1) Omni-Edit is trained by utilizing the supervision from seven different specialist models to ensure task coverage. (2) we utilize importance sampling based on the scores provided by large multimodal models (like GPT-4o) instead of CLIP-score to improve the data quality. (3) we propose a new editing architecture called EditNet to greatly boost the editing success rate, (4) we provide images with different aspect ratios to ensure that our model can handle any image in the wild. We have curated a test set containing images of different aspect ratios, accompanied by diverse instructions to cover different tasks. Both automatic evaluation and human evaluations demonstrate that Omni-Edit can significantly outperform all the existing models. Our code, dataset and model will be available at https://tiger-ai-lab.github.io/OmniEdit/","Image editing, particularly when following user instructions to apply semantic transformations to real-world photos, has seen significant advancements. Recently, text-guided image editing (Brooks et al., 2023) has gained prominence over traditional methods such as mask-based or region-based editing (Meng et al., 2022). With the rise of diffusion models (Rombach et al., 2022; Podell et al., 2024; Chen et al., 2024a; Sauer et al., 2024), numerous diffusion-based image editing techniques have emerged. Generally, they can be roughly divided into two types: (1) Inversion-based methods (Parmar et al., 2023; Kawar et al., 2023; Gal et al., 2023; Xu et al., 2023; Tumanyan et al., 2023; Tsaban & Passos, 2023) propose to perform zero-shot image editing by inverting the diffusion process and manipulating the attention map in the intermediate diffusion steps to achieve desired editing goal. (2) End-to-end methods (Brooks et al., 2023; Zhang et al., 2024a; Sheynin et al., 2024; Zhao et al., 2024; Fu et al., 2024) propose to fine-tune an existing diffusion model on large-scale image editing pairs to learn the editing operation in an end-to-end fashion. End-to-end methods have generally achieved better performance than inversion-based methods and gained higher popularity. Table 1: Comparison of Omni-Edit with all the existing end-to-end image editing models. The scores are based on a preliminary studies on around 50 prompts. Property InstructP2P MagicBrush UltraEdit MGIE HQEdit CosXL Omni-Edit Training Dataset Properties Real Image? ✗ ✓ ✓ ✓ ✗ ✗ ✓ Any Res? ✗ ✗ ✗ ✗ ✗ ✗ ✓ High Res? ✗ ✗ ✗ ✗ ✓ ✗ ✓ Fine-grained Image Editing Skills Obj-Swap \faStar\faStar\faStarO \faStar\faStar\faStarO \faStar\faStar\faStarO \faStar\faStarHalfO\faStarO \faStar\faStar\faStarO \faStar\faStarO\faStarO \faStar\faStar\faStarHalfO Obj-Add \faStar\faStarO\faStarO \faStar\faStar\faStarO \faStar\faStarO\faStarO \faStar\faStarHalfO\faStarO \faStar\faStarO\faStarO \faStar\faStarO\faStarO \faStar\faStar\faStarHalfO Obj-Remove \faStar\faStarO\faStarO \faStar\faStar\faStarO \faStar\faStarO\faStarO \faStar\faStarHalfO\faStarO \faStar\faStarO\faStarO \faStar\faStarO\faStarO \faStar\faStar\faStarHalfO Attribute \faStar\faStar\faStarO \faStar\faStarO\faStarO \faStar\faStar\faStarO \faStar\faStarHalfO\faStarO \faStar\faStarO\faStarO \faStar\faStarO\faStarO \faStar\faStar\faStarHalfO Back-Swap \faStar\faStar\faStarO \faStar\faStar\faStarO \faStar\faStar\faStarO \faStar\faStarHalfO\faStarO \faStar\faStar\faStarO \faStar\faStar\faStarO \faStar\faStar\faStarHalfO Environment \faStar\faStarO\faStarO \faStar\faStarO\faStarO \faStar\faStarO\faStarO \faStar\faStarHalfO\faStarO \faStar\faStarO\faStarO \faStar\faStar\faStarO \faStar\faStar\faStarHalfO Style \faStar\faStar\faStarO \faStar\faStarO\faStarO \faStar\faStar\faStarO \faStar\faStarHalfO\faStarO \faStar\faStarO\faStarO \faStar\faStar\faStarHalfO \faStar\faStar\faStarHalfO Despite their effectiveness, end-to-end methods face a significant limitation: the scarcity of human-annotated image editing pairs. As a result, all current end-to-end approaches depend on synthetic training data. For instance, existing datasets are synthesized using techniques such as Prompt2Prompt (Hertz et al., 2023) or mask-based editing models like SD-Inpaint (Rombach et al., 2022), and DALLE-2/3 (Ramesh et al., 2022; Betker et al., 2023). However, these synthetic data generation pipelines exhibit significant biases, resulting in the following limitations: Limited Editing Capabilities: The synthetic data is heavily influenced by the underlying generation models. For example, Prompt2Prompt struggles with localized edits, such as adding, removing, or swapping objects, while SD-Inpaint and DALLE-2 are ineffective at global edits, such as style or background changes. As a result, models trained on such data inherit these limitations. Poor Data Quality Control: Most approaches use simplified filtering mechanisms like CLIP-score (Radford et al., 2021) or DINO-score (Caron et al., 2021) to automatically select training samples. However, recent studies (Ku et al., 2024) show that these metrics exhibit poor correlation with actual data quality, leading to suboptimal training data that negatively impacts the model. Lack of Support for Varying Resolutions: All current models are trained on square image editing pairs, making their generalization to non-square images poor. In our preliminary studies, we curate a few prompts for seven different desired tasks to observe their success rate across the board. We show our findings in Table 1. This show that these models are truly biased in their skills caused by the underlying synthesis pipeline. In this paper, we introduce Omni-Edit, a novel model designed to address these challenges through four key innovations: 1. Specialist-to-Generalist Supervision: We propose learning a generalist editing model, Omni-Edit, by leveraging supervision from multiple specialist models. Unlike previous approaches that rely on a single expert, we conduct an extensive survey and construct (or train) seven experts, each specializing in a different editing task. These specialists provide supervisory signals to Omni-Edit. 2. Importance Sampling: To ensure high-quality training data, we employ large multimodal models to assign quality scores to synthesized samples. Given the computational cost of GPT-4o (Achiam et al., 2023), we first distill its scoring ability into InternVL2 (Chen et al., 2024b) through medium-sized samples. Then we use the InternVL2 model for large-scale scoring. 3. EditNet Architecture: We introduce EditNet, a novel diffusion-transformer-based architecture (Peebles & Xie, 2022) that facilitates interaction between the control branch and the original branch via intermediate representations. This architecture enhances Omni-Edit ’s ability to comprehend diverse editing tasks. 4. Support for Any Aspect Ratio: During training, we incorporate a mix of images with varying aspect ratios as well as high resolution, ensuring that Omni-Edit can handle images of any aspect ratio with any degradation in the output quality. We curate an image editing benchmark Omni-Edit-Bench, which contains diverse images of different resolutions and diverse prompts that cover all the listed editing skills. We perform comprehensive automatic and human evaluation to show the significant boost of Omni-Edit over the existing baseline models like CosXL-Edit (Boesel & Rombach, 2024), UltraEdit (Zhao et al., 2024), etc. On automatic metrics like VIEScore (Ku et al., 2024), we can outperform all the existing approaches by a reasonable margin in terms of both perceptual quality and semantic consistency. We also performed a human evaluation and observed an overall improvement 20% over the best baseline editing model CosXL-Edit (Boesel & Rombach, 2024)."
https://arxiv.org/html/2411.07184v1,SAMPart3D: Segment Any Part in 3D Objects,"3D part segmentation is a crucial and challenging task in 3D perception, playing a vital role in applications such as robotics, 3D generation, and 3D editing. Recent methods harness the powerful Vision Language Models (VLMs) for 2D-to-3D knowledge distillation, achieving zero-shot 3D part segmentation. However, these methods are limited by their reliance on text prompts, which restricts the scalability to large-scale unlabeled datasets and the flexibility in handling part ambiguities. In this work, we introduce SAMPart3D, a scalable zero-shot 3D part segmentation framework that segments any 3D object into semantic parts at multiple granularities, without requiring predefined part label sets as text prompts. For scalability, we use text-agnostic vision foundation models to distill a 3D feature extraction backbone, allowing scaling to large unlabeled 3D datasets to learn rich 3D priors. For flexibility, we distill scale-conditioned part-aware 3D features for 3D part segmentation at multiple granularities. Once the segmented parts are obtained from the scale-conditioned part-aware 3D features, we use VLMs to assign semantic labels to each part based on the multi-view renderings. Compared to previous methods, our SAMPart3D can scale to the recent large-scale 3D object dataset Objaverse and handle complex, non-ordinary objects. Additionally, we contribute a new 3D part segmentation benchmark to address the lack of diversity and complexity of objects and parts in existing benchmarks. Experiments show that our SAMPart3D significantly outperforms existing zero-shot 3D part segmentation methods, and can facilitate various applications such as part-level editing and interactive segmentation.","3D part segmentation is a fundamental 3D perception task that is essential for various application areas, such as robotic manipulation, 3D analysis and generation, part-level editing [38] and stylization [7]. In the past few years, data-driven fully supervised methods [30, 31, 34, 53, 23] have achieved excellent results on closed-set 3D part segmentation benchmarks [27, 3]. However, these methods are limited to segmenting simple objects due to the restricted quantity and diversity of 3D data with part annotations. Despite the recent release of large-scale 3D object datasets [9, 10, 45], acquiring part annotations for such vast amounts of 3D assets is time-consuming and labor-intensive, which prevents 3D part segmentation from replicating the success of data scaling and model scaling in 2D segmentation [21]. To achieve zero-shot 3D part segmentation in the absence of annotated 3D data, several challenges need to be addressed. The first and most significant challenge is how to generalize to open-world 3D objects without 3D part annotations. To tackle this, recent works [25, 56, 20, 1, 47] have utilized pre-trained 2D foundation vision models, such as SAM [21] and GLIP [22], to extract visual information from multi-view renderings and project it onto 3D primitives, achieving zero-shot 3D part segmentation. However, these methods rely solely on 2D appearance features without 3D geometric cues, leading to the second challenge: how to leverage 3D priors from unlabeled 3D shapes. PartDistill [41] has made a preliminary exploration by introducing a 2D-to-3D distillation framework to learn 3D point cloud feature extraction, but it cannot scale to large 3D datasets like Objaverse [9] due to the need for predefined part labels and the constrained capabilities of GLIP. Building on existing works, we further explore the third challenge: the ambiguity of 3D parts, which manifests primarily in semantics and granularity. Semantic ambiguity arises from the vague textual descriptions of parts. Existing methods rely on vision-language models (VLMs) like GLIP, which require a part label set as text prompt. Unfortunately, not all 3D parts can be clearly and precisely described in text. Granularity ambiguity considers that a 3D object can be segmented at multiple levels of granularity. For example, the human body can be divided into broader sections, such as upper and lower halves, or into finer parts like limbs, torso, and head. Previous methods rely on fixed part label sets and lack flexible control over segmentation granularity. To tackle the three aforementioned challenges, in this work, we propose SAMPart3D, a scalable zero-shot 3D part segmentation framework that segments object parts at multiple granularities without requiring preset part labels as text prompts. We argue that previous works overly rely on predefined part label sets and GLIP, limiting their scalability to complex, unlabeled 3D datasets and their flexibility in handling semantic ambiguity of 3D parts. To address this, we abandon GLIP and instead utilize the more low-level, text-independent DINOv2 [29] model for 2D-to-3D feature distillation, eliminating the reliance on part label sets and enhancing both scalability and flexibility. Besides, to handle the ambiguity in segmentation granularity, we employ a scale-conditioned MLP [19] distilled from SAM for granularity-controllable 3D part segmentation. The distillation from DINOv2 and SAM is divided into two training stages to balance efficiency and performance. After obtaining the segmented 3D parts, we adaptively render multi-view images for each part based on its visual area, then use the powerful Multi-modal Large Language Models (MLLMs) [24, 6, 42, 15] to assign semantic descriptions for each part based on the renderings, yielding the final part segmentation results. In summary, our contributions are as follows: • We introduce SAMPart3D, a scalable zero-shot 3D part segmentation framework that segments object parts at multiple granularities without requiring preset part labels as text prompts. • We propose a text-independent 2D-to-3D distillation, which enables learning 3D priors from large-scale unlabeled 3D objects and can handle part ambiguity in both semantic and granularity aspects. The distillation is two-stage, striking a balance between segmentation performance and training efficiency. • We introduce PartObjaverse-Tiny, a 3D part segmentation dataset which provides detailed semantic and instance level part annotations for 200 complex 3D objects. • Extensive experiments demonstrate that SAMPart3D achieves outstanding part segmentation results on complex and diverse 3D objects compared to existing zero-shot 3D part segmentation methods. Furthermore, our method can facilitate various applications, such as interactive segmentation and part-level editing."
https://arxiv.org/html/2411.07167v1,Cascaded Dual Vision Transformer for Accurate Facial Landmark Detection,"Facial landmark detection is a fundamental problem in computer vision for many downstream applications. This paper introduces a new facial landmark detector based on vision transformers, which consists of two unique designs: Dual Vision Transformer (D-ViT) and Long Skip Connections (LSC). Based on the observation that the channel dimension of feature maps essentially represents the linear bases of the heatmap space, we propose learning the interconnections between these linear bases to model the inherent geometric relations among landmarks via channel-split ViT. We integrate such channel-split ViT into the standard vision transformer (i.e., spatial-split ViT), forming our Dual Vision Transformer to constitute the prediction blocks. We also suggest using long skip connections to deliver low-level image features to all prediction blocks, thereby preventing useful information from being discarded by intermediate supervision. Extensive experiments are conducted to evaluate the performance of our proposal on the widely used benchmarks, i.e., WFLW [45], COFW [3], and 300W [34], demonstrating that our model outperforms the previous SOTAs across all three benchmarks.","Facial landmark detection involves locating a set of predefined key points on face images, serving as a fundamental step for supporting various high-level applications, including face alignment [14, 19, 47, 47], face recognition [51, 56, 43], face parsing [36] and 3D face reconstruction [8, 5, 22]. Early approaches of landmark detection relied on statistical models [4, 11], but have been surpassed by modern landmark detectors that use convolutional neural networks (i.e., CNN) [38, 53, 55]. CNNs learn a transformation between image features and a set of 2D coordinates or regress heatmaps to represent the probability distribution of landmarks. Besides, these detectors [6, 44, 18, 21, 54] usually employ cascaded networks in conjunction with intermediate supervision to progressively refine the predictions, producing remarkable advances. To capture the intrinsic geometric relationships among landmarks for accurate predictions, researchers have also developed various effective methods using Graph Convolutional Networks (GCNs) [25, 24], Graph Attention Networks (GATs) [31] or Transformers [49, 23, 46]. However, these methods primarily utilize patch-based image features to learn spatial relations among landmarks. In this paper, we have developed a vision transformer-based model architecture and modeled the intrinsic geometric relationships among landmarks by computing the correlations between the linear bases of heatmap space, enabling us to achieve new SOTA results across all three benchmarks. Following the design paradigm of cascaded networks [6, 44, 18, 21, 54], we repeat prediction blocks in conjunction with intermediate supervisions. Specially, in our architecture, we propose two unique designs: Dual Vision Transformer (D-ViT), which constitutes the prediction blocks, and Long Skip Connections (LSC), the strategy for connecting these blocks. The standard vision transformer [10] discretizes images or feature maps into small patches, then rearranges them into a sequence to extract global and local image features. However, it lacks the ability to model the underlying geometric characteristics of landmarks. To address this, we propose incorporating the channel-split ViT to model the inherent relationships among landmarks. Specifically, the prediction block outputs a feature map F\in\mathbb{R}^{C\times H\times W} for intermediate supervision, which can be split along the channels into F=(f_{1},f_{2},...,f_{C}). Therefore, when considering using convolution operations to regress the feature map F into the heatmaps, the heatmap for each landmark is actually a linear combination of f_{m}. To put it another way, the channel dimension of feature maps essentially represents the linear bases of the heatmap space. Based on such insight, we take advantage of the transformer architecture to learn underlying relationships among these linear bases, allowing for adaptive computation of their interconnections through the multi-head self-attention mechanism. Finally, the spatial-split ViT and the channel-split ViT together form our Dual Vision Transformer (D-ViT). Following the classic stacked Hourglasses networks [29, 44, 18, 54], which utilize residual connections between two sequential hourglass architectures, we first also apply the same connection strategy to boost the network. However, we find that when the number of prediction blocks exceeds 4, the detection performance is instead diminished by deeper model architecture. As far as we know, this is caused by intermediate supervisions, which can lead to the inadvertent discard of useful information. To handle this problem, we suggest using long skip connections to deliver low-level image features to all prediction blocks, thereby making deeper network architectures feasible. We evaluate the performance of our proposal on the widely used benchmarks, i.e., WFLW [45], COFW [3], and 300W [34]. Extensive experiments demonstrate that our approach outperforms the previous state-of-the-art methods and achieves a new SOTA across all three datasets. The main contributions can be summarized as follows: 1) We introduce a facial landmark detector based on our unique dual vision transformer (D-ViT), which is able to effectively capture contextual image features and underlying geometric relations among landmarks via spatial-split and channel-split features. 2) To avoid losing useful information due to intermediate supervision and make deeper network architectures feasible, we propose a unique connection strategy, i.e., Long Skip Connections, to transmit low-level image features from ResNet to each prediction block. 3) Extensive experiments are conducted to evaluate our approach, demonstrating its good generalization ability and superior performance compared to existing SOTAs across three publicly available datasets (i.e., WFLW, COFW, and 300W). Our code will be released for reproduction."
https://arxiv.org/html/2411.07138v1,Nuremberg Letterbooks: A Multi-Transcriptional Dataset of Early 15th Century Manuscripts for Document Analysis,"Most datasets in the field of document analysis utilize highly standardized labels, which, while simplifying specific tasks, often produce outputs that are not directly applicable to humanities research. In contrast, the Nuremberg Letterbooks dataset, which comprises historical documents from the early 15th century, addresses this gap by providing multiple types of transcriptions and accompanying metadata. This approach allows for developing methods that are more closely aligned with the needs of the humanities. The dataset includes 4 books containing 1711 labeled pages written by 10 scribes. Three types of transcriptions are provided for handwritten text recognition: Basic, diplomatic, and regularized. For the latter two, versions with and without expanded abbreviations are also available. A combination of letter ID and writer ID supports writer identification due to changing writers within pages. In the technical validation, we established baselines for various tasks, demonstrating data consistency and providing benchmarks for future research to build upon.","Background & Summary In historical document digitization and handwritten text recognition, a significant challenge lies in bridging the gap between merely scanning ancient manuscripts and truly accessing and understanding the corpus they present. While digitization has made these texts more available, it does not inherently make them comprehensible or usable for varied research purposes. The process of transitioning from a physical document to a digital corpus encompasses numerous complexities, especially in transcription and interpretation, which vary widely based on the research field and study objectives. • Computer scientists, for instance, often work with basic transcriptions of documents. These transcriptions are simplified versions tailored for training text recognition models. • In contrast, German studies require transcriptions that are as close as possible to the original text, capturing its visual features and nuances. This level of detail is crucial for studies focused on linguistic and cultural contexts. • Historians, on the other hand, lean towards regularized versions of texts where abbreviations are resolved and special characters like “” are normalized to suit the reading habits of the contemporary audience. This approach facilitates content analysis and interpretation, making historical texts more accessible and understandable. These regularized versions often culminate in the creation of scholarly editions. The Nuremberg Letterbooks illustrate the critical need for varied transcription methods in document analysis. Historically, they were used to record the outgoing letters of Nuremberg’s small council to other cities and individuals. The topics of the correspondence range from everyday economic or legal matters of individual citizens to discussions of imperial politics with the kings or other major cities. An interdisciplinary project team worked collaboratively on some of the oldest preserved books. In this project, three types of annotations have been made available for four successive books, each tailored to meet the specific needs of different research fields. The basic transcription is primarily intended for automatic text recognition and often serves as the default in other datasets.[1, 2, 3, 4] The diplomatic and regularized transcriptions cater to the needs of scholars in German Studies and historians, respectively. Additionally, the dataset includes information on expanded abbreviations, as well as metadata such as writer ID and the starting and ending points of letters. With its diverse annotations, this dataset is not just a valuable resource for historical research but also serves as a starting point for developing models that can handle different types of transcriptions. As the volume of scanned documents from archives continues to grow, the demand for diverse automatic transcription methods becomes increasingly important. Such models can greatly assist humanities researchers, providing them with access to digitized texts without requiring additional adjustments to fit each field’s specific requirements. Ultimately, this dataset seeks to bridge the gap between the vast quantity of digitized historical texts and the complex, varied demands of humanities research, thereby enabling a more profound and streamlined exploration of our history."
https://arxiv.org/html/2411.07135v1,Edify 3D: Scalable High-Quality 3D Asset Generation,"We introduce Edify 3D, an advanced solution designed for high-quality 3D asset generation. Our method first synthesizes RGB and surface normal images of the described object at multiple viewpoints using a diffusion model. The multi-view observations are then used to reconstruct the shape, texture, and PBR materials of the object. Our method can generate high-quality 3D assets with detailed geometry, clean shape topologies, high-resolution textures, and materials within 2 minutes of runtime.","The creation of detailed digital 3D assets is essential for developing scenes, characters, and environments across various digital domains. This capability is invaluable to industries such as video game design, extended reality, film production, and simulation. For 3D content to be production-ready, it must meet industry standards, including precise mesh structures, high-resolution textures, and material maps. Consequently, producing such high-quality 3D content is often an exceedingly complex and time-intensive process. As demand for 3D digital experiences grows, the need for efficient, scalable solutions in 3D asset creation becomes increasingly crucial. Recently, many research works have investigated into training AI models for 3D asset generation (Lin et al., 2023). A significant challenge, however, is the limited availability of 3D assets suitable for model training. Creating 3D content requires specialized skills and expertise, making such assets much scarcer than other visual media like images and videos. This scarcity raises a key research question of how to design scalable models to generate high-quality 3D assets from such data efficiently. Edify 3D is an advanced solution designed for high-quality 3D asset generation, addressing the above challenges while meeting industry standards. Our model generates high-quality 3D assets in under 2 minutes, providing detailed geometry, clean shape topologies, organized UV maps, textures up to 4K resolution, and physically-based rendering (PBR) materials. Compared to other text-to-3D approaches, Edify 3D consistently produces superior 3D shapes and textures, with notable improvements in both efficiency and scalability. This technical report provides a detailed description of Edify 3D. Figure 2: Pipeline of Edify 3D. Given a text description, a multi-view diffusion model synthesizes the RGB appearance of the described object. The generated multi-view RGB images are then used as a condition to synthesize surface normals using a multi-view ControlNet (Zhang et al., 2023). Next, a reconstruction model takes the multi-view RGB and normal images as input and predicts the neural 3D representation using a set of latent tokens. This is followed by isosurface extraction and subsequent mesh post-processing to obtain the mesh geometry. An upscaling ControlNet is used to increase the texture resolution, conditioning on mesh rasterizations to generate high-resolution multi-view RGB images, which are then back-projected onto the texture map. Core capabilities. Edify 3D features the following capabilities: • Text-to-3D generation. Given an input text description, Edify 3D generates a digital 3D asset with the aforementioned properties. • Image-to-3D generation. Edify 3D can also create a 3D asset from a reference image of the object, automatically identifying the foreground object in the image. Model design. The core technology of Edify 3D relies on two types of neural networks: diffusion models (Song and Ermon, 2019; Ho et al., 2020) and Transformers (Vaswani et al., 2017). Both architectures have demonstrated great scalability and success in improving generation quality as more training data becomes available. Following Li et al. (2024), we train the following models: • Multi-view diffusion models. We train multiple diffusion models to synthesize the RGB appearance and surface normals of an object from multiple viewpoints (Shi et al., 2023b). The input can be a text prompt, a reference image, or both. • Reconstruction model. Using the synthesized multi-view RGB and surface normal images, a reconstruction model predicts the geometry, texture, and materials of the 3D shape. We employ a Transformer-based model (Hong et al., 2023) to predict a neural representation of the 3D object as latent tokens, followed by isosurface extraction and mesh processing. The final output of Edify 3D is a 3D asset that includes the mesh geometry, texture map, and material map. Fig. 2 illustrates the overall pipeline of Edify 3D. In this report, we provide: • A detailed discussion of the design choices in the Edify 3D pipeline. • An analysis of the scaling behaviors of model components and properties. • An application of Edify 3D to scalable 3D scene generation from input text prompts."
https://arxiv.org/html/2411.07132v1,"Token Merging for Training-Free Semantic Binding 
in Text-to-Image Synthesis","Although text-to-image (T2I) models exhibit remarkable generation capabilities, they frequently fail to accurately bind semantically related objects or attributes in the input prompts; a challenge termed semantic binding. Previous approaches either involve intensive fine-tuning of the entire T2I model or require users or large language models to specify generation layouts, adding complexity. In this paper, we define semantic binding as the task of associating a given object with its attribute, termed attribute binding, or linking it to other related sub-objects, referred to as object binding. We introduce a novel method called Token Merging (ToMe), which enhances semantic binding by aggregating relevant tokens into a single composite token. This ensures that the object, its attributes and sub-objects all share the same cross-attention map. Additionally, to address potential confusion among main objects with complex textual prompts, we propose end token substitution as a complementary strategy. To further refine our approach in the initial stages of T2I generation, where layouts are determined, we incorporate two auxiliary losses, an entropy loss and a semantic binding loss, to iteratively update the composite token to improve the generation integrity. We conducted extensive experiments to validate the effectiveness of ToMe, comparing it against various existing methods on the T2I-CompBench and our proposed GPT-4o object binding benchmark. Our method is particularly effective in complex scenarios that involve multiple objects and attributes, which previous methods often fail to address. The code will be publicly available at https://github.com/hutaihang/ToMe.","Text-to-image generation has seen significant advancements with the recent introduction of diffusion models ramesh2022dalle2 ; Rombach_2022_CVPR_stablediffusion ; deepfloyd , with their capabilities of generating high-fidelity images from text prompts. Despite these achievements, aligning the generated images with the text prompts, which is referred to as semantic alignment hu2024ella ; li2023divide_bind , remains a notable challenge. One of the most common issues observed in existing text-to-image (T2I) generation models is the lack of proper semantic binding, where a given object is not properly binding to its attributes or related objects. For example, as illustrated in Fig. 1, even a state-of-the-art T2I model such as SDXL podell2023sdxl can struggle to generate content that accurately reflects the intended nuances of text prompts. Figure 1: Current state-of-the-art T2I models often struggle with semantic binding in generated images according to textual prompts. For example, hats and sunglasses are placed on incorrect objects. We introduce a novel method ToMe to address these challenges. To address the persistent challenges of aligning T2I diffusion models with the intricate semantics of text prompts, a variety of enhancement strategies karthik2023iffirst ; liu2023correcting ; zhou2023maskdiffusion are proposed, either by optimizing the latent representations wang2023tokencompose ; zhang2024enhancing ; zhang2024object_energy , guiding the generation by layout priors qi2023layeredrenderdiff ; wu2023harnessing ; zhao2023loco or fine-tuning the T2I models feng2023ranni ; jiang2024comat . Despite these advancements, these methods still encounter limitations, particularly in generating high-fidelity images involving complex scenarios where an object is binding with multiple objects or attributes. In this paper, we categorize semantic binding into two categories. First, attribute binding involves correctly associating objects with their attributes, a topic that has been studied in prior work rassin2024linguistic_binding . Second, object binding, which entails effectively linking objects to their related sub-objects (for example, a ‘hat’ and ‘glasses’), is less explored in the existing literature. Previous methods often struggled to address this aspect of semantic binding. One of the main problems is the misalignment of objects with their corresponding sub-objects. Existing solutions address this through an explicit alignment process of the attention maps chefer2023attend ; li2023divide_bind or by factorizing the generation projects into layout phases and generation phase qu2023layoutllm . In this paper, we propose a simple solution to the attention alignment problem called token merging (ToMe). Instead of multiple attention maps, which can be misaligned, we join these objects in a single composite token that represents the object and its attributes and sub-objects. This composite token has a single cross-attention map that ensures semantic alignment. The composite token is simply constructed by summing the CLIP text embeddings of the various tokens it represents. For example, the phrase “a dog with hat” is abbreviated as “a dog*” by aggregating the text embeddings corresponding to the last three words, as shown in Fig. 4. To justify the applied embedding addition in ToMe, we experimented with the semantic additivity of the text embeddings (in Fig. 3). Furthermore, to mitigate potential semantic misalignment in the end tokens from the long sequences, we propose end token substitution (ETS) technique. As the T2I generation predominantly determines the layout during earlier phases hertz2022prompt , we introduce an entropy loss and a semantic binding loss to update the token embeddings in early steps, integrating ToMe with an iterative update for the composite tokens. The entropy loss is defined as the entropy of the cross-attention map corresponding to the updated composite token. This loss aims to enhance generation integrity by ensuring diverse attention across relevant areas of the image, thereby preventing focusing on non-essential regions. The semantic binding loss encourages the new learned token to infer the same noise prediction as the original corresponding phrase. This alignment further reinforces the semantic coherence between the text and the generated image. Our final method ToMe is quantitatively assessed using the widely adopted T2I-CompBench huang2023t2i_compbench and our proposed GPT-4o achiam2023chatgpt4 object binding benchmark. Comparative evaluations against various types of approaches reveal that ToMe outperforms them by a significant margin. Remarkably, our approach is user-friendly, requiring no dependence on large language models or specific layout information. In qualitative evaluations, we notably achieve superior generation quality, particularly in scenarios involving multi-object multi-attribute generation. This further underscores the superiority of our method. In summary, the main contributions of this paper are as follows: • We analyze the problem of semantic binding, and highlight the role of the [EOT] token (Fig. 2), and the problems with misaligned cross-attention maps (Fig. 7). In addition, we explore token additivity as a possible solution (Fig. 3). • We introduce a training-free approach called Token Merging (Fig. 4), denoted as ToMe, as a more efficient and robust solution for semantic binding. It is further enhanced by our proposed end token substitution and iterative composite token updates techniques. • In experiments conducted on the widely used T2I-CompBench benchmark and our GPT-4o object binding benchmark, we compared ToMe with various state-of-the-art approaches and consistently outperformed them by significant margins."
https://arxiv.org/html/2411.07126v1,Edify Image: High-Quality Image Generation with Pixel Space Laplacian Diffusion Models,"We introduce Edify Image, a family of diffusion models capable of generating photorealistic image content with pixel-perfect accuracy. Edify Image utilizes cascaded pixel-space diffusion models trained using a novel Laplacian diffusion process, in which image signals at different frequency bands are attenuated at varying rates. Edify Image supports a wide range of applications, including text-to-image synthesis, 4K upsampling, ControlNets, 360^{\circ} HDR panorama generation, and finetuning for image customization.","The field of text-to-image synthesis has witnessed remarkable progress in recent years, with state-of-the-art models (Betker et al., 2023; Esser et al., 2024; Baldridge et al., 2024; Podell et al., 2023) generating increasingly realistic and diverse images from natural language descriptions. These models typically leverage large-scale diffusion-based architectures trained on billions of image-text pairs. The ability to generate high-resolution, photorealistic images has far-reaching implications across domains such as content creation, gaming, synthetic data generation, and the design of digital avatars. In this technical report, we present Edify Image, a family of pixel-space diffusion models capable of generating high-resolution images with exceptional controllability. We train our models in a cascaded fashion, where a base model generates low-resolution images, and subsequent models progressively upsample the images from the previous stage. Our models are trained using a novel multi-scale Laplacian diffusion process, in which image signals at different frequency bands are attenuated at varying rates. This enables our models to capture and refine details with precision across multiple scales, resulting in photorealistic, pixel-perfect generations. Using the Laplacian Diffusion Model formulation, we train a suite of diffusion models capable of generating images from various input signals. • Text-to-image models. We train a two-stage cascaded text-to-image diffusion model that can generate 1K resolution images from input text. Our model handles long text prompts, generates images with varying aspect ratios, exhibits improved fairness and diversity when generating human subjects, and can support the use of camera controls such as pitch and depth of field. • 4K Upsampler. We train an upsampler model that takes a 1K resolution image as input and upsamples it to 4K resolution. The upsampler involves fine-tuning the 1K resolution generator on 4K images with an additional low-resolution input conditioning. Our model is capable of synthesizing high-frequency details while remaining faithful to the low-resolution input image. • ControlNets. We train ControlNets on the 256 resolution base models for various modalities, including depth, sketch, and inpainting mask. The 1K and 4K base models can be reused for upsampling. Our model can generate high-quality images while enabling flexible structural controls. In addition, we also support the following two capabilities: • 360^{\circ} HDR Panorama Generation. We design an algorithm for generating 4K, 8K and 16K resolution HDR panorama from the input text. We utilize the base text-to-image models to perform sequential inpainting in which images from different perspectives are generated in an overlapping manner and stitched together with consistency. • Finetuning. We propose an algorithm for finetuning the base text-to-image models on a small subset of reference images. Our model is especially capable of generating various hyper-realistic humans with identities consistent with the reference set."
https://arxiv.org/html/2411.07121v1,"Decoding Visual Experience and Mapping Semantics 
through Whole-Brain Analysis Using fMRI Foundation Models","Neural decoding, the process of understanding how brain activity corresponds to different stimuli, has been a primary objective in cognitive sciences. Over the past three decades, advancements in functional Magnetic Resonance Imaging (fMRI) and machine learning have greatly improved our ability to map visual stimuli to brain activity, especially in the visual cortex. Concurrently, research has expanded into decoding more complex processes like language and memory across the whole brain, utilizing techniques to handle greater variability and improve signal accuracy. We argue that “seeing” involves more than just mapping visual stimuli onto the visual cortex; it engages the entire brain, as various emotions and cognitive states can emerge from observing different scenes. In this paper, we develop algorithms to enhance our understanding of visual processes by incorporating whole-brain activation maps while individuals are exposed to visual stimuli. We utilize large-scale fMRI encoders and Image generative models (encoders & decoders) pre-trained on large public datasets, which are then fine-tuned through Image-fMRI contrastive learning. Our models hence can decode visual experience across the entire cerebral cortex, surpassing the traditional confines of the visual cortex. Using a public dataset (BOLD5000), we first compare our method with state-of-the-art approaches to decoding visual processing and show improved predictive semantic accuracy by 43%. A network ablation analysis suggests that beyond the visual cortex, the default mode network contributes most to decoding stimuli, in line with the proposed role of this network in sense-making and semantic processing. Additionally, we implemented zero-shot imagination decoding on an extra validation dataset, achieving a p-value of 0.0206 for mapping the reconstructed images and ground-truth text stimuli, which substantiates the model’s capability to capture semantic meanings across various scenarios. These findings underscore the potential of employing comprehensive models in enhancing the nuanced interpretation of semantic processes.Keywords: Computer Vision, Cognitive Science, Artificial Intelligence, Generative Models","1 Main Understanding the relationship between brain activity and the characteristics of external stimuli, commonly known as “neural decoding” [26] or identifying “neural representations,” [28] is fundamental to advancing neuroscience. In the last three decades, the development and continuous improvement of functional magnetic resonance imaging (fMRI) allowed researchers to map spatial patterns of activity at the level of millimeter-cubed “voxels” onto patterns of stimuli to understand where in the brain information about different stimuli characteristics is held. The majority of this research has focused on mapping representations of visual stimuli onto the visual cortex with increasing success [26, 53, 10, 49].The field of visual decoding has progressed steadily in terms of the accuracy with which stimuli can be predicted based on brain activity. Still, the pace of this progress has sped up with the recent improvements in machine learning and deep learning methods that can identify more complex relationships between visual activity and visual stimuli [10, 49, 53, 37]. While recent advances in neural decoding have significantly enhanced the ability of models to reconstruct visual stimuli from neural activities in the visual cortex, these models predominantly excel at capturing basic, low-level visual features such as edges, textures, and other pixel-wise attributes [26, 4]. Yet, they struggle to decode the semantics of visual input [10], which is crucial for the understanding of the visual experience. Running parallel to this work has been attempts to map the representations of more complex higher-order processes such as language [2], semantics [25], movie-watching [20], or autobiographical memory [3] onto either specific networks known to be involved in these processes, or more recently the whole brain [41]. Since these representations vary significantly across individuals both in terms of their functional units and spatial arrangements, these works collect datasets that sample more participants than those used in visual studies and statistical approaches such as multi-voxel pattern analysis [36] or representational similarity analysis [29] are leveraged to capture between-subject representation and variance. Recent evidence increasingly suggests that whole-brain processing plays a crucial role in many fundamental tasks, including visual perception. For example, one study suggests that there is a retinotopic code in the default mode network that allows a mapping between past experiences and incoming visual information [51]. Another study suggests that visual images are encoded in terms of the actions they afford, represented across the whole brain [56]. Predictive coding [1] and active inference [15] theories propose that all visual processing is subject to a comparison with an internal model of the world provided via top-down connections with higher-order brain regions. This suggests that advances in decoding visual activity will benefit from the inclusion of whole-brain information. However, whole-brain data presents a significantly higher dimensionality, requiring orders of magnitude larger datasets to effectively train machine learning algorithms for neural decoding. This is why methods optimized for visual cortex activity have been successful in decoding visual stimuli-the visual cortex comprises a more constrained set of voxels, allowing smaller and less complex models to perform the task effectively. Expanding decoding efforts to whole-brain data necessitates more sophisticated models and larger datasets to capture the complexity of neural processing across multiple brain regions. Inspired by these two lines of research and making use of recently developed foundation models [5] that leverage pretraining on large datasets to maximize predictive accuracy in more specialized data, we propose a novel approach to decode visual representations present throughout the whole brain (Fig. 1), referred to as Whole-brain Analysis of Visual Experience (WAVE). It leverages the ability of these foundation models [5] to identify between-subjects patterns from large amounts of generic data (e.g., various tasks of fMRI and natural image separately) to improve the predictions of models applied to smaller, more specialized datasets even for tasks that involve large amounts of between-person variance (e.g., mapping semantic representations). First, we compare our method with state-of-the-art approaches to decoding visual processing and show improved predictive accuracy. We highlight that even after removing the visual cortex we can show high predictive accuracy on a traditional visual dataset, emphasizing the importance of whole-brain processing for even simple visual tasks. Subsequently, we enhance our model’s interpretability through post-hoc analyses. Our findings reveal that higher-order cortical hierarchy provides more predictive power for visual scenes with greater complexity. Furthermore, we demonstrate the model’s ability to capture semantic information across different scenarios. We used a separate fMRI imagination dataset where participants gave verbal descriptions followed by fMRI during scenario visualization. Our model excelled at decoding scenarios semantically similar to images from the original dataset. This highlights its capacity to generalize semantic understanding beyond the original training data. This research represents both methodological and conceptual advances: we believe that using these methods can improve the accuracy of brain-stimuli mapping in a way that can be applied across a greater range of datasets. Ultimately, this could lead to more precise modeling of neural processes, gaining novel insights into human cognitive processes."
https://arxiv.org/html/2411.07118v1,ConvMixFormer- A Resource-efficient Convolution Mixer for Transformer-based Dynamic Hand Gesture Recognition,"Transformer models have demonstrated remarkable success in many domains such as natural language processing (NLP) and computer vision. With the growing interest in transformer-based architectures, they are now utilized for gesture recognition. So, we also explore and devise a novel ConvMixFormer architecture for dynamic hand gestures. The transformers use quadratic scaling of the attention features with the sequential data, due to which these models are computationally complex and heavy. We have considered this drawback of the transformer and designed a resource-efficient model that replaces the self-attention in the transformer with the simple convolutional layer-based token mixer. The computational cost and the parameters used for the convolution-based mixer are comparatively less than the quadratic self-attention. Convolution-mixer helps the model capture the local spatial features that self-attention struggles to capture due to their sequential processing nature. Further, an efficient gate mechanism is employed instead of a conventional feed-forward network in the transformer to help the model control the flow of features within different stages of the proposed model. This design uses fewer learnable parameters which is nearly half the vanilla transformer that helps in fast and efficient training. The proposed method is evaluated on NVidia Dynamic Hand Gesture and Briareo datasets and our model has achieved state-of-the-art results on single and multimodal inputs. We have also shown the parameter efficiency of the proposed ConvMixFormer model compared to other methods. The source code is available at https://github.com/mallikagarg/ConvMixFormer.","Deep learning models, particularly Convolutional Neural Networks (CNNs), have demonstrated the ability to automatically learn hierarchical representations of raw input data, through a series of convolutional and pooling layers [5]. With time, attention-based mechanisms came into existence that focus more on important features. Earlier, deep based [17, 31], RNN-based and LSTM-based [33] models were used for recognizing continuous gestures. Later, transformer network [41] was used for capturing information from sequential data [12] which takes single and multimodal inputs using a late fusion approach. Although transformers are very efficient in various applications, they pose some limitations in terms of performance and model complexity, specifically when used for computer vision tasks. For an image as input data, the sequential length corresponds to the number of tokens or patches extracted from the image. Even for a 224 \times 224 image size, the number of tokens can be significant, resulting in a quadratic increase in attention computations, since the attention scales quadratically in the self-attention module of the transformer model. Therefore, while designing the proposed model, we consider the transformer model’s drawbacks for gesture recognition. We propose a novel token mixer that involves convolution for mixing spatial tokens of the input gesture image called ConvMixFormer, which generalizes with fewer parameters than the vanilla transformer. The main idea of using convolution as a token mixer as it is efficient in terms of parameters and FLOPs. Meanwhile, convolution also helps extract local features from the input, enabling it to capture fine-grained details and spatial relationships. This is required in gesture recognition tasks as hand and finger movements are involved in gestures, which can be of different sizes depending on the signer’s age. By applying convolutions across the sequence, the model can capture interactions between adjacent tokens, allowing it to capture spatial dependencies and patterns. Convolutional layers also exhibit translation equivalence, meaning that if the input is translated, the output will also be translated by the same amount, which makes the model rotation and translation invariant. Overall, using convolution as a token mixer in transformer models offers several advantages, including local feature extraction, complexity reduction, interaction between tokens, and comparable performance with fewer parameters. However, in the literature, ConvMixer [40] is also proposed which uses depthwise convolution to mix spatial locations followed by pointwise convolution to mix channel locations. While depthwise convolutions offer advantages such as reduced computational cost and model size, they also pose some disadvantages compared to standard convolutions: (a) Depthwise separable convolutions consider independent input channels for processing which does not allow it to capture complex spatial patterns as in the case of convolutions. Therefore, it may struggle to capture inter-channel dependencies effectively. This limitation could result in a reduced ability to capture cross-channel spatial information. (b) Depthwise separable convolutions involve separate operations for each channel, leading to more memory accesses compared to standard convolutions, where each filter interacts with all input channels simultaneously. Keeping in view the limitations of the transformer model and mixers proposed in the literature, we proposed the standard convolution as the token mixer for dynamic gesture recognition. We also propose the use of a Gated mechanism in the feed-forward network named a Gated Depthwise Feed Forward Network, (GDFN) to control the flow of information within different stages of the proposed transformer model. Thus, the key contributions are: 1. We design a lightweight resource-efficient convolution-based transformer model, ConvMixFormer, for dynamic hand gesture recognition. 2. We propose a novel convolution token mixer to efficiently replace the attention mechanism in the transformer with the convolution layer that enables the model to capture complex spatial patterns from the input. 3. We propose the use of a Gated Depthwise Feed Forward Network (GDFN) which helps the control of the information flow in a sequential transformer model. This helps to focus on relevant features while suppressing irrelevant information, potentially improving the model’s robustness and generalization. 4. ConvMixFormer proves its efficacy on the Briareo and NVGesture dataset by achieving state-of-the-art performance with significantly fewer parameters on single and multimodal inputs."
https://arxiv.org/html/2411.07097v1,Arctique: An artificial histopathological dataset unifying realism and controllability for uncertainty quantification,"Uncertainty Quantification (UQ) is crucial for reliable image segmentation. Yet, while the field sees continual development of novel methods, a lack of agreed-upon benchmarks limits their systematic comparison and evaluation: Current UQ methods are typically tested either on overly simplistic toy datasets or on complex real-world datasets that do not allow to discern true uncertainty. To unify both controllability and complexity, we introduce Arctique, a procedurally generated dataset modeled after histopathological colon images. We chose histopathological images for two reasons: 1) their complexity in terms of intricate object structures and highly variable appearance, which yields challenging segmentation problems, and 2) their broad prevalence for medical diagnosis and respective relevance of high-quality UQ. To generate Arctique, we established a Blender-based framework for 3D scene creation with intrinsic noise manipulation. Arctique contains 50,000 rendered images with precise masks as well as noisy label simulations. We show that by independently controlling the uncertainty in both images and labels, we can effectively study the performance of several commonly used UQ methods. Hence, Arctique serves as a critical resource for benchmarking and advancing UQ techniques and other methodologies in complex, multi-object environments, bridging the gap between realism and controllability. All code is publicly available, allowing re-creation and controlled manipulations of our shipped images as well as creation and rendering of new scenes.","The crucial importance of reliable UQ for the deployment of segmentation algorithms to safety-critical real-world settings has long been recognized by the machine learning community, and the field has seen substantial development of methodology over past years (see e.g. [12, 1, 31]). However, there is a glaring lack of comprehensive evaluation of UQ methods, which makes it difficult to contextualize new methods within the existing paradigms, and renders the choice of suitable UQ methods burdensome for practitioners. One reason for the lack of comparative insight is that often UQ methods are developed from theoretical considerations and tested on hand-crafted toy datasets but fail to provide meaningful, interpretable results on complex real-world datasets [20, 6]. Towards more insightful benchmarking of UQ methods, it is desirable to establish benchmark datasets with ground-truth uncertainty. However, in real-world settings, ground truth uncertainty is usually unattainable. Thus related works have resorted to empirically obtained (and therefore not fully quantifiable and/or controllable) distribution shifts and label noise [20, 3], which has greatly advanced the field, albeit by construction still does not facilitate comprehensive insight into method behavior. Synthetic data generation offers a promising avenue towards improved insight by providing clearly defined data properties and annotations (see [17] for an example from the realm of Explainable AI). However, previous synthetic data generation methodologies proposed in the context of challenging image segmentation problems either excel in controllability but fall short in complexity [30, 39], or vice-versa aim at improved complexity and realism but at the cost of falling short in controllability [8, 41], the latter because learnt image generation, while able to offer some level of conditioning on sought image properties, neither provides full control nor full insight into the image generation process. To address this gap, we introduce Arctique (ARtificial Colon Tissue Images for Quantitative Uncertainty Evaluation), a procedurally generated histopathological dataset designed to mirror the properties of images derived from H&E stained colonic tissue biopsies, as acquired routinely for safety-critical medical diagnoses in clinical practice [36]. Histopathological images offer a rich and challenging landscape for the application of advanced machine learning methodology, particularly in segmentation [2, 25]. The essential task of accurately delineating and classifying cellular structures is challenging even for trained professionals, due to many sources of uncertainty, e.g. overlapping structures, partial information from the underlying physical tissue-slicing process, and the inherent variability of biological tissues. The demanding nature of this task is reflected in the relative scarcity of fully annotated real-world data sets and high inter-annotator variability (see e.g [14]). Arctique offers the creation of realistic synthetic histopathological images at full controllability, allowing users to manipulate a range of easily interpretable parameters that effectively serve as ""sliders"" for image- as well as label uncertainties. Arctique provides 50,000 pre-rendered 512x512-sized images for training and evaluation of segmentation tasks, shipped with exact masks (2D and 3D), metadata storing characteristics of cellular objects, and rendering parameters to re-generate scenes. Furthermore, Arctique provides two main avenues for the controlled study of uncertainty: (1) a blender-based generation framework, which allows to re-generate and manipulate scenes, and (2) a data loader for post-processing images and emulating noisy labels. To assess Arctique’s degree of realism, we show that segmentation networks trained exclusively on Arctique can achieve promising zero-shot performance on real H&E images, proving its ability to learn meaningful semantic attributes. To showcase how Arctique can be used for insightful benchmarking of UQ methods, we assess foreground-background segmentation and semantic segmentation and measure the effect of uncertainty in the images and the labels separately. We benchmark the performance of four widely used UQ-methods (Maximum Softmax Response (MSR), Test Time Augmentation (TTA [38]), Monte-Carlo Dropout (MCD [11]) and Deep Ensembles (DE [22])). For each uncertainty scenario we measure model performance, predictive uncertainty, epistemic uncertainty and aleatoric uncertainty. Overall, we find that our manipulations increase predictive uncertainty in all four benchmarked UQ methods. In particular, we find that their aleatoric uncertainty components mostly track our devised label-level manipulations while their epistemic components mostly track our devised image-level manipulations. This serves as proof-of-concept that Arctique facilitates meaningful and comprehensive UQ benchmarking. Arctique was rendered and assessed on an internal resource of Nvidia A40 GPUs. Our work amounted to an estimated total of 150 GPU-hours. Dataset access The current version of our dataset, as well as the complete version history, can be accessed via https://doi.org/10.5281/zenodo.11635056. We provide access to 50,000 training and 1,000 test images along with their corresponding instance and semantic masks, including 400 additional exemplary variations corresponding to 50 of the test images. We also provide the dataset used for the experimental results presented in this paper as well as the respective noisy variations. The complete codebase containing scripts for dataset generation, model training and uncertainty estimation is available on GitHub: https://github.com/Kainmueller-Lab/arctique"
https://arxiv.org/html/2411.07096v2,Extreme Rotation Estimation in the Wild,"We present a technique and benchmark dataset for estimating the relative 3D orientation between a pair of Internet images captured in an extreme setting, where the images have limited or non-overlapping field of views. Prior work targeting extreme rotation estimation assume constrained 3D environments and emulate perspective images by cropping regions from panoramic views. However, real images captured in the wild are highly diverse, exhibiting variation in both appearance and camera intrinsics. In this work, we propose a Transformer-based method for estimating relative rotations in extreme real-world settings, and contribute the ExtremeLandmarkPairs dataset, assembled from scene-level Internet photo collections. Our evaluation demonstrates that our approach succeeds in estimating the relative rotations in a wide variety of extreme-view Internet image pairs, outperforming various baselines, including dedicated rotation estimation techniques and contemporary 3D reconstruction methods.","The problem of estimating the relative 3D orientation between a pair of images is embodied in fundamental computer vision tasks, such as camera localization [4, 33, 32] and 3D reconstruction [35, 27, 41]. Establishing pixel correspondences (either explicitly or implicitly) is typically a prerequisite for computing the relative rotation between the images. Correspondences, however, cannot be extracted in extreme settings where the images have little to no overlap. As dense imagery may not necessarily be available for many practical applications, a natural question arises: How can we estimate the relative rotation between non-overlapping RGB images, without the use of additional data (such as depth or temporal information)? Figure 1: Given a pair of (possibly) non-overlapping images captured in the wild—e.g., under arbitrary illumination and intrinsic camera parameters—such as the images of the Dam Square in Amsterdam depicted in red and blue boxes above∗, our technique estimates the relative 3D rotation between the images. ∗The background panorama is illustrated for visualization purposes only. We have recently seen pioneering efforts addressing the task of relative rotation estimation in such extreme non-overlapping settings [6, 10]. Prior work has proposed end-to-end neural architectures, demonstrating that hidden cues, such as vanishing points or the directions of cast shadows, can implicitly guide the model for inferring the relative orientation between the images. To facilitate learning and evaluation, datasets constructed from panoramic views were adopted. These datasets emulate perspective views by cropping sub-areas from these panoramas, enabling generation of image pairs with various degrees of overlap. However, while such emulated views perhaps capture some of the challenges associated with extreme-view imagery, are they sufficient for representing real images—particularly, images captured in the wild? In this paper, we present a new approach that tackles the problem of extreme rotation estimation in the wild. Consider the boxed images in Figure 1. Internet (i.e., in the wild) images may vary due to a wide range of factors, including transient objects, weather conditions, time of day, and the cameras’ intrinsic parameters. To explore this problem, we introduce a new dataset, ExtremeLandmarkPairs (ELP), assembled from publicly-available scene-level Internet image collections. We observe that the set of real extreme-view image pairs is limited, as Internet datasets are typically scene-centric, with nearby cameras commonly capturing overlapping views. Therefore, to facilitate training, we propose a progressive learning scheme that leverages and augments images cropped from panoramic views, allowing for gradually generalizing the model onto real Internet data. In particular, we construct datasets with varying field of views, that better resemble the distribution of real data, and perform image-level appearance augmentations by leveraging recent advancements in text-to-image Diffusion models [29, 19, 5]. To estimate extreme rotations in the wild, we propose a Transformer-based model that is provided with auxiliary channels, including the spatial distribution of local keypoints and matches and semantic segmentation maps, allowing for better reasoning over real image pairs with little or no overlap. Our results demonstrate that our model can accurately predict the relative rotations for a wide variety of extreme-view image pairs that vary in illumination, dynamic regions, and intrinsic parameters. We conduct extensive experiments, quantifying performance both over real Internet data and also over emulated perspective images cropped from panoramic views. Our evaluation shows that our model significantly improves over strong baselines when considering real images, while achieving comparable performance over emulated perspective image pairs."
https://arxiv.org/html/2411.07076v1,StoryTeller: Improving Long Video Description through Global Audio-Visual Character Identification,"Existing large vision-language models (LVLMs) are largely limited to processing short, seconds-long videos and struggle with generating coherent descriptions for extended video spanning minutes or more. Long video description introduces new challenges, such as plot-level consistency across descriptions. To address these, we figure out audio-visual character identification, matching character names to each dialogue, as a key factor. We propose StoryTeller, a system for generating dense descriptions of long videos, incorporating both low-level visual concepts and high-level plot information. StoryTeller uses a multimodal large language model that integrates visual, audio, and text modalities to perform audio-visual character identification on minute-long video clips. The results are then fed into a LVLM to enhance consistency of video description. We validate our approach on movie description tasks and introduce MovieStory101, a dataset with dense descriptions for three-minute movie clips. To evaluate long video descriptions, we create MovieQA, a large set of multiple-choice questions for the MovieStory101 test set. We assess descriptions by inputting them into GPT-4 to answer these questions, using accuracy as an automatic evaluation metric. Experiments show that StoryTeller outperforms all open and closed-source baselines on MovieQA, achieving 9.5% higher accuracy than the strongest baseline, Gemini-1.5-pro, and demonstrating a +15.56% advantage in human side-by-side evaluations. Additionally, incorporating audio-visual character identification from StoryTeller improves the performance of all video description models, with Gemini-1.5-pro and GPT-4o showing relative improvement of 5.5% and 13.0%, respectively, in accuracy on MovieQA. Datasets and code are available at https://github.com/hyc2026/StoryTeller.","Generating detailed video descriptions is a fundamental challenge in video understanding. While large vision-language models (LVLMs) have made significant progress [41, 24, 8, 40, 23, 22], they remain limited to processing seconds-long videos and generating low-level visual concepts like objects, scenes and atomic actions. Real-world videos, such as movies, are much longer and require high-level information for effective description, such as plot development. Recent research works address long videos by segmenting them, generating descriptions for each segment, and combining them using large language models [47, 13, 14]. However, there is still a gap to resolve the key challenge of long video description: maintaining consistency. This includes ensuring logical coherence in the storyline, consistency in character descriptions and motivations, and overall continuity and fluidity of the description. Accurate character identification is essential for ensuring consistency in long video descriptions. While existing studies [32, 18] have improved the quality of video descriptions by focusing on character identification and tracking through visual elements, audio information is equally critical, as dialogue significantly contributes to drive the story progressing. Nevertheless, a gap remains in linking dialogue with on-screen characters to identify speakers, and further improve the capability for long video descriptions. Figure 1: Pipeline of StoryTeller, consisting of three main modules: Stage I - Video Segmentation, where long videos are divided into seconds-long clips that are relatively independent and internally complete; Stage II - Audio-Visual Character Identification, where characters are identified for each dialogue line using both audio and visual cues; and Stage III - Description Generation, where detailed descriptions are generated for each short clip, ultimately producing a coherent and consistent narrative for the entire long video. To address these issues, we propose a new task: given a video and its cast list with character photos and names, identify the speaker for each dialogue line in the video using both visual and auditory cues. If the character is in the cast, provide their name; otherwise, give a descriptive label. We refer to this task as audio-visual character identification. This task is challenging as it requires integrating visual, auditory, and textual information to link dialogue lines to character identities. To tackle this, we developed a multimodal large language model equipped with both visual and auditory inputs. Due to the limitation of models with visual encoder in handling only short segments, it struggles to utilize global auditory information (e.g., some lines spoken by the same character through a long video). To address this, we introduce a global decoding algorithm that incorporates global information during inference to improve accuracy of audio-visual character identification. Furthermore, we propose StoryTeller, a novel system for generating dense descriptions of long videos, incorporating both basic visual concepts and high-level plot information with high consistency. The system consists of three components: a video segmentation module, an audio-visual character identification module, and an identity-aware description generation module, as illustrated in Figure 1. A major challenge in long video description is the lack of training and test data. To address this, we introduce MovieStory101, a new dataset focused on movies—long videos with high information density. The videos in MovieStory101 are from Movie101[45] and Movie101v2 [46], comprising 187 movies split into 5,982 three-minute clips. Each clip is annotated with storyline descriptions, subtitles, and character identity labels for each dialogue line. The storyline annotations ensure a complete understanding of the movie’s plot. By training on this dataset, we aim to unleash the model’s ability to generate plot-level descriptions. In adiition, evaluating long video descriptions is also challenging. Besides human evaluation, we propose an automated evaluation method for MovieStory101’s test set: MovieQA. MovieQA has 38 multiple-choice questions in average for each video, covering visual actions, character relationships, and plot details. We evaluate a long video description by inputting it into GPT-4 [28] to answer these questions, using the accuracy as an evaluation metric. Based on MovieQA and human side-by-side evaluation, we demonstrate that StoryTeller outperforms Gemini-1.5-pro [35], GPT-4o [29], and several advanced open-source LVLMs [24, 20, 41, 7] in generating descriptions for long videos. Specifically, the 7B model-based StoryTeller achieves an accuracy in the MovieQA evaluation that is 9.5% higher than the best-performing baseline model, Gemini-1.5-pro. In human side-by-side evaluation, StoryTeller shows an advantage of +15.56% over Gemini-1.5-pro. Additionally, we verify that incorporating audio-visual character identification results from StoryTeller as input features consistently enhances the long video description capabilities of various LVLMs. In particular, Gemini-1.5-pro and GPT-4o achieve relative improvements of 5.5% and 13.0% in accuracy on MovieQA, respectively. Figure 2: Audio-visual character identification module. (a) A multimodal large language model integrating visual, audio and text inputs is employed to identify characters in seconds-long videos. (b) Global Decoding: During inference, consistent mapping of global IDs across different short clips ensures the same global ID is assigned the same character name."
https://arxiv.org/html/2411.07074v1,Increasing Rosacea Awareness Among Population Using Deep Learning and Statistical Approaches,"Approximately 16 million Americans suffer from rosacea according to the National Rosacea Society. To increase rosacea awareness, automatic rosacea detection methods using deep learning and explainable statistical approaches are presented in this paper. The deep learning method applies the ResNet-18 for rosacea detection, and the statistical approaches utilize the means of the two classes, namely, the rosacea class vs. the normal class, and the principal component analysis to extract features from the facial images for automatic rosacea detection. The contributions of the proposed methods are three-fold. First, the proposed methods are able to automatically distinguish patients who are suffering from rosacea from people who are clean of this disease. Second, the statistical approaches address the explainability issue that allows doctors and patients to understand and trust the results. And finally, the proposed methods will not only help increase rosacea awareness in the general population but also help remind the patients who suffer from this disease of possible early treatment since rosacea is more treatable at its early stages. The code and data are available at https://github.com/cyang322/rosacea_detection.git.","Rosacea is a common skin condition that causes flushing or long-term redness on a person’s face, and it may also cause enlarged blood vessels and small, pus-filled bumps [1]. Some symptoms may flare for weeks to months and then go away for a while. In its early stages, rosacea might present as mild, transient redness (flushing), which can easily be overlooked. Patients might not seek medical attention until the condition has progressed, which can complicate early diagnosis. Early detection of rosacea, therefore, helps people increase rosacea awareness. Towards that end, we propose automatic rosacea detection methods using deep learning and statistical approaches. When applying the supervised deep learning methods on medical images, such as patients’ face images suffering from rosacea, there is always a problem of lacking sufficient labelled training data. The confidentiality of patients’ information further exacerbates the problem. The fact that some disease is rare makes it even more challenging to collect sufficient training data. While research on rosacea and related skin conditions has been conducted using machine learning and computer vision/deep learning algorithms, most high-performing studies leveraging deep learning used datasets with nearly 10,000 images or more. Several studies and others [2],[3],[4],[5] utilized substantial data; however, the datasets in these works are fully confidential, making them difficult to reproduce[6]. As a result, the deep learning methods tend to overfit the limited training data, which often leads to a biased representation of the population distribution. Although there has been work trying to generate more rosacea patients’ facial images[7] using the GAN[8], no work was done utilizing these generated images to improve the detection of rosasea to the best of our knowledge. Even if in certain cases the deep neutral networks achieve decent performance on the test data, the black-box nature of the deep learning models makes it hard to explain. However, the interpretability and transparency are required to ensure patient safety and clinical acceptance. We present a deep learning method and explainable statistical approaches for rosacea detection in this paper to address the issues of both explainable and limitation of the training data. The contributions of our proposed methods are three-fold. First, our proposed methods are able to automatically distinguish patients who are suffering from rosacea from people who are clean of this disease while trained on limited generated data. Second, our statistical approaches address the explainability issue that allows doctors and patients to understand and trust our results. And finally, our proposed methods will not only help increase rosacea awareness in the general population but also help remind the patients who suffer from this disease of possible early treatment since rosacea is more treatable at its early stages."
https://arxiv.org/html/2411.06991v1,SIESEF-FusionNet: Spatial Inter-correlation Enhancement and Spatially-Embedded Feature Fusion Network for LiDAR Point Cloud Semantic Segmentation,"The ambiguity at the boundaries of different semantic classes in point cloud semantic segmentation often leads to incorrect decisions in intelligent perception systems, such as autonomous driving. Hence, accurate delineation of the boundaries is crucial for improving safety in autonomous driving. A novel spatial inter-correlation enhancement and spatially-embedded feature fusion network (SIESEF-FusionNet) is proposed in this paper, enhancing spatial inter-correlation by combining inverse distance weighting and angular compensation to extract more beneficial spatial information without causing redundancy. Meanwhile, a new spatial adaptive pooling module is also designed, embedding enhanced spatial information into semantic features for strengthening the context-awareness of semantic features. Experimental results demonstrate that 83.7% mIoU and 97.8% OA are achieved by SIESEF-FusionNet on the Toronto3D dataset, with performance superior to other baseline methods. A value of 61.1% mIoU is reached on the semanticKITTI dataset, where a marked improvement in segmentation performance is observed. In addition, the effectiveness and plug-and-play capability of the proposed modules are further verified through ablation studies.","LiDAR point cloud semantic segmentation is considered crucial for scene understanding. It is regarded as the foundation for safe navigation in autonomous driving [1] and intelligent robotics [2]. However, due to the unstructured, unordered nature and sparsity of LiDAR point cloud data, traditional 2D deep learning methods [3] are difficult to directly apply to 3D point cloud processing [4]. In recent years, research in point cloud semantic segmentation has primarily focused on transforming point clouds into more regular structures, with projection-based [5, 6, 7] and voxel-based [8, 9, 10] methods included. However, the loss of critical local spatial features and the increase in computational overhead are accompanied by such methods during the transformation process [11]. Recently, researchers have proposed methods for the direct processing of raw point cloud data [12, 13], which reduce information loss, minimize dependence on complex processing and improve computational efficiency. And to overcome the difficulties associated with processing large-scale outdoor LiDAR point cloud data, hierarchical sampling methods and various local feature encoding modules have been proposed [13, 14, 15]. The enhancement of local contextual features in point clouds is improved by these approaches. Despite improvements in segmentation performance, several challenges are still encountered by point-based methods: 1) Boundary overlaps between different semantic classes are often unavoidable during neighborhood construction. Spatial features in existing methods are either inadequately encoded due to improper selection of spatial information or rigidly integrated in a mechanical manner. The inter-correlation of spatial information is inadequately considered in such integration, leading to information redundancy. 2) The connectivity of point cloud boundary points is weakened, and the accuracy of boundary segmentation is reduced due to insufficient fusion of spatial and semantic information. In applications of autonomous driving and robot navigation, potential risks are met by this boundary ambiguity, directly impacting navigation decisions and the safety of perception systems. To tackle the issue of boundary ambiguity between different semantic classes, a LiDAR point cloud semantic segmentation network SIESEF-FusionNet is proposed, based on enhanced spatial interconnectivity and spatially-embedded feature fusion. Specifically, an enhanced local spatial encoding (ELSE) module is designed to strengthen spatial inter-correlation by incorporating inverse distance weighting and angular compensation information. Additionally, spatially-embedded adaptive pooling (SEAP) module is utilized to embed the enhanced local spatial encoding into local semantic features for improving the contextual representation capability of local features. The proposed method offers the following advantages over existing point-based approaches: 1) Effectiveness: the impact of point cloud boundary ambiguity is mitigated and enhancing semantic segmentation performance; 2) Plug-and-play capability: integration into various model architectures can be readily achieved, effectively enhancing the feature learning capability of the model. The competitive segmentation performance of SIESEF-FusionNet is evaluated through extensive experiments on two outdoor LiDAR datasets, demonstrating the potential of the proposed method for intelligent transportation scenarios. Furthermore, the effectiveness and plug-and-play capability of the proposed modules are further validated through ablation studies. In summary, the primary contributions of this work are as follows: • A novel ELSE module is proposed to enhance the inter-correlation of local spatial information, thereby mitigating boundary ambiguity in the semantic segmentation of point clouds. To the best of our knowledge, this work is the first to consider spatial inter-correlation by utilizing inverse distance weighting and angular compensation in the application of LiDAR semantic segmentation. • A SEAP module is devised to embed enhanced local spatial encoding within local semantic features, preserving more features with sufficiently combined spatial information and semantic information than other pooling methods, thereby augmenting contextual perception capabilities. • Compared to the existing methods, the proposed ELSE and SEAP modules are plug-and-play compatible, allowing for effortless integration into point cloud segmentation networks and enhancing deep feature extraction. The remainder of this paper is organized as follows. Section II reviews recent advancements in 3D semantic segmentation. Section III provides a comprehensive description of SIESEF-FusionNet. Section IV provides a detailed account of extensive experiments and analyzes the results. Finally, Section V summarizes the main conclusions."
https://arxiv.org/html/2411.06976v1,A Hierarchical Compression Technique for 3D Gaussian Splatting Compression,"3D Gaussian Splatting (GS) demonstrates excellent rendering quality and generation speed in novel view synthesis. However, substantial data size poses challenges for storage and transmission, making 3D GS compression an essential technology. Current 3D GS compression research primarily focuses on developing more compact scene representations, such as converting explicit 3D GS data into implicit forms. In contrast, compression of the GS data itself has hardly been explored. To address this gap, we propose a Hierarchical GS Compression (HGSC) technique. Initially, we prune unimportant Gaussians based on importance scores derived from both global and local significance, effectively reducing redundancy while maintaining visual quality. An Octree structure is used to compress 3D positions. Based on the 3D GS Octree, we implement a hierarchical attribute compression strategy by employing a KD-tree to partition the 3D GS into multiple blocks. We apply farthest point sampling to select anchor primitives within each block and others as non-anchor primitives with varying Levels of Details (LoDs). Anchor primitives serve as reference points for predicting non-anchor primitives across different LoDs to reduce spatial redundancy. For anchor primitives, we use the region adaptive hierarchical transform to achieve near-lossless compression of various attributes. For non-anchor primitives, each is predicted based on the k-nearest anchor primitives. To further minimize prediction errors, the reconstructed LoD and anchor primitives are combined to form new anchor primitives to predict the next LoD. Our method notably achieves superior compression quality and a significant data size reduction of over 4.5\times compared to the state-of-the-art compression method on small scenes datasets.","3D Gaussian Splatting (GS) [3DGS] has demonstrated substantial advances in the field of novel view synthesis due to its impressive visual quality with ultra fast training speed. Different from Neural Radiance Field (NeRF) [nerf] with implicit representations, 3D GS uses serial explicit scattered isotropic ellipsoids to reconstruct the 3D scene. Each Gaussian consists of a 3D point center and several attributes, including scale vector, rotation quaternion, Spherical Harmonic (SH) coefficients, and opacity. Leveraging a highly optimized CUDA-based rendering implementation, 3D GS enables rapid training and rendering, making it highly suitable for practical applications. Additionally, its explicit data format is not only easy to understand and analyze, but also facilitates downstream processing (e.g., MGA [streaming]), making it a promising candidate for industrial application and standardization efforts. However, explicit point-based representations inherently result in significant storage overhead, as each point and its associated attributes must be stored independently. For example, reconstructing a large scene typically requires several million Gaussians, which can consume more than one gigabyte of memory. Therefore, compression of 3D GS becomes an essential technology to mitigate storage and transmission overhead. Currently, research on 3D GS compression can be categorized into two distinct branches: generative compression and traditional compression [GGSC]. The majority of studies focus on generative compression, employing techniques such as pruning, codebooks, and entropy constraints to produce more compact data representations during 3D GS generation. For example, LightGaussian [lightgaussian] prunes insignificant Gaussians based solely on the opacity parameter, and HAC [hac] introduces a hash-grid assisted context model to reduce spatial redundancy. However, although traditional compression has hardly been studied now, it remains an equally important area of research. GGSC [GGSC] is the first work to address this gap by proposing a simple but effective graph-based compression anchor. However, the performance of GGSC is limited as it does not fully exploit the spatial redundancy of 3D GS. In this paper, we introduce a Hierarchical GS Compression (HGSC) technique. We first prune Gaussians based on primitive importance scores assessed by both global significance and local significance: global significance is determined by each Gaussian’s contribution to rendering color across different views, while local significance is associated with the volume of each Gaussian, both of which are crucial for maintaining final rendering quality. Then, an Octree [octree] structure is employed to compress 3D positions. To reduce the influence of point merging within a voxel in Octree, the reconstructed points are recolored by applying the attributes of the nearest Gaussian from original 3D GS to ensure consistency. Based on the 3D GS Octree, we implement a hierarchical compression strategy. Specifically, we use a KD-tree [KD-tree] to split the 3D GS into multiple blocks and apply Farthest Point Sampling (FPS) [FPS] to select anchor primitives in each block and then generate varying Levels of Details (LoDs) primitives. These anchor primitives serve as references for predicting non-anchor primitives across different LoDs to reduce spatial redundancy. For anchor primitives, we employ the region adaptive hierarchical transform (RAHT) [RAHT] to achieve near-lossless compression of various attributes to enhance prediction accuracy. For non-anchor primitives, each is predicted by the k-nearest anchor primitives. Subsequently, the discrepancies between the predicted and actual attributes are quantized and subsequently encoded using the LZ77 [LZ77] codec. To minimize prediction errors, the current reconstructed LoD and anchor primitives are combined to form the new anchor primitives for predicting the next LoD. Overall, our method achieves better compression efficiency and reduced processing time compared to the benchmark GGSC."
https://arxiv.org/html/2411.06971v1,MapSAM: Adapting Segment Anything Model for Automated Feature Detection in Historical Maps,"Automated feature detection in historical maps can significantly accelerate the reconstruction of the geospatial past. However, this process is often constrained by the time-consuming task of manually digitizing sufficient high-quality training data. The emergence of visual foundation models, such as the Segment Anything Model (SAM), offers a promising solution due to their remarkable generalization capabilities and rapid adaptation to new data distributions. Despite this, directly applying SAM in a zero-shot manner to historical map segmentation poses significant challenges, including poor recognition of certain geospatial features and a reliance on input prompts, which limits its ability to be fully automated. To address these challenges, we introduce MapSAM, a parameter-efficient fine-tuning strategy that adapts SAM into a prompt-free and versatile solution for various downstream historical map segmentation tasks. Specifically, we employ Weight-Decomposed Low-Rank Adaptation (DoRA) to integrate domain-specific knowledge into the image encoder. Additionally, we develop an automatic prompt generation process, eliminating the need for manual input. We further enhance the positional prompt in SAM, transforming it into a higher-level positional-semantic prompt, and modify the cross-attention mechanism in the mask decoder with masked attention for more effective feature aggregation. The proposed MapSAM framework demonstrates promising performance across two distinct historical map segmentation tasks: one focused on linear features and the other on areal features. Experimental results show that it adapts well to various features, even when fine-tuned with extremely limited data (e.g. 10 shots). The code will be available at https://github.com/Xue-Xia/MapSAM.","From the 18th century onwards, the rapid development of cartography and geodesy led to the massive production of topographic maps at various scales [1]. For centuries, these maps have provided rich, detailed, and often geometrically accurate representations of numerous geographic entities. They serve as valuable resources for studying the evolution of urbanization and infrastructure, understanding geographic and anthropogenic changes over time, and exploring socio-ecological system dynamics [2, 3]. While information in raw scanned historical maps cannot be directly utilized, efficiently and effectively extracting features from these maps is crucial for ensuring accessibility and enabling spatio-temporal analysis for researchers and the public. The process of detecting the precise locations of geographic features from historical maps and assigning them semantic labels is known as historical map segmentation [4]. Current approaches to historical map segmentation are developed using traditional Convolutional Neural Networks (CNN) or Vision Transformers (ViT) [5], which require large volumes of high-quality labeled training data [6, 7, 8]. However, the manual labeling process for map data is extremely labor-intensive, demanding significant time and effort to ensure accuracy and consistency across the dataset. Recently, The rise of vision foundation models has introduced a new research paradigm within the realm of image segmentation [9]. Departing from the conventional approach of training domain-specific CNN or ViT models with extensive training data, foundation models, with their remarkable generalization capability, can quickly adapt to new downstream tasks with only low-resource fine-tuning or even zero-shot learning [10, 11, 12]. The Segment Anything Model (SAM) [13] is one of the first vision foundation models for image segmentation. The core design of SAM lies with “Promptable Segmentation”, a concept where a manually crafted prompt (e.g., points, boxes, masks, text) serves as input, yielding the desired segmentation mask as output. Fig. 1 showcases examples of using zero-shot SAM to segment features in historical maps. When directly applied to historical map segmentation, vanilla SAM encounters two major shortcomings: 1) It struggles to generalize effectively across various feature detection tasks in historical maps, indicating the need for incorporating domain-specific knowledge to enhance its performance; 2) It relies on human intervention to provide prompts, highlighting the desirability of a fully automatic solution. To overcome these challenges, we introduce MapSAM, an end-to-end framework designed to efficiently adapt SAM to the historical map domain. Figure 1: Examples of using zero-shot SAM and MapSAM for feature segmentation in historical maps. Rows (a) and (b) depict the segmentation of railways (two thick parallel lines) and vineyards (a set of vertical strokes), respectively. Zero-shot SAM requires manual prompts—green and red points representing positive and negative point prompts, respectively—but fails to delineate clear boundaries for the target objects effectively. In contrast, MapSAM eliminates the need for manual intervention and significantly improves segmentation accuracy. MapSAM adopts parameter-efficient fine-tuning (PEFT) and automatic prompting to customize SAM for historical map segmentation. Specifically, we freeze the heavyweight image encoder, and insert learnable DoRA (Weight-Decomposed Low-Rank Adaptation) [14] layers to acquire domain-specific knowledge for map feature extraction. DoRA does not alter the model architecture. Instead, it serves as a low-rank decomposition of the pre-trained weight matrix, enabling fine-tuning with only a minimal number of parameters. This approach allows for efficient adaptation of the pre-trained model to the target domain, even with limited resources for fine-tuning. Besides, to eliminate the reliance on human intervention for input prompts, we introduce an auto-prompt generator module‒a specially designed CNN that creates a coarse mask by fusing image embeddings from different encoder layers. Based on the confidence scores in the coarse mask, two points are selected as positive and negative location priors, serving as point prompts to be fed into the prompt encoder. Additionally, inspired by PerSAM [12], providing low-level positional prompts (e.g., the coordinates of a point) along with high-level target semantics offers the decoder additional visual cues, thus enhancing the segmentation result. Therefore, we extract the embedding of the target object using the coarse mask and fuse it with the original prompt tokens to form the final positional-semantic prompt tokens. These prompt tokens interact with the image embedding within the mask decoder to generate the final segmentation mask. However, Transformer-based models are often claimed to have slow convergence due to global context processing in the cross-attention layers of the Transformer decoder [15, 16]. To address this, Mask2Former [17] proposes masked attention, which constrains attention within the foreground target regions for intensive feature aggregation, achieving better convergence and results. Motivated by it, we introduce masked attention in MapSAM by leveraging the coarse mask to compel the prompt tokens to attend to localized object regions. With the aforementioned designs, we apply MapSAM to two customized historical map datasets: one featuring linear features for railway detection and the other featuring areal objects for vineyard detection. Both demonstrate favorable segmentation performance. We summarize the contributions of our paper as follows: 1. We pioneer the adaptation of the foundation model, SAM, for historical map segmentation. The proposed MapSAM is versatile and prompt-free, capable of automatically detecting both linear and areal features without the need for modifying the model architecture. 2. By incorporating advanced techniques tailored to historical map data—such as low-rank-based fine-tuning, auto-prompt generation, multi-layer feature fusion, positive-negative location prior selection, and masked attention—our framework achieves highly competitive results against domain-specific CNN models like U-Net [18] and current state-of-the-art SAM-based models like SAMed [10] and few-shot SAM [19]. 3. Since the trainable parameters of our method are lightweight, MapSAM also exhibits outstanding performance in a few-shot learning setting, demonstrating a strong capability to generalize from a minimal amount of training data. 4. There is a significant shortage of widely accessible historical map datasets for researchers to evaluate their techniques. To promote the advancement of automatic feature detection in the historical map domain, we will make our railway and vineyard datasets publicly available at https://doi.org/10.3929/ethz-b-000691430."
https://arxiv.org/html/2411.06966v1,Robust Fine-tuning of Zero-shot Models via Variance Reduction,"When fine-tuning zero-shot models like CLIP, our desideratum is for the fine-tuned model to excel in both in-distribution (ID) and out-of-distribution (OOD). Recently, ensemble-based models (ESM) have been shown to offer significant robustness improvement, while preserving high ID accuracy. However, our study finds that ESMs do not solve the ID-OOD trade-offs: they achieve peak performance for ID and OOD accuracy at different mixing coefficients. When optimized for OOD accuracy, the ensemble model exhibits a noticeable decline in ID accuracy, and vice versa. In contrast, we propose a sample-wise ensembling technique that can simultaneously attain the best ID and OOD accuracy without the trade-offs. Specifically, we construct a Zero-Shot Failure (ZSF) set containing training samples incorrectly predicted by the zero-shot model. For each test sample, we calculate its distance to the ZSF set and assign a higher weight to the fine-tuned model in the ensemble if the distance is small. We term our method Variance Reduction Fine-tuning (VRF), as it effectively reduces the variance in ensemble predictions, thereby decreasing residual error. On ImageNet and five derived distribution shifts, our VRF further improves the OOD accuracy by 1.5 - 2.0 pp over the ensemble baselines while maintaining or increasing ID accuracy. VRF achieves similar large robustness gains (0.9 - 3.1 pp) on other distribution shifts benchmarks. Codes are available in https://github.com/BeierZhu/VRF.","To ensure the reliability of machine learning systems, it is essential to develop models that can generalize to unseen, out-of-distribution environments. Large pre-trained models such as CLIP radford2021learning and ALIGN jia2021scaling have recently shown remarkable robustness against challenging distribution shifts. However, it is widely acknowledged that these improvements in robustness are most pronounced in the zero-shot setting, while conventional fine-tuning on these models often compromises robustness when compared to zero-shot performance wortsman2022robust ; kumarfine ; pmlr-v180-kumar22a . This phenomenon is known as the ID-OOD trade-offs, i.e., improving performance on in-distribution (ID) data can sometimes lead to decreased performance on out-of-distribution (OOD) data khani2021removing ; tripuraneni2020theory . In recent years, ensemble-based models (ESMs) have demonstrated significant success in addressing the ID-OOD dilemma yong2023spurious ; wortsman2022robust ; pmlr-v180-kumar22a ; zhu2024generalized . Specifically, denote the input as \mathbf{x}, the zero-shot model as \hat{\mathbb{P}}(y|\mathbf{x};\theta_{\mathsf{zs}}) and the fine-tuned model as \hat{\mathbb{P}}(y|\mathbf{x};\theta_{\mathsf{ft}}), existing ESMs typically employ the output-space ensemble (OSE) pmlr-v180-kumar22a ; zhu2024generalized , which outputs \hat{\mathbb{P}}(y|\mathbf{x};\theta_{\mathsf{ose}})=\alpha\hat{\mathbb{P}}(y|% \mathbf{x};\theta_{\mathsf{ft}})+(1-\alpha)\hat{\mathbb{P}}(y|\mathbf{x};% \theta_{\mathsf{zs}}), and the weight-space ensemble (WSE) wortsman2022robust ; yong2023spurious , which outputs \hat{\mathbb{P}}(y|\mathbf{x};\theta_{\mathsf{wse}})=\hat{\mathbb{P}}(y|% \mathbf{x};\alpha\theta_{\mathsf{ft}}+(1-\alpha)\theta_{\mathsf{zs}}),\ \text{% where}\ \alpha\in[0,1]. Compared to fine-tuned models, ESMs offer significant accuracy enhancements under distribution shift, while maintaining high ID accuracy. Figure 1: (a) ID-OOD frontier curves for the CLIP ViT-B/16 model on the ID (ImageNet) and OOD (IN-{V2, R, A, Sketch} and ObjectNet) datasets by varying the mixing coefficient \alpha. The ensemble model achieves its best ID and OOD performance at different \alpha values. Our method VRF simultaneously attains the best ID and OOD accuracy, outperforming the ensemble by 3.6\% on OOD and 1.6\% on ID at its optimal performance points.(b) Relationship between the ratio of fine-tuned accuracy to zero-shot accuracy (\frac{\text{Acc}_{\mathsf{ft}}}{\text{Acc}_{\mathsf{zs}}}) and the distance to the zero-shot failure set (d(\mathbf{x})). \frac{\text{Acc}_{\mathsf{ft}}}{\text{Acc}_{\mathsf{zs}}} demonstrates a monotonic decrease as d(\mathbf{x}) increases. However, ESM cannot fully address the ID-OOD trade-offs. In Figure 1 (a), by varying the mixing coefficient \alpha, we plot the ID-OOD frontier curves ( pink line) for the CLIP ViT-B/16 model on ImageNet deng2009imagenet (ID) and five derived distribution-shifted datasets (OOD): ImageNet-V2 recht2019imagenet , ImageNet-R hendrycks2021many , ImageNet-A hendrycks2021natural , ImageNet-Sketch wang2019learning and ObjectNet barbu2019objectnet . We find that the ensemble model achieves its optimal ID and OOD performance at different \alpha values: the best ID accuracy is achieved at \alpha=0.5 and the best OOD accuracy is obtained at \alpha=0.3. When the ensemble model reaches its optimal value for OOD, the performance on ID decreases by 3.6\% relative to its peak. Similarly, when the ensemble model is optimized for ID, the performance on OOD decreases by 1.6\% relative to its best value – the ID-OOD trade-offs still persist for ESMs. This raises a natural question: Can ensemble-based models simultaneously attain the best ID and OOD accuracy? In this paper, we affirmatively answer this question by proposing a sample-wise ensembling technique, dubbed variance reduction fine-tuning (VRF). This method is motivated by an empirical finding illustrated in Fig 1 (b). For each sample in the training dataset, if the fine-tuned model correctly predicts the label while the zero-shot model fails, we collect its features representation in the fine-tuned model as the zero-shot failure (ZSF) set. We then measure the distance d(\mathbf{x}) of each test sample \mathbf{x} to the ZSF set. Based on this distance, test samples are grouped into bins, and we compute the ratio of fine-tuned accuracy to zero-shot accuracy: \frac{\text{Acc}_{\mathsf{ft}}}{\text{Acc}_{\mathsf{zs}}} for each bin (implementation details are in Section C.7). Interestingly, we observe that the ratio \frac{\text{Acc}_{\mathsf{ft}}}{\text{Acc}_{\mathsf{zs}}} monotonically decreases as d(\mathbf{x}) increases. Intuitively, the closer a sample is to the ZSF set, the more likely it is that the zero-shot model makes incorrect predictions, whereas the fine-tuned model is more likely to be accurate, leading to a higher \frac{\text{Acc}_{\mathsf{ft}}}{\text{Acc}_{\mathsf{zs}}} ratio. Therefore, we use the distance to assign weights to the models: a smaller d(\mathbf{x}) results in a higher weight for the fine-tuned model, and vice versa. As depicted by the orange diamond in Fig. 1 (a), by leveraging the sample-wise weights, our VRF simultaneously attains the best ID and OOD accuracy. In Section 5, we show that on a variety of different models and tasks, our VRF approach consistently outperforms the existing fine-tuning and ensembling methods, including linear probing, end-to-end fine-tuning, LP-FT kumarfine , OSE and WSE wortsman2022robust . In specific, on ImageNet and five derived distribution shifts, our VRF further improves the OOD accuracy by 1.5 - 2.0 pp over the ensemble baselines while maintaining or increasing ID accuracy. Furthermore, in Section 4, we justify our approach by demonstrating that it effectively minimizes the variance of the ensemble models, resulting in reduced residual error."
https://arxiv.org/html/2411.06959v1,ENAT: Rethinking Spatial-temporal Interactions in Token-based Image Synthesis,"Recently, token-based generation approaches have demonstrated their effectiveness in synthesizing visual content. As a representative example, non-autoregressive Transformers (NATs) can generate decent-quality images in just a few steps. NATs perform generation in a progressive manner, where the latent tokens of a resulting image are incrementally revealed step-by-step. At each step, the unrevealed image regions are padded with [MASK] tokens and inferred by NAT, with the most reliable predictions preserved as newly revealed, visible tokens. In this paper, we delve into understanding the mechanisms behind the effectiveness of NATs and uncover two important interaction patterns that naturally emerge from NAT’s paradigm: Spatially (within a step), although [MASK] and visible tokens are processed uniformly by NATs, the interactions between them are highly asymmetric. In specific, [MASK] tokens mainly gather information for decoding. On the contrary, visible tokens tend to primarily provide information, and their deep representations can be built only upon themselves. Temporally (across steps), the interactions between adjacent generation steps mostly concentrate on updating the representations of a few critical tokens, while the computation for the majority of tokens is generally repetitive. Driven by these findings, we propose EfficientNAT (ENAT), a NAT model that explicitly encourages these critical interactions inherent in NATs. At the spatial level, we disentangle the computations of visible and [MASK] tokens by encoding visible tokens independently, while decoding [MASK] tokens conditioned on the fully encoded visible tokens. At the temporal level, we prioritize the computation of the critical tokens at each step, while maximally reusing previously computed token representations to supplement necessary information. ENAT improves the performance of NATs notably with significantly reduced computational cost. Experiments on ImageNet-2562 & 5122 and MS-COCO validate the effectiveness of ENAT. Code and pre-trained models will be released at https://github.com/LeapLabTHU/ENAT.","Recent years have witnessed an unprecedented growth in the field of AI-generated content (AIGC). In computer vision, diffusion models [10, 59, 61] have emerged as an effective approach. On the contrary, within the context of natural language processing, content is typically synthesized via the generation of discrete tokens using Transformers [72, 19, 5, 55]. Such discrepancy has excited a growing interest in exploring token-based generation paradigms for visual synthesis [7, 85, 33, 87, 6, 35]. Different from diffusion models, these approaches utilize a discrete data format akin to language models. This makes them straightforward to harness well-established language model optimizations such as the refined scaling strategies [5, 54, 31, 73] and the progress in model infrastructure [65, 12, 8, 34, 96]. Moreover, explorations in this field may facilitate the development of more advanced, scalable multimodal models with a unified token space [17, 68, 18, 44, 90] as well as general-purpose vision foundation models that integrate visual understanding and generation capabilities [35, 69]. Figure 1: The generation process of NATs starts from a masked canvas, decode multiple tokens per step, and are then mapped to the pixel space using a pre-trained VQ-decoder [13]. The recent advances in token-based visual generation have seen the rise of non-autoregressive Transformers (NATs) [7, 33, 6, 53], which are distinguished by their abilities to fulfill efficient and high-quality visual synthesis. As shown111We illustrate with 4\times4 tokens for simplicity; the actual token map size may be 16\times16 or larger. in Figure 1, NATs follow a progressive generation paradigm: at each generation step, a certain number of latent tokens of the resulting image are decoded in parallel, and the model carries out this process iteratively to produce the final complete token maps. More specifically, at each step, the unknown latent tokens of the image are represented with [MASK] tokens and concatenated with the tokens that have been decoded (i.e., visible tokens). Then, the full set of [MASK] and visible tokens is fed into a Transformer-based model, predicting the proper values of the unknown tokens, with the most reliable predictions preserved as the increments of visible tokens for the next generation step. In this paper, we seek to advance the understanding of the mechanisms behind the effectiveness of NATs’ progressive generation procedures. Our investigation uncovers two important findings regarding the spatial and temporal interactions within NATs: Spatially, at each generation step, even though both [MASK] and visible tokens are treated equivalently within the computational graphs of NATs, the visible tokens naturally learn to mainly provide information for [MASK] tokens to infer the unknown image content, and their corresponding deep representations can be built in the absence of [MASK] tokens. Temporally, the interactions between adjacent generation steps mainly concentrate on updating the representations of a small number of “critical tokens” on top of the previous steps. In fact, the computation for the remaining majority of tokens is generally repetitive. Inspired by these findings, we propose to develop novel NAT models to explicitly encourage these critical interaction mechanisms emerged naturally when trained for visual generation, yielding EfficientNAT (ENAT). Specifically, at the spatial level, we disentangle the computations of visible and [MASK] tokens by encoding visible tokens independently of [MASK] tokens. [MASK] tokens are then processed by attending to the fully contextualized features of visible tokens, as shown in Figure 3b. As an interesting observation derived from disentanglement, we find that prioritizing the computation for visible tokens, particularly when the computation is maximized for visible tokens and minimized for [MASK] tokens (even with only a single network layer), further improves the performance of NATs by a large margin. At the temporal level, we concentrate computation on the “critical tokens” while maximally reusing the representation of previously computed tokens to supplement the necessary information, as illustrated in Figure 4b. Empirically, the effectiveness of ENAT is validated on ImageNet 256\times256 [60], ImageNet 512\times512 [60] and MS-COCO [36]. ENAT is able to achieve significantly reduced computational cost compared to conventional NATs while outperforming them notably (e.g., 24% relative improvement with 1.8\times lower cost, see Table 6a)."
https://arxiv.org/html/2411.06921v1,UMFC: Unsupervised Multi-Domain Feature Calibration for Vision-Language Models,"Pre-trained vision-language models (e.g., CLIP) have shown powerful zero-shot transfer capabilities. But they still struggle with domain shifts and typically require labeled data to adapt to downstream tasks, which could be costly. In this work, we aim to leverage unlabeled data that naturally spans multiple domains to enhance the transferability of vision-language models. Under this unsupervised multi-domain setting, we have identified inherent model bias within CLIP, notably in its visual and text encoders. Specifically, we observe that CLIP’s visual encoder tends to prioritize encoding domain over discriminative category information, meanwhile its text encoder exhibits a preference for domain-relevant classes. To mitigate this model bias, we propose a training-free and label-free feature calibration method, Unsupervised Multi-domain Feature Calibration (UMFC). UMFC estimates image-level biases from domain-specific features and text-level biases from the direction of domain transition. These biases are subsequently subtracted from original image and text features separately, to render them domain-invariant. We evaluate our method on multiple settings including transductive learning and test-time adaptation. Extensive experiments show that our method outperforms CLIP and performs on par with the state-of-the-arts that need additional annotations or optimization. Our code is available at https://github.com/GIT-LJc/UMFC.","Recently, Vision-Language Foundation Models (VLFMs) such as CLIP radford2021clip , BLIP li2022blip , Flamingo alayrac2022flamingo and ALIGN jia2021align have demonstrated remarkable performance across various downstream tasks. These VLFMs formulate the training objective as contrastive learning, leveraging millions of image-text pairs to establish a shared embedding space. Equipped with a wide range of visual and text representations, VLFMs exhibit the capability to tackle downstream tasks in a zero-shot manner. Despite VLFMs being exposed to abundant examples, they may still encounter examples with new variations in downstream tasks. To address the problem of distribution shift between the pre-training and downstream domains, a natural approach involves fine-tuning VLFMs on various target tasks, such as prompt engineering zhou2022coop ; zhou2022cocoop and adapter learning gao2024clipadapter ; Zhang2022TipAdapter . However, these methods generally require labeled samples for fine-tuning, which is prohibitively expensive to be satisfied in reality. Conversely, abundant unlabeled data are often available for downstream tasks. Notably, in practical scenarios, the unlabeled data typically contains multiple domains, which exacerbates the adaptation difficulty of VLFMs. Therefore, in this paper, we aim to improve the adaptation performance of VLFMs directly on unlabeled data that naturally spans multiple domains. Figure 1: On DomainNet dataset, we visualize (a) The accuracy of CLIP on the six domains. (b) The image features extracted by CLIP’s image encoder across different domains. The visualization show that CLIP exhibits inherent model bias. (c) The number of predictions for different classes on quickdraw and painting domains. In this unsupervised multi-domain setting, we observe that CLIP cannot perform well when unlabeled data are drawn from mixed distributions. As shown in Figure 1, even within the same class space, the accuracy of CLIP varies significantly across different domains. While CLIP performs exceptionally well for images from common distributions encountered during pre-training, e.g., achieving 83.0\% accuracy on real domain, it struggles with rare distributions encountered during pre-training, e.g., only achieving 14.2\% accuracy on quickdraw domain. Above observations highlight that CLIP exhibits model biases that lead to incorrect predictions in specific scenarios. This raises a fundamental question: where do these biases originate? We point out that these model biases stem from deficiencies in the visual encoder and textual encoder. On the side of visual encoder, we observe that CLIP’ visual encoder prioritizes encoding domain information over discriminative category information. As shown in Figure 1, features from the same domain clearly cluster together, whereas a notable gap separates features from different domains. This phenomenon indicates that CLIP’s visual encoder exhibits a higher sensitivity to domain information over category information. When the domain of downstream tasks shifts from pre-trained tasks, the mismatch in domain-specific information encoded in image features could adversely affect classification accuracy. On the side of textual encoder, we observe that CLIP demonstrates varying category preferences across different domains. Specifically, CLIP tends to classify images into categories whose name are closely related to corresponding domain. As shown in Figure 1, within “quickdraw” domain, a large portion of samples (\sim 30\%) are classified as “squiggle” or “line” categories. Conversely, within “painting” domain, CLIP favors categories like “paintcan” and “paintbrush”. This observation shows that the class embeddings encoded by CLIP’s textual encoder inherently carry domain-specific information, misleading the model to prioritize categories highly associated with respective domains. Due to the combined effects of visual and textual encoder biases, CLIP’s performances vary significantly across different domains. In this paper, we aim to calibrate CLIP to mitigate the model biases. Initially, we analyze the model biases from a probabilistic standpoint. The factors influencing p(y|x), affected by domain variable z, can be decoupled into two parts: the sample distribution conditioned on classes p(x|y,z) and the class distribution p(y|z). If the two probability distributions are independent of z, domain shifts will not affect the predictions. To this end, we propose Unsupervised Multi-domain Feature Calibration (UMFC), a simple yet efficient framework for calibrating CLIP to generalize to various downstream tasks using multi-domain unlabeled data. UMFC jointly calibrates CLIP through two training-free modules: Image Feature Calibration module (IFC) and Text Feature Calibration module (TFC). Firstly, IFC focuses on calibrating CLIP’s image encoder to prioritize category-level over domain-level information, thereby reducing prediction error caused by domain shifts. Specifically, we calculate the average image features for each domain i, denoted as \mu_{i}, and posit that the prediction of \mu_{i} reflects the inherent bias of CLIP’s image encoder within that domain. By subtracting this domain-specific bias from original predictions, we can derive domain-agnostic predictions. Secondly, TFC focuses on calibrating CLIP’s text encoder to remove its preference towards domain-related class names. As observed by Dunlap2022lads ; Patashnik2021StyleCLIP ; fahes2023poda , a “global direction” exists in CLIP, representing the shift from training distribution to unseen distribution, shared across image and text embedding spaces. Motivated by this observation, we utilize the shift direction between different-domain images to estimate the text shift direction. Then, we subtract this shift vector to counteract the text encoder’s bias towards domain-related categories. By combing IFC and TFC, we can calibrate the features on both CLIP’s image and text encoders, thereby alleviating the model bias in downstream tasks. We validate the efficacy of UMFC on three downstream tasks: unsupervised calibration, transductive learning, and test-time adaptation, demonstrating consistent gains over CLIP. UMFC presents a low-cost solution for classification, unlocking the potential of CLIP-like models for practical scenarios characterized by abundant images across multiple domains but scarce labels."
https://arxiv.org/html/2411.06911v2,Gaussian Process Emulators for Few-Shot Segmentation in Cardiac MRI,"Segmentation of cardiac magnetic resonance images (MRI) is crucial for the analysis and assessment of cardiac function, helping to diagnose and treat various cardiovascular diseases. Most recent techniques rely on deep learning and usually require an extensive amount of labeled data. To overcome this problem, few-shot learning has the capability of reducing data dependency on labeled data. In this work, we introduce a new method that merges few-shot learning with a U-Net architecture and Gaussian Process Emulators (GPEs), enhancing data integration from a support set for improved performance. GPEs are trained to learn the relation between the support images and the corresponding masks in latent space, facilitating the segmentation of unseen query images given only a small labeled support set at inference. We test our model with the M&Ms-2 public dataset to assess its ability to segment the heart in cardiac magnetic resonance imaging from different orientations, and compare it with state-of-the-art unsupervised and few-shot methods. Our architecture shows higher DICE coefficients compared to these methods, especially in the more challenging setups where the size of the support set is considerably small. The code is available on GitLab 111https://gitlab.com/bruno˙viti/gpe˙4˙cardiac˙fss.","Medical imaging techniques like computed tomography (CT) and magnetic resonance imaging (MRI) are established technologies to assess patient health. Automatic medical image segmentation plays an important role by mapping anatomical structures to specific semantic labels which allows more in-depth analysis through follow-up applications [15]. This is especially relevant in cardiac imaging, with cardiovascular diseases being the world’s leading cause of death [25]. In recent years, deep learning (DL) models, and in particular Convolutional Neural Networks (CNNs), have been extensively employed to automate and accelerate heart segmentation, obtaining remarkable results in terms of accuracy [5]. CNNs generally require a large amount of labeled data for training, posing a challenge in the medical field with a lack of data and limited manual segmentations. Moreover, CNNs assume that training and test data are drawn from the same distribution, i.e., that they are i.i.d. Therefore, in order for the model to perform well on a different data distribution, it would require a large-scale labeled dataset to update the networks’ parameters for adaptation. Consequently, several DL approaches have been suggested to face the latter problem. For example, recent works proposed unsupervised domain adaptation or generalization architectures in the cardiac segmentation scenario [6, 13, 18, 4]. These models alleviate the need for extensively labeled data in the target domain by leveraging knowledge from a related source domain where labeled data is more abundant. However, while these techniques can handle perturbations between training and test images from different modalities and sources, they are not designed to adapt to novel image orientation [7]. Another promising technique, which can effectively reduce data dependency, is few-shot segmentation (FSS) [23, 3]. In the FSS framework, the model is trained to learn how to segment a novel image, denoted as query, having at its disposal only a small set of labeled examples, denoted as support. Especially in the medical field, FSS is gaining attention, and several FSS architectures have been proposed [8, 14, 17, 21]. The method’s effectiveness highly depends on the mechanism that extracts the information from the support set and integrates this new information with the query image. Most of the existing models rely on the prototype alignment paradigm, which tries to learn a common representation space, where the feature vectors of different object classes can be aligned, e.g. [24]. Differently, Johnander et al. [11] proposed a novel approach based on GPEs, which they exploit to learn a mapping between dense local deep feature vectors and their corresponding mask values. Saha et al. [22] also used FSS in combination with GPEs and applied their method to microscopy images. However, the majority of these approaches [8, 14, 17, 21, 24] are limited to predicting binary segmentations. As we also show in this work exemplarily for [14], this results in a non-efficient class-by-class segmentation for multi-label query images [9]. To face this issue, [9] introduced an extension of prototype alignment to perform one-step multi-class segmentation. They employ a self-supervised training approach, facing the challenge of limited labeled image data. However, in [9] they still consider the same type of images during training and testing, namely 2D short-axis cardiac MRI. In contrast, our goal is to cope with the scarcity of data in the cardiac setting and to make segmentation models more adaptable to different cardiac image orientations. Therefore, we propose a model for multilabel cardiac image segmentation, which combines a U-Net-like architecture [20] with GPEs as extractors of information from the support set [11]. Specifically, we use the contracting branch of the U-Net to bring both the query and the support images into the latent space, in which the GPEs are trained to learn the relationship between the support images and the corresponding masks. This additional information is aggregated into the expanding branch, leading to a more accurate segmentation of the query. We evaluate our model on the public M&Ms2 dataset [2, 16], and assess its ability to generalize from 2D short-axis (SA) to long-axis (LA) cardiac MRI."
https://arxiv.org/html/2411.06908v1,EVQAScore: Efficient Video Question Answering Data Evaluation,"Video question-answering (QA) is a core task in video understanding. Evaluating the quality of video QA and video caption data quality for training video large language models (VideoLLMs) is an essential challenge. Although various methods have been proposed for assessing video caption quality, there remains a lack of dedicated evaluation methods for Video QA. To address this gap, we introduce EVQAScore, a reference-free method that leverages keyword extraction to assess both video caption and video QA data quality. Additionally, we incorporate frame sampling and rescaling techniques to enhance the efficiency and robustness of our evaluation, this enables our score to evaluate the quality of extremely long videos. Our approach achieves state-of-the-art (SOTA) performance (32.8 for Kendall correlation and 42.3 for Spearman correlation, 4.7 and 5.9 higher than the previous method PAC-S++) on the VATEX-EVAL benchmark for video caption evaluation. Furthermore, by using EVQAScore for data selection, we achieved SOTA results with only 12.5% of the original data volume, outperforming the previous SOTA method PAC-S and 100% of data. Our code is available at https://anonymous.4open.science/r/EVQAScore-FFA6/.","With the rapid advancements in large language models (LLMs) [30, 36] and multimodal large language models (MLLMs) [45, 40], Figure 1: We give an example of EVQAScore, from the figure we can see that Answer1 is low quality and contains few information while Answer2 contains more information and is a more comprehensive answer. We can see our EVQAScore is the only score can correctly identify the high quality data. Among MLLMs, VideoLLMs achieve competitive performance in traditional multimodal tasks such as visual recognition [44], video understanding [41, 35], and action recognition [39]. Moreover, their excellent language understanding capabilities enable strong performance in text-rich tasks, such as video question-answering [9] and video-centric dialogues [39]. Most existing VideoLLMs focus on modifying model architectures to effectively utilize information from multiple modalities [38, 39, 26, 43, 17]. While model effectiveness is crucial, data quality also plays a significant role in the success of VideoLLMs. For instance, Li et al. [18], Wang et al. [39] demonstrate that higher-quality training data enhances the performance of VideoLLMs. Therefore, evaluating video QA data quality is essential. However, previous methods such as EMScore and PAC-S [34, 33] focus exclusively on video caption evaluation while neglecting video QA evaluation. Moreover, these methods struggle with understanding the correct semantics of longer captions or QA pairs. As illustrated in Figure 1, EMScore and PAC-S fail to identify higher-quality video QA pairs. These methods overlook the importance of efficient and robust data management and face three key challenges: C1. Inability to Accurately Understand Caption and QA Semantics Previous methods such as EMScore [34] and PAC-S [33] rely on TF-IDF to evaluate video caption data. However, TF-IDF merely computes word frequency and does not capture semantic meaning. As a result, these methods often fail to accurately evaluate video captions and are unable to assess video QA data with precision. Thus, there is a need for improved semantic understanding in Video QA evaluation. C2. Low Evaluation Efficiency Given the extensive volume and length of video data, large-scale training datasets frequently occupy substantial storage space, ranging from several hundred gigabytes to tens of terabytes [38, 39, 20]. Evaluating video caption and QA data is time-consuming and computationally costly. Previous methods like EMScore [34] and PAC-S [33] process every frame, leading to considerable computational waste. Therefore, developing a computationally efficient evaluation metric is essential. C3. Lack of VideoLLM Experiments Data plays a crucial role in the success of VideoLLMs [38, 39, 26, 43, 17]. Although various methods have been proposed to evaluate the quality of video caption data, they have not incorporated VideoLLM experiments to assess the effectiveness of their methods in training VideoLLMs. Consequently, additional VideoLLM experiments are needed. To address these challenges, we introduce EVQAScore, which utilizes LLMs to extract keywords for enhanced video captioning and video QA data evaluation. To support the evaluation of longer videos, we efficiently compute EVQAScore through uniform frame sampling, which reduces computational costs by a factor of 30 without affecting results. Compared to previous methods such as EMScore [34] and PAC-S [33], our approach can be applied to video QA evaluation and achieves superior visual understanding through keyword extraction, outperforming the TF-IDF technique used by EMScore and PAC-S. Additionally, due to its efficiency and keyword extraction capabilities, our method can handle extremely long captions and videos, a feat not possible for EMScore and PAC-S because of the 77-word limit imposed by CLIP. The core contributions of this paper are summarized as follows: • New Perspective: We introduce a novel approach, EVQAScore, designed for the evaluation of Video QA and video caption data quality. To the best of our knowledge, this is the first systematic effort to evaluate the quality of Video QA data. • New Method: Our approach leverages LLMs and keyword extraction techniques to improve the understanding of QA data in Video QA evaluation. Additionally, we employ uniform frame sampling to enhance scalability, allowing EVQAScore to be applied to extremely long videos and large datasets. • Efficient Video QA Data Evaluation: We use uniform frame extraction, resulting in a computational cost reduction of over 30 times compared to processing all frames. Furthermore, we demonstrate that this reduction does not lead to any performance degradation. • SOTA Performance for Video Caption Evaluation: We evaluated the performance of our method on the VATEX-Eval benchmark for Video Caption data, achieving a Kendall correlation of 32.8 and a Spearman correlation of 42.3, which are 4.7 and 5.9 points higher, respectively, than the previous method PAC-S++. • SOTA Performance for Video QA Evaluation: We constructed a dataset comprising 400K video entries, consisting of 200K noisy samples and 200K high-quality samples. Using our EVQAScore and the previous SOTA model PAC-S, we filtered the data, retrieving a significantly lower amount of noisy data than PAC-S. Additionally, when evaluated on ActivityNet, MSRVTT, MSVD, TGIF, MVBench, and two longer benchmarks—VideoChatGPT Bench and VideoChatGPT Bench Diverse—our EVQAScore outperformed PAC-S and 100% of data with only 12.5% of data."
https://arxiv.org/html/2411.06896v1,BuckTales: A multi-UAV dataset for multi-object tracking and re-identification of wild antelopes,"Understanding animal behaviour is central to predicting, understanding, and mitigating impacts of natural and anthropogenic changes on animal populations and ecosystems. However, the challenges of acquiring and processing long-term, ecologically relevant data in wild settings have constrained the scope of behavioural research. The increasing availability of Unmanned Aerial Vehicles (UAVs), coupled with advances in machine learning, has opened new opportunities for wildlife monitoring using aerial tracking. However, limited availability of datasets with wild animals in natural habitats has hindered progress in automated computer vision solutions for long-term animal tracking. Here we introduce BuckTales, the first large-scale UAV dataset designed to solve multi-object tracking (MOT) and re-identification (Re-ID) problem in wild animals, specifically the mating behaviour (or lekking) of blackbuck antelopes. Collected in collaboration with biologists, the MOT dataset includes over 1.2 million annotations including 680 tracks across 12 high-resolution (5.4K) videos, each averaging 66 seconds and featuring 30 to 130 individuals. The Re-ID dataset includes 730 individuals captured with two UAVs simultaneously. The dataset is designed to drive scalable, long-term animal behaviour tracking using multiple camera sensors. By providing baseline performance with two detectors, and benchmarking several state-of-the-art tracking methods, our dataset reflects the real-world challenges of tracking wild animals in socially and ecologically relevant contexts. In making these data widely available, we hope to catalyze progress in MOT and Re-ID for wild animals, fostering insights into animal behaviour, conservation efforts, and ecosystem dynamics through automated, long-term monitoring.","In a rapidly changing world, behaviour is the most flexible tool in an animal’s toolkit to solve problems and adapt to novel challenges. While numerous observational and automated sensor-based techniques exist to study animal behaviour, the past decade has seen a significant increase in the use of animal tracking from camera-based technology (Couzin and Heins, 2023; Tuia et al., 2022). The ability to record detailed movement and behavioural data of individual animals non-invasively, and over extended periods, has facilitated a range of discoveries across various aspects of animal lives—from the study of animal architecture (Smith et al., 2021) and the understanding of individual and collective animal decision-making (Sridhar et al., 2021; Sampaio et al., 2024), to revealing the mechanisms of social interactions that result in coordinated collective movement (Torney et al., 2018; Sridhar et al., 2023) and anti-predatory strategies (Rosenthal et al., 2015; Sosna et al., 2019; Davidson et al., 2021). While insights gained from vision-based animal tracking are on the rise, a lot of this work is still restricted to controlled laboratory settings (Naik et al., 2023). To translate these methods for research on animals in their natural habitats, biologists are beginning to leverage advances in aerial imagery (through the use of Unmanned Aerial Vehicles or UAVs) and state-of-the-art techniques in computer vision and machine learning to record and track freely-moving animals in the wild (Koger et al., 2023; Price et al., 2023; Ozogány et al., 2023; Maeda et al., 2021). However, the problem remains that most publicly available datasets of animal monitoring from UAVs do not offer ground truth for movement tracking problems. Furthermore, animals in the wild do not restrict their movement to specific areas captured within field-of-view of a single UAV. Therefore, recording behaviours occurring in relatively large spatial scales requires robust tracking solutions and the fusion of information from multiple simultaneously-flying UAVs. Here we present BuckTales, the first dataset designed to study the behaviour of wild animals (in this case, blackbuck) in a large area using multiple simultaneously-flying UAVs. The dataset focuses on the task of UAV-based multi-object tracking (MOT) and re-identification (Re-ID) of wild animals. The MOT dataset contains 22.5K frames (12.5 min) in total with the longest video >3 min (5805 frames) and average of 75 individuals per video. The Re-ID dataset focuses on the task of merging the tracking data from multiple UAVs and thus annotations are provided for each antelope moving in the overlapping region of a UAV pair. These data are collected in collaboration with behavioural biologists studying lekking–an extremely rare mating system observed in <2% of mammalian species. Thus, our dataset presents a unique opportunity where results from algorithms trained here may have cross-disciplinary and direct real-world applications. This article is written for both biologists and computer scientists. Therefore, we provide details for creating such datasets and the relevant code base for biologists to replicate our process. Similarly, for the computer science community, we offer baseline experiments with commonly known detection and tracking methods to demonstrate limitations of state-of-the-art methods. An added contribution of our dataset is that it is accompanied by specific analyses that highlight its suitability to becoming a benchmark dataset for solving large scale animal tracking with multiple camera sensors. Figure 1: A schematic of the data collection strategy and dataset details. The image in the top displays top-down view of a blackbuck lek from a single drone with male territories marked. The close-ups on the right show an example male and female. The bottom figure is a simplified data collection scheme, shown here with two drones (note that the actual collection scheme involved three drones). Three types of annotations are made available with the manuscript: object detection, multi-object tracking and re-identification."
https://arxiv.org/html/2411.06893v1,Multi-scale Frequency Enhancement Network for Blind Image Deblurring,"Image deblurring is an essential image preprocessing technique, aiming to recover clear and detailed images form blurry ones. However, existing algorithms often fail to effectively integrate multi-scale feature extraction with frequency enhancement, limiting their ability to reconstruct fine textures. Additionally, non-uniform blur in images also restricts the effectiveness of image restoration. To address these issues, we propose a multi-scale frequency enhancement network (MFENet) for blind image deblurring. To capture the multi-scale spatial and channel information of blurred images, we introduce a multi-scale feature extraction module (MS-FE) based on depthwise separable convolutions, which provides rich target features for deblurring. We propose a frequency enhanced blur perception module (FEBP) that employs wavelet transforms to extract high-frequency details and utilizes multi-strip pooling to perceive non-uniform blur, combining multi-scale information with frequency enhancement to improve the restoration of image texture details. Experimental results on the GoPro and HIDE datasets demonstrate that the proposed method achieves superior deblurring performance in both visual quality and objective evaluation metrics. Furthermore, in downstream object detection tasks, the proposed blind image deblurring algorithm significantly improves detection accuracy, further validating its effectiveness androbustness in the field of image deblurring.","With the increasing use of sensors and imaging devices, images have become an important medium for information. However, during the image acquisition process, motion blur inevitably occurs due to rapid object movement or camera shake. Blurred images not only negatively affect visual perception but also have adverse effects on downstream computer version tasks such as image segmentation and object detection. Early methods mainly focused on non-blind deblurring algorithms [1, 2, 3], which utilized prior knowledge, such as sparse prior [4], patch prior [5], and gradient prior [6], to estimate the blur kernel. Using the estimated blur kernel \mathit{k}, deconvolution operations are performed on the blurred images through inverse filtering, Wiener filtering, R-L filtering, and regularization algorithms to obtain sharp images. These deblurring methods rely heavily on the accurate estimation of the blur kernel \mathit{k}. However, in real-world scenarios, both the blur kernel \mathit{k} and the noise \mathit{n} in the equation are unknown, making the inverse problem of solving for the sharp image \mathit{I} an ill-posed problem with no unique solution. Consequently, non-blind deblurring algorithms have significant limitations in practical applications and are often inadequate for handling complex blur in real-world scenes. In recent years, many deep learning-based methods [7, 8, 9] have been proposed. These methods learn the nonlinear mapping relationship between the blurred image and the sharp image through end-to-end training[10]. They are better equipped to handle non-uniform blur in real-world scenarios. Aiming at the problem that most methods fail to integrate multi-scale feature extraction with frequency enhancement and insufficiently consider the issue of non-uniform blur, which restrict their ability to reconstruct fine textures, we propose a novel image deblurring method based on multi-scale blur perception and frequency enhancement. This method employs a single U-Net as the backbone network, combining multi-scale feature extraction with frequency enhancement, thereby integrating multi-scale features extracted from different network layers to achieve improved deblurring performance. The multi-scale feature extraction module (MS-FE) effectively captures multi-scale spatial and channel features of the images, facilitating a better understanding of both the overall structure and local details, which enhances the capability to handle non-uniform blur. The frequency enhanced blur perception module (FEBP) can simultaneously handle spatial and frequency domain features, offering superior processing capabilities for non-uniform blur in complex scenes. By employing multi-strip pooling, the network is able to perceive blurred regions in both horizontal and vertical directions, enabling targeted enhancement of deblurring performance based on its orientation. Furthermore, the use of discrete wavelet transform (DWT) [11] allows for the decomposition of the image into distinct frequency components, converting spatial domain features into the wavelet domain. This method can effectively restore edge and detail information in the image and thereby reducing detail loss due to blurring. The main contributions of this paper are summarized as follows. • We design a multi-scale feature extraction module (MS-FE) using depthwise separable convolution to capture details at various levels within the image, thereby enhancing the understanding of its overall structure and local details. • We design a frequency enhanced blur perception module (FEBP) that utilizes multi-strip pooling to perceive non-uniform blur in images. Additionally, we employ wavelet transform to capture frequency-domain information, supplementing the texture details of the image and enhancing the recovery of image textures. • Extensive experiments have demonstrated that our MFENet achieves better visual effect and quantitative metrics than the state-of-the-art techniques on the GoPro and HIDE datasets. Futhermore, in downstream object detection tasks, the proposed blind image deblurring algorithm significantly improves detection accuracy. In the subsequent sections of this paper, section 2 of this paper introduces the related research work of deep learning methods for image deblurring. Section 3 details the proposed methods and network structure. Section 4 describes the experimental details and analyzes the experimental results. Section 5 summarizes the full text."
https://arxiv.org/html/2411.06872v1,Multi-Modal interpretable automatic video captioning,"Video captioning aims to describe video contents using natural language format that involves understanding and interpreting scenes, actions and events that occurs simultaneously on the view. Current approaches have mainly concentrated on visual cues, often neglecting the rich information available from other important modality of audio information, including their inter-dependencies. In this work, we introduce a novel video captioning method trained with multi-modal contrastive loss that emphasizes both multi-modal integration and interpretability. Our approach is designed to capture the dependency between these modalities, resulting in more accurate, thus pertinent captions. Furthermore, we highlight the importance of interpretability, employing multiple attention mechanisms that provide explanation into the model’s decision-making process. Our experimental results demonstrate that our proposed method performs favorably against the state-of the-art models on commonly used benchmark datasets of MSR-VTT and VATEX.","Video captioning has the goal of automatically generating sentence to describe the video content [42, 13]. This area of research has emerged as an important area with multiple applications ranging from content indexing and retrieval [24, 33] to assist individuals with visual impairments [45] . However, complex nature of the video presents a significant challenge for captioning tasks. Compared to image captioning [7, 44, 22], videos consists of temporal sequences where multiple actions and events occur in the same time or in rapid succession. Additionally, audio cues, like as speech, music, and environmental sounds, introduce an extra layer of complexity that need to be managed to generate coherent caption. Despite making substantial progress, existing methods in this task is still considered inadequate in capturing the local and global representation. Various works often relies only on raw-pixels and one modality only [47, 26, 54, 58] and often neglect other modalities such as audio, speech or audio-caption. To address this, recent multi-modal approaches [8, 39, 15] integrate various modalities, however, work to relate such modalities is still lacking. This can be problematic, given that there exist depedency between these modalities on the video (i.e. video is mainly composed of both Auditory and Visual information that is interconnected). This is also compounded by a significant challenge lies in the interpretability of such complex systems. Understanding the reasoning behind a model’s caption generation is crucial for enhancing trust in these models. Thus our works will serve as the first mutli-modal work combining audio-caption and interpretability for video-captioning, that also relates these modalities on the same time. In this work, we propose a novel video captioning framework that leverages neural architectures and multi-modal fusion techniques to address aforementioned challenges. Our approach integrates visual and auditory information through a unified encoder-decoder model, utilizing attention mechanisms to focus on relevant features across different modalities. We also incorporate interpretability techniques to provide insights into the model’s decision-making process. We evaluate our model on benchmark datasets. Hence, the contributions of this work are as follow: 1. We introduce a multi-modal fusion strategy that effectively combines visual and audito features, enhancing the model’s ability to generate higher quality captions. 2. We incorporate interpretability techniques to analyze and visualize the model’s attention, providing insights into its decision-making process. 3. We show our competitive results on both well established and largest video captioning datasets of MSR-VTT and VATEX. The remainder of this paper is structured as follows: Sec 2 reviews related work in video captioning. Sec 3 details our proposed methodology, including the model architecture and training process. Sec 4 presents experimental results and analysis. Finally, Section 5 concludes the paper and outlines potential directions for future research."
https://arxiv.org/html/2411.06869v1,CapeLLM: Support-Free Category-Agnostic Pose Estimation with Multimodal Large Language Models,"Category-agnostic pose estimation (CAPE) has traditionally relied on support images with annotated keypoints, a process that is often cumbersome and may fail to fully capture the necessary correspondences across diverse object categories. Recent efforts have begun exploring the use of text-based queries, where the need for support keypoints is eliminated. However, the optimal use of textual descriptions for keypoints remains an underexplored area. In this work, we introduce CapeLLM, a novel approach that leverages a text-based multimodal large language model (MLLM) for CAPE. Our method only employs query image and detailed text descriptions as an input to estimate category-agnostic keypoints. We conduct extensive experiments to systematically explore the design space of LLM-based CAPE, investigating factors such as choosing the optimal description for keypoints, neural network architectures, and training strategies. Thanks to the advanced reasoning capabilities of the pre-trained MLLM, CapeLLM demonstrates superior generalization and robust performance. Our approach sets a new state-of-the-art on the MP-100 benchmark in the challenging 1-shot setting, marking a significant advancement in the field of category-agnostic pose estimation.","Most category-specific pose estimation concentrates on training a model to recognize keypoints of a single category, such as human [30, 25, 32], vehicle [19, 20], or animal [33, 11]. This task aims to learn from images with diverse patterns, enabling the model to possess robustness on unexpected poses and scenes not encountered during training. On the contrary, those models cannot cope with categories and keypoints which have not been met before. To estimate keypoints of novel categories, it’s essential for a model to comprehend where the points are located following additional information to figure. To this end, it is required to provide proper information about novel categories and their keypoints, allowing the model to leverage this data to deduce corresponding locations. Figure 1: Architectural difference from conventional CAPE methods. Previous methods (top) are support-dependent approaches requiring support images and keypoint annotations, but ours (bottom) is one of the support-free approaches which do not need any additional images and annotations, but just text description of the keypoints Category-Agnostic Pose Estimation (CAPE) tackles this issue [31, 24, 7, 22, 16, 2, 21]. It predicts keypoint positions of novel objects by employing the existing input image (called “query image”) with a set of supports. Typically, the support data comprises a pair: an image that belongs to the same category with the query but has a different pose from it, coupled with its corresponding keypoint annotations. A prevalent technique for utilizing support data is to train the model to fuse and connect features between the query and support images [31, 24, 7, 16, 2, 21]. In the pioneering work of [31], the training was done by maximizing the similarity between the coarse features of both images. Although this method is straightforward and intuitive, it poses a high risk of overfitting to the categories used in training. To address this problem, a two-stage architecture [24] was introduced by incorporating a process to refine the similarities produced by the model, leading to a more consistent performance. Recently, the types of support information have been diversified. A prominent example is to integrate skeletal structures—representing the connections between keypoints—into the training process as new ones. This technique not only achieves higher accuracy but also ensures robust performance against occlusions [7]. While maintaining the overall framework from previous approaches, an attempt has also been made to replace traditional support information with textual data [22]. By introducing the text-based approach, CAPE methods have gained some freedom from the inherent reliance on the supports, just replacing them with a sequence of keypoint names that are not unseen in training. Although outperforming the preceding designs [31, 24, 7] using the supports in 1-shot setting, this text-based method does not reach the 5-shot results of [7]. Employing the support information—support images and corresponding keypoint annotations—comes with inherent drawbacks. Since this method aligns support and query images that differ in many aspects aside from belonging to the same category, inadequate generalization during training can cause the model’s performance to vary depending on the quality of the support data, even with the same query image. Additionally, because keypoint information in the support is based on human annotations, it is inconvenient to update annotations whenever keypoints are modified. Even if the method [22] that uses an image with text as input seems to overcome those limitations, a structural dependency utilizing skeletal representations still exists, suggesting that a new approach that is simple yet yields reliable outputs is necessary. Here, we aim to address this issue by utilizing a Multimodal Large Language Model (MLLM). An MLLM [14, 27, 29] is based on a language model trained on vast amounts of multimodal data. Fully employing the Large Language Model’s (LLM’s) capability to comprehend texts, it generates the intended outputs, such as bounding boxes [28, 27, 29], masks [12]. Referring to the result that detailed descriptions make the model be improved [14] and the limitation of contrastive-learning methods in long sequences [34], we argue that it is desirable to reason keypoints of novel categories by adopting an instruction which has detailed information about them as an input and process it by exploiting the capability of LLMs serving trustworthy results [14]. Our work represents an effort to expand the MLLMs to CAPE. In order to adapt MLLMs into CAPE, we first defined the names and descripitons of keypoints and convert these textual information into an instrcution format appropriate to CAPE. Based on the instrcution, we set a model architecture to predict category-agnostic keypoint locations. The model includes a visual encoder and a LLM, as shown in Figure 1. Moreover, we investigate the performance variations caused by changes in the instruction and model structures. Consequently, we achieved state-of-the-art results on the MP-100 benchmark, surpassing the 5-shot performance of existing models with a 1-shot framework. Our contributions can be summarized as follows: • We introduce CapeLLM, a support image-free CAPE framework with advanced query-text comprehension capabilities, leveraging an MLLM. This work presents the first integration of a mechanism within the CAPE framework to comprehensively understand text query description through the application of MLLM. • We establish an optimal configuration for instructing the MLLM for CAPE, based on extensive experimental evaluations. This involves curating a comprehensive set of keypoint names and descriptions across all categories, and identifying the most effective configuration of the instruction given these descriptions for CAPE. • We achieve state-of-the-art results on the MP-100 dataset for CAPE, surpassing the 5-shot accuracy of existing methods [7, 22] with the 1-shot CapeLLM. Table 1 presents a comparative analysis highlighting the advantages of our approach over previous methods."
https://arxiv.org/html/2411.06851v1,"Fast and Efficient Transformer-based Method for 
Bird’s Eye View Instance Prediction","Accurate object detection and prediction are critical to ensure the safety and efficiency of self-driving architectures. Predicting object trajectories and occupancy enables autonomous vehicles to anticipate movements and make decisions with future information, increasing their adaptability and reducing the risk of accidents. Current State-Of-The-Art (SOTA) approaches often isolate the detection, tracking, and prediction stages, which can lead to significant prediction errors due to accumulated inaccuracies between stages. Recent advances have improved the feature representation of multi-camera perception systems through Bird’s-Eye View (BEV) transformations, boosting the development of end-to-end systems capable of predicting environmental elements directly from vehicle sensor data. These systems, however, often suffer from high processing times and number of parameters, creating challenges for real-world deployment. To address these issues, this paper introduces a novel BEV instance prediction architecture based on a simplified paradigm that relies only on instance segmentation and flow prediction. The proposed system prioritizes speed, aiming at reduced parameter counts and inference times compared to existing SOTA architectures, thanks to the incorporation of an efficient transformer-based architecture. Furthermore, the implementation of the proposed architecture is optimized for performance improvements in PyTorch version 2.1. Code and trained models are available at https://github.com/miguelag99/Efficient-Instance-PredictionKeywords: Instance prediction, Autonomous Driving, NuScenes.","I INTRODUCTION Nowadays, object detection, tracking, and prediction play critical roles in autonomous driving that have a direct impact on the safety and efficiency of Self Driving Vehicles (SDV). Object detection and tracking give these systems the ability to identify and classify important environmental elements such as pedestrians and vehicles. This is crucial to allow the vehicle to make the right decisions, avoiding collisions and granting the safety of the vehicle occupants, pedestrians, and other road users. On the other hand, the prediction of object trajectories and occupancy allows SDV to anticipate movements and take proactive measures, improving their ability to adapt to dynamic situations and reducing the risk of accidents. One State-Of-The-Art (SOTA) approach to this problem is to perform detection, tracking, and prediction independently so that the detection and tracking are focused on obtaining information about the past of the objects to perform the future prediction stage [1],[2]. Using this approach in a real-world system can lead to significant errors in the prediction stage due to the accumulated errors between the stages. It should also be noted that the performance evaluation of these motion prediction systems is usually based on the ground truth of the previous stages, resulting in metrics that do not consider the noise present in real systems, in which the detections and tracking are not perfect. Figure 1: Our proposed architecture uses a multi-camera system to predict the position of the instances in the scene. We achieve similar results as our baseline PowerBEV reducing the number of parameters and latencies. In recent years, techniques such as ”Lift, Splat, Shoot” [3] have succeeded in improving the feature representation of multi-camera perception systems thanks to a Bird’s-Eye View (BEV) transformation. This representation maintains the three-dimensional and depth values of the vehicle’s surroundings, discarding only the height information. As a result, this has led to the evolution of end-to-end systems that directly predict environmental elements from the vehicle’s sensor data. Architectures such as [4][5] use the images provided by a multi-camera setup to generate information about the instances of the scenes as well as information about their future motion and occupancy. Other SOTA approaches such as [6] propose a simplified instance prediction pipeline, training the system to perform only the BEV instance segmentation and the corresponding flow prediction. Usually, these end-to-end systems have relatively high processing times and number of parameters, which can be a challenge if they have to be deployed in a real vehicle. With this problem as the main focus, we propose a multi-camera BEV instance prediction architecture that uses the simplified paradigm presented in [6] and efficient attention modules specialized in dense tasks. The proposed architecture aims for fewer parameters and inference time than other SOTA architectures. We have also adapted the implementations of SOTA models to PyTorch version 2.1 for fair comparison."
https://arxiv.org/html/2411.06786v1,ScaleKD: Strong Vision Transformers Could Be Excellent Teachers,"In this paper, we question if well pre-trained vision transformer (ViT) models could be used as teachers that exhibit scalable properties to advance cross architecture knowledge distillation research, in the context of adopting mainstream large-scale visual recognition datasets for evaluation. To make this possible, our analysis underlines the importance of seeking effective strategies to align (1) feature computing paradigm differences, (2) model scale differences, and (3) knowledge density differences. By combining three closely coupled components namely cross attention projector, dual-view feature mimicking and teacher parameter perception tailored to address the alignment problems stated above, we present a simple and effective knowledge distillation method, called ScaleKD. Our method can train student backbones that span across a variety of convolutional neural network (CNN), multi-layer perceptron (MLP), and ViT architectures on image classification datasets, achieving state-of-the-art knowledge distillation performance. For instance, taking a well pre-trained Swin-L as the teacher model, our method gets 75.15%|82.03%|84.16%|78.63%|81.96%|83.93%|83.80%|85.53% top-1 accuracies for MobileNet-V1|ResNet-50|ConvNeXt-T|Mixer-S/16|Mixer-B/16|ViT-S/16|Swin-T|ViT-B/16 models trained on ImageNet-1K dataset from scratch, showing 3.05%|3.39%|2.02%|4.61%|5.52%|4.03%|2.62%|3.73% absolute gains to the individually trained counterparts. Intriguingly, when scaling up the size of teacher models or their pre-training datasets, our method showcases the desired scalable properties, bringing increasingly larger gains to student models. We also empirically show that the student backbones trained by our method transfer well on downstream MS-COCO and ADE20K datasets. More importantly, our method could be used as a more efficient alternative to the time-intensive pre-training paradigm for any target student model on large-scale datasets if a strong pre-trained ViT is available, reducing the amount of viewed training samples up to 195\times. The code is available at https://github.com/deep-optimization/ScaleKD.","Background. The great success of deep learning in computer vision (CV) has been driven by an explosion of neural network architectures among which convolutional neural networks (CNNs) [1, 2, 3], vision transformers (ViTs) [4, 5] and multi-layer perceptrons (MLPs) [6, 7, 8] are three major model categories. While CNNs were the de facto models for about a decade, recent progress shows that large ViT models have attained state-of-the-art performance on many visual recognition tasks such as image classification, image segmentation, and object detection. In principle, ViTs extend the philosophy of predominant transformer architectures [9] in natural language processing (NLP) to vision tasks. They convert an image into a sequence of equal-sized patches treated as tokens resembling words in NLP, then apply the dot-product self-attention mechanism over the sequence of image patches. ViTs designed in this way couple with a powerful data-hungry learning paradigm: models are first pre-trained on massive datasets (with supervised or self-supervised [10, 11] or cross-modality learning [12, 13]) and then fine-tuned on target datasets (with supervised learning). As the size of ViT models or pre-training datasets increases, the pre-trained models tend to have improved generalization performance. Despite this notable model performance scalability, the pre-training process of ViTs leads to significantly huge expenses. Furthermore, large pre-trained ViTs are memory-hungry and computationally intensive, prohibiting their deployment in many resource-constrained application scenarios. In contrast, CNNs and MLPs are still widely used in industry, due to the wider availability of effective implementations and optimizations compared to ViTs. Motivation of This Work. In parallel, knowledge distillation (KD) has proven to be a promising model compression pathway and has attracted lots of research interests. It relies on a teacher-student framework that transfers the knowledge learned by a large teacher model to a compact student model, aiming to make the student model can have improved performance to substitute the teacher model in deployment. However, most existing KD methods [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35] focus on CNN architectures, and usually perform evaluation on small datasets with non-mainstream student models for industrial applications, lagging far behind the evolution of neural network architectures. Although there have been few recent efforts [36, 37, 38, 39] on using ViT teachers, they explore narrow focuses that use small ViT teachers without pre-training on massive datasets, following the ways previously studied in CNN-based KD methods. In this paper, we attempt to connect knowledge distillation research with well pre-trained ViT models that stand out for their remarkable scalability, via a new viewpoint. Specifically, we question whether well pre-trained ViT models could be used as teachers that effectively transfer their scalable properties to target student models having different typed architectures such as CNN and MLP or heterogeneous ViT structures (we refer ‘cross architecture KD’ to such a more generalized formulation in this work), in the context of using mainstream large-scale visual recognition benchmarks. Problem Analysis. To answer the question in our motivation, we think the knowledge transfer difficulties are rooted in the following three aspects of differences: (1) Differences in feature computing paradigm. In terms of semantic units, ViTs operate on a sequence of equal-sized image patches added with positional embeddings, whereas CNNs operate on regular grids of pixels. In terms of core operations, ViTs rely on self-attention operations to model global feature dependencies, whereas CNNs rely on convolution operations to model local features. Although MLPs also use a patchify stem as ViTs, they rely on fully connected operations instead of self-attention operations and do not use positional embeddings, showing inferior feature learning ability. These differences in feature computing paradigm pose the first knowledge transfer barrier to overcome. (2) Differences in model scale. On the micro scale, model scale differences among ViTs, CNNs, and MLPs lie in network width, network depth, building blocks, etc. On the macro scale, model scale differences come from the capability of scaling the model size for ViTs, CNNs and MLPs towards better performance and generalization ability. As a result, these differences in model scale make the capacity of different network architectures typically vary significantly, emerging as the second knowledge transfer barrier to address. (3) Differences in knowledge density. Under the prevalent pre-training and fine-tuning paradigm, when scaling up pre-training datasets, large ViTs usually exhibit obviously superior performance scalability than top-performing CNNs and MLPs in terms of fine-tuning on both upstream image classification tasks and downstream dense prediction tasks [40, 41]. As for knowledge distillation in this work, we assume that pre-training datasets are no longer accessible and only well pre-trained ViT teacher models are available, avoiding the expensive pre-training process and making the setting well suited for real applications. Under this context, when training student models on upstream image classification datasets like ImageNet-1K, the knowledge density between teacher and student models is different, which appears as the third barrier to handle. From the above analysis, we can conclude that the design of effective schemes to align (1) feature computing paradigm differences, (2) model scale differences, and (3) knowledge density differences between the pre-trained ViT teacher and target student models, plays the key role to attain our goal. Design Insights and Contributions. Accordingly, we present Scalable Knowledge Distillation (ScaleKD), a simple and effective cross architecture KD method, which addresses the above difficulties in a progressive manner. Fundamentally, to bridge the feature computing paradigm differences between ViT and the other heterogeneous architectures, we propose cross attention projector (CAP, shown in Figure 1(a)), motivated by some previous works [42, 43, 44] that utilize cross attention mechanisms to align different modalities. For semantic unit differences, CAP utilizes positional embeddings and a patchify stem to transform the semantic units of CNN and MLP into transformer-like tokens. To further bridge core operation differences, CAP employs cross-attention operation and trainable queries that share the same attributes as the teacher’s features to model global interdependencies on the student’s features. In this way, CAP could align computing paradigm differences between the ViT teacher and the heterogeneous student in form, serving as the base component in our method. Different from feature computing paradigm differences, model scale differences and knowledge density differences are not explicitly and separately modeled in the KD process, as they are intertwined under the prevailing pre-training and fine-tuning paradigm and are finally encoded in teacher and student models’ feature space and parameter space. In light of this, we investigate both feature and parameter spaces of teacher and student models and observe two critical phenomena: Figure 1: Overview of three core components in our ScaleKD, which are (a) cross attention projector, (b) dual-view feature mimicking, and (c) teacher parameter perception. Note that the teacher model is frozen in the distillation process and there is no modification to the student’s model at inference. • Feature Space: As shown in Figure 2 and Figure 5, the frequency distributions of the features for the pre-trained ViTs are extremely imbalanced, where the direct component (zero frequency) response is dominant among all frequencies. This indicates that conducting feature distillation under such an imbalanced distribution may neglect the features of all other alternative components. • Parameter Space: As the parameters of the pre-trained ViTs in the fine-tuning stage are slightly changed, their pre-training knowledge remains in the parameter space. Although the pre-training datasets are not accessible in this work, the student still has the opportunity to obtain the pre-training knowledge by aligning its parameter space to the teacher’s. Inspired by these two insightful observations, we formulate our method from two new perspectives. Based on the observation in feature space, we design dual-view feature mimicking (DFM, shown in Figure 1(b)), whose key insight is to complement the neglected alternative features in the KD process. Specifically, DFM employs CAP as the feature projector and incorporates two feature mimicking paths. In the first path, DFM conducts feature mimicking in the original space to learn the teacher’s global features. In the second path, by removing the direct component in frequency space, DFM highlights the subtle alternative responses in feature mimicking, thus avoiding the neglect of these features. As a result, the two paths are complementary to each other, jointly promoting the feature space alignment. Based on the observation in parameter space, we propose teacher parameter perception (TPP, shown in Figure 1(c)), whose target is to transfer the pre-training knowledge by establishing a connection between teacher’s and student’s parameter spaces. Thanks to the aligned feature computing paradigm by CAP, TPP could bridge the student’s early stages to the teacher’s later stages and form a proxy feature processing path, where their parameter spaces join hands for KD optimization. By applying feature distillation in this path, the student’s parameter space tends to be gradually aligned with the teacher’s, and the pre-training knowledge would be transferred from the teacher to the student. Since the distillation learning processes in feature space and parameter space are the two sides of the same coin, DFM and TPP could naturally reinforce each other in essence. Benefited from the progressive designs, CAP, DFM, and TPP can be seamlessly integrated into a neat and effective cross architecture knowledge distillation method, called ScaleKD, which addresses the above three problems as a whole. Although ScaleKD has multiple feature mimicking paths, they only exist in the training stage. That is, ScaleKD does not alter the student’s structure and introduces no additional cost in the inference stage. By conducting systematic experiments on several mainstream large-scale vision benchmarks, we validate the effectiveness and generalization ability of our method. Figure 2: Feature distribution of BEiT-L/14 [41] in the frequency domain, where the direct component response is dominant. Details on drawing this figure are shown in Figure 5."
https://arxiv.org/html/2411.06780v1,HSTrack: Bootstrap End-to-End Multi-Camera 3D Multi-object Tracking with Hybrid Supervision,"In camera-based 3D multi-object tracking (MOT), the prevailing methods follow the tracking-by-query-propagation paradigm, which employs track queries to manage the lifecycle of identity-consistent tracklets while object queries handle the detection of new-born tracklets. However, this intertwined paradigm leads the inter-temporal tracking task and the single-frame detection task utilize the same model parameters, complicating training optimization. Drawing inspiration from studies on the roles of attention components in transformer-based decoders, we identify that the dispersing effect of self-attention necessitates object queries to match with new-born tracklets. This matching strategy diverges from the detection pre-training phase, where object queries align with all ground-truth targets, resulting in insufficient supervision signals. To address these issues, we present HSTrack, a novel plug-and-play method designed to co-facilitate multi-task learning for detection and tracking. HSTrack constructs a parallel weight-share decoder devoid of self-attention layers, circumventing competition between different types of queries. Considering the characteristics of cross-attention layer and distinct query types, our parallel decoder adopt one-to-one and one-to-many label assignment strategies for track queries and object queries, respectively. Leveraging the shared architecture, HSTrack further improve trackers for spatio-temporal modeling and quality candidates generation. Extensive experiments demonstrate that HSTrack consistently delivers improvements when integrated with various query-based 3D MOT trackers. For example, HSTrack improves the state-of-the-art PF-Track method by +2.3\% AMOTA and +1.7\% mAP on the nuScenes dataset.","Figure 1: Paradigm-level comparison. Our proposed method (Fig. 1c) effectively transforms the decoupled associative learning of the tracking-by-learnable-association paradigm (Fig. 1b) into enhanced auxiliary supervision, thus moderating the representation conflicts in single query of the tracking-by-query-propagation paradigm (Fig. 1a). The perception system is an indispensable component in autonomous driving. Within the system, consistent and accurate 3D multi-object tracking (MOT) provides reliable observations for prediction and planning. Camera-based algorithms have raised significant attention due to their cost-effectiveness (Hu et al. 2023a; Scheidegger et al. 2018). Since identity-aware tracking is largely reliant on identity-agnostic detection, this has prompted many works (Zhang et al. 2022a; Pang et al. 2023; Li et al. 2023c; Ding et al. 2024) to focus on the design of paradigms that extend from detection to tracking. In multi-view 3D object detection, research is primarily bifurcated into dense Bird’s-Eye View (BEV)-based (Huang et al. 2022; Li et al. 2023a, 2022) and sparse query-based algorithms (Wang et al. 2021; Liu et al. 2022, 2023; Wang et al. 2023). Inspired by the advances in Transformers for object detection, sparse query-based detectors (Wang et al. 2021; Liu et al. 2022, 2023; Wang et al. 2023) employ object queries to interact with multi-view image features and encode each object independently. These object-centric detectors have been naturally extended to identity-aware multi-object tracking algorithms. One paradigm for this extension allows track queries to alternately propagate and update, tracking targets with the same identity cross frames, while the object query detect new-born targets, referred to as tracking-by-query-propagation (TBQP, Fig. 1a) (Wang et al. 2021; Pang et al. 2023; Ding et al. 2024). Both queries are separately matched with distinct tracklets based on one-to-one label assignment. However, this combined design complicates the training process. On one hand, tracking models are typically initialized with pre-trained weights of detection. In the pre-training stage for single-frame detection, object queries are matched with all ground-truth targets, which inherently enables the object queries to consistently detect targets that persist. The propensity of object queries to match new-born objects not only reduces the supervision signals but also tends to confuse the model. On the other hand, a single query embedding perform both intra-temporal detection and inter-temporal association, which is difficult to optimize since both tasks share the same network parameters. Another paradigm for the extension of query-based MOT decouples detection and tracking, known as tracking-by-learnable-association (TBLA) (Li et al. 2023c; Ding et al. 2023). As illustrated in Fig. 1b, TBLA freezes the parameters of detector, allowing a differentiable learnable module to associate both queries and utilizing the matched object query to update track query. However, TBLA not only restricts the learning of object feature in detector but also limits the spatio-temporal modeling in tracker. This paper, from the perspective of optimizing training efficiency, aims to reconsider the relationship between detection and tracking in the tracking-by-query-propagation paradigm at a granular level. Drawing inspiration from works (Hu et al. 2023b; Jia et al. 2023), we conduct a detailed analysis of the components in the transformer-based decoder: the standard decoder layers include self-attention, cross-attention, and FFNs. The role of cross-attention is to dynamically aggregate multi-view image features, resulting in duplicate candidates for object queries and allowing track queries to model temporal context for iterative updates. The aggregation also closes the feature distance among queries around a single target. The role of self-attention is to de-duplicate candidates from both object queries and track queries, dispersing queries far away from each other. Based on the analysis, we argue that the dispersing effect of self-attention, forcibly isolating object queries from track queries, is sub-optimal. This not only causes the supervision of object queries to be inconsistent between detection pre-training and tracking fine-tuning, but also hinders the effective association between track queries and object queries. To address these issues, we present HSTrack, a novel plug-and-play method for end-to-end multi-camera 3D MOT framework that constructs a parallel weight-share decoder with hybrid supervisions for distinct queries, as illustrated in Fig. 1c. The parallel decoder removes all self-attention layers from the standard decoder. During tracking fine-tuning, both types of queries are collaboratively fed into the two decoders. The standard decoder still uses separate one-to-one supervision for both queries, while the parallel decoder uses one-to-one supervision for track queries and one-to-many supervision for object queries. This decoupled assignment in the parallel decoder ensures that track queries maintain identity-aware characteristics in spatio-temporal modeling, while object queries obtain more and better candidates representations. Subsequently, query association is employed between object queries and track queries to construct the affinity matrix. We treat pairs of queries matched to the same ground-truth targets as positive samples and others as negative samples, constructing bidirectional associative supervision. In this way, the auxiliary associative supervision enables the two types of queries to learn from each other and gain more useful information, thus harmonizing both detection and tracking. We conduct extensive experiments with multiple tracking-by-query-propagation paradigm methods to validate the effectiveness of HSTrack. Experimental results show that HSTrack significantly enhances the performance of both detection and tracking for end-to-end multi-view 3D MOT trackers. Specifically, HSTrack achieves +2.3\% improvement in AMOTA and +1.7\% improvement in mAP combined with the state-of-the-art PF-Track method on the nuScenes dataset."
https://arxiv.org/html/2411.06776v1,Machine vision-aware quality metrics for compressed image and video assessment,"A main goal in developing video-compression algorithms is to enhance human-perceived visual quality while maintaining file size. But modern video-analysis efforts such as detection and recognition, which are integral to video surveillance and autonomous vehicles, involve so much data that they necessitate machine-vision processing with minimal human intervention. In such cases, the video codec must be optimized for machine vision. This paper explores the effects of compression on detection and recognition algorithms (objects, faces, and license plates) and introduces novel full-reference image/video-quality metrics for each task, tailored to machine vision. Experimental results indicate our proposed metrics correlate better with the machine-vision results for the respective tasks than do existing image/video-quality metrics.","As the field of computer-vision continues to evolve, an increasing number of algorithms are being deployed in real-world applications. A popular application of this technology is video analytics, which has become integral to video surveillance, autonomous vehicles and other systems. Video analytics, for example, has two crucial tasks: detection and recognition. Depending on the system, the subjects of these tasks can be traffic signs, vehicles (object detection), human faces (detection/recognition), license plates (detection/recognition), and so on. As the number of video-surveillance cameras increases, automating these tasks becomes more critical given that human operators are incapable of processing such vast quantities of data. To ensure efficient storage and transmission of such extensive data, the captured images and videos require compression. Lossy compression standards such as JPEG, H.264/AVC, H.265/HEVC, and AV1 serve this purpose; their development involved optimizing the visual quality of the compressed content. The best way to assess visual quality is subjective human ratings, but they can be time-consuming and expensive. Hence the use of full reference (FR) quality-assessment methods such as PSNR, SSIM [24], and VMAF [2], some of which correlate highly with subjective scores [5]. These methods enable us to quickly and cost-effectively configure and develop codecs while emphasizing visual quality. Most state-of-the-art detection and recognition algorithms are based on deep neural networks, and their effectiveness is evaluated not visually, but through performance metrics. Compression directly affects the performance and reliability of computer-vision algorithms, especially at high compression ratios [6, 19]. Vision researchers and developers have therefore attempted to determine image quality for algorithms and develop codecs for machine vision [1]. For video surveillance system the main objects for analysis are people and vehicles, thus we choose three main video analytics algorithms for our machine-oriented quality metrics: object detection (including vehicles, persons, faces, and license plates), face recognition, license plates recognition. Often video surveillance systems use a specific detection/recognition algorithm (e.g. YOLOv5). Optimizing camera-data compression for them requires a comparatively fast method that predicts detection/recognition performance on the already encoded video, thereby enabling selection of encoding parameters to maximize that performance. If the target neural-network-based detection/recognition algorithms are evaluating the relative performance of codec prototypes or codec settings, they need lots of time and computational power owing to the number of parameters and the neural-network size. For instance, the recent x264 codec has almost 50 settings; selection of these parameters through an exhaustive search, even without using complex neural networks, would take centuries [30]. During investigation for our machine oriented metric we have following targets: 1. Achieve high correlation score with mentioned three main video analytics algorithms for its particular implementations with lower computational complexity. 2. Considering question about metric generalization for different implementation detection/recognition algorithms. Additionally, detection/recognition algorithms are imperfect, and identifying the cause of potential errors is impossible. For example, an object could be truly unrecognizable in the encoded image or video, necessitating quality improvement, or one algorithm may have certain limitations whereas another can detect the object error-free. Running multiple detection/recognition algorithms to improve the robustness of such an evaluation method would be even more time-consuming and computationally intensive. To address these issues, our paper makes three important contributions: 1. First, we propose a methodology of measuring image and video quality in terms of detection/recognition performance. 2. Second, we analyze detection- and recognition-performance correlation with that of popular image-quality-assessment (IQA) and video-quality-assessment (VQA) methods on widely used image and video codecs. Our results show little to no correlation between their outputs and detection/recognition performance. 3. Third, we propose new video-quality metrics based on convolutional-neural-network (CNN) models with respect to object detection, face detection/recognition, and license-plate detection/recognition performance. We validated our metrics by checking their correlation with the performance of the machine-vision algorithms for the corresponding tasks."
https://arxiv.org/html/2411.06764v1,Multi-Stage Knowledge Integration of Vision-Language Models for Continual Learning,"Vision Language Models (VLMs), pre-trained on large-scale image-text datasets, enable zero-shot predictions for unseen data but may underperform on specific unseen tasks. Continual learning (CL) can help VLMs effectively adapt to new data distributions without joint training, but faces challenges of catastrophic forgetting and generalization forgetting. Although significant progress has been achieved by distillation-based methods, they exhibit two severe limitations. One is the popularly adopted single-teacher paradigm fails to impart comprehensive knowledge, The other is the existing methods inadequately leverage the multimodal information in the original training dataset, instead they rely on additional data for distillation, which increases computational and storage overhead. To mitigate both limitations, by drawing on Knowledge Integration Theory (KIT), we propose a Multi-Stage Knowledge Integration network (MulKI) to emulate the human learning process in distillation methods. MulKI achieves this through four stages, including Eliciting Ideas, Adding New Ideas, Distinguishing Ideas, and Making Connections. During the four stages, we first leverage prototypes to align across modalities, eliciting cross-modal knowledge, then adding new knowledge by constructing fine-grained intra- and inter-modality relationships with prototypes. After that, knowledge from two teacher models is adaptively distinguished and re-weighted. Finally, we connect between models from intra- and inter-task, integrating preceding and new knowledge. Our method demonstrates significant improvements in maintaining zero-shot capabilities while supporting continual learning across diverse downstream tasks, showcasing its potential in adapting VLMs to evolving data distributions.","Figure 1: Performance (%) changes during continual learning on MTIL[1] for each task, in which MulKI with C_{0} and with C_{i-1} represent adopting only the initial model and only the preceding model for distillation, respectively. The metric “Transfer” denotes averaging the results of unseen tasks only, while “Current Avg.” averages only the results of seen tasks, the two metrics represent the mitigation of two forgetting issues, respectively. Our method effectively mitigates both forgetting issues. We omit the results of task 0 (lower than 55%) in (b) for clearer observation. We exclude LwF-VR here for its low accuracy. In recent years, Vision Language Models (VLMs) have garnered substantial attention due to their exceptional zero-shot generalization capabilities [2, 3, 4, 5], which is largely attributed to their extensive training on vast datasets. However, in real-world applications, data typically arrives in a continuous, streaming fashion, rendering large-scale, unified training less feasible. As a result, Continual Learning techniques for Vision Language Models (CL-VLMs) have been developed to focus on the incremental and adaptive learning for VLMs. Nevertheless, during the adaptation process for downstream tasks, CL-VLMs encounter two predominant challenges: 1) catastrophic forgetting[6], which entails the erosion of previously acquired knowledge, and 2) generalization forgetting, which compromises the model’s zero-shot generalization capability. These challenges pose significant obstacles to the practical deployment of VLMs, underscoring the need for advanced strategies to mitigate the adverse effects of knowledge degradation across evolving data streams. Figure 2: Illustration of three distillation paradigms and the utilization of additional data, where additional data is marked in bold. (a) LwF-VR[7] utilizes texts from past tasks (including texts in pretrain) to implement single-teacher distillation between current and previous models. (b) ZSCL[1] utilizes reference images and texts to implement single-teacher distillation between current and the initial models. (c) Our method utilizes only training data of current task, and implementing dual-teacher distillation among current, previous and the initial models. Through concerted research efforts, CL-VLMs has developed various strategies to tackle these challenges[8, 7, 1, 9]. Among these, distillation-based techniques stand out by eliminating the need for replayed samples and enhancing knowledge transfer across tasks. Notable approaches such as LwF-VR [7] and ZSCL [1] have achieved significant milestones. However, these methods still exhibit severe limitations that are pervasive in current distillation-based CL-VLM approaches, thereby hindering further development. Firstly, the single-teacher distillation paradigm [7, 1] fails to impart comprehensive knowledge to the student model, akin to how a single-subject teacher cannot cultivate the most outstanding students. For example, LwF-VR [7] employs the preceding model, while ZSCL [1] leverages the initial model. Both the pretrained and preceding models address either catastrophic forgetting or generalization forgetting, or partially mitigate these issues. As depicted in Fig.1(a), single-teacher distillation from the preceding model is less effective in mitigating generalization forgetting, a similar trend is observed with initial model distillation in Fig.1(b). Secondly, existing distillation methods inadequately leverage the multimodal information available in the original training dataset. For example, LwF-VR simplistically samples texts from the pretraining dataset for distillation, while ZSCL employs ImageNet [10] as a reference dataset. The extensive reference dataset not only demands substantial memory and computational resources but also contains overly rich semantic information, leading to significant performance degradation in ZSCL when the reference dataset is altered [1]. Consequently, both methods overly rely on supplementary data for distillation rather than leveraging the knowledge from the original training set, as illustrated in Fig.2(a) and (b). In general, the aforementioned deficiencies arise primarily from the imperfect integration and transfer of knowledge between modalities. According to the Platonic Representation Hypothesis [11], knowledge across different modalities exhibit convergent properties and equal importance in the platonic space after adaptation through extensive data. Besides, a single-teacher distillation approach is insufficient for forming a learning process akin to that of human beings. To address these issues, we draw on Knowledge Integration Theory (KIT) [12], which is well-known in the field of education. KIT emphasizes constructing new knowledge by establishing cross-modal connections between new information and prior knowledge. It outlines four stages for optimal human education: Eliciting Ideas, Adding New Ideas, Distinguishing Ideas, and Making Connections. Our approach aims to enhance existing distillation-based CL-VLMs by adhering to these four stages. Guided by KIT, we propose MulKI, a Multi-Stage Knowledge Integration network designed to emulate the human learning process in distillation methods. MulKI achieves this through four stages. First, in the Eliciting Ideas stage, we utilize prototypes to align different modalities, identifying and aligning core knowledge from each modality. Next, for Adding New Ideas stage, MulKI enriches existing knowledge by constructing fine-grained intra- and inter-modality relationships with prototypes, introducing new knowledge and information of teacher models from three levels. As for the Distinguishing Ideas stage, we refine and combine the knowledge from two teacher models through adaptively adjusting the sample-wise learning focus with respect to different teachers. Finally, in the Making Connections stage, MulKI integrates knowledge between models, balancing preceding and new knowledge from both intra- and inter-task perspectives. By following these four stages, MulKI not only facilitates comprehensive cross-modal knowledge integration, but also enhances the model’s adaptability in diverse data environments. Unlike LwF-VR[7] and ZSCL[1], MulKI requires no additional data or preservation of prototypes from prior tasks. Prototypes are promptly purged from memory upon task completion, requiring no data storage beyond model weights and using only the training dataset and pretrained CLIP weights. Figure 2(c) shows the paradigm of our method. In summary, our contributions are summarized as follows: (1) We introduce the Multi-Stage Knowledge Integration (MulKI) network, inspired by the KIT educational theory. Mimicking the human learning process, MulKI enhances existing distillation-based methods, fostering outstanding student model. (2) We design a four-stage learning process for MulKI akin to that of human beings, including Eliciting Ideas, Adding New Ideas, Distinguishing Ideas, and Making Connections. MulKI constructs high-quality knowledge by establishing cross-modal connections between new information and prior knowledge. (3) Our MulKI explores internal relationships across modalities by leveraging prototypes. It integrates new knowledge from various perspectives under a dual-teacher paradigm, without the need for extra data. The learning focus of the two teacher models is adaptively adjusted for optimal performance."
https://arxiv.org/html/2411.06757v1,LuSh-NeRF: Lighting up and Sharpening NeRFs for Low-light Scenes,"Neural Radiance Fields (NeRFs) have shown remarkable performances in producing novel-view images from high-quality scene images. However, hand-held low-light photography challenges NeRFs as the captured images may simultaneously suffer from low visibility, noise, and camera shakes. While existing NeRF methods may handle either low light or motion, directly combining them or incorporating additional image-based enhancement methods does not work as these degradation factors are highly coupled. We observe that noise in low-light images is always sharp regardless of camera shakes, which implies an implicit order of these degradation factors within the image formation process. This inspires us to explore such an order to decouple and remove these degradation factors while training the NeRF. To this end, we propose in this paper a novel model, named LuSh-NeRF, which can reconstruct a clean and sharp NeRF from a group of hand-held low-light images. The key idea of LuSh-NeRF is to sequentially model noise and blur in the images via multi-view feature consistency and frequency information of NeRF, respectively. Specifically, LuSh-NeRF includes a novel Scene-Noise Decomposition (SND) module for decoupling the noise from the scene representation and a novel Camera Trajectory Prediction (CTP) module for the estimation of camera motions based on low-frequency scene information. To facilitate training and evaluations, we construct a new dataset containing both synthetic and real images. Experiments show that LuSh-NeRF outperforms existing approaches. Our code and dataset can be found here: https://github.com/quzefan/LuSh-NeRF.","Neural Radiance Fields (NeRFs) mildenhall2021nerf ; barron2021mip ; barron2022mip ; chen2021mvsnerf ; liu2020neural have achieved notable success in modeling 3D scene information via implicit functions learned from a set of 2D images with known camera poses. Since optimizing NeRFs by measuring the colorimetric errors of training views essentially requires bright and sharp training images, hand-held low-light photography, which is prevalent in our daily life (e.g., in nighttime scenes), cannot be used directly for training NeRFs to produce visually pleasing novel view images (Fig. 1(b)), due to the co-existence of low visibility, noise, and camera motion blur in the captured images. (a) A Low-light Scene (b) NeRF mildenhall2021nerf (c) LEDNet zhou2022lednet +NeRF mildenhall2021nerf (d) Restormer zamir2022restormer +LLNeRF wang2023lighting (e) PairLIE fu2023learning +DP-NeRF lee2023dp (f) Ours Figure 1: Given a hand-held captured low-light scene (a), while (a combination of) existing low-light enhancement/NeRF methods may not produce visually pleasing novel-view images ((b)-(e)), our LuSh-NeRF can produce bright and sharp results (f). A straightforward solution is to incorporate existing low-light enhancement/deblurring methods (e.g., Xu_2023_CVPR_Structure ; Fu_2023_CVPR_contrastive ; zhou2022lednet ; zamir2022restormer ; fu2023learning ; wang2022semantic ) to preprocess the captured images before using them for training the NeRFs. However, it raises two problems. First, as these methods are typically image-based, they do not consider the multi-view consistency. Second, These enhancement methods may introduce additional artifacts (e.g., overexposure and unnatural color contrast). We note that there are some NeRF methods proposed for handling low-light scenes wang2023lighting ; cui2024aleth and scene motions ma2022deblur ; wang2023bad ; lee2023dp ; dai2023hybrid . However, while the former assumes no camera motions occur during the capture, the latter cannot handle low-light scenes. Directly applying them to our problem does not work. A visual example is shown in Fig. 1((c) to (e)), where existing methods struggle to render the desired results. We observe that in the captured low-light images, noise always appears sharp regardless of the camera shakes, due to the independent sensor noise generation within the collection and transformation of photons into electronic signals in the camera Image Signal Processor (ISP). This implies an implicit order of low visibility, sensor noise, and blur, which inspires us to model such an implicit order to decouple and remove those degradation factors for NeRF’s training in an unsupervised manner. In this paper, we propose a novel method, called LuSh-NeRF, to Light up and Sharpen NeRF by sequentially modeling the degradation factors. Specifically, the brightness of training images is first scaled up to provide more contextual information (which simultaneously amplifies the noise in the images). We then propose two novel modules, i.e., a Scene-Noise Decomposition (SND) module and a Camera Trajectory Prediction (CTP) module, to handle the noise and camera shake problems. The SND module learns to decouple noise from the implicit scene representation by explicitly learning a noise field through leveraging the multi-view feature consistency of NeRF. The CTP module then estimates the camera trajectories for sharpening image details based on the low-frequency information of denoised scene images rendered by SND. The two modules are optimized in an iterative manner such that the denoised scene representation of the SND module provides more information for predicting trajectories in the CTP module, while the results with sharp details from the CTP module in turn facilitate the denoising process in the SND module. To facilitate model training and evaluations, we construct a new dataset consisting of five synthesized scenes and five real scenes. As shown in Fig. 1(f), our LuSh-NeRF can render bright and sharp novel-view results. In summary, this work has the following main contributions: • We propose the first method (LuSh-NeRF) to reconstruct a NeRF from hand-held low-light photographs, by decoupling and removing degradation factors through modeling their implicit orders. • Our LuSh-NeRF contains two novel modules, a novel SND module for noise removal from the implicit scene representation and a novel CTP module for handling camera motions. • We construct the first dataset for training and evaluations. Experiments show that LuSh-NeRF outperforms existing methods."
https://arxiv.org/html/2411.06727v2,KAN not Work: Investigating the Applicability of Kolmogorov-Arnold Networks in Computer Vision,"Kolmogorov-Arnold Networks (KAN) have gained attention for their potential in capturing complex patterns, but the applicability of KAN and its variant, Convolutional KAN (CKAN), in computer vision remains a subject of significant debate. This study analyzes the performance of KAN and CKAN in computer vision tasks, with a focus on their robustness across different data scales and noise levels. The results show that CKAN, due to its extremely low generalization ability, often performs worse than traditional CNNs. Although the original KAN demonstrates strong fitting capabilities, its generalization ability is also weak, making it unsuitable for replacing the final layer of a model with an NLP layer. To address this issue, we propose a smoothness regularization method and a ”Segment Deactivation” technique, both of which significantly improve the generalization ability of KAN, leading to enhanced performance when replacing the final NLP layer.","In recent years, neural network[11, 9, 19] architectures have achieved remarkable breakthroughs in computer vision, especially in tasks such as image classification, and semantic segmentation[17, 22, 23]. Traditional models, particularly Convolutional Neural Networks (CNNs)[18, 17, 28, 10, 13] and the more recent Transformer architectures[33, 4, 6, 4, 20, 34, 35, 5], have been widely adopted in these areas due to their ability to capture spatial hierarchies and long-range dependencies. CNNs leverage translational invariance through convolutional filters, while Transformers effectively handle complex patterns in images using self-attention mechanisms. These models have thus become mainstream approaches in computer vision tasks. Alongside these established models, Kolmogorov-Arnold Networks (KAN)[21] have emerged as an intriguing alternative due to their theoretical efficiency and compact design. KAN are inspired by the Kolmogorov-Arnold representation theorem[14, 15], which suggests that any continuous multivariate function can be represented by a sum of univariate functions. Based on this principle, KAN utilize learnable activation functions, such as B-spline functions[29, 27], allowing them to adapt flexibly to complex patterns with fewer parameters compared to traditional networks. This characteristic makes KAN theoretically appealing for applications where model interpretability and parameter efficiency are prioritized, thus attracting significant research interest[32, 8, 2, 1, 3]. This study aims to systematically evaluate the performance of KAN and its variants across several core computer vision tasks to explore their potential and limitations in vision applications, including image classification and semantic segmentation. In our experiments, we assessed the performance of KAN and Convolutional KAN[1] models under varying data scales and noise levels. The results show that although KAN and Convolutional KAN exhibit strong fitting capabilities theoretically, their high sensitivity to noise severely impacts their robustness in vision tasks, leading to poor performance in practical vision applications. To enhance KAN stability in vision tasks, we propose a novel smoothness regularization method designed to mitigate excessive oscillations in model parameters. Specifically, this regularization constrains the rate of change in spline functions, ensuring that the KAN model maintains smooth transitions when learning complex patterns, thus enhancing model stability and preventing overfitting. Additionally, we introduce a new technique called Segment Deactivation, which, during training, probabilistically reduces specific spline segments to linear functions. This approach effectively enhances the robustness of the training process by simplifying the model’s complexity, leading to improved performance of KANs on high-dimensional vision tasks. To summarize, our Contributions Are as Follows: 1. Evaluation of KANs in Vision Tasks. We evaluated the performance of Kolmogorov-Arnold Networks (KAN) and its variants in key computer vision tasks, including image classification and segmentation, and identified significant generalization issues in their application to computer vision tasks. 2. Smoothness Regularization. To address KANs’ sensitivity to noise, we propose a smoothness regularization method that stabilizes the model, enhancing robustness in noisy environments. 3. Segment Deactivation. We introduce Segment Deactivation, a technique that reduces certain spline segments to linear functions, improving KAN stability and adaptability to vision tasks."
https://arxiv.org/html/2411.06725v1,GTA-Net: An IoT-Integrated 3D Human Pose Estimation System for Real-Time Adolescent Sports Posture Correction,"With the advancement of artificial intelligence, 3D human pose estimation-based systems for sports training and posture correction have gained significant attention in adolescent sports. However, existing methods face challenges in handling complex movements, providing real-time feedback, and accommodating diverse postures, particularly with occlusions, rapid movements, and the resource constraints of Internet of Things (IoT) devices, making it difficult to balance accuracy and real-time performance. To address these issues, we propose GTA-Net, an intelligent system for posture correction and real-time feedback in adolescent sports, integrated within an IoT-enabled environment. This model enhances pose estimation in dynamic scenes by incorporating Graph Convolutional Networks (GCN), Temporal Convolutional Networks (TCN), and Hierarchical Attention mechanisms, achieving real-time correction through IoT devices. Experimental results show GTA-Net’s superior performance on Human3.6M, HumanEva-I, and MPI-INF-3DHP datasets, with Mean Per Joint Position Error (MPJPE) values of 32.2mm, 15.0mm, and 48.0mm, respectively, significantly outperforming existing methods. The model also demonstrates strong robustness in complex scenarios, maintaining high accuracy even with occlusions and rapid movements. This system enhances real-time posture correction and offers broad applications in intelligent sports and health management.","In modern society, with the spread of health awareness and the promotion of educational policies, the proportion of adolescents participating in sports activities has significantly increased. Whether in school physical education classes or extracurricular sports clubs and training programs, young people are engaging in various sports to varying degrees Nekoui et al. (2020). These activities not only contribute to the physical development and healthy growth of adolescents but also help cultivate their teamwork and competitive spirit. However, as sports activities become more widespread, another issue is gradually coming to the forefront—many adolescents, due to a lack of professional guidance, are prone to developing incorrect postures during exercise Mei (2023); Ran et al. (2024). While these incorrect postures may not cause obvious harm in the short term, over time, they can accumulate and lead to a series of sports injuries, such as muscle strains, arthritis, and scoliosis. These injuries not only affect the athletic performance of young people but could also pose serious threats to their physical development and long-term health Badiola-Bengoa and Mendez-Zorrilla (2021); Kulkarni and Shenoy (2021). Adolescents are in a crucial stage of physical development, and correct posture during exercise is vital to their growth Afsar et al. (2023). Unlike adults, adolescents’ bones and muscles have not yet fully matured, and if incorrect postures are developed during this period, they may adversely affect the normal development of their skeletal structure and muscle groups, potentially leading to lifelong health problems. Therefore, it is particularly important to correct adolescents’ postures in a timely manner and help them maintain the correct posture during exercise. However, traditional posture correction methods often rely on the experience of coaches or teachers, who judge students’ postures through visual observation or video analysis Kwon et al. (2021). This approach is not only time-consuming and labor-intensive but also susceptible to human error, leading to inconsistent correction results. Especially in large-scale physical education classes or training programs, it is challenging for coaches to monitor and correct each student’s posture in real-time and with accuracy, resulting in many adolescents’ incorrect postures going uncorrected. With the continuous advancement of technology, the application of the Internet of Things (IoT) and Artificial Intelligence (AI) is becoming increasingly widespread in various fields, providing new technological means to address the issue of posture correction in adolescent sports Zhang and Tao (2020). By utilizing advanced sensor technology and 3D human pose estimation algorithms, athletes’ postures can be accurately captured and analyzed. Although these technologies have already been preliminarily applied in the training of some professional athletes, their application in the field of adolescent physical education remains relatively limited Xu et al. (2022a). Traditional correction methods are not only time-consuming and labor-intensive but also struggle to provide real-time feedback, failing to meet the needs of large groups of adolescents. Therefore, intelligent posture correction systems that integrate IoT technology have broad application prospects in adolescent physical education. These systems can use smart devices and sensor networks to monitor adolescents’ postures in real-time and automatically generate correction suggestions, helping them adjust their posture during exercise promptly to avoid potential sports injuries Groos et al. (2021). The introduction of intelligent technology will make physical education more personalized and precise, effectively enhancing the scientific and safety aspects of adolescent sports training. To address the challenges in adolescent posture correction during sports activities, early research predominantly utilized 2D image analysis for posture estimation. For instance, one study proposed a 2D posture estimation approach using deep neural networks (DNNs), which predicted posture by analyzing human key points frame by frame Dang et al. (2019). This method processed images through multiple neural network layers to detect and locate body joints, performing well in static images. However, when dealing with dynamic movements, it struggled to track posture changes effectively due to a lack of temporal information, resulting in lower accuracy in complex scenarios, especially during fast, intricate motions. To enhance posture estimation, other research employed convolutional neural networks (CNNs) to extract multi-scale features for key point detection Cao et al. (2023). While CNNs improved feature extraction and achieved high accuracy, their reliance on 2D information limited the ability to process 3D posture changes, making it challenging to handle continuous movements and occlusions. In the realm of 3D posture estimation, researchers proposed a model based on fully connected networks (FCNs) that directly predicted 3D poses from 2D key points Ma et al. (2021). Despite its computational efficiency, the method’s reliance on high-quality input data reduced accuracy and stability when noise, occlusions, or changes in viewing angles were present, limiting its use in complex environments. Another study explored 3D posture estimation using a single-view convolutional neural network Sun et al. (2020a). This approach simplified operations and reduced the need for multi-camera setups, showing good performance in simple movements. However, its accuracy was significantly constrained in complex scenarios, particularly with large rotations or occlusions, and the single viewpoint limited the ability to fully capture 3D motion complexity. These studies highlight the fundamental limitations of previous approaches, such as the lack of temporal information, reliance on 2D data, and the inability to handle complex motion. Specifically, 2D posture estimation methods fall short in capturing dynamic changes and complex postures, while simple 3D estimation models are highly dependent on the quality of input data, making them susceptible to noise and occlusion. Additionally, in large-scale adolescent physical education settings, these methods struggle to provide efficient, low-latency real-time feedback, limiting their practical application. Therefore, there is an urgent need for a solution that combines 3D pose estimation, real-time feedback, and IoT integration to address these shortcomings and meet the demands of adolescent sports posture correction. To address these challenges, we introduce GTA-Net, an innovative system designed for 3D human pose estimation, which not only accurately captures and analyzes adolescents’ sports postures but also integrates Internet of Things (IoT) technology to enable real-time data transmission and feedback. Supported by IoT devices, our system can continuously monitor and analyze adolescents’ postures in real-time without interfering with their normal physical activities. The data processing framework enabled by IoT technology is depicted in Figure 1, providing a typical example of how sensors, edge devices, and cloud computing work together for efficient data aggregation and pose estimation. It is important to note that this figure serves as a general reference for IoT-based data processing and not the overall architecture of our proposed GTA-Net system. Through the integration of advanced spatio-temporal modeling techniques such as Joint-GCN and Bone-GCN, and the incorporation of Attention-Augmented Temporal Convolutional Networks (TCN), the system ensures high-precision posture estimation across various sports scenarios while significantly improving data processing efficiency and feedback speed. These innovations allow our system to offer more robust posture analysis, addressing issues such as occlusions and complex movements that traditional methods struggle to handle. This system not only offers scientific support for posture correction among adolescents but also lays a solid foundation for preventing potential sports injuries. By introducing a novel architecture that combines 3D pose estimation with IoT for real-time monitoring and correction, this research provides key technological advancements that support the future development of intelligent sports systems and opens new pathways for scientifically managing adolescent sports activities. Figure 1: The IoT-based framework for 3D pose estimation and real-time feedback. The system collects posture data using motion capture sensors, video sensors, and depth cameras. This data is processed by a central unit via IoT technology, enabling real-time 3D pose estimation and feedback for posture correction. Our contributions are primarily reflected in the following three aspects: • We proposed an intelligent posture correction system based on 3D human pose estimation, enabling high-precision monitoring and correction of adolescents’ sports postures. This overcomes the limitations of traditional 2D methods, which struggle to accurately capture dynamic and complex movements, ensuring more reliable and precise posture correction. • The system integrates IoT technology, achieving real-time data transmission and feedback, which significantly improves the system’s responsiveness and user experience. Unlike previous methods that often suffer from latency, this system ensures timely feedback, which is crucial for immediate posture correction in sports training. • We validated the effectiveness and practicality of the system through experiments, providing a scientific solution for posture correction and injury prevention in adolescent sports. This contrasts with prior approaches that often lacked real-world validation, ensuring that our solution is both applicable and effective in practical sports training scenarios. This research not only provides robust technical support for the scientific management of adolescent sports activities but also lays an important foundation for the future development of the intelligent sports field."
https://arxiv.org/html/2411.06719v1,Shallow Signed Distance Functions for Kinematic Collision Bodies,"We present learning-based implicit shape representations designed for real-time avatar collision queries arising in the simulation of clothing. Signed distance functions (SDFs) have been used for such queries for many years due to their computational efficiency. Recently deep neural networks have been used for implicit shape representations (DeepSDFs) due to their ability to represent multiple shapes with modest memory requirements compared to traditional representations over dense grids. However, the computational expense of DeepSDFs prevents their use in real-time clothing simulation applications. We design a learning-based representation of SDFs for human avatars whoes bodies change shape kinematically due to joint-based skinning. Rather than using a single DeepSDF for the entire avatar, we use a collection of extremely computationally efficient (shallow) neural networks that represent localized deformations arising from changes in body shape induced by the variation of a single joint. This requires a stitching process to combine each shallow SDF in the collection together into one SDF representing the signed closest distance to the boundary of the entire body. To achieve this we augment each shallow SDF with an additional output that resolves whether or not the individual shallow SDF value is referring to a closest point on the boundary of the body, or to a point on the interior of the body (but on the boundary of the individual shallow SDF). Our model is extremely fast and accurate and we demonstrate its applicability with real-time simulation of garments driven by animated characters.","Simulation of deformable objects is ubiquitous in modern computer graphics applications. Whether it is the intricate stretching and folding of cloth, or the squash, stretch and contraction of soft tissues in virtual characters, elastic deformation is essential for creating satisfactory visual realism in modern visual effects and video games. Simulation of this type is complex and computationally expensive in general. The most challenging aspects are generally self-collision detection/resolution (Bridson et al., 2002; Baraff and Witkin, 1998; Wang, 2014; Wu et al., 2020) and the rapid solution of large systems of nonlinear equations (Tang et al., 2016; Wang et al., 2018; Wang and Yang, 2016). However, another important aspect is detection and resolution of collision constraints with kinematic geometric objects in the scene. These kinematic objects are not influenced by the deformable objects in the scene (e.g. due to large mass ratios), however their motion often determines the deformation of the elastic objects of interest. Clothing draping and interacting with a kinematic body shape is perhaps the most important example of this, and it is the focus of our approach. In most simulation techniques, elastic object collision with kinematic collision bodies is imposed as a constraint on the governing equations. These constraints are detected and enforced using a variety of geometric descriptions of the collision body. We focus on the use of machine learning techniques for accelerating this process. Although many recent methods have investigated the use of these techniques in clothing simulation, most replace the simulation process altogether with a neural network. While fast, learning techniques are still limited in accuracy compared to simulation, particularly in the case of free flowing cloth with significant inertia. Our aim is to avoid these limitations by replacing just the kinematic body collision portion of a typical cloth simulation pipeline with machine learning enhancement. Our proposed approach is unique in this way, however we briefly discuss a number or recent techniques that utilize machine learning in relevant ways. Romero et al. (2023) use a neural network to learn a displacement mapping for resolving elastic object collisions against kinematic rigid bodies with reduced models for deformable objects. Tan et al. (2022) note that purely-learning based cloth techniques suffer greatly from self and kinematic body collision artifacts. Santesteban et al. (2021) also address this problem by adding a repulsion term into their loss function so that trained cloth will be less likely to penetrate the body. Betiche et al. (2020; 2021) also add body-collision based loss terms into training to discourage cloth/body penetration. Gundogdu et al. (2019) do this as well. Signed distance functions (SDFs) generally have constant query time (e.g. when pre-computed and stored over dense grids) and are very useful when rapidly detecting and resolving collisions between a kinematic animated body and simulated clothing (Osher and Fedkiw, 2003). However, SDF calculation (e.g. over dense grid nodes) is expensive and is therefore usually done as an offline/pre-computation. Furthermore, while pre-computed SDFs over regular grids are effective, there are some notable drawbacks. First, SDFs are usually pre-computed at frame-rate time intervals since sub-frame time steps would require excessive computation of the SDFs (even as a pre-computation). Temporal interpolation of SDFs can be used for sub-frame time queries (Selle et al., 2008). Dynamic time step sizes (e.g. resulting from CFL conditions with explicit or semi-implicit time stepping) cannot be known a priori and require this sub-frame interpolation of precomputed SDFs (or excessive non-pre-computation of SDFs on the fly). Also, the storage cost of SDFs at each frame in an animation for each character in the scene quickly becomes excessive. Lastly, the motion of the character must be known completely before the simulation is carried out if pre-computation is to be used. While this is a reasonable assumption for some applications (e.g. offline visual effects), it is not possible in real-time simulation environments where the user is actively redefining the character motion on the fly. Recently, a variety of neural network models for SDFs have been proposed. (Ortiz et al., 2022; Koptev et al., 2023) use neural networks to approximate signed distance functions for scene reconstruction for robotics in real time.(Genova et al., 2019) learn a general shape template from data. (Sitzmann et al., 2020) use meta-learning to perform the same task as (Park et al., 2019), representing multiple 3D shapes. (Liu et al., 2023) use unsigned distance function for shape construction for volume rendering. Ma et al. (2021) use neural networks to learn SDF representations of point clouds. Chabra et al. (2020) utilize local shape patches to increase the variety of shapes representable with neural SDFs. The Deep SDF approach of Park et al. (2019) is particularly powerful. In this case, a network is trained to represent a discrete collection of shapes by training over their individual SDFs (sampled over regular grids). Each shape is encoded with a representative shape vector in the process. These functions can represent a wide range of shapes and utilize dramatically less memory than a collection of SDFs defined over regular grids. In the context of representing the body shape of a kinematic animated character, DeepSDFs could be used to model the SDF of the skin surface rigged with joint-based skinning (e.g. linear blend skinning) over a discrete collection of joint states. However, real-time simulation in this context requires collision queries against the shape of the kinematic avatar skin at continuous samples of the joint state since animation states cannot be discretely sample a priori. To enable application of learning-based SDF techniques in real-time clothing simulation, we design a neural network SDF that depends continuously on both the collision query point and the kinematic joint-state vector. Rather than using a single DeepSDF defined over the entire body, we use a collection of extremely shallow and computationally efficient networks that represent the skin surface very accurately near individual joints. This joint-local approach efficiently focuses network degrees of freedom where they are needed and allows for additive scaling complexity of training data burden (in the number of joints). That is, each joint network can be trained separately without the need to couple the effect of distant joints. However, by decoupling into joint-centric shallow SDFs we lose some information about the signed distance to the surface of the complete skinned character since each joint SDF refers to only a portion of the character. This means that the joint-centric SDF zero-isocontours may coincide with the true boundary or may coincide with an internal boundary specific to the joint. We correct for this by training our networks to know whether or not the signed-distance value is associated with a true boundary or an internal boundary. This knowledge allows us to blend the join-centric SDFs into an efficient and accurate SDF for the skin of the character. Linear blend skinning (LBS) (Magnenat-Thalmann et al., 1989) is an effective and widely-used means for defining the skin surface of animated characters from a kinematic joint state. However, the LBS surface is not guaranteed to be self-intersection free which complicates the definition of a SDF representation (e.g. near joints with large ranges of motion like the elbow and knee). We compensate for this by training on surfaces that have had LBS self-collisions resolved in a simulation post-process. We demonstrate the accuracy and efficiency of our approach with real-time simulation of clothing colliding against representative animated skin surfaces of human avatars. In summary, our contributions can be listed as: • Learning-based SDFs that vary continuously with the kinematic joint state of animated characters. • Shallow joint-centric neural networks trained to represent local skin deformation. • A boolean variable returned by each joint-centric shallow SDF that indicates whether a query point is associated with a fictitious joint-internal surface or the global skin boundary. • A blending mechanism for computing the SDF to the union of the regions defined by each joints shallow SDF. • Resolution of self-collision artifacts in the SDF of LBS surfaces."
https://arxiv.org/html/2411.06703v1,United Domain Cognition Network for Salient Object Detection in Optical Remote Sensing Images,"Recently, deep learning-based salient object detection (SOD) in optical remote sensing images (ORSIs) have achieved significant breakthroughs. We observe that existing ORSIs-SOD methods consistently center around optimizing pixel features in the spatial domain, progressively distinguishing between backgrounds and objects. However, pixel information represents local attributes, which are often correlated with their surrounding context. Even with strategies expanding the local region, spatial features remain biased towards local characteristics, lacking the ability of global perception. To address this problem, we introduce the Fourier transform that generate global frequency features and achieve an image-size receptive field. To be specific, we propose a novel United Domain Cognition Network (UDCNet) to jointly explore the global-local information in the frequency and spatial domains. Technically, we first design a frequency-spatial domain transformer block that mutually amalgamates the complementary local spatial and global frequency features to strength the capability of initial input features. Furthermore, a dense semantic excavation module is constructed to capture higher-level semantic for guiding the positioning of remote sensing objects. Finally, we devise a dual-branch joint optimization decoder that applies the saliency and edge branches to generate high-quality representations for predicting salient objects. Experimental results demonstrate the superiority of the proposed UDCNet method over 24 state-of-the-art models, through extensive quantitative and qualitative comparisons in three widely-used ORSIs-SOD datasets. The source code is available at: https://github.com/CSYSI/UDCNet.","I INTRODUCTION Salient object detection (SOD) mimics the human visual perception mechanism by aiming to identify the most attractive objects or regions within a visual scene from the input data. SOD employed as a pre-processing stage, it has found extensive application in various computer vision tasks, including image segmentation [1], change detection [2], visual tracking [3], and among others. At the beginning, SOD tasks are focused on natural scene images (NSIs). Over an extended period, some NSIs-SOD methods [4, 5, 6, 7, 8, 9, 10] have achieved tremendous performance. Recently, as remote sensing imaging devices matured, a significant amount of optical remote sensing images (ORSIs) is collected. To better analyze ORSIs, researchers begin to apply SOD tasks to detect remote sensing. ORSIs are more challenging compared to NSIs because they are acquired from satellite or aerial sensors. The objects in ORSIs, as well as their types, scales, illuminations, imaging orientations, and backgrounds, are complex and variable. Early ORSIs-SOD methods [11, 12, 13] usually focused on hand-crafted features or low-level priors (e.g., vision and knowledge-oriented saliency [11], color prior [12]) to detect salient objects from ORSIs. However, the non learnability of traditional ORSIs-SOD methods [11, 12, 13] results in unsatisfactory results. Subsequently, with the development of neural network and the open source of some large-scale datasets (i.e., ORSSD [14], EORSSD [15], and ORSI-4199 [16]), numerous ORSIs-SOD methods based on convolutional neural network (CNN) begin to emerge. In particular, Li et al. [14] first introduced an end-to-end deep network in ORSIs. Later, Zhang and Cong el al. [15] proposed an end-to-end dense attention fluid network to achieve SOD. Similarly, EMFINet [17] and MFENet [18] enhanced the discrimination of initial features by conducting multi-scale feature enhancement. In addition, ERPNet [19] and SEINet [20] utilized edge cues to improve the performance of saliency maps. Furthermore, other CNN-based methods [21, 22, 23, 24] have reached respectable performance by increasing the difference between objects and backgrounds using different strategies (e.g., semantic guidance [25], distilling knowledge [26]) in the spatial domain. While these ORSIs-SOD methods [14, 15, 16, 17, 27, 24] have been successful, we have identified a limitation in their focus primarily on local spatial features within the spatial domain. Spatial features, being local in nature and pixel-based, restrict the pixel’s relation solely to itself and its neighboring pixels, resulting in a limited receptive field. Even when certain strategies (e.g., atrous convolution [28]) are adopted to increase the receptive field to aggregate features over a larger region, spatial features are still fundamentally based on local pixel information. This is not conducive to the accurate inference and detection of remote sensing objects (As shown in MCCNet [27] and MJRBM [16] in Fig. 1). Meanwhile, some ORSIs-SOD methods attempt to explore global relationships through utilizing self-attention or different pooling operations within local spatial features, such as HFANet [24], GeleNet [29], GLGCNet[30], and UG2L [31]. Specifically, Wang et al. [24] designed a hybrid encoder by combining CNN and Transformer structures to obtain global and local contexts for reducing the disturbance of complex backgrounds. Liu et al. [31] developed a global context block to capture diverse context while modeling long-distance relations for inferring salient objects from ORSIs. Although self-attention mechanisms and various pooling operations allow the model to learn certain global relationships between pixels, local correlations often remain the primary focus when dealing with spatial features. This is largely due to the fact that, in most natural and remote sensing images, the correlations between neighboring pixels are typically more prominent. As depicted in HFANet [24] and UG2L [31] from Fig. 1, their performance is still constrained by spatial features with local attributes, leading to incorrect salient objects. How to resolve the locality of features from the spatial domain is significantly meaningful for accurate ORSIs-SOD tasks. Figure 1: Visual results between the proposed UDCNet model and existing spatial domain-based ORSIs-SOD methods (i.e., MCCNet [27], MJRBM [16], HFANet [24], and UG2L [31]). To break the limitations of local spatial features, we introduce an innovative thinking by leveraging the Fourier transform to project these local spatial features into the frequency domain, thereby acquiring frequency features endowed with a global perspective. Extensive researches [32, 33, 34] have demonstrated the notion that frequency features derived through the Fourier transform with image-level receptive field inherently possess global properties. Following this, both local spatial and global frequency features undergo a complementary optimization process, culminating in a high-quality representation enriched with both global and local semantics for better inferring remote sensing objects (as indicated in “Ours-R” from Fig. 1). Specifically, we propose a novel United Domain Cognition Network (UDCNet) for accurate ORSIs-SOD task. In particularly, we design the frequency-spatial domain transformer (FSDT) block that includes four important components, that is, spatial perception self-attention (SPSA), frequency perception self-attention (FPSA), adaptive fusion strategy (AFS), and cross-domain feed-forward network (CDFFN). Technically, the SPSA and FPSA respectively act on local spatial features and global frequency features, correcting the importance information within features through a self-attention structure. Then, we introduce the AFS to adaptively aggregate local spatial features and global frequency features. Next, these global-local information in the proposed CDFFN interact optimization to improve feature expression ability. In addition, we construct the dense semantic excavation (DSE) module to capture into higher-level semantic information by leveraging well-designed atrous convolutions, and utilized dense connections to enhance the correlation, thereby guiding the localization of salient objects. Finally, we design the dual-branch joint optimization (DJO) decoder, which consists of two branches: the edge branch and the saliency branch. The saliency branch enhances the structural information of salient objects through reverse optimization in both the frequency and spatial domains. The edge branch increases the boundary information of objects by introducing an edge refinement. Extensive experimental results on three widely-used benchmarks (i.e., ORSSD [14], EORSSD [15], and ORSI-4199 [16]) demonstrate that the proposed UDCNet method outperforms 24 state-of-the-art (SOTA) SOD methods. Our main contributions are summarized as follows: • We propose a novel United Domain Cognition Network (UDCNet) that generates high-quality representations by integrating global-local information from spatial and frequency domains for better detecting salient objects. • We develop the frequency-spatial domain transformer (FSDT) block, which effectively optimizes and aggregates spatial and frequency features through the collaborative working of SPSA, FPSA, AFS, and CDFFN. • We construct the dense semantic excavation (DSE) module, which extracts higher-level semantic to guide the positioning of remote sensing objects by adopting well-designed convolutions and dense connections. • We design the dual-branch joint optimization (DJO) decoder, which utilizes saliency and edge branches to obtain object structure and supplement boundary information for predicting accurate saliency maps. The remainder of this paper is organized as follows. Section II provides the related work, while Section III gives the details of our UDCNet. Experimental results are shown in Section IV. Finally, the concluding remarks are depicted in Section V. Figure 2: Overall framework of our UDCNet method. We use the ResNet50 [35] or PVTv2 [36] as the backbone, and design the frequency-spatial domain transformer (FSDT) block that contains a spatial perception self-attention (SPSA), a frequency perception self-attention (FPSA), an adaptive fusion strategy (AFS) and a cross-domain feed-forward network (CDFFN) to simultaneously model global relationships and local details. Furthermore, we propose the dense semantic excavation (DSE) module to perform semantic enhancement. In addition, we design the dual-branch joint optimization (DJO) decoder to integrate multi-level features for predicting high-quality saliency maps."
https://arxiv.org/html/2411.06702v1,Track Any Peppers: Weakly Supervised Sweet Pepper Tracking Using VLMs,"In the Detection and Multi-Object Tracking of Sweet Peppers Challenge, we present Track Any Peppers (TAP) - a weakly supervised ensemble technique for sweet peppers tracking. TAP leverages the zero-shot detection capabilities of vision-language foundation models like Grounding DINO to automatically generate pseudo-labels for sweet peppers in video sequences with minimal human intervention. These pseudo-labels, refined when necessary, are used to train a YOLOv8 segmentation network. To enhance detection accuracy under challenging conditions, we incorporate pre-processing techniques such as relighting adjustments and apply depth-based filtering during post-inference. For object tracking, we integrate the Matching by Segment Anything (MASA) adapter with the BoT-SORT algorithm. Our approach achieves a HOTA score of 80.4%, MOTA of 66.1%, Recall of 74.0%, and Precision of 90.7%, demonstrating effective tracking of sweet peppers without extensive manual effort. This work highlights the potential of foundation models for efficient and accurate object detection and tracking in agricultural settings.","The integration of computer vision technologies into horticulture has become increasingly vital for modern agricultural practices. Accurate detection and tracking of small objects, such as sweet peppers in densely populated fields, is essential for monitoring crop health, identifying diseases [12, 13, 14], assessing harvest readiness, phenotyping [4] and making informed decisions that enhance sustainability and production efficiency [5, 11, 10]. Automating these tasks not only reduces the reliance on manual labor but also improves precision and scalability in crop management. Traditional object tracking algorithms require extensive training on large, annotated datasets [7]. This process involves manually assigning instance tracking IDs across video frames in addition to labeling bounding boxes—a labor-intensive and time-consuming endeavor that can be prohibitively expensive. The need for such detailed annotations makes it impractical to frequently update models to adapt to the dynamic conditions commonly found in agricultural environments. Recent advancements in large foundation models have showcased remarkable zero-shot and generalization capabilities, particularly in vision-language models (VLMs) like Grounding DINO [8] and Segment Anything Model (SAM) [6]. These models can perform object detection without task-specific training data, presenting an opportunity to mitigate the extensive manual effort typically required for dataset annotation. In this work, we propose a novel methodology that leverages the zero-shot detection capabilities of foundation models to generate pseudo-labels for object instances across video sequences. By utilizing off-the-shelf VLMs, we automatically obtain bounding boxes for target objects with minimal human intervention. Human experts are involved only to refine these pseudo-labels when necessary, thereby reducing annotation costs compared to traditional methods. Building upon these pseudo-labels, we train a YOLOv8 segmentation network using a combination of the refined labels and publicly available datasets, as detailed in Section 2. To enhance detection accuracy, especially in challenging conditions like high illumination, we incorporate pre-processing techniques such as relighting adjustments. During post-inference, depth-based filtering is applied to further refine the results. For object tracking, we employ a hybrid approach that integrates the Matching by Segment Anything (MASA) [7] adapter with the BoT-SORT [1] algorithm. Our experimental results demonstrate the effectiveness of the proposed methodology, achieving a HOTA score of 80.4%, MOTA of 66.1%, Recall of 74.0%, and Precision of 90.7%. These metrics indicate that our approach can successfully track sweet peppers without the need for extensive human intervention, highlighting the potential of leveraging foundation models for efficient and accurate object detection and tracking in agricultural settings. Further details of our overall framework and an analysis of key factors are explored in Section 2 and 3."
https://arxiv.org/html/2411.06700v1,"HomoMatcher: Dense Feature Matching Results with
Semi-Dense Efficiency by Homography Estimation","Feature matching between image pairs is a fundamental problem in computer vision that drives many applications, such as SLAM. Recently, semi-dense matching approaches have achieved substantial performance enhancements and established a widely-accepted coarse-to-fine paradigm. However, the majority of existing methods focus on improving coarse feature representation rather than the fine-matching module. Prior fine-matching techniques, which rely on point-to-patch matching probability expectation or direct regression, often lack precision and do not guarantee the continuity of feature points across sequential images. To address this limitation, this paper concentrates on enhancing the fine-matching module in the semi-dense matching framework. We employ a lightweight and efficient homography estimation network to generate the perspective mapping between patches obtained from coarse matching. This patch-to-patch approach achieves the overall alignment of two patches, resulting in a higher sub-pixel accuracy by incorporating additional constraints. By leveraging the homography estimation between patches, we can achieve a dense matching result with low computational cost. Extensive experiments demonstrate that our method achieves higher accuracy compared to previous semi-dense matchers. Meanwhile, our dense matching results exhibit similar end-point-error accuracy compared to previous dense matchers while maintaining semi-dense efficiency.","Feature matching is a fundamental computer vision task that estimates pairs of pixels corresponding to the same 3D point from two images. This task is crucial for many downstream applications, such as Structure from Motion (SfM) (Schonberger and Frahm 2016; He et al. 2024), Simultaneous Localization and Mapping (SLAM) (Mur-Artal, Montiel, and Tardos 2015; Mur-Artal and Tardós 2017), visual localization (Sarlin et al. 2019; Wang et al. 2024a), image stitching (Zaragoza et al. 2013a), etc. Early approaches predominantly relied on feature detectors, which involved identifying salient points in a pair of images, crafting descriptors for these points, and subsequently accomplishing feature matching. The focus during this period was on creating more efficient feature detectors, leading to the development of methods like SIFT (Lowe 2004), ORB (Rublee et al. 2011), and other learning-based techniques (DeTone, Malisiewicz, and Rabinovich 2018). However, the dependence on detectors significantly reduce robustness, resulting in failures in scenarios with textureless regions or large viewpint changes. Figure 1: An visualization of dense matching results from our proposed HomoMatcher and dense matching method RoMa (Edstedt et al. 2024). The HomoMatcher operates within a semi-dense framework, maintaining efficiency and enabling the flexible expansion of dense mappings from semi-dense results. Middle row is RoMa’s results, which show warps with certainty values above a threshold of 0.02. Bottom row presents our results, demonstrating our method capability for dense matching refinement. Recently, LoFTR (Sun et al. 2021) introduces a detector-free method based on a coarse-to-fine paradigm. It leverages the context aggregation and positional encoding capabilities of Transformer (Vaswani 2017) to generate discriminative coarse features, making it more adept at handling textureless scene. Mutual nearest neighbor strategies are employed to obtain coarse matches, which are then used to extract corresponding patch pairs from fine-level feature maps with high-resolution for further refinement. Fine-matching is performed based on the correlation and expectation calculated from the source patch center point and the target patch. ASpanFormer (Chen et al. 2022) processes an uncertainty-driven scheme to adaptively adjust local attention span, improving model performance through stronger feature representation. Nevertheless, the fine-matching still relies on point-to-patch refinement. This method of calculating expectation using point-to-patch matching can be influenced by irrelevant regions, leading to spatial variance that may affect fine-grained accuracy (Wang et al. 2024b). Several methods have also refined fine-level matching. Efficient LoFTR (Wang et al. 2024b) employs a two-stage refinement strategy to reduce the size of the corresponding patches, but it still relies on computing point-to-patch correlation expectations. (Chen et al. 2024) uses both patches for fine-matching, but directly regresses the offset of the source patch center without leveraging the geometric relationships between the two patches. As a result, it still only achieves semi-dense matching with a single point per patch. To address the aforementioned issues and considering the perspective transformation relationship among matched patches (Zaragoza et al. 2013a), we propose a lightweight yet effective homography estimation network to determine the fine-grained mapping between matched patch pairs. Our approach aligns patches by focusing on highly correlated regions, leveraging richer constraints to minimize the influence of irrelevant areas and achieve more accurate results. With the obtained homography estimation, sparse or dense matching between the two patches can be performed freely and rapidly. Prior to this, detector-free methods like LoFTR encountered challenges in maintaining consistency of keypoints throughout sequential image matching in SLAM or SfM applications. Specifically, when an image is matched as a target at one moment and later served as a source, the resulting keypoints could be inconsistent, thereby impacting the Bundle Adjustment (BA) process during SLAM back-end optimization which needs a set of 2D keypoint locations in multi-view images corresponding to the same 3D point (Peng et al. 2022). Our method can obtain match results from any position within the patches, ensuring continuity of keypoints during sequential matching. Compared to dense matching methods (Edstedt et al. 2023, 2024), our model maintains the efficiency of semi-dense approaches. The fine-matching module we proposed can be directly integrated into existing detector-free methods utilizing a coarse-to-fine framework. We conduct comprehensive experiments on the LoFTR and ASpanFormer models, demonstrating that our method significantly enhances model performance, even reaching state-of-the-art levels for semi-dense matching methods. Remarkably, our lightweight version also boosts the original model performances while maintaining faster processing speeds. We also calculated the end-point error, a deterministic metric commonly used in the dense method (Edstedt et al. 2024), to explicitly evaluate the model’s performance in fine-grained matching. The experimental results indicate that our method significantly outperforms other semi-dense approaches and achieves similar results to dense methods. In summary, our main contributions are as follows: • We introduce a novel fine-matching module based on homography estimation, which suppresses spatial variance caused by irrelevant regions during refinement through patch-to-patch global alignment, achieving more accurate sub-pixel level matches. • By leveraging homography estimation between patches, our method provides matching results for any point within the patch, ensuring keypoint repeatability. Additionally, it allows for densification of matches with the efficiency of semi-dense methods. • The proposed method can be directly integrated into existing semi-dense approaches, and experiments demonstrate that replacing their fine-matching modules with our method significantly improves matching accuracy."
https://arxiv.org/html/2411.06665v1,Learning from Different Samples: A Source-free Framework for Semi-supervised Domain Adaptation,"Semi-supervised domain adaptation (SSDA) has been widely studied due to its ability to utilize a few labeled target data to improve the generalization ability of the model. However, existing methods only consider designing certain strategies for target samples to adapt, ignoring the exploration of customized learning for different target samples. When the model encounters complex target distribution, existing methods will perform limited due to the inability to clearly and comprehensively learn the knowledge of multiple types of target samples. To fill this gap, this paper focuses on designing a framework to use different strategies for comprehensively mining different target samples. We propose a novel source-free framework (SOUF) to achieve semi-supervised fine-tuning of the source pre-trained model on the target domain. Different from existing SSDA methods, SOUF decouples SSDA from the perspectives of different target samples, specifically designing robust learning techniques for unlabeled, reliably labeled, and noisy pseudo-labeled target samples. For unlabeled target samples, probability-based weighted contrastive learning (PWC) helps the model learn more discriminative feature representations. To mine the latent knowledge of labeled target samples, reliability-based mixup contrastive learning (RMC) learns complex knowledge from the constructed reliable sample set. Finally, predictive regularization learning (PR) further mitigates the misleading effect of noisy pseudo-labeled samples on the model. Extensive experiments on benchmark datasets demonstrate the superiority of our framework over state-of-the-art methods. Our code will be available after acceptance.","Deep neural networks (DNNs) have achieved great success in many computer vision tasks (Krizhevsky, Sutskever, and Hinton 2012, 2017; Simonyan and Zisserman 2014). However, these networks need to provide rich labels during training (Rawat and Wang 2017; Alzubaidi et al. 2021). Therefore, domain adaptation (DA) is proposed to alleviate this problem and help the model migrate from a source domain with rich labels to a target domain with different distributions of no or few labels (Ganin and Lempitsky 2015; Pan et al. 2010). Depending on the number of target labels, DA can be divided into unsupervised domain adaptation (UDA) and semi-supervised domain adaptation (SSDA). This paper focuses on SSDA, which can utilize a few target labels to expand semantic information and learn target knowledge, thereby achieving domain alignment. Figure 1: Differences between the proposed method and existing methods. Figures (a) and (b) show that our method focuses on fine-tuning in a semi-supervised target domain without the source data. Based on this learning framework, SOUF performs customized learning for different types of target samples. Figure (c) shows that our method achieves SoTA with a large advantage on the DomainNet and Office-Home datasets. Due to its practical advantages, SSDA has attracted more attention and has been widely studied (Saito et al. 2019; Li et al. 2021a, b; Yan et al. 2022; Yu and Lin 2023; Huang, Zhu, and Chen 2023; Li, Li, and Yu 2023, 2024; He, Liu, and Yin 2024). However, existing methods focus on the study of target sample learning strategies and ignore the importance of customized learning for different types of target samples. Most existing methods (Yan et al. 2022; Huang, Zhu, and Chen 2023; Yu and Lin 2023) only design a series of learning strategies for unlabeled samples, but when there is knowledge bias in the distribution of unlabeled target samples (such as noisy pseudo-label samples), the learned knowledge is unreliable. At the same time, existing methods (Yan et al. 2022; Li, Li, and Yu 2023) ignore the potential deep knowledge of labeled target samples, which is difficult to learn with simple supervised learning. Therefore, it is necessary to design a framework to use different strategies for the comprehensive mining of different target samples. To fill this gap, this paper proposes a novel source-free framework called SOUF, which decouples the SSDA from the perspective of learning different samples. Decoupling the target domain according to different samples can help the model better understand the target domain by learning different sub-tasks. Compared with existing SSDA methods, our framework can more fully utilize the powerful representation and transfer capabilities of the model, thereby better adapting to semi-supervised target domains. As shown in Figure 1, unlike the training paradigm of most existing SSDA methods, this paper considers a source-free scenario (Liang, Hu, and Feng 2020), i.e., fine-tuning the source-pretrained model in the target domain. Our SOUF is proposed to decouple the semi-supervised target domain from the perspective of learning from unlabeled, reliable labeled, and noisy pseudo-labeled target samples. Specifically, we propose probability-based weighted contrastive learning (PWC) for unlabeled target samples to help the model learn more discriminative representations. The adaptive weights assign lower weights to low-confidence samples to reduce the impact of erroneous semantic information, which can better help the model learn unlabeled target knowledge (Zhang et al. 2023). In the semi-supervised fine-tuning stage, it is also essential to further learn the latent knowledge of labeled samples. We combine labeled samples with high-confidence unlabeled samples to construct a new set of reliable labeled samples. We propose reliability-based mixup contrastive learning (RMC) to mix transformer patches from the constructed reliable labeled sample set and learn complex target representations, which can capture the commonalities and differences between different samples (Zhang et al. 2017; Chen et al. 2022; Zhu, Bai, and Wang 2023). Finally, even if we learn the feature knowledge of labeled and unlabeled samples through sufficient contrastive learning, the learned knowledge will still be biased when there is much noise in target pseudo-labeled samples (Song et al. 2019; Liu et al. 2020). We leverage predictions of pseudo-labeled samples to constrain the probabilistic output from the predictive regularization learning (PR) perspective to improve the performance of the model when facing complex target data. Our contributions can be summarized as follows: 1) We propose a novel source-free framework for SSDA scenarios to decouple and customize learning of different types of target samples. 2) We design a series of novel learning methods for unlabeled, reliably labeled, and noisy pseudo-labeled target samples. 3) Our framework is one of the first attempts to solve SSDA using a source-free transformer-based framework. 4) Compared with existing SSDA methods, our method achieves state-of-the-art performance and does not need to access any source data during the semi-supervised adaptation stage."
https://arxiv.org/html/2411.06657v1,Renaissance: Investigating the Pretraining of Vision-Language Encoders,"In the past several years there has been an explosion of available models for vision-language tasks. Unfortunately, the literature still leaves open a number of questions related to best practices in designing and training such models. In this paper we seek to answer several questions related to the pretraining of vision-language encoders through meta-analysis. In our first set of experiments, we show that we can save significant compute at no cost to downstream performance, by freezing large parts of vision-language models during pretraining. In our second set of experiments we examine the effect of basing a VL transformer on a vision model versus a text model. Additionally, we introduce a VL modeling platform called Renaissance that we use to conduct all of the experiments. This program offers a great deal of flexibility in creating, training and evaluating transformer encoders for VL modeling. The source code for Renaissance can be found at https://github.com/bsu-slim/renaissance.","In the span of a few years, dozens of vision-language (VL) transformers have appeared in the literature with a bewildering array of architectures and training methods (see Fields and Kennington (2023) for a review). VL tasks, such as NLVR2 Suhr et al. (2018) where the model is tasked with answering questions about images (see Figure 4 for an example) and image captioning require models to somehow represent and fuse both text and image information. Unfortunately, knowledge of best practices for training and implementing these models has lagged far behind the model development process. This stands in contrast to the NLP domain, where studies such as Rogers et al. (2021) and Kaplan et al. (2020) have thoroughly investigated the inner workings and best training practices for NLP transformers. To date, there have been only a handful of studies analyzing VL-transformers, such as Bugliarello et al. (2021), and the collected literature still fails to address some very basic questions concerning VL modeling with transformers. In this paper we begin to address this gap by providing a systematic analysis geared toward shedding light on some basic aspects of training transformers for vision-language modeling. In particular, we focus on the pretraining and fine-tuning of transformer-encoder architectures. Transformer encoders are best suited toward discriminative tasks such as the NLVR2 benchmark that we mentioned in the opening paragraph and we do not address generative tasks like image captioning here. In our first set of experiments (Section 4), we ask whether it is possible to save compute by freezing parts of the model during pretraining and examining the effect on downstream performance. In our second and final set of experiments (Section 5) we compare the performance of a VL transformer based on a pretrained text encoder versus one based on a pretrained vision transformer. Both sets of experiments will help to establish best training practices for those interested in training VL transformers and hopefully also provide theoretical insight. To perform our experiments, we created a novel VL framework that we call Renaissance that streamlines the ability to evaluate different VL model types (e.g., 1-tower and 2-tower) against a suite of benchmarks. The specific contributions of this paper can be summarized as follows: • We introduce a software platform Renaissance that offers a range of options for creating, training and testing vision-language transformer encoder models. • We demonstrate that a great deal of compute can be saved by freezing parts of two-tower encoder models during pretraining. In particular, freezing the visual module can actually lead to small increases in performance. When both modules are frozen there is some loss in downstream performance, though the benefits may outweigh the costs for those with compute-limited training setups. • We show that when training a one-tower encoder model, it is best to initialize the model’s weights randomly than to use pretrained weights from either a text or a vision encoder model."
https://arxiv.org/html/2411.06652v1,LFSamba: Marry SAM with Mamba for Light Field Salient Object Detection,"A light field camera can reconstruct 3D scenes using captured multi-focus images that contain rich spatial geometric information, enhancing applications in stereoscopic photography, virtual reality, and robotic vision. In this work, a state-of-the-art salient object detection model for multi-focus light field images, called LFSamba, is introduced to emphasize four main insights: (a) Efficient feature extraction, where SAM is used to extract modality-aware discriminative features; (b) Inter-slice relation modeling, leveraging Mamba to capture long-range dependencies across multiple focal slices, thus extracting implicit depth cues; (c) Inter-modal relation modeling, utilizing Mamba to integrate all-focus and multi-focus images, enabling mutual enhancement; (d) Weakly supervised learning capability, developing a scribble annotation dataset from an existing pixel-level mask dataset, establishing the first scribble-supervised baseline for light field salient object detection. https://github.com/liuzywen/LFScribble","Light field (LF) cameras [1] play an important role in stereoscopic photography, virtual reality, and robotic vision applications, because they can reconstruct 3D scenes via multi-view and multi-focus images. Multi-view images [2, 3] reflect the panoramic views of objects, effectively addressing the occlusion problem, while multi-focus images [4] perceive the spatial context of objects, benefiting the partition of foreground objects and background. Fig 1 presents examples of multi-focus images, which typically consist of some focal slices focused at different depth levels and an all-focus image synthesized from all focal slices via photo-montage technique. Each focal slice asynchronously focuses on different depth positions and blur the others, while the all-focus image simultaneously depicts the full scene appearance and ignore the depth of field of objects. Salient object detection (SOD) in multi-focus images can extract attractive objects for the observers based on both all focal slices and all-focus images. Compared with the current predominant method which finetunes Segment Anything Model (SAM) [5] using adapters [6] exclusively on all-focus images, our method locates the correct salient objects by perceiving the depth information embedded in multi-focus images, as shown in Fig 1. Three key perspectives which benefit SOD task for multi-focus LF images will be elaborated. Figure 1: Examples of multi-focus light field images. Each example consists of a set of focal slices and an all-focus image. Red boxes indicate the focused region. Compared with “SAM” finetuned exclusively on the all-focus image, “Ours” finetuned on multi-focus images are closer to the pixel-level mask annotation “GT”. “Scribble” refers to the sparse annotation. Feature extraction ability: Detecting salient objects that attract observers requires a highly effective feature extractor. In the past few years, transformer encoders have outperforms convolutional ones [7]. Based on long range dependency of transformer framework and massive amount of training samples, SAM has been widely used to encode features, showing the discriminative advantage. However, in LF SOD task, it is necessary to encode multiple focal slices along with an all-focus image. To both mitigate the computation cost and enhance feature discrimination, a frozen SAM encoder with a group of fine-tuned adapters is used to encode focal slice features and all-focus features. Fusion ability: Fusion in light field includes fusion among different focal slices and fusion between focal slices and all-focus images. The former is the sequence fusion of the images with the same modality, while the later is multi-modal fusion. Because Mamba can model long-range dependencies in long sequence data [8], it is adopted to process the sequence of focal slices, extracting implicit spatial structure information from the scenes. Furthermore, an inter-modal Mamba is designed to achieve multi-modal fusion, highlighting commonalities and suppressing redundancies. Weakly supervised learning ability: Annotation is important for deep learning model to learn latent mapping from input to output. Existing methods trained LF SOD models via dense annotation with high labor costs. To eliminate the labelling overhead, a scribble annotation dataset and a weak supervised learning method is constructed and exploited, respectively. The last column of Fig 1 gives annotation examples which use sparse scribbles to indicate the foreground [9]."
https://arxiv.org/html/2411.06632v1,ACRA Format Instructions for Authors,"The ACRA Proceedings will appear in CD-ROM form only. To ensure that all papers in the Proceedings have a uniform appearance, authors are asked to adhere to the following instructions. In addition, we will accept, and in fact encourage, submissions in the final format. This file includes the style instructions for submissions.","Although ACRA is a CD-ROM only conference, papers should be prepared so that they can be printed out. 1.1 Corresponding Author Details The corresponding author is requested to email the following information along with the paper: 1. title of the paper, 2. name and postal address, email address. 1.2 Word Processing Software As detailed below, ACRA has prepared and made available a set of LaTeX macros and Word templates for use in formatting your paper. If you are using some other word processing software (such as WordPerfect, etc.), please follow the format instructions given below and ensure that your final paper looks as much like this sample as possible."
https://arxiv.org/html/2411.06602v1,Adaptive and Temporally Consistent Gaussian Surfels for Multi-view Dynamic Reconstruction,"3D Gaussian Splatting has recently achieved notable success in novel view synthesis for dynamic scenes and geometry reconstruction in static scenes. Building on these advancements, early methods have been developed for dynamic surface reconstruction by globally optimizing entire sequences. However, reconstructing dynamic scenes with significant topology changes, emerging or disappearing objects, and rapid movements remains a substantial challenge, particularly for long sequences. To address these issues, we propose AT-GS, a novel method for reconstructing high-quality dynamic surfaces from multi-view videos through per-frame incremental optimization. To avoid local minima across frames, we introduce a unified and adaptive gradient-aware densification strategy that integrates the strengths of conventional cloning and splitting techniques. Additionally, we reduce temporal jittering in dynamic surfaces by ensuring consistency in curvature maps across consecutive frames. Our method achieves superior accuracy and temporal coherence in dynamic surface reconstruction, delivering high-fidelity space-time novel view synthesis, even in complex and challenging scenes. Extensive experiments on diverse multi-view video datasets demonstrate the effectiveness of our approach, showing clear advantages over baseline methods. Project page: https://fraunhoferhhi.github.io/AT-GS","Recovering dynamic scenes with high fidelity from multi-view videos presents a significant challenge in computer vision and graphics, with applications spanning virtual reality, cinematic effects, and interactive media. While many existing methods focus on creating visually appealing and immersive representations of dynamic environments, they often fall short when integrated in modern graphics engines, which require precise and temporally stable surface meshes for tasks such as geometry editing, physics-based simulations, animation, and texture mapping. Therefore, our goal is to develop a method that not only delivers photorealistic rendering of dynamic scenes but also ensures the reconstruction of geometrically accurate and temporally consistent surfaces. In recent years, the rise of Neural Radiance Fields (NeRF) has gained considerable attention for their powerful ability to achieve photorealistic free-viewpoint rendering using compact volumetric representation and differentiable alpha composition [47, 48, 4]. Building on this foundation, numerous subsequent works [51, 53, 3, 17, 55, 37, 42] have further explored the synthesis of free-viewpoint videos for dynamic scenes. While NeRF-inspired approaches have driven significant progress, they often struggle with inefficiencies in training time and rendering speed. In contrast, the recent introduction of 3D Gaussian Splatting (3DGS) [31] marks a significant transition towards explicit point-based representations using differentiable rasterization, which offers more efficient training and high-fidelity real-time rendering. Recent advancements [45, 59, 38, 43, 35, 73, 69, 25, 14, 29, 20] have further demonstrated that Gaussian Splatting achieves superior performance in rendering complex, time-varying environments. Existing surface reconstruction techniques, including those based on multi-view stereo [75, 7, 15, 11], neural implicit representations [66, 18, 8, 39], and more recently, 3D Gaussian Splatting [22, 24, 77, 13] have proven effective in static scenes. However, directly adapting them per-frame to time-varying real-world scenes presents challenges, such as significantly prolonged training times and temporal inconsistencies across frames. An alternative approach is to reconstruct the entire dynamic sequence within a single holistic model [68, 46, 44, 6, 12, 2, 28], such as by deforming a canonical space. However, globally representing dynamic scenes with significant topology changes, emerging or disappearing objects, and rapid movements remains a substantial challenge, particularly for long sequences. To address these challenges, we propose Adaptive and Temporally Consistent Gaussian Surfels (AT-GS), a novel method for efficient and temporally consistent dynamic surface reconstruction from multi-view videos. Our approach utilizes a coarse-to-fine incremental optimization process based on a per-frame Gaussian surfels representation. Initially, we train the first frame of the sequence using a standard static multi-view reconstruction technique representing the scene as Gaussian surfels [13]. For each subsequent frame, we learn the SE(3) transformation to coarsely align Gaussian surfels from the previous frame to the current one. We introduce a unified densification strategy combining the strengths of clone and split. Additionally, we design adaptive probability density function (PDF) sampling, guiding the splitting process using the magnitude of view-space positional gradients. Another challenge in dynamic reconstruction is maintaining temporal consistency. In dynamic reconstruction, slight temporal jittering between frames caused by the randomness of optimization can lead to visible artifacts, especially in regions with minimal textures, where different Gaussian configurations may produce visually identical results. To address this, we first predict the optical flow between neighboring frames of the same view and warp the rendered normal map from the previous frame to the current one. We then enforce consistency in the curvature maps derived from the normals of consecutive frames. This method indirectly ensures temporal coherence in the rendered depth maps and Gaussian orientations, resulting in more stable and accurate final surface geometry. In summary, our contributions include: • A method for efficiently reconstructing dynamic surfaces from multi-view videos using Gaussian surfels. • A unified and gradient-aware densification strategy for optimizing dynamic 3D Gaussians with fine details. • A temporal consistency approach that ensures stable and coherent surface reconstructions across frames by enforcing consistency on curvature maps. • Extensive experiments that demonstrate our method’s advantages including fast training, high-fidelity novel view synthesis, and accurate surface geometry. Figure 2: Pipeline of Our Method. Starting with the Gaussian surfels from the previous frame (t-1), we first estimate their coarse translation and rotation to align with the current frame (t). Subsequently, we optimize all Gaussian attributes, incorporating our gradient-guided densification strategy. For each training view, we render opacity, depth, normal, and color maps ( from top to bottom in the dashed box) using differentiable tile-based rasterization. Additionally, we predict optical flow between consecutive frames, which warps the rendered normal map from frame t-1 to frame t. We then ensure temporal consistency of the underlying surface by comparing curvature maps derived from the warped and rendered normal maps. Furthermore, we apply photometric loss, depth-normal consistency loss, and mask loss for supervision. Finally, Poisson reconstruction is employed to generate a mesh from the unprojected depth and normal maps."
https://arxiv.org/html/2411.06596v1,Graph Neural Networks for modelling breast biomechanical compression,"Breast compression simulation is essential for accurate image registration from 3D modalities to X-ray procedures like mammography. It accounts for tissue shape and position changes due to compression, ensuring precise alignment and improved analysis. Although Finite Element Analysis (FEA) is reliable for approximating soft tissue deformation, it struggles with balancing accuracy and computational efficiency. Recent studies have used data-driven models trained on FEA results to speed up tissue deformation predictions. We propose to explore Physics-based Graph Neural Networks (PhysGNN) for breast compression simulation. PhysGNN has been used for data-driven modelling in other domains, and this work presents the first investigation of their potential in predicting breast deformation during mammographic compression. Unlike conventional data-driven models, PhysGNN, which incorporates mesh structural information and enables inductive learning on unstructured grids, is well-suited for capturing complex breast tissue geometries. Trained on deformations from incremental FEA simulations, PhysGNN’s performance is evaluated by comparing predicted nodal displacements with those from finite element (FE) simulations. This deep learning (DL) framework shows promise for accurate, rapid breast deformation approximations, offering enhanced computational efficiency for real-world scenarios.","Breast cancer is one of the most prevalent and, life-threatening cancers affecting women worldwide. The high incidence rate underscores the critical need for early and accurate diagnosis to improve patient outcomes [19, 25]. Common diagnostic modalities include mammography, magnetic resonance imaging (MRI), ultrasound, and dedicated breast computed tomography (BCT), each offering unique benefits in detecting and characterizing breast lesions [2, 20, 23, 24]. Integrating and correlating information from these diverse imaging techniques can significantly enhance diagnostic accuracy. By synthesizing and simulating data from 3D modalities like MRI and BCT into 2D representations as seen in mammography, clinicians can leverage the comprehensive structural details of MRI or BCT with the practical format of mammography. This multi-modal strategy provides a more comprehensive perspective of breast tissue, enhancing lesion detection and assessment, and ultimately improving diagnostic outcomes and patient care. Achieving realistic breast compression simulation is crucial for multi-modal breast imaging correspondence, commonly approached using Finite Element Analysis (FEA) methods. However, the limited computational efficiency of FEA poses a significant challenge in medical image registration, as it requires substantial time and resources to solve biomechanical models accurately. This limitation affects the performance of existing multimodal registration techniques. To address this issue, researchers have proposed data-driven models that train different machine learning algorithms with FEA results to speed up tissue deformation approximations by prediction [9, 10, 12, 13, 14]. However, these methods overlook valuable information contained within the FE mesh structure, such as node connections and distances, and their number of parameters depends on the mesh resolution. To the best of our knowledge, this study is the first to adapt the PhysGNN model proposed by Salehi [15] for simulating breast compression. It provides a comparative analysis of the results using both quantitative and qualitative metrics against existing FEA methods and discusses its benefits and limitations."
https://arxiv.org/html/2411.06558v1,Region-Aware Text-to-Image Generation via Hard Binding and Soft Refinement,"In this paper, we present RAG, a Regional-Aware text-to-image Generation method conditioned on regional descriptions for precise layout composition. Regional prompting, or compositional generation, which enables fine-grained spatial control, has gained increasing attention for its practicality in real-world applications. However, previous methods either introduce additional trainable modules, thus only applicable to specific models, or manipulate on score maps within cross-attention layers using attention masks, resulting in limited control strength when the number of regions increases. To handle these limitations, we decouple the multi-region generation into two sub-tasks, the construction of individual region (Regional Hard Binding) that ensures the regional prompt is properly executed, and the overall detail refinement (Regional Soft Refinement) over regions that dismiss the visual boundaries and enhance adjacent interactions. Furthermore, RAG novelly makes repainting feasible, where users can modify specific unsatisfied regions in the last generation while keeping all other regions unchanged, without relying on additional inpainting models. Our approach is tuning-free and applicable to other frameworks as an enhancement to the prompt following property. Quantitative and qualitative experiments demonstrate that RAG achieves superior performance over attribute binding and object relationship than previous tuning-free methods.","Recent advancements in diffusion models [28, 11, 29, 20, 23, 27, 36] have substantially enhanced the aesthetic appeal and prompt adherence in text-to-image synthesis. The prevailing trend sees the denoising architecture shifting from UNet [26] to the Diffusion Transformer (DiT) [20], which excels in scaling with large datasets. Transformer-based diffusion models like PixArt-\alpha [5], Stable Diffusion 3/3.5 [8], and FLUX.1 [3] have set a new benchmark, surpassing the quality of earlier UNet-based models such as Stable Diffusion 1.5 [25] and SDXL [21]. Furthermore, employing more robust text encoders, including T5-XXL [22], has demonstrated the ability to render visual text and significantly enhance prompt adherence. Some innovative studies even leverage large language models (LLMs) for text representation, with examples like Kolors [30] utilizing GLM [6] and Playground V3 [17] employing Llama3-8B [7]. Despite this significant progress in generating high-quality images from prompts, achieving precise fine-grained spatial control remains elusive. In essence, current generative models still struggle with comprehending the quantity and spatial arrangement of objects. To address these limitations, the concept of regional prompting, also known as regional control, regional grounding, or composition generation, has emerged. Unlike providing a single global description, achieving fine-grained region-controllable generation requires users to supply not only the spatial location (e.g., a segmentation mask or bounding box) but also a corresponding description for each region. Several approaches have been proposed under this setting, broadly falling into two categories: tuning-based and tuning-free. For tuning-based methods, they often necessitate the training of an additional module to handle explicit conditions like bounding boxes. For example, GLIGEN [16] integrates regional inputs into new trainable layers through a gated mechanism, where each grounding token is a combination of the semantics of the grounded entity and its spatial location. Similarly, InstanceDiffusion [31] and MS-Diffusion [32] also incorporate learnable blocks to handle per-instance conditioning. These methods generally deliver strong performance in precise regional control but are limited to specific base models due to the introduction of extra trainable components. On the other hand, tuning-free methods, such as Multidiffusion [1], RPG [34], and Omost [19], operate on the denoised latent space or attention score map with a mask for each region. They frequently employ a split-and-merge strategy but face challenges in maintaining precise control as the number of regions increases. In this paper, we adopt a tuning-free manner and aim to improve its control strength and coherence when dealing with multiple regions. Specifically, we present RAG, a novel Regional-Aware text-to-image Generation method for precise regional control, which is composed of two sub-tasks, Regional Hard Binding and Regional Soft Refinement. First, we implement region-aware hard binding at the beginning of the denoising process to ensure that each regional prompt is executed accurately. This step breaks down the input prompt into several regional prompts, each with its respective spatial position, and then merges the individually denoised regional latents into the original image latent. Second, to dismiss the visual boundaries and enhance interaction between adjacent regions, regional soft refinement is applied within the cross-attention layers at the subsequent steps to obtain a regional latent, where K and V are from regional text tokens while Q is from original image latent, followed by a weighted recombination of base image latent and regional latent. Furthermore, leveraging robust control and fusion capabilities, our framework supports users to refine specific unsatisfactory regions from the last generation while keeping all other regions intact. Both quantitative and qualitative results demonstrate our superior performance over attribute binding and object relationship than previous state-of-the-art tuning-free methods. Our contributions are summarized as follows: • We propose RAG, a tuning-free Regional-Aware text-to-image Generation framework on the top of DiT-based model (FLUX.1-dev), with two novel components, Regional Hard Binding and Regional Soft Refinement, for precise and harmonious regional control. • RAG novelly makes image repainting feasible, allowing users to modify specific unsatisfactory regions in the previous generation while keeping all other regions intact without need for additional inpainting models. • Extensive qualitative and quantitative experiments demonstrate that RAG shows superior performance over attribute binding, object relationship and complex composition on T2I-CompBench benchmark, in comparison with previous state-of-the-arts. Figure 2: The overall framework of RAG, which divides regional-aware generation into two stages: (1) Regional Hard Binding ensures the proper response of regional prompts by processing each region individually with its fundamental description, and bound at the first r steps to ensure accurate attribute representation and entity localization. (2) Regional Soft Refinement improves the harmony of adjacent region via enabling the interaction of regional local conditions with global image latent within the cross-attention layers at the later T-r steps. The lower left corner shows an example of spatial region set for regional hard binding and regional soft refinement."
https://arxiv.org/html/2411.06525v1,I2VControl-Camera: Precise Video Camera Control with Adjustable Motion Strength,"Video generation technologies are developing rapidly and have broad potential applications. Among these technologies, camera control is crucial for generating professional-quality videos that accurately meet user expectations. However, existing camera control methods still suffer from several limitations, including control precision and the neglect of the control for subject motion dynamics. In this work, we propose I2VControl-Camera, a novel camera control method that significantly enhances controllability while providing adjustability over the strength of subject motion. To improve control precision, we employ point trajectory in the camera coordinate system instead of only extrinsic matrix information as our control signal. To accurately control and adjust the strength of subject motion, we explicitly model the higher-order components of the video trajectory expansion, not merely the linear terms, and design an operator that effectively represents the motion strength. We use an adapter architecture that is independent of the base model structure. Experiments on static and dynamic scenes show that our framework outperformances previous methods both quantitatively and qualitatively. ††\dagger Corresponding author","Video generation technologies are explored to synthesize dynamic and coherent visual content, conditioned on various modalities including text (Blattmann et al., 2023c; Wang et al., 2024a; Gupta et al., 2023) and images (Blattmann et al., 2023b; Chen et al., 2024). Video generation has broad application potential across various fields, such as entertainment, social media, and film production. Motion controllability is crucial for ensuring that generated videos accurately meet user expectations, with camera control being one of the most important aspects. Camera control is the process of adjusting the position, angle, and motion of a camera, resulting in changes to the composition, perspective, and dynamic effects of a video. This technique is essential for generating professional-quality videos, as it influences the attention of viewers and enhances the expressiveness of scenes. Although precise camera control is crucial for producing high-quality videos, existing methods still face challenges. The first challenge pertains to the precision and stability of control. The lack of precision would result in an inaccurate reflection of the user control intention, significantly degrading user satisfaction. The second challenge is ensuring the natural dynamics of the subjects themselves, independent of camera movements. Similar to the challenges in multi-view (Mildenhall et al., 2020; Kerbl et al., 2023) and 3D geometric algorithms (Wang et al., 2021), where static scenes are much easier to handle than dynamic ones (Pumarola et al., 2020; Cai et al., 2022), generating plausible dynamics in videos proves to be more complex than managing static elements. Figure 1: We propose I2VControl-Camera, a novel camera control method for image-to-video generation. Our approach supports any camera movement style, offering high control precision and stability, ensuring natural dynamics and adjustable motion strength, which can be seen in Sec. 4. While AnimateDiff (Guo et al., 2024b) utilizes LoRA (Hu et al., 2022) strategy for controlling camera movements, the motion-LoRAs are confined to a limited set of fixed movement modes, lacking flexibility, and it only allows for coarse control, thus failing to provide precise scale adjustments. A direct and intuitive approach allowing for arbitrary camera movements is embedding the camera pose matrix, as in MotionCtrl (Wang et al., 2023). However, this method results in sparse input signals that heavily rely on the training set distribution, which leads to poor generalization capability. Consequently, it may inadequately respond to less common camera parameters within the training dataset, and thus hinders precise control over the motion’s amplitude. Although CameraCtrl (He et al., 2024) attempts to mitigate this sparsity issue by employing Plücker embeddings (Sitzmann et al., 2021), this parameterization lacks information of the input image, and it actually does not offer any additional information compared to the camera matrix used in MotionCtrl. Another natural strategy is novel view synthesis, which uses 3D implicit representations that can be rendered from arbitrary views, such as Cat3D (Gao* et al., 2024). Unfortunately, this strategy cannot support subject motion well, thus undermining the core goal of creating dynamic video content. In this paper, we propose I2VControl-Camera, a camera control method (some examples shown in Fig. 1) to surmount these prevalent issues in image-to-video generation, enhancing the control precision and adding control over the dynamic strength of subject motion in video output. To ensure control precision and stability, we use point trajectories in the camera coordinate system as our control signals, instead of extrinsic matrix. From the point trajectory function, we extract the linear term to serve as a proxy for camera control, ensuring high precision, stability and user friendliness. To control the motion strength, we further represent object motions with higher-order terms in the trajectory function and explicitly model the degree of dynamics. Specifically, we employ the derivative of the high-order terms to compute the motion speed of each point and integrate them in the image domain to obtain the entire motion strength as the control input of the network. This approach allows us to accurately gauge and adjust the amplitude of subject motion dynamics. We construct training data from regular RGB videos registering 3D tracking information and motion mask for them. Our approach features an adapter architecture that remains agnostic to the underlying base model structure. Experimentally, we conduct experiments in both static and dynamic scenes. For static scenes, we can set the motion strength to zero, resulting in significantly higher precision than previous methods. In dynamic scenes, we can configure a higher motion strength, which allows for both high control precision and vivid subject motion. Our approach outperforms previous methods both quantitatively and qualitatively. In summary, our contributions include: • We explicitly model decoupled motion representations: 3D rigid point trajectories and motion strength for camera and subject motion controls. • We propose a data pipeline to construct training control signals from RGB videos. • For both static and dynamic scenes, our method outperformances previous methods both quantitatively and qualitatively."
https://arxiv.org/html/2411.06510v1,Offline Handwritten Signature Verification Using a Stream-Based Approach,"Handwritten Signature Verification (HSV) systems distinguish between genuine and forged signatures. Traditional HSV development involves a static batch configuration, constraining the system’s ability to model signatures to the limited data available. Signatures exhibit high intra-class variability and are sensitive to various factors, including time and external influences, imparting them a dynamic nature. This paper investigates the signature learning process within a data stream context. We propose a novel HSV approach with an adaptive system that receives an infinite sequence of signatures and is updated over time. Experiments were carried out on GPDS Synthetic, CEDAR, and MCYT datasets. Results demonstrate the superior performance of the proposed method compared to standard approaches that use a Support Vector Machine as a classifier. Implementation of the method is available at https://github.com/kdMoura/stream_hsv.","Handwritten Signature Verification (HSV) systems aim to automatically distinguish between genuine signatures, belonging to the claimed individual, and forgeries. In offline HSV, signatures are represented as digital images captured after the writing process is completed, as opposed to online systems that analyze the signing dynamics [7]. Offline HSV systems can be categorized into two approaches: writer-dependent (WD) and writer-independent (WI). In WD systems, a unique classifier is trained for each enrolled user, offering potentially higher accuracy. However, this approach requires individual training data for each new user. Conversely, WI systems utilize a single classifier for all users, hence being more scalable [5]. In this case, the classification is performed on a dissimilarity space where the pattern recognition is reduced to a 2-class problem by using differences between claimed and reference signatures through a Dichotomy Transformation (DT) [5]. In general, HSV development entails two distinct datasets: a development set utilized for training and an exploitation set employed during the testing phase [6]. Each set comprises the signatures of enrolled users. While more samples generally lead to better generalization models, real-world applications often face data availability limitations regarding both user count and sample volume [5]. Therefore, the system’s capability to model signature variations is constrained to the present available data. After the training process, the resulting HSV is expected to achieve generalization to the whole set of existing users and their signatures. Nonetheless, by relying on a training process with a finite dataset, the current literature does not account for the inherent variability and changing behavior of handwriting signatures. Signatures exhibit the highest intra-class variability compared to other biometric traits [5]. Additionally, signature patterns are time-sensitive as they evolve as we age. Besides, diverse factors can impact the signing process, including emotional states, stress levels, fatigue, and influences from substances like alcohol or drugs [1]. Writing results are intrinsically related to cognitive-motor and neuromotor conditions, being affected by any minor impairment [1]. Given these challenges, we pose the following general research question: how can a signature verification system adapt to the inherent variability and evolving nature of handwritten signatures over time, maintaining high verification performance while mitigating the problem of limited data? To answer this question, we propose a framework to handle signature verification in an adaptive manner, where the input data is processed as a stream of offline signatures rather than a batch mode. In the proposed framework, incoming signatures are first tested and then used to improve the system by updating its current state. In this approach, SigNet-S [20], one of the state-of-the-art representation models, is employed to extract features of incoming claimed signatures. These feature vectors are then compared to corresponding reference vectors stored in the database to create dissimilarity samples via a stream dichotomy transformation. Lastly, the adaptive WI-classifier is updated based on the dissimilarity vectors. To the best of our knowledge, no prior work has considered signatures in an open-set, stream-based configuration. The main contributions of this article are as follows: 1. Stream HSV: we propose a novel HSV framework that adapts over time. This framework treats signatures as an infinite data stream, enabling continuous learning and improvement. 2. A stream dichotomy transformation: we introduce a stream dichotomy transformation process to facilitate adaptive learning from the incoming signature stream and address the challenge of imbalanced data ratios commonly encountered in stream scenarios. 3. Signature stream generation method: to facilitate evaluation using standard batch configurations, we introduce a method for generating signature streams based on the existing HSV evaluation protocol."
https://arxiv.org/html/2411.06481v1,KMM: Key Frame Mask Mamba for Extended Motion Generation,"Human motion generation is a cut-edge area of research in generative computer vision, with promising applications in video creation, game development, and robotic manipulation. The recent Mamba architecture shows promising results in efficiently modeling long and complex sequences, yet two significant challenges remain: Firstly, directly applying Mamba to extended motion generation is ineffective, as the limited capacity of the implicit memory leads to memory decay. Secondly, Mamba struggles with multimodal fusion compared to Transformers, and lack alignment with textual queries, often confusing directions (left or right) or omitting parts of longer text queries. To address these challenges, our paper presents three key contributions: Firstly, we introduce KMM, a novel architecture featuring Key frame Masking Modeling, designed to enhance Mamba’s focus on key actions in motion segments. This approach addresses the memory decay problem and represents a pioneering method in customizing strategic frame-level masking in SSMs. Additionally, we designed a contrastive learning paradigm for addressing the multimodal fusion problem in Mamba and improving the motion-text alignment. Finally, we conducted extensive experiments on the go-to dataset, BABEL, achieving state-of-the-art performance with a reduction of more than 57% in FID and 70% parameters compared to previous state-of-the-art methods.","Text-to-motion (T2M) generation (Guo et al. 2022) involves creating realistic 3D human movements from text descriptions, with promising applications in game development, video creation, and digital humans. Previous generation methods that leverage VAE (Guo et al. 2022; Petrovich, Black, and Varol 2022; Zhong et al. 2023; Tevet et al. 2022a), GAN (Lin and Amer 2018; Harvey et al. 2020; Barsoum, Kender, and Liu 2018), autoregressive (Jiang et al. 2024; Pinyoanuntapong et al. 2024b; Guo et al. 2024), and diffusion-based (Zhang et al. 2024a; Shafir et al. 2023; Chen et al. 2023; Zhang et al. 2023a) approaches using motion-text pair data have achieved overall success in downstream tasks (Raab et al. 2023; Zhang et al. 2024d; Xiao et al. 2023). The recent Mamba (Gu and Dao 2023) architecture shows promising potential for long-context modeling and has already made encouraging attempts in human motion grounding (Wang, Kang, and Mu 2024) and generation (Zhang et al. 2024c, b). Although there are many different methods for text-to-motion generation, they all share a common approach: learning a common latent space for both motion and text. However, these methods pose two significant challenges. First, previous works are based on transformers (Athanasiou et al. 2022; Lee et al. 2024) and diffusion (Yang, Su, and Wen 2023; Shafir et al. 2023; Zhang et al. 2023b; Petrovich et al. 2024) techniques, which are not solving the extended motion generation problem from an architecture perspective. In contrast, Mamba (Gu and Dao 2023) is a naturally suitable architecture for long-sequence motion generation due to its recurrent modelling and linear scaling with sequence length (Dao and Gu 2024). Although Mamba supports long-sequence generation, adapting it for extended motion generation remains challenging due to implicit memory decay. This memory decay is the core limitation impacting Mamba’s performance in generating extended motion sequences. Additionally, despite some efforts to adapt Mamba to multimodal tasks (Wang, Kang, and Mu 2024; Zhang et al. 2024c), the results remain unsatisfactory. Mamba still has obvious shortcomings in multimodal fusion compared to attention mechanisms (Dong et al. 2024; Xie et al. 2024), leading to weak text-motion alignment and poor generation results. Specifically, previous works underperform in two key scenarios, including misunderstanding directional instructions and struggling to handle extended text queries. For instance, when testing on queries containing directions such as left and right, models often generate incorrect or opposite directional motions, as illustrated in Figure 1. Additionally, when testing on complex and lengthy text prompts, models often fail to generate sufficient motions corresponding to the instructions or omit latter part of the text. This phenomenon suggests the limitation of the text encoder. More specifically, most methods employ frozen CLIP (Radford et al. 2021) encoders, which fail to adequately tackle the difficulty of aligning with complex text prompts. Consequently, the models struggle to fully comprehend the text descriptions and generate the corresponding motion sequences. To overcome these challenges, our paper presents three key contributions: • Firstly, to tackle the memory decay problem in Mamba, we introduce Key frame Masking Modeling (KMM), a novel approach that selects key frames based on local density and pairwise distance. This method allows the model to focus on learning from masked key frames, which is more effective for the implicit memory architecture of Mamba than random masking. This advancement represents a pioneering method that customizes frame-level masking in the Mamba model within the latent space. • Additionally, to address the issue of poor text-motion alignment in the Mamba architecture caused by ineffective multimodal fusion, we proposed a novel method that leverages contrastive learning. Instead of relying on a fixed CLIP text encoder, our approach dynamically learns text encodings, enabling the generation of more accurate motions by encoding text queries with better alignment. • Lastly, we conducted extensive experiments on the BABEL dataset (Punnakkal et al. 2021), the go-to benchmark for extended motion generation. Our method achieved a more than 57% improvement in FID and reduced the number of parameters by 70% compared to previous state-of-the-art methods."
https://arxiv.org/html/2411.06478v1,"Superpixel Segmentation:
A Long-Lasting Ill-Posed Problem","For many years, image over-segmentation into superpixels has been essential to computer vision pipelines, by creating homogeneous and identifiable regions of similar sizes. Such constrained segmentation problem would require a clear definition and specific evaluation criteria. However, the validation framework for superpixel methods, typically viewed as standard object segmentation, has rarely been thoroughly studied. In this work, we first take a step back to show that superpixel segmentation is fundamentally an ill-posed problem, due to the implicit regularity constraint on the shape and size of superpixels. We also demonstrate through a novel comprehensive study that the literature suffers from only evaluating certain aspects, sometimes incorrectly and with inappropriate metrics. Concurrently, recent deep learning-based superpixel methods mainly focus on the object segmentation task at the expense of regularity. In this ill-posed context, we show that we can achieve competitive results using a recent architecture like the Segment Anything Model (SAM), without dedicated training for the superpixel segmentation task. This leads to rethinking superpixel segmentation and the necessary properties depending on the targeted downstream task.","Introduced by [1] and made popular by the SLIC method [2], superpixel segmentation continues to play a crucial role in the computer vision community, as illustrated by Fig. 1, which details the occurrences of related terms in research papers over the years. The use of superpixel segmentation is prevalent across various applications such as optical flow [3], saliency estimation [4], stereo matching [5], image segmentation [6] or representation learning [7]. Many methods have been proposed for different image types such as natural 2D images [2], videos [8], 3D medical images [9]. Figure 1: Occurrences of superpixel terms in research papers (source: Google Scholar). Unlike traditional segmentation approaches, superpixel methods are initially based on a regularity constraint that enforces regions to have similar sizes, thus following a pseudo-grid structure and resulting in a clearly distinguishable tiling pattern. This additional regularity constraint sets superpixel segmentation apart, providing a unique approach to partitioning images into meaningful regions. Such image under representation may be of high interest to reduce the computational burden of computer vision tasks but also for interactive applications, e.g., [10]. Over the years, numerous methods for superpixel segmentation have been proposed in the literature. Among these, the SLIC (Simple Linear Iterative Clustering) method [2] stands out for its popularity, leveraging a local k-means algorithm on simple color and spatial features. Many extensions of SLIC have been developed, incorporating contour constraints [11, 12], using other feature spaces [13, 14], or exploring non-iterative variants [15]. Other approaches use graph-based algorithms [16, 17, 18], or propose a hierarchical segmentation, e.g., [19]. Recently, deep learning methods utilizing convolutional neural networks have emerged [20, 21, 22, 23, 24, 25], offering the advantage of using higher-level features compared to traditional superpixel methods which rely on pixel- or patch-level features. With such high-level features, these methods can produce much more accurate superpixel segmentations, also with reduced computational time. Several studies and reviews have focused on extensive performance comparisons between these methods [26, 27, 28, 29, 30, 31, 32]. We refer the reader to these works for a detailed review of existing superpixels methods. An ill-posed problem? Image segmentation is already considered an ill-posed problem [33], but this is enforced in the context of over-segmentation into superpixels. Although the regularity constraint in superpixel segmentation is particularly beneficial for ensuring stability, for user interaction [10], or for some downstream applications [34], it can prevent the accurate segmentation of image objects, especially thin structures. Additionally, different methods interpret and respect this regularity constraint to varying degrees, significantly affecting their segmentation performance. Therefore, comparisons between methods can be biased, and most evaluations of this regularity aspect only rely on qualitative visual assessments. As a consequence, recent deep learning-based approaches tend to bypass this constraint, resulting in very irregular regions that are often hard to identify. For these reasons, the superpixel segmentation particularly appears as an ill-posed problem. Beyond the inherent issue induced by the lack of proper definition of the regularity constraint, the validation of superpixel segmentation methods also faces several challenges. Limited comparisons, inadequate or redundant metrics, biased experiments, and suboptimal parameters for compared method all contribute to these challenges. To the best of our knowledge, there is no recent comprehensive study on the validation framework of superpixel methods. Is it just about object segmentation? Most segmentation datasets are usually annotated at a fine level, but the segmentation may remain mainly semantic. Segmentation performance thus mainly depends on the alignment of superpixels boundaries with those of groundtruth objects. With the advent of deep learning-based methods, able to extract object-level features, segmentation performance have significantly improved, but, in turns, methods tend to focus their evaluation on this single aspect. Thus, superpixel segmentation is very close to a standard object segmentation problem, but with an implicit, rarely and poorly evaluated regularity constraint that greatly impacts segmentation accuracy. Low-level feature-based approaches are generally limited but offer better generalizability. In contrast, deep learning-based methods, while capable of extracting high-level features, may tend to be less generalizable. Recently, large architectures like [35] have emerged as interesting solutions because they are highly generalizable without the need for retraining. Such knowledge may be of interest to serve as basis for defining superpixel segmentation. Contributions • We take a step back on superpixel segmentation, showing that it is fundamentally an ill-posed problem (Section II). It is ill-posed because superpixels inherently try to balance capturing colors or textures while maintaining shape regularity, leading to a contradictory objective. Moreover, this constraint often remains implicit and is rarely or poorly evaluated. • We also demonstrate that the superpixel literature suffers from an evaluation problem (Section III). Recent papers may still use inadequate or redundant metrics, compare to state-of-the-art methods with suboptimal settings, or use biased experiments to establish their superiority. To address this, we propose the first comprehensive study of the superpixel segmentation validation framework, analyzing the bias in the use of metrics, parameter settings, and validation experiments. This study particularly highlights the importance of measuring regularity of superpixel segmentations. • Finally, we demonstrate that the problem can now almost be seen as a standard object segmentation problem (Section IV). We propose to evaluate an approach that uses an agnostic segmentation method like SAM (Segment Anything Model [35]) to generate object-level segmentations, and then fills these with simple regular superpixels based on pixel-level features. This method, without being trained to generate superpixels, achieves state-of-the-art results. It also maintains the original regularity constraint of superpixels, thus getting the best of both worlds, contrary to recent deep learning-based methods. Source code of the method will be made available."
https://arxiv.org/html/2411.06463v1,RL-Pruner: Structured Pruning Using Reinforcement Learning for CNN Compression and Acceleration,"Convolutional Neural Networks (CNNs) have demonstrated exceptional performance in recent years. Compressing these models not only reduces storage requirements, making deployment to edge devices feasible, but also accelerates inference, thereby reducing latency and computational costs. Structured pruning, which removes filters at the layer level, directly modifies the model architecture. This approach achieves a more compact architecture while maintaining target accuracy, ensuring that the compressed model retains good compatibility and hardware efficiency. Our method is based on a key observation: filters in different layers of a neural network have varying importance to the model’s performance. When the number of filters to prune is fixed, the optimal pruning distribution across different layers is uneven to minimize performance loss. Layers that are more sensitive to pruning should account for a smaller proportion of the pruning distribution. To leverage this insight, we propose RL-Pruner, which uses reinforcement learning to learn the optimal pruning distribution. RL-Pruner can automatically extract dependencies between filters in the input model and perform pruning, without requiring model-specific pruning implementations. We conducted experiments on models such as GoogleNet, ResNet, and MobileNet, comparing our approach to other structured pruning methods to validate its effectiveness. Our code is available at https://github.com/Beryex/RLPruner-CNN.","Convolutional neural networks (CNNs) have demonstrated outstanding performance across a range of computer vision tasks, including image classification, detection, and segmentation (Krichen, 2023; Bharadiya, 2023; O’Shea and Nash, 2015; Gu et al., 2018; Li et al., 2022). As these architectures become wider and deeper, they gain an enhanced ability to extract complex features from input data. However, this increase in parameters leads to substantial computational costs, making inference both resource-intensive and slow. Moreover, there is a growing need to deploy CNNs on edge devices, which are often limited in computational power and memory (Rashid et al., 2022; Stahl et al., 2021). Consequently, effective methods for compressing CNNs are increasingly in demand, aiming to reduce parameter counts and accelerate inference while maintaining the original model’s performance. Such compression techniques are crucial for creating efficient and deployable CNNs that can meet the challenges of real-world applications. Several techniques have been proposed to compress CNNs, most of which fall into one of four categories: structured and unstructured pruning, quantization, low-rank factorization, and knowledge distillation (Cheng et al., 2020; Deng et al., 2020). Structured pruning (Fang et al., 2023; He and Xiao, 2024; Anwar et al., 2017) removes entire filters from neural networks and directly modifies the model’s architecture, enabling both compression and realistic acceleration on standard hardware. Unstructured pruning (Zhang et al., 2018; Ma et al., 2022), also known as weight pruning, removes specific unimportant elements from the weight matrix, which requires hardware or libraries that support sparse computation to achieve practical speedup. Low-rank factorization (Swaminathan et al., 2020; Idelbayev and Carreira-Perpinan, 2020) approximates the model’s weight matrix by decomposing it into a product of low-rank matrices. Quantization (Wu et al., 2016; Gong et al., 2014; Zhou et al., 2017) reduces the bitwidth of the weight data in a model, achieving significant compression, but also requires hardware support to realize theoretical speedups for low-bit quantization, such as binary or ternary quantization. Knowledge distillation (Gou et al., 2021; Cho and Hariharan, 2019) transfers knowledge from a larger, more advanced teacher model to a smaller student model, which may not generate a new compressed architecture but helps recover the performance loss due to compression. In this paper, we primarily focus on structured pruning, though we hope the insights from our methods will also inform future work in other categories. Figure 1: Increase in test error for a compressed VGG-16 model after pruning 10% of filters from each individual layer. The pruning is conducted based on three different baselines: the dense model, a model with 5% of filters pruned from all layers, and a model with 10% of filters pruned from all layers. The results highlight that different layers have varying sensitivity to pruning, and this sensitivity changes dynamically throughout the pruning process. A negative increase indicates that performance improves. In structured pruning, while it is crucial to identify the least important filters to prune from weight matrices with minimal performance degradation, it is equally important to assess the importance of different layers to the model’s performance and determine which layers should be pruned more or less. This involves learning the sparsity distribution across layers. As shown in Figure 1, the importance of different convolutional and linear layers to the overall model performance varies significantly. Moreover, after a certain extent of pruning, the relative importance among different layers also changes. To leverage this insight, we can assign higher filter sparsity to less important layers and lower sparsity to more important layers, adjusting the sparsity distribution dynamically throughout the pruning process to minimize the performance drop. In this paper, we introduce a novel approach called RL-Pruner. Unlike other structured pruning methods that learn the sparsity distribution among layers during sparse training (Liu et al., 2017a; Ding et al., 2021; Fang et al., 2023), RL-Pruner is a post-training structured pruning method that utilizes reinforcement learning with sampling to learn the optimal sparsity distribution across layers for several pruning steps. At each step, the current model architecture is defined as the state, while the pruning sparsity distribution acts as the policy. RL-Pruner adds Gaussian noise to the policy distribution to generate the real pruning sparsity distribution as an action, producing the corresponding compressed architecture as the next step’s state. Each generation is treated as a sampling process. For each step, RL-Pruner maintains a replay buffer that stores the pruning sparsity distribution actions and their corresponding Q values, computed by the reward function. The reward function can be defined flexibly, such as based on the compressed model’s test error, a combination of test error and parameter reduction ratio, or other criteria depending on the specific use case. After each pruning step, RL-Pruner updates the sparsity distribution policy to learn the optimal sparsity distribution. When computational resources allow, post-training stages are periodically applied after several pruning steps to recover any performance loss caused by pruning. We employ knowledge distillation in these post-training stages, with the original model acting as the teacher and the compressed model as the student. RL-Pruner can automatically extract dependencies between filters in different layers of the input model by tracking tensor computations. Currently, our approach supports several popular CNN architectures, including residual connections, concatenation connections and skip connections. As a result, our method does not require model-specific pruning implementations and can perform pruning autonomously, enhancing the method’s generality. To validate the effectiveness of RL-Pruner, we apply the proposed method to several popular CNNs used for image classification, including VGGNet (Simonyan and Zisserman, 2014), ResNet (He et al., 2016), GoogLeNet (Szegedy et al., 2015), and MobileNet (Howard, 2017), using the CIFAR-10 and CIFAR-100 datasets (Krizhevsky et al., 2009). According to the experimental results, RL-Pruner achieves 60% channel sparsity for VGG-19 on CIFAR-100 and 40% channel sparsity for GoogLeNet and MobileNetV3-Large on CIFAR-100, all with a performance drop of less than 1%."
https://arxiv.org/html/2411.06456v1,"Dropout the High-rate Downsampling: 
A Novel Design Paradigm for UHD Image Restoration","With the popularization of high-end mobile devices, Ultra-high-definition (UHD) images have become ubiquitous in our lives. The restoration of UHD images is a highly challenging problem due to the exaggerated pixel count, which often leads to memory overflow during processing. Existing methods either downsample UHD images at a high rate before processing or split them into multiple patches for separate processing. However, high-rate downsampling leads to significant information loss, while patch-based approaches inevitably introduce boundary artifacts. In this paper, we propose a novel design paradigm to solve the UHD image restoration problem, called D2Net. D2Net enables direct full-resolution inference on UHD images without the need for high-rate downsampling or dividing the images into several patches. Specifically, we ingeniously utilize the characteristics of the frequency domain to establish long-range dependencies of features. Taking into account the richer local patterns in UHD images, we also design a multi-scale convolutional group to capture local features. Additionally, during the decoding stage, we dynamically incorporate features from the encoding stage to reduce the flow of irrelevant information. Extensive experiments on three UHD image restoration tasks, including low-light image enhancement, image dehazing, and image deblurring, show that our model achieves better quantitative and qualitative results than state-of-the-art methods.","With the widespread adoption of high-end mobile devices, such as smartphones, high-resolution images have become almost mainstream, capable of capturing more detailed visual content. However, various factors often lead to poor image quality during the imaging process, such as insufficient lighting conditions and unfavorable weather conditions. Image restoration constitutes a significant and demanding undertaking in the field of computer vision, aiming to recover degraded images into clear, realistic, and clean images. Although a series of promising methods have emerged for the task of image restoration, achieving satisfactory results, especially the learning-based methods [23, 35, 47, 1, 43, 17, 24, 22, 5, 6, 33, 42, 41, 40, 38], these methods face challenges when it comes to transferring them to Ultra-high-definition (UHD) images. The high pixel count of UHD images (i.e., 3840 \times 2160) often leads to memory overflow issues during the processing of these methods. Figure 1: Comparison between previous methods and our proposed method for UHD image restoration. Due to hardware limitations, previous methods had to resort to patch-based or downsample-based approaches to enable consumer-grade GPUs to process UHD images. However, patch-based methods [32] introduce boundary artifacts during subsequent stitching processes, while downsample-based methods [18, 46] result in significant information loss, both of which can affect the quality of image enhancement. In contrast, our D2Net allows for direct full-resolution inference on UHD images. To tackle this issue, there are currently two primary processing methods: patch-based methods [32] and downsample-based methods [46, 18, 37], as illustrated in Fig. 1. Patch-based methods refer to the approach of dividing UHD images into non-overlapping patches during processing. These patches are individually processed and then stitched to form the final restored image. However, this method inevitably introduces boundary artifacts during the stitching process. Additionally, the process of dividing and stitching patches incurs additional time overhead. Downsample-based methods involve high-rate downsampling of UHD images before processing. After completing the image restoration steps, the images are then upsampled to restore their original resolution. Undoubtedly, high-rate downsampling leads to a significant loss of valuable information, which impacts the quality of the final enhanced image. In this paper, we design a novel framework to solve the UHD image restoration problem called D2Net, which allows for direct full-resolution inference of UHD images. This represents a new design paradigm that completely discards the conventional approaches to UHD image restoration (patch-based and downsample-based). Specifically, we delve deeper into the characteristics of the Fourier domain, as previous works primarily focus on the Fourier transform’s capability to some extent in disentangling image degradation and content components. We find that when an image is transformed from the spatial domain to the frequency domain, the features in the frequency domain naturally exhibit interactions. In light of this, we design the Fourier-based Global Feature Extraction (FGFE) Module to capture long-range dependencies of features. In addition, UHD images, due to their high resolution, contain richer and more complex patterns. Therefore, we design the Multi-scale Local Feature Extraction (MLFE) Module to capture the multi-scale local features of the UHD images. It is worth noting that the convolution kernel sizes in the MLFE module are relatively large. This is because, considering the high-resolution characteristics of UHD images, the commonly used 1\times 1 convolution or 3\times 3 convolution are often unable to capture high-quality local features. Moreover, we also design an Adaptive Feature Modulation Module (AFMM) that dynamically adjusts the fusion of encoded and decoded features. Unlike the simple stacking of features used in previous approaches, this method effectively suppresses the flow of irrelevant information, leading to enhanced performance. Extensive experiments across three datasets demonstrate that D2Net surpasses prior arts in terms of restoration quality and generalization ability. In summary, our main contributions are three-fold: • To the best of our knowledge, we propose the first UHD restoration method that enables direct full-resolution inference on consumer-grade GPUs, without the need for high-rate downsampling or dividing the images into several patches. • We design a novel UHD restoration framework named D2Net. D2Net leverages the FGFE module to achieve long-range modeling and utilizes the MLFE module to capture multi-scale local features. In addition, we design the AFMM to dynamically adjust the fusion of encoded and decoded features. • We conduct comprehensive quantitative and qualitative evaluations of our method, which demonstrate that D2Net achieves state-of-the-art results across multiple UHD image restoration tasks."
https://arxiv.org/html/2411.06449v1,"Improved Video VAE 
for Latent Video Diffusion Model","Variational Autoencoder (VAE) aims to compress pixel data into low-dimensional latent space, playing an important role in OpenAI’s Sora and other latent video diffusion generation models. While most of existing video VAEs inflate a pretrained image VAE into the 3D causal structure for temporal-spatial compression, this paper presents two astonishing findings: (1) The initialization from a well-trained image VAE with the same latent dimensions suppresses the improvement of subsequent temporal compression capabilities. (2) The adoption of causal reasoning leads to unequal information interactions and unbalanced performance between frames. To alleviate these problems, we propose a keyframe-based temporal compression (KTC) architecture and a group causal convolution (GCConv) module to further improve video VAE (IV-VAE). Specifically, the KTC architecture divides the latent space into two branches, in which one half completely inherits the compression prior of keyframes from a lower-dimension image VAE while the other half involves temporal compression through 3D group causal convolution, reducing temporal-spatial conflicts and accelerating the convergence speed of video VAE. The GCConv in above 3D half uses standard convolution within each frame group to ensure inter-frame equivalence, and employs causal logical padding between groups to maintain flexibility in processing variable frame video. Extensive experiments on five benchmarks demonstrate the SOTA video reconstruction and generation capabilities of the proposed IV-VAE (wpy1999.github.io/IV-VAE).","1 INTRODUCTION In recent years, video generation has made significant advances in both academia and industry, showcasing cinematic-level visuals and the potential of world simulator (e.g., OpenAI’s Sora (OpenAI, 2024)). In particular, diffusion optimization methods on latent space have garnered widespread attention and dominated the video generation field due to their high efficiency and stability, resulting in a series of outstanding Latent Video Diffusion Models (LVDMs), e.g., Stable Video Diffusion (SVD) (Blattmann et al., 2023), Open-Sora (hpcaitech, 2024), Open-Sora-Plan (Chen et al., 2024a). To achieve a bidirectional mapping between the high-dimensional pixel space and the low-dimensional latent space, Variational Autoencoder (VAE) (Kingma, 2013) are employed to encode the original video continuously into the latent space, along with spatial and even temporal compression. The compression rate and reconstruction quality of VAE directly determines the information effectiveness of the video corpus, which plays an important role in the exploration of LVDMs. As the most commonly used video VAE, SVD-VAE (Blattmann et al., 2023) freezes the weights of the 2D encoder based on the image VAE (Rombach et al., 2022) and retrains a temporal decoder containing 3D convolutional blocks, which enables the feature interaction across different frames and significantly reduces flicker artifacts. Unfortunately, this method struggles to effectively eliminate redundancy in the temporal dimension, limiting the LVDMs to perceive and generate long sequences of video content. To address the aforementioned issues, several works (Yang et al., 2024; hpcaitech, 2024; Chen et al., 2024a; Qin et al., 2024; Zhao et al., 2024) have proposed to construct a 3D VAE with both spatial and temporal compression capabilities, which can be summarized in the following three points. In terms of model structure, the UNet network (Ronneberger et al., 2015) with strong pixel-perception capabilities is still adopted, consistent with the classic image VAE series (i.e., Stable Diffusion). For the operator implementation, most of the methods (Yang et al., 2024; hpcaitech, 2024; Chen et al., 2024a; Qin et al., 2024) adopt 3D causal convolution with only forward sequence correlation, which can encode the image information independently (Yu et al., 2023b) while ensuring the continuity of the long sequence video encoding. During the optimization process, a well-trained 2D image VAE, focusing only on spatial compression, is used as an initialization for 3D video VAEs with equivalent latent channel dimensions (hpcaitech, 2024; Chen et al., 2024a; Qin et al., 2024; Zhao et al., 2024), which can achieve better results compared to training a temporal-spatial compressed video VAE from scratch (Yu et al., 2023a). Although existing methods achieve video reconstruction under 4\times time compression, we find that there remain deficiencies in the above three key processes, limiting the improvement of the video quality and continuity in reconstruction results. \begin{overpic}[width=433.62pt]{Figure/Figure1.png} \end{overpic} Figure 1: (a) Information preservation degree of reconstruction results for different VAEs on video data. (b) Performance of different frames within a frame group on kinetics-600 dataset. See Appendix A.1 and A.2 for the specific calculation process and definitions of the two diagrams. First, initialization from a well-trained image VAE with the same latent dimension cannot support further 4\times time compression, leading to a sharp decline in initial spatial compression performance and slow improvement in subsequent temporal compression. As shown in Fig. 1(a), we calculate the reconstruction similarity of different VAE as the information preservation degree on video data (See Appendix A.1). We note that: (1) As the latent channel number Z increases, the spatial compression capacity gain produced by the image VAE gradually diminishes (blue dashed lines), and even the spatial compression capability provided by an image VAE with dimension Z/2 is sufficient for a temporal-spatial compressed video VAE with dimension Z. (2) The video VAE that inherits image weights of a lower dimension Z/2 and uses the remaining latent channels to store the spatial information of extra frames, i.e., acquiring the initial 2\times time compression capacity (red curves), converges even better and faster than one that inherits weights of the same dimension Z (green curves). This inspires us to start from an image prior with a lower number of latent channels and leave the remaining features for temporal compression, promoting a more balanced temporal-spatial compression initialization. To this end, we propose a dual-branch keyframe-based temporal compression architecture, where a 2D branch inheriting the low-dimensional image VAE prior focuses on keyframe compression, and a 3D branch faciliates temporal compression. Second, the unidirectional information flow of 3D causal convolution prevents the interaction between video frames, exacerbating inter-frame unbalanced performance under temporal compression. For M frames (a frame group) upsampled from the same compressed frame, the front frame cannot interact with other frames in the group due to the forward flow of causal convolution, while the last frame can acquire the information of the whole frame group, resulting in the preference of later frames in the optimization process. As shown in Fig. 1(b), the methods (Yang et al., 2024; hpcaitech, 2024; Chen et al., 2024a) based on causal convolution suffer from significant inter-frame flicker, which is indicated by the fluctuating SSIM values across different frames. This inspires us to propose group causal convolution, which groups the input frames based on the temporal compression rate. Padding operations with causal logic are applied between groups to ensure the continuity in processing variable frame videos, and standard convolution operations are used within each group of M frames to share equivalent interactive information, achieving smoother reconstruction results. In addition, the local receptive field struggles to meet the same temporal compression demand at high resolutions. For the same video, the higher the resolution, the more pixels the same motion spans, making it more challenging for the model to capture motion patterns. To this end, we introduce dilated convolutions and added multiple attention modules to expand the receptive field, thus improving the temporal motion perception capability of the model at high resolutions. Finally, considering the increasing attention to content diversity and resolution in the video generation field, there is a lack of a suitable open source benchmark for comprehensive evaluation of video VAE reconstruction capabilities. Most of the datasets (e.g., Kinetics-600 (Carreira et al., 2018) and UCF-101 (Soomro et al., 2012)) are 720P or lower, and cannot be used for higher resolution (e.g., 1080P) evaluations. Moreover, high-resolution datasets (e.g., Panda-70M (Chen et al., 2024b)) consist mainly of slow-motion and fixed-shot videos, which hardly reflects the overall performance of video VAEs under varying motion speeds. Therefore, we re-collect a subset of 2000 videos with 1080P containing various motion speeds from the OpenVid-0.4M (Nan et al., 2024) dataset, named MotionHD, as a supplement to the overall evaluation. Experimental results show that our method achieves state-of-the-art video reconstruction at various resolutions and motion rates. In summary, the contributions of this paper include: (1) We propose a keyframe-based temporal compression architecture to accelerate convergence speed by providing more balanced and powerful temporal-spatial compression initialization. (2) We introduce group causal convolution as a replacement for causal convolution to achieve better and balanced performance between frames, while maintaining causal logic. (3) Extensive experiments on five benchmark demonstrate the SOTA video reconstruction and generation capabilities of the proposed IV-VAE."
https://arxiv.org/html/2411.06444v1,SamRobNODDI: Q-Space Sampling-Augmented Continuous Representation Learning for Robust and Generalized NODDI,"Neurite Orientation Dispersion and Density Imaging (NODDI) microstructure estimation from diffusion magnetic resonance imaging (dMRI) is of great significance for the discovery and treatment of various neurological diseases. Current deep learning-based methods accelerate the speed of NODDI parameter estimation and improve the accuracy. However, most methods require the number and coordinates of gradient directions during testing and training to remain strictly consistent, significantly limiting the generalization and robustness of these models in NODDI parameter estimation. In this paper, we propose a q-space sampling augmentation-based continuous representation learning framework (SamRobNODDI) to achieve robust and generalized NODDI. Specifically, a continuous representation learning method based on q-space sampling augmentation is introduced to fully explore the information between different gradient directions in q-space. Furthermore, we design a sampling consistency loss to constrain the outputs of different sampling schemes, ensuring that the outputs remain as consistent as possible, thereby further enhancing performance and robustness to varying q-space sampling schemes. SamRobNODDI is also a flexible framework that can be applied to different backbone networks. To validate the effectiveness of the proposed method, we compared it with 7 state-of-the-art methods across 18 different q-space sampling schemes, demonstrating that the proposed SamRobNODDI has better performance, robustness, generalization, and flexibility.","\IEEEPARstart Diffusion MRI (dMRI) is a widely used medical imaging tool and the only non-invasive method for probing tissue microstructure based on the restricted diffusion of water molecules in biological tissues[1]. Various dMRI models have been developed to characterize specific microstructural features, such as diffusion tensor imaging (DTI)[2], diffusion kurtosis imaging (DKI)[3], stretched-exponential model (SEM)[4], intravoxel incoherent motion (IVIM) [5], spherical mean technique (SMT)[6], neurite orientation dispersion and density imaging (NODDI)[7], and soma and neurite density imaging (SANDI)[8]. Among these, NODDI is a popular microstructural model based on physiological components. NODDI-derived brain microstructural parameters can reflect changes associated with various neurological and psychiatric disorders, as well as brain development, maturation, and aging across the lifespan[9, 10]. Microstructural parameter estimation in NODDI is highly nonlinear and require dense q-space sampling to achieve high-quality parameter estimation. However, dense sampling necessitates extensive acquisition time and increases the susceptibility to motion artifacts[11]. Additionally, the original NODDI method requires significant processing time to fit parameters using nonlinear least squares[7], which limits the possibility of real-time processing on scanners[12]. To address this limitation, various methods have been proposed to reduce computational costs [13, 14]. However, these methods remain relatively slow in processing [12], and their estimation performance may deteriorate when fewer directions are utilized [15, 16, 17]. Therefore, further efforts are needed to reduce processing time while maintaining accuracy. Recently, deep learning (DL) methods have been widely applied to MRI data reconstruction and processing. In the field of dMRI, many studies have successfully applied deep learning to microstructural estimation, achieving the goal of reducing processing time while maintaining accuracy[15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]. Compared with traditional methods, deep learning-based methods can speed up estimation and maintain accuracy, but have poor generalization [12]. Existing deep learning-based NODDI parameter estimation methods can be roughly divided into two categories, one that only considers DWI signals [15, 17, 18, 19, 20, 21, 16, 22, 23] and the other that considers both DWI signals and b-vector information [24, 26, 25, 27, 28, 29, 12, 30]. We describe them separately below. 1.1 DL estimation using only DWI signals We first introduce some deep learning methods for NODDI parameter estimation that use only DWI signals [15, 17, 18, 19, 20, 21, 16, 22, 23]. For example, Golkov et al.[15] proposed using a multilayer perceptron (MLP) to estimate tissue microstructure from advanced signal models by directly mapping diffusion signals obtained from a reduced number of diffusion gradients to tissue microstructure measures. As diffusion signals can be interpreted as measurements in the q-space, this mapping was referred to as the q-space deep learning[15]. Ye et al. introduced MEDN[16], an end-to-end deep network that learns NODDI microstructure using a dictionary-based framework. Subsequently, Ye et al. proposed MESC-Net[17] and MESC-SD[18], which use LSTM to estimate NODDI microstructural parameters by processing dMRI signals as 3D patches and flattening them into one-dimensional vectors for training. In a study by Gibbons et al.[21], a convolutional neural network with residual connections was used to simultaneously generate NODDI and GFA parameter maps from undersampled q-space imaging, achieving a 10-fold reduction in scan time compared to traditional methods. Additionally, inspired by the superior feature extraction capabilities of transformers over convolutional structures, Zheng et al.[19] proposed a transformer-based learning framework called Microstructure Estimation Transformer for Sparse Coding (METSC) for microstructure parameter estimation. In summary, these deep learning methods for microstructural parameter estimation have achieved good performance, enabling accelerated parameter estimation while ensuring accuracy. However, these methods only consider DWI signals, requiring the number and coordinates of the directions during testing to be strictly consistent with those during training. This leads to poor generalization and clinical applicability, limiting their use in real clinical settings. 1.2 DL estimation using both DWI signals and b-vectors Additionally, there are methods that take both the DWI signal and b-vector information into account simultaneously [24, 26, 25, 27, 28, 29, 12, 30]. For example, Chen et al.[24] proposed a graph convolutional neural network (GCNN)-based method for estimating NODDI parameter from undersampled dMRI, which fully considers q-space angular neighboring information. Furthermore, Chen et al.[26] introduced a hybrid graph transformer (HGT) method, which explicitly considers q-space geometry using graph neural networks (GNN) and leverages spatial information through a novel residual dense transformer (RDT). To fully utilize both 3D spatial and angular information in dMRI signals, Chen et al.[25] further proposed 3D HGT to enhance NODDI estimation performance. Park et al.[12] proposed a generalized diffusion parameter mapping network (DIFFnet), which projects diffusion data into a fixed-size matrix called ”Qmatrix” to be used as input for training. The Qmatrix incorporates the gradient directions and b-value information for each signal, allowing for variable gradient schemes and b-values during actual testing. However, discretized projection may lead to a loss of precision. Additionally, higher dimensions result in slower processing speeds. Moreover, this method is voxel-wise, operating on a per-voxel basis without considering the redundant information between patches. Vishwesh et al.[29] directly used SH coefficients to estimate microstructural parameters. By converting the DWI signal into an SH representation with a fixed dimensionality, different microstructural parameters can be estimated with variable numbers of diffusion directions. However, this study focuses on using single-shell DWI signals with all directions to estimate different microstructural parameters, without adequately considering multi-shell DWI signals. Additionally, the information between different sampling directions is not fully exploited. Sedlar et al.[28] applied spherical convolutional neural networks (CNNs) to white matter NODDI microstructural imaging in dMRI, proposing a spherical CNN model with full-spectrum convolution and nonlinear layers (Fourier_s2cnn). FourierS2cnn uses dMRI signals as input, followed by a denoising layer, and then converts them into SH coefficients for training. Since the denoising layer directly operates on the dMRI signals, the performance significantly degrades when the sampling directions during testing differ from those used in training, thus failing to handle the generalization of sampling directions. To address these issues, we propose a q-space sampling augmentation-based continuous representation learning framework for robust and generalized NODDI. Our main contributions can be summarized as follows: • A q-space sampling-augmented continuous representation learning framework (SamRobNODDI) is developed to achieve robust and generalized NODDI. • A novel continuous representation learning method with q-space sampling augmentation is proposed to fully explore the information across different diffusion directions, thereby improving robustness and flexibility to varying diffusion directions. • A sampling consistency loss is designed to constrain output results across different sampling schemes, further enhancing the performance and robustness of the proposed method. • Extensive experimental validation is conducted, including experimental comparisons (different sampling schemes in training and testing, different sampling rates, etc.) and ablation studies (different losses, different backbones, etc.). The results demonstrate that our proposed SamRobNODDI exhibits better performance, robustness, and flexibility compared to existing state-of-the-art methods. The rest of the paper is organized as follows. Section 2 describes the proposed framework in details. Section 3 presents experimental setup. Section 4 demonstrates the results and discussion. Finally, Section 5 provides the conclusions of the paper. Figure 1: Overview of SamRobNODDI. It contains both a training stage and a testing stage. During the training stage, the process consists of q-space sampling augmentation, continuous representation learning, and consistency loss. In the testing stage, only the continuous representation learning process is performed."
https://arxiv.org/html/2411.06442v1,Local Implicit Wavelet Transformer for Arbitrary-Scale Super-Resolution,"Implicit neural representations have recently demonstrated promising potential in arbitrary-scale Super-Resolution (SR) of images. Most existing methods predict the pixel in the SR image based on the queried coordinate and ensemble nearby features, overlooking the importance of incorporating high-frequency prior information in images, which results in limited performance in reconstructing high-frequency texture details in images. To address this issue, we propose the Local Implicit Wavelet Transformer (LIWT) to enhance the restoration of high-frequency texture details. Specifically, we decompose the features extracted by an encoder into four sub-bands containing different frequency information using Discrete Wavelet Transform (DWT). We then introduce the Wavelet Enhanced Residual Module (WERM) to transform these four sub-bands into high-frequency priors, followed by utilizing the Wavelet Mutual Projected Fusion (WMPF) and the Wavelet-aware Implicit Attention (WIA) to fully exploit the high-frequency prior information for recovering high-frequency details in images. We conducted extensive experiments on benchmark datasets to validate the effectiveness of LIWT. Both qualitative and quantitative results demonstrate that LIWT achieves promising performance in arbitrary-scale SR tasks, outperforming other state-of-the-art methods. The code is available at https://github.com/dmhdmhdmh/LIWT.","Single Image Super-Resolution (SISR) refers to the process of recovering a high-resolution (HR) image from a single low-resolution (LR) image and has been widely applied across various fields [Shi et al.(2013)Shi, Caballero, Ledig, Zhuang, Bai, Bhatia, de Marvao, Dawes, O’Regan, and Rueckert, Thornton et al.(2006)Thornton, Atkinson, and Holland, Zou and Yuen(2011), Gunturk et al.(2004)Gunturk, Altunbasak, and Mersereau, Duan et al.(2024)Duan, Qu, Yang, Wang, Zhang, and Song]. Most existing SISR models comprise a deep neural network (DNN) with an upsampling module like learnable deconvolutions or pixel shuffling [Dong et al.(2016)Dong, Loy, and Tang, Shi et al.(2016)Shi, Caballero, Huszár, Totz, Aitken, Bishop, Rueckert, and Wang] and can only deal with integer scaling factors, and these models necessitate retraining when encountering new scaling factors. Recent work has achieved arbitrary-scale super-resolution (SR) by replacing the upsampling layer typically used in previous methods with a local implicit image function and has demonstrated exemplary performance [Chen et al.(2021b)Chen, Liu, and Wang, Lee and Jin(2022)]. These methods based on local implicit functions first extract features from LR images through a DNN-based encoder and then employ multi-layer perceptrons (MLPs) to map the 2D query coordinates of the HR image and the aggregated representation of the corresponding local region features (called latent code) to RGB values. There are two limitations to these existing methods. Firstly, the coordinate-based local ensemble technique [Chen et al.(2021b)Chen, Liu, and Wang, Lee and Jin(2022)] used for querying RGB values fails to consider the relevance of features within local regions. Ensemble weights are typically computed based on the rectangular area between the query point and each nearest point (Figure 1(a)). These weights are solely dependent on the positional relationship between the query point and its nearest coordinates of local features and do not account for the features themselves, thus limiting the reconstruction performance of the model. Secondly, only the four nearest latent codes to the query point are used when querying RGB values based on coordinates. We argue that the representational capacity of LR features directly obtained from the encoder is limited, especially in large-scale SR, which may lead to blurry results lacking texture details. Introducing high-frequency prior information of image features into the local implicit functions is therefore necessary. Figure 1: Motivation and effectiveness of our method. (a) LIIF [Chen et al.(2021b)Chen, Liu, and Wang] and LTE [Lee and Jin(2022)] have difficulty reconstructing high-frequency details using the local ensemble technique. (b) LIWT introduces the high-frequency prior via DWT. (c) LIWT can reconstruct high-frequency details using attention weight based on the high-frequency prior. Many existing methods have shown that high-frequency prior information obtained from discrete wavelet transform (DWT) can improve the performance of SR models based on deep learning [Hsu and Jian(2023), Zou et al.(2022)Zou, Chen, Wu, Zhang, Xu, and Shao, Xin et al.(2020)Xin, Li, Jiang, Wang, Huang, and Gao]. However, the scale transformation rate of the DWT when applied to features or images is limited to powers of 2, and most DWT-based methods rely on this property for inverse transformation to achieve upsampling, thereby not achieving arbitrary-scale SR. To better restore high-frequency details while achieving arbitrary resolution upscaling, we propose the Local Implicit Wavelet Transformer (LIWT), which leverages cross-attention to exploit the high-frequency information obtained from DWT fully and accounts for the relevance of the features within a local region. As shown in Figure 1(b), LIWT consists of a Wavelet Mutual Projected Fusion (WMPF), a Wavelet-aware Implicit Attention (WIA), and a decoder. We first extract features from the LR image using an encoder and then decompose the features obtained into low-frequency components LL and high-frequency components LH, HL, and HH using DWT. To enhance LIWT’s capability to capture high-frequency details, we designed the Wavelet Enhancement Residual Module (WERM) to integrate the four components obtained by DWT and output features with high-frequency priors. Subsequently, we fuse the high-frequency priors with the features from the encoder using WMPF to assist in reconstruction. Then, we employed WIA to generate attention maps based on query coordinates and sample nearest-neighbor interpolated latent vectors from both the high-frequency prior features and the original features. By applying these attention maps to these feature embeddings, LIWT focuses more on high-frequency details in the image. Finally, the decoder utilizes attention feature embeddings to generate RGB values. As illustrated in Figure 1(c), employing attention weight based on high-frequency priors enables the reconstruction of high-frequency details. The main contributions of our work are summarized as follows: (1) We introduce the Local Implicit Wavelet Transformer (LIWT), which integrates features obtained from DWT into local implicit image functions using the designed WERM and WIA to enhance performance; (2) We demonstrate that LIWT can be effectively integrated into different encoders to enhance performance, outperforming other arbitrary-scale SR methods; (3) We conduct a comprehensive analysis of LIWT. Extensive experimental results demonstrate that the proposed LIWT can produce superior or comparable results on benchmark datasets."
https://arxiv.org/html/2411.06441v1,Detecting AutoEncoder is Enough to Catch LDM Generated Images,"In recent years, diffusion models have become one of the main methods for generating images. However, detecting images generated by these models remains a challenging task. This paper proposes a novel method for detecting images generated by Latent Diffusion Models (LDM) by identifying artifacts introduced by their autoencoders. By training a detector to distinguish between real images and those reconstructed by the LDM autoencoder, the method enables detection of generated images without directly training on them. The novelty of this research lies in the fact that, unlike similar approaches, this method does not require training on synthesized data, significantly reducing computational costs and enhancing generalization ability. Experimental results show high detection accuracy with minimal false positives, making this approach a promising tool for combating fake images.","In the past few years, there has been significant progress in image generation. New diffusion models such as DDPM [1] and DDIM [2] have outperformed the previous state-of-the-art approach, GANs, in image generation [3] and introduced a new paradigm for generative modeling. Although these diffusion-based models achieve better quality for generated images, they have a significant drawback – they require extensive computational resources for training and generation. Additionally, they cannot be easily scaled to generate high-resolution images because they operate in a space with numerous dimensions – the pixel space. In [4] the authors proposed a new method called LDM (Latent Diffusion Model), which operates in a much smaller latent space instead of the pixel space. By conditioning the generation process on text, it became possible to generate images based on text prompts. LDM trained and generated new samples much faster, and it became a standard for text-to-image generation. Using large datasets of images and text, such as LAION-5B [5], companies began to introduce image generation capabilities into their products. Examples include MidJourney, DALLE 3, and Adobe FireFly, which allow non-technical users to create images using text queries. Some companies provide their models and code for training as open-source, such as Stable Diffusion. This enables enthusiasts to customize these models and disable built-in watermarks. The rise of user-friendly graphical interfaces for image generation, such as ComfyUI[6] and stable-diffusion-webui[7], further lowers the entry barrier for creating images. These tools allow users to generate images autonomously without relying on third parties. In 2022, OpenAI reported that users were creating more than 2 million images daily using DALL-E [8], and in 2023, Adobe reported that Adobe FireFly generated over 1 billion images in 3 months after its launch [9]. Figure 1 shows the trends of search queries related to image generation. Figure 1: Popularity graph of image generation-related search queries on Google To prevent the spread of misinformation through generated images and to properly label them, reliable detectors are needed, with a focus on minimizing false positives. This study presents a simple yet effective approach for detecting images generated by latent diffusion models. We hypothesize that artifacts introduced by the encoder and decoder of the autoencoder, which projects images from the RGB pixel space to a lower-dimensional latent space and back, are sufficient for detecting images generated by latent diffusion models. This also simplifies the detection of manipulations such as inpainting (partial image editing), as they require projecting images into the latent space. Unlike other methods, our solution leverages artifacts introduced by the autoencoder encoder and decoder to effectively detect LDM-generated images without the need for training on the generated images themselves. The proposed method shows high generalization, resilience to distortions such as JPEG compression, and significantly reduces computational costs, making it an efficient tool for detecting images created by various latent diffusion models. This constitutes the scientific novelty and practical significance of the proposed solution. The paper is organized as follows: Section II provides an overview of the current state of research and practical results in detecting diffusion model-generated images. Section III describes the proposed solution, a method for detecting latent diffusion models without training on generated images. Section IV presents experimental results demonstrating the effectiveness of the proposed approach. Section V offers a brief analysis of the strengths and weaknesses of the solution. Finally, the conclusion summarizes the findings and suggests future research directions."
https://arxiv.org/html/2411.06390v2,SplatFormer: Point Transformer for Robust 3D Gaussian Splatting,"3D Gaussian Splatting (3DGS) has recently transformed photorealistic reconstruction, achieving high visual fidelity and real-time performance. However, rendering quality significantly deteriorates when test views deviate from the camera angles used during training, posing a major challenge for applications in immersive free-viewpoint rendering and navigation. In this work, we conduct a comprehensive evaluation of 3DGS and related novel view synthesis methods under out-of-distribution (OOD) test camera scenarios. By creating diverse test cases with synthetic and real-world datasets, we demonstrate that most existing methods, including those incorporating various regularization techniques and data-driven priors, struggle to generalize effectively to OOD views. To address this limitation, we introduce SplatFormer, the first point transformer model specifically designed to operate on Gaussian splats. SplatFormer takes as input an initial 3DGS set optimized under limited training views and refines it in a single forward pass, effectively removing potential artifacts in OOD test views. To our knowledge, this is the first successful application of point transformers directly on 3DGS sets, surpassing the limitations of previous multi-scene training methods, which could handle only a restricted number of input views during inference. Our model significantly improves rendering quality under extreme novel views, achieving state-of-the-art performance in these challenging scenarios and outperforming various 3DGS regularization techniques, multi-scene models tailored for sparse view synthesis, and diffusion-based frameworks. Project page: https://sergeyprokudin.github.io/splatformer/.","Novel view synthesis (NVS) focuses on transforming 2D RGB images into immersive 3D scenes, allowing users to navigate within augmented reality (AR) and virtual reality (VR) environments. Traditionally, this problem has been approached using a standard novel view interpolation protocol, where test views are sampled at fixed intervals along the trajectory of the input views. Several NVS methods have emerged based on this protocol, with 3D Gaussian splatting (3DGS) (Kerbl et al., 2023) recently gaining attention for achieving real-time and high-fidelity results in view interpolation. However, AR and VR applications require not only smooth transitions between input views but also the ability to explore novel regions of interest from viewing angles outside the input distribution. For instance, users may want to observe a scene from high-elevation angles, which are often missing from the captured views. While novel view interpolation has seen significant advancements, this out-of-distribution novel view synthesis (OOD-NVS) task remains underexplored, particularly in terms of evaluation protocols and methodology. A related area of research involves 3D reconstruction from sparse or monocular input views, where methods often need to hallucinate unseen content (Liu et al., 2023a; Chan et al., 2023; Kwak et al., 2024; Liu et al., 2023b). While hallucination can be beneficial for creative applications, it may be undesirable in settings that demand accurate reconstructions, such as 3D visualization of surgical procedures (Hein et al., 2024), and unnecessary in typical daily capture scenarios. Imagine you are capturing a statue in a museum. By varying the camera’s elevation and walking around the object, you might be able to capture most of its features. However, the spatial distribution of camera angles is likely uneven, even heavily skewed, creating certain out-of-distribution views where some parts of the object are only sparsely covered. An example is illustrated in Fig. 1, where input views are captured from a user’s perspective, circling around an object at varying but close elevation angles. The out-of-distribution (OOD) target views observe the object from a top-down perspective, a significant deviation from the input distribution. We define this challenge as out-of-distribution novel view synthesis (OOD-NVS). We argue that this issue is practically relevant for everyday capture scenarios, yet it has been largely overlooked by the research community. To study this problem, we render 3D assets from ShapeNet (Chang et al., 2015), Objaverse 1.0 (Deitke et al., 2023), and Google Scanned Objects (Downs et al., 2022) datasets. As shown in Fig. 1, existing NVS methods perform poorly on the OOD views when restricted to low-elevation inputs, highlighting the need for a novel approach to address this problem. Substantial research efforts have been directed towards robust 3D reconstruction with insufficient input views. First, some 3DGS variants regularize the Gaussian attributes through implicit bias in neural radiance fields (Mihajlovic et al., 2024) or geometry consistency terms (Huang et al., 2024a). Second, a number of methods attempt to exploit priors from external datasets. For example, some supervise the rendered depth maps using stereo estimators (Zhu et al., 2024), though these methods face scale ambiguity issues. Certain methods pretrain feature grids (Chen et al., 2023; Sen et al., 2023) on large datasets, but these priors are often limited to a single object category. Other methods use 2D priors from pretrained diffusion models (Sargent et al., 2024) but struggle with multi-view inconsistencies. Additionally, some feed-forward models predict 3D primitives from a few input views (Chen et al., 2024a; b; Yu et al., 2021), yet they handle no more than four images due to computational constraints, limiting their ability to leverage dense multi-view inputs. Most of these approaches are evaluated only on view interpolation or sparse-view reconstruction, failing to address the artifacts encountered in the OOD-NVS settings. Figure 1: We investigate the out-of-distribution (OOD) novel view synthesis (NVS), where test views significantly differ from input views. This scenario contrasts with prior in-distribution NVS, where test views interpolate between densely captured input views, Sparse NVS with a few large-baseline input views, and Nerfbusters NVS (Warburg* et al., 2023), where test views share similar angles with input views. Existing NVS methods, including MipNeRF360 (Barron et al., 2022), and those designed for sparse inputs like LaRa (Chen et al., 2024a), face challenges in this setting, while our method shows notable improvements. Defining an implicit regularization to improve OOD-NVS poses a significant challenge. We hypothesize that addressing this issue requires careful consideration of three key aspects: 1) leveraging generic priors from large-scale datasets, 2) ensuring 3D consistency in renderings, and 3) fully utilizing the rich geometric information from all input views. To meet these needs, we propose SplatFormer, a novel learning-based feed-forward 3D transformer designed to operate on Gaussian splats. SplatFormer refines an initial 3DGS set—optimized using all input views—into a new, enhanced set that produces multi-view consistent 2D renderings under OOD conditions with fewer artifacts. Our method begins by optimizing 3DGS from the input views. While this initial 3D representation effectively integrates multi-view information from the captured images, we observe that the shapes, appearances, and spatial structure of the Gaussian splats become biased toward the input view distribution. This often results in elongated Gaussian splats that cover only the thin areas projected on the input views, leading to sparse surface coverage. Furthermore, these splats can form unordered geometric structures that appear correct from the input views but exhibit significant artifacts when rendered under OOD views. Unlike previous works that rely on hand-crafted regularization techniques (Xie et al., 2024; Li et al., 2024b), we adapt point transformer (Zhao et al., 2021), an attention-based architecture designed for 3D scene understanding, to process the 3DGS as a point cloud set with Gaussian attributes serving as features. The attention mechanism in the point transformer learns to capture multi-view information embedded in the 3DGS, focusing on the local neighborhood within the spatial structure pre-computed by the initial 3DGS. It outputs residuals that are added to the input Gaussian attributes. The updated 3DGS is then rendered from novel views, and a photometric error between the rendered and ground-truth images is minimized to train the SplatFormer. We curate large-scale training pairs of initial, flawed 3DGS sets, and ground-truth images of in-distribution and OOD views using ShapeNet and Objaverse 1.0, which are made feasible by the fast optimization of 3DGS and the availability of large-scale 3D and multi-view datasets. By training on this dataset, SplatFormer learns generalizable priors for refining 3DGS, effectively removing artifacts in the OOD views while maintaining 3D consistency. We evaluate SplatFormer against baseline models using the proposed OOD-NVS evaluation protocols. Our experiments demonstrate that once trained, SplatFormer significantly reduces artifacts in 3DGS OOD-view renderings, showing substantial improvements in both quantitative and qualitative results for test scenes from ShapeNet and Objaverse. Additionally, we demonstrate that SplatFormer’s artifact removal capabilities generalize to novel object categories in previously unseen datasets, such as Google Scanned Objects (Downs et al., 2022), as well as real-world captures. In summary, we make the following contributions: • We introduce OOD-NVS, a new experimental protocol specifically designed to evaluate the performance of NVS methods when rendering 3D scenes from novel viewing angles that fall outside the distribution of input views. Our results demonstrate that existing methods struggle to generalize under the OOD-NVS protocol; • We propose SplatFormer, a novel learning-based model that refines flawed 3D Gaussian splats to mitigate artifacts in OOD views. SplatFormer is the first approach to apply the point transformer to 3DGS processing, effectively leveraging multi-view information from a dense set of input views and learning a 3D rendering prior to remove artifacts; • We demonstrate that SplatFormer significantly improves the performance of 3DGS-based methods on OOD-NVS tasks, achieving substantial gains in object-centric scenes, while also demonstrating potential for application in unbounded environments."
https://arxiv.org/html/2411.06381v1,SAN: Structure-Aware Network for Complex and Long-tailed Chinese Text Recognition,"In text recognition, complex glyphs and tail classes have always been factors affecting model performance. Specifically for Chinese text recognition, the lack of shape-awareness can lead to confusion among close complex characters. Since such characters are often tail classes that appear less frequently in the training-set, making it harder for the model to capture its shape information. Hence in this work, we propose a structure-aware network utilizing the hierarchical composition information to improve the recognition performance of complex characters. Implementation-wise, we first propose an auxiliary radical branch and integrate it into the base recognition network as a regularization term, which distills hierarchical composition information into the feature extractor. A Tree-Similarity-based weighting mechanism is then proposed to further utilize the depth information in the hierarchical representation. Experiments demonstrate that the proposed approach can significantly improve the performances of complex characters and tail characters, yielding a better overall performance. Code is available at https://github.com/Levi-ZJY/SAN","Chinese text recognition plays an important role in the field of text recognition due to its huge audience and market. Most current text recognition methods, including Chinese text recognition methods, are character based, where characters are the basic elements of the prediction. Specifically, most methods fit into the framework formulated by Beak et al. [1], which includes an optional rectifier (Trans.), a feature extractor (Feat.), a sequence modeler (Seq.), and a character classifier (Pred.). Many typical Chinese text recognition methods [9, 25, 29], also fall into this category, where the feature extractor generally takes the form of a Convolutional Neural Network, and the classifier part is mostly implemented as a linear classifier decoding input features into predicted character probabilities. However, the naive classification strategy has limited performance on Chinese samples, due to the large character set, severely unbalanced character frequency, and the complexity of Chinese glyphs. To address the frequency skew, compositional learning strategies are widely used in low-shot Chinese character recognition tasks [31, 6, 12]. For compositional information exploited, the majority of implementations [31, 20, 2, 21, 24] utilize the radical representation, where the components and the structural information of each character are hierarchically modeled as a tree. Specifically, the basic components serve as leaf nodes and the structural information (the spatial placement of each component) serves as non-leaf nodes. Some methods are also seen to utilize stroke [6] or Wubi [12] representations. Besides the Chinese language, characters in many other languages can be similarly decomposed into basic components [6, 4, 16, 3]. These methods somewhat improve text recognition performance under low-shot scenarios. However, compositional-based methods are rarely seen in regular recognition methods due to their complexity and less satisfactory performance. This limitation is solved by PRAB [7], which proposes to use radical information as a plug-in regularization term. The method decodes the character feature at each timestamp to its corresponding radical sequence and witnesses significant overall performance improvement on several SOTA baselines. However, the method still has two major limitations. First, PRAB [7] only applies to text recognition methods with explicitly aligned character features [18, 13] and does not apply to implicitly aligned CTC-based methods like CRNN [17]. Furthermore, PRAB and most of the aforementioned radical method treats large parts and small parts alike, ignoring the depth information of the hierarchical representation. To alleviate the limitations, we propose a Structural-Aware Network (SAN) which distills hierarchical composition information into the feature extractor with the proposed alignment-agnostic and depth-aware Auxilary Radical Branch (ARB). The ARB serves as a regularization term which directly refines feature maps extracted by the Feat. stage to preserve the hierarchical composition information without explicitly aligning the feature map to each character. The module allows the model to focus more on local features and learn more structured visual features, which significantly improves complex character recognition accuracy. As basic components are shared among head and tail classes alike, it also improves the tail-classes performance by explicitly exploiting their connections with head classes. Furthermore, we proposed a novel Tree Similarity (TreeSim) semimetric serves as a more fine-grained measure of the visual similarity between two characters. The proposed TreeSim semimetric further allows us to exploit the depth information of the hierarchical representation, which is implemented by weighting the tree nodes accordingly in ARB. The suggested method substantially enhances the accuracy of complex character recognition. As fundamental elements are shared between head and tail classes, it also boosts the performance of tail classes by leveraging their relationships with head classes. Experiments demonstrate the effectiveness of ARB in optimizing the recognition performance of complex characters and long-tailed characters, and it also improves the overall recognition accuracy of Chinese text. The contributions of this work are as follows: • We propose a SAN for complex character recognition by utilizing the hierarchical components information of the character. • ARB based on the tree modeling of the label is introduced, which enhances the structure awareness of visual features. ARB shows promising improvement in complex character and long-tailed character recognition and it also improves the overall recognition accuracy of Chinese text. • We propose a novel TreeSim method to measure the similarity of two characters, and propose a TreeSim-based weighting mechanism for ARB to further utilize the depth information in the hierarchical representation."
https://arxiv.org/html/2411.06378v1,PKF: Probabilistic Data Association Kalman Filter for Multi-Object Tracking,"In this paper, we derive a new Kalman filter with probabilistic data association between measurements and states. We formulate a variational inference problem to approximate the posterior density of the state conditioned on the measurement data. We view the unknown data association as a latent variable and apply Expectation Maximization (EM) to obtain a filter with update step in the same form as the Kalman filter but with expanded measurement vector of all potential associations. We show that the association probabilities can be computed as permanents of matrices with measurement likelihood entries. We also propose an ambiguity check that associates only a subset of ambiguous measurements and states probabilistically, thus reducing the association time and preventing low-probability measurements from harming the estimation accuracy. Experiments in simulation show that our filter achieves lower tracking errors than the well-established joint probabilistic data association filter (JPDAF), while running at comparable rate. We also demonstrate the effectiveness of our filter in multi-object tracking (MOT) on multiple real-world datasets, including MOT17, MOT20, and DanceTrack. We achieve better higher order tracking accuracy (HOTA) than previous Kalman-filter methods and remain real-time. Associating only bounding boxes without deep features or velocities, our method ranks top-10 on both MOT17 and MOT20 in terms of HOTA. Given offline detections, our algorithm tracks at 250+ fps on a single laptop CPU. Code is available at https://github.com/hwcao17/pkf.","Figure 1: Example scene with high ambiguity in DanceTrack [40] with green box detections and orange box tracks. Applying our probabilistic data association Kalman filter on this sequence significantly increases the association quality according to IDF1 [38], association accuracy (AssA) [24], association recall (AssRe) [24], and the combined HOTA metric [24]. In tasks where ambiguity exists between measurements and variables of interest, probabilistic data association can prevent catastrophic estimation failures. For example, in multi-object tracking (MOT), there can be a lot of occlusions causing high ambiguity in data association. An illustration of how probabilistic data association improves MOT is shown in Figure 1. Methods utilizing the Kalman filter [6, 47, 13] have achieved outstanding performance in MOT but have not considered the impact of probabilistic data association on the tracking process. We derive a new formulation of the Kalman filter with probabilistic data association. Previous works [3, 12] show that the Kalman filter can be derived using variational inference (VI) by maximizing the evidence lower bound (ELBO) of the input and measurement data likelihood. To deal with ambiguous associations, we approach the VI problem using Expectation-Maximization (EM) with the data association as the latent variable. In the E-step, we show that the association weights can be computed as permanents of matrices with measurement likelihood entries using accelerated permanent algorithms [32, 20]. We also show that the weight computation can be extended to cases with missing detections and clutter. In the M-step, optimizing the EM objective once leads to a Kalman filter with the usual prediction and update steps but with an extended measurement vector and noise covariance matrix capturing all possible data associations. Our formulation is related to the well-known joint probabilistic data association filter (JPDAF) [2]. However, while JPDAF first computes a posterior given each associated measurement, i.e., maximizes the ELBO of each measurement, and then computes the weighted average of the posteriors, our filter directly optimizes the overall ELBO weighted by the data association probabilities. We show that our algorithm achieves lower tracking errors than JPDAF, while running at comparable tracking rates. We also apply our filter to MOT. We observe that, with accurate detection algorithms [36, 35], binary associations are fast and work well when the ambiguity is not severe and incorporating measurements with low association weights can harm the estimation accuracy. Therefore, we employ a hybrid data association procedure. We first do binary data association (e.g., via [23, 21]) and then perform an ambiguity check which outputs a set of ambiguous measurements and tracks. Then, probabilistic data association is only performed on the ambiguous set, thus reducing the computation time and preventing low-probability measurements from harming the accuracy. We test our algorithm on multiple real-world MOT datasets, including MOT17 [27], MOT20 [15], and DanceTrack [40]. Our method outperforms previous Kalman-filter methods [47, 13] while maintaining almost the same inference speed. We name our probabilistic data association Kalman filter the PKF with P emphasizing both the probabilistic nature of the data association and the matrix permanent computation of the association weights. Our contributions are summarized as follows. • We formulate state estimation with ambiguous measurements as a VI problem and use EM to derive the PKF, a new Kalman filter with probabilistic data association. • We show that association probabilities can be computed using matrix permanents and design a hybrid association procedure with an ambiguity check to reduce computation complexity and exclude low-probability associations. • We demonstrate the effectiveness of the PKF in comparison to the JPDAF and on multiple real-world MOT datasets. PKF ranks top-10 on both MOT17 and MOT20 by associating only bounding boxes and runs at 250+ fps on a single laptop CPU given offline detections."
https://arxiv.org/html/2411.06365v1,Through the Curved Cover: Synthesizing Cover Aberrated Scenes with Refractive Field,"Recent extended reality headsets and field robots have adopted covers to protect the front-facing cameras from environmental hazards and falls. The surface irregularities on the cover can lead to optical aberrations like blurring and non-parametric distortions. Novel view synthesis methods like NeRF and 3D Gaussian Splatting are ill-equipped to synthesize from sequences with optical aberrations. To address this challenge, we introduce SynthCover to enable novel view synthesis through protective covers for downstream extended reality applications. SynthCover employs a Refractive Field that estimates the cover’s geometry, enabling precise analytical calculation of refracted rays. Experiments on synthetic and real-world scenes demonstrate our method’s ability to accurately model scenes viewed through protective covers, achieving a significant improvement in rendering quality compared to prior methods. We also show that the model can adjust well to various cover geometries with synthetic sequences captured with covers of different surface curvatures. To motivate further studies on this problem, we provide the benchmarked dataset containing real and synthetic walkable scenes captured with protective cover optical aberrations.","The deployment of extended reality (XR) devices in commercial settings depends on their ability to perceive their surroundings accurately and reliably. Recently, XR headsets have been equipped with translucent covers over their front-facing cameras to enhance durability and safety. However, these protective covers present challenges for novel view synthesis due to surface irregularities that can cause image aberrations, leading to degraded synthesis quality. We propose SynthCover, a novel framework for neural view synthesis through refractive surfaces, which addresses this challenge by providing a geometric estimation of surface irregularities that is jointly optimized with the synthesis process, enabling high-quality rendering even from cover-aberrated captures. Image aberration has been a persistent issue in photography and imaging, particularly for 3D reconstruction. Common optical aberrations, such as blurring and distortions, occur even with monochromatic light sources. To mitigate these effects, specialized blind deblurring algorithms are often used to preprocess and restore image quality, while distortion correction typically relies on physical camera model approximations [13, 30, 1, 12, 17, 25], using parametric models to realign distorted pixels. Photogrammetry and novel view synthesis frameworks usually derive these parameters using Structure-from-Motion (SfM). However, these methods often struggle to generalize across the non-symmetrical distortions introduced by curved covers [23, 27, 40], a design feature increasingly common in modern XR headsets and dome cameras used in field robotics. Surface irregularities from the polishing process of these covers can deviate from the intended geometry, causing irregular distortions and image blurring. While high-precision interferometers can measure these irregularities in a lab environment before deployment, scaling this measurement process for mass production and after deployment remains a major hurdle. Synthesizing novel viewpoints from aberrated image sequences is therefore non-trivial, as the surface irregularities introduce non-radially symmetric distortions and optical artifacts that conventional calibration techniques struggle to handle [35, 29, 15, 10, 23, 36]. These distorted features cannot be reliably matched to their warped counterparts, especially in complex scenes [39, 15, 21, 17, 25]. Recent advancements have explored embedding camera parameter tuning within reconstruction objectives, allowing for synthesis with noisy calibration parameters [18, 35] and reducing dependency on precise initializations. However, these methods struggle when camera rays deviate significantly from the assumed camera model, which is often the case with curved covers. To address these limitations, we introduce SynthCover, an innovative framework that explicitly models the cover’s geometry and adjusts the camera rays accordingly to account for optical aberrations. SynthCover effectively handles the complex distortions introduced by curved covers, surpassing the limitations of conventional camera model-based methods. By learning the cover geometry, our framework adds an additional degree of freedom for ray propagation, enabling an accurate representation of cover-induced distortions while adhering to physical refractive laws. Our framework demonstrates superior performance compared to state-of-the-art methods, including camera-calibrating novel view synthesis frameworks, in rendering cover-aberrated sequences from real-world and simulated captures. We demonstrate that our cover geometry modeling approach effectively handles ray distortions in complex scenes, producing results superior to existing novel view synthesis frameworks [18, 37, 19] for modeling camera-aberrated sequences. Our contributions can be summarized as follows: • End-to-end novel view synthesis for cover-protected cameras. • Estimating protective-cover’s surface figure geometries through ray tracing. • Protective cover aberrated walkable scene captures for indoor and outdoor environments. We note several assumptions in our work for estimating the surface geometries. Surface geometry estimation in optics usually involves considering spherical aberration, coma, astigmatism, curvature of field, and distortion. Since images, once captured, contain compressed light information and thus incomplete ray characteristics, we here simplify the modeling to handle two general categories of aberration: image blurring and distortions. We assume a monochromatic incoming light compatible with the ray tracing representation in the novel view synthesis methods. The material dispersion model is assumed to be constant with wavelength. Figure 2: (a) Refraction from curved cover distorts light paths and causes rendering artifacts. (b) Our method addresses this by modeling the cover geometry and analytically bending the rays conforming to Snell’s physical refraction law."
https://arxiv.org/html/2411.06363v1,Layer-Wise Feature Metric of Semantic-Pixel Matching for Few-Shot Learning,"In Few-Shot Learning (FSL), traditional metric-based approaches often rely on global metrics to compute similarity. However, in natural scenes, the spatial arrangement of key instances is often inconsistent across images. This spatial misalignment can result in mismatched semantic pixels, leading to inaccurate similarity measurements. To address this issue, we propose a novel method called the Layer-Wise Features Metric of Semantic-Pixel Matching (LWFM-SPM) to make finer comparisons. Our method enhances model performance through two key modules: (1) the Layer-Wise Embedding (LWE) Module, which refines the cross-correlation of image pairs to generate well-focused feature maps for each layer; (2)the Semantic-Pixel Matching (SPM) Module, which aligns critical pixels based on semantic embeddings using an assignment algorithm. We conducted extensive experiments to evaluate our method on four widely used few-shot classification benchmarks: miniImageNet, tieredImageNet, CUB-200-2011, and CIFAR-FS. The results indicate that LWFM-SPM achieves competitive performance across these benchmarks. Our code will be publicly available on https://github.com/Halo2Tang/Code-for-LWFM-SPM.","Humans have the ability to abstract and generalize low-level visual elements, such as contours, edges, colors, textures, and shapes, to form high-level semantic features that aid in recognizing and understanding the similarities and differences between objects. This capability is particularly crucial in few-shot classification tasks, as it allows models to accurately identify and distinguish between different categories based on contrasting critical high-level semantic features, even when faced with a limited number of samples from new categories. In contrast, traditional deep learning methods [1, 2] typically rely on large amounts of labeled data for training in order to recognize and classify specific objects or concepts. In few-shot learning scenarios, these models may encounter challenges, as they are not specifically designed to learn from a small amount of data quickly. Recently, few-shot learning methods have been introduced to address this limitation, typically requiring only a few images to understand the characteristics of a class and generalize these features to unseen images for inductive reasoning. Among these methods, metric-based approaches [3, 4, 5] are computationally efficient, as they do not require extensive parameter tuning or complex model structures. They rely on learning image embeddings to measure the similarity between objects and perform classification. However, through our study of prior methods, we identified two key limitations in metric-based methods: (1) The different semantic focuses produced by various levels of the backbone have not been fully utilized. Features at different levels typically focus on distinct aspects of the image [6]. In [3, 4], only features from the final layer of the backbone are utilized. However, this layer may be overly focused on a specific aspect of the image. In few-shot learning, it is crucial to leverage a wider variety of features to assess the similarity between image pairs more effectively. Chen et al. proposed using a self-attention mechanism to learn the relationships between multi-level features from the backbone but introduced excessive parameters and computational complexity [5]. (2) Semantic pixel misalignment is also a common challenge. In these methods, the final query embedding and support embedding are typically compared using element-wise cosine similarity. However, directly performing element-wise comparisons can often fail to correctly match semantically similar pixels, as illustrated in Fig. 1 (c). To address the issue of utilizing semantic focus, we propose a Layer-wise Embedding (LWE) Module. By computing the correlation map generated from different levels of information and performing layer-wise production, we effectively integrate diverse semantic features without relying on convolution or self-attention operations. As a result, our approach reduces both the number of parameters and the computational complexity. However, after aggregating multi-level semantic features, we observed a persistent issue of positional inconsistency between semantically corresponding objects in paired images. To tackle this challenge, we rearrange the semantic pixel positions in the image pair by leveraging the Hungarian matching algorithm and a learnable matcher. This ensures that the most similar semantic pixels are aligned spatially, enabling the calculation of a more accurate similarity score, which further aids in classification. Our proposed method integrates these two modules by using the LWE to learn semantic embeddings from different levels and the SPM to achieve pixel-wise similarity matching, which creates an end-to-end process that takes an image pair as input and outputs a similarity score. To sum up, our main contributions are as follows: \bullet We propose a novel Layer-wise Embedding Module that efficiently integrates semantic information from different levels with lightweight operations. \bullet We introduce the Semantic-Pixel Matching Module, which leverages the Hungarian algorithm and a learnable matcher to capture pixel-level correlations between image pairs, addressing the limitations of prior work in accurately assessing true similarity. \bullet Our proposed LWEM-SPM combines these two modules, and extensive experiments demonstrate that our method achieves competitive performance. Fig. 1: The key difference between our method and previous approaches lies in our layer-wise embedding computation and the way similarity is computed between query and support images. As shown in (a), Previous work typically uses CNNs or Transformers to generate single-layer image embeddings; however, a single-layer embedding may not effectively integrate complex semantic information. This feature is then used to compute a single-layer correlation map, which is applied to reweight the image features. In contrast, as shown in (b), our method integrates multi-layer outputs from the backbone to form multi-level correlation maps. We then compute layer-wise weights for the features at different levels, creating an image embedding that captures diverse semantic focuses across different levels while avoiding the complexity of CNNs and Transformers. In (c), prior methods typically calculate the similarity between corresponding pixels at the same locations in both images. However, this approach overlooks the possibility that semantically similar pixels may be located in different positions, making it difficult to assess the true similarity between the image pairs accurately. In contrast, (d) our method employs a matching algorithm that identifies the most similar pixel in the support image for each pixel in the query image, even if their positions do not align perfectly. This allows for a more accurate evaluation of the true similarity score."
https://arxiv.org/html/2411.06347v1,"Classification in Japanese Sign Language 
Based on Dynamic Facial Expressions","Sign language is a visual language expressed through hand movements and non-manual markers. Non-manual markers include facial expressions and head movements. These expressions vary across different nations. Therefore, specialized analysis methods for each sign language are necessary. However, research on Japanese Sign Language (JSL) recognition is limited due to a lack of datasets. The development of recognition models that consider both manual and non-manual features of JSL is crucial for precise and smooth communication with deaf individuals. In JSL, sentence types such as affirmative statements and questions are distinguished by facial expressions. In this paper, we propose a JSL recognition method that focuses on facial expressions. Our proposed method utilizes a neural network to analyze facial features and classify sentence types. Through the experiments, we confirm our method’s effectiveness by achieving a classification accuracy of 96.05%.","In Japan, communication methods that rely on knowledge of the Japanese language are frequently used between deaf and hearing individuals. For instance, there are tools such as written communication and speech-to-text applications. However, many deaf individuals struggle with communicating in Japanese because Japanese Sign Language (JSL) has its unique vocabulary and grammar, separate from Japanese. Furthermore, many hearing individuals are not familiar with JSL. The development of JSL recognition methods is required in order to ensure precise and smooth communication between deaf and hearing. Previous research on sign language recognition has primarily focused on American, German, and Chinese sign languages. Moreover, these studies often concentrate on hand movements and apply hand pose estimation techniques for recognition. However, non-manual markers such as facial expressions and body orientation are indispensable for a complete understanding of sign language sentences. In this paper, we propose a recognition method for JSL that focuses on non-manual markers. These markers have significant impact on syntactic and semantic information. For example, when hand gestures expressed in an affirmative statement are accompanied by facial expressions such as wide-open eyes, raised eyebrows, and a tucked chin, the sentence transforms into a Yes/No-question. When paired with repeated weak head shakes and furrowed eyebrows, it is classified as a WH-question. Using a neural network, we analyze facial expressions to distinguish the affirmative sentence, Yes/No-question, and WH-question."
https://arxiv.org/html/2411.06344v1,"CityGuessr: City-Level Video Geo-Localization 
on a Global Scale","Video geolocalization is a crucial problem in current times. Given just a video, ascertaining where it was captured from can have a plethora of advantages. The problem of worldwide geolocalization has been tackled before, but only using the image modality. Its video counterpart remains relatively unexplored. Meanwhile, video geolocalization has also garnered some attention in the recent past, but the existing methods are all restricted to specific regions. This motivates us to explore the problem of video geolocalization at a global scale. Hence, we propose a novel problem of worldwide video geolocalization with the objective of hierarchically predicting the correct city, state/province, country, and continent, given a video. However, no large scale video datasets that have extensive worldwide coverage exist, to train models for solving this problem. To this end, we introduce a new dataset, “CityGuessr68k” comprising of 68,269 videos from 166 cities all over the world. We also propose a novel baseline approach to this problem, by designing a transformer-based architecture comprising of an elegant “Self-Cross Attention” module for incorporating scenes as well as a “TextLabel Alignment” strategy for distilling knowledge from textlabels in feature space. To further enhance our location prediction, we also utilize soft-scene labels. Finally we demonstrate the performance of our method on our new dataset as well as Mapillary(MSLS)[43]. Our code and datasets are available here.","Geolocalization refers to the process of determining the geographic position of a sample, which can be an image, a video or a text description of a place. If the input sample is strictly visual, i.e., an image or a video, the problem is also termed as Visual Place Recognition (VPR). Geolocalizing images has gained popularity over time, witnessing substantial advancements in the field. In contrast, video geolocalization is currently in its early stages of development. The relevance of video geolocalization today cannot be understated. When the origin of a video is unknown, determining the part of the world the video was recorded in, can assist in a variety of investigative and exploratory applications. The current transformation of social media has resulted in an explosion of video content, making it a valuable resource. Videos also tend to have more visual information as compared to images, owing to the temporal context that images lack. This makes the problem of video geolocalization even more essential. There are different levels of granularities in geolocalization problems, right from street identification to worldwide geolocalization, each having its own significance. Image geolocalization has been attempted on both ends of the spectrum[22][34][46][51][52][25][27][3], with varying levels of success using different approaches catering to the specific problems at hand. Same isn’t the case with video geolocalization. There has been some research at the fine-grained level[32][42][49], but the same problem at the global level remains largely unsolved. Thus, in this work we formulate a unique problem of worldwide video geolocalization in an attempt to leverage the information affluence of video domain to address this issue. Geolocalization in general can be performed in two ways. Retrieval is the more popular approach where a query input is compared with a gallery of known references, which gives the location of the query, provided we are able to find the best match. Although retrieval approaches are more accurate at the fine-grained level, they tend to be computationally expensive and depend heavily on the domains of the queries and references. Any domain shifts tend to have massive repercussions which can snowball quickly into larger issues. Also, constructing a gallery that covers the entire world is not feasible. The second approach, i.e., classification overcomes these limitations. Classification constitutes dividing the region of interest, which in our case is the entire world, into classes, be it in the form of places, or literal partitions of the globe; and identifying the class a sample belongs to. Geolocalization via classification not only decreases compute, but also covers the entire world with ease. As an added advantage, classification can be performed at different hierarchies (city level, state/province level, country level, continent level, etc.), enabling the user to adjust as per their requirements. Many recent works focusing on the problem of image-geolocalization[44][33][25][27][3], have proposed classification-based methods for this reason. A general classification pipeline includes an encoder backbone (CNN[20] or transformer[6]-based) to obtain a feature embedding of the input image, and an MLP[16] for class prediction. Previous works also have some additional components or changes in the architecture to aid the classification task, like incorporation of scenes. Keeping this in mind we propose a very unique way of incorporating scenes in our model. Text on the other hand, is relatively unexplored for aiding geolocalization. Intuitively, humans are more likely to identify a location, if they can associate a name to a picture/video of that place which they have previously seen. Consequently, it follows that distilling knowledge from text into a geolocalization model would enhance the model’s prediction capability. This motivates us to incorporate text from labels, i.e. city/state/country/continent names during the training procedure by aligning the features of our model to the text embedding of labels without the use of any additional information. In this paper, we propose a classification-based approach for video geolocalization at global scale. Our objective is to predict the city in which an input video was recorded, and subsequently, the above mentioned hierarchies of state/province, country and continent. Our proposed method comprises of a transformer[6]-based model, with a novel Self-Cross Attention module for scene prediction to assist with the training. We also incorporate soft-labels for computing scene prediction loss during training of our model. A TextLabel Alignment strategy is also implemented for feature enhancement. To the best of our knowledge this is the first attempt to solve this problem, and hence it can serve as a baseline approach for future research. An obstacle in solving this problem, is the absence of a large scale worldwide dataset for video geolocalization. Existing video geolocalization datasets focus only on specific regions, like BDD[47] in California and New York, USA; KITTI[8] in Karlsruhe, Germany; Brno-Toyota[21] in Brno Czech Republic; and Seqgeo[49] in Vermont, USA. This dense coverage of a limited area works well for retrieval-based geolocalization approaches, but it restricts the data domain to limited parts of the world. To train any model on the global scale, geographic coverage is essential as it exposes the model to a diverse set of locations with variations in environment, infrastructure and salient features instrumental in generalization of the approach. In that context, Mapillary(MSLS)[43], is an image sequence dataset, which covers 30 cities around the world. The geographical coverage, although more than the previously mentioned datasets, is still lacking. The number of sequences in Mapillary is also relatively small, which limits its scope for large scale generalized training. Thus there is a requirement for a large scale global level dataset with a substantial geographical coverage. To this end, we propose CityGuessr68k consisting of 68,269 videos from 166 cities all over the world. We also provide soft-scene labels for all video samples in the dataset. Our main contributions can be summarized as follows: • We formulate a novel problem of worldwide video geolocalization • To benchmark this new problem, we introduce the first global-scale video dataset named ‘CityGuessr68k’, containing 68,269 videos from 166 cities. • We propose a baseline approach with a transformer-based architecture with two primary components – Self-Cross Attention module for incorporating scenes (which leverages soft-scene labels for location prediction enhancement) – TextLabel Alignment strategy for distilling knowledge from textlabels in feature space • We demonstrate the efficacy of our model with performance results on CityGuessr68k as well as Mapillary(MSLS) datasets."
https://arxiv.org/html/2411.06318v1,SEM-Net: Efficient Pixel Modelling for image inpainting with Spatially Enhanced SSM,"Image inpainting aims to repair a partially damaged image based on the information from known regions of the images. Achieving semantically plausible inpainting results is particularly challenging because it requires the reconstructed regions to exhibit similar patterns to the semanticly consistent regions. This requires a model with a strong capacity to capture long-range dependencies. Existing models struggle in this regard due to the slow growth of receptive field for Convolutional Neural Networks (CNNs) based methods and patch-level interactions in Transformer-based methods, which are ineffective for capturing long-range dependencies. Motivated by this, we propose SEM-Net, a novel visual State Space model (SSM) vision network, modelling corrupted images at the pixel level while capturing long-range dependencies (LRDs) in state space, achieving a linear computational complexity. To address the inherent lack of spatial awareness in SSM, we introduce the Snake Mamba Block (SMB) and Spatially-Enhanced Feedforward Network. These innovations enable SEM-Net to outperform state-of-the-art inpainting methods on two distinct datasets, showing significant improvements in capturing LRDs and enhancement in spatial consistency. Additionally, SEM-Net achieves state-of-the-art performance on motion deblurring, demonstrating its generalizability. Our source code is available: https://github.com/ChrisChen1023/SEM-Net.","Image inpainting is a highly challenging low-level vision task in computer vision due to its ill-posed nature. It aims to repair partially damaged or missing regions by leveraging information from the known areas [58]. Successful inpainting relies heavily on advanced image representation learning, particularly in capturing both short-range and long-range dependencies [11, 58], to ensure consistent reconstruction between the filled and visible contents. Convolutional Neural Networks (CNNs) are widely used as backbone networks in image inpainting due to their strong performance in learning generalizable representations from images and their effective mining of short-range dependencies through convolution operations [52, 40, 49, 34]. However, their slow-grown receptive field constrains the perception of the global context and hampers the ability to capture long-range dependencies within the image. This limitation is particularly problematic for low-level vision tasks like image inpainting, where single-pixel reconstruction must preserve pixel consistency while accounting for dependencies over larger distances. To address this limitation, researchers have shifted towards transformer-based architectures [25, 2] to better capture the long-range dependencies (LRDs) and global structure. However, transformer-based methods suffer from quadratic computational complexity, which restricts their ability to learn spatial LRDs only at the patch level rather than the pixel level. [54] attempts to model images at the pixel level with transformer, but it focuses on semantic features rather than spatial relations, which means it still lacks the ability to effectively capture spatial LRDs. LRDs are critical in image inpainting, as a lack of LRDs often results in low-quality outcomes due to insufficient context capturing. This is evidenced by the inconsistent eye colours and patterns, as shown in the visualization of the prominent CNN-based method [42] and transformer-based method [25] (Sample I of Fig. 1), where the visible red eye fails to guide the accurate reconstruction of the other eye. Sample I Sample II Input LAMA[42] MAT[25] M-Unet SEM-Net GT Figure 1: Comparisons with the state-of-the-art CNN-based method [42] and transformer-based method [25]. M-Unet is a variant of directly applying the Mamba model [31] followed by a feedforward network [54] in a U-Net. Red boxes and arrows highlight major differences. Our SEM-Net demonstrates the strong capability to capture LRDs visualised by the consistent eye colors and patterns, and addresses the challenge of lack of spatial awareness in M-Unet. Please refer to the supplementary material for more quantitative results. One feasible solution to this challenge is from [14], which proposes an emerging Selective State Space Model (SSM), known as Mamba. SSM has demonstrated its efficient and effective capacity in learning LRDs, and good adaptability in computer vision [29]. As shown in Sample I of Fig. 1, directly adopting SSM [31] (M-Unet) captures LRDs effectively and achieves more consistent eye colour. However, as the vanilla SSM scans the data as a sequence with a single fixed direction, it lacks 2D spatial awareness, making the way to model pixels in SSM crucial. As illustrated in Sample II of Fig. 1, a vanilla SSM model [31] shows positional drifting of the inpainted left eye (upper than the right eye). This insight introduces two key challenges: (i) how to maintain the continuity and consistency of pixel adjacency for pixel-level dependencies learning while processing the SSM recurrence; and (ii) how to effectively integrate 2D spatial awareness to the predominant linear recurrent-based SSMs. To solve these challenges, we propose SEM-Net: Spatially-Enhanced SSM Network for image inpainting, which is a simple yet effective encoder-decoder architecture incorporating four-stage Snake Mamba Blocks (SMB). The proposed SMB is assembled by two novel modules, which holistically integrate local and global spatial awareness into the model. Specifically, we introduce the Snake Bi-Directional Modelling module (SBDM) in place of vanilla SSM. It brings the crucial spatial context into a linear recurrent system, modelling images in two directions by consistently scanning each pixel with a snake shape. Moreover, we explicitly incorporate positional embedding into the sequences via a Position Enhancement Layer (PE layer) to strengthen the long-range positional awareness and improve the sensitivity to specific parts of the sequence(e.g., masked regions). We further propose Spatially- Enhanced Feedforward Network (SEFN) to complement the local spatial dependencies. aiming to leverage spatial information stored in the feature before SBDM, to refine the feature after SBDM with a gating mechanism. Comparative experiments show that SEM-Net outperforms state-of-the-art approaches across two distinct datasets, i.e, CelebA-HQ [22] and Places2 [60]. Detailed qualitative comparison demonstrates that our method achieves a significant improvement in capturing spatial LRDs while preserving better spatial structure. In addition, SEM-Net achieves state-of-the-art performance on two motion-deblurring datasets, further demonstrating our method’s generalizability in image representation learning. Our main contributions are summarized as follows: • We propose a novel U-shaped Spatially-Enhanced SSM architecture focused on capturing short- and long-range spatial dependencies in image inpainting. To the best of our knowledge, SEM-Net is the first SSM-based model in this research field. • We propose a Snake Mamba Block (SMB), involving a Snake Bi-Directional Modelling (SBDM) module and a Position Enhancement Layer (PE layer), to implicitly integrate crucial spatial context awareness into a linear recurrent SSM, and explicitly enhance the long-range positional awareness. • We propose a Spatially-Enhanced Feedforward Network (SEFN) to complement local spatial dependencies learning among pixels, enhancing the spatial awareness throughout image representation learning."
https://arxiv.org/html/2411.06297v1,Adaptive Aspect Ratios with Patch-Mixup-ViT-based Vehicle ReID,"Vision Transformers (ViTs) have shown exceptional performance in vehicle re-identification (ReID) tasks. However, non-square aspect ratios of image or video inputs can negatively impact re-identification accuracy. To address this challenge, we propose a novel, human perception driven, and general ViT-based ReID framework that fuses models trained on various aspect ratios. Our key contributions are threefold: (i) We analyze the impact of aspect ratios on performance using the VeRi-776 and VehicleID datasets, providing guidance for input settings based on the distribution of original image aspect ratios. (ii) We introduce patch-wise mixup strategy during ViT patchification (guided by spatial attention scores) and implement uneven stride for better alignment with object aspect ratios. (iii) We propose a dynamic feature fusion ReID network to enhance model robustness. Our method outperforms state-of-the-art transformer-based approaches on both datasets, with only a minimal increase in inference time per image.The code is released here: Adaptive_AR_PM_TransReID.","Vehicle re-identification (ReID) is critical in intelligent transportation systems, tasked with identifying vehicles across multiple non-overlapping cameras [1]. Vehicle Re-ID has seen success with both Convolutional Neural Network (CNN) [2, 3, 4] and Vision Transformers (ViTs) backbones [5, 6, 7, 8, 9]. Challenges such as variations in viewpoint, pose, illumination, occlusion, background clutter, similar vehicle models, resolution differences, temporal changes, aspect ratios, and privacy constraints, all contribute to the complexity of obtaining accurate identification. Addressing these issues requires deep learning models to extract robust and discriminative features that can withstand these variations [10, 11]. A combination of global and local features is essential for effective vehicle representation [12, 13, 14]. While distinct identifiers such as license plates are accurate and widely used in urban areas [15, 16], they are less applicable in the highway context due to privacy concerns and practical limitations. Numerous benchmark datasets, such as VeRi-776 [10], PKU-VD [17], VehicleID [18], and VERI-Wild [19], are crucial for developing algorithms for vehicle ReID [20]. State-of-the-art models leverage self-attention mechanisms, with ViTs capturing discriminative details better than CNN-based methods [6]. ViTs adapt transformers from NLP [21] to computer vision, using self-attention on image patches for tasks like image classification. Variants such as DeiT [22], Swin [23], and PVT [24] have proven effective in image classification, detection, and Re-ID [25]. However, the varying aspect ratios across datasets present challenges in model training. Unlike CNN-based models, ViTs treat the entire image as a sequence of patches, necessitating careful resizing and cropping [26, 27, 28, 29]. Early ViT implementations adopted resizing strategies from CNNs, often distorting aspect ratios and potentially compromising performance, especially in tasks dependent on object shape and scale. Subsequent studies explored padding strategies, adaptive post-patch extraction, trainable resizing networks, aspect-ratio aware attention mechanisms, and multi-scale/multi-aspect training [30, 31, 32, 33, 26, 34]. However, these approaches introduce computational burdens, data requirements, and optimization challenges. Figure 1: (Left) Existing Method: Image size is typically fixed and set to a single square shape. (Right) Our Method: Combined Vision Transformer (ViT)-based ReID model that dynamically fuses features extracted from multiple models. Each model is trained on a different fixed size and aspect ratio. In summary, there is a research gap in applying ViTs to vehicle ReID, including optimizing scaling and resizing strategies, understanding aspect ratio effects, and exploring patch-level mixup [35] in multi-aspect ratio scenarios. Intra-image mixup can help models learn detailed features despite resizing distortions. For instance, TransReID [6] introduced a jigsaw patch module (JPM) to enhance feature robustness, but pixel-level intra-mixup remains unexplored. Figure 2: PM module. We propose aspect ratio as a key factor affecting vehicle ReID performance and the robustness of feature learning in ViTs. We conduct comprehensive experiments to explore the effects of various aspect ratios on ViT-based ReID. To enhance the model’s generality to various aspect ratios, we dynamically fuse features from several models trained on images with different aspect ratios, as shown in Fig. 1. Additionally, we propose a novel intra-image patch mixup (PM) data augmentation method to improve the model’s learning of details and mitigate overfitting during training. We also employ an uneven stride strategy in the patchify step to reduce distortion caused by resizing. The key contributions of this work are summarized as follows: • Identified aspect ratio as a critical factor affecting vehicle ReID performance and ViT’s feature learning robustness. • Conducted extensive experiments on the impact of aspect ratios on ViT-based ReID. • Developed a general ViT-based ReID framework that uses dynamic feature fusion across different aspect ratios to enhance robustness. • Introduced intra-image patch mixup (PM) to improve learning and reduce overfitting. • Implemented an uneven stride strategy to reduce distortion from resizing."
https://arxiv.org/html/2411.06287v1,Hidden in Plain Sight: Evaluating Abstract Shape Recognition in Vision-Language Models,"Despite the importance of shape perception in human vision, early neural image classifiers relied less on shape information for object recognition than other (often spurious) features. While recent research suggests that current large Vision-Language Models (VLMs) exhibit more reliance on shape, we find them to still be seriously limited in this regard. To quantify such limitations, we introduce IllusionBench, a dataset that challenges current cutting-edge VLMs to decipher shape information when the shape is represented by an arrangement of visual elements in a scene. Our extensive evaluations reveal that, while these shapes are easily detectable by human annotators, current VLMs struggle to recognize them, indicating important avenues for future work in developing more robust visual perception systems. The full dataset and codebase are available at: https://arshiahemmat.github.io/illusionbench/","Deep neural networks have accomplished remarkable breakthroughs in visual recognition over the past decade Krizhevsky et al. (2012); He et al. (2016); Dosovitskiy et al. (2020); Radford et al. (2021); Gemini Team et al. (2023); but these models have also shown longstanding, fundamental limitations – for instance, the performance of these models degrades when faced with common corruptions and perturbations Hendrycks and Dietterich (2019), or natural out-of-distribution data Hendrycks et al. (2021). How can we facilitate more robust neural vision models? A natural place to begin is by considering the source of robustness in human vision. Human object recognition is largely based on shape perception Landau et al. (1988); Biederman and Ju (1988); Xu et al. (2004); Baker and Kellman (2018), which is essential to the robustness of human vision due to the invariance of shape to common transformations such as translation, rotation, scaling, and changes in illumination, color, and texture Kendall (1984); Hummel (2001); Ommer (2013); Dryden and Mardia (2016). As such, substantial work in computer vision has focused on improving and evaluating shape perception (e.g., Ritter et al., 2017; Geirhos et al., 2019; Islam et al., 2021; Geirhos et al., 2021; Gavrikov et al., 2024, inter alia), finding that early deep vision models relied much more on texture than shape in image classification Geirhos et al. (2019); Islam et al. (2021); Pinto et al. (2022a); Benarous et al. (2023); Subramanian et al. (2023), which is believed to contribute to their lack of robustness Geirhos et al. (2020); Gavrikov et al. (2024). Later work observed that vision encoders trained with larger-scale data weakly supervised by language (e.g., CLIP; Radford et al., 2021) show improvements in shape recognition (Geirhos et al., 2021; Gavrikov et al., 2024). While clear indicators of progress in visual perception of neural vision models, it is important to note that all of the above studies on shape recognition in vision models have relied on two standard datasets, Cue Conflict and Stylized-ImageNet Geirhos et al. (2019), which presents several concerns – for instance, these datasets do not include coherent, naturalistic visual scenes; they are built using legacy style transfer techniques that damage shape information and prevent the reproduction of fine-grained textures; and each image includes only a single object class represented as an abstract shape using perceptually uniform textures (see Section 2 for a more detailed critique). Figure 1: Can vision-language models recognize these shapes? IllusionBench dataset contains images in which scene elements are arranged to represent abstract shapes. To address these limitations, we introduce IllusionBench,111 We use “Illusion” in the name of our benchmark because images in our dataset can be understood as instances of pareidolia, an illusion caused by the tendency of the human visual system to identify familiar shapes in complex scenes. Our dataset should not be confused with HallusionBench Guan et al. (2023), which instead serves as a diagnostic tool to distinguish between VLM reasoning error modes, such as those caused by the language component versus visual component of VLMs. which represents shape information by an arrangement of visual elements existing in coherent, naturalistic scenes (see Figure 1). We evaluate vision-language models (VLMs) using IllusionBench in three scenarios: (1) measuring zero-shot performance of generative VLMs (e.g., LLava Liu et al. (2024b), GPT-4o OpenAI (2023), and Gemini Gemini Team et al. (2023)); (2) measuring few-shot performance of VLMs using in-context learning (e.g., (Zhao et al., 2023)); and (3) fine-tuning contrastive VLMs (e.g., CLIP Radford et al. (2021)) to recognize abstract shapes and testing their ability to generalize to unseen scenes. We find that, while human annotators can easily identify these shapes, VLMs struggle to identify shapes and instead focus on the scene components, failing to exhibit the abstract shape recognition capabilities that are essential for enabling humanlike visual robustness."
https://arxiv.org/html/2411.06232v1,Crowd3D++: Robust Monocular Crowd Reconstruction with Upright Space,"This paper aims to reconstruct hundreds of people’s 3D poses, shapes, and locations from a single image with unknown camera parameters. Due to the small and highly varying 2D human scales, depth ambiguity, and perspective distortion, no existing methods can achieve globally consistent reconstruction and accurate reprojection. To address these challenges, we first propose Crowd3D which leverages a new concept, Human-scene Virtual Interaction Point (HVIP), to convert the complex 3D human localization into 2D-pixel localization with robust camera and ground estimation to achieve globally consistent reconstruction. To achieve stable generalization on different camera FoVs without test-time optimization, we propose an extended version, Crowd3D++, which eliminates the influence of camera parameters and the cropping operation by the proposed canonical upright space and ground-aware normalization transform. In the defined upright space, Crowd3D++ also designs an HVIPNet to regress 2D HVIP and infer the depths. Besides, we contribute two benchmark datasets, LargeCrowd and SyntheticCrowd, for evaluating crowd reconstruction in large scenes. Experimental results demonstrate the effectiveness of the proposed method. The source code and data will be made publicly available for research purposes.","3D pose, shape, and location reconstruction for hundreds of people from a single image will help model crowd behavior for simulation and security monitoring. However, no existing methods can achieve global consistency in this task. In this paper, we aim to reconstruct the 3D poses, shapes, and locations of hundreds of people in the global camera space from a single large-scene image with unknown camera parameters, as shown in Fig. 1 Although regression-based [1, 2, 3, 4, 5, 6, 7, 8] and diffusion-based [9, 10] monocular human pose and shape reconstruction methods achieve excellent pixel-aligned projection performance mostly with the weak perspective modeling, estimating global camera space locations together with human poses and shapes for a single person or multiple people from a single image is still a complex problem due to the depth ambiguity. Existing multi-human pose and shape reconstruction methods [7, 11, 12, 13, 14] reconstruct multi-human 3D poses, shapes, and depths in an assumed camera space with a constant FoV (Field of View). These methods cannot regress so many people from an entire large-scene image [15] due to the relatively small and varying human scales compared to the image size. Simply using a top-down strategy (detecting and reconstructing each person separately) with any additional depth estimation method will lead to wrong global reprojection when shifting the output to the global camera space. Recently, GroupRec [8] fixes this reprojection issue under regular Field of View (FoV) cases by an initial depth computation [16] and a test-time optimization strategy with additional correlations provided by the hypergraph relational reasoning network. However, the outputs of GroupRec may include incorrect spatial locations and incorrect reprojections for inputs with large camera FoV, as shown in Fig. 2, Wrong spatial locations are caused by depth ambiguity; the reason for the incorrect reprojection is that the perspective distortion is not correctly resolved, i.e., the visible human orientations in the image are coupled with both the camera intrinsics and the cropping position during the single human reconstruction from the cropped images. In general, there are four significant challenges in reconstructing hundreds of people with global consistency from a single image: (1) There are a large number of people with relatively small and highly varying 2D scales; (2) Due to the depth ambiguity from a single view, it is difficult to directly estimate absolute 3D positions and 3D poses of people in the large scene; (3) Generalizing to an arbitrary camera is challenging because the reprojection function of a local area is related to the global camera intrinsics and the cropping position; (4) There are no large-scene image datasets with hundreds of people to train and evaluate crowd reconstruction in large scenes. (a) (b) Figure 2: The reconstruction results of the state-of-the-art method [8] include wrong relative position (a) and incorrect reprojection (b). In contrast, the proposed Crowd3D++ overcomes these challenges. The full visualization of Crowd3D++ corresponding to (a) can be found in Fig. 1. To address these challenges, we propose Crowd3D, the first framework for crowd reconstruction from a single large-scene image. To deal with a large number of people and various human scales, we propose an adaptive human-centric cropping scheme for a consistent scale proportion of people among different cropped images. To ensure globally consistent spatial locations and coherence with the scene, we propose a progressive ground-guided reconstruction network, Crowd3DNet, to reconstruct globally consistent human body meshes from the cropped images by pre-estimating the camera parameters and the ground plane in the camera space. To solve the depth ambiguity of a single image, we present a novel concept called Human-scene Virtual Interaction Point (HVIP) for effectively converting the 3D crowd spatial localization problem into a progressive 2D image space regression problem. Benefiting from HVIP, our model can reconstruct people with various poses, including non-standing ones. To mitigate the influence of perspective distortion of different cameras, Crowd3D employs a self-supervised scene-specific optimization to generalize to new scenes. However, the optimization increases the computation time, and learning how the camera parameter affects the reconstruction results is difficult due to the lack of ground-truth 3D annotations. This difficulty may result in inaccurate poses and reprojection performance, as shown in Fig. 1. To adapt to new scenes with arbitrary camera parameters without test-time optimization, we propose Crowd3D++ with a canonical upright space where the influence of camera parameters and the cropping operation is eliminated, and the human semantic scales and orientations in the image are normalized. To allow conversion between the camera space and the upright space of a person, we propose a ground-aware normalization transform with the help of the reliable ground plane estimation of Crowd3D. We use the state-of-the-art methods [5, 4] for single human reconstruction in the upright space and then convert the results to the camera space with this ground-aware normalization transform. Therefore, we achieve globally consistent reconstruction in camera space while maintaining nearly the same reprojection performance just as these state-of-the-art methods reconstruct a single human via weak perspective modeling. Besides, we design an HVIPNet to estimate 2D HVIP in the upright space and upgrade our human-centric cropping scheme to iterative ground-aware cropping, eliminating the need for cropping hyper-parameters. Significantly, Crowd3D++ can be fed with only an image as input or an image with camera parameters or ground plane for higher accuracy. We also collect and annotate LargeCrowd, a benchmark dataset with over 100K labeled humans (2D bounding boxes, 2D keypoints, 3D ground plane, and HVIPs) in 733 gigapixel images (19200\times 6480) of 9 scenes. To our best knowledge, this is the first large-scene crowd dataset that enables training and evaluation on large-scene images with hundreds of people. Besides, we synthesize and render a virtual dataset at the 9600\times 5400 image resolution with publicly released scenes and scanned human models for quantitative testing under various camera parameters. To summarize, our main contributions include: • We propose Crowd3D, a framework that achieves globally consistent human reconstruction in camera space from a single large-scene image containing hundreds of people. • We propose the Human-scene Virtual Interaction Point (HVIP) concept and the progressive position transform to solve the depth ambiguity problem. • We propose an extended version, Crowd3D++, that can generalize to new scenes with arbitrary camera parameters without test-time optimization via the upright space. • We propose a robust ground-aware normalization transform to enable the bi-directional conversion between the camera space and the newly defined upright space. We also design an HVIPNet to regress 2D HVIP in the upright space. • We contribute LargeCrowd, a benchmark dataset with over 100K labeled crowded people, and synthesize a virtual dataset SyntheticCrowd for quantitative evaluation under various camera parameters. Compared to our previous conference version [17], we make the following improvements in Crowd3D++: (1) We upgrade the human-centric cropping scheme to iterative ground-aware cropping, which eliminates the need for cropping hyper-parameters; (2) We propose the upright space and the ground-aware normalization transform to eliminates the influence of the camera parameters and cropping operation, eliminating the test-time scene-specific optimization of Crowd3D; (3) We design an HVIPNet for the HVIP estimation where the 2D semantic scale and the 2D orientation of a human are normalized while Crowd3DNet in Crowd3D mixes these influencers; (4) We build a synthetic dataset to evaluate the 3D reconstruction accuracy of the proposed method under different camera settings."
https://arxiv.org/html/2411.06206v1,Text2CAD: Text to 3D CAD Generation via Technical Drawings,"The generation of industrial Computer-Aided Design (CAD) models from user requests and specifications is crucial to enhancing efficiency in modern manufacturing. Traditional methods of CAD generation rely heavily on manual inputs and struggle with complex or non-standard designs, making them less suited for dynamic industrial needs. To overcome these challenges, we introduce Text2CAD, a novel framework that employs stable diffusion models tailored to automate the generation process and efficiently bridge the gap between user specifications in text and functional CAD models. This approach directly translates the user’s textural descriptions into detailed isometric images, which are then precisely converted into orthographic views, e.g., top, front, and side, providing sufficient information to reconstruct 3D CAD models. This process not only streamlines the creation of CAD models from textual descriptions but also ensures that the resulting models uphold physical and dimensional consistency essential for practical engineering applications. Our experimental results show that Text2CAD effectively generates technical drawings that are accurately translated into high-quality 3D CAD models, showing substantial potential to revolutionize CAD automation in response to user demands.","Industrial Computer-Aided Design (CAD) models are essential tools in modern manufacturing. They serve as detailed blueprints for a wide range of products, from simple tools to complex machinery. CAD software enables engineers and designers to create precise geometric representations of products, facilitating visualization, simulation, and manufacturing. Automating the generation of these models from conceptual descriptions not only enhances productivity and reduces time-to-market but also fosters innovation. This aligns with broader industry trends toward digital transformation and smart manufacturing. Despite progress, automating the generation of CAD from user requests poses challenges (Kasik, Buxton, and Ferguson 2005). Traditional methods, often manual and time-consuming, struggle with complex designs and fail to handle the nuances required by modern specifications. Recent advances in diffusion models have shown the potential to generate detailed images from textual prompts (Zhao et al. 2023; Chen et al. 2024; Ruiz et al. 2023; Chefer et al. 2023; Zhang, Rao, and Agrawala 2023). However, these models typically do not grasp three-dimensional constraints, leading to outputs that, while visually impressive, fall short in practical engineering applications. CAD models, on the other hand, can be effectively represented through technical drawings, which are 2D projections of the 3D model to orthographic views (Governi et al. 2013). Although diffusion models excel at generating these 2D images, they often do not maintain the necessary physical and dimensional consistency required for these drawings to be directly usable in manufacturing. This inconsistency is a significant limitation, as each drawing must accurately reflect the physical properties of the CAD model across multiple orthographic views to be useful in practical applications. (a) Figure 1: Overview of our method. Our method converts textual descriptions into a 3D CAD model through a multi-step process. First, the text is transformed into an isometric image representing the described features. This image is then mapped into orthographic technical drawings, which serve as the foundation for generating the 3D CAD model. In response to these automation challenges, we develop Text2CAD, a framework that leverages isometric drawings and stable diffusion models to bridge the gap between textual descriptions and precise CAD models, as shown in Figure 1. Given a user’s textural description of a 3D object, our method starts by generating an isometric image that effectively captures the geometric features of the 3D object across various perspectives. Accordingly, a novel view generation difMohsefusion model takes the isometric image as input and transforms it into detailed orthographic views. This approach not only streamlines the CAD generation process, but also ensures the accuracy and consistency needed for practical manufacturing applications. By enabling the direct conversion of text descriptions into comprehensive technical drawings, Text2CAD significantly enhances the efficiency and accessibility of CAD model creation, aligning with the demands of modern industry. To facilitate robust training and evaluation of our model, we introduce a new dataset comprising detailed descriptions and corresponding technical drawings of CAD models. This dataset serves as a foundation for training and benchmarking the Text2CAD framework. Our experimental results confirm that the Text2CAD framework reliably produces technical drawings that are accurately translated into practical 3D CAD models. This reveals the capability of our framework to bridge the gap between textual descriptions and functional engineering output, effectively streamlining the CAD model generation. Our main contributions are threefold: • We introduce Text2CAD, a novel framework that uses stable diffusion models to automate the creation of CAD models from textual descriptions. • Our method streamlines the process by generating a detailed isometric drawing and transforming it into consistent orthographic views, e.g., top, front, and side. • Experimental results demonstrate that Text2CAD reliably produces technical drawings that translate into high-quality 3D CAD models, effectively bridging the gap between textual prompts and practical engineering outputs."
https://arxiv.org/html/2411.06197v1,Multi-object Tracking by Detection and Query: an efficient end-to-end manner,"Multi-object tracking is advancing through two dominant paradigms: traditional tracking by detection and newly emerging tracking by query. In this work, we fuse them together and propose the tracking-by-detection-and-query paradigm, which is achieved by a Learnable Associator. Specifically, the basic information interaction module and the content-position alignment module are proposed for thorough information Interaction among object queries. Tracking results are directly Decoded from these queries. Hence, we name the method as LAID. Compared to tracking-by-query models, LAID achieves competitive tracking accuracy with notably higher training efficiency. With regard to tracking-by-detection methods, experimental results on DanceTrack show that LAID significantly surpasses the state-of-the-art heuristic method by 3.9% on HOTA metric and 6.1% on IDF1 metric. On SportsMOT, LAID also achieves the best score on HOTA metric. By holding low training cost, strong tracking capabilities, and an elegant end-to-end approach all at once, LAID presents a forward-looking direction for the field.","Multi-Object Tracking (MOT) is a vital task in computer vision. Given a video with tracking classes, MOT aims to recognize, localize and assign consistent identification numbers to targets over time. Fundamentally, it can be partitioned into the detection task and association task. How to manage the relationship between the two tasks has been a central theme throughout the development of the field. At present, the field is driven forward by two leading paradigms, the conventional tracking by detection (Bewley et al. 2016; Wojke, Bewley, and Paulus 2017; Zhang et al. 2022) and the emerging tracking by query (Zeng et al. 2022; Gao and Wang 2023), which are progressing in tandem. The tracking-by-detection paradigm separates the two fundamental tasks apart. Objects are first detected spatially and then associated temporally. It is a clear and well-modularized framework where the association stage becomes the focus. Appearance information and motion patterns are two inherent tracking cues to be considered. Several issues yet reside when integrating the two cues. In the aspect of appearance information, a specific model or branch is typically exploited to extract appearance features for re-identification (ReID). However, the ReID model needs independent training with extra efforts and its features maybe sub-optimal in the MOT setting (Seidenschwarz et al. 2023). While a branch would raise competition among the detection and association tasks in the major model (Zhang et al. 2021). Regarding to the motion patterns, assumptions are required by motion models to predict object positions. Its effectiveness is limited as they are largely simplified, failing to represent actual motion information. Finally, the two kinds of cues are transformed into an affinity matrix, based on which objects are grouped into trajectories. This is a heuristic process and requires sophisticated designs with hand-crafted hyper-parameters. The property causes the weakness to tackle complex scenarios containing various motion patterns, heavy occlusions etc. In addition, although the tracking-by-detection paradigm focuses on the association stage, complex multi-step settings scatter the paradigm into many cells. As a result, it lacks elegance and overall holism. The tracking-by-query paradigm carries out the two fundamental tasks simultaneously (Zeng et al. 2022; Gao and Wang 2023). Its models are modified from Transformer-based detectors (Carion et al. 2020) and perform coherently with the help of query mechanism. Compared to tracking-by-detection methods, they achieve remarkable association capabilities. But they abandon the well-modularized framework and couples the two tasks together, which makes them intra-conflicted and cumbersome. As there are works alleviating the confliction problem (Zhang, Wang, and Zhang 2023; Yan et al. 2023; Yu et al. 2023), training the two tasks as a whole remains low efficiency because of the discrepancies between them. Specifically, the detection task focuses on single images while the association task requires consecutive frames to learn temporal cues, within which the spatial information is abundant for detection. Moreover, detectors could easily refer to strong data augmentations like Mosaic, Mixup, etc, to enhance the detection performance, which is nontrivial in the training of a tracker. Last, the coupling feature makes it isolated from the convenience when detectors already have satisfiable detection performance in the tracking scenarios. Based on the preceding analysis, we cannot help but ask: Can we achieve excellent association capabilities and the elegant end-to-end approach of tracking-by-query models while still maintaining the clearly structured framework of tracking-by-detection methods? It is an open question. In this work, we answer it by adding a learnable associator upon pretrained detectors, persisting the tracking-by-detection paradigm. Meanwhile, the associator handles objects in the form of object query, which are directly decoded into predictions, following the tracking-by-query paradigm. On top of the two prerequisites, the rest of issues are tackled by the associator. Concretely, we solve them through the interaction and decoding steps. First of all, the Basic Information Interaction (BII) module is proposed to supply interactions among detection queries and track queries. Owing to the BII module primarily focuses on the content part of queries, the Content-Position Alignment (CPA) module is consequentially advocated to update the positional aspect, fostering the alignment of the two parts. Experiencing the BII and CPA modules, the fully interacted object queries are decoded into prediction results via a Transformer decoder layer. To sum up, the tracker with the Learnable Associator could capture complicated tracking cues and realize impressive performance through the Interaction and Decoding process, which is named as LAID. LAID represents a novel tracking-by-detection-and-query paradigm. It is displayed and compared with previous paradigms in Figure 1. We evaluate LAID on large-scale datasets, DanceTrack and SportsMOT. With simple and effective methods, LAID surpasses the state-of-the-art heuristic tracking-by-detection method Hybrid-SORT (Yang et al. 2024) by 3.9% on HOTA metric and 6.1% on IDF1 metric. When compared to current end-to-end methods, LAID achieves competitive performance in a more efficient manner. The results of SportsMOT also demonstrate the effectiveness of LAID. Overall, the contributions of this work are summarized as follows. • We propose LAID to achieve MOT through a novel tracking-by-detection-and-query paradigm, combining low training cost, strong association capabilities and an elegant end-to-end fashion. • We propose the BII module and the CPA module, guaranteeing the effectiveness of LAID. • We acquire an impressive balance between tracking accuracy and training cost compared with mainstream MOT methods. Figure 2: The overall framework of LAID. Detection queries are generated by pretrained detectors. They are responsible for detecting new-born objects. Track queries are initially copied from the detection queries of new-born objects. Afterwards, they are linked to these objects and propagated over time. In the associator, the two types of object queries get interacted and then directly decoded into tracking results. For the first frame I_{0}, as track queries do not exist, detection queries are immediately decoded into final results."
https://arxiv.org/html/2411.06173v1,LSSInst: Improving Geometric Modeling in LSS-Based BEV Perception with Instance Representation,"With the attention gained by camera-only 3D object detection in autonomous driving, methods based on Bird-Eye-View (BEV) representation especially derived from the forward view transformation paradigm, i.e., lift-splat-shoot (LSS), have recently seen significant progress. The BEV representation formulated by the frustum based on depth distribution prediction is ideal for learning the road structure and scene layout from multi-view images. However, to retain computational efficiency, the compressed BEV representation such as in resolution and axis is inevitably weak in retaining the individual geometric details, undermining the methodological generality and applicability. With this in mind, to compensate for the missing details and utilize multi-view geometry constraints, we propose LSSInst, a two-stage object detector incorporating BEV and instance representations in tandem. The proposed detector exploits fine-grained pixel-level features that can be flexibly integrated into existing LSS-based BEV networks. Having said that, due to the inherent gap between two representation spaces, we design the instance adaptor for the BEV-to-instance semantic coherence rather than pass the proposal naively. Extensive experiments demonstrated that our proposed framework is of excellent generalization ability and performance, which boosts the performances of modern LSS-based BEV perception methods without bells and whistles and outperforms current LSS-based state-of-the-art works on the large-scale nuScenes benchmark. Code is available at https://github.com/WeijieMax/LSSInst.","As a crucial component in 3D perception, 3D object detection can be applied in various fields, such as autonomous driving and robotics. Although LiDAR-based 3D detection methods [43, 21, 55, 48] are verified as having remarkable performance, research in camera-based methods [35, 45, 46, 17, 16, 27] has received increasing attention in recent years. The reasons can be attributed not only to the lower deployment cost but also to the advantages offered by long-range distance and the identification of visual road elements [27, 34]. However, unlike LiDAR sensors that provide direct and accurate depth information, detecting objects solely based on camera sensor images poses a significant challenge. Thus, how to utilize multi-view images to build up effective representations has become a key issue. Figure 1: The conceptual comparison of LSSInst with previous camera-based fashions. Recently, significant progress has been achieved in methods that utilize the bird’s-eye view (BEV) whose view transformation can be mainly categorized as forward type [25, 24, 11, 34] based on lift-splat-shoot (LSS) [35] and backward type [27, 49] based on the learnable BEV query. Due to its purely implicit aggregation by uninterpretable but forcibly dense queries, the backward type shows lower performance and expansibility [13], enabling the LSS-based forward type to become mainstream in modern BEV paradigms for camera-only 3D detection at present. Based on the LSS hypothesis and the fact that most objects in the scene are close to the ground, LSS-based BEV provides a perspective with minimal parallax ambiguity and information loss in observing the objects as a whole. Illustrated by Fig. 1 (I), these methods lift images into frustums based on depth distribution prediction and splat them into a BEV space, gathering information from multiple 2D views for a global representation of the scene. This representation is in the form of a planar view with compressed height (z-axis) and reduced resolution to ensure computational efficiency. The BEV feature benefits from its holistic representation and dense feature space, making it well-suited for capturing the scene’s structure and data distribution. However, the geometrically-compressed nature of the BEV representation such as resolution and axis reduction, inherently limits its ability to provide precise 3D position descriptions of objects or fully exploit detailed features for object matching particularly in the 3D detection task which requires accurate prediction of 3D object bounding boxes. Meanwhile, as another typical fashion and shown in Fig. 1 (II), sparse-based methods [46, 31, 3] skip the BEV formulation and directly leverage object-level representations and exploit the 3D geometric prior to regress object bounding boxes from the 3D perspective. However, due to the initialization semantic dispersity [22], especially in more complex 3D perception, they fail to capture abundant object-aware features from the image at once in comparison with the BEV feature that fits adequate semantic information in the scene, resulting in lower overall performance than the contemporaneous BEV-based methods. Table 1: The per-category AP comparison between two typical fashions with equivalent detection ability (\DeltamAP less than 0.5%) methods on the nuScenes \mathtt{test} set. Group 1 Group 2 Methods BEVDet [17] Spatial-DETR [10] CFT-BEV3D [19] DETR3D [46] mAP 42.4 42.5 41.7 41.2 car 0.643 0.610 0.628 0.603 truck 0.350 0.332 0.348 0.333 bus 0.358 0.330 0.347 0.290 pedestrian 0.411 0.462 0.416 0.455 bicycle 0.296 0.327 0.299 0.308 traffic_cone 0.601 0.629 0.596 0.627 barrier 0.614 0.582 0.607 0.565 Following this, there are some interesting and corroborative findings in the per-class AP comparison between the two aforementioned fashions as shown in Tab. 1. Notably, considering the practical variety such as data augmentation and training strategies, the difference between the overall mAP values of selected methods in the same group is strictly less than 0.5% which ensures both detection abilities are equivalent. We can observe that there is the same AP tendency among the classes. Specifically, BEV representation seems more attentive to regular objects (car, bus, truck, barrier) with distinct movements or common positions in the scene, with relative insensitivity to the objects (pedestrian, bicycle, traffic cone) with uncertain trajectories or dispersed locations, which further proves its characteristics of fitting data distribution and leaning to the scene-level focus. Inspired by this, to brighten the complementary synergy of both fashions and make up for the missing details in the representation formulation of current LSS-based BEV perception as well as utilize multi-view geometry constraints, we are motivated to propose LSSInst, incorporating the sparse instance-level representations based on the scene-level representations to look back for more detailed feature with geometric matching. As illustrated in Fig. 1 (III), based on the global scene-level pre-feature, instance-level features are pushed to look back at the image locally, focusing on more fine-grained pixel features and allowing for flexible geometric matching, which ultimately generates a final perception result that combines globally-semantic and locally-geometric information. However, this collaboration also poses challenges, as the most straightforward solution of naively sharing the bounding box proposal is intuitively and experimentally failed 111See Sec. 4.5 and Tab. 6 for more details. As aforementioned, traditional sparse-based detection methods suffer from initialization semantic dispersity and inadequate semantic understanding of the scene, the above solution would sever the coherence with the dense representations. With this in mind, we propose the instance adaptor module to establish semantic coherence between the scene and instances and an instance branch for detection. The instance adaptor module generates multiple sparse queries and their corresponding 3D boxes through multi-level adaptive aggregation. The instance branch focuses on fine-grained sparse feature extraction and geometric matching using prepared inputs, such as box embeddings and spatiotemporal sampling and fusion. On the nuScenes dataset, our LSSInst method demonstrates strong generalization ability. Compared to other typical LSS-based methods, LSSInst achieves significant improvements in mAP. Specifically, it outperforms BEVDet by 5.0%, BEVDepth by 2.2%, BEVStereo by 2.6%, and surpasses the state-of-the-art LSS-based method SOLOFusion by 1.6%. Our main contributions can be concluded as follows: i) We proposed LSSInst, a two-stage framework that improves the geometric details in LSS-based BEV perception with instance representations; ii) We proposed the instance adaptor to maintain the BEV-to-instance semantic coherence and a newly-designed instance branch to look back and aggregate features spatiotemporally for improvement; iii) The proposed framework was verified with great generalization ability and surpassed the state-of-the-art LSS-based methods by extensive experimental results."
https://arxiv.org/html/2411.06119v1,"Scalable, Tokenization-Free Diffusion Model Architectures with Efficient Initial Convolution and Fixed-Size Reusable Structures for On-Device Image Generation","Vision Transformers and U-Net architectures have been widely adopted in the implementation of Diffusion Models. However, each architecture presents specific challenges while realizing them on-device. Vision Transformers require positional embedding to maintain correspondence between the tokens processed by the transformer, although they offer the advantage of using fixed-size, reusable repetitive blocks following tokenization. The U-Net architecture lacks these attributes, as it utilizes variable-sized intermediate blocks for down-convolution and up-convolution in the noise estimation backbone for the diffusion process. To address these issues, we propose an architecture that utilizes a fixed-size, reusable transformer block as a core structure, making it more suitable for hardware implementation. Our architecture is characterized by low complexity, token-free design, absence of positional embeddings, uniformity, and scalability, making it highly suitable for deployment on mobile and resource-constrained devices. The proposed model exhibit competitive and consistent performance across both unconditional and conditional image generation tasks. The model achieved a state-of-the-art FID score of 1.6 on unconditional image generation with the CelebA.","Figure 1: Comparative neural network architectures, contrasting the Vision Transformer (ViT) on the left with the proposed hardware-friendly, low-complexity, scalable, tokenization-free neural network on the right, which features positional embeddings free transformer block-based model. Figure 2: Proposed hardware-friendly architecture of the text-conditional diffusion model in the image space or the latent space. The core structure (transformer block) produces an output with the same dimensions as the input. The input and output dimensions of the core structure are determined based on the initial convolution. For unconditional image generation, the output of the initial convolution is directly passed to the core structures for further processing. Text embeddings are derived from the CLIP [31] text encoder, while image embeddings are obtained using the Latent Diffusion Model [34] encoder. Diffusion Models [38, 11, 43] have gained popularity in recent generative model tasks due to their stable training compared to Generative Adversarial Networks [8, 44, 52, 9, 48] and their effective image sampling using robust strategies such as classifier-free guidance [12]. Consequently, Diffusion Models are widely employed in various image generation tasks [2, 49, 36], including text-to-image generation [33, 32, 5, 34, 26, 37], image super-resolution [7, 47], and layout-to-image generation [50, 3]. These models operate through a forward noising process and a backward denoising process, both implemented using Markov Chains [28]. The backward or reverse processes are typically realized using CNN-based architectures, such as U-Net [35], or Vision Transformer-based models [30, 1, 45]. U-Net-based models [11, 43, 34] have proven to be highly effective in implementing the diffusion mechanism. These models efficiently capture the spatial structure of images through a sequence of downsampling, mid-layer convolution, and upsampling stages. This makes the U-Net structure challenging to implement on devices, where varying block sizes are required to handle differently in terms of resource allocation on hardware. In contrast, Vision Transformers [15] exhibit scalability while maintaining consistent intermediate block sizes; however, they introduce the overhead of tokenization and additional positional embeddings to ensure coherence among the tokens derived from the input image, which results in increased model latency. Consequently, training and inference times would increase, making it difficult to deploy on real-time systems. This raises the question: Can we design an architecture with the same-sized repetitive blocks as transformer blocks and without the additional overhead of positional embeddings and tokens? This work introduces a hardware-friendly neural network architecture for image generation using a diffusion model. This architecture is distinguished by its low complexity, token-free design, uniformity, scalability, and hardware efficiency. The core component of our architecture is the initial convolution blocks, which serve as the basis for two different configurations. In Configuration I (stride \mathcal{S}=2), increasing the parameter count by expanding the embedding dimension or the number of reusable core structures (transformer blocks) results in a minimal increase in computational complexity while preserving high-quality image generation. In contrast, in Configuration II (stride \mathcal{S}=1), increasing the parameter count by similarly expanding the embedding dimension or the number of reusable core structures leads to higher computational complexity. However, the performance of Configuration II is superior when compared to Configuration I. Following the initial convolution, the entire image is processed through uniform-sized core structures, similar to those in Vision Transformers, which incorporate attention mechanisms and MLP layers. Additionally, the initial convolution block in our architecture effectively eliminates the overhead associated with tokenization and removes the need for positional embeddings, as required in Vision Transformers (as illustrated in Figure 1). These core structures can be scaled to achieve higher image quality, as measured by the FID [10] score. The initial convolution is crucial for extracting comprehensive spatial information from the image and combines the time embedding information with feature maps of the image, while the subsequent transformer blocks are responsible for extracting meaningful information from the features during the denoising phase of the diffusion process. (a) (b) (c) (d) (e) (f) Figure 3: Ablation study of design choices of the proposed architecture on CIFAR-10. (a) Comparison of FID vs GMAC with different embedding dimensions and number of blocks with configuration-I (stride \mathcal{S}=2). (b) Non-linearity in the initial convolution block. (c) Normalization in the initial convolution block. (d) Number of transformer blocks (12, 24 and 32). (e) Linear layer vs slicing operation in the Decoder block. (f) Conv2D vs ConvTranspose2D operation in the decoder block"
https://arxiv.org/html/2411.06106v2,Personalize to generalize: Towards a universal medical multi-modality generalization through personalization,"The differences among medical imaging modalities, driven by distinct underlying principles, pose significant challenges for generalization in multi-modal medical tasks. Beyond modality gaps, individual variations — such as differences in organ size and metabolic rate — further impede a model’s ability to generalize effectively across both modalities and diverse populations. Despite the importance of personalization, existing approaches to multi-modal generalization often neglect individual differences, focusing solely on common anatomical features. This limitation may result in weakened generalization in various medical tasks. In this paper, we unveil that personalization is critical for multi-modal generalization. Specifically, we propose an approach to achieve personalized generalization through approximating the underlying personalized invariant representation \mathbb{X}_{h} across various modalities by leveraging individual-level constraints and a learnable biological prior. We validate the feasibility and benefits of learning a personalized \mathbb{X}_{h}, showing that this representation is highly generalizable and transferable across various multi-modal medical tasks. Extensive experimental results consistently show that the additionally incorporated personalization significantly improves performance and generalization across diverse scenarios, confirming its effectiveness.","Figure 1: A diagram of medical modalities and individual differences. Individual variations may be significant and warrant further research attention from the medical intelligence community. Three-dimensional medical images, generated through specialized techniques and radiopharmaceuticals, excel at highlighting specific physiological features, collectively providing a comprehensive view of a patient’s structural and functional characteristics. However, this distinction between medical modalities creates significant generalization challenges in medical image analysis, especially when certain modalities may be inaccessible due to an individual’s financial constraints or physical limitations. As illustrated in Fig. 1, contemporary research in medical intelligence is mainly concentrated on structural modalities that depict physical anatomy, including Magnetic Resonance Imaging (MRI) [53] and Computed Tomography (CT) [33, 51] scans. Other studies [49] focus on the functional modalities associated with biochemistry, such as Positron Emission Tomography (PET) scans. For clarity, we categorize these modalities for generalization tasks into two types: homogeneous generalization, which pertains to generalizing within structural or functional modalities (e.g., T1, T2, T1ce, and Flair in MRI, as shown in Fig. 1); and heterogeneous generalization, which involves generalizing across both structural and functional modalities, such as CT and PET. An ideally well-generalized medical model across modalities should provide integrated insights derived from all modalities when only a subset is accessible for individuals, even for downstream transferring. Even though pre-training can significantly enhance downstream generalization, some recent approaches concentrate on learning common physical anatomy invariance at the class level [23], which may not be feasible for functional biochemistry-based models. Another line of research [40, 46, 23] primarily addresses the transferability of single-modal tasks and may not be suitable for multi-modal scenarios. In addition, most research on homogeneous generalization for medical tasks focuses on structural sequences of MRI or CT, employing strategies such as cross-modality transfer [30, 25, 51] or targeting challenges like missing modality segmentation [31, 6, 36, 37, 51]. Heterogeneous generalization presents a greater challenge due to the disparities between structural and functional information. Despite its significance, very few efforts [34] have addressed heterogeneous generalization, mainly focusing on one-directional modality transfer (e.g., PET to CT or MRI), and rarely exploring the model’s generalizability and transferability for other tasks in this context. Moreover, while medical modality differences have garnered attention from the research community, individual differences seem to have been overlooked by previous methods. As each person is fundamentally different from the average of the population [45], the differences between individuals across anatomy and metabolism are conspicuous (as shown in Fig. 1). Ignoring these individual differences may impede a model’s ability to generalize effectively across the broader population, which could be crucial for medical multi-modality generalization. This paper unveils that homogeneous and heterogeneous settings for multi-modality generalization can be tackled in the scope of personalization. To address this problem comprehensively, we formally introduce the concept of the personalized invariant representation for multi-modal generalization, denoted as \mathbb{X}_{h}, and its constraints as outlined in Hypothesis 3.1. Furthermore, personalized invariant \mathbb{X}_{h}, which learns aggregated biological information from all possible modalities specific to the individual, is likely to enhance performance across various medical tasks for that person. Building on this hypothesis, this paper proposes a general approach aimed at enhancing the generalization of various medical imaging tasks through personalization. Specifically, our method constructs an approximation of \mathbb{X}_{h} using the learnable biological prior knowledge \mathbb{O}, via decomposition, invariance, and equivariance constraints during pre-training (refer to Sec. 3.2). The learned approximation of \mathbb{X}_{h} can then be utilized to enhance performance in downstream generalization tasks, irrespective of whether a domain gap exists between the pre-training data and downstream data. Importantly, this paper demonstrates that obtaining a personalized invariant representation, \mathbb{X}_{h}, is feasible through our approach, and such invariance leads to generalization improvements across various medical tasks. To validate our methodology, we conduct experiments on modality transfer and missing modality segmentation tasks, addressing not only the homogeneous generalization of MRI but also the rarely explored heterogeneous generalization, such as generalization between PET and CT. Our findings reveal that our approach successfully captures comprehensive, personalized information even when only partial modalities are available for a given individual (see Fig. 3). Moreover, extensive experiments on both homogeneous (Sec. 4) and heterogeneous (Secs. 5 and 6) generalization demonstrate that our approach can be adapted for downstream tasks and surpasses current state-of-the-art (SOTA) methods in multiple tasks, validating its superiority. We will publicly release our code, checkpoints, and data splits upon acceptance."
https://arxiv.org/html/2411.06098v2,LT-DARTS: An Architectural Approach to Enhance Deep Long-Tailed Learning,"Deep long-tailed recognition has been widely studied to address the issue of imbalanced data distributions in real-world scenarios. However, there has been insufficient focus on the design of neural architectures, despite empirical evidence suggesting that architecture can significantly impact performance. In this paper, we attempt to mitigate long-tailed issues through architectural improvements. To simplify the design process, we utilize Differential Architecture Search (DARTS) to achieve this goal. Unfortunately, existing DARTS methods struggle to perform well in long-tailed scenarios. To tackle this challenge, we introduce Long-Tailed Differential Architecture Search (LT-DARTS). Specifically, we conduct extensive experiments to explore architectural components that demonstrate better performance on long-tailed data and propose a new search space based on our observations. This ensures that the architecture obtained through our search process incorporates superior components. Additionally, we propose replacing the learnable linear classifier with an Equiangular Tight Frame (ETF) classifier to further enhance our method. This classifier effectively alleviates the biased search process and prevents performance collapse. Extensive experimental evaluations demonstrate that our approach consistently improves upon existing methods from an orthogonal perspective and achieves state-of-the-art results with simple enhancements.","Deep neural networks have demonstrated remarkable performance in image recognition tasks, yet they are predominantly utilized in settings where the data adhere to a balanced distribution. In reality, data distributions in many practical scenarios are long-tailed (Buda, Maki, and Mazurowski 2017), characterized by a few head classes with abundant samples and many tail classes with limited samples. It has been proved that conventional deep learning methods tend to exhibit sub-optimal performance in such a context. To address this challenge, the community has proposed numerous approaches including data augmentation (Li et al. 2021; Hong et al. 2022; Ahn, Ko, and Yun 2023), class re-sampling strategies (Liu et al. 2021; Bai et al. 2023), decoupling learning (Kang et al. 2019; Zhou et al. 2023) and so on. Nevertheless, most existing methods pay insufficient attention to neural architecture design, which is crucial for deep models to achieve optimal performance. Figure 1: The performance of different architectures on the long-tailed CIFAR-10-LT dataset. The direction of black dashed lines indicates better architectures, as they achieve higher accuracy with fewer architectural parameters. It is widely recognized that meticulous architectural design contributes to improved performance on balanced data distribution, and this consensus unsurprisingly extends to long-tailed data distribution. As depicted in Fig. 1, we conduct extensive training on several architectures and report their parameter sizes and performance. It can be observed that different backbones lead to various performances and an optimal architecture may achieve superior performance even with fewer parameters, which validates the feasibility of approaching the long-tailed problem from an architectural perspective. In terms of efficiently designing an architecture suited for long-tailed data, we leverage the power of differentiable architecture search (DARTS). (a) DARTS with CE (vanilla) (b) DARTS with LDAM (c) \beta-DARTS with CE (d) \beta-DARTS with LDAM Figure 2: Running DARTS methods on CIFAR-10-LT, where the blue line denotes the process of proxy search, and the red line denotes the optimal architecture trained from scratch. Two baselines are depicted, where the green line corresponds to ResNet and the yellow line represents ResNeXt. (a)(c) Network parameters are updated based on the Cross-Entropy (CE) loss (i.e., \mathcal{L}_{train} = CE). (b)(d) \mathcal{L}_{train} = LDAM, which is a re-balancing loss function designed for long-tailed learning. DARTS (Liu, Simonyan, and Yang 2018) is an efficient neural architecture search method with minimal computational resource requirements. On balanced datasets, the architectures it discovers perform on par with those designed by experts. Unfortunately, as shown in Fig 2, directly applying DARTS on the long-tailed dataset fails to achieve satisfactory results. Even the latest improved version \beta-DARTS (Ye et al. 2022) underperforms compared to expert-designed networks, which deviation contrasts with the performance on balanced data distribution. Moreover, it is noteworthy that the integration of the re-balancing loss LDAM (Cao et al. 2019) only yields a marginal improvement, suggesting that the naive combination of DARTS and re-balancing strategy also fails to achieve satisfactory results. To address this issue, we propose Long-Tailed Differential ARchiTecture Search (LT-DARTS), a framework tailored for the architecture search tasks on long-tailed data. Specifically, we systematically conduct extensive experiments to evaluate the performance of various architectural components under long-tailed data, including topology, activation functions and their placements, convolution designs, and normalization methods. Drawing from the observations, we propose two novel convolution operations, thereby establishing a novel search space. Moreover, we further enhance our method from the search strategy. We introduce the Equiangular Tight Frame (ETF) classifier, designed to mitigate weight shifts in classifiers caused by long-tailed data, including both weight norm and weight angle shifts. Furthermore, we find that the ETF classifier also alleviates performance collapse, which we validate through theoretical analysis. Extensive experimental results indicate the seamless integration of our method with existing approaches consistently enhancing the performance. Through only simple enhancements, our method achieves state-of-the-art results. Our contributions can be concluded as the following: • We examine the long-tailed issue from an architectural perspective, which is paid little attention to. With this foundation, we methodically investigate architectural components that demonstrate enhanced performance in long-tailed scenarios, leading to insightful observations. Leveraging these insights, we propose two innovative convolution operations and establish a fresh search space. • To further strengthen our approach from the search strategy, we incorporate the ETF classifier. Specifically, we observe a bias in the learnable classifier weights during the search process, which the ETF classifier inherently resolves. Additionally, we show that the ETF classifier also helps prevent performance collapse. • Extensive experiments show that the architectures we discover outperform crafted designed architectures. And it can seamlessly integrate with existing solutions tailored to long-tailed issues, further enhancing model performance. Our research provides a complementary perspective for the long-tailed community."
https://arxiv.org/html/2411.06091v1,Pattern Integration and Enhancement Vision Transformer for Self-supervised Learning in Remote Sensing,"Recent self-supervised learning (SSL) methods have demonstrated impressive results in learning visual representations from unlabeled remote sensing images. However, most remote sensing images predominantly consist of scenographic scenes containing multiple ground objects without explicit foreground targets, which limits the performance of existing SSL methods that focus on foreground targets. This raises the question: Is there a method that can automatically aggregate similar objects within scenographic remote sensing images, thereby enabling models to differentiate knowledge embedded in various geospatial patterns for improved feature representation? In this work, we present the Pattern Integration and Enhancement Vision Transformer (PIEViT), a novel self-supervised learning framework designed specifically for remote sensing imagery. PIEViT utilizes a teacher-student architecture to address both image-level and patch-level tasks. It employs a proposed, Geospatial Pattern Cohesion (GPC) module to explore the natural clustering of patches, enhancing the differentiation of individual features. A Feature Integration Projection (FIP) module is employed to further refine masked token reconstruction using geospatially clustered patches. We validated PIEViT across multiple downstream tasks, including object detection, semantic segmentation, and change detection. Experiments demonstrated that PIEViT enhances the representation of internal patch features, providing significant improvements over existing self-supervised baselines. It achieves excellent results in object detection, land cover classification, and change detection, underscoring its robustness, generalization, and transferability for remote sensing image interpretation tasks.","Remote sensing technology, employing aerial or satellite platforms, facilitates the acquisition of earth observation data from afar, marking its utility across diverse sectors such as agriculture monitoring, environmental surveillance evaluation, urban planning, and disaster management. At the heart of these applications lies the interpretation of remote sensing imagery, a task that has been considerably challenging [1]. The advent of deep learning technology [2] has significantly enhanced the precision and automation of remote sensing image analysis, encompassing object detection, land cover classification, and change detection. Nonetheless, the intrinsic complexity of remote sensing scenarios—attributable to variations in sensor technology, atmospheric conditions, and image resolution—results in the performance disparity of analytical models across different contexts. The prohibitive costs associated with manual annotation further exacerbate this issue, rendering the re-labeling of samples for specific scenarios impractical. Consequently, there is a growing demand for models endowed with superior generalization capabilities, capable of adeptly navigating the multifaceted landscape of remote sensing applications. In Self-Supervised Learning (SSL), algorithms learn useful features by generating labels or targets from the input data itself, facilitating an understanding of the data. A key advantage of this approach is its independence from manually annotated data, enabling more efficient utilization of large volumes of unlabeled data. In the realm of remote sensing, employing SSL for pre-training on a wide array of scene images enhances feature representation capabilities and generalizability across different scenarios. Through transfer learning, outstanding results can be achieved across various imagery-based, downstream tasks. Presently, contrastive learning [3, 4] and masked learning [5, 6] stand as two widely employed SSL methods in both computer vision and remote sensing fields. In contrastive learning, models are trained to distinguish between a candidate sample and positive (similar) and negative (dissimilar) samples; in masked learning, the objective is to reconstruct obscured portions based on contextual information. Contrastive learning focuses on learning representations that differentiate between samples, while masked learning emphasizes the reconstruction or prediction of missing data based on partial information. Both methods have demonstrated significant potential in remote sensing applications. However, unlike natural images, remote sensing images are predominantly scenographic, often featuring multiple ground objects without a clear foreground target. This characteristic imposes certain limitations on the methods above. For instance, in contrastive learning, random cropping to generate positive sample pairs may result in two images with completely dissimilar features due to the scenographic nature of remote sensing images, potentially leading to semantic confusion in the model. Pixel-level masked learning, on the other hand, entirely disregards semantic information. Seeking a method that discerns both the semantic information of different scene images and the semantic details within each area of the images is essential for effectively processing scenographic remote sensing imagery. It has been observed that scenographic images in remote sensing typically exhibit specific distribution patterns, where natural elements (such as mountains, bodies of water, forests, lakes, and grasslands) and man-made elements (such as buildings, farmlands, plowed lands, and gardens) often display pronounced clustering. This tendency for feature aggregation leads to the formation of discernible geographic pattern clusters among individual image patches and their neighbors, thereby establishing intricate spatial relationships across the varying patches and their adjacent areas. Building on these observations, we introduce a novel SSL feature learning algorithm named the Pattern Integration and Enhancement Vision Transformer. PIEViT consists of a student network and a teacher network, simultaneously addressing image-level and patch-level tasks. For the image-level task, we compute the cross-entropy loss between the class tokens extracted by the student and teacher networks from randomly cropped versions of the same image. For the patch-level task, we randomly mask some patch input to the student network and then calculate the cross-entropy loss between each masked patch token and the corresponding patch token in the teacher network. By integrating image-level loss with patch-level loss and backpropagating through the student network’s parameters, the teacher network is updated using the Exponential Moving Average (EMA) strategy [7]. To better differentiate the features of each patch within images, we propose the Geospatial Pattern Cohesion (GPC) module to explore and leverage the natural clustering of landscape elements observed in remote sensing imagery, substantially augmenting the model’s proficiency in decoding intricate spatial distributions. Furthermore, we present the Feature Integration Projection (FIP) module, which utilizes the Geospatial Pattern Cohesion scores generated by the teacher network to guide and refine the reconstruction of masked tokens in the student network. This approach not only increases the complexity of the learning task but also bolsters the learning of nuanced differences in internal features across the images. Overall, the primary contributions of this work can be summarized as follows: • We introduce the Pattern Integration and Enhancement Vision Transformer (PIEViT), which supervises the generation of masked patches in the student branch with features aggregated from geospatially clustered patches in the teacher branch. PIEViT enhances the distinctiveness of internal area features within images, achieving superior performance in downstream tasks. • Unlike the simple Multilayer Perceptron (MLP)-based projection head, our method computes the score from the Geospatial Pattern Cohesion (GPC) module between each Query Patch and its neighboring patches, selects the top k patches based on their scores, and then processes these patches through a Feature Integration Projection (FIP) module. Compared to MLP, FIP is more adept at capturing a broader range of patch-level semantic information. • After pretraining on the unlabeled dataset, we validated our approach across multiple downstream tasks. PIEViT possesses a strong capability to represent rich semantic and local information, achieving SOTA results in object detection, land cover classification, and change detection, in comparison with classical models with similar size. This demonstrates its generalization and transferability for remote sensing image interpretation tasks."
https://arxiv.org/html/2411.06071v1,GlocalCLIP: Object-agnostic Global-Local Prompt Learning for Zero-shot Anomaly Detection,"Zero-shot anomaly detection (ZSAD) is crucial for detecting abnormal patterns in target datasets without using training samples, specifically in scenarios where there are distributional differences between the target domain and training data or where data scarcity arises because of restricted access. Although recently pretrained vision-language models demonstrate strong zero-shot performance across various visual tasks, they focus on learning class semantics, which makes their direct application to ZSAD challenging. To address this scenario, we propose GlocalCLIP, which uniquely separates global and local prompts and jointly optimizes them. This approach enables the object-agnostic glocal semantic prompt design to effectively capture general normal and anomalous patterns without dependency on specific objects in the image. We refine the text prompts for more precise adjustments by utilizing deep-text prompt tuning in the text encoder. In the vision encoder, we apply V-V attention layers to capture detailed local image features. Finally, we introduce glocal contrastive learning to improve the complementary learning of global and local prompts, effectively detecting abnormal patterns across various domains. The generalization performance of GlocalCLIP in ZSAD was demonstrated on 15 real-world datasets from both the industrial and medical domains, achieving superior performance compared to existing methods.","Anomaly detection involves identifying abnormal data that deviate from normal data patterns. It has become a crucial technology in various industries, such as manufacturing and healthcare (Bergmann et al., 2019; 2020; Roth et al., 2022; Xie et al., 2023; Liu et al., 2023a). Anomaly detection in industrial visual inspection and medical image diagnosis is directly related to product quality control and patient safety. Therefore, rapid and accurate anomaly detection can prevent substantial economic losses caused by quality degradation and medical accidents resulting from diagnostic errors. Traditional anomaly-detection models operate through one-class classification that involves learning normal patterns from a single class (Sohn et al., 2020; Zavrtanik et al., 2021; McIntosh & Albu, 2023; Liu et al., 2023c). This approach effectively focuses on learning normal data within a single class; however, its industrial application is severely limited due to the following challenges: (1) A separate model needs to be trained for each class, which is both time-consuming and costly. Additionally, the model requires retraining when a new class is introduced, leading to inefficiency. (2) There may be distributional differences between the training data and the actual test environment data. The discrepancy between the previously learned normal patterns and the target data can degrade the generalization performance of the model, particularly when the target domain has little relevance to the training data. (3) In cases where data access is restricted due to confidentiality, gathering sufficient training data may be difficult, potentially resulting in overfitting or underfitting because of the inability to fully learn normal patterns (Liu et al., 2023b). Recent research has focused on zero-shot anomaly detection (ZSAD) to address these issues. ZSAD enables the detection of abnormal patterns across various classes and domains without relying on training data from the target domain. ZSAD has been effectively applied in various fields owing to the emergence of large-scale pre-trained models, such as vision-language models (VLMs). Among existing VLMs, Contrastive Language-Image Pre-training (CLIP) simultaneously learns images and text, demonstrating strong zero-shot performance in diverse areas, including industrial visual inspection, medical image analysis, video understanding, and robotic vision (Radford et al., 2021; Yao et al., 2021; Tschannen et al., 2023; Geng et al., 2023; Guo et al., 2023; Zhao et al., 2023; Ni et al., 2022; Sontakke et al., 2024). However, the CLIP relies heavily on global information from images, reducing its applicability to ZSAD (Jeong et al., 2023; Chen et al., 2023). Jeong et al. (2023) proposed a method to detect anomalies through multiscale patches using window patches and introduced a compositional prompt ensemble approach to address the challenges in finding optimal prompts. Furthermore, Zhou et al. (2023) proposed an object-agnostic prompt that simplifies prompt design by reducing dependency on class semantics and suggested extracting local features in CLIP through a diagonally prominent attention map layer. Cao et al. (2024) introduced hybrid prompts for ZSAD by incorporating both static and dynamic learnable prompts into CLIP, optimizing them through auxiliary anomaly detection data. However, these studies did not consider incorporating global and local information at the prompt level or training them in a complementary manner. Therefore, a ZSAD method that separates global and local prompts and learns them complementarily is needed to improve performance. In this study, we propose GlocalCLIP, a novel approach that addresses the limitations of existing methods by distinctly separating and complementarily learning global and local prompts. GlocalCLIP builds on the pre-trained CLIP model and introduces a prompt structure that enables the complementary learning of global and local visual features. Specifically, we design an object-agnostic glocal semantic prompt that applies to both normal and anomalous cases, enabling contextual anomaly detection while explicitly separating global and local prompts. In the text encoder, we utilize deep-text prompt tuning by inserting learnable tokens for fine-grained text prompts. In the vision encoder, we modify the traditional query-key-value (QKV) attention into value-value (V-V) attention, enabling more precise learning of fine-grained features from local regions (Vaswani, 2017; Zhou et al., 2023; Li et al., 2024). Finally, we propose a global contrastive learning to address the insufficient complementarity between independently learned global and local prompts and to jointly optimize their integration. Through experiments on 15 real-world image datasets, GlocalCLIP demonstrates enhanced anomaly detection performance and strong generalization, even in the presence of discrepancies between the training data and the target domain. The contributions of this paper are summarized as follows. • We introduce a novel ZSAD approach named GlocalCLIP, the first framework to explicitly separate global and local prompts through an object-agnostic glocal semantic prompt design. This design enables the learning of prompts that generalize across a wide range of normal and anomalous patterns without being tied to specific object classes, allowing the model to effectively detect fine-grained visual anomalies. • We address the insufficient complementarity between global and local prompts by introducing a glocal contrastive learning approach. Through joint optimization of global and local prompts, this approach effectively aligns them to capture both global and local visual features, thereby enhancing the robustness of ZSAD. • Comprehensive experiments validate the effectiveness and generalization capability of GlocalCLIP on 15 real-world datasets, covering a diverse range of classes from both industrial and medical domains, and demonstrate its strong performance and ability to generalize across various categories."
https://arxiv.org/html/2411.06067v1,AI-Driven Stylization of 3D Environments,"In this our system, we discuss methods to stylize a scene of 3D primitive objects into a higher fidelity 3D scene using novel 3D representations like NeRFs and 3D Gaussian Splatting. Our approach leverages existing image stylization systems and image-to-3D generative models to create a pipeline that iterativly stylizes and composites 3D objects into scenes. We show our results on adding generated objects into a scene and discuss limitations.","The rapid evolution of 3D scene generation technologies has substantially enhanced the feasibility and quality of generating and manipulating three-dimensional environments, particularly through the advent of Neural Radiance Fields (NeRFs) and other generative models. These advancements have opened new possibilities for virtual reality (VR), augmented reality (AR), and other applications requiring rich, immersive, and dynamically editable 3D content. However, despite significant progress, the challenge remains in the accessibility and ease of use of these technologies for users without extensive technical expertise in 3D design. Recognizing this gap, we introduce a pipeline designed to empower individuals with little to no background in 3D design to furnish and restyle their living spaces effectively. Our approach leverages a user-friendly interface where users can draw basic primitives within a scanned room and input stylistic preferences through simple text prompts. The system then automates the conversion of these inputs into a fully furnished and stylized 3D room model, viewable in real-time. The major contribution of our work lies in integrating systems such as InstructPix2Pix for image stylization and SIGNeRF for seamless object integration within NeRFs, tailored to simplify the design process. This pipeline not only democratizes 3D interior design but also enhances the accessibility and usability of sophisticated 3D modeling tools. By leveraging intuitive user interaction and automation, our method opens up new avenues for personal creativity and practical application in home design, offering significant benefits to users unfamiliar with traditional 3D modeling tools. This paper outlines our methodology, discusses the implementation of our pipeline, and presents results that validate the effectiveness of our approach in creating immersive and aesthetically pleasing 3D environments tailored to user specifications. Our contribution marks a step forward in making advanced 3D scene generation accessible to a broader audience."
https://arxiv.org/html/2411.06048v1,"An Empirical Analysis on Spatial Reasoning Capabilities of 
Large Multimodal Models","Large Multimodal Models (LMMs) have achieved strong performance across a range of vision and language tasks. However, their spatial reasoning capabilities are under-investigated. In this paper, we construct a novel VQA dataset, Spatial-MM, to comprehensively study LMMs’ spatial understanding and reasoning capabilities. Our analyses on object-relationship and multi-hop reasoning reveal several important findings. Firstly, bounding boxes and scene graphs, even synthetic ones, can significantly enhance LMMs’ spatial reasoning. Secondly, LMMs struggle more with questions posed from the human perspective than the camera perspective about the image. Thirdly, chain of thought (CoT) prompting does not improve model performance on complex multi-hop questions involving spatial relations. Lastly, our perturbation analysis on GQA-spatial reveals that LMMs are much stronger at basic object detection than complex spatial reasoning. We believe our benchmark dataset and in-depth analyses can spark further research on LMMs spatial reasoning. 111Spatial-MM benchmark is available at: https://github.com/FatemehShiri/Spatial-MM","Large Multimodal Models (LMMs) have shown impressive generalization ability on several vision and language tasks. Several recent works, however, showed that these models lack spatial understanding Tong et al. (2024); Li et al. (2023a); Liu et al. (2023e); Lei et al. (2024a); Prasad et al. (2023). As can be seen in Figure 1, multimodal LLMs, including GPT-4o, often fail to answer questions from a human perspective within an image. The focus of this work is to study the understanding of spatial relations by top-performing LMMs. Moreover, we go beyond evaluating only the final answers to directly analyzing the intermediate reasoning steps generated by chain of thought (CoT) prompting in multi-hop visual question-answering (VQA) tasks. We ground LMMs ’ reasoning steps into a scene graph format and verify whether they form a valid path. More specifically, we ask the following questions: (i) What spatial relations are missed by models, and why it happen? (ii) How can additional symbolic visual information, such as bounding boxes or scene graphs, improve the performance of LMMs? Which of these symbolic information is more useful, and how can they be integrated in the reasoning process effectively? (iii) How does the questions complexity affect LMMs in handling spatial relations? (iv) How does the reasoning path of LMMs behave when they fail to answer a multi-hop question? Is the failure due to incorrect spatial reasoning or non-spatial reasoning? Figure 1: benchmarking the spatial reasoning capabilities of GPT-4o Achiam et al. (2023) (Date accessed: June 12, 2024). Text in red and green signifies an incorrect and ground-truth answers, respectively. The accuracy of GPT-4o in answering questions related to the human’s viewpoint in the image is only 27.5%. To address these questions, we construct Spatial-MM, a novel, challenging dataset, and comprehensively LMMs spatial reasoning capabilities from different angles. We analyze four top-performing LMMs on Spatial-MM and GQA-spatial Kamath et al. (2023) benchmarks to identify problems with visual question answering (VQA) evaluation methodology. Our comprehensive analyses reveal a number of important insights that point to future research directions. Our contributions can be summarized as follows. \bullet We present a new, challenging spatial-aware benchmark that incorporates a variety of spatial relationship types, accounting for both human and camera perspectives. \bullet Our coprehensive empirical analyses show that: (i) bounding boxes and scene graphs, even synthetic ones, can significantly enhance LMMs’ spatial reasoning, (ii) LMMs struggle more with questions posed from the human perspective than the camera perspective about the image, (iii) chain of thought (CoT) prompting does not improve model performance on complex multi-hop questions involving spatial relations, and (iv) LMMs are much stronger at basic object detection than complex spatial reasoning."
https://arxiv.org/html/2411.06041v1,PointCG: Self-supervised Point Cloud Learning via Joint Completion and Generation,"The core of self-supervised point cloud learning lies in setting up appropriate pretext tasks, to construct a pre-training framework that enables the encoder to perceive 3D objects effectively. In this paper, we integrate two prevalent methods, masked point modeling (MPM) and 3D-to-2D generation, as pretext tasks within a pre-training framework. We leverage the spatial awareness and precise supervision offered by these two methods to address their respective limitations: ambiguous supervision signals and insensitivity to geometric information. Specifically, the proposed framework, abbreviated as PointCG, consists of a Hidden Point Completion (HPC) module and an Arbitrary-view Image Generation (AIG) module. We first capture visible points from arbitrary views as inputs by removing hidden points. Then, HPC extracts representations of the inputs with an encoder and completes the entire shape with a decoder, while AIG is used to generate rendered images based on the visible points’ representations. Extensive experiments demonstrate the superiority of the proposed method over the baselines in various downstream tasks. Our code will be made available upon acceptance.","Self-supervised representation learning (SSRL) aims to fully exploit the statistical and structural knowledge inherent in unlabeled datasets, enabling the encoder of the pre-training model to extract informative and discriminative representations. The pre-trained encoder can be subsequently applied to various downstream tasks such as classification, segmentation, and object detection [1, 2]. The core of SSRL lies in the design of appropriate pretext tasks aimed at aiding the encoder in achieving a full perception and understanding of the inputs. Figure 1: Qualitative and quantitative comparison of models using different pretext tasks. Chamfer Distance (CD) and Structural Similarity Index (SSIM) are employed as the quantitative metrics. For the masked point modeling (MPM) task, we utilize the method proposed in Point-MAE [3] with the inputs of visible points from arbitrary views (see Sec. 3.1). For the 3D-to-2D generation task, we define the pretext task as generating images from arbitrary views. The result of the model using only MPM exhibits group clustering at the edges, while our method yields sharpened and clear edges that closely align with the ground truth. The model relying solely on 3D-to-2D generation fails to capture three-dimensional structural information, while our method can effectively preserve the geometric structure. Directly combining both tasks generates point clouds and images superior to using only MPM or 3D-to-2D generation (Direct Combination) but with lower Linear-SVM accuracy. Based on the tasks employed, existing self-supervised pre-training methods can be broadly classified into two paradigms: contrastive learning and generative learning, both of which have attained great success in processing 2D images [4, 5, 6] and 3D point clouds [7, 3, 8, 9, 10, 11]. Compared to contrastive learning, generative learning is considered as a more data-efficient pre-training method, capable of capturing the patterns of the inputs with relatively limited data volume [12]. Therefore, it is highly favored in the context of data scarcity within the field of 3D vision, where masked point modeling [7, 3, 8, 13, 10] and 3D-to-2D generation [11, 14] stand out as two representative generative learning methods. Among them, masked point modeling drives the model to predict arbitrary missing parts based on the remaining points. Accomplishing this task requires a thorough understanding of the spatial properties and global-local context of point clouds. 3D-to-2D generation employs a cross-modal pretext task which translates a 3D object point cloud to its diverse forms of 2D rendered images (e.g., silhouette, depth, contour). Pre-training with pixel-wise precise supervision drives the backbone to perceive the fine-grained edge details of 3D objects. However, both of the above methods have their own limitations. As revealed in [15, 16, 17], due to the irregularity of point clouds, commonly used point set similarity metrics (e.g., Chamfer Distance and Earth Mover’s Distance) in masked point modeling cannot provide explicit point-to-point supervision between ground truth and generated point clouds. The lack of precise correspondence results in limited feature representation capability of the pre-trained backbone network. Conversely, 3D-to-2D generation [11, 14] alleviates the issue of insufficient supervision signals by utilizing regular 2D images as the generation objective, offering pixel-wise precise supervision. However, relying solely on images from limited views as ground truth may overlook the structural information from occluded point sets, diminishing the backbone’s perception of the spatial properties of point clouds. As shown in Fig. 1, masked point modeling exhibits subpar performance in reconstructing some challenging areas (e.g., edges) due to the lack of point-to-point supervision. Besides, 3D-to-2D generation yields images lacking three-dimensional structural information, attributed to the lack of explicit geometric guidance. These observations collectively indicate the models’ inadequate perception of the inputs, consequently reducing their performance on downstream tasks. Based on the aforementioned analysis, an intuitive method is to combine these two pretext tasks to retain their individual merits while compensating for their respective limitations. However, as shown in Fig. 1, while the model directly combining both tasks outperforms those relying solely on MPM or 3D-to-2D generation in generating high-quality point clouds or images, its Linear-SVM accuracy is lower (88.41\% vs 90.72\% and 91.13\%). We argue that the encoder’s involvement in both tasks can lead to confusion when generating content for two modalities concurrently. Furthermore, to accomplish both tasks, the model shifts its training focus toward the decoder, which is typically discarded after pre-training. This phenomenon diminishes the feature extraction capability of the encoder, ultimately reducing the Linear SVM accuracy. Figure 2: Visualization of the unmasked points (a), the masked points (b), the completed point cloud composed of green unmasked points and gray masked points (c), and the completed point cloud in blue with overlapping points highlighted in red (d). To address these issues, we propose PointCG, a framework that effectively integrates masked point modeling and 3D-to-2D generation tasks. This framework incorporates a Hidden Point Completion (HPC) module and an Arbitrary-view Image Generation (AIG) module. Existing MAE-based MPM methods often employ a random masking strategy based on Farthest Point Sampling (FPS) and K-Nearest Neighbor (KNN) techniques. However, the inputs of unmasked patches (Fig. 2 (a)) preserve the overall shape of an object and exhibit substantial overlap with the target points (highlighted in red in Fig. 2 (d)). The leakage of overall structure and point location information enables the model to reconstruct the object without a holistic comprehension of the entire structure, which limits the learning capacity of the encoder during pre-training. To overcome this limitation, we select the visible points from arbitrary views by removing hidden points as input and introduce the HPC module to complete the point clouds. For the 3D-to-2D generation task, we employ the arbitrary-view image generation as the pretext task, which generates the image from an arbitrary view based on the representations of visible points extracted by the encoder. Furthermore, the cross-modal feature alignment is introduced to align the feature spaces of point clouds and images, which enables simultaneous content generation across both modalities and refocuses the training on the encoder. Specifically, we extract features from both the input point clouds and their corresponding rendered 2D images, encouraging feature proximity for the same instance while maintaining feature separation for different instances. Through the effective integration of HPC and AIG, the pre-trained encoder achieves a comprehensive understanding of 3D objects and can extract high-quality 3D representations. We evaluate our model and the proposed modules with a variety of downstream tasks and ablation studies. We further demonstrate that informative representations can be effectively learned from the restricted points, and such representations facilitate effortless masked point modeling and arbitrary-view image generation."
https://arxiv.org/html/2411.06023v1,Dynamic Textual Prompt For Rehearsal-free Lifelong Person Re-identification,"Lifelong person re-identification attempts to recognize people across cameras and integrate new knowledge from continuous data streams. Key challenges involve addressing catastrophic forgetting caused by parameter updating and domain shift, and maintaining performance in seen and unseen domains. Many previous works rely on data memories to retain prior samples. However, the amount of retained data increases linearly with the number of training domains, leading to continually increasing memory consumption. Additionally, these methods may suffer significant performance degradation when data preservation is prohibited due to privacy concerns. To address these limitations, we propose using textual descriptions as guidance to encourage the ReID model to learn cross-domain invariant features without retaining samples. The key insight is that natural language can describe pedestrian instances with an invariant style, suggesting a shared textual space for any pedestrian images. By leveraging this shared textual space as an anchor, we can prompt the ReID model to embed images from various domains into a unified semantic space, thereby alleviating catastrophic forgetting caused by domain shifts. To achieve this, we introduce a task-driven dynamic textual prompt framework in this paper. This model features a dynamic prompt fusion module, which adaptively constructs and fuses two different textual prompts as anchors. This effectively guides the ReID model to embed images into a unified semantic space. Additionally, we design a text-visual feature alignment module to learn a more precise mapping between fine-grained visual and textual features. We also developed a learnable knowledge distillation module that allows our model to dynamically balance retaining existing knowledge with acquiring new knowledge. Extensive experiments demonstrate that our method remarkably outperforms SOTAs under various settings. Our model even outperforms rehearsal-based state-of-the-art approaches by 15.9\%/14.8\% over mAP/rank1 across several seen datasets and 17.4\%/17.9\% over mAP/rank1 on multiple unseen datasets over four training orders.","Person re-identification (ReID) endeavors to identify individuals across various cameras and viewpoints. Recently, this task has attracted more and more attention due to its potential applications in fields like social security, attributed to extensive datasets and continuous technique innovation. Nevertheless, practical ReID models must continually adapt to new datasets in real-world scenarios, necessitating a continuous learning approach to accommodate diverse data distributions. Therefore, researchers have proposed the Lifelong Person Re-identification (LReID) task. The primary focus here is to mitigate catastrophic forgetting, where the model risks losing previously learned knowledge from seen domains when exposed to new datasets. LReID assumes that only new datasets at the current moment are visible, while the previously trained seen datasets are no longer accessible for training. Figure 1: The inspirations of our rehearsal-free method. Natural language descriptions of pedestrian images from various domains act as domain-independent anchors. These consistent descriptions guide the mapping of images into a unified semantic space, effectively mitigating the issue of catastrophic forgetting. The challenges associated with the LReID [1, 2] can be summarized as follows: 1) The model tends to adapt to the domain distribution of new incoming data, leading to forgetting the knowledge acquired from the previous datasets, i.e., catastrophic forgetting problem. 2) Existing traditional lifelong learning tasks mainly focus on classification and emphasize inter-class differences. However, LReID requires capturing fine-grained discriminative features due to the significant intra-class similarities. 3) In practical scenarios, not every pedestrian has been observed previously. Therefore, the LReID model needs to maintain high performance across both seen and unseen domains. Previous LReID methods predominantly rely on rehearsal strategy [3, 4, 5]. However, the retained data of these methods is continually increased with the number of seen training domains. Besides, these methods also face the risk of data privacy. Other methods without rehearsal usually constrain its ability to acquire new knowledge to balance the model’s stability and plasticity. Thus, the performance of these methods often exhibits suboptimal when confronted with new datasets and is typically inferior to the strategy of data rehearsal. Different from existing works, in this paper, we propose using textual description as guidance to encourage the ReID model to learn domain-invariant features. The key insight is to construct a shared textual space by describing pedestrian images in an invariant style. This shared space serves as an anchor to guide the ReID model in mapping image features into a unified semantic space. This could prevent the ReID model from overfitting to specific data domains and alleviate the risk of catastrophic forgetting. Moreover, mapping all pedestrians into this unified semantic space helps the ReID model mitigate the effects of domain shift, thereby enhancing its generalization capability. To achieve our objective, we introduce a Dynamic Textual Prompt (DTP) framework, consisting of three key components: the Dynamic Prompt Fusion (DPF) module, the Text-Visual Feature Alignment (TFA) module, and the Learnable Knowledge Distillation (LKD) module. The DPF module constructs textual prompts to guide the ReID model in embedding pedestrian images into a unified semantic space. We generate textual prompts by utilizing a pre-trained captioning module to create pedestrian descriptions and distill existing knowledge from CLIP [6] using learnable vectors. These prompts are dynamically integrated using a fusion module. Besides, in the TFA module, we align the visual features of each local pedestrian region with the textual features of corresponding local phrases sequentially. This fine-grained alignment between visual features and textual prompts ensures effective guidance for embedding all pedestrians into a unified semantic space. Moreover, the LKD module plays a crucial role by producing adaptive trade-off parameters for learning without forgetting (lwf) loss. This helps strike a balance between learning from new incoming data and preventing catastrophic forgetting. The experimental results clearly demonstrate that our DTP method achieves superior performance, even outperforming data rehearsal methods by a significant margin. Specifically, our DTP surpasses state-of-the-art competitors by 15.9\%/14.8\% over mAP/rank1 on seen datasets and 17.4\%/17.9\% over mAP/rank1 on unseen datasets across four training orders. The main contributions are summarized as, \bullet We introduce a novel dynamic textual prompt framework for the LReID task, effectively mitigating catastrophic forgetting by mapping image features with varying distributions into a unified semantic space. \bullet In our DTP model, we propose a dynamic prompt fusion module to generate textual prompts adaptively. Additionally, a text-visual feature alignment module is introduced to align image features and textual prompts for better fine-grained feature representation. \bullet Extensive experiments demonstrate the effectiveness of our method, outperforming the state-of-the-art methods on four protocols."
https://arxiv.org/html/2411.06019v1,": An “Optimizing-Sparsifying"" Simplification Framework for Compact and High-Quality 3D Gaussian Splatting","3D Gaussian Splatting (3DGS) has emerged as a mainstream for novel view synthesis, leveraging continuous aggregations of Gaussian functions to model scene geometry. However, 3DGS suffers from substantial memory requirements to store the multitude of Gaussians, hindering its practicality. To address this challenge, we introduce GaussianSpa, an optimization-based simplification framework for compact and high-quality 3DGS. Specifically, we formulate the simplification as an optimization problem associated with the 3DGS training. Correspondingly, we propose an efficient “optimizing-sparsifying” solution that alternately solves two independent sub-problems, gradually imposing strong sparsity onto the Gaussians in the training process. Our comprehensive evaluations on various datasets show the superiority of GaussianSpa over existing state-of-the-art approaches. Notably, GaussianSpa achieves an average PSNR improvement of 0.9 dB on the real-world Deep Blending dataset with 10\times fewer Gaussians compared to the vanilla 3DGS. Our project page is available at https://gaussianspa.github.io/.","Novel view synthesis has become a pivotal area in computer vision and graphics, driving advancements in applications such as virtual reality, augmented reality, and immersive media experiences [21]. NeRF [35] has recently gained prominence in this domain because it can generate high-quality, photorealistic images from sparse input views by representing scenes as continuous volumetric functions based on neural networks. However, NeRF often requires extensive computational resources and long training times, making it less practical for real-time applications and large-scale reconstructions. 3D Gaussian Splatting (3DGS) [25] has emerged as a powerful alternative, leveraging continuous aggregations of Gaussian functions to model scene geometry and appearance. Unlike NeRF, which relies on neural networks to approximate volumetric radiance fields, 3DGS directly represents scenes using a collection of Gaussians. This approach excels in capturing details and smooth transitions, offering significant advantages in training and rendering speed. 3DGS achieves superior visual fidelity [28] compared to NeRF while reducing computational overhead, making it more suitable for interactive applications that demand quality and performance. Despite its strengths, 3DGS suffers from significant memory requirements that hinder its practicality. The main issue is the massive memory consumption associated with storing a large number of Gaussians that are needed to represent complex scenes. Each Gaussian occupies memory space for its parameters, including position, covariance, and color attributes. In densely sampled scenes, the sheer volume of Gaussians leads to memory usage that exceeds the capacity of typical hardware, making it challenging to handle higher-resolution scenes and limiting its applicability in resource-constrained environments. Existing works, e.g., Mini-Splatting [20], LightGaussian [19], LP-3DGS [54], EfficientGS [31], and RadSplat [40], have predominantly focused on mitigating this issue by removing a certain number of Gaussians. Techniques such as pruning and sampling aim to discard unimportant Gaussians based on hand-crafted criteria such as opacity [25, 53], importance score (hit count) [20, 19, 40], dominant primitives [31], and binary mask [54, 29]. However, these criteria are generally utilized to determine the importance of Gaussian points from a single heuristic perspective, limiting robustness in dynamic scenes or under varying lighting conditions. Moreover, the sudden one-shot removal may cause permanent loss of Gaussians that are crucial to visual synthesis, making it challenging to recover the original performance after even long-term training, as shown in Figure 2. Consequently, while these methods can alleviate memory and storage burdens to some extent, they often lead to sub-optimal rendering outcomes with loss of details and visual artifacts, thereby compromising the quality of the synthesized views. In this paper, we present an optimization-based simplification framework, GaussianSpa, for compact and high-quality Gaussian Splatting. In the proposed framework, we formulate 3DGS simplification as a constrained optimization problem under a target number of Gaussians. Then, we propose an efficient “optimizing-sparsifying"" solution for the formulated problem by splitting it into two simple sub-problems that are alternately solved in the “optimizing"" step and the “sparsifying"" step. Instead of permanently removing a certain number of Gaussians, GaussianSpa incorporates the “optimizing-sparsifying” algorithm into the training process, gradually imposing a substantial sparse property onto the trained Gaussians. Hence, our GaussianSpa can simultaneously enjoy maximum information preservation from the original Gaussians and a desired number of reduced Gaussians, providing compact 3DGS models with high-quality rendering. Overall, our contributions can be summarized as follows: • We propose a general 3DGS simplification framework that formulates the simplification objective as an optimization problem and solves it in the 3DGS training process. In solving the formulated optimization problem, our proposed framework gradually restricts Gaussians to the target sparsity constraint without explicitly removing a specific number of points. Hence, GaussianSpa can maximally maintain and smoothly transfer the information to the sparse Gaussians from the original model. • We propose an efficient “optimizing-sparsifying"" solution for the formulated problem, which can be integrated into the 3DGS training with negligible costs, separately solving two sub-problems. In the “optimizing"" step, we optimize the original loss function attached by a regularization with gradient descent. In the “sparsifying"" step, we analytically project the auxiliary Gaussians onto the constrained sparse space. • We comprehensively evaluate GaussianSpa through extensive experiments on various complex scenes, demonstrating improved rendering quality compared to existing approaches. Particularly, with as high as 10\times fewer number of Gaussians than the vanilla 3DGS, GaussianSpa achieves an average 0.4 dB improvement on the Mip-NeRF 360 [4] and Tanks&Temples [27] datasets, 0.9 dB on Deep Blending [23] dataset. Furthermore, we conduct various visual quality assessments, showing that GaussianSpa exhibits high-quality rendering of details and sparse 3D Gaussian views."
https://arxiv.org/html/2411.05993v1,A Modular Conditional Diffusion Framework for Image Reconstruction,"Diffusion Probabilistic Models (DPMs) have been recently utilized to deal with various blind image restoration (IR) tasks, where they have demonstrated outstanding performance in terms of perceptual quality. However, the task-specific nature of existing solutions and the excessive computational costs related to their training, make such models impractical and challenging to use for different IR tasks than those that were initially trained for. This hinders their wider adoption, especially by those who lack access to powerful computational resources and vast amount of training data. In this work we aim to address the above issues and enable the successful adoption of DPMs in practical IR-related applications. Towards this goal, we propose a modular diffusion probabilistic IR framework (DP-IR), which allows us to combine the performance benefits of existing pre-trained state-of-the-art IR networks and generative DPMs, while it requires only the additional training of a relatively small module (0.7M params) related to the particular IR task of interest. Moreover, the architecture of the proposed framework allows for a sampling strategy that leads to at least four times reduction of neural function evaluations without suffering any performance loss, while it can also be combined with existing acceleration techniques such as DDIM. We evaluate our model on four benchmarks for the tasks of burst JDD-SR, dynamic scene deblurring, and super-resolution. Our method outperforms existing approaches in terms of perceptual quality while it retains a competitive performance with respect to fidelity metrics.","With the advent of deep learning we have witnessed outstanding results in a wide range of computer vision tasks [89], including many challenging blind image restoration (IR) problems [84] such as burst imaging [40], super-resolution (SR) [9], deconvolution [58], etc. The standard approach for supervised learning in a blind IR setting involves training a feed-forward network that should estimate the latent image based on the available low-quality measurements. Such models are usually trained to maximize fidelity metrics like PSNR or SSIM, but the visual quality of the resulting images is sub-optimal [6]. The inclusion of perceptual losses [30] to the objective can improve the visual results, but fails to convincingly address the problem. Figure 1: The proposed architecture consists of three modules: a Denoising Network \bm{\phi}^{D}_{\bm{\theta}_{D}}\left(\tilde{\bm{x}}_{t},\tilde{\sigma}_{t}\right), an IR Network \bm{\phi}^{IR}_{\bm{\theta}_{IR}}\left(\bm{y}\right) and a Fusion Network \bm{\phi}^{F}_{\bm{\theta}_{F}}\left(\bm{x}_{0}^{IR},\bm{x}_{0}^{D},t\right). A small version of MIRNet [81] is used as the Denoising Network, while a pre-trained SwinIR [42] or BSRT [50] or FFTFormer [34] is used as the IR Network, depending on the IR task. See section 3.3 for a detailed description. A promising direction towards IR results of high visual quality is to consider such problems within a generative framework. Several generative models have been recently proposed including Variational Autoencoders (VAEs) [33], Generative Adversarial Neural Networks (GANs) [22], Normalizing Flows (NFs) [16] and Diffusion Probabilistic Models (DPMs) [65]. Due to their impressive results in image generation, they have been further utilized to perform conditional sampling of high-quality images, with their low-quality or distorted counterparts playing the role of the conditional input [21, 38, 46, 39]. To date, DPMs appear to be the most promising framework and lead to the best results among all existing generative approaches. Nevertheless, there are certain limitations that existing DPMs face, which hinder their wider adoption in IR tasks. In particular, inference of such models involves a sampling process that requires a large number (in the order of hundreds) of neural function evaluations (NFEs), which can be computationally very expensive, especially when considering images of high resolution. Another important limitation is that an efficient conditioning on the image measurements has yet to be proposed for DPMs in order to make them applicable to a wider range of blind IR problems. Indeed, all of the existing methods aim to learn the parameters of a single input-conditioned network for a specific blind IR task. As a result, the trained model overfits on the distribution of the condition space, and the whole model has to be retrained if we need to employ it to a different reconstruction task than the one that was initially trained for. Considering the huge amount of data and computational resources required for training a single DPM (see appendix A), such re-training becomes infeasible if at least one of the previous requirements is not satisfied. In this work we aim to address the above issues by proposing a novel conditional diffusion network coupled with an accelerated sampling process. Specifically, our network adopts an improved conditioning strategy and is built on the foundation of existing off-the-shelf IR networks paired with a denoising module, which is applicable to a variety of reconstruction problems without requiring any re-training. Additionally, we introduce an accelerated sampling procedure that is enabled by our proposed network architecture and allows the merging of a large number of sampling steps in a single one, computed with a single NFE. Our proposed acceleration can work in tandem with accelerated sampling schemes such as DDIM [66]. To assess the performance of our network, we validate it on three challenging blind IR tasks, namely, burst joint demosaicking, denoising and super-resolution (JDD-SR), dynamic scene deblurring, and 4\times single image super-resolution (SISR). In all of the tested scenarios, our approach demonstrates the best perception-distortion trade-off among the state-of-the-art (SOTA) methods, while compared to other DPM-based solutions it requires a smaller number of sampling steps."
https://arxiv.org/html/2411.05964v1,Utilisation of Vision Systems and Digital Twin for Maintaining Cleanliness in Public Spaces,"Nowadays, the increasing demand for maintaining high cleanliness standards in public spaces results in the search for innovative solutions. The deployment of CCTV systems equipped with modern cameras and software enables not only real-time monitoring of the cleanliness status but also automatic detection of impurities and optimisation of cleaning schedules. The Digital Twin technology allows for the creation of a virtual model of the space, facilitating the simulation, training, and testing of cleanliness management strategies before implementation in the real world. In this paper, we present the utilisation of advanced vision surveillance systems and the Digital Twin technology in cleanliness management, using a railway station as an example. The Digital Twin was created based on an actual 3D model in the Nvidia Omniverse Isaac Sim simulator. A litter detector, bin occupancy level detector, stain segmentation, and a human detector (including the cleaning crew) along with their movement analysis were implemented. A preliminary assessment was conducted, and potential modifications for further enhancement and future development of the system were identified.","Nowadays, more and more cities and businesses are turning to advanced solutions to maintain order and cleanliness in public and private spaces. One such innovative tool is video surveillance, sometimes used together with IoT (Internet of Things) devices sheng2020 . Initially used mainly for security purposes, it is now also increasingly used in cleanliness management balchandani2017 . CCTV systems, equipped with modern cameras and software, make it possible not only to monitor the state of cleanliness in real time, but also to identify areas in need of intervention and to coordinate the work of cleaning services. The use of video surveillance in cleaning has a number of benefits, especially in high-traffic areas. It allows a quicker response to incidents such as littering in inappropriate places, vandalism or pollution of public spaces. Thanks to image analysis, it is also possible to optimise cleaning schedules and manage human resources more efficiently. Video surveillance is therefore becoming an indispensable element in a cleanliness management strategy, contributing to improving the quality of life of residents and the aesthetics of the environment. This paper presents a discussion of selected problems related to the design of a cleaning system using vision processing. The analysis of litter detection, the occupancy of litter bins, the detection of stains on the floor and the determination of the location of the cleaning crew based only on video surveillance data are identified as key areas of focus. The various aspects and practical applications of the digital twin for creating, simulating, training and testing these systems using the example of cleaning maintenance are also discussed, using a railway station as an example industryDT . We present the advantages and benefits of creating a digital virtual model of a public space, using a railway station as an example, and apply advanced image processing and analysis methods. Our contribution is the development of a vision system for maintaining cleanliness in high-traffic areas, using Digital Twin technologies as an example of a railway station. The system incorporates key elements to detect and analyse cleanliness situations in real time. The system’s objective is to check the occupancy of litter bins, monitor cleanliness, and convey data to the cleaning services. The monitoring of the space is conducted in real time, thereby enabling the prompt detection of events. Due to design constraints, the system utilises solely video surveillance data, thus avoiding any interference with the building infrastructure. The anticipated benefits of employing this system are increased cleanliness, reduced cleaning costs (cleaning only when necessary) and maintenance management analysis. To the authors’ knowledge, this is the first vision system of its kind based on Digital Twin technology and the Nvidia Omniverse Isaac Sim. Section 2 presents state-of-the-art methods for detecting litter, bins, people, segmenting stains and analysing their status. Section 3 describes the proposed vision system. Section 4 contains the results of the system on real data and Section 5 provides a summary and future works discussion."
https://arxiv.org/html/2411.05939v1,GCI-ViTAL: Gradual Confidence Improvement with Vision Transformers for Active Learning on Label Noise,"Active learning aims to train accurate classifiers while minimizing labeling costs by strategically selecting informative samples for annotation. This study focuses on image classification tasks, comparing AL methods on CIFAR10, CIFAR100, Food101, and the Chest X-ray datasets under varying label noise rates. We investigate the impact of model architecture by comparing Convolutional Neural Networks (CNNs) and Vision Transformer (ViT)-based models. Additionally, we propose a novel deep active learning algorithm, GCI-ViTAL, designed to be robust to label noise. GCI-ViTAL utilizes prediction entropy and the Frobenius norm of last-layer attention vectors compared to class-centric clean set attention vectors. Our method identifies samples that are both uncertain and semantically divergent from typical images in their assigned class. This allows GCI-ViTAL to select informative data points even in the presence of label noise while flagging potentially mislabeled candidates. Label smoothing is applied to train a model that is not overly confident about potentially noisy labels. We evaluate GCI-ViTAL under varying levels of symmetric label noise and compare it to five other AL strategies. Our results demonstrate that using ViTs leads to significant performance improvements over CNNs across all AL strategies, particularly in noisy label settings. We also find that using the semantic information of images as label grounding helps in training a more robust model under label noise. Notably, we do not perform extensive hyperparameter tuning, providing an out-of-the-box comparison that addresses the common challenge practitioners face in selecting models and active learning strategies without an exhaustive literature review on training and fine-tuning vision models on real-world application data.","While most works in literature often compare results to baseline active learning algorithms such as random selection or simple entropy-based selection, the extensive hyper-parameter tuning performed during training but often left out of the manuscripts leads to authors stating vastly different performances for the same CNN architecture, active learning algorithm [1], and label noise rate [2]. This not only raises questions about the credibility of reported state-of-the-art results but can also delay actual progress in developing active learning schemes that are robust to label noise and achieve performances comparable to models trained on clean labels. The work on non-active training of DL models in the presence of label noise, as well as the training of DL models on noise-free datasets in an active learning setting, are well addressed in the literature. However, the intersection of these niches has a long way to go [3]. AL algorithms seek to train an optimal model with minimal training data that is labeled iteratively. The most common AL methods seek to explore diverse training examples or focus on samples that the DL algorithm is uncertain about. AL in the presence of label noise is a particularly challenging topic since training DL models with a higher concentration of incorrect labels presents problems for the back-propagation algorithm’s ability to converge as demonstrated in [2]. It has also been shown that without sufficient training data, large models can memorize the noisy labels, and fail to generalize to the test set [4, 5, 6, 7, 8]. Figure 1 depicts the basic AL framework for training DL image classifiers in the presence of label noise. Figure 1: The main components in the AL framework in the presence of a noisy oracle. Each of these components may vary depending on the complexity of the data to be learned and the available resources. Most work in active learning with label noise has focused on the development of query selection algorithms that lead to highly informative and diverse data samples as well as noise-robust DL models. In this work, we compile a unified view of existing works on active learning for image classification with label noise. We are particularly interested in the fine-tuning CNN and ViT models pre-trained on the imageNet-1k dataset. We explore different DL model architectures and AL strategies on different datasets while varying the label noise rates up to 60%. We re-implement the commonly used baseline AL strategies namely: random query, maximum entropy, margin-based selection, model delta, and hybrid uncertainty sampling with diversity. This work seeks to address the following: • In the realm of active learning, where the emphasis is often on query selection, and in image classification with label noise, which is typically trained without active learning, reliable benchmarks for active learning algorithms with label noise are scarce. To tackle this issue, we conduct experiments by training multiple deep-learning models for image classification using different active learning algorithms and varying levels of label noise. We present our findings by reporting test results on popular datasets, including CIFAR10, CIFAR100, Food101, and Chest X-ray images (pneumonia). • Given that the ViT-based models currently outperform CNNs in image classification on CIFAR10 [9, 10], and post competitive results on CIFAR100, Food101, Chest X-ray images (pneumonia), and other classification datasets [11, 10, 12], how does the ViTs compare to CNN-based models in an active learning setting with label noise (ALLN), and what can be done to improve on ViT learners in this setting? • Lastly, we propose an active learning scheme customized for the properties of the transformer network to improve on active learning in the presence of label noise. We also provide new insights based on using ViTs for AL under label noise and propose avenues to advance this work."
https://arxiv.org/html/2411.05927v1,Moving Off-the-Grid: Scene-Grounded Video Representations,"Current vision models typically maintain a fixed correspondence between their representation structure and image space. Each layer comprises a set of tokens arranged “on-the-grid,” which biases patches or tokens to encode information at a specific spatio(-temporal) location. In this work we present Moving Off-the-Grid (MooG), a self-supervised video representation model that offers an alternative approach, allowing tokens to move “off-the-grid” to better enable them to represent scene elements consistently, even as they move across the image plane through time. By using a combination of cross-attention and positional embeddings we disentangle the representation structure and image structure. We find that a simple self-supervised objective—next frame prediction—trained on video data, results in a set of latent tokens which bind to specific scene structures and track them as they move. We demonstrate the usefulness of MooG’s learned representation both qualitatively and quantitatively by training readouts on top of the learned representation on a variety of downstream tasks. We show that MooG can provide a strong foundation for different vision tasks when compared to “on-the-grid” baselines111Project page: https://moog-paper.github.io/..","Learning visual representations of the physical world is at the core of computer vision. Recent years have seen a surge of vision models that address this problem via self-supervised learning [8, 5, 23, 40]. By leveraging objectives such as contrastive learning [8, 5] and masked image modelling [23], great strides have been made towards learning useful representations from image data. The vast majority of these methods use convolutional networks [35], vision transformers [14, 54] or a combination thereof [4]. This choice of architecture comes to no surprise, as it inherently reflects the structure of the underlying datasets: images are (typically) represented as grids of pixels, which are conveniently and efficiently processed using 2D convolutions and patch-based heuristics. This grid-based processing, however, leads to an inherent entanglement between the representation structure and image structure. In other words, specific tokens or feature vectors of the representation are encouraged to capture the contents of a specific image location, instead of binding to the underlying content of the physical scene. This issue is particularly apparent when processing video: when there is motion in the scene, either by ego-motion or object motion, the contents of the scene will move across the image plane and as such the representation (i.e. in terms of what is encoded where) will change accordingly. However, many down-stream scene understanding tasks require observing how individual objects (or object parts) change their configuration over time, even when other factors like camera motion translate the objects around the image plane. In this case, a representation that preserves correspondences between meaningful scene elements and representational elements is likely preferred. As a consequence, many works targeting object-centric tasks such as object detection [4, 36], tracking [38, 33, 25], or segmentation [34], have adopted specialized architectural components that learn object-based representations: representations that are lifted from the image grid to bind to individual objects. These representations, however, are specialized to object-centric tasks and either need to be learned with detailed supervision [4, 38, 34] or have difficulty scaling to diverse real-world raw video data [33, 19]. In this paper we propose a transformer-based video model that learns representations that are “off-the-grid” (OTG) in a self-supervised manner, providing consistent features that bind to underlying scene elements, and tracking them as they move through time. Our method, Moving Off-the-Grid (MooG), makes extensive use of cross-attention to learn a latent set of tokens that is decoupled from the image grid: tokens are updated via cross-attention when a new input frame arrives, and decoded back into images via cross-attention. MooG can process videos of arbitrary length by iteratively updating the representation as new frames are observed. In summary, our contributions are as follows: • We introduce Moving Off-the-Grid (MooG), a novel transformer-based recurrent video representation model that is capable of learning OTG representations via a simple next-frame prediction loss. • We qualitatively demonstrate that the OTG representation of MooG binds to different parts of the scene and tracks its content under motion, whereas a grid-based representation fails to do so. • Finally, we demonstrate how this representation facilitates a variety of downstream vision tasks, including point tracking, monocular depth estimation, and object tracking. Our approach outperforms self-supervised grid-based baselines, such as DINO [5, 40], and performs competitively with domain-specific approaches, such as TAP-Net [12] and TAPIR [13] for point tracking."
https://arxiv.org/html/2411.05902v1,Autoregressive Models in Vision: A Survey,"Autoregressive modeling has been a huge success in the field of natural language processing (NLP). Recently, autoregressive models have emerged as a significant area of focus in computer vision, where they excel in producing high-quality visual content. Autoregressive models in NLP typically operate on subword tokens. However, the representation strategy in computer vision can vary in different levels, i.e., pixel-level, token-level, or scale-level, reflecting the diverse and hierarchical nature of visual data compared to the sequential structure of language. This survey comprehensively examines the literature on autoregressive models applied to vision. To improve readability for researchers from diverse research backgrounds, we start with preliminary sequence representation and modeling in vision. Next, we divide the fundamental frameworks of visual autoregressive models into three general sub-categories, including pixel-based, token-based, and scale-based models based on the strategy of representation. We then explore the interconnections between autoregressive models and other generative models. Furthermore, we present a multi-faceted categorization of autoregressive models in computer vision, including image generation, video generation, 3D generation, and multi-modal generation. We also elaborate on their applications in diverse domains, including emerging domains such as embodied AI and 3D medical AI, with about 250 related references. Finally, we highlight the current challenges to autoregressive models in vision with suggestions about potential research directions. We have also set up a Github repository to organize the papers included in this survey at: https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey.111Literature reviewed is current as of October 31, 2024, with updates planned.","Autoregressive models, which generate data by predicting each element in a sequence based on the previous elements through conditional probabilities, initially gained prominence in the field of natural language processing (NLP) (Vaswani, 2017; Radford et al., 2019; Brown et al., 2020; Achiam et al., 2023; Wan et al., 2023; Zhou et al., 2023a). This success can be attributed to their inherent advantage of capturing long-range dependencies and producing high-quality, contextually relevant outputs. Especially empirical scaling laws (Henighan et al., 2020; Hoffmann et al., 2022; Muennighoff et al., 2023; Tao et al., 2024; Lyu et al., 2023) reveal that increasing model size and compute budgets consistently improves cross-entropy loss across various domains like image generation, video modeling, multimodal tasks, and mathematical problem solving, following a universal power-law relationship. Inspired by their achievements in NLP, autoregressive models have recently begun to demonstrate formidable potential in computer vision. Figure 1: We provide a timeline of representative visual autoregressive models, which illustrates the rapid evolution of visual autoregressive models from early pixel-based approaches like PixelRNN in 2016 to various advanced systems recently. We are excitedly witnessing the rapid growth in this field. The timeline in Figure 1 illustrates the key milestones and developments in the evolution of visual autoregressive models, highlighting their transition from NLP to computer vision. To date, autoregressive models have been applied to a wide array of generative tasks, including image generation (Parmar et al., 2018; Chen et al., 2020), image super-resolution (Guo et al., 2022; Li et al., 2016), image editing (Yao et al., 2022; Crowson et al., 2022), image-to-image translation (Li et al., 2024e; d) and video generation (Tulyakov et al., 2018; Hong et al., 2022), multi-modal tasks (Yu et al., 2023c; Lu et al., 2022) and medical tasks (Ren et al., 2024; Tudosiu et al., 2024), .etc. This broad applicability underscores the potential for further exploration and application of autoregressive models. With the rapid proliferation of visual autoregressive models, keeping up with the latest advancements has become increasingly challenging. Therefore, a comprehensive survey of existing works is both timely and crucial for the research community. This paper endeavors to provide a thorough overview of recent developments in visual autoregressive and explores potential directions for future improvements. We emphasize that there are at least three distinct categories of visual autoregressive models defined by their sequence representation strategies: pixel-based, token-based, and scale-based models. Pixel-RNN (Van Den Oord et al., 2016), as a representative pixel-wise model in pioneering next-pixel prediction by transforming a 2D image into a 1D pixel sequence, capturing both local and long-range dependencies but with high computational cost. Next-token prediction, inspired by NLP, compresses images into discrete tokens for efficient high-resolution processing, exemplified by models like VQ-VAE (Van Den Oord et al., 2017). VAR (Tian et al., 2024) introduces next-scale prediction, a hierarchical method generating content across multiple scales from coarse to fine autoregressively, capturing visual information at multiple resolutions. Each category offers unique advantages and challenges, making them promising directions for future research. We further introduce a multi-perspective categorization of autoregressive models applied to computer vision, classifying existing models based on criteria such as the sequence representation strategy, the underlying framework, or the target task. Our categorization aims to provide a structured overview of how these models are utilized across various vision tasks. We then present both quantitative and qualitative metrics to assess their performance and applicability. Finally, we highlight the current limitations of autoregressive models, such as computational complexity and mode collapse, and propose potential directions for future research. In summary, this survey makes several contributions: • Given the recent surge of advances based on visual autoregressive models, we provide a comprehensive and timely literature review of these models, aiming to offer readers a quick understanding of the generic autoregressive modeling framework. • We categorize visual autoregressive models based on their sequence representation strategies and systematically compile applications across various domains. This aims to help researchers in specific fields quickly identify and learn about related work. • We provide a comprehensive review of autoregressive models in vision from about 250 related references and summarize their evaluations compared with GAN/Diffusion/MAE-based methods in four image generation benchmarks (ImageNet, MS-COCO, MJHQ-30K, and GenEval bench). {forest} forked edges, for tree= grow=east, reversed=true, anchor=base west, parent anchor=east, child anchor=west, base=center, font=, rectangle, draw=hidden-draw, rounded corners, align=center, text centered, minimum width=8em, edge+=darkgray, line width=1pt, s sep=3pt, inner xsep=2pt, inner ysep=3pt, line width=0.8pt, text width=6em, ver/.style=rotate=90, child anchor=north, parent anchor=south, anchor=center, , where level=1text width=8em,font=,align=center, where level=2text width=16em,font=,align=center, where level=3text width=16em,font=,align=center, where level=4text width=16em,font=,align=center, [Autoregressive Models in Vision, ver , text width=20em [Image Generation (§3.1.1), fill=white!10 , text width=15em [Unconditional Image Generation (§3.1.1), fill=orange!10 , text width=20em [Pixel-wise Generation (§3.1.1), fill=orange!10, text width=20em [PixelRNN (Van Den Oord et al., 2016), PixelCNN (Van den Oord et al., 2016) PixelCNN++ (Salimans et al., 2017), Gated PixelCNN (Reed et al., 2016), PMARD (Reed et al., 2017),, fill=orange!10, text width=34em , align=left] [PixelSNAIL (Chen et al., 2018), Image Transformer (Parmar et al., 2018), ImageGPT (Chen et al., 2020), fill=orange!10, text width=34em, align=left] ] [Token-wise Generation (§3.1.1.2), fill=orange!10 , text width=20em [VQ-VAE (Van Den Oord et al., 2017), VQ-VAE-2 (Razavi et al., 2019), VQGAN (Esser et al., 2021b), ViT-VQGAN (Yu et al., 2021a), Efficient-VQGAN (Cao et al., 2023), TiTok (Yu et al., 2024c) VQGAN-LC (Zhu et al., 2024b), LlamaGen (Sun et al., 2024a) RQ-VAE (Lee et al., 2022a), MoVQ (Zheng et al., 2022), DQ (Huang et al., 2023) FSQ (Mentzer et al., 2023), Wavelet Tokenizer (Mattar et al., 2024), DCT-Transformer (Nash et al., 2021), MAGVIT (Yu et al., 2023a), MAGVIT-v2 (Yu et al., 2023b), Open-MAGVIT2 (Luo et al., 2024), OmniTokenizer (Wang et al., 2024b), , fill=orange!10, text width=34em, align=left] [VQ-VAE (Van Den Oord et al., 2017), VQ-VAE-2 (Razavi et al., 2019), VQGAN (Esser et al., 2021b), LlamaGen (Sun et al., 2024a), DeLVM (Guo et al., 2024), SAIM (Qi et al., 2023), MAR (Li et al., 2024c), SAR (Liu et al., 2024g), RAL (Ak et al., 2020) ImageBART (Esser et al., 2021a), DisCo-Diff (Xu et al., 2024) AiM (Li et al., 2024b), ZigMa Hu et al. (2024), DiM (Teng et al., 2024), DiG (Zhu et al., 2024c), Diffusion-RWKV (Fei et al., 2024), fill=orange!10, text width=34em, align=left] ] [Scale-wise Generation (§3.1.1.14), fill=orange!10 , text width=20em [VAR (Zhang et al., 2024a) , fill=orange!10, text width=34em] ] ] [Text-to-Image Synthesis (§3.1.2), fill=green!1 , text width=20em [Token-wise Generation (§6), fill=green!1, text width=20em [DALL·E (Ramesh et al., 2021), CogView (Ding et al., 2021), CogView2 (Ding et al., 2022), Parti (Yu et al., 2022), Make-a-Scene (Gafni et al., 2022), LQAE (Liu et al., 2024b), Fluid (Fan et al., 2024) , fill=green!1, text width=34em, align=left] [VQ-Diffusion (Gu et al., 2022), Kaleido Diffusion (Gu et al., 2024) DART (Zhao et al., 2024) , fill=green!1, text width=34em] [LLM4GEN (Liu et al., 2024f), V2T (Zhu et al., 2024a), MARS (He et al., 2024) Lumina-mGPT (Liu et al., 2024a), fill=green!1, text width=34em] [IconShop (Wu et al., 2023b), Make-a-Story (Rahman et al., 2023) SEED-Story (Yang et al., 2024), fill=green!1, text width=34em, align=left] ] [Scale-wise Generation (§3.1.2.4), fill=green!1, text width=20em [STAR (Ma et al., 2024), VAR-CLIP (Zhang et al., 2024a), , fill=green!1, text width=34em] ] ] [Image-to-Image Translation (§3.1.3), fill=blue!10, text width=20em [Image Painting (§3.1.3), fill=blue!10, text width=20em [QueryOTR (Yao et al., 2022), BAT-Fill (Yu et al., 2021b), fill=blue!10, text width=34em] ] [Multi-view Generation (§7), fill=blue!10, text width=20em [MIS (Shen et al., 2024), SceneScript (Avetisyan et al., 2024) , fill=blue!10, text width=34em] ] [Visual In-Context Learning (§7), fill=blue!10, text width=20em [MAE-VQGAN (Bar et al., 2022), VICL (Bai et al., 2024b) , fill=blue!10, text width=34em] ] ] [Image Editing (§3.1.4), fill=yellow!10 , text width=20em [Text-driven Image Editing (§8), fill=yellow!10, text width=20em [VQGAN-CLIP (Crowson et al., 2022), Make-A-Scene (Gafni et al., 2022) , fill=yellow!10, text width=34em ] ] [Image-driven Image Editing (§8), fill=yellow!10, text width=20em [ControlAR (Li et al., 2024e), ControlVAR (Li et al., 2024d) M2M (Shen et al., 2024), CAR (Yao et al., 2024), MSGNet (Cardenas et al., 2021) MVG (Ren et al., 2024), fill=yellow!10, text width=34em] ] ] ] [Video Generation (§3.2), fill=pink!5 , text width=15em [Unconditional Video Generation (§3.2.1), fill=pink!5, text width=20em [VPNs (Kalchbrenner et al., 2017), MoCoGAN (Tulyakov et al., 2018), VideoTransformer (Weissenborn et al., 2019), LVT (Rakhimov et al., 2020), VideoGPT (Yan et al., 2021), TATS (Ge et al., 2022), PVDM (Yu et al., 2023d), MAGVIT-v2 (Yu et al., 2024b) , fill=pink!5, text width=34em, align=left] ] [Conditional Video Generation (§3.2.2), fill=pink!5 ,text width=20em [IRC-GAN (Deng et al., 2019), Godiva (Wu et al., 2021), LWM (Liu et al., 2024c), CogVideo (Hong et al., 2022), NÜWA (Wu et al., 2022), NUWA-Infinity (Liang et al., 2022), Phenaki (Villegas et al., 2022), ART-V (Weng et al., 2024), ViD-GPT (Gao et al., 2024), Loong (Wang et al., 2024e), PAV (Xie et al., 2024a), iVideoGPT (Wu et al., 2024c), fill=pink!5, text width=34em, align=left ] [Convolutional LSTM (Shi et al., 2015), PredRNN Wang et al. (2017), E3D-LSTM (Wang et al., 2019), SV2P (Babaeizadeh et al., 2018), PVV (Walker et al., 2021), HARP (Seo et al., 2022), LVM Bai et al. (2024a), ST-LLM (Liu et al., 2025), Pyramid Flow (Jin et al., 2024a), fill=pink!5, text width=34em, align=left] [MAGE (Hu et al., 2022), VideoPoet (Kondratyuk et al., 2024) , fill=pink!5, text width=34em] ] [Embodied AI (§3.2.3), fill=pink!5 ,text width=20em [IRIS (Micheli et al., 2023), iVideoGPT (Wu et al., 2024d), Genie (Bruce et al., 2024), GR-1 (Wu et al., 2024b), GR-2 (Cheang et al., 2024), , fill=pink!5, text width=34em, align=left ] ] ] [3D Generation (§3.3), fill=violet!10 , text width=15em [Motion Generation (§3.3.1), fill=violet!10, text width=20em [AMD (Han et al., 2024), T2M-GPT (Zhang et al., 2023a), HiT-DVAE (Bie et al., 2022), HuMoR (Rempe et al., 2021) , fill=violet!10, text width=34em ] ] [Point Cloud Generation (§3.3.2), fill=violet!10, text width=20em [CanonicalVAE (Cheng et al., 2022), Octree Transformer (Ibing et al., 2023), ImAM (Qian et al., 2024), Argus3D (Qian et al., 2024) , fill=violet!10, text width=34em ] ] [Scene Generation (§3.3.3), fill=violet!10, text width=20em [Make-A-Scene (Gafni et al., 2022), SceneScript (Avetisyan et al., 2024) , fill=violet!10, text width=34em ] ] [3D Medical Generation (§3.3.4), fill=violet!10, text width=20em [SynthAnatomy (Tudosiu et al., 2022), BrainSynth (Tudosiu et al., 2024), ConGe (Zhou & Khalvati, 2024), 3D-VQGAN (Zhou et al., 2023b), Unalign (Corona-Figueroa et al., 2023), AutoSeq (Wang et al., 2024c), fill=violet!10, text width=34em, align=left ] ] ] [Multimodal Generation (§3.4), fill=teal!7, text width=18em [Understanding Framework (§3.4.1), fill=teal!7, text width=20em [BEiT (Bao et al., 2021), BEiT-v2 (Peng et al., 2022), LLaVA (Liu et al., 2024d), VL-BEiT (Bao et al., 2022), BEiT-v3 (Wang et al., 2022b) , fill=teal!7, text width=34em, align=left ] ] [Unified Framework (§3.4.2), fill=teal!7, text width=20em [OFA (Wang et al., 2022a), CogView (Ding et al., 2021), M6 (Lin et al., 2021), ERNIE-ViLG (Zhang et al., 2021), NEXT-GPT (Wu et al., 2023c), SEED (Ge et al., 2024), Emu2 (Sun et al., 2024b), LaViT (Jin et al., ), Video-LaViT (Jin et al., 2024b), X-ViLA (Ye et al., 2024), fill=teal!7, text width=34em, align=left] [Chameleon (Team, 2024), Transfusion (Zhou et al., 2024) ScalingLaw (Aghajanyan et al., 2023), RA-CM3 (Yasunaga et al., 2022), SHOW-o (Xie et al., 2024b), Flamingo (Alayrac et al., 2022) MXQ-VAE (Lee et al., 2022b), CoTVL (Ge et al., 2023), Emu (Sun et al., 2023) Janus (Wu et al., 2024a), VILA-U (Wu et al., 2024e), Emu3 (Wang et al., 2024d), fill=teal!7, text width=34em, align=left] ] ] ] Figure 2: Literature taxonomy of autoregressive models in vision."
https://arxiv.org/html/2411.05900v1,Enhancing Cardiovascular Disease Prediction through Multi-Modal Self-Supervised Learning,"Accurate prediction of cardiovascular diseases remains imperative for early diagnosis and intervention, necessitating robust and precise predictive models. Recently, there has been a growing interest in multi-modal learning for uncovering novel insights not available through uni-modal datasets alone. By combining cardiac magnetic resonance images, electrocardiogram signals, and available medical information, our approach enables the capture of holistic status about individuals’ cardiovascular health by leveraging shared information across modalities. Integrating information from multiple modalities and benefiting from self-supervised learning techniques, our model provides a comprehensive framework for enhancing cardiovascular disease prediction with limited annotated datasets.We employ a masked autoencoder to pre-train the electrocardiogram ECG encoder, enabling it to extract relevant features from raw electrocardiogram data, and an image encoder to extract relevant features from cardiac magnetic resonance images. Subsequently, we utilize a multi-modal contrastive learning objective to transfer knowledge from expensive and complex modality, cardiac magnetic resonance image, to cheap and simple modalities such as electrocardiograms and medical information. Finally, we fine-tuned the pre-trained encoders on specific predictive tasks, such as myocardial infarction. Our proposed method enhanced the image information by leveraging different available modalities and outperformed the supervised approach by 7.6% in balanced accuracy.","Cardiovascular diseases (CVDs) are the leading cause of death worldwide resulting in 20.5 million deaths in 2021 [Lindstrom et al.(2022)Lindstrom, DeCleene, Dorsey, Fuster, Johnson, LeGrand, Mensah, Razo, Stark, Varieur Turco, et al.] which makes the early diagnosis and prevention of CVD a critical task in medical research. Patient data, including laboratory values, imaging data, medical histories from the electronic health record (EHR), and lifestyle contribute to a comprehensive patient profile and are essential for healthcare decisions. Integrating these diverse data sources in real time facilitates more effective prevention and treatment strategies. Several medical data are used for detecting CVD. Cardiac magnetic resonance (CMR) images are the gold standard for assessing cardiac structure, and electrocardiograms (ECGs) are commonly used for electrophysiological evaluation. Early algorithms focus on automated ECG interpretation [Noseworthy et al.(2022)Noseworthy, Attia, Behnken, Giblon, Bews, Liu, Gosse, Linn, Deng, Yin, et al., Khurshid et al.(2022)Khurshid, Friedman, Reeder, Di Achille, Diamant, Singh, Harrington, Wang, Al-Alusi, Sarma, et al.] or CMR image analysis [Raisi-Estabragh et al.(2022)Raisi-Estabragh, Salih, Gkontra, Atehortúa, Radeva, Boscolo Galazzo, Menegaz, Harvey, Lekadir, and Petersen]. A major limitation of current models is that they are limited to algorithms trained on a single modality and don’t translate between different modalities, however, to capture the complexity of human biology, there is a need to go beyond traditional clinically-focused and expert-curated features and include critically important, but often neglected, data types on which doctors’ evaluation rely on [Gong et al.(2022)Gong, Bai, Zheng, Smith, and Beckmann]. Machine learning models can leverage the complementary information present in different modalities to develop a joint characterization of physiological states similar to the conventional approach of diagnosis by doctors and further enhancing their effectiveness. Several studies have employed multimodal data to improve diagnostic capabilities. Khurshid et al\bmvaOneDotdeveloped a deep learning-based model to estimate left ventricular (LV) mass from 12-lead ECGs, which showed a strong correlation with LV mass on cardiac MRI imaging and cardiovascular events [Khurshid et al.(2021)Khurshid, Friedman, Pirruccello, Di Achille, Diamant, Anderson, Ellinor, Batra, Ho, Philippakis, et al.]. Christensen et al. developed a model that learned to interpret cardiac ultrasound images by correlating them with expert cardiologists’ annotations [Christensen et al.(2024)Christensen, Vukadinovic, Yuan, and Ouyang]. Borsos et al. proposed a multimodal deep learning architecture combining tabular data and imaging to predict dichotomized mRS scores three months post-Acute Ischemic Stroke [Borsos et al.(2024)Borsos, Allaart, and van Halteren]. Amal et al. provided a comprehensive review of multimodal approaches in healthcare for CVD analysis, emphasizing various fusion strategies [Amal et al.(2022)Amal, Safarnejad, Omiye, Ghanzouri, Cabot, and Ross]. Ensemble learning improves deep learning model performance through various approaches but it is challenging due to high training costs, the need for inducing diversity among models, complex model selection, and combining predictions, requiring further research to optimize these aspects. [Ganaie et al.(2022)Ganaie, Hu, Malik, Tanveer, and Suganthan] Recent models have proven to be fruitful for use in biomedical prediction tasks, but the risk of overfitting remains due to the limited size of annotated datasets for supervised learning. Results of generative models such as Autoencoders (AEs) [Alain and Bengio(2014)] on multi-modal clinical measurements show that they perform well on aligning the embeddings from diverse modalities and constructing a holistic representation for characterizing physiological state [Radhakrishnan et al.(2023)Radhakrishnan, Friedman, Khurshid, Ng, Batra, Lubitz, Philippakis, and Uhler]. AEs are employed to learn cross-modal representations from large multimodal datasets. Healthcare datasets such as UK Biobank [Sudlow et al.(2015)Sudlow, Gallacher, Allen, Beral, Burton, Danesh, Downey, Elliott, Green, Landray, et al.] serve as an excellent resource for learning clinically relevant representations. Hager et al\bmvaOneDotattempted to combine images and tabular data for multi-modal pre-training of representation by optimizing a CLIP loss and predicting myocardial infarction (MI) for the downstream task [Hager et al.(2023)Hager, Menten, and Rueckert].They have shown that multi-modal prediction using CMR and tabular data outperforms uni-modal prediction. However, it relies solely on the image encoder for downstream tasks, ignoring the extensive information contained in the tabular data or other relevant modalities such as ECG. In healthcare, ECG signals and basic medical information, are readily available or easily measured. However, CMR imaging, which provide detailed anatomical and physiological information about the heart, is costly to obtain and may not always be accessible. Turgut et al\bmvaOneDottried a self-supervised contrastive learning [Chen et al.(2020b)Chen, Kornblith, Norouzi, and Hinton] approach that transfers domain-specific information from CMR images to ECG embeddings. They predict the subject-specific risk of various CVDs and determine distinct cardiac phenotypes solely from ECG data and demonstrate that learned ECG embeddings incorporate information from CMR image with self-supervised training on both modalities [Turgut et al.(2023)Turgut, Müller, Hager, Shit, Starck, Menten, Martens, and Rueckert]. However, integrating tabular data, which includes demographics, lifestyle, and lab tests, with imaging data in multi-modal datasets is essential for informed clinical decision-making in healthcare [Acosta et al.(2022)Acosta, Falcone, Rajpurkar, and Topol]. This project aims to improve the existing classification of CVD using only ECG signals and patient medical information for diagnosing MI by incorporating the embedding that is constructed with multiple modalities including CMR images. This study demonstrates – to the best of our knowledge, for the first time – how to successfully integrate all available modalities in different formats as in realistic medical scenarios for predicting different downstream tasks. Furthermore, it improves the alignment of different modalities in latent space by leveraging the huge amount of unannotated data with self-supervised learning. Importantly, we increase the balanced accuracy by 7.6% on a well-characterized clinical dataset for MI diagnosis.Our code is publicly available at https://github.com/FraGirla/MMSSL-for-CVD-Pred."
https://arxiv.org/html/2411.05898v1,"Integrating Object Detection Modality into 
Visual Language Model for Enhanced Autonomous Driving Agent","In this paper, we propose a novel framework for enhancing visual comprehension in autonomous driving systems by integrating visual language models (VLMs) with additional visual perception module specialised in object detection. We extend the Llama-Adapter architecture by incorporating a YOLOS-based detection network alongside the CLIP perception network, addressing limitations in object detection and localisation. Our approach introduces camera ID-separators to improve multi-view processing, crucial for comprehensive environmental awareness. Experiments on the DriveLM visual question answering challenge demonstrate significant improvements over baseline models, with enhanced performance in ChatGPT scores, BLEU scores, and CIDEr metrics, indicating closeness of model answer to ground truth. Our method represents a promising step towards more capable and interpretable autonomous driving systems. Possible safety enhancement enabled by detection modality is also discussed.","The rapid advancements in autonomous driving systems have led to an increased focus on developing end-to-end models capable of handling complex driving scenarios. Despite significant progress, current approaches still face challenges in generalisation, especially when faced with rare or unseen situations. Moreover, the ability to interact with human users and provide explanations for the model’s decisions is crucial for building trust and acceptance of autonomous vehicles. To address these challenges, the recently introduced DriveLM challenge Sima et al., (2023) aims to leverage the power of vision-language models (VLMs) and large language models (LLMs) in the context of autonomous driving. By combining the visual understanding capabilities of VLMs with the reasoning and natural language processing abilities of LLMs, DriveLM seeks to improve generalisation and enable interactive communication between autonomous vehicles and human users. Inspired by the DriveLM framework Sima et al., (2023), this paper presents a novel approach that integrates additional modalities into the LLMs to enhance its perception and reasoning capabilities for autonomous driving tasks. Our method builds upon the Llama-Adapter Zhang et al., 2023a , a parameter-efficient fine-tuning approach that allows for the incorporation of task-specific knowledge into the pre-trained LLM. The common practice for image perception capability in Llama-Adapter is to incorporate a pre-trained image embedder, specifically the CLIP Radford et al., 2021a model with trainable vision transformers (ViT) Dosovitskiy et al., (2020) to generate adaptation queries. The queries are then projected and appended onto layer-wise token embeddings. However, such a perceptual network has critical limitation, which is based on the employment of CLIP. Although CLIP is effective at capturing global contextual information, it struggles to accurately detect and locate objects in the image, as it is primarily trained on perceptual prompts rather than position-level annotations. To address this limitation, we propose the integration of a detection network into the Llama-Adapter framework. The detection network leverages pretrained YOLOS Fang et al., 2021b and postfix vision transformers to process multi-view camera inputs and generate rich feature representations that accurately capture object-specific information, such as positions and bounding boxes. Moreover, we introduced trainable ID-separator token to address confusion of object-camera relationship to concatenated YOLOS output. By incorporating the detection network, our approach enables: 1) The capability to sense local, fine-grained details in the driving scene, complementing the global understanding provided by the perceptual network. 2) Understanding of different viewpoints of BEV images via trainable ID tokens and detection-result-oriented fine-tuning. 3) enhancement of scene understanding robustness and potential defence against vulnerabilities from visual modality. We believe that a standalone detector is crucial for autonomous driving, which helps to ensure safer and more reliable decision-making by improving the capability to manage complex scenarios such as the detection of pedestrians, vehicles, and traffic signs. To evaluate the effectiveness of our approach, we conducted extensive experiments on the DriveLM challenge, comparing our model’s performance against state-of-the-art baselines. We demonstrate significant improvements in the ChatGPT score, the BLEU score, and the CIDEr score (see Sec. 4.2). The main contributions of this paper can be summarised as follows: (1) We identify the limitations of relying solely on CLIP-based features for perception in the Llama-Adapter framework and propose the integration of a detection network to overcome these challenges. (2) We leverage pretrained YOLOS and vision transformers in the detection network to accurately capture object-specific information and enhance the perceptual capabilities of the Llama-Adapter. (3) We demonstrate significant improvements in multiple matrices on overall driving performance through extensive experiments on the DriveLM challenge, showcasing the benefits of integrating the detection network."
https://arxiv.org/html/2411.05887v1,Predictive Digital Twin for Condition Monitoring Using Thermal Imaging,"This paper explores the development and practical application of a predictive digital twin specifically designed for condition monitoring, using advanced mathematical models and thermal imaging techniques. Our work presents a comprehensive approach to integrating Proper Orthogonal Decomposition (POD), Robust Principal Component Analysis (RPCA), and Dynamic Mode Decomposition (DMD) to establish a robust predictive digital twin framework. We employ these methods in a real-time experimental setup involving a heated plate monitored through thermal imaging. This system effectively demonstrates the digital twin’s capabilities in real-time predictions, condition monitoring, and anomaly detection. Additionally, we introduce the use of a human-machine interface that includes virtual reality, enhancing user interaction and system understanding. The primary contributions of our research lie in the demonstration of these advanced techniques in a tangible setup, showcasing the potential of digital twins to transform industry practices by enabling more proactive and strategic asset management.","Digital twin (Rasheed et al., 2020) technology is widely regarded as a game-changing innovation, offering the potential to revolutionize how industries manage, monitor, and optimize their assets. With a capability scale (Stadtmann et al., 2023b) ranging from 0-5 (0-standalone, 1-descriptive, 2-diagnostic, 3-predictive, 4-prescriptive, 5-autonomous), digital twins enable deeper integration between physical assets and their virtual counterparts. Among the most valuable types of digital twins for industrial applications are those operating at the diagnostic and predictive levels, which are essential for condition monitoring. These digital twins provide actionable insights, facilitating predictive maintenance and proactive decision-making to mitigate potential failures before they occur. To ensure that a digital twin remains closely synchronized with its physical counterpart and avoids operational drift, it must be continuously fed with real-time data. This requirement emphasizes the importance of designing assets with the potential digital twin in mind, ensuring they are properly instrumented for seamless data integration. Thermal imaging (Bagavathiappan et al., 2013; Menges et al., 2024), in particular, offers a promising solution for condition monitoring, given its non-intrusive nature, high-resolution measurements, and cost efficiency. As temperature is often a key indicator of asset health, thermal imaging allows for accurate monitoring without disrupting system operations. However, thermal imaging generates highly complex, high-dimensional datasets, making it necessary to employ computationally efficient algorithms to process the data and extract meaningful insights. A shortage of such high-dimensional datasets has historically hindered the validation of new algorithms for digital twin applications. While many recent methods have been proposed, several rely heavily on black-box models (Sandhu et al., 2024; Jiang et al., 2024; Belay et al., 2024a), such as neural networks, which can lack transparency and interpretability, posing challenges in high-stakes environments where understanding the decision-making process is critical. To address this aspect of modeling, recent works (Menges et al., 2024; Belay et al., 2024b; Menges and Rasheed, 2024) have built upon methods with a robust mathematical foundation. The current work combines two such methods (Menges et al., 2024; Menges and Rasheed, 2024) originally tested independently on offline data and integrates them into a unified online digital twin framework for condition monitoring applied to a heated plate monitored using thermal imaging. The results are communicated using a human machine interface consisting of virtual reality. The main contributions of this work are: 1. the development of a digital twin-ready extendable physical framework for condition monitoring based on high dimensional thermal imaging. 2. the creation and demonstration of a corresponding extendable real-time diagnostic and predictive digital twin powered by predictive models based on a robust mathematical foundation. 3. blurring the distinction between the physical and the digital twin frameworks using a virtual reality interface. These contributions highlight a practical step forward from theoretical concepts to real-world applications, advancing proactive and strategic asset management. While this work demonstrates the effectiveness of digital twin technology, it also lays the groundwork for future developments, with the potential to improve industrial operations. The structure of this paper systematically addresses the development and implementation of a predictive digital twin for condition monitoring. Section 2 presents the required theories to understand the work. Section 3 presents the development of the physical framework for condition monitoring. Section 4 includes all the information required to reproduce the content presented in this work. Results and Discussions are presented in Section 5. Finally, the paper concludes (Section 6) with a summary of the key contributions and outlines future work aimed at enhancing the digital twin framework and extending its applicability to other use cases."
https://arxiv.org/html/2411.05880v1,Towards Equitable ASD Diagnostics: A Comparative Study of Machine and Deep Learning Models Using Behavioral and Facial Data,"Autism Spectrum Disorder (ASD) is often underdiagnosed in females due to gender-specific symptom differences overlooked by conventional diagnostics. This study evaluates machine learning models, particularly Random Forest and convolutional neural networks, for enhancing ASD diagnosis through structured data and facial image analysis. Random Forest achieved 100% validation accuracy across datasets, highlighting its ability to manage complex relationships and reduce false negatives, which is crucial for early intervention and addressing gender biases. In image-based analysis, MobileNet outperformed the baseline CNN, achieving 87% accuracy, though a 30% validation loss suggests possible overfitting, requiring further optimization for robustness in clinical settings. Future work will emphasize hyperparameter tuning, regularization, and transfer learning. Integrating behavioral data with facial analysis could improve diagnosis for underdiagnosed groups. These findings suggest Random Forest’s high accuracy and balanced precision-recall metrics could enhance clinical workflows. MobileNet’s lightweight structure also shows promise for resource-limited environments, enabling accessible ASD screening. Addressing model explainability and clinician trust will be vital.","Autism Spectrum Disorder (ASD) is a neurodevelopmental condition characterized by difficulties in social communication, interaction, and behavior, typically manifesting in early childhood [1]. ASD affects approximately 1 in 36 children in the U.S., with boys diagnosed nearly 3.8 times more often than girls, leading to a gender-based diagnostic gap that frequently leaves females undiagnosed or misdiagnosed until later in life. Delays in detection can negatively impact long-term outcomes in communication, education, and mental health, contributing to significant economic costs; the annual financial burden of ASD in the U.S. was estimated at $268 billion in 2015, projected to reach $461 billion by 2025. These societal and personal impacts underscore the importance of recognizing and understanding the core symptoms of ASD, which often vary in their presentation across individuals and can complicate timely diagnosis. Fig. 1 visually represents these key characteristics, including social interaction difficulties, repetitive behaviors, restricted interests, language delays, eye contact avoidance, communication difficulties, sensory sensitivities, nonverbal challenges, emotional regulation issues, and a need for routine. Figure 1: Visual representation of the key characteristics associated with ASD, highlighting the core aspects such as social interaction difficulties, repetitive behaviors, restricted interests, language delays, eye contact avoidance, communication difficulties, sensory sensitivities, nonverbal challenges, emotional regulation, and need for routine. The icons around the central figure illustrate the common behavioral and sensory traits often associated with ASD. The underdiagnosis of ASD in females can be attributed to several factors. Diagnostic criteria for ASD, which have been largely derived from studies focusing on male populations, may not adequately capture the more subtle or socially masked manifestations of the condition seen in females. Many females engage in ”camouflaging,” a strategy involving the suppression or masking of autistic traits in social settings, making clinical detection more challenging. Furthermore, existing diagnostic tools may not be sufficiently sensitive to gender-specific presentations of ASD, resulting in significant diagnostic delays. On average, females are diagnosed with ASD later in life than males, with some remaining undiagnosed well into adulthood. Several theoretical models have attempted to explain these gender differences in autism presentation. The Female Autism Phenotype (FAP) theory suggests that females exhibit different behavioral characteristics compared to males, which may influence their diagnosis [2]. Similarly, the Female Protective Effect (FPE) posits that females require a higher genetic or environmental burden to display comparable autistic traits as males [3]. Another influential theory, the Extreme Male Brain (EMB) hypothesis, posits that ASD may exaggerate cognitive patterns typically associated with males, driven by higher androgen levels [4]. However, recent research has questioned the link between androgen levels and autism, highlighting the complexity of these theoretical frameworks. Given these challenges, there is an urgent need for diagnostic tools that are more sensitive to the unique presentation of ASD in females. While traditional clinical methods are helpful, they may not fully capture the complexity of the condition, particularly in underdiagnosed populations. Advances in machine learning (ML) offer a promising solution. ML techniques can analyze large, complex datasets to detect patterns that may elude human evaluators, making them particularly suitable for identifying subtle differences in ASD presentations across genders. Several studies have demonstrated the utility of ML in diagnosing ASD and differentiating it from other neurodevelopmental conditions, such as attention deficit hyperactivity disorder (ADHD). However, the potential for ML to specifically improve diagnosis in females has been underexplored. This study aims to address this gap by leveraging ML models to enhance the accuracy of ASD diagnosis in females. By applying convolutional neural networks (CNNs) to facial image data and traditional classifiers to behavioral data, we aim to identify gender-specific patterns that could improve early detection in females. Face detection, a subfield of computer vision, holds particular promise for this task, as facial features may differ between males and females with ASD, offering new biological insights. Although previous studies in this area have focused primarily on male subjects, this study seeks to extend these techniques to female populations, providing a novel approach to reducing the diagnostic gender gap. In summary, current clinical guidelines for ASD diagnosis, mainly based on male presentations, are inconsistently applied, which disproportionately impacts females. ML, mainly when applied to facial and behavioral data, holds the potential to provide a scalable, objective, and sensitive method for diagnosing ASD in females. By evaluating the performance of various ML models across multiple datasets, this study seeks to improve early diagnosis and address the long-standing gender disparity in ASD detection. 1.1 Research Problem ASD affects individuals across all genders, yet early diagnosis is essential for ensuring timely access to intervention, support, and tailored treatments. While early diagnosis can significantly improve long-term outcomes in communication, education, and mental health, current diagnostic tools are predominantly designed based on how ASD manifests in boys. This creates a significant gap in diagnosing females, whose symptoms may differ in presentation, often resulting in delayed diagnosis or misdiagnosis [5]. These diagnostic discrepancies prevent women from receiving appropriate early interventions, which are crucial for improving quality of life and functional outcomes. Moreover, ASD diagnostic procedures are time-consuming and complex, often requiring multidisciplinary assessments and lengthy evaluations, which further delay access to care. Although ML has emerged as a potential tool to streamline diagnosis, relying solely on ML models is insufficient to capture the full range of ASD presentations across genders. A more integrated approach—combining ML with complementary techniques—is needed to ensure that gender-specific diagnostic nuances are identified and addressed effectively. 1.2 Purpose of Study The purpose of this study is twofold: (1) to evaluate the limitations of current ASD diagnostic tools with a particular focus on their inability to detect ASD in females, and (2) to explore how face detection techniques can be applied to improve the accuracy and timeliness of ASD diagnosis across genders. We aim to investigate which facial features are most relevant for distinguishing ASD in males and females, thereby providing new insights into the biological and genetic factors that may influence ASD presentation. Specifically, this study seeks to answer the following research question: Can combining facial feature analysis with ML models enhance the accuracy of ASD diagnosis in females compared to traditional diagnostic methods? 1.3 Motivation Accurate and timely diagnosis of ASD is crucial, as failing to diagnose ASD when it is present (a false-negative) can result in missed opportunities for early intervention, which is known to improve long-term developmental outcomes significantly. Additionally, a delayed diagnosis can leave individuals and families without necessary support and access to services. On the other hand, incorrectly diagnosing ASD (a false-positive) can lead to unnecessary treatments and diagnostic procedures, increasing emotional and financial strain on families and placing additional burdens on healthcare systems. The lack of a definitive medical test for ASD further complicates the diagnostic process, requiring clinicians to rely on observed behaviors and developmental history, which may not accurately reflect the experiences of all individuals—particularly females. Many autistic women do not receive timely or accurate diagnoses, which hinders their ability to advocate for their needs and limits access to critical resources. Research has shown that receiving an ASD diagnosis later in life can significantly improve self-identity, mental health, and access to necessary accommodations. Our goal is to develop a more efficient and accurate method for identifying women who may be on the autism spectrum, enabling earlier intervention and empowering women to self-identify with greater confidence, even without a formal assessment. 1.4 Contributions This study contributes to the growing field of ASD diagnosis by proposing an innovative approach that combines ML with face detection techniques to improve diagnostic accuracy, particularly for females. While face detection has been explored in ASD research, few studies have examined its potential to capture gender-specific diagnostic features. Our approach integrates facial analysis with ML models to provide a more nuanced understanding of ASD in males and females. By comparing a baseline CNN model with a more advanced CNN architecture, we explore the trade-offs between model complexity and diagnostic performance, offering insights into which models are most effective for this task. Our contributions include: • Developing a hybrid approach using facial and behavioral data for ASD diagnosis. • Providing a comparative analysis of different CNN models for ASD classification. • Highlighting the importance of biological and genetic factors in understanding ASD manifestation across genders. Form the audience perspective, this research is designed for ML specialists, biologists, geneticists, and clinical practitioners interested in the intersection of technology and ASD diagnosis. For ML experts, the study provides a case study of how advanced models like CNNs can be applied to diagnostic challenges. Biologists and geneticists may find the focus on facial feature analysis and its potential links to ASD informative for their research on the biological basis of neurodevelopmental disorders. Clinicians and medical professionals will benefit from insights into how ML can support gender-sensitive diagnostic practices, particularly for underdiagnosed populations such as women. 1.5 Paper Organization The remainder of this paper is organized as follows: Section 2 reviews the related literature on ASD diagnosis and the use of ML in healthcare. Section 3 presents our proposed method, combining ML and face detection techniques. Section 4 describes the experimental setup, including datasets and model configurations. Section 5 discusses the results of our experiments, and Section 6 concludes the paper with final thoughts and suggestions for future research."
https://arxiv.org/html/2411.05879v1,Smile upon the Face but Sadness in the Eyes: Emotion Recognition based on Facial Expressions and Eye Behaviors,"Emotion Recognition (ER) is the process of identifying human emotions from given data. Currently, the field heavily relies on facial expression recognition (FER) because facial expressions contain rich emotional cues. However, it is important to note that facial expressions may not always precisely reflect genuine emotions and FER-based results may yield misleading ER. To understand and bridge this gap between FER and ER, we introduce eye behaviors as an important emotional cues for the creation of a new Eye-behavior-aided Multimodal Emotion Recognition (EMER) dataset. Different from existing multimodal ER datasets, the EMER dataset employs a stimulus material-induced spontaneous emotion generation method to integrate non-invasive eye behavior data, like eye movements and eye fixation maps, with facial videos, aiming to obtain natural and accurate human emotions. Notably, for the first time, we provide annotations for both ER and FER in the EMER, enabling a comprehensive analysis to better illustrate the gap between both tasks. Furthermore, we specifically design a new Eye-behavior-aided Multimodal Emotion recognition Transformer (EMERT) architecture to concurrently enhance performance in both ER and FER by efficiently identifying and bridging the emotion gap between the two. Specifically, our EMERT employs modality-adversarial feature decoupling and multi-task Transformer to augment the modeling of eye behaviors, thus providing an effective complement to facial expressions. In the experiment, we introduce seven multimodal benchmark protocols for a variety of comprehensive evaluations of the EMER dataset. The results show that the EMERT outperforms other state-of-the-art multimodal methods by a great margin, revealing the importance of modeling eye behaviors for robust ER. To sum up, we provide a comprehensive analysis of the importance of eye behaviors in ER, advancing the study on addressing the gap between FER and ER for more robust ER performance. Our EMER database and the trained EMERT models will be publicly available at https://anonymous.4open.science/r/EMER-database.","Emotion recognition (ER) seeks to comprehend and identify various psycho-emotional states exhibited by humans across diverse behaviors and environments, which is of great importance in human-computer interaction and cognitive science [1]. ER also has broad applications in public security, education, healthcare, and advertising [2, 3]. Recently, promising progress has been witnessed with the development of Facial Expression Recognition (FER) approaches [4, 5, 6, 7]. In these methods, facial expressions have been considered one of the most informative indicators of emotional signals and are widely utilized for ER. Current FER-based ER solutions are mostly based on visual cues in images or videos captured by cameras [8, 9]. To achieve this, several FER datasets have been developed, including static facial image-based datasets and dynamic video-based datasets. Static facial image-based datasets, like SFEW [10], JAFFE [11], etc., mainly consist of still face images with related emotion category labels. Dynamic video-based datasets, contrary to static datasets, gather and annotate dynamic facial information from videos, which contain the datasets like AFEW 7.0 [12], DFEW [13], etc.. Using these visual FER datasets, promising FER-based methods for the ER task can be developed for recognizing six basic emotions (i.e., happiness, sadness, fear, surprise, disgust, and anger) following P. Ekman [14]. However, these methods only relying on the single visual facial expression signals may not be always reliable for ER, due to the camouflageability of facial expressive behaviours [8, 15] . In the area of ER, existing literature has shown that relying solely on visual facial expression signals is inadequate due to the subjective and camouflaged nature of facial expressions [1, 16]. This limitation results in a significant ‘emotion gap’ when applied to various scenarios. Here, the emotion gap refers to the disparity between facial expressions and genuine emotions of individuals [17]. As an intuitive example, when a person is concealing his or her sad feelings, he or she may put a big smile on the face as a natural response. In such scenarios, conventional FER methods would be misled by the smile expression and could not recognize the true sadness emotion robustly. To bridge this emotion gap, there are some initial works on introducing data from other modalities, such as Electroencephalography (EEG) data [18, 19, 20], for the ER task. The EEG data refers to the electrogram signals of the spontaneous electrical activity of the brain. The EEG-based ER datasets include MAHNOB-HCI [18] which integrates facial videos, audio, and EEG-based physiological signals for 30 emotion sequences, DEAP [19] which collects facial expression videos and EEG signals from 32 participants, and DECAF [20] which consists of facial videos and EEG-based physiological signals from 30 participants. Although the EEG data can be helpful, it will necessitate a more costly investment in EEG collection devices and will be more intrusive for the participants [21]. Since the expensive intrusive devices would inevitably affect the feelings of human emotion testers [22], existing related datasets are modest in size and limited. Moreover, even with the EEG data, existing multimodal approaches [23, 24] that incorporate EEG for the ER task do not explicitly handle the emotion gap and clarify whether the improvements are more from EEG or more from complicated model designs. As a result, it remains a missing piece in the current literature to comprehensively analyze the gap between FER and ER tasks using multimodal data. Figure 1: Examples with their annotations from our EMER database. The first row of each example is the facial expression video, the second is the eye movement sequence containing gaze point coordinates, gaze directions, pupil diameter changes, eye positions, gaze time, etc., and the third is the emotion-related eye fixation maps that show the participant focused on video contents when the emotion occurs. TABLE I: Summary of existing popular multimodal emotion databases and our proposed EMER database. Database #Part. Data Quality Non-invasive Sensor Visual Facial Images Eye Move Data Eye Fixation Maps Both ER & FER Anno. Emotion Gap Analysis CMU-MOSI [25] N/A Noisy ✓ ✓ CMU-MOSEI [26] N/A Noisy ✓ ✓ IEMOCAP [21] 10 Clean ✓ ✓ eNTERFACE’05 [27] 42 Clean ✓ ✓ DECAF [20] 30 Clean ✓ DEAP [28] 32 Clean ✓ SEED-IV [29] 15 Clean ✓ SEED-V [30] 20 Clean ✓ MAHNOB-HCI [18] 27 Clean ✓ ✓ Our EMER 121 Clean ✓ ✓ ✓ ✓ ✓ ✓ To address the above issues, we introduce a new Eye-behavior-based, spontaneous, Multimodal ER dataset, called EMER, to formulate a novel task for ER. The EMER integrates a variety of emotion signals for ER, including facial expressions, eye movement sequences, and emotion-related eye fixation maps. We introduce the eye behavior signals mainly by taking inspiration from Hess et al. [31] and several other psychological studies [32], [33] which illustrated that eye behaviors, like eye movements and eye fixation contents, are intuitive responses to emotions. Besides, unlike other modalities such as EEG data, eye behaviors can be collected by non-invasive devices [21] and can effectively reduce the risk of being concealed by misleading facial expressions [34]. For the collection of EMER, we employ a stimulus material-induced spontaneous emotion generation approach. Specifically, we first employ four emotion experts to select 28 emotion stimulus videos, and then recruit 121 participants to watch the selected stimulus materials to induce the production of short-term and spontaneous emotion states. Ultimately, we collect 1,303 high-quality multimodal data sequences with eye behaviors and facial expressions recorded by a non-invasive Tobbi Pro Fusion eye-tracking device 111https://www.tobii.com/products/eye-trackers/screen-based/tobii-pro-fusion and a high-definition camera, simultaneously. Moreover, we also employ a combined labeling approach to provide comprehensive, high-reliability annotations, including both emotion and facial expression annotations for all data in EMER, to enable deeper analysis and understanding of the mentioned emotion gap between FER and ER. This can contribute significant insights and advance the emotion-related research. To the best of our knowledge, this is the first study that formulates a comprehensive eye-behavior-based multimodal dataset for the ER task and can enable an explicit analysis of the emotion gap between FER and ER. Figure 1 provides an intuitive illustration of our motivation and the collected multimodal data with the corresponding multiple annotations, which clearly reveals a significant gap between emotions and facial expressions. To better illustrate the advantages of our dataset, Table I compares our EMER dataset with other existing FER-based multimodal emotion datasets. In addition to a new dataset, we further design a new Eye-behavior-focused Multimodal ER Transformer (EMERT) architecture. Although the existing multimodality methods [35, 36] can be directly applied to our EMER dataset, they are not optimized for modeling eye behaviors and addressing the emotion gap because our new dataset is the first of its kind. Built upon the fusion of available eye-behavior data and visual facial data, our EMERT attempts to apply adversarial learning and multi-task Transformer to help explicitly extract modality-sensitive high-level emotion features, so that the gap between facial expression information and eye behavior information can be better modeled and bridged for more effective ER. To sum up, we summarize the key contributions of this study as follows: • We create a novel eye-behavior-based multimodal ER dataset (EMER). This EMER dataset provides multimodal, participant-rich, and spontaneous affective data for the ER task, which provides both the FER and ER labels for comprehensive analysis. The EMER dataset contains eye movement sequences, emotion-related eye fixation maps, and facial expression videos collected from 1,303 samples involving 121 participants, which contributes a significant amount of high-quality data for the ER task. To the best of our knowledge, the EMER dataset is first-of-its-kind, providing a novel research direction in the ER task. • The EMER dataset introduces comprehensive annotation information. We cover 3-class coarse ER and FER labels (namely positive, negative and neutral), 7-class fine ER and FER labels (namely happiness, sadness, fear, surprise, disgust, anger, and neutral), 2-dimensional continuous emotion ratings (namely valence and arousal), as well as facial expression intensity. All of the annotation information contributes to the explicit investigation of the emotion gap between FER and ER, aiming to delve into the details of how to improve ER with multimodality data. • We devise a novel Eye-behavior-based Multimodal ER Transformer (EMERT) to achieve robust ER performance by explicitly and effectively bridging the emotion gap between facial expressions and true emotions. The EMERT has shown significant benefits in our multimodal ER task. • We carried out a comprehensive evaluation of various multimodal methods on our EMER dataset, introducing seven benchmarking protocols for evaluation. Furthermore, by addressing the gap between FER and ER, we can further demonstrate that the FER task can also take advantage of the emotional cues from multimodal data, which again strengthens our claim that analyzing the emotion gap between FER and ER explicitly is important for future research."
https://arxiv.org/html/2411.05858v1,Saliency Assisted Quantization for Neural Networks,"Deep learning methods have established a significant place in image classification. While prior research has focused on enhancing final outcomes, the opaque nature of the decision-making process in these models remains a concern for experts. Additionally, the deployment of these methods can be problematic in resource-limited environments. This paper tackles the inherent black-box nature of these models by providing real-time explanations during the training phase, compelling the model to concentrate on the most distinctive and crucial aspects of the input. Furthermore, we employ established quantization techniques to address resource constraints. To assess the effectiveness of our approach, we explore how quantization influences the interpretability and accuracy of Convolutional Neural Networks through a comparative analysis of saliency maps from standard and quantized models. Quantization is implemented during the training phase using the Parameterized Clipping Activation method, with a focus on the MNIST and FashionMNIST benchmark datasets. We evaluated three bit-width configurations (2-bit, 4-bit, and mixed 4/2-bit) to explore the trade-off between efficiency and interpretability, with each configuration designed to highlight varying impacts on saliency map clarity and model accuracy. The results indicate that while quantization is crucial for implementing models on resource-limited devices, it necessitates a trade-off between accuracy and interpretability. Lower bit-widths result in more pronounced reductions in both metrics, highlighting the necessity of meticulous quantization parameter selection in applications where model transparency is paramount. The study underscores the importance of achieving a balance between efficiency and interpretability in the deployment of neural networks.","Deep Neural Networks (DNNs) have transformed fields like computer vision, natural language processing, autonomous driving, and healthcare by enabling sophisticated pattern recognition from extensive datasets. These networks have driven notable advancements in accuracy for tasks such as image classification, language translation, and predictive analytics. However, despite their remarkable capabilities, DNNs are frequently criticized for being ”black boxes,” as their decision-making processes remain largely opaque and difficult to interpret. This opacity presents significant challenges, especially in high-stakes areas like medicine, finance, and autonomous systems, where understanding a model’s decision rationale is essential for building trust, ensuring reliability, and maintaining safety [1, 2, 3]. To address these concerns, a significant body of research has focused on developing methods to interpret and explain the decisions made by DNNs. One of the most widely used approaches is the generation of saliency maps, which provide visual representations of the regions in the input data that most strongly influence the model’s predictions. Saliency maps are typically produced using gradient-based techniques, which calculate the gradient of the output with respect to the input features to identify areas of importance [4, 5, 6]. These maps are invaluable tools for interpreting model behavior, as they allow users to gain insights into what the model is focused on when making decisions. However, the effectiveness of saliency maps can be compromised by noise and other distracting elements, which may obscure the true regions of interest and lead to misleading interpretations [7, 8]. To mitigate these issues, several advanced techniques have been proposed. For instance, the SmoothGrad method reduces noise by averaging multiple saliency maps generated from slightly perturbed versions of the input [9]. Other methods, such as Integrated Gradients [11] and Layer-wise Relevance Propagation [12], modify the backpropagation process to enhance the stability and reliability of the generated saliency maps. Despite these advancements, ensuring that saliency maps are both accurate and robust remains a key challenge, particularly when the model or the input data is subject to slight variations [13, 14, 15]. Parallel to the advancement of interpretability methods, there has been growing interest in optimizing neural networks for deployment in resource-limited environments, such as edge devices. A fundamental technique in this optimization is quantization, which reduces the precision of a neural network’s parameters and activations, commonly from 32-bit floating-point to lower-precision formats like 8-bit integers. This reduction considerably lowers the memory footprint and computational requirements, making it feasible to deploy complex models on devices with limited resources [20, 21, 22]. Quantization is essential for the efficient operation of DNNs in settings with constraints on computational power, memory, and energy, such as mobile devices, embedded systems, and other edge computing platforms [16, 26]. Various quantization methods have been developed to manage the trade-off between reducing computational complexity and maintaining high accuracy. Quantization-Aware Training (QAT) is particularly effective, as it integrates quantization into the training phase, enabling the network to adjust to lower precision throughout training. This approach generally achieves better accuracy than Post-Training Quantization, which applies quantization to an already trained model without further training [18]. Additionally, hardware accelerators like Field-Programmable Gate Arrays (FPGAs) and Application-Specific Integrated Circuits (ASICs) are often used to maximize the performance of quantized models, leveraging their capability to efficiently handle varied numerical representations [19]. Given the growing importance of both model interpretability and efficient deployment, an emerging question is how these two areas intersect, particularly with regard to saliency. Specifically, as models are increasingly quantized for deployment in resource-limited environments, it becomes imperative to understand how quantization affects the interpretability of these models. Does the reduction in precision compromise the quality and reliability of saliency maps? Can quantized models still provide meaningful insights into their decision-making processes, or does quantization obscure these explanations? This paper addresses these critical questions by conducting a comprehensive comparison between ”regular model saliency” and ”quantized model saliency.” We systematically investigate whether the process of quantization affects the generation of saliency maps, and if so, how this impact manifests in different scenarios. Our study applies various saliency methods to both high-precision and quantized versions of neural networks, followed by a detailed analysis of the resulting saliency maps in terms of their clarity, stability, and alignment with the original high-precision models. Through this comparison, we aim to contribute to the broader discourse on balancing model efficiency with interpretability, offering insights that could guide the future development of deployable, yet interpretable, AI systems."
https://arxiv.org/html/2411.05838v1,"StegaVision: Enhancing Steganography with Attention Mechanism
(Student Abstract)","Image steganography is the technique of embedding secret information within images. The development of deep learning has led to significant advances in this field. However, existing methods often struggle to balance image quality, embedding capacity, and security. This paper proposes a novel approach to image steganography by enhancing an encoder-decoder architecture with attention mechanisms, specifically focusing on channel and spatial attention modules. We systematically investigate five configurations: (1) channel attention, (2) spatial attention, (3) sequential channel followed by spatial attention, (4) spatial attention followed by channel attention and (5) parallel channel and spatial attention. Our experiments show that adding attention mechanisms improves the ability to embed hidden information while maintaining the visual quality of the images. The increase in the PSNR and SSIM scores shows that using a parallel combination of channel and spatial attention improves image quality and hiding capacity simultaneously. This is in contrast to previous works where there is a tradeoff between them. This study shows that attention mechanisms in image steganography lead to better hiding of secret information. Our code is available at https://github.com/vlgiitr/StegaVision.","Steganography is the practice of hiding information within other data. This has evolved significantly with deep learning techniques. While traditional methods like Least Significant Bit (LSB) and transform domain techniques have laid the foundation, they often suffer from limitations in robustness and capacity. Deep learning-based approaches, particularly convolutional neural networks (CNNs) (Subramanian et al. 2021), have shown promise in addressing these challenges by learning complex patterns for embedding and extracting hidden data. The attention mechanism has recently been applied to steganography to enhance the performance of embedding and extraction processes in GAN-based architectures (Tan et al. 2021). Channel attention focuses on what features are important by assigning different weights to each channel. This allows the model to emphasize informative features and suppress irrelevant ones. Spatial attention, conversely, determines where to focus within the feature maps, refining the spatial details critical for the task of image steganography. Our study examines the effectiveness of integrating channel and spatial attention mechanisms within encoder-decoder-based models for steganography. We experiment with different configurations of attention mechanisms to determine how these modifications impact the quality and security of steganography. We hope the insights we provide will guide future research in this field. Model PSNR Cover SSIM Cover PSNR Secret SSIM Secret MSE Loss Cover MSE Loss Secret Baseline 10.658 0.831 10.276 0.796 0.1620 0.1701 Channel Only 10.666 0.831 10.289 0.797 0.1619 0.1699 Spatial Only 10.734 0.835 10.266 0.799 0.1612 0.1703 Channel-Spatial Parallel 10.672 0.832 10.431 0.808 0.1616 0.1684 Channel then Spatial 10.504 0.815 10.090 0.779 0.1640 0.1725 Spatial then Channel 10.614 0.827 10.172 0.787 0.1627 0.1715 Table 1: Performance comparison of different attention mechanism configurations in steganography models. Best results are in bold and second best results are in italics."
https://arxiv.org/html/2411.05837v1,On the Trade-Off between Stability and Fidelity of Gaussian-Smoothed Saliency Maps,"Gradient-based saliency maps have been widely used to interpret the decisions of neural network classifiers and discover phenomena from their learned functions. Standard gradient-based maps are frequently observed to be highly sensitive to the randomness of training data and the stochasticity in the training process. In this work, we study the role of randomized smoothing in the well-known Smooth-Grad algorithm in the stability of the gradient-based maps to the randomness of training samples. We extend the algorithmic stability framework to gradient-based saliency maps and prove bounds on the stability error of standard Simple-Grad, Integrated-Gradients, and Smooth-Grad saliency maps. Our theoretical results suggest the role of Gaussian smoothing in boosting the stability of gradient-based maps to the randomness of training settings. On the other hand, we analyze the faithfulness of the Smooth-Grad maps to the original Simple-Grad and show the lower fidelity under a more intense Gaussian smoothing. We support our theoretical results by performing several numerical experiments on standard image datasets. Our empirical results confirm our hypothesis on the fidelity-stability trade-off in the application of Gaussian smoothing to gradient-based interpretation maps.","Figure 1: The stability-fidelity trade-off introduced by Gaussian smoothing. We trained neural networks on two disjoint training set splits of ImageNet, and computed the Simple-Grad and Smooth-Grad maps for the same test sample. The similarity between the two maps was higher in Smooth-Grad, while Smooth-Grad loses faithfulness to the Simple-Grad map. Deep learning models have attained state-of-the-art results over a wide array of supervised learning tasks including image classification [19], speech recognition [12], and text categorization [25]. The trained deep neural networks have been utilized not only to address the target classification task but also to discover the underlying rules influencing the label assignment to a feature vector. To this end, saliency maps, which highlight the features influencing the neural network’s predictions, have been widely used to gain an understanding of phenomena from data-driven models [9] and further to find new discoveries and insights. For example, [26] shows the application of saliency maps to identify the region in fundus images related to anemia, and [4] demonstrates that saliency maps can assist clinicians in diagnosing knee injury from MRI scans. Specifically, gradient-based maps have been widely applied to compute saliency maps for neural net classifiers. Standard gradient-based saliency maps such as Simple-Grad [30] and Integrated- Gradients [33], represent the input-based gradient of a neural network at a test sample, which reveals the input features with a higher local impact on the neural network classifier’s output. Therefore, the assignment of feature importance scores by a gradient-based saliency map can be utilized to reveal the main features influencing the phenomenon. However, a consideration for drawing conclusions from gradient-based maps is their potential sensitivity to the training algorithm of the neural network classifier. [2, 35] have shown that gradient maps could be significantly different for independent yet identically distributed training sets, or for a different random initialization used for training the classifier. The sensitivity of the saliency maps is indeed valuable in debugging applications, where the map is used to identify potential reasons behind a misclassification case. On the other hand, the instability of gradient-based maps could hinder the applications of Simple-Grad and Integrated-Gradients maps for phenomena discovery [2], as for a generalizable understanding of the underlying phenomenon, the resulting saliency maps need to have limited dependence on the stochasticity of training data sampled from the underlying data distribution. In this work, we specifically study the influence of Gaussian smoothing used in the Smooth-Grad algorithm [31] on the stability of a gradient-based map. We provide theoretical and numerical evidence that Gaussian smoothing could increase the stability of saliency maps to the stochasticity of the training setting, which can improve the reliability of applying the gradient-based map for phenomena understanding using the neural net classifier. To theoretically analyze the stability properties of saliency maps, we utilize the algorithmic stability framework in [5] to quantify the expected robustness of gradient maps to the stochasticity of a randomized training algorithm, e.g. stochastic gradient descent (SGD), and randomness of training data drawn from the underlying distribution. Following the analysis in [13], we define the stability error of a stochastic training algorithm and bound the error in terms of the number of training data and SGD training iterations for the Simple-Grad, Integrated-Gradients, and Smooth-Grad. Our stability error bounds suggest the improvement in stability by applying the randomized smoothing mechanism in Smooth-Grad. Specifically, our error bounds improve by a factor 1/\sigma under a standard deviation parameter \sigma of the Smooth-Grad’s Gaussian noise. In addition to the stability of gradient maps under Gaussian smoothing, we also bound the faithfulness of the Gaussian smoothed saliency maps to the original Simple-Grad and Integrated-Gradients interpretation maps. We show that by increasing the standard deviation parameter \sigma, the fidelity error grows proportionally to parameter \sigma. This relationship highlights a trade-off between the stability and fidelity of saliency maps, indicating the stabilization offered by Gaussian smoothing at the price of a higher fidelity error compared to the original saliency map. As illustrated in Figure 1, the Smooth-Grad saliency maps could be considerably more stable to the training set of the neural network, while they could signifinctly differ from the original Simple-Grad map. We note that different applications of saliency maps may prioritize stability or fidelity differently. For instance, debugging misclassifications would require higher fidelity, while phenomena discovery would prioritize higher stability. Therefore, understanding the stability-fidelity properties of Smooth-Grad maps helps with a principled application of the algorithm to different tasks. We perform numerical experiments to test our theoretical results on the algorithmic stability and fidelity of interpretation maps under Gaussian smoothing. In the experiments, we tested several standard saliency maps and image datasets. We empirically analyzed a broader range of instability sources in the training setting and demonstrated that Gaussian smoothing can lead to higher stability to the changes in the training algorithm and neural network architecture. The numerical results indicate the impact of Gaussian smoothing in reducing the sensitivity of the gradient-based interpretation map to the stochasticity of the training algorithm at the cost of a higher difference from the original gradient map. We can summarize this work’s main contribution as: • Studying the algorithmic stability and generalization properties of gradient-based saliency maps. • Proving bounds on the algorithmic stability error of saliency maps under vanilla and noisy stochastic gradient descent (SGD) training algorithms. • Analyzing the fidelity of Smooth-Grad maps and their faithfulness to the Simple-Grad map. • Providing numerical evidence on the stability-fidelity trade-off of Smooth-Grad maps on standard image datasets."
https://arxiv.org/html/2411.05832v1,"Diversify, Contextualize, and Adapt:
Efficient Entropy Modeling for Neural Image Codec","Designing a fast and effective entropy model is challenging but essential for practical application of neural codecs. Beyond spatial autoregressive entropy models, more efficient backward adaptation-based entropy models have been recently developed. They not only reduce decoding time by using smaller number of modeling steps but also maintain or even improve rate–distortion performance by leveraging more diverse contexts for backward adaptation. Despite their significant progress, we argue that their performance has been limited by the simple adoption of the design convention for forward adaptation: using only a single type of hyper latent representation, which does not provide sufficient contextual information, especially in the first modeling step. In this paper, we propose a simple yet effective entropy modeling framework that leverages sufficient contexts for forward adaptation without compromising on bit-rate. Specifically, we introduce a strategy of diversifying hyper latent representations for forward adaptation, i.e., using two additional types of contexts along with the existing single type of context. In addition, we present a method to effectively use the diverse contexts for contextualizing the current elements to be encoded/decoded. By addressing the limitation of the previous approach, our proposed framework leads to significant performance improvements. Experimental results on popular datasets show that our proposed framework consistently improves rate–distortion performance across various bit-rate regions, e.g., 3.73% BD-rate gain over the state-of-the-art baseline on the Kodak dataset.","Most neural image codecs [18, 17, 8, 9, 11, 15] first transform an image into a quantized latent representation. It is then encoded into a bitstream via an entropy coding algorithm, which relies on a learned probability model known as the entropy model. According to the Shannon’s source coding theorem, the minimum expected length of a bitstream is equal to the entropy of the source. Thus, accurately modeling entropy of the quantized latent representation is crucial. Entropy models estimate a joint probability distribution over the elements of the quantized latent representation. Generally, it is assumed that all elements follow conditionally independent probability distributions. To satisfy this, the probability distributions are modeled in context-adaptive manners, which is key to accurate entropy modeling [18]. Recent methods are based on the joint backward and forward adaptation where the probability distributions adapt by leveraging contexts in two different ways: directly using previously encoded/decoded elements (i.e., backward adaptation), and extracting and utilizing an additional hyper latent representation (i.e., forward adaptation). Here, the type of contexts leveraged can be diverse depending on the spatial range they cover. First, each element has dependencies with other elements in the same spatial location along the channel dimension. Since the channel-wise dependencies correspond to the local image area (e.g., a 16\times 16 patch), we denote them as the “local” context. Second, dependencies exist among spatially adjacent elements, and we refer to them as the “regional” context. Lastly, long-range spatial dependencies span the entire image area, referred to as the “global” context. For the backward adaptation, the modeling order, i.e., which elements are modeled first, is an important factor, and the key lies in how effectively we can utilize diverse contexts in the modeling process. Early studies employ spatial autoregressive (AR) models that access regional context including the most spatially adjacent elements. However, they suffer from significantly slow decoding times due to the inevitably large number of modeling steps, which is equal to the spatial dimensions [18]. To enhance efficiency in entropy modeling, several attempts reduce the number of modeling steps while leveraging diverse contexts: a 10-step channel-wise AR model [17], a 2-step spatial non-AR model with a checkerboard pattern [8], and a 4-step non-AR model that operates across spatial and channel dimensions using a quadtree partition [14]. Figure 1: DCA diversifies the hyper latent representations and contextualizes the current elements by leveraging the diverse hyper latent representations along with the previous elements. As a result, the probability distributions adapt effectively, leading to accurate entropy modeling. Entropy models based on the efficient backward adaptation methods have led to significant improvements. However, they are still limited in fully leveraging contexts for forward adaptation. Since they use multiple neural layers with downsampling and upsampling for modeling hyper latent representation, they can only access the regional context. This limits the performance improvement due to the insufficient contexts (Figure 1a). In particular, this limitation is exacerbated at the first step where only forward adaptation is utilized due to the absence of previous elements (Figure 6). Therefore, it is necessary to develop effective forward adaptation in synergy with the efficient backward adaptation. In this paper, we propose a simple yet effective entropy modeling framework, called DCA (Diversify, Contextualize, and Adapt), leveraging sufficient contexts for forward adaptation without compromising on bit-rate (Figure 1b). Building on the quadtree partition-based backward adaptation [14], we introduce a strategy of diversification, i.e., extracting local, regional, and global hyper latent representations unlike only a single regional one in the previous approach. Note that simply using more contexts for forward adaptation does not guarantee performance improvements because forward adaptation requires additional bit allocation unlike backward adaptation. Then, we propose how to effectively utilize the diverse contexts along with the previously modeled elements for contextualizing the current elements to be encoded/decoded. To consider step-wise different situations, e.g., increased number of previous elements over steps, our contextualization method is designed to utilize each hyper latent representation separately in a step-adaptive manner. Additionally, our contextualization method proceeds in the sequence of regional, global, and local hyper latent representations. Similarly to backward adaptation, we empirically observe that modeling order also matters in forward adaptation. Our main contributions are summarized as follows: • We propose a strategy of diversifying contexts for forward adaptation by extracting three different hyper latent representations, i.e., local, regional, and global ones. This strategy can provide sufficient contexts for forward adaptation without compromising on bit-rate. • We introduce how to effectively leverage the diverse contexts, i.e., previously modeled elements and the three hyper latent representations. We empirically show that the modeling order of three types of contexts affects the performance. • Through the diversification and contextualization methods, our DCA effectively adapts, resulting in significant performance improvements. For example, DCA achieves 3.73% BD-rate gain over the state-of-the-art method [14] on the Kodak dataset."
https://arxiv.org/html/2411.05827v1,A Theory of Stabilization by Skull Carving,"Accurate stabilization of facial motion is essential for applications in photoreal avatar construction for 3D games, virtual reality, movies, and training data collection. For the latter, stabilization must work automatically for the general population with people of varying morphology. Distinguishing rigid skull motion from facial expressions is critical since misalignment between skull motion and facial expressions can lead to animation models that are hard to control and can not fit natural motion. Existing methods struggle to work with sparse sets of very different expressions, such as when combining multiple units from the Facial Action Coding System (FACS). Certain approaches are not robust enough, some depend on motion data to find stable points, while others make one-for-all invalid physiological assumptions. In this paper, we leverage recent advances in neural signed distance fields and differentiable isosurface meshing to compute skull stabilization rigid transforms directly on unstructured triangle meshes or point clouds, significantly enhancing accuracy and robustness. We introduce the concept of a stable hull as the surface of the boolean intersection of stabilized scans, analogous to the visual hull in shape-from-silhouette and the photo hull from space carving. This hull resembles a skull overlaid with minimal soft tissue thickness, upper teeth are automatically included. Our skull carving algorithm simultaneously optimizes the stable hull shape and rigid transforms to get accurate stabilization of complex expressions for large diverse sets of people, outperforming existing methods.","Figure 2. Even with a headrest, the whole head moves when performing expressions (left). Stabilization is the process of estimating the rigid transform to remove the skull motion from non-rigid expression deformation (right). \Description [Figure explaining stabilization]The figure is made of 4 smaller pictures. The two on the left show a real-world picture of the capture rig with a girl neutral and AU10AU16AU25 expression with visible head motion. The two on the right show how stabilization starts with a large distance between the mesh and the skull and ends with the two well-aligned. High-resolution photorealistic avatar likeness capture for hero characters in 3D games or virtual reality face-to-face conversation or digital double for movies is a mature but complex process that still requires significant manual effort. On the other hand, major progress in the field of generative AI promises to make avatar creation as simple as generating images from text. However, there is still some way to go before generated photoreal 3D facial animations become convincing enough for use in the previously mentioned applications. See (Zollhöfer et al., 2018) for a review of the field, (Saito et al., 2024) for a recent example of a machine-learned photo-real animated avatar and (Wu et al., 2024) for an example of text-to-avatar generation. In any case, one of the best ways to improve both photoreal 3D avatar likeness capture and generative AI training data is to automate as many steps as possible in the creation pipeline while maintaining or improving the quality of the final result, especially for animations. Current avatar animation models whether they are based on blendshapes or linear blend skinning or more advanced machine learning methods first need the captured data to be normalized. The goal of likeness capture is to obtain a controllable model that is independent of the capturing modalities, for example, the head position and angle with respect to the rest of the body. A small but crucial step is to estimate the position and orientation of the skull. The skull must stay fixed when other control variables change, for example, opening the lips should not cause the upper teeth and eyes to move. Strong facial expressions are often correlated with some head motion. Removing as much correlation between control variables from the training data is desirable for next-generation neural network-based facial animation models. If the goal of likeness capture is creating a database of heads representative of the general population and resources are limited, using static expressions with combined FACS units for a larger group of people instead of using 4D data from a smaller group is a good compromise. This leads to the challenge of estimating the skull pose of various complex expressions from static captures. Existing methods require either 4D temporal data to find stable features or make simplifying assumptions about the skull shape and skin thickness, hence they are not appropriate for stabilizing an expression database including a variety of ethnicity, age, and body mass index (BMI), with a large difference in morphology and skin thickness."
https://arxiv.org/html/2411.05826v1,From Pixels to Prose: Advancing Multi-Modal Language Models for Remote Sensing,"Remote sensing has evolved from simple image acquisition to complex systems capable of integrating and processing visual and textual data. This review examines the development and application of multi-modal language models (MLLMs) in remote sensing, focusing on their ability to interpret and describe satellite imagery using natural language. We cover the technical underpinnings of MLLMs, including dual-encoder architectures, Transformer models, self-supervised and contrastive learning, and cross-modal integration. The unique challenges of remote sensing data–varying spatial resolutions, spectral richness, and temporal changes–are analyzed for their impact on MLLM performance. Key applications such as scene description, object detection, change detection, text-to-image retrieval, image-to-text generation, and visual question answering are discussed to demonstrate their relevance in environmental monitoring, urban planning, and disaster response. We review significant datasets and resources supporting the training and evaluation of these models. Challenges related to computational demands, scalability, data quality, and domain adaptation are highlighted. We conclude by proposing future research directions and technological advancements to further enhance MLLM utility in remote sensing.","Remote sensing has evolved from basic image capture to advanced systems that integrate visual and textual data processing. This progression has led to the development of multi-modal language models (MLLMs) capable of interpreting and describing satellite imagery using natural language [1, 2]. Such integration enhances the analysis of Earth observation data, making it more accessible and informative. Applying MLLMs to remote sensing data has significantly improved automated Earth observation analysis. These models support vital applications in environmental monitoring, urban planning, and disaster response by enabling more efficient and accurate extraction of information from satellite imagery [3, 4]. This advancement enhances decision-making processes across various sectors. This review examines the integration of MLLMs in remote sensing, focusing on their technical foundations, current applications, and potential future developments. The objective is to provide a comprehensive overview of how these models enhance the interpretation of satellite imagery and to identify areas for further research and application. Fig 1 presents a taxonomy of MLLM in remote sensing, categorizing the key components and their interrelationships as discussed in this review. The taxonomy is structured into six primary branches: Technical Foundations, Current Applications and Implementations, Datasets and Resources, Challenges and Limitations, and Future Directions and Conclusions. It serves as a guide for readers through the complexities of integrating and applying MLLMs to remote sensing tasks. {forest} for tree= grow=east, scale=0.75, growth parent anchor=west, parent anchor=east, child anchor=west, l sep=8mm, s sep=4mm, edge path= [\forestoptionedge,-¿, ¿=latex] (!u.parent anchor) – +(5pt,0pt) —- (.child anchor) \forestoptionedge label; , align=left, [MLLM and Remote Sensing, root, [Future Directions and Conclusions, level1, [Potential Applications [3, 5, 1, 6, 7, 8], leaf] [Research Opportunities [3, 1], leaf] ] [Challenges and Limitations, level1, [Application-Specific Challenges, level2, [Domain Adaptation [9, 4, 10, 11], leaf] ] [Technical Challenges, level2, [Computational Requirements [12, 13, 14], leaf] [Model Scalability [15, 16, 17], leaf] [Data Quality Issues [18, 19, 20, 11], leaf] ] ] [Datasets and Resources, level1, [Evaluation Metrics, level2, [CIDEr [21], leaf] [METEOR [22], leaf] [BLEU [23], leaf] ] [Training Resources, level2, [Training Frameworks [24, 25], leaf] [Pre-trained Models [3], leaf] ] [Benchmark Datasets, level2, [ChatEarthNet [26], leaf] [RS5M [9], leaf] [RSICap [27], leaf] ] ] [Current Applications and Implementations, level1, [Cross-Modal Applications, level2, [Visual Question Answering [28, 29, 30, 31], leaf] [Image-to-Text Generation [32, 33, 34, 35], leaf] [Text-to-Image Retrieval [36, 4, 10, 11], leaf] ] [Image Understanding, level2, [Change Detection [37, 38, 39], leaf] [Object Detection [40, 41, 42, 43, 44, 45, 46], leaf] [Scene Description [47, 34, 48, 49], leaf] ] ] [Technical Foundations, level1, [Remote Sensing Data Characteristics, level2, [Temporal Aspects [37, 50, 51, 52], leaf] [Spectral Information [53, 54, 55, 56], leaf] [Spatial Resolution [3, 57, 58, 59], leaf] ] [Multi-Modal Language Models, level2, [Integration Approaches [60, 61, 62, 63], leaf] [Learning Mechanisms [3, 57, 64, 65], leaf] [Architecture Components [66, 67, 68], leaf] ] ] ] Figure 1: Taxonomy of Multi-Modal Language Models for Remote Sensing"
https://arxiv.org/html/2411.05823v1,FlexCAD: Unified and Versatile Controllable CAD Generation with Fine-tuned Large Language Models,"Recently, there is a growing interest in creating computer-aided design (CAD) models based on user intent, known as controllable CAD generation. Existing work offers limited controllability and needs separate models for different types of control, reducing efficiency and practicality. To achieve controllable generation across all CAD construction hierarchies, such as sketch-extrusion, extrusion, sketch, face, loop and curve, we propose FlexCAD, a unified model by fine-tuning large language models (LLMs). First, to enhance comprehension by LLMs, we represent a CAD model as a structured text by abstracting each hierarchy as a sequence of text tokens. Second, to address various controllable generation tasks in a unified model, we introduce a hierarchy-aware masking strategy. Specifically, during training, we mask a hierarchy-aware field in the CAD text with a mask token. This field, composed of a sequence of tokens, can be set flexibly to represent various hierarchies. Subsequently, we ask LLMs to predict this masked field. During inference, the user intent is converted into a CAD text with a mask token replacing the part the user wants to modify, which is then fed into FlexCAD to generate new CAD models. Comprehensive experiments on public dataset demonstrate the effectiveness of FlexCAD in both generation quality and controllability. Code will be available at https://github.com/microsoft/CADGeneration/FlexCAD.","A computer-aided design (CAD) model is a digital representation of a 2D or 3D object. It has been widely used across numerous industries, including architecture, product design and manufacturing, facilitating precise, efficient, and innovative development Ganin et al. (2021); Khan et al. (2024). In commonly used CAD tools like SolidWorks and AutoCAD, sketch-and-extrude modeling (SEM) is prevalent. This involves drawing 2D sketches and then extruding them into 3D shapes. Compared to other representations, such as Constructive Solid Geometry (CSG) Yu et al. (2024), B-rep Xu et al. (2024), or voxel Li et al. (2023) and point cloud Khan et al. (2024)-based formats, SEM, incorporating multiple CAD construction hierarchies including sketch-extrusion, extrusion, sketch, face, loop and curve (see Fig. 3(a)), directly illustrates the drawing process of a 3D object. This allows for easy editing and reuse of CAD models, which is essential in CAD development. Recently, there is an increasing interest in developing generative models to automatically produce SEM of a CAD model111In the following, we will use CAD model to refer to SEM of a CAD model for brevity.. Specifically, DeepCAD Wu et al. (2021) focuses on uncontrollable generation, where a CAD model is generated from a randomly sampled vector. However, providing controllability, i.e., generating CAD models according to user intent, is crucial for the practical application of generative models. To address this, SkexGen Xu et al. (2022) and Hnc-cad Xu et al. (2023) implement disentangled codebooks to offer some levels of control. As each codebook encodes a particular construction hierarchy, their controllability is quite restricted. For instance, SkexGen does not allow selecting a specific sketch for modifications when a CAD model comprises multiple sketches, nor can it handle finer-grained hierarchies such as faces and loops. Hnc-cad lacks control over the topology and geometry of curves. In summary, existing methods face challenges in providing adequate controllability across all CAD construction hierarchies. Additionally, they require separate models to deliver different types of control, which is inefficient and less practical. The emergence of large language models (LLMs) offers insights for addressing these challenges. First, LLMs have exhibited remarkable success in handling diverse user queries with a single and unified model Chung et al. (2024). This phenomenon not only occurs in natural language tasks but also extends to other areas with domain-specific fine-tuning, such as human motion generation Jiang et al. (2024) and crystal material synthesis Gruver et al. (2024). Second, LLMs might have acquired CAD-related knowledge during the pre-training by learning CAD-specific codes, such as JSCAD codes Makatura et al. (2023). Third, prior to the rise of LLMs, small transformer-based models were explored for tasks like uncontrollable generation and image-to-sketch translation in the 2D sketch domain Ganin et al. (2021), showcasing the possibility of LLMs from a different perspective. In this work, we introduce FlexCAD, a unified model designed for controllable CAD generation across all hierarchies by fine-tuning LLMs. As shown in Fig. 1, FlexCAD receives the original CAD model along with the part the user wants to modify (highlighted in blue). Here, users can specify the part in any hierarchy. FlexCAD then generates multiple new CAD models, altering only the selected part. To achieve these abilities, first, FlexCAD translates a CAD model into a concise and structured text (see Fig. 3). Specifically, in each sketch, the curve type (e.g., a line) is directly represented as textual tokens. The numerical data indicating geometry (e.g., point coordinates in a line) is converted into decimal integers and then into textual tokens. A special token is added to mark the end of each hierarchy. Tokens from the finer-level hierarchy are concatenated to form the representation for the coarser-level hierarchy. We use a similar way to convert each extrusion. Consequently, unlike the one-hot representation used in Xu et al. (2022), FlexCAD provides a concise text representation of a CAD model, facilitating easier processing and understanding by LLMs. Second, FlexCAD introduces a hierarchy-aware masking strategy to enable fine-tuning LLMs for various controllable CAD generation tasks (see Fig. 2). During training, we replace a hierarchy-aware field, which contains a sequence of tokens in the CAD text, with a mask token. This field can be set adaptably to reflect various hierarchies. Then, we ask LLMs to predict the masked field. To achieve this, we design prompt templates for all hierarchies, where the mask tokens are tailored to match the corresponding hierarchies. These templates are uniformly sampled at each epoch during the fine-tuning of LLMs. In this way, we ensure that the generation tasks for all hierarchies are learned in a single and unified model. Besides, unlike Xu et al. (2022; 2023) that requires multi-stage training, FlexCAD achieves end-to-end training. During inference, a CAD model is represented as a CAD text with a mask token replacing the part the user wants to change. The masked CAD text is fed into the fine-tuned LLMs to get predictions. After infilling the masked text with these predictions, FlexCAD produces CAD texts that can be rendered into new CAD models. Overall, our contributions are: • We propose FlexCAD, a unified and versatile model for controllable CAD generation across all hierarchies, including sketch-extrusion, extrusion, sketch, face, loop and curve. • To the best of our knowledge, FlexCAD is the first to leverage LLMs for controllable CAD generation. It converts a CAD model into a brief, structured text and employs hierarchy-aware masking to fine-tune LLMs for various controllable CAD generation tasks. • We conduct extensive experiments on public datasets. Despite its simplicity, FlexCAD greatly improves generation quality and controllability, showing its effectiveness on the tasks presented in this work and indicating potential for other CAD generation scenarios."
https://arxiv.org/html/2411.05822v1,SPACE: SPAtial-aware Consistency rEgularization for anomaly detection in Industrial applications,"In this paper, we propose SPACE, a novel anomaly detection methodology that integrates a Feature Encoder (FE) into the structure of the Student-Teacher method. The proposed method has two key elements: Spatial Consistency regularization Loss (SCL) and Feature converter Module (FM). SCL prevents overfitting in student models by avoiding excessive imitation of the teacher model. Simultaneously, it facilitates the expansion of normal data features by steering clear of abnormal areas generated through data augmentation. This dual functionality ensures a robust boundary between normal and abnormal data. The FM prevents the learning of ambiguous information from the FE. This protects the learned features and enables more effective detection of structural and logical anomalies. Through these elements, SPACE is available to minimize the influence of the FE while integrating various data augmentations. In this study, we evaluated the proposed method on the MVTec LOCO, MVTec AD, and VisA datasets. Experimental results, through qualitative evaluation, demonstrate the superiority of detection and efficiency of each module compared to state-of-the-art methods.","Anomaly detection (AD) is a crucial task, particularly challenging in domains where acquiring abnormal samples for the training set is impractical, such as medical [56, 54, 44, 43] and industrial applications [55, 8, 42, 31]. The task is conventionally categorized into two groups: reconstruction-based [3, 41, 13, 32, 5, 16, 34] and embedding similarity-based methods [14, 37, 19]. The reconstruction-based methods regenerate input data and measure the reconstruction error for anomaly detection while embedding similarity-based approaches focus on the distance or similarity between embedded features to identify outliers. (a) Classification (b)Anomaly detection Figure 1: The ambiguities of applying strong augmentation for industrial data. (a) S-T (b) S-T with FE (c) SPACE Figure 2: Comparison of existing and proposed approaches for anomaly detection: (a) represents the basic S-T structure, (b) shows the structure combining S-T and FE, and (c) depicts the SPACE structure proposed. Recently, Student-Teacher (S-T) approaches have demonstrated remarkable performance in AD [6, 47, 53, 46]. This method uses an ImageNet pre-trained model as a teacher and trains the student exclusively on normal data to detect anomalies. However, collecting sufficient data for effective student model training is challenging. Data augmentation methods, widely adopted in tasks like classification and segmentation [28, 26, 30, 33, 58, 22], face limitations in industrial AD datasets [4, 59]. The main challenge is that abnormal regions are small portions of the entire image, while the rest is normal. Applying strong augmentations to normal data makes it more similar to anomalies rather than normals. Fig. 1(a) illustrates why applying strong augmentation is particularly challenging in industrial data. There are several approaches to leveraging limited augmentations for this task. RegAD [18] uses only weak augmentations on normal samples for the task. Also, CutPaste [25] and DRAEM [49] employ augmentations to create abnormals, rather than normal data, to detect anomalies. The recently released dataset, MVTec LOCO [5] requires detection not only of structural abnormalities in small areas but also of logical anomalies. These logical anomalies refer to cases where individual objects may be normal, but the overall image level is abnormal. For example, in the pushpin dataset, a situation where there is more than one pin or no pin at a location that should have one is defined as a logical abnormality, even if the pins have normal shapes. Identifying logical anomalies with existing approaches can be challenging. To address the issue, EfficientAD [2] and GCAD [5] train the student model to mimic the features of the Feature Encoder (FE). However, this may still introduce unclear features, potentially leading to false positives. To enhance both structural and logical performance by leveraging both weak and strong augmentations, we propose a framework called Spatial-aware Consistency Regularization for Anomaly (SPACE). The framework has two key elements, which are Spatial Consistency regularization Loss (SCL) and Feature converter Module (FM). SCL serves two crucial roles. One is that prevents the student model from overly mimicking the teacher model during the S-T learning process, thus mitigating overfitting. The other is that helps the student model broaden the boundaries of normal features by avoiding anomalous regions generated through augmentations and selectively learning features. FM prevents ambiguous information conveyed by the FE from being learned by the student model. This helps protect the learned features and enables more effective detection of both structural and logical anomalies. Fig. 2 shows the differences between SPACE and the existing structures. We conduct extensive evaluations of the proposed method on the MVTec AD, MVTec LOCO, and VisA datasets. Experimental results demonstrate its superiority compared to the state-of-the-art methods in both detection and the effectiveness of each module with qualitative evaluations. Our contributions in this paper are fourfold: • We introduce a novel loss function, SCL, which enables the utilization of both strong and weak augmentations for the AD task. • We propose a simple yet powerful FM that improves not only logical but also structural AD performance. • SPACE outperforms the state-of-the-art methods in terms of Image-level AUROC on the MVTec AD, MVTec LOCO, and VisA datasets. • We provide extensive ablation studies to validate the effectiveness of SCL and FM. Figure 3: The overall architecture: The method combines a structural branch, detecting fine-grained anomalies through consistency regularization, and a logical branch, focusing on shape anomalies using the FM."
https://arxiv.org/html/2411.07223v1,Grounding Video Models to Actions through Goal Conditioned Exploration,"Large video models, pretrained on massive amounts of Internet video, provide a rich source of physical knowledge about the dynamics and motions of objects and tasks. However, video models are not grounded in the embodiment of an agent, and do not describe how to actuate the world to reach the visual states depicted in a video. To tackle this problem, current methods use a separate vision-based inverse dynamic model trained on embodiment-specific data to map image states to actions. Gathering data to train such a model is often expensive and challenging, and this model is limited to visual settings similar to the ones in which data are available. In this paper, we investigate how to directly ground video models to continuous actions through self-exploration in the embodied environment – using generated video states as visual goals for exploration. We propose a framework that uses trajectory level action generation in combination with video guidance to enable an agent to solve complex tasks without any external supervision, e.g., rewards, action labels, or segmentation masks. We validate the proposed approach on 8 tasks in Libero, 6 tasks in MetaWorld, 4 tasks in Calvin, and 12 tasks in iThor Visual Navigation. We show how our approach is on par with or even surpasses multiple behavior cloning baselines trained on expert demonstrations while without requiring any action annotations.","1 Introduction Large video models (Brooks et al., 2024; Girdhar et al., 2023; Ho et al., 2022) trained on a massive amount of Internet video data capture rich information about the visual dynamics and semantics of the world for physical decision-making. Such models are able to provide information on how to accomplish tasks, allowing them to parameterize policies for solving many tasks (Du et al., 2024). They are further able to serve as visual simulators of the world, allowing simulation of the visual state after a sequence of actions (Brooks et al., 2024; Yang et al., 2024c), and enabling visual planning to solve long-horizon tasks (Du et al., 2023). However, directly applying video models zero-shot for physical decision-making is challenging due to embodiment grounding. While generated videos provide a rich set of visual goals for solving new tasks, they do not explicitly provide actionable information on how to reach each visual goal. To ground video models to actions, existing work has relied on training an inverse dynamics model or goal-conditioned policy on a set of demonstrations from the environment (Black et al., 2023; Du et al., 2024; 2023). Such an approach first requires demonstrations to be gathered in the target environment and embodiment of interest, which demands human labor or specific engineering (e.g. teleoperation or scripted policy). In addition, the learned policies may not generalize well to areas in an environment that are out-of-distribution of the training data. Recently, Ko et al. (2023) proposes an approach to directly regress actions from video models, without requiring any action annotations. In Ko et al. (2023), optical flow between synthesized video frames is computed and used, in combination with a depth map of the first image, to compute a rigid transform of objects in the environment. Robot actions are then inferred by solving for actions that can apply the specified rigid transform on an object. While such an approach is effective on a set of evaluated environments, it is limited in action resolution as the inferred object transforms can be imprecise due to both inaccurate optical flow and depth, leading to a relatively low success rate in evaluated environments (Ko et al., 2023). In addition, it is difficult to apply this approach to many robotic manipulation settings such as deformable object manipulation, where there are no explicit object transforms to compute. Figure 1: Grounding Video Models to Actions. Our approach learns to ground a large pretrained video model into continuous actions through goal-directed exploration in the environment. Given a synthesized video, a goal-conditioned policy attempts to reach each visual goal in the video, with data in the resulting real-world execution saved in a replay buffer to train the goal-conditioned policy. We propose an alternative manner to directly ground a video model to actions without using annotated demonstrations. In our approach, we learn a goal-conditioned policy, which predicts the actions to reach each synthesized frame in a video. We learn the policy in an online manner, where given a specified task, we use each intermediate synthesized frame as a visual goal for a goal-conditioned policy from which we obtain a sequence of actions to execute in an environment. We then use the image observations obtained from execution in the environment as ground-truth data to train our goal-conditioned policy. We illustrate our approach in Figure 1. In practice, directly using synthesized images as goals for exploration often leads to insufficient exploration. Agents often get stuck in particular parts of an environment, preventing the construction of a robust goal-conditioned policy. To further improve exploration, we propose to generate chunks of actions to execute in an environment given a single visual state. By synthesizing and executing a chunk of actions we can explore the environment in a more directed manner, enabling us to achieve a more diverse set of states. We further intermix goal-conditioned exploration with random exploration to further improve exploration. Overall, our approach has three contributions: (1) We propose goal-conditioned exploration as an approach to ground video models to continuous actions. (2) We propose a set of methods for effective exploration in the environment, using a combination of chunked action prediction for temporally coherent exploration along with periodic randomized actions for robust state coverage. (3) We illustrate the efficacy of our approach on a set of simulated manipulation and navigation environments."
https://arxiv.org/html/2411.07165v1,Acoustic-based 3D Human Pose Estimation Robust to Human Position,"This paper explores the problem of 3D human pose estimation from only low-level acoustic signals. The existing active acoustic sensing-based approach for 3D human pose estimation implicitly assumes that the target user is positioned along a line between loudspeakers and a microphone. Because reflection and diffraction of sound by the human body cause subtle acoustic signal changes compared to sound obstruction, the existing model degrades its accuracy significantly when subjects deviate from this line, limiting its practicality in real-world scenarios. To overcome this limitation, we propose a novel method composed of a position discriminator and reverberation-resistant model. The former predicts the standing positions of subjects and applies adversarial learning to extract subject position-invariant features. The latter utilizes acoustic signals before the estimation target time as references to enhance robustness against the variations in sound arrival times due to diffraction and reflection. We construct an acoustic pose estimation dataset that covers diverse human locations and demonstrate through experiments that our proposed method outperforms existing approaches.","Human pose estimation has diverse applications including rehabilitation support, elderly monitoring, and disaster relief efforts. Traditional approaches to 3D human pose estimation have primarily employed RGB videos and images [Martinez et al.(2017)Martinez, Hossain, Romero, and Little, Pavlakos et al.(2017)Pavlakos, Zhou, Derpanis, and Daniilidis], transient light [Isogawa et al.(2020)Isogawa, Yuan, O’Toole, and Kitani], event data [Scarpellini et al.(2021)Scarpellini, Morerio, and Del Bue, Chen et al.(2022)Chen, Shi, Ye, Yang, Sun, and Wang], radio frequency (RF)/Wi-Fi signals [Zhao et al.(2018b)Zhao, Tian, Zhao, Alsheikh, Li, Hristov, Kabelac, Katabi, and Torralba, Jiang et al.(2020)Jiang, Xue, Miao, Wang, Lin, Tian, Murali, Hu, Sun, and Su], and millimeter wave [Kong et al.(2022)Kong, Xu, Yu, Chen, Ma, Chen, Chen, and Kong, Xue et al.(2021)Xue, Ju, Miao, Wang, Wang, Zhang, and Su]. Additionally, methods that combine some of these approaches as a multimodal framework also exist [An et al.(2022)An, Li, and Ogras, Yang et al.(2024)Yang, Huang, Zhou, Chen, Xu, Yuan, Zou, Lu, and Xie]. However, optical signals face challenges such as obstruction and poor performance in low-light conditions [Lee et al.(2023)Lee, Rim, Jeong, Kim, Woo, Lee, Cho, and Kwak]. Furthermore, RGB video and images acquire high-resolution measured data, which raises concerns regarding the protection of personal information. Wireless signal-based methods are restricted in environments employing precision machinery, such as medical facilities or aircraft. One possible solution to these challenges is the utilization of acoustic signals. Acoustic signals have much longer wavelengths (meter scale) compared to optical signals (nanometer scale) or RF/Wi-Fi signals (centimeter scale). Therefore, acoustic signals are more susceptible to diffraction and less affected by obstruction. Moreover, acoustic signals offer consistent performance irrespective of lighting conditions and their usage is not hindered by the presence of precision machinery. Recent studies have explored passive acoustic sensing for gesture recognition and human pose estimation by leveraging human speech [Li et al.(2021)Li, Kang, Pei, Zhe, Zhang, He, and Bao, Ginosar et al.(2019)Ginosar, Bar, Kohavi, Chan, Owens, and Malik], ambient sounds [Gao et al.(2020)Gao, Oh, Grauman, and Torresani], or the sound of playing a musical instrument [Shlizerman et al.(2018)Shlizerman, Dery, Schoen, and Kemelmacher-Shlizerman]. These methods require sounds produced by the subjects themselves, which limits the use case. Alternatively, Shibata et al\bmvaOneDotproposed a 3D human pose estimation approach using active acoustic sensing with Time-Stretched-Pulse (TSP) signals [Shibata et al.(2023)Shibata, Kawashima, Isogawa, Irie, Kimura, and Aoki]. In this approach, a subject is positioned between a speaker and microphone (see Fig. 1(a)), where the speaker repeatedly emits TSP signals to create an acoustic field, and human poses are estimated based on how the acoustic field distorts as a subject moves. However, this method primarily relies on how the acoustic signal emitted from the speaker is obstructed by the human body to estimate the human pose. It implicitly assumes that the target subject is positioned on a straight line between the speaker and the microphone, although in the real world, meeting such constraints is extremely rare. Through the preliminary experiments, we found that the estimation accuracy significantly decreases when the subject deviates from this line, due to the difficulty of capturing subtle changes in sound signals caused by human body movements. Fig. 1(c) visualizes the acoustic features used as input to the model. The dimensions of these features are reduced by the Principal Component Analysis (PCA). The acoustic features in the settings without any subject and those shown in figures (a) and (b) are represented in different colors. From this figure, it is confirmed that the acoustic features when a person moves away from this line (blue dots) approach the features when there is no subject (green dots), indicating sound diffraction and reflection convey much less human pose information than sound obstruction caused by a person standing on the aforementioned line (red dots). To overcome this limitation, this paper proposes an acoustic-based 3D human pose estimation method, which remains effective regardless of the subject’s standing positions. While Shibata et al\bmvaOneDotprimarily relied on signal obstruction by the human body as their main clue, in this paper, we also consider cases where the position of the person is not on the straight line connecting the speaker and the microphone, as shown in Fig. 1(b). Therefore, it is necessary to consider signal diffraction and reflection from the subject as well as signal obstruction. From a technical perspective, this implies the need to solve two extremely challenging issues: (i) The relatively long wavelengths of acoustic signals tend to cause specular reflections off the surface of the human body. Consequently, the sound intensity of the reflected acoustic signals is greatly influenced by the positions of reflection and recording microphones. (ii) The arrival time of sound emitted from a speaker until it is recorded can vary due to signal diffraction and reflection. In this paper, we aim to develop methods capable of addressing these challenges. First, to enhance robustness against variations in the subject’s position, we introduce a position discriminator module. This module uses intermediate features of the pose estimation module to predict human positions, while the pose estimation module is trained to maximize the uncertainty of human positions, through adversarial training. Furthermore, to achieve robust pose estimation against changes in the arrival time of sound due to sound diffraction and reflection, we propose to introduce a reference window into the pose estimation module to consider signals prior to the target time to be estimated. Additionally, we perform data augmentation by shifting the phase of the acoustic signal, which allows for a reduction in the amount of data per subject location, enabling the preparation of a dataset that covers diverse positions. As the first attempt at non-invasive 3D human pose estimation regardless of the subject’s position, we construct a new dataset containing data from positions away from the straight line connecting the speaker and the microphones. In summary, the technical contributions of this study are as follows: (1) We have worked towards realizing a practical non-invasive 3D human pose estimation method based on active acoustic signals while subjects are placed in multiple positions. (2) We introduced a position discriminator module to enhance robustness against variations in the subject’s standing position. Additionally, we constructed a pose estimation model that considers acoustic signals prior to the estimation target time to achieve robust estimation against changes in sound arrival times due to signal diffraction and reflection. (3) To effectively learn from limited data, we performed data augmentation by shifting the phase of the acoustic signal. (4) As the first attempt to estimate non-invasively 3D human pose regardless of the subject’s position, we constructed a dataset containing data from multiple positions away from the straight line connecting the speaker and the microphones."
https://arxiv.org/html/2411.07146v1,Lost in Tracking Translation: A Comprehensive Analysis of Visual SLAM in Human-Centered XR and IoT Ecosystems,"Advancements in tracking algorithms have empowered nascent applications across various domains, from steering autonomous vehicles to guiding robots to enhancing augmented reality experiences for users. However, these algorithms are application-specific and do not work across applications with different types of motion; even a tracking algorithm designed for a given application does not work in scenarios deviating from highly standard conditions. For example, a tracking algorithm designed for robot navigation inside a building will not work for tracking the same robot in an outdoor environment. To demonstrate this problem, we evaluate the performance of the state-of-the-art tracking methods across various applications and scenarios. To inform our analysis, we first categorize algorithmic, environmental, and locomotion-related challenges faced by tracking algorithms. We quantitatively evaluate the performance using multiple tracking algorithms and representative datasets for a wide range of Internet of Things (IoT) and Extended Reality (XR) applications, including autonomous vehicles, drones, and humans. Our analysis shows that no tracking algorithm works across different applications and scenarios within applications. Ultimately, using the insights generated from our analysis, we discuss multiple approaches to improving the tracking performance using input data characterization, leveraging intermediate information, and output evaluation.","Tracking systems are fundamental to immersive Extended Reality (XR) applications, facilitating accurate and real-time navigation and mapping that are crucial for creating immersive and interactive experiences (Billinghurst et al., 2015; Klein and Murray, 2007). However, various challenges must be addressed for accurate tracking, particularly in human-centered scenarios like XR (Cadena et al., 2016; Nwankwo and Rueckert, 2023). Tracking systems in XR face additional challenges due to human factors such as unpredictable movements, inter-individual variability, contextual factors, cognitive load, occlusions from body parts, physical safety concerns, adaptive requirements, and the need for real-time interaction. These elements introduce layers of unpredictability and complexity, further complicating the tracking process. These challenges are intertwined, involving (1) the environment’s complexity, (2) various locomotion demands, and (3) the inherent limitations of sensing and tracking systems. As a result, while tracking methods are often presented as generic, their performance significantly varies across different environments, locomotion scenarios, and application settings, such as drones (Burri et al., 2016), autonomous vehicles (Geiger et al., 2013), robotics (Zhang et al., 2022), and other human-centered (Ngo et al., 2022; Vávra et al., 2017; Fu et al., 2021) and non-human-centered environments (Aqel et al., 2016; Milz et al., 2018). To understand these challenges, it is essential to examine the specific factors contributing to the complexity of broader tracking systems and how human factors add to their complexity. First, the complexity of the environment can vary with the number of objects, lighting conditions, occlusions, weather, reflective surfaces, and scene changes. For instance, tracking in a crowded urban setting with changing lighting and reflective surfaces is particularly difficult. In XR applications, this complexity is heightened by the unpredictability of human interactions and the dynamic nature of the environment. Additionally, humans can seamlessly transition between different environments, such as walking from a room to a corridor to the outdoors, without a break. This continuous movement across varied settings introduces additional challenges for tracking systems, as they must constantly adapt to new conditions and maintain accuracy. Second, locomotion differs across applications. Vehicles, robots, and humans move differently, each posing unique challenges. In human-centric applications, such as XR, abrupt movements can cause blurred images. Even in-vehicle navigation typically involves fewer abrupt movements, maintaining tracking accuracy is difficult due to varying speeds and accelerations in parking lots, urban areas, and highways (Milz et al., 2018). Tracking human movement adds another layer of complexity, as humans frequently stop, walk, and change speeds unpredictably, making consistent speed maintenance challenging. Third, sensors such as IMU sensors, depth cameras, and RGB cameras each have specific issues (Chong et al., 2015). IMU sensors can drift over time (Anwar et al., 2019; Trippel et al., 2017), depth cameras struggle with lighting and reflective surfaces (Halmetschlager-Funek et al., 2018), and RGB cameras are affected by lighting variations. In XR applications, these sensor limitations are compounded by the need to integrate data from multiple sources in real-time (Ungureanu et al., 2020). To overcome these challenges, prior work has developed tracking algorithms that leverage various computational approaches. For example, traditional SLAM methods heavily depend on carefully engineered features and manually designed system components (Campos et al., 2021; Zubizarreta et al., 2020; Schops et al., 2019; Qin et al., 2018). These methods often lack robustness, meaning they struggle to maintain accuracy and reliability in dynamic and diverse real-world scenarios where conditions can vary significantly. Factors such as changing lighting conditions, moving objects, and varying environmental textures can degrade their performance. Conversely, end-to-end learning approaches (Zhou et al., 2018; Bloesch et al., 2018; Wang et al., 2020) learn system components directly from data, which can lead to improved adaptability. However, these approaches can also face robustness issues, as they may fail to generalize when encountering unfamiliar situations or environments not represented in their training data. Hybrid approaches (Tang et al., 2020; Krishna Murthy et al., 2020; Zhou et al., 2017) aim to enhance overall performance by combining traditional and learning-based methods, leveraging both strengths. While this improves the average case performance, it often sacrifices the best-case performance the individual approaches might achieve. To comprehensively address these challenges, it is important not only to evaluate tracking systems within XR environments but also to compare their performance against other application domains, such as autonomous vehicles and drones. Evaluating tracking methods across these varied domains provides a broader perspective on the strengths and weaknesses of different approaches. Autonomous vehicles and drones present unique challenges, such as high-speed movement and indoor-outdoor environmental variability, which can inform improvements in XR tracking systems. By understanding how these systems perform in different contexts, we can derive insights that contribute to developing more robust and versatile tracking solutions that can be applied across multiple domains, including but not limited to XR. Additionally, it is crucial to examine how these algorithms behaved in their original use cases (Valente et al., 2019; Zhang et al., 2021a; Milz et al., 2018; Wang et al., 2022; Chen et al., 2020) before XR became prominent. Understanding their foundational performance and limitations in traditional applications will provide a deeper insight into their adaptability and potential enhancements needed for XR environments. This paper aims to address these challenges by systematically understanding the challenges, technical requirements bottlenecks, and potential solution directions needed to enhance tracking performance in XR and beyond. In doing so, we make the following contributions: (1) Taxonomy of challenges. We categorize the algorithmic, environmental, and locomotion-related challenges tracking systems face and their impact on XR applications. This taxonomy provides a structured overview of the difficulties inherent in visual SLAM tracking by highlighting the specific issues that need to be addressed to improve tracking performance in various human-in-loop and other Internet of Things (IoT) systems. (2) Charting tracking performance. We quantitatively evaluate the performance of state-of-the-art tracking algorithms across three distinct datasets, each representing a different application domain, environment, motion, and tracking target with unique complexities, including representative IoT systems like autonomous vehicles and drones and human-in-the-loop systems such as XR. (3) Dataset characterization. Building on observations from our quantitative evaluation across traditional, end-to-end learning-based, and hybrid tracking systems, we conduct a preliminary proof of concept data characterization. This analysis highlights the importance of understanding how dataset properties impact tracking performance and identifies potential adaptive solutions for specific environments and use cases. Unlike existing surveys that focus on specific applications or isolated aspects of tracking systems, our comprehensive evaluation empirically examines a broader range of scenarios and system types. This approach systematically presents challenges and performance bottlenecks across diverse contexts, providing a robust foundation for developing adaptable and reliable tracking solutions. These insights are especially valuable for XR applications, where tracking systems must adapt to the unpredictability of human behavior. By addressing current challenges and conducting proof-of-concept case studies, this paper serves as both a reference point for researchers and a springboard for future innovations in Visual SLAM tracking in XR and beyond."
https://arxiv.org/html/2411.07039v1,"Learning Collective Dynamics of Multi-Agent Systems 
using Event-based Vision","This paper proposes a novel problem: vision-based perception to learn and predict the collective dynamics of multi-agent systems, specifically focusing on interaction strength and convergence time. Multi-agent systems are defined as collections of more than ten interacting agents that exhibit complex group behaviors. Unlike prior studies that assume knowledge of agent positions, we focus on deep learning models to directly predict collective dynamics from visual data, captured as frames or events. Due to the lack of relevant datasets, we create a simulated dataset using a state-of-the-art flocking simulator, coupled with a vision-to-event conversion framework. We empirically demonstrate the effectiveness of event-based representation over traditional frame-based methods in predicting these collective behaviors. Based on our analysis, we present event-based vision for Multi-Agent dynamic Prediction (evMAP), a deep learning architecture designed for real-time, accurate understanding of interaction strength and collective behavior emergence in multi-agent systems.Keywords Multi-Agent System \cdot Event Camera \cdot Swarm Behavior","The systems of large number (>10) of agents, hereafter referred to as a multi-agent system, are crucial in a wide range of autonomy applications, including swarm robotics [1] and fleets of autonomous vehicles [2]. Inspired by collective behaviors observed in nature such as fish schools and bird flocks, these systems aim to achieve collective goals through the interaction among individual agents using a set of decentralized rules. Analytical flocking models such as Reynolds model [3] or Vicsek model [4] replicate collective behaviors observed in nature, but these models require precise localization which is rarely possible in the real-world applications. Therefore, real-time prediction of collective behavior, like how and when agents will achieve a collective goal, is essential for adapting the local rules and controlling multi-agent systems in a real-world environment [5, 6] as illustrated in Figure 1. This prediction is valuable in competitive settings like swarm herding [7], where understanding the system dynamics of adversarial agents can enhance strategic control. The prediction is crucial for optimizing resources and minimizing risks in complex operations, such as coordinating astrobots in telescopes [8, 9], where precise maneuvering and dense formations are important. As swarm operations scale in complexity, the prediction of collective behavior becomes increasingly critical, underscoring the need for advancement in methods for learning and control of multi-agent systems. Figure 1: Application examples of collective dynamic prediction of multi-agent system. Multi-agent dynamic prediction is helpful for both systems that are under and beyond control. Figure 2: Several methods for understanding dynamics in a multi-agent system. (a) Many previous studies in multi-agent prediction require pre-processing for detecting agents. This paper focuses on scene-based perception: compared to (b-1) frame-based methods, (b-2) event-based methods demonstrate their effectiveness in understanding multi-agent dynamics. This paper introduces the novel problem of real-time prediction of collective behavior in multi-agent dynamics from visual observations. Many previous studies on multi-agent systems assume prior knowledge of agent states and are primarily designed to predict individual agent trajectories [10, 11, 12]. A potential approach involves integrating object detection with trajectory predictors (Figure 2(a)). However, challenges exist in both object detection and trajectory prediction for understanding multi-agent dynamics. In object detection, the small size of agents and their high density in the scenes (Figure 3) hinder deep learning models from accurately determining agent positions [13, 14]. For trajectory prediction, inferring agent-wise trajectories and collective behavior from multiple trajectories becomes computationally infeasible with M\times T\times N trajectories (M: number of agents, T: sequence length, N: number of sequences). Moreover, combining a multi-agent trajectory predictor with an object detector requires substantial computational resources. Even with exact positions provided using GPS augmented for each agent (e.g., drones) instead of object detection, high energy usage and latency from continuous GPS usage, as well as the computational burden from multi-agent trajectory predictions, present challenges in employing state (position)-based multi-agent prediction models for systems with large number of agents. In contrast, we are inspired by the success of deep learning (DL) methods in learning dynamics for prediction and control from visual inputs without the need for exact state knowledge [15, 16, 17, 18]. While existing vision-to-dynamics models have been demonstrated for systems with a few agents, predicting the dynamics of collective multi-agent systems (with more than 10 agents) from vision remains an unexplored area. Our objective is to directly learn and predict the collective dynamics (not the states of each agent) of a multi-agent system from scenes captured via frame and event-based cameras, as illustrated in Figure 2(b). Figure 3: Sample flocking scenes [19, 20] and simulators (NetLogo [21], AgentPy [22]). We propose leveraging advancements in event cameras [23], which capture per-pixel brightness changes with high temporal resolution and dynamic range, to predict multi-agent collective dynamics using vision. Event-based vision has recently achieved significant improvements in object recognition, detection, and segmentation, as well as tracking high-speed objects [24, 25, 26, 27, 28]. However, applying it to understand multi-agent dynamics remains largely unexplored (Figure 2(b-2)). To address the lack of datasets for vision-based analysis of large interacting agent groups, we have created a new dataset based on Reynolds’ rule [3] using NetLogo [21] and AgentPy [22]. Frame-based inputs were generated (Figure 3) and converted to event-based data using the v2e [29] framework. Our results demonstrate that event-based methods outperform frame-based methods in capturing real-time dynamics and predicting collective behavior from early observations. Additionally, our proposed model shows superior performance in capturing time-varying dynamics compared to other event-based approaches. This paper makes following unique contributions: • This paper introduces a novel problem, vision to prediction of collective multi-agent dynamics for real-time perception and control of multi-agent system. We study deep learning models for prediction of collective multi-agent dynamics from frame- and event- based visual inputs. To the best of our knowledge, this is the first study to discuss multi-agent dynamic prediction from visual observation. • This paper performs a comparative study between frame- and event-based methods, and empirically demonstrate the advantage of event representation in learning and predicting collective behavior of multi-agent systems. Prior works have studied processing event representation for various tasks, but to the best of our knowledge, this is the first work demonstrating event-based methods for predicting multi-agent dynamics. • We present a new transformer-based deep learning architecture, event-based vision for Multi-Agent dynamic Prediction (evMAP), to learn dynamics of multi-agent systems. In particular, the model is designed to efficiently recognize dynamic changes in the multi-agent systems."
https://arxiv.org/html/2411.07025v1,Scaling Mesh Generation via Compressive Tokenization,"We propose a compressive yet effective mesh representation, Blocked and Patchified Tokenization (BPT), facilitating the generation of meshes exceeding 8k faces. BPT compresses mesh sequences by employing block-wise indexing and patch aggregation, reducing their length by approximately 75% compared to the original sequences. This compression milestone unlocks the potential to utilize mesh data with significantly more faces, thereby enhancing detail richness and improving generation robustness. Empowered with the BPT, we have built a foundation mesh generative model training on scaled mesh data to support flexible control for point clouds and images. Our model demonstrates the capability to generate meshes with intricate details and accurate topology, achieving SoTA performance on mesh generation and reaching the level for direct product usage.","Meshes, the cornerstone of 3D geometric representation, are widely utilized in various applications, including video games, cinematic productions, and simulations. Despite their widespread adoption, the meticulous craft of meshes with functional topologies demands substantial design effort. This labor-intensive process acts as a bottleneck, impeding the evolution of 3D content creation and the progress of immersive human-computer interaction. Recent research [36, 3, 45, 4, 5, 38] has tried to automate the mesh sculpting process via auto-regressive Transformers. These methods directly generate vertices and faces as human-crafted to maintain the high-quality mesh topology, yielding promising results on low-poly mesh generation. The foundation of modeling meshes with auto-regressive transformers is mesh tokenization, which converts the mesh into a one-dimensional sequence. PolyGen [26] and MeshXL [3] directly tokenize the mesh by converting the vertex coordinates to sorted triplets, each defining a tuple of quantized 3D coordinates. They learn the one-dimensional sequence with a joint or two separate auto-regressive transformers. MeshGPT [36] and its variants [45, 4] utilize an auto-encoder to convert meshes into latent sequences. MeshAnythingv2 [5] and Edgerunner [38] propose improved tokenization methods to compress vanilla mesh sequences further. However, these methods still convert meshes to relatively long sequences, limiting the ability of generative models to learn with high-poly meshes. To scale up mesh generation, a more compressive representation is demanded to extend the scope of the training data. In this paper, we propose a compressive yet efficient mesh representation called Blocked and Patchified Tokenization (BPT). BPT converts Cartesian coordinates to block-wise indexes, which makes the initial attempt to compress mesh tokens at the vertex level. Then, we select the vertices connected with most faces (i.e., the highest vertex degree) as the patch center. The faces around the center vertices are aggregated as patches, compressing mesh tokens at the face level. Our approach significantly reduces the length of the vanilla mesh sequence by around 75%, achieving the SoTA compression ratio across existing tokenization. Empowered with BPT, our mesh generative model can utilize millions of meshes with intricate details, significantly improving its performance and robustness. BPT facilitates a wide range of 3D applications. We demonstrate its effectiveness via conditional mesh generation on point clouds and images. Our model empowers even unprofessional users to produce meshes at the product-ready level. Its applicability spans a spectrum of practical domains of 3D content creation, revealing the dawn of a new era in 3D generation. Our contributions can be summarized as follows: • We introduce Blocked and Patchified Tokenization (BPT), a compressive yet effective tokenization with a state-of-the-art compression ratio of around 75%. • Empowered by BPT, we investigated the scaling of mesh data across diverse face configurations, revealing that incorporating extended data improves generation performance and robustness. • We build a mesh foundation model that supports conditional generation based on images and point clouds, enabling users to create product-ready 3D assets. Figure 3: The proposed Blocked and Patchified Tokenization (BPT). (a) We convert the coordinates from the Cartesian system to block-wise indexes. The coordinates are first separated equally into several blocks. Then, vertices inside each block are located with 1-dim indexes. (b) The nearby faces are aggregated as patches to compress the mesh sequence. Each patch center is set as the vertex connected with the most unvisited faces. Subsequently, other vertices within the patch are included in the subsequence to create a complete patch."
https://arxiv.org/html/2411.06916v1,Slowing Down Forgetting in Continual Learning,"A common challenge in continual learning (CL) is catastrophic forgetting, where the performance on old tasks drops after new, additional tasks are learned. In this paper, we propose a novel framework called ReCL to slow down forgetting in CL. Our framework exploits an implicit bias of gradient-based neural networks due to which these converge to margin maximization points. Such convergence points allow us to reconstruct old data from previous tasks, which we then combine with the current training data. Our framework is flexible and can be applied on top of existing, state-of-the-art CL methods to slow down forgetting. We further demonstrate the performance gain from our framework across a large series of experiments, including different CL scenarios (class incremental, domain incremental, task incremental learning) different datasets (MNIST, CIFAR10), and different network architectures. Across all experiments, we find large performance gains through ReCL. To the best of our knowledge, our framework is the first to address catastrophic forgetting by leveraging models in CL as their own memory buffers.","Continual learning (CL) is a paradigm in machine learning where models are trained continuously to adapt to new data without forgetting previously learned information [46, 67]. This is relevant for real-world applications such as autonomous driving [54], medical devices [64], predictive maintenance [21], and robotics [59]. In these applications, the entire data distribution cannot be sampled prior to model training and (fully) storing the arriving data is not possible. The main challenge of CL is catastrophic forgetting [38]. Catastrophic forgetting refers to problems where the performance on old data drops as new data is learned. Catastrophic forgetting typically happens because prior knowledge is encoded in the combination of the model parameters, yet updating these model parameters during training on new data leads to a change in knowledge and can thus cause the model to forget what it has learned before. To slow down forgetting, numerous methods have been proposed [51, 26, 30, 33, 37, 50, 73, 13, 66, 24, 36]. Existing methods can be categorized into two main groups [52, 15, 67]: (1) Memory-based methods rely on external memory to either store samples from old tasks [e.g., 49, 50] or store separately trained generative networks for generating old data on demand [e.g., 56, 41]. However, this is often not practical due to the fact that CL is often used because streaming data becomes too large and cannot be stored. (2) Memory-free methods change the learning objective, which may help to slow down forgetting at the cost of learning new concepts more slowly [e.g., 26, 52, 15]. Here, we contribute a different, orthogonal strategy in which we propose to leverage the trained model as its own memory buffer. In this paper, we develop a novel framework, called Reconstruction from Continual Learning (ReCL), to slow down forgetting in CL. Different from existing works, our framework leverages the implicit bias of gradient-based neural network training, which causes convergence to margin maximization points. Such convergence points essentially memorize old training data in the model weights, which allows us then to perform a dataset reconstruction attack. We then integrate this reconstruction attack into CL, where we minimize a data reconstruction loss to recover the training samples of old tasks. Crucially, our framework is flexible and can be used on top of existing CL methods to slow down forgetting and thus improve performance. We evaluate the effectiveness of our framework across a wide range of experiments. (1) We show the performance gains from our framework across three standard CL scenarios: class incremental learning, domain incremental learning, and task incremental learning. (2) We compare the performance across different datasets, including MNIST [28] and CIFAR10 [2]. (3) We compare both multi-layer perceptrons (MLPs) and convolutional neural networks (CNNs), finding that consistent performance gains can be achieved across different network architectures. (4) Lastly, we demonstrate the flexibility of our framework by combining our ReCL frameworks with several state-of-the-art CL methods, namely, EWC [26], ER [50], and UPGD [15]. Here, we find that our framework improves over the vanilla version of the CL methods and can slow down forgetting further. Across all experiments, we find a consistent performance gain from using our ReCL framework. Contributions:111Code: https://anonymous.4open.science/r/slower-forgetting/. (1) We present a novel CL framework called ReCL to slow down forgetting in CL. To the best of our knowledge, ours is the first method to slow down catastrophic forgetting in which trained models are leveraged as their own memory buffers. (2) We propose to leverage the implicit bias of margin maximization points to create training data in CL. (3) We demonstrate that our ReCL is flexible and that it consistently slows down forgetting."
https://arxiv.org/html/2411.06842v1,"Maximizing domain generalization in fetal brain tissue segmentation: the role of synthetic data generation, intensity clustering and real image fine-tuning","Fetal brain tissue segmentation in magnetic resonance imaging (MRI) is a crucial tool that supports the understanding of neurodevelopment, yet it faces challenges due to the heterogeneity of data coming from different scanners and settings, and due to data scarcity. Recent approaches based on domain randomization, like SynthSeg, have shown a great potential for single source domain generalization, by simulating images with randomized contrast and image resolution from the label maps. In this work, we investigate how to maximize the out-of-domain (OOD) generalization potential of SynthSeg-based methods in fetal brain MRI. Specifically, when studying data generation, we demonstrate that the simple Gaussian mixture models used in SynthSeg enable more robust OOD generalization than physics-informed generation methods. We also investigate how intensity clustering can help create more faithful synthetic images, and observe that it is key to achieving a non-trivial OOD generalization capability when few label classes are available. Finally, by combining for the first time SynthSeg with modern fine-tuning approaches based on weight averaging, we show that fine-tuning a model pre-trained on synthetic data on a few real image-segmentation pairs in a new domain can lead to improvements in the target domain, but also in other domains. We summarize our findings as five key recommendations that we believe can guide practitioners who would like to develop SynthSeg-based approaches in other organs or modalities.","1.1 Motivation Fetal development is a crucial period of early life that plays an important role in shaping future physiological and neurological development (Halfon et al., 2014). Monitoring brain maturation during this period and detecting deviations from typical neurodevelopment is essential for the early detection of pathologies and timely intervention (Saleem, 2014). While ultrasonography (US) is the most widely used imaging modality for fetal examination due to its broad availability and low cost, fetal magnetic resonance imaging (MRI) is increasingly gaining prominence (Glenn and Barkovich, 2006). MRI is less operator-dependent than US and offers superior soft-tissue contrast, which is particularly valuable in studying fetal brain development, with numerous studies highlighting its potential in both healthy and pathologically developing fetuses (Jakab et al., 2021; Aviles Verdera et al., 2023; Machado-Rivas et al., 2022; Shen et al., 2022). Conditions such as ventriculomegaly, spina bifida, agenesis of the corpus callosum, cytotoxic and vasogenic edema, and intracranial hemorrhages can be effectively identified and assessed through MRI (Garel and Garel, 2004; Pfeifer et al., 2019). Although automated methods for fetal brain MRI analysis show particular promise in enabling rapid and reliable scan processing (Uus et al., 2023), they struggle with the same limitations as most of the approaches developed for medical imaging: limited data availability and domain shifts, which are particularly exaggerated in the fetal population (Dockès et al., 2021; Varoquaux and Cheplygina, 2022; Guan and Liu, 2022). Figure 1: Illustration of the variations observed in fetal super-resolution reconstructed volumes causing domain shifts, presented for fetuses with different gestational ages (wGA - gestational age in weeks), MR acquisition sites and field strengths (KISPI: 1.5T, 3T, GE Healthcare; CHUV: 3T, Siemens Healthineers; KCL: 0.55T, Siemens Healthineers), and super-resolution algorithm used (IRTK, MIAL, NeSVoR) (Kuklisova-Murgasova et al., 2012; Tourbier et al., 2020; Xu et al., 2023). Both neurotypically (indicated by crosses) and pathologically (indicated by a hospital icon) developing fetuses are presented. Domain shift, or distribution shift, occurs when machine learning (ML) or deep learning (DL) models are trained on one dataset but tested on a different one with significantly varying characteristics. In MRI, domain shifts can result from variations in the imaging protocol, differences in the scanner hardware, and manufacturer disparities (Yan et al., 2020), and in fetal MRI these parameters largely vary across centers (Kebiri et al., 2022; Xu et al., 2024; Ciceri et al., 2024). Fetal MRI poses additional challenges due to the small size of the region of interest and the high variability of brain structures, which change significantly during gestation and due to pathological processes (Stiles and Jernigan, 2010; Dubois et al., 2020) creating additional heterogeneity. Furthermore, fetal MR images are typically acquired as orthogonal stacks of two-dimensional (2D) slices to mitigate motion artifacts, necessitating super-resolution (SR) reconstruction algorithms to create a high-resolution three-dimensional (3D) volume (Rousseau et al., 2010; Gholipour et al., 2010; Kuklisova-Murgasova et al., 2012; Tourbier et al., 2015; Ebner et al., 2020). SR algorithms introduce further variability through reconstruction artifacts, use of skull stripping, and differences in image appearance and contrast, adding another layer of domain shift (Xu et al., 2023). These compounded shifts complicate training models that perform well and consistently across different datasets. Figure 1 illustrates the variability of fetal MR images related to changes in the brain morphology across maturation, acquisition parameters, magnetic field strength, and SR methodology used. A recent study by Payette et al. (2024a) presenting the Fetal Tissue Annotation and Segmentation (FeTA) 2022 MICCAI challenge results shows that the aforementioned domain shifts can degrade the performance of the models when they are trained and tested on different clinical datasets. Their results reveal drops in performance occurring for white matter (WM), grey matter (GM) and ventricles segmentation in the out-of-domain (OOD) testing compared to in-domain testing. This emphasizes the importance of tackling domain shifts in fetal MRI analysis, induced by high morphological variability of the fetal brain, the MRI instrumentation and acquisition, and SR algorithms. Many techniques have been developed to address domain shifts, the most widely used being data augmentation (Shorten and Khoshgoftaar, 2019), which aims at altering the images in a controlled manner before learning to make the model more robust to heterogeneous data. Recently, however, domain randomization (Tobin et al., 2017) techniques have seen great success in the field of MR image analysis (Billot et al., 2021, 2023b; Liu et al., 2024a). These methods begin with label maps rather than real images, generating synthetic images with randomized contrasts using Gaussian mixture models, alongside common perturbations such as geometric deformations and resampling that simulate different image resolution and partial volume effects. This strategy has demonstrated excellent OOD generalization performance, especially in MRI, a modality often challenged by substantial contrast variability across centers, scanners, and sequence variants (Billot et al., 2023b). 1.2 Contributions In this work, we aim to investigate how to make the best use of the synthetic-data-driven domain randomization approaches (Tobin et al., 2017) for fetal brain MRI tissue segmentation. We study how various sources of synthetic and real data impact the model’s performance in a single-source domain generalization (SSDG) setting. In this setting, training data come from only one domain (source), and the objective is to generalize to any potential testing (target) domain. This paper presents a comprehensive examination of domain randomization methods in fetal brain MRI, with a particular focus on SynthSeg-based approaches. Our contributions can be summarized as follows: (i) We train multiple variants of SynthSeg across datasets from different acquisition sites, with varying field strengths, processing methods, and image quality. We investigate how two key components of our fetal-tailored model, FetalSynthSeg (Zalevskyi et al., 2024)—namely intensity clustering and the use of meta-classes (where tissues with similar intensities are grouped as one label)—improve OOD generalization. (ii) We explore the use of physics-based intensity generation methods, such as numerical phantoms like FaBiAN (Lajous et al., 2022), as an alternative to Gaussian mixture models for intensity sampling. This allows us to study the effectiveness of achieving contrast invariance using more physically-grounded simulations. (iii) We show how data quality significantly impacts the generalization ability of synthetic methods, emphasizing the need to carefully consider the quality of training data in domain generalization tasks. (iv) Finally, for the first time, we combine SynthSeg with modern fine-tuning approaches based on weight-space averaging (Rame et al., 2022; Wortsman et al., 2022b; Gagnon-Audet et al., 2023). We demonstrate that fine-tuning SynthSeg on even a small number of real images from a new domain can improve generalization both in-domain and out-of-domain, providing a flexible method to boost performance across varying datasets. The code, the trained model, and a Docker image with the model are available at https://github.com/Medical-Image-Analysis-Laboratory/FetalSynthSeg."
https://arxiv.org/html/2411.06810v1,JPEG AI Image Compression Visual Artifacts: Detection Methods and Dataset,"In recent years learning-based image compression methods have significantly improved and started to outperform conventional codecs. However, neural network approaches can unexpectedly introduce visual artifacts in some images. In this work, we propose methods to separately detect three types of artifacts (texture and boundary degradation, color change, and text corruption), to localize the affected regions, and to quantify the artifact strength. We consider only those regions which have been distorted just by the neural compression, while being recovered successfully by a conventional codec at a comparable bitrate. The proposed methods have been employed to collect artifacts for the JPEG AI verification model with respect to HM-18.0, the H.265 reference software. We processed about 350,000 unique images from the Open Images dataset with different compression quality parameters and created a dataset of 46,440 artifacts with validation using crowd-sourced subjective assessment. The proposed dataset and methods are valuable for testing neural network-based image codecs, identifying bugs in the codecs, and enhancing their performance. We make source code of the methods and the dataset publicly available.","Since 2018, researchers have actively pursued neural-based image compression and have demonstrated the superiority of neural codecs over classical models. In 2023 alone, more than 500 papers on neural compress methods have been published [], [], []. The capabilities of neural codecs have attracted considerable attention from both the scientific and industrial communities. As a result, the development of the JPEG AI standard for neural-based image compression has begun []. A neural codec based on the JPEG AI standard is already in development and has demonstrated 40% better performance compared to the advanced VVC intra codec. The standard is set to be published in October 2024, which will bring even more attention to the field. Neural codecs use deep learning models to compress images, that is, they learn nonlinear transformations, providing a more compact bit representation and better coding performance than traditional methods that use a predefined set of algorithms to compress data. As a result, neural codecs can produce higher quality images, but they can also introduce artifacts that are not present in traditional codecs. Therefore, the challenge arises to study the shortcomings of neural compression methods, develop metrics to detect neural compression artifacts, and create a dataset of such images. The assembled dataset can be used to test neural methods, identify their shortcomings and further improve them. Existing image quality assessment methods such as PSNR, SSIM and others are not suitable for this task for several reasons: 1. These algorithms respond poorly to small area neural compression artifacts that are nevertheless noticeable to humans, i.e., they correlate poorly with human perception of the image. 2. They are mainly aimed at obtaining a single number - an estimate of image quality, which does not allow to achieve artifact localization. 3. They are used as target metrics in most works on new compression methods, so it is incorrect to evaluate methods by them. 4. They assess image quality in general, i.e. they do not give any information about the type of artefact. The objective of this project was to develop metrics sensitive to neural artifacts, such as distortion of text, color, textures, and borders in compressed images. The metrics are designed to detect even minor distortions that significantly degrade the perception of the image. An artifact, in this context, is a distortion relative to the original image, present in an image compressed by a neural codec and absent or less noticeable in an image compressed by a classical codec. These metrics identify images that are less resilient to neural compression techniques compared to traditional algorithms. Using these metrics, a dataset was compiled, containing 53260 images with various types of artifacts and compression ratios. Additionally, a subjective verification of the automatically detected artifacts was performed to ensure accuracy. Our main contributions are: 1. Development of detection methods for three types of neural network compression artifacts. 2. Conducted subjective comparisons to validate identified artifacts. 3. Compilation of a dataset containing examples of neural compression artifacts. 4. Demonstrated higher correlation of our methods compared to existing image quality assessment methods with subjective evaluations."
https://arxiv.org/html/2411.06750v1,SynStitch: a Self-Supervised Learning Network for Ultrasound Image Stitching Using Synthetic Training Pairs and Indirect Supervision,"Ultrasound (US) image stitching can expand the field-of-view (FOV) by combining multiple US images from varied probe positions. However, registering US images with only partially overlapping anatomical contents is a challenging task. In this work, we introduce SynStitch, a self-supervised framework designed for 2DUS stitching. SynStitch consists of a synthetic stitching pair generation module (SSPGM) and an image stitching module (ISM). SSPGM utilizes a patch-conditioned ControlNet to generate realistic 2DUS stitching pairs with known affine matrix from a single input image. ISM then utilizes this synthetic paired data to learn 2DUS stitching in a supervised manner. Our framework was evaluated against multiple leading methods on a kidney ultrasound dataset, demonstrating superior 2DUS stitching performance through both qualitative and quantitative analyses. The code will be made public upon acceptance of the paper.","Ultrasound (US) imaging is widely used in various diagnostic applications [1, 2]. However, the limited field-of-view (FOV) in 2DUS poses challenges for visualizing anatomical structures and accurate downstream analysis [3]. Image stitching methods allow generating images with larger FOVs by combining multiple US images from different probe positions. Existing stitching techniques [4] can be generally categorized into feature-based methods, which use feature points and geometric descriptors, intensity-based methods, which rely on pixel intensity similarities, and tracker-based methods [5] that require additional hardware. Specifically in US stitching, Jyotirmoy et al. [6] developed a block-matching method for 3DUS. Gomez et al. [7] proposed a manifold-learning framework for 2DUS patch fusion and enhanced 3DUS stitching using a support vector machine to selectively utilize salient keypoints [8]. Wright et al. [9] introduced an iterative spatial transformer network to align 3DUS volumes to a common atlas-space. They later developed a reinforcement learning strategy leveraging additional anatomical annotation for reward function construction [10]. These methods are either not based on learning [6, 8], or they use an unsupervised learning scheme [7], or require extensive manual annotation [9, 10] which prevents the generalizability to other US datasets. Therefore, the potential of deep learning (DL) in US stitching remains underexplored. Fig. 1: SynStitch overview. We first train the SSPGM to generate a realistic 2DUS image I_{s} from an input image I with a random affine matrix \mathcal{A}. Then we freeze the SSPGM and we train ISM on the synthetic stitching pairs. Fig. 2: Overview of the SSPGM. (a) Training: The SSPGM is trained to learn outpainting given the outpainting condition C. (b) Inference: The trained SSPGM performs outpainting under the specified condition C. (c) Stitching pair generation: For a single input image I, a synthetic condition C_{s} is generated and fed into the pre-trained SSPGM, which then generates I_{s} as a stitching pair for I, with associated affine matrix \mathcal{A}. (d) SSPGM results with custom affine matrices. T, R, S indicate translation, rotation, scaling. SSPGM can generate synthetic 2DUS images with a sequence of affine transformations. In this work, we focus on stitching 2DUS images using affine transformations. Accurately stitching US images is challenging due to the uncontrolled motion of anatomical structures [10] and the presence of noise, shadows, and other artifacts. Importantly, the sector-shaped FOV poses significant challenges to both feature-based and intensity-based methods as it dominates loss/similarity functions. Feature-based methods [11, 12] can be misled by the sharp edges of the FOV. Similarly, intensity-based methods [13, 14, 15, 16] tend to align the FOVs to minimize the intensity differences, thus neglecting the alignment of anatomical content. To mitigate the effects of the FOV, previous work proposed a patch-based method [6]. However, this approach faces challenges in achieving optimal patch selection especially in the narrow parts of the FOV. To address these challenges, we propose SynStitch, a self-supervised learning (SSL) framework designed for 2DUS stitching (Fig. 1). We begin by training a synthetic stitching pair generation module (SSPGM) based on the ControlNet architecture [17, 18] that, given an input image I, generates a 2DUS image I_{s} to form a realistic stitching pair (I,I_{s}). Ideal US stitching pairs should maintain identical FOV shape, visualizing overlapping regions but with partially dissimilar anatomical content due to the movement of the US probe. Therefore, SSPGM is trained to synthesize an I_{s} that resembles I in anatomical content but includes a random affine transformation \mathcal{A} on the shared content (red contour in Fig. 1). The pair (I,I_{s}) is then fed into the image stitching module (ISM). ISM functions as a plug-and-play module, adaptable with any existing registration networks. During training, ISM aligns I (moving image) to I_{s} (fixed image) in a supervised manner using true affine transform \mathcal{A} by finding the matrix \mathcal{A}^{\prime} minimizing the difference between the aligned image I^{\prime}=I(\Phi(\mathcal{A^{\prime}})) and the ground truth I(\Phi(\mathcal{A})), where \Phi represents the affine coordinate transformation. ISM emphasizes the alignment of shared content, mitigating the impact of the FOV shape. We evaluate SynStitch against state-of-the-art feature-based and intensity-based methods on a 2DUS kidney dataset using both pixel-centric metrics and manually annotated keypoints. Our results show that SynStitch significantly (p<0.05) enhances 2DUS stitching performance."
https://arxiv.org/html/2411.06651v2,"Machine learning enabled velocity model building with uncertainty
quantification","Accurately characterizing migration velocity models is crucial for a wide range of geophysical applications, from hydrocarbon exploration to monitoring of CO2 sequestration projects. Traditional velocity model building methods such as Full-Waveform Inversion (FWI) are powerful but often struggle with the inherent complexities of the inverse problem, including noise, limited bandwidth, receiver aperture and computational constraints. To address these challenges, we propose a scalable methodology that integrates generative modeling, in the form of Diffusion networks, with physics-informed summary statistics, making it suitable for complicated imaging problems including field datasets. By defining these summary statistics in terms of subsurface-offset image volumes for poor initial velocity models, our approach allows for computationally efficient generation of Bayesian posterior samples for migration velocity models that offer a useful assessment of uncertainty. To validate our approach, we introduce a battery of tests that measure the quality of the inferred velocity models, as well as the quality of the inferred uncertainties. With modern synthetic datasets, we reconfirm gains from using subsurface-image gathers as the conditioning observable. For complex velocity model building involving salt, we propose a new iterative workflow that refines amortized posterior approximations with salt flooding and demonstrate how the uncertainty in the velocity model can be propagated to the final product reverse time migrated images. Finally, we present a proof of concept on field datasets to show that our method can scale to industry-sized problems.","Subsurface characterization of the earth’s subsurface is important for hydrocarbon exploration [48], monitoring of CO2 storage projects [14], geothermal energy projects [27] and various other applications [42]. Generally, subsurface characterization is achieved by observing the interaction between specific physical phenomena (such as electrodynamics, gravity, and acoustic wave propagation) and subsurface properties. This tomographic information is then resolved into images that are analyzed for different characterization requirements. Although our framework is generally applicable, we focus on characterization of acoustic properties by means of probing the subsurface with acoustic waves. Out of the various methods, FWI stands out as a powerful tool due to its ability to resolve high-quality acoustic images in complex structures [41]. In spite of its advantages, FWI still has shortcomings due in part to the nature of the problem but also due to the specific computational challenges that FWI brings since it requires solving many wave-equation partial differential equations (PDEs). The particular challenges of the FWI inverse problem are that the observations are corrupted by noise, are limited in frequency bandwidth, computational simulations will always contain some approximation to the true physics and due to practical engineering considerations are mostly restricted to sensing the upcoming waves at the surface so will suffer from some sort of limited aperture. All of these factors contribute to FWI’s ill-posed nature in the sense that many subsurface scenarios are capable of explaining the limited data available. Traditional workflows approach this challenge by introducing prior information to regularize the vast search space, such as Total Variation (TV) [12]. While this has served well to produce deterministic solutions, it does not express the multi-solution nature of FWI and does not represent a realistic prior of the multiscale complexity of Earth structures. We use Bayesian inference as a principled framework to combine observable data (seismic shot gathers) with prior knowledge (training samples) and output a family of Earth models that give users practical uncertainty quantification (UQ). One of the major gaps in geophysical inversion is the difficulty of computing Bayesian posterior distributions that are grounded in useful prior information and incorporate the complex physics of the problem. Existing approaches either are too expensive to scale to large-scale problems (sampling-based methods), fail to capture the full extent of uncertainty (local methods), or rely on approximations that weaken the physical fidelity of the results (convolution-based methods). As a form of variational inference [22], our approach sidesteps the computational problem of sampling the posterior distribution by optimizing instead for an amortized (read generalized) approximation to the posterior that is learned from training examples and can be applied computationally efficiently at test time with small compute (as measured by PDE solves) on various datasets."
https://arxiv.org/html/2411.06513v1,PRISM: Privacy-preserving Inter-Site MRI Harmonization via Disentangled Representation Learning,"Multi-site MRI studies often suffer from site-specific variations arising from differences in methodology, hardware, and acquisition protocols, thereby compromising accuracy and reliability in clinical AI/ML tasks. We present PRISM (Privacy-preserving Inter-Site MRI Harmonization), a novel Deep Learning framework for harmonizing structural brain MRI across multiple sites while preserving data privacy. PRISM employs a dual-branch autoencoder with contrastive learning and variational inference to disentangle anatomical features from style and site-specific variations, enabling unpaired image translation without traveling subjects or multiple MRI modalities. Our modular design allows harmonization to any target site and seamless integration of new sites without the need for retraining or fine-tuning. Using multi-site structural MRI data, we demonstrate PRISM’s effectiveness in downstream tasks such as brain tissue segmentation and validate its harmonization performance through multiple experiments. Our framework addresses key challenges in medical AI/ML, including data privacy, distribution shifts, model generalizability and interpretability. Code is available at https://github.com/saranggalada/PRISM","Deep Learning (DL) has shown immense potential in the medical field, leveraging the vast amounts of data generated in healthcare settings. Yet, several unique challenges hinder its seamless integration and large-scale adoption. A key obstacle is the fragmentation of medical data across numerous sites [1], necessitating data pooling for robust DL model training and higher statistical power [2]. However, the sensitive nature of medical data often precludes inter-site data sharing, hindering collaborative work [1]. Fig. 1: Concept illustration of the PRISM framework Federated Learning (FL) [3] offers a promising solution, enabling model training across decentralized sites without data sharing [1]. However, FL algorithms are particularly susceptible to inter-site heterogeneity and data distribution shifts, which are pervasive in biomedical imaging data [4]. These “site-specific effects” arise from non-biological factors like differences in acquisition protocols and scanner hardware [5]. Despite efforts to prospectively standardize acquisition parameters, site-specific effects persist, potentially compromising the accuracy, reliability, and interpretability of DL models—factors critical in medical decision-making [2, 6]. In MRI, site-specific effects manifest as variations in image contrast, resolution, and noise artifacts, leading to unreliable results when applying DL algorithms across different sites or scanners [2]. Moreover, data skew across sites introduces biases, hampering the development of robust, generalizable DL models [4]. Retrospective harmonization of MRI has been proposed as a solution to mitigate inter-site heterogeneity and non-biological variations [5, 7]. Statistical methods, such as intensity histogram matching, empirical Bayes estimation, and z-score normalization, have been widely used, but rely on specific data assumptions and primarily address batch effects rather than complex underlying variations [6]. In recent years, deep learning-based harmonization methods have shown superior performance in removing site-specific effects while preserving anatomical information [7]. These approaches often frame harmonization as an image translation problem, leveraging generative models such as GANs and autoencoders[7, 6]. While promising, several of these existing methods require paired data or traveling subjects (patients scanned across multiple sites), as well as large amounts of data, which are often unavailable in real-world scenarios [7]. Additionally, current approaches lack flexibility to harmonize to arbitrary target sites and easily incorporate new sites without retraining, due to their pairwise design. To address these limitations, we introduce PRISM (Privacy-preserving Inter-Site MRI Harmonization), a modular deep learning framework for harmonizing structural MRI. PRISM approaches harmonization from a content-style disentanglement perspective, leveraging contrastive learning [8] and variational inference [9] to separate anatomical and style information. This disentanglement, coupled with conditional decoding of desired anatomy and style, enables effective multi-site harmonization. Key features of PRISM include: 1. Privacy-preserving architecture enabling multi-site collaboration without data sharing. 2. Unpaired image translation capability, eliminating the need for traveling subjects or multiple image modalities. 3. Adaptability to data skew across sites. 4. Flexibility to harmonize to any target site and seamlessly integrate new sites without the need for re-training or fine-tuning, allowing for adaptable harmonization depending on downstream tasks or imaging modalities."
https://arxiv.org/html/2411.06508v1,Understanding the Role of Equivariance in Self-supervised Learning,"Contrastive learning has been a leading paradigm for self-supervised learning, but it is widely observed that it comes at the price of sacrificing useful features (e.g., colors) by being invariant to data augmentations. Given this limitation, there has been a surge of interest in equivariant self-supervised learning (E-SSL) that learns features to be augmentation-aware. However, even for the simplest rotation prediction method, there is a lack of rigorous understanding of why, when, and how E-SSL learns useful features for downstream tasks. To bridge this gap between practice and theory, we establish an information-theoretic perspective to understand the generalization ability of E-SSL. In particular, we identify a critical explaining-away effect in E-SSL that creates a synergy between the equivariant and classification tasks. This synergy effect encourages models to extract class-relevant features to improve its equivariant prediction, which, in turn, benefits downstream tasks requiring semantic features. Based on this perspective, we theoretically analyze the influence of data transformations and reveal several principles for practical designs of E-SSL. Our theory not only aligns well with existing E-SSL methods but also sheds light on new directions by exploring the benefits of model equivariance. We believe that a theoretically grounded understanding on the role of equivariance would inspire more principled and advanced designs in this field. Code is available at https://github.com/kaotty/Understanding-ESSL.","Self-supervised learning (SSL) of data representations has made remarkable progress. Existing SSL methods can be categorized into two types: invariant SSL (I-SSL) and equivariant SSL (E-SSL). The idea of I-SSL is to encourage the representation to be invariant to input augmentations (e.g., color jittering). Contrastive learning that pulls positive samples closer and pushes negative samples apart is widely believed to be a prominent I-SSL paradigm, leading to rapid progress in recent years [13, 50, 5, 80, 46, 30, 3, 32, 33, 60, 49, 14, 37, 15, 77, 64, 81]. Nevertheless, since invariant representations lose augmentation-related information (e.g., color information), their performance on downstream tasks can be hindered, as frequently observed in practice [47, 17, 34]. In view of these limitations of I-SSL, there has been a growing interest in revisiting E-SSL. Contrary to I-SSL, E-SSL learns representations that are sensitive to (or aware of) the applied transformation.111A rigorous definition of feature equivariance requires more restrictions; for example, the transformations must be invertible. In existing SSL literature, equivariance is (loosely) referred to as the property that features are aware of general input transformations (not necessarily invertible). We follow this convention in this paper. For instance, RotNet [31] is an early exemplar of E-SSL that learns discriminative features by predicting the rotation angles from randomly rotated images [43]. It has also been exploited in recent works and achieves promising improvements in conjunction with I-SSL [73, 67, 17, 18, 25, 54, 34]. Recently, E-SSL has shown potential for serving as the foundation for building visual world models [26]. Despite this intriguing progress in practice, compared to invariant SSL methods with a vast literature of theoretical analyses [58, 66, 48, 35, 68, 59, 78], there is little theoretical understanding of equivariant SSL methods. A particular difficulty lies in the understanding of the pretraining tasks, which may seem quite irrelevant to downstream classification. Taking RotNet as an example, the random rotation angle is independent of the image class, so it is unclear how rotation-equivariant representations are helpful for image classification. Generally speaking, it is unclear why, when, and how equivariant representations can generalize to downstream tasks. In view of this situation, the primary goal of this paper is not to design a new E-SSL variant, but to revisit the basic E-SSL methods and understand their essential working mechanisms. We fulfil this goal by proposing a simple yet theoretically grounded explanation for understanding general E-SSL from an information-theoretic perspective. We show that the effectiveness of E-SSL can be understood via the “explaining-away” effect in statistics, which implies an intriguing synergy effect between the image class C and the equivariant transformation A (e.g., rotation) such that almost surely, they have strictly positive mutual information when given the input X, i.e., I(C;A|X)>0 that explains the effectiveness of E-SSL. This understanding also provides valuable guidelines for practical E-SSL design with three principles to pursue a large synergy effect I(C;A|X): lossy transformations, class relevance, and shortcut pruning, as been validated on practical datasets. Theoretically, we also quantitatively analyze the influence of data transformation on the synergy effect with a theory model. Equipped with these theoretical findings, we further revisit advanced E-SSL methods in the recent literature [73, 67, 17, 18, 25, 54, 34] and find that many of these empirical successes can be well explained in our framework from two aspects: finer equivariance and multivariate equivariance. Besides, motivated by our theory, we also discover an under-explored aspect of E-SSL, model equivariance, where we show that adopting equivariant neural networks can yield strong improvements for certain E-SSL methods. These fruitful theoretical and practical merits suggest that our E-SSL theory provides a general and practically useful explanation for understanding and designing E-SSL methods that have the potential to guide future E-SSL designs."
https://arxiv.org/html/2411.06503v2,Diffusion Sampling Correction via Approximately 10 Parameters,"Diffusion Probabilistic Models (DPMs) have demonstrated exceptional performance in generative tasks, but this comes at the expense of sampling efficiency. To enhance sampling speed without sacrificing quality, various distillation-based accelerated sampling algorithms have been recently proposed. However, they typically require significant additional training costs and model parameter storage, which limit their practical application. In this work, we propose PCA-based Adaptive Search (PAS), which optimizes existing solvers for DPMs with minimal learnable parameters and training costs. Specifically, we first employ PCA to obtain a few orthogonal unit basis vectors to span the high-dimensional sampling space, which enables us to learn just a set of coordinates to correct the sampling direction; furthermore, based on the observation that the cumulative truncation error exhibits an “S”-shape, we design an adaptive search strategy that further enhances the sampling efficiency and reduces the number of stored parameters to approximately 10. Extensive experiments demonstrate that PAS can significantly enhance existing fast solvers in a plug-and-play manner with negligible costs. For instance, on CIFAR10, PAS requires only 12 parameters and less than 1 minute of training on a single NVIDIA A100 GPU to optimize the DDIM from 15.69 FID (NFE=10) to 4.37.","Figure 1: PCA-based sampling correction. We first utilize PCA to obtain a few orthogonal unit vectors that span the space of the sampling trajectories and then learn the coordinates to correct the sampling directions in regions of large curvature along the ground truth trajectory. Diffusion Probabilistic Models (DPMs) [35, 10, 39, 40, 13] have demonstrated impressive generative capabilities in various fields, including image generation [7, 29], text-to-image generation [31, 3], video generation [5], and speech synthesis [37], garnering widespread attention. DPMs introduce noise into the data through a forward process and then generate the actual output by iterative denoising during the reverse process. Compared to other generative models such as Generative Adversarial Networks (GANs) [8] and Variational Autoencoders (VAEs) [16], DPMs offer advantages in generating high-quality outputs and maintaining training stability. However, the denoising process in DPMs often requires hundreds or thousands of iterative steps, resulting in slow sampling speeds that severely hinder practical applications. Existing sampling algorithms for accelerated DPMs can be categorized into two main categories: training-free and training-based methods. Training-free methods [36, 23, 24, 2, 21, 13, 49, 50, 47] typically reduce discretization errors in each sampling iteration by designing fast solvers through analytical approaches, can achieve sampling quality comparable to the original 1000 number of function evaluations (NFE) with only 20 NFE. However, when NFE is less than 10, the accumulated truncation errors in these methods can be significantly magnified, leading to non-convergent sampling, which is ineffective and remains time-consuming. Training-based methods [32, 22, 41, 44] generally enhance sampling efficiency significantly, with the potential to achieve one-step sampling that matches the quality of the original 1000 NFE. Nonetheless, these methods often incur high computational costs and save additional model parameters. Even for the relatively simple CIFAR10 dataset, they may require over 100 A100 GPU hours [32, 41], posing challenges for practical applications. Moreover, training-based methods often establish new paths between noise and data distributions, disrupting the interpolation capability between two disconnected modes. To address these issues, we propose PCA-based Adaptive Search (PAS), a method that can correct the truncation errors of existing fast solvers with minimal training costs and learnable parameters in a plug-and-play manner. Additionally, PAS retains the interpolation capability between two disconnected modes. Inspired by previous observation that the sampling trajectories of DPMs lie in a low-dimensional subspace embedded in high-dimensional space [51], we propose employing Principal Component Analysis (PCA) to obtain a few orthogonal unit basis vectors in the high-dimensional space of the sampling trajectories, then learning the corresponding coefficients (i.e., coordinates) along each basis vector to determine the correct sampling direction. This approach avoids training neural networks to directly produce high-dimensional outputs, significantly reducing the number of learnable parameters and training costs. Furthermore, we observe that the accumulated truncation errors of existing fast solvers exhibit an “S”-shape. We have designed an adaptive search strategy to balance the sampling steps that require correction and the truncation error. This further enhances the sampling efficiency of our method while reducing the amount of parameters required for storage. We validate the effectiveness of PAS on various unconditional and conditional pre-trained DPMs, across five datasets with resolutions ranging from 32 to 512. Results demonstrate that our method can significantly improve the image quality with negligible costs. Our contributions are summarized as follows: • We propose a new plug-and-play training paradigm with about 10 parameters for existing fast DPMs solvers as an efficient alternative to the high-cost training-based algorithms, rendering the learnable parameters and training costs negligible. • We design an adaptive search strategy to reduce correction steps, further enhancing the sampling efficiency of our method and decreasing the stored parameters. • Extensive experiments across various datasets validate the effectiveness of the proposed PAS method in further enhancing the sampling efficiency of existing fast solvers."
https://arxiv.org/html/2411.06486v1,DDIM-Driven Coverless Steganography Scheme with Real Key,"Typical steganography embeds secret information into images by exploiting their redundancy. Since the visual imperceptibility of secret information is a key factor in scheme evaluation, conventional methods aim to balance this requirement with embedding capacity. Consequently, integrating emerging image generation models and secret transmission has been extensively explored to achieve a higher embedding capacity. Previous works mostly focus on generating stego-images with Generative Adversarial Networks (GANs) and usually rely on pseudo-keys, namely conditions or parameters involved in the generation process, which are related to secret images. However, studies on diffusion-based coverless steganography remain insufficient. In this work, we leverage the Denoising Diffusion Implicit Model (DDIM) to generate high-quality stego-images without introducing pseudo-keys, instead employing real keys to enhance security. Furthermore, our method offers low-image-correlation real-key protection by incorporating chaotic encryption. Another core innovation is that our method requires only one-time negotiation for multiple communications, unlike prior methods that necessitate negotiation for each interaction.","As secure communication becomes increasingly critical in the digital era, steganography—techniques for concealing sensitive information within various media—has gained substantial interest. Typical steganography embeds secret data within a carrier by modifying its statistical or perceptual attributes, yet this dependence on modification can restrict the flexibility and security of applications. In response, coverless steganography has emerged, eliminating carrier modification by generating or selecting covers based on data properties alone, thereby enhancing security and adaptability. Recently, advanced generative models have opened new avenues in coverless steganography, providing more dynamic implementations. Generative Adversarial Networks (GANs)[2] are models consisting of a generator that creates synthetic data and a discriminator that evaluates whether the data is real or fake. The generator learns to produce increasingly realistic data through this process, making GANs effective for generating images and other complex data types. The earliest attempt at generation-based coverless steganography was made by Duan and Song in 2018, who introduced using GANs to create independent images from secret data. This approach marked a significant shift, generating stego-images without altering existing media. Building on this foundation, Hu et al. applied Deep Convolutional GANs (DCGANs), incorporating image quality assessments and testing resilience against stego-analysis attacks. These advancements provided a stronger security foundation but highlighted the need for greater robustness in stego-image quality. Subsequent approaches continued refining GAN-based methods. For instance, Chen et al. developed a hybrid approach with StarGAN, Peng et al. incorporated gradient descent with GANs, Zhou et al. applied the Glow model with bijective mappings between latent and image space. Despite these advancements, challenges remain in achieving optimal security, image quality, and robustness, particularly against distortions and compression artifacts. Due to limitations in GAN-based models, this paper introduces diffusion models as an alternative for coverless steganography. Diffusion models, such as Denoising Diffusion Probabilistic Models (DDPMs)[10], transform random noise into structured data through iterative refinement, generating high-quality images in a stable, progressive manner. Building on DDPMs, Denoising Diffusion Implicit Models (DDIMs)[12] further enhance efficiency by enabling a more deterministic sampling process, making them particularly suitable for steganography by ensuring consistent information extraction from the generated images. Recent works have extended the application of diffusion models to steganography, focusing on embedding different types of information into images. One scheme uses diffusion models to embed text within images[6], showcasing the model’s ability to integrate textual data with visual content. CRoSS[1] applies diffusion models to embed the secret image into a generated image. While these works demonstrate diffusion’s adaptability, both rely on pseudo-keys as mentioned above to encrypt and extract the embedded content. Besides, these methods require frequent key negotiations for communication sessions, limiting their practical applicability in secure communication. In this paper, we propose a new coverless steganography method that creates high-quality stego-images by utilizing the DDIM. Our diffusion-based method delivers similar or better visual fidelity than previous GAN-based methods[5], which frequently suffer from poor image quality. Additionally, our method overcomes the problem of pseudo-key dependency present in existing diffusion-based systems by combining reversible steganography and chaotic encryption. Rather than using pseudo-keys, we employ a steganographic real key with a wide key space, improving security and confidentiality in secure communication. Our contributions can be summarized as follows: • Diffusion-Based Steganography: We propose a practical coverless steganography method utilizing DDIM, which generates high-quality stego-images with enhanced visual fidelity compared to GAN-based methods. • Removal of Pseudo-Keys: Unlike previous methods that rely on pseudo-keys associated with the secret images, our method leverages real keys with a large key space, significantly boosting security. • Chaotic Encryption for Key Protection: Our method offers real-key protection with low correlation to the secret images, enhancing security and confidentiality by incorporating chaotic encryption. • Efficient Key Negotiation: Our method requires only one-time key negotiation for multiple communications, improving efficiency over prior methods that necessitate negotiation with each interaction."
https://arxiv.org/html/2411.06353v1,Deep Active Learning in the Open World,"Machine learning models deployed in open-world scenarios often encounter unfamiliar conditions and perform poorly in unanticipated situations. As AI systems advance and find application in safety-critical domains, effectively handling out-of-distribution (OOD) data is crucial to building open-world learning systems. In this work, we introduce ALOE, a novel active learning algorithm for open-world environments designed to enhance model adaptation by incorporating new OOD classes via a two-stage approach. First, diversity sampling selects a representative set of examples, followed by energy-based OOD detection to prioritize likely unknown classes for annotation. This strategy accelerates class discovery and learning, even under constrained annotation budgets. Evaluations on three long-tailed image classification benchmarks demonstrate that ALOE outperforms traditional active learning baselines, effectively expanding known categories while balancing annotation cost. Our findings reveal a crucial tradeoff between enhancing known-class performance and discovering new classes, setting the stage for future advancements in open-world machine learning.","Modern machine learning models have achieved remarkable progress by leveraging large amounts of labeled data LeCun et al. (2015); He et al. (2016); Khosla et al. (2020). Despite this success, most models are developed for closed-world settings, assuming that both training and test data originate from the same distribution. However, this assumption does not align with real-world environments, where models are inevitably encounter out-of-distribution (OOD) data with previously unseen classes Hendrycks & Gimpel (2022); Hendrycks et al. (2022); Salehi et al. (2022). Constrained by their fixed class boundaries, traditional models often struggle to generalize effectively to these novel classes, limiting their adaptability in open-world scenarios. Furthermore, obtaining human supervision for these novel examples in open-world scenarios is often time-consuming and costly, posing a significant challenge to model adaptation and improvement. Active learning, which iteratively selects the most informative examples for labeling, has emerged as a promising approach to address the expensive nature of gathering human supervision. By prioritizing examples that provide the most significant information gain, active learning can enhance the model’s learning efficiency while reducing the need for extensive human annotation. This approach is particularly valuable in open-world scenarios, where the high annotation cost and time-consuming nature of labeling make traditional supervised learning methods impractical. Despite its potential, existing active deep learning algorithms (see (Zhan et al., 2022; Zhang et al., 2024a) for overviews) have rarely been studied under open-world scenarios, particularly those involving novel classes and imbalanced data distributions. In this work, we addresses this gap by developing a novel active learning algorithm that integrates energy-based OOD detection techniques to handle the complexity of open-world environments. Our method is designed for multi-class classification tasks in the open-world, where the model encounters both known and unknown classes after deployment. Of the few works that study active learning under open-world settings, existing methods are often tailored to specific vision tasks, which result in highly specialized algorithm designs. When these methods are simplified for more generic problems like classification, they often reduce to basic uncertainty or diversity sampling techniques. In contrast, our approach in this paper offers a more comprehensive and versatile solution. By bridging OOD detection and diversity in our sampling strategy, we provide a more comprehensive sampling strategy compared to approaches that focus on only one of these aspects. We propose ALOE (Active Learning in Open-world Environments), a two-stage algorithm tailored for open-world active learning. ALOE addresses the unique challenges of class discovery in open-world settings by dynamically incorporating new OOD classes through a structured sampling strategy: In the first stage, ALOE performs diversity sampling to select a representative set of examples from the data pool. This sampling ensures broad coverage of the data distribution, capturing a range of potential new classes and concepts. By focusing on diversity, the algorithm increases the probability of identifying and learning from rare or infrequent classes that may otherwise be overlooked in a random sampling approach. The second stage leverages energy scoring function to rank examples within each cluster, prioritizing instances most likely to belong to unknown OOD classes. The energy-based OOD detection provides a unified framework that distinguishes between in-distribution (ID) and OOD examples with greater resilience to model overconfidence. By focusing annotation efforts on these high-priority examples, ALOE accelerates the discovery and learning of new classes, even when the annotation budget is limited. To empirically validate our approach, we conducted experiments on long-tail imbalanced image classification datasets. This choice of datasets is motivated by the prevalence of long-tail distributions in real-world scenarios, where rare classes or concepts are often underrepresented. Such imbalanced distributions make random sampling ineffective in discovering unknown classes, particularly those with small sample sizes. Our experimental results demonstrate the effectiveness of our approach. Compared with random sampling, on ImageNet-LT, ALOE saves 70% of annotation cost to achieve the same accuracy. In six out of six experimental settings (5.2.2 and 5.2.3), we observed that our algorithm performs the best comparing to all baseline experiments. These highlights underscore the potential of our method to significantly enhance model adaptation in open-world environments. Lastly, our work reveals a novel tradeoff between improving performance on known classes and discovering new ones. This finding opens up an important avenue for future research, as balancing these competing objectives is crucial for developing truly adaptive AI systems. In summary, our proposed active learning algorithm offers a promising solution for adapting machine learning models to new, previously unseen conditions in open-world environments. By efficiently incorporating unknown instances and minimizing human annotation efforts, our approach paves the way for more robust and adaptable AI systems capable of adapting to dynamic, real-world scenarios."
https://arxiv.org/html/2411.06346v1,Activation Map Compression through Tensor Decomposition for Deep Learning,"Internet of Things and Deep Learning are synergetically and exponentially growing industrial fields with a massive call for their unification into a common framework called Edge AI. While on-device inference is a well-explored topic in recent research, backpropagation remains an open challenge due to its prohibitive computational and memory costs compared to the extreme resource constraints of embedded devices. Drawing on tensor decomposition research, we tackle the main bottleneck of backpropagation, namely the memory footprint of activation map storage. We investigate and compare the effects of activation compression using Singular Value Decomposition and its tensor variant, High-Order Singular Value Decomposition. The application of low-order decomposition results in considerable memory savings while preserving the features essential for learning, and also offers theoretical guarantees to convergence. Experimental results obtained on main-stream architectures and tasks demonstrate Pareto-superiority over other state-of-the-art solutions, in terms of the trade-off between generalization and memory footprint.111Code: https://github.com/Le-TrungNguyen/NeurIPS2024-ActivationCompression.git","Figure 1: We compress the activations that will be later employed for backpropagation. Recent advances in Deep Learning have enabled Deep Neural networks to be used as an efficient solution for a wide variety of use cases, including computer vision [21, 39, 30], speech recognition [8, 31] and natural language processing [45, 42]. Much of this performance improvement is linked to the exponential increase in the number of parameters in the neural architectures. According to Sevilla et al., the release of AlphaGo [47] in late 2015 marks the advent of a new era, which they call the “Large Scale Era”, in reference to the computational cost of training doubling every 8 to 17 months [38]. At the root of exponential growth in neural network size is the improvement in hardware capabilities, particularly those designed for large-scale parallel computing, such as GPUs and TPUs [1]. While this trend demonstrates the strength of neural networks as a powerful generalization tool in many fields, it goes in the opposite direction when it comes to environmental concerns [41], making the deployment of newer architectures increasingly difficult. This is particularly true for edge devices such as mobile phones and embedded systems, which cannot afford the high computing or memory costs. To address these challenges, three interconnected factors must be taken into account: power consumption, memory usage, and latency. When considering larger neural networks with more layers and nodes, reducing their storage and computational cost becomes essential, especially for certain real-time applications such as edge computing. In addition, recent years have seen significant advances in the fields of intelligent edge and embedded intelligence, creating unprecedented opportunities for researchers to address the fundamental challenges of deploying deep learning systems on edge devices with limited resources (e.g. memory, CPU, power, bandwidth). Efficient deep learning methods can have a significant impact on distributed systems and embedded devices for artificial intelligence. Since training is supposed to take place in the cloud, most research on model compression and acceleration is specifically focused on inference [7]. There is, however, an emerging area of research concerning on-device training, which represents a decisive advance in the field of artificial intelligence, with considerable implications for a variety of practical situations [17]. Models trained offline on a dataset built at one point in time tend to fall victim to data drift when deployed “in the wild” [36]. Its combination with online learning strategies has the potential to enable continuous model improvement after deployment [14], thus adapting the model predictions to observed evolutions in the data distribution. We can illustrate this with the example of sensors in autonomous vehicles, where deep learning models must correctly classify vehicles with new designs never seen in the training set. Other advantages of on-device learning include security and privacy. By processing data locally, sensitive information remains more secure and less susceptible to data breach, a major concern in applications such as healthcare. The main challenge limiting the feasibility of on-device learning lies in the computational demands of the backward pass, as gradient computation and parameter updates are significantly more resource-intensive than the forward pass (Appendix A.1). On embedded devices, memory and computation limitations act as strict budgets that must not be exceeded. Some approaches address the memory constraints by exploring alternatives to traditional backpropagation, including unsupervised learning for image segmentation [51], the Forward-Forward algorithm [16], and PEPITA [34]. While promising, these methods typically fall short of backpropagation-based techniques in terms of performance. A pioneering effort by Lin et al., demonstrated that fine-tuning a deep neural network within a 256 kB of memory is feasible by selectively updating a sub-network, achieving a good results [25]. In a complementary approach, Yang et al. proposed reducing the number of unique elements in the gradient map through patch-based compression of the input and gradients of a given layer with respect to the output, thereby lowering memory costs and speeding up the learning process [53]. Inspired by tensor decomposition methods, we propose a method that compresses activation maps, reducing the memory demands for backpropagation while maintaining the deep neural network’s generalization capability(Fig. 1). Our approach adaptively captures the majority of tensor variance, offering guaranteed accuracy in the gradient estimation. The key contributions of our work are as follows: • We propose to exploit powerful low-rank approximation algorithms to compress activation maps, enabling efficient on-device learning with controlled information loss (Sec. 3.2 and Sec. 3.3). • We provide a theoretical foundation for our method, along with an error analysis demonstrating that high compression ratios are achievable with limited performance degradation (Sec. 3.4). • We extensively explore a diverse experimental landscape, demonstrating the generalization capacity of our proposed algorithm (Sec. 4)."
https://arxiv.org/html/2411.06308v1,Exploring Out-of-distribution Detection for Sparse-view Computed Tomography with Diffusion Models,"Recent works demonstrate the effectiveness of diffusion models as unsupervised solvers for inverse imaging problems. Sparse-view computed tomography (CT) has greatly benefited from these advancements, achieving improved generalization without reliance on measurement parameters. However, this comes at the cost of potential hallucinations, especially when handling out-of-distribution (OOD) data. To ensure reliability, it is essential to study OOD detection for CT reconstruction across both clinical and industrial applications. This need further extends to enabling the OOD detector to function effectively as an anomaly inspection tool. In this paper, we explore the use of a diffusion model, trained to capture the target distribution for CT reconstruction, as an in-distribution prior. Building on recent research, we employ the model to reconstruct partially diffused input images and assess OOD-ness through multiple reconstruction errors. Adapting this approach for sparse-view CT requires redefining the notions of ""input"" and ""reconstruction error"". Here, we use filtered backprojection (FBP) reconstructions as input and investigate various definitions of reconstruction error. Our proof-of-concept experiments on the MNIST dataset highlight both successes and failures, demonstrating the potential and limitations of integrating such an OOD detector into a CT reconstruction system. Our findings suggest that effective OOD detection can be achieved by comparing measurements with forward-projected reconstructions, provided that reconstructions from noisy FBP inputs are conditioned on the measurements. However, conditioning can sometimes lead the OOD detector to inadvertently reconstruct OOD images well. To counter this, we introduce a weighting approach that improves robustness against highly informative OOD measurements, albeit with a trade-off in performance in certain cases.","Computational imaging is among many application areas that have gained attention with the uptake of diffusion models. CT reconstruction is a typical computational imaging challenge, aiming to reconstruct images from raw measurements (i.e., X-ray projections of object slices acquired from several directions). Given some projection measurements \mathbf{y} and a forward measurement operator \mathbf{A}, the goal is to retrieve \mathbf{x}\in\mathbb{R}^{n} from the forward model: \mathbf{y}=\mathbf{Ax}+\mathbf{\eta},\quad\mathbf{y}\in\mathbf{R}^{m},\quad% \mathbf{A}\in\mathbb{R}^{m\times n}, (1) where \mathbf{\eta}\in\mathbb{R}^{m} denotes a noise vector. As high doses of radiation are usually not well-tolerated, achieving to reconstruct high-quality images from significantly reduced number of projections (i.e., m\ll n) is crucial. This requires addressing the ill-posed nature of the problem, since without the utilization of prior knowledge on \mathbf{x}, exact retrieval of it is not possible (\mathbf{x}\mapsto\mathbf{y} is many-to-one). Some recent works leveraged the diffusion model as prior [1, 2, 3, 4] to tackle general inverse imaging problems from Bayesian viewpoint. They rely on the idea of training a diffusion model on a set \left\{\mathbf{x}^{(1)},\mathbf{x}^{(2)},\cdots,\mathbf{x}^{(N)}\right\}\sim p% (\mathbf{x}) to implicitly learn p(\mathbf{x}), so that one can sample from it. They simply incorporate the measurement process to the iterative sampling scheme to approximate conditional samples (i.e., samples from posterior p(\mathbf{x}|\mathbf{y})) by maintaining consistency between the prior and the measurement model. This approach suggests a better generalization capability than a conditional generative model, as the model remains unaware of the measurement process, thereby eliminating the need for retraining when the measurement parameters change. Moreover, since the model is not trained on a specific task, it can flexibly be adapt to different tasks. Figure 1: Schematic of sparse-view imaging setup and reconstruction framework augmented with out-of-distribution detector. Another application area for diffusion models is out-of-distribution (OOD) detection. This versatile task serves various purposes, such as ensuring reliability and aiding in inspection. Due to the diverse nature of OOD data in most scenarios, it is impractical to formalize OOD detection as a supervised classification task. Unsupervised OOD detection has garnered significant attention, as it relies solely on ID data for training (i.e., one-class learning), mitigating generalization issues stemming from underrepresented OOD data. The literature is dominated by two approaches: likelihood-based and reconstruction-based OOD detection. The former involves training a density estimation model, allowing detection of OOD samples by assigning them low likelihoods. However, recent studies [5, 6, 7, 8, 9] have shown that these models often assign unexpectedly higher probabilities to the OOD samples from an unrelated distribution (e.g., a model trained on Fashion-MNIST assigns higher likelihoods to the MNIST samples than those from Fashion-MNIST.) The latter, on the other hand, involves reconstructing input data using a generative model and comparing the reconstructed samples with the original ones. This method relies on the assumption that the model struggles to accurately reconstruct OOD samples due to their dissimilarity to the training distribution. Therefore, when presented with OOD samples, the reconstruction errors are expected to be higher compared to in-distribution samples. Due to the superiority of diffusion models in terms of mode coverage and sample quality compared to other generative models, they become a promising option for reconstruction-based OOD detection [10, 11, 12, 13, 14, 15]. An accurate and automated CT reconstruction pipeline is a sought-after target for many clinical and industrial practices. The aforementioned diffusion models appear to be aligned with this objective but necessitate an OOD detection mechanism, either to prevent reconstructions in situations that surpass the model’s generalization capacity or to purposely inspect abnormalities. Despite the extensive literature demonstrating the potential of diffusion models as unsupervised inverse problem solvers and OOD detection tools separately, to the best of our knowledge, this will be the first study at their intersection. Moreover, from a broader point of view, there is a gap in the literature regarding the applicability of the reconstruction-based OOD detection paradigm to inverse problems, where the inputs are incomplete in some manner. As far as we know, the recent study [16] stands as the only research employing a reconstruction-based OOD detection approach for image reconstruction, where the models are trained in supervised manner and OOD detection task is approached from an uncertainty estimation perspective. So, the main focus of this paper is exploring the use of diffusion models for OOD detection in unsupervised sparse-view CT reconstruction. Our goal is to address questions such as: Can a diffusion model trained to capture the prior distribution for a sparse-view CT reconstruction task be also utilized for OOD detection as shown in Fig. 1? Given the in-distribution dataset of full-view images, how well can this model generalize to the distribution shift caused by the sparse-view mappings of in-distribution images? Does conditioning on the measurements aid or hinder the OOD detection process? How should we redefine the notion of reconstruction error, given that its usual definition is impractical in the case of sparse-view tomography? We believe that the answers to these questions are crucial for better understanding and may provide insights for future research directions on reliable CT reconstruction pipelines. We conduct all experiments on MNIST images [17] within a one-class experimental setup. In this setup, the diffusion model is trained on one digit, treating all other digits as OOD. The reader should be aware that the term “reconstruction” throughout the paper may refer to either CT reconstruction or reconstruction for OOD detection. Although these tasks may occasionally coincide, context is crucial for understanding the intended meaning. We will specifically use “CT reconstruction” to denote the former. Similarly, the term “data” can sometimes be ambiguous. It usually refers to projection data or measurements, as in “data fidelity,” but when used in phrases like “data manifold” and “training data,” it refers to images."
https://arxiv.org/html/2411.06236v2,Zero-Shot NAS via the Suppression of Local Entropy Decrease,"Architecture performance evaluation is the most time-consuming part of neural architecture search (NAS). Zero-Shot NAS accelerates the evaluation by utilizing zero-cost proxies instead of training. Though effective, existing zero-cost proxies require invoking backpropagations or running networks on input data, making it difficult to further accelerate the computation of proxies. To alleviate this issue, architecture topologies are used to evaluate the performance of networks in this study. We prove that particular architectural topologies decrease the local entropy of feature maps, which degrades specific features to a bias, thereby reducing network performance. Based on this proof, architectural topologies are utilized to quantify the suppression of local entropy decrease (SED) as a data-free and running-free proxy. Experimental results show that SED outperforms most state-of-the-art proxies in terms of architecture selection on five benchmarks, with computation time reduced by three orders of magnitude. We further compare the SED-based NAS with state-of-the-art proxies. SED-based NAS selects the architecture with higher accuracy and fewer parameters in only one second. The theoretical analyses of local entropy and experimental results demonstrate that the suppression of local entropy decrease facilitates selecting optimal architectures in Zero-Shot NAS.","Neural architecture search (NAS) algorithms automate the process of designing architectures, overcoming the limitations in terms of manual design efficiency. Nevertheless, the evaluation of networks relies on computationally expensive network training, which makes NAS dependent on high-performance GPUs (Real et al. 2019; Liu et al. 2018; Liu, Simonyan, and Yang 2019). Numerousds studies have been conducted to improve the search speed of NAS metho (Ren et al. 2021; Liu et al. 2021b; Jaafra et al. 2019; Baymurzina, Golikov, and Burtsev 2022; Cai et al. 2018). Zero-Shot NAS (Li et al. 2024) is a promising paradigm in accelerating network performance evaluation. It leverages geometric features derived from the network parameters or gradient landscape as evaluation criteria without complete training of networks. Challenges persist despite the effectiveness of existing zero-cost proxies. (i) Current zero-cost proxies run networks or invoke backpropagations, which is time-consuming for large search spaces (e.g., as shown in Example 1). (ii) Most proxies rely on the input data. The reliance on input data inadvertently underestimates the significance of architecture properties. To overcome these challenges, we propose a topology-based proxy to extract the architecture properties as evaluation criteria. Objectives. The overriding objective of this study is to develop a zero-cost proxy distinguished by its speed and reduced consumption of floating-point operations. This proxy is supposed to be data-free and network-running-free. Method and Results. From the perspective of entropy decrease, this study explores the impact of network architecture on the local entropy of feature maps. We prove that decreasing the local entropy of feature maps degrades specific features to biases, reducing network performance. Furthermore, irrational operation settings (including convolution, pooling, and skip connection) are proven to trigger local entropy decrease. Based on the above analyses, the suppression of local entropy decrease (SED) proxy is proposed to quantify the suppression of local entropy decrease as a proxy for network performance. The computation of SED, which is data-free and network-running-free, relies solely on architecture topologies to accelerate the evaluation. A schematic of SED calculation is provided in Figure 1. Experimental results demonstrate that SED outperforms most state-of-the-art (SOTA) zero-cost proxies on multiple benchmarks. In terms of efficiency, SED significantly accelerates architecture evaluation, taking only 3.4e-5 seconds to evaluate an architecture in NATS-Bench-TSS. In contrast, existing SOTA methods (e.g., grad_norm (Abdelfattah et al. 2021)) take at least 0.54 seconds. In terms of architecture selection, SED selects architectures ranked 3rd, 1st, and 24th for NATS-Bench-TSS using CIFAR-10, CIFAR-100, and ImageNet16-120, respectively. The Spearman’s \rho of SED achieves 0.09 higher than the SOTA proxy on NAS-Bench-301 out of 2,000 randomized architectures. Moreover, the SED-based NAS selects the architectures with the highest accuracy among the compared proxies in complete NAS tasks. For example, SED-based NAS selects architecture with 81.09% test accuracy, which is 0.42% higher than the SOTA proxies. Contributions. Compared with previous work, this study utilizes network architecture topologies to construct the proxy rather than network parameters and input data. The main contributions of this study are as follows: • Theoretically, we prove that specific network architectures reduce local entropy. Moreover, for classification tasks, entries in feature maps degrade to biases as the local entropy decreases, thereby reducing network performance. • Based on the theoretical analyses, the suppression of local entropy decrease caused by architectures is quantified as a proxy for network performance. This proxy outperforms SOTA proxies on multiple benchmarks, reducing the time consumed in architecture evaluation by three orders of magnitude compared to SOTA proxies. Example 1 Evaluation of a single architecture in the NATS-Bench-TSS benchmark using grad_norm (Abdelfattah et al. 2021) on a 3090 GPU takes only 0.54 seconds. NATS-Bench-TSS consists of 4 nodes and 5 related operations, totaling 15,625 architectures, and it takes 2.3 GPU hours to evaluate all the architectures in NATS-Bench-TSS. The architecture space with 5 nodes and 5 related operations, using the exact construction mechanism as NAS-Bench-TSS, contains 9,765K architectures. However, evaluating this architecture space takes at least 1,480 GPU hours based on grad_norm."
https://arxiv.org/html/2411.06184v1,Alleviating Hyperparameter-Tuning Burden in SVM Classifiers for Pulmonary Nodules Diagnosis with Multi-Task Bayesian Optimization,"In the field of non-invasive medical imaging, radiomic features are utilized to measure tumor characteristics. However, these features can be affected by the techniques used to discretize the images, ultimately impacting the accuracy of diagnosis. To investigate the influence of various image discretization methods on diagnosis, it is common practice to evaluate multiple discretization strategies individually. This approach often leads to redundant and time-consuming tasks such as training predictive models and fine-tuning hyperparameters separately. This study examines the feasibility of employing multi-task Bayesian optimization to accelerate the hyperparameters search for classifying benign and malignant pulmonary nodules using RBF SVM. Our findings suggest that multi-task Bayesian optimization significantly accelerates the search for hyperparameters in comparison to a single-task approach. To the best of our knowledge, this is the first investigation to utilize multi-task Bayesian optimization in a critical medical context.","Against the backdrop that lung cancer continues to be the leading cause of cancer mortality worldwide, precisely differentiation between benign and malignant pulmonary nodules has always been the focus of machine learning and clinical medicine [1, 2, 3]. Machine learning based pulmonary nodule diagnosis has opened up new opportunities to relax the limitation from physicians’ subjectivity, experiences and fatigue, thereby speeding up the diagnostic process and saving more lives [4]. Distinguishing malignant and benign nodules traditionally involves tasks such as image preprocessing, feature learning and classification model construction, which affect each other [5]. For instance, the radiomic features extracted in the feature learning are easily affected by the image discretization strategy in the image preprocessing, which in turn affects the diagnostic accuracy of the classification model [6]. Therefore, to achieve better diagnostic accuracy, it is necessary to find a set of optimal policy combinations from the policy space which defines the approaches and/or their parameters for implementing each task. In reality, related work is limited to only finding combinations of strategies and parameters for a specific task without explicitly acknowledging inter-task relationships. Ignorance of inter-task relationships frequently induces repetitive algorithm-tuning work that is dependent on expert knowledge, and impacts of strategies and parameters used in this task on subsequent tasks are unpredictable. Furthermore, it induces huge overhead accompanied by time-expensive medical image processing. To quickly evaluate the performance of the diagnostic model under different policy combinations, alleviating the burden of model hyperparameter tuning, we, consequently, charted an alternative route by leveraging multi-task Bayesian optimization (MTBO) [7, 8]. By appreciating the useful inter-task relationships, an accelerated search could be expected by transferring these commonalities among tasks, and thereby possibly alleviate time-consuming repeated searches. On the way to this destination, we raise two challenging questions: Can multiple medical classification tasks be solved simultaneously by sharing knowledge across tasks, rather than optimizing one by one? By using multi-task Bayesian optimization, are speed-ups in search and less loss achievable? Our contributions are two-fold, and Figure 1 summarizes the complete flow of this study. Figure 1: Workflow of the proposed MTBO based SVMs hyperparameter-tuning method on pulmonary nodule classification. The collected whole body CT scans are first preprocessed to delineate the pulmonary nodules. Then, the segmented nodules are discretized via multiple strategies. After that, for the result of each discretization strategy, various types of features are extracted as the input to a specific SVM. Finally, a multi-task Gaussian process is established through the correlation between tasks, the query point is selected by maximizing the acquisition function, and the optimal hyperparameters are determined by iterative query. For the first question, we generate multiple medical tasks (distinguishing benign from malignant pulmonary nodules) through different image discretization strategies. Then the RBF SVMs are used as the classification models, and the hyperparameters of multiple SVM classifiers are tuned simultaneously using MTBO to obtain the optimal model. This is the first study to apply MTBO technology to the medical field. For the second question, the proposed algorithm MTBO for multiple SVM classifiers efficiently finds suitable hyperparameters for the diagnostic models and generally outperforms the performance of single-task Bayesian optimization. Moreover, in the optimization of multi-tasks that require a lot of time to evaluate, MTBO has greater application potential. The application of machine learning in medical imaging still faces daunting challenges. Our research aims to narrow the gap between machine learning research and clinics-oriented decision-making practice."
https://arxiv.org/html/2411.06181v1,EPI-NAF: Enhancing Neural Attenuation Fields for Limited-Angle CT with Epipolar Consistency Conditions,"Neural field methods, initially successful in the inverse rendering domain, have recently been extended to CT reconstruction, marking a paradigm shift from traditional techniques. While these approaches deliver state-of-the-art results in sparse-view CT reconstruction, they struggle in limited-angle settings, where input projections are captured over a restricted angle range. We present a novel loss term based on consistency conditions between corresponding epipolar lines in X-ray projection images, aimed at regularizing neural attenuation field optimization. By enforcing these consistency conditions, our approach, Epi-NAF, propagates supervision from input views within the limited-angle range to predicted projections over the full cone-beam CT range. This loss results in both qualitative and quantitative improvements in reconstruction compared to baseline methods.","In 3D limited-angle computed tomography (LACT), the goal is to reconstruct a radiodensity voxel grid from X-ray projection images acquired over a restricted range of angles. This inverse problem is ill-posed and presents significant challenges in various applications, such as electron microscopy [1], breast tomosynthesis [2], C-arm CT [3], and non-destructive testing [4]. Since the 1970s, several paradigms for CT reconstruction have been developed. Traditional analytical reconstruction methods, such as the Feldkamp-Davis-Kress (FDK) algorithm [5], often introduce significant artifacts into reconstructed images in limited-angle scenarios. Iterative reconstruction techniques, such as ART [6] and SART [7], along with more modern variations incorporating image priors [8, 9, 10], provide improvements by reducing artifacts and enhancing image fidelity. In recent years, deep learning-based methods have emerged as powerful tools for addressing the challenges of LACT, achieving notable advancements in reconstruction quality [11, 12]. However, these methods often operate on 2D slices due to the scarcity of 3D CT data. Inspired by the success of Neural Radiance Fields (NeRF) [13] in inverse rendering, a new family of approaches represents the attenuation coefficient volume as a neural field: a continuous function of 3D coordinates, parameterized by neural networks [14, 15, 16, 17, 18]. Neural field-based approaches optimize the scene representation to match a given set of input views by leveraging the differentiability of the rendering process. The representation is typically optimized seperately for every scene. These approaches produce state-of-the-art results in the sparse-view CT setting. However, in the limited-angle setting, certain regions of the reconstructed volume may remain significantly underconstrained, often resulting in blurry or low-quality reconstructions. To address this limitation, we propose a novel regularization term for neural field optimization in the LACT setting, based on epipolar consistency conditions in X-ray imaging [19]. These conditions, derived from the Grangeat theorem [20], enforce consistency between the derivatives of line integrals along corresponding epipolar lines, across arbitrary epipolar geometries. Our key insight is that these terms can be leveraged to constrain neural field optimization with respect to projections both within and outside the limited-angle region. By enforcing these conditions across the entire 180° range111For consistency with previous works we assume semicircular acquisition trajectory, but the extension to 360° is trivial., our proposed approach, Epi-NAF, effectively propagates supervision from input views within the limited-angle range to unseen angles. This serves as a powerful complementary loss term alongside the standard neural field loss. In the following sections, we describe our method and demonstrate that it outperforms both traditional and neural field-based methods, both quantitatively and qualitatively, across a range of CT scans and limited-angle configurations. Fig. 1: Epipolar consistency in X-ray imaging. P_{0} and P_{1} are two X-ray projection images, where \bm{c}_{0} and \bm{c}_{1} are the corresponding X-ray source locations. We consider the cosine-weighted projection images, \tilde{P_{0}} and \tilde{P_{1}}, obtained by multiplying the intensity value of each pixel by cos(\beta), where \beta is the angle between the pixel, the source location and the image origin O. The source locations, along with a point in the body p define an epipolar plane, which intersects the projection images at l_{0}, l_{1}. The two corresponding epipolar lines are defined by the angle \alpha and the distance to the origin t."
https://arxiv.org/html/2411.05985v1,Emotional Images: Assessing Emotions in Images and Potential Biases in Generative Models,"This paper examines potential biases and inconsistencies in emotional evocation of images produced by generative artificial intelligence (AI) models and their potential bias toward negative emotions. In particular, we assess this bias by comparing the emotions evoked by an AI-produced image to the emotions evoked by prompts used to create those images. As a first step, the study evaluates three approaches for identifying emotions in images—traditional supervised learning, zero-shot learning with vision-language models, and cross-modal auto-captioning—using EmoSet, a large dataset of image-emotion annotations that categorizes images across eight emotional types. Results show fine-tuned models, particularly Google’s Vision Transformer (ViT), significantly outperform zero-shot and caption-based methods in recognizing emotions in images. For a cross-modality comparison, we then analyze the differences between emotions in text prompts—via existing text-based emotion-recognition models—and the emotions evoked in the resulting images. Findings indicate that AI-generated images frequently lean toward negative emotional content, regardless of the original prompt. This emotional skew in generative models could amplify negative affective content in digital spaces, perpetuating its prevalence and impact. The study advocates for a multidisciplinary approach to better align AI emotion recognition with psychological insights and address potential biases in generative AI outputs across digital media.","The information environment increasingly relies on visual media, with visual-first platforms like YouTube, Instagram, Pinterest, and TikTok comprising four of the top five most-used social media platforms in the US (Gottfried 2024). At the same time, the academic consensus has increasingly solidified around two findings: 1) the presence of visual media (Casas and Williams 2019; Geise, Panke, and Heck 2021) and 2) the presence of highly emotional content—especially negative—increases engagement in online spaces (Rathje, Bavel, and Linden 2021; Paletz et al. 2023). These findings, taken with the increasing use of social media content and online data as training data for modern large- and visual-language models, have the potential to produce an “unvirtuous cycle” of artificial intelligence (AI) and AI-generated content. In this cycle, content creators produce more emotionally evocative visual media to gain more engagement, generative AI systems then over-represent negative emotions in the media they produce, which then feed back into the information environment, receive more engagement, gain a larger share of visual media we see, and get ingested in the next round of training generative-AI systems. This paper begins an investigation of this unvirtuous cycle by developing scalable methods to assess emotion evocation in images. To this end, we evaluate three distinct approaches to emotion identification in images—traditional supervised methods, zero-short learning with state-of-the-art vision-language models, and cross-modal auto-captioning approaches—and use these methods to compare emotional salience in textual AI prompts with the emotions evoked in the images produced from these prompts. Leveraging the EmoSet dataset, which contains manually labeled image-emotion pairs for 118,102 images, we demonstrate that fine-tuning computer vision models—particularly Google’s Vision Transformer (ViT) (Dosovitskiy et al. 2021)—substantially outperforms zero-shot and auto-captioning approaches, with textual auto-captioning approaches performing particularly poorly in comparison. Using this fine-tuned ViT model coupled with the state-of-the-art Demux model for emotion classification in text, we compare the emotions present in a text-based generative-AI prompt to the emotions evoked by the produced image. Through this comparison, we test the central hypothesis underlying this unvirtuous cycle: that generative-AI systems are biased toward producing images that evoke negative emotions regardless of the underlying prompt. Results show that, for the task of emotion recognition in images, fine-tuning off-the-shelf computer vision models, particularly Google’s ViT, substantially outperforms other standard approaches, specifically zero-shot learning using vision-language models and automated captioning approaches. Then, to assess the potential biases between the emotions evoked by AI-generated images and the text prompts that are used to generate them, pulled from Wang et al. (2022), we find, for at least one standard and available generative AI model, the emotions evoked by an image do appear more negative than their source prompts. This result yields evidence of the concerning hypothesis above, namely that a generative AI model may nudge its users and audiences toward negative emotions. This finding is particularly concerning given the state of our modern information environment, where we know negative emotions are contagious (Kramer, Guillory, and Hancock 2014). Hence, we advocate for a multidisciplinary approach to better align AI emotion recognition with psychological insights and address potential biases in generative AI outputs across digital media."
https://arxiv.org/html/2411.05959v1,Efficient Self-Supervised Barlow Twins from Limited Tissue Slide Cohorts for Colonic Pathology Diagnostics,"Colorectal cancer (CRC) is one of the few cancers that have an established dysplasia-carcinoma sequence that benefits from screening. Everyone over 50 years of age in Canada is eligible for CRC screening. About 20% of those people will undergo a biopsy for a pre-neoplastic polyp and, in many cases, multiple polyps. As such, these polyp biopsies make up the bulk of a pathologist’s workload. Developing an efficient computational model to help screen these polyp biopsies can improve the pathologist’s workflow and help guide their attention to critical areas on the slide. DL models face significant challenges in computational pathology (CPath) because of the gigapixel image size of whole-slide images and the scarcity of detailed annotated datasets. It is, therefore, crucial to leverage self-supervised learning (SSL) methods to alleviate the burden and cost of data annotation. However, current research lacks methods to apply SSL frameworks to analyze pathology data effectively. This paper aims to propose an optimized Barlow Twins framework for colorectal polyps screening. We adapt its hyperparameters, augmentation strategy and encoder to the specificity of the pathology data to enhance performance. Additionally, we investigate the best Field of View (FoV) for colorectal polyps screening and propose a new benchmark dataset for CRC screening, made of four types of colorectal polyps and normal tissue, by performing downstream tasking on MHIST and NCT-CRC-7K datasets. Furthermore, we show that the SSL representations are more meaningful and qualitative than the supervised ones and that Barlow Twins benefits from the Swin Transformer when applied to pathology data. Codes are avaialble from https://github.com/AtlasAnalyticsLab/PathBT.","Computational Pathology (CPath) is an innovative field at the intersection of pathology and computer science. It aims to successfully create a framework of digital diagnostics that helps extract meaningful representations from the raw data in the oncology domain. Leveraging tools such as Deep Learning (DL), CPath enhances histopathological Whole Slide Images (WSIs) analysis and provides valuable diagnosis insights [3]. CPath is expected to integrate easily with existing diagnostic workflows, improving diagnostic accuracy, reproducibility, and disease detection and grading efficiency [4]. The recent advancements in DL methods in CPath have led to enhanced performance, but new challenges have emerged. DL methods in CPath often require large amounts of annotated data. However, annotating data is a time-consuming and expensive task that requires the expertise of pathologists with years of extensive clinical experience. This leads to a scarcity of publicly available, detailed annotated datasets and the need to leverage these limited annotations to train accurate models. As digital pathology expands worldwide, identifying methods less dependent on costly annotations becomes critical. To exploit large unlabeled or weakly annotated datasets, it is crucial to capitalize on Self-Supervised Learning (SSL), which trains on large amounts of unlabeled data and can outperform supervised pre-training [5]. Additionally, recent works have shown that SSL pre-training on pathology data can improve performance on downstream pathology tasks [2]. This paper focuses on addressing DL challenges in CPath to classify colorectal polyps and prevent colorectal cancer (CRC). CRC stands as one of the most prevalent causes of cancer-related mortality, but it is also one of the most preventable cancers. However, bottlenecks in patient screening schedules due to a shortage of pathologists induce delayed diagnoses [6]. Therefore, integrating CPath into clinical workflows could enhance diagnosis and lead to rapid care, increasing survival as deadly diseases, notably cancer, are detected precisely and efficiently. Hence, developing efficient SSL screening tools for accurately classifying colorectal polyps is crucial for effective CRC screening [7]. Integrated into Gastrointestinal (GI) cancer screening, CPath has shown excellent diagnosis accuracy, on par with board-certified pathologists. With the need for precise cancer grading for treatment personalization, DL is now a cornerstone of the fight against cancer and has achieved high Area Under the Curve (AUC) scores above 0.98. Additionally, it was shown that the deepest models obtain the highest performance due to their superior generalization capabilities [7, 3]. However, such models require more extensive training data [7]. Pathology images are highly specific as they have neither canonical orientations nor high colour variations. The Field of View (FoV) of the patches used for model training is also significant, as different FoV convey varying contextual information [2]. Despite advancements in GI and CRC screening, current methods do not adequately study the appropriate FoV for CRC screening. Additionally, they lack a thorough comparison of features learned through supervised and self-supervised learning (SSL) approaches. Furthermore, a more comprehensive analysis is needed on how self-supervised learning methods, originally developed for natural images, can be adapted to the unique characteristics of pathological data, particularly through data augmentation techniques. The future of CPath relies on SSL and its adaptation to the pathology data’s specific characteristics. In this paper, we conduct an in-depth analysis of the Barlow Twins [1] framework and propose to optimize this SSL framework for feature embedding from colorectal polyps for CRC screening. Our study yields several contributions: we propose (1) an enhanced Barlow Twins framework for pathology data by adapting the hyperparameters, augmentation strategy and pretraining strategy, (2) an evaluation of Barlow Twins representations on the patch and slide level introducing this SSL method in a MIL framework, (3) an investigation on the best FoV for CRC screening, (4) quantitative and qualitative comparisons of the SSL and supervised features, and (5) a new benchmark dataset for CRC screening after transferring the weights from our private dataset to MHIST and NCT-CRC-7k [8, 9, 10]. Code and pre-trained model weights (in PyTorch) are available at https://github.com/AtlasAnalyticsLab/PathBT for further contributions to the research community. The paper is organized as follows: Section 2 reviews the related work and discusses the existing approaches to self-supervised learning in CPath. Section 3 describes the proposed methodology, detailing the patch dataset curation and the different techniques used. Section 4 presents the experimental results and analyzes the qualitative and quantitative performance of the proposed methods, including heatmap and UMAP examination. In Section 5, we discuss the implications of our findings and suggest potential directions for future research."
https://arxiv.org/html/2411.05901v1,ViT Enhanced Privacy-Preserving Secure Medical Data Sharing and Classification,"Privacy-preserving and secure data sharing are critical for medical image analysis while maintaining accuracy and minimizing computational overhead are also crucial. Applying existing deep neural networks (DNNs) to encrypted medical data is not always easy and often compromises performance and security. To address these limitations, this research introduces a secure framework consisting of a learnable encryption method based on block-pixel operation to encrypt the data and subsequently integrate it with the Vision Transformer (ViT). The proposed framework ensures data privacy and security by creating unique scrambling patterns per key, providing robust performance against leading bit attacks and minimum difference attacks.","Advancements in ML and DL models, supported by cloud platforms like Google Cloud and Microsoft Azure, offer high computational power but also introduce privacy risks in shared environments [1, 2, 3]. In response, the research proposed ViT enhances the learnable encryption method and obfuscates image details while preserving classification-relevant features, ensuring HIPAA compliance. When tested on MRI and histopathological datasets, the method showed strong defense against attacks such as leading bit-attack, and minimum difference attacks, keeping medical data safe in the cloud."
https://arxiv.org/html/2411.05886v1,UnDIVE: Generalized Underwater Video Enhancement Using Generative Priors,"With the rise of marine exploration, underwater imaging has gained significant attention as a research topic. Underwater video enhancement has become crucial for real-time computer vision tasks in marine exploration. However, most existing methods focus on enhancing individual frames and neglect video temporal dynamics, leading to visually poor enhancements. Furthermore, the lack of ground-truth references limits the use of abundant available underwater video data in many applications. To address these issues, we propose a two-stage framework for enhancing underwater videos. The first stage uses a denoising diffusion probabilistic model to learn a generative prior from unlabeled data, capturing robust and descriptive feature representations. In the second stage, this prior is incorporated into a physics-based image formulation for spatial enhancement, while also enforcing temporal consistency between video frames. Our method enables real-time and computationally-efficient processing of high-resolution underwater videos at lower resolutions, and offers efficient enhancement in the presence of diverse water-types. Extensive experiments on four datasets show that our approach generalizes well and outperforms existing enhancement methods. Our code is available at github.com/suhas-srinath/undive.","The goal of underwater enhancement is to reduce artifacts and recover lost colors from the water scattering effect [30] in images and videos. Underwater enhancement finds applications in areas such as coral reef monitoring [25], archaeology [65] and underwater robotics [76]. Underwater video enhancement (UVE) is often challenging due to multiple reasons. Collecting high-quality videos amidst distortions like blur, reduced illumination, complex channel attenuation, and obtaining ground-truth video data to supervise learning-based algorithms are extremely cumbersome. Input PhISHNet Ours (a) Frame x_{t-1} (b) Frame x_{t} (c) Frame x_{t+1} Figure 1: UnDIVE (bottom row) enhances contiguous video frames (top row) from the UOT32 [36] dataset (DeepSeaFish video), while maintaining consistent colors and illumination as opposed to image-based methods such as PhISH-Net (middle row). Recent works [40, 80, 27, 52, 66]222Work done while at Indian Institute of Science. try to solve UVE by training convolutional neural networks through the supervision of large-scale ground truth data, but often fail to generalize to diverse and unseen water types. While learning-based methods [40, 86, 59, 87] have been reasonably successful in achieving underwater image enhancement (UIE), they cannot be directly scaled to UVE since they do not account for object motion in videos. Moreover, variations in the illumination across enhanced frames from UIE methods cause flicker-like artifacts in the enhanced videos. Despite the greater requirement of videos than images in marine applications, very few UVE methods exist. UIE methods have been very successful in restoring the color and contrast in underwater scenes. Earlier UIE methods attempted to solve enhancement through pixel adjustments and classical priors [50, 63]. With the development of deep learning, data-centric methods [44, 16, 40] leveraged paired image training to learn good enhancement. To generalize better, unsupervised methods [17] have also been developed for UIE. Despite numerous advancements in UIE, scaling these techniques to videos via frame aggregation remains challenging due to the lack of temporal alignment. To address the aforementioned challenges, we propose to solve the UVE problem through the introduction of temporal consistency into a physics-driven spatial enhancement network that mitigates artifacts that arise due to water scattering. Firstly, to learn efficient spatial enhancement, we propose to learn a generative prior through a self-supervised denoising diffusion probabilistic model (DDPM) [26] that learns robust representations of underwater images. To the best of our knowledge, this is the first work that learns a generative prior using diffusion for UVE. The encoder of the UNet learned by the DDPM is subsequently integrated into the UVE framework for efficient downstream enhancement. In general, video enhancement [81, 6, 51] and restoration methods [47, 43] incorporate motion information during the learning process to make videos more temporally consistent through the alignment of objects or representations between consecutive frames. However, such transformer-based methods are prone to overfitting and do not generalize well (simultaneously perform well on diverse datasets and water-types). To tackle this issue, we propose to incorporate motion into UVE through an unsupervised optical flow loss so that objects in consecutive enhanced frames exhibit smoother motion and uniform illumination, colors, and contrast. Leveraging the generalization capability of the generative prior and the unsupervised temporal consistency loss, we propose an Underwater Domain Independent Video Enhancement (UnDIVE) framework that can efficiently process high-resolution videos with fairly low complexity and inference times. To summarize, our contributions are as follows: • A two-stage training framework for UVE. The first stage learns a prior on underwater images, and the second stage utilizes this prior to learn spatial and temporal enhancement. • A generative prior trained on carefully chosen underwater images learned through a DDPM to provide robust representations. The learned encoder is subsequently integrated into the UnDIVE network for downstream enhancement. • An unsupervised temporal consistency loss to incorporate motion information into a physics-driven spatial enhancement network, enabling enhanced videos to maintain uniform illumination, accurate colors, and improved contrast. • Through extensive experiments, we demonstrate the superiority and generalizability of UnDIVE over other enhancement methods on four diverse underwater video datasets on multiple no-reference (NR) visual quality metrics."
https://arxiv.org/html/2411.05885v1,Alternative Learning Paradigms for Image Quality Transfer,"Image Quality Transfer (IQT) aims to enhance the contrast and resolution of low-quality medical images, e.g. obtained from low-power devices, with rich information learned from higher quality images. In contrast to existing IQT methods in the literature which adopt supervised learning frameworks, in this work, we propose two novel formulations of the IQT problem. The first approach uses an unsupervised learning framework, whereas the second is a combination of both supervised and unsupervised learning. The unsupervised learning approach considers a sparse representation (SRep) and dictionary learning model, which we call IQT-SRep, whereas the combination of supervised and unsupervised learning approach is based on deep dictionary learning (DDL), which we call IQT-DDL. The IQT-SRep approach trains two dictionaries using a sparse representation model using pairs of low- and high-quality volumes. Subsequently, the sparse representation of a low-quality block, in terms of the low-quality dictionary, can be directly used to recover the corresponding high-quality block using the high-quality dictionary. On the other hand, the IQT-DDL approach explicitly learns a high-resolution dictionary to upscale the input volume, while the entire network, including high dictionary generator, is simultaneously optimised to take full advantage of deep learning methods. The two models are evaluated using a low-field magnetic resonance imaging (MRI) application aiming to recover high-quality images akin to those obtained from high-field scanners. Experiments comparing the proposed approaches against state-of-the-art supervised deep learning IQT method (IQT-DL) identify that the two novel formulations of the IQT problem can avoid bias associated with supervised methods when tested using out-of-distribution data that differs from the distribution of the data the model was trained on. This highlights the potential benefit of these novel paradigms for IQT.","Image Quality Transfer (IQT) Alexander et al. (2014, 2017); Lin et al. (2019); Tanno et al. (2021); Lin et al. (2021, 2022); Kim et al. (2023) is a machine learning technique that is used to enhance the resolution and contrast of low-quality clinical data using rich information in high-quality images. For example given an image from a standard hospital scanner or rapid acquisition protocol, we might estimate the image we would have got from the same subject using a high-power experimental scanner available only in specialist research centres or a richer acquisition protocols too lengthy to run on every patient. IQT is a vital component of efforts to democratise the capabilities of high power rare experimental systems broadening the accessibility e.g. to lower and middle income countries Anazodo et al. (2022). This technique learns mappings from low-quality (e.g. clinical) to high-quality (e.g.experimental) images exploiting the similarity of image structure across subjects, regions, modalities, and scales. The mapping may then operate directly on low-quality images to estimate the corresponding high-quality images. Early work Alexander et al. (2017); Blumberg et al. (2018); Tanno et al. (2021) focused on diffusion MRI and showed remarkable ability to enhance both contrast and resolution and enabled tractography to recover small pathways impossible to reconstruct at the acquired resolution. Recent work Lin et al. (2021) extends the idea to standard structural MRI, particularly targeting application to low-field MRI systems. IQT technique Alexander et al. (2017) differs from super-resolution in computer vision Lau et al. (2023); Zhou et al. (2020, 2021); Li et al. (2024) in several key aspects. In general super-resolution aim to up-sample an image, whereas IQT aims to transfer the quality of information from an image to the other. This means that IQT is not limited to increasing the spatial resolution of images. While super-resolution techniques primarily focus on enhancing the spatial resolution, IQT also aims to improve the image contrast. This dual enhancement is crucial for medical imaging applications where both resolution and contrast are necessary for accurate diagnosis and analysis. Moreover, super-resolution techniques are generally used to upsample images, making them appear sharper and more detailed. In contrast, IQT is specifically designed to transfer the quality from high-quality images to low-quality images. This is particularly beneficial in medical imaging, where high-quality images from advanced scanners are used to enhance the quality of images obtained from lower-power or less advanced scanners. Lastly, IQT differs from modality transfer methods, which maps one modality to another to obtain multi-modality information Iglesias et al. (2021, 2023, 2022), whereas IQT’s primary goal is to enhance the existing image quality, specifically improving resolution and contrast rather than the developing new content. By highlighting these differences, we aim to clearly delineate the unique characteristics and advantages of the IQT task. Machine learning models are often trained on a specific data distribution, but may encounter unseen data from different distributions in real-world scenarios. This poses a critical challenge for the security and reliability of machine learning systems, especially in some error-sensitive applications, such as medical diagnosis including the application investigated in this work. One of its powerful capabilities lies in the promising generalisation ability from training data to unseen in-distribution (InD) data. However, the finite training data cannot guarantee the completeness of data distribution, so it is inevitable to encounter out-of-distribution (OOD) data. Machine learning models can be broadly categorised into supervised, unsupervised and self-supervised learning models. In supervised learning, the model is trained by paring inputs with their expected outputs. However, this is far from being practical, since the full data distribution cannot be represented in the training data set. To circumvent this difficulty, unsupervised and self-supervised learning methods can be used. All IQT models proposed in the literature use supervised learning frameworks to learn a regression between matched patches in low- and high-quality images. In particular deep learning frameworks substantially outperform the original random-forest implementation in terms of global error metrics for enhancement of both diffusion-tensor MRI and low-field structural MRI Alexander et al. (2014, 2017); Lin et al. (2019); Tanno et al. (2021); Lin et al. (2021, 2022). However, interpretation of images enhanced via such regression models needs caution. First, regression models in general can lead to bias that depends on the training data distribution Obermeyer et al. (2019). In particular, inputs (here patches) that are rare in the training data are often skewed towards outputs more common in training data; and degenerate regions of the input-space where the mapping is ambiguous are often mapped to a consistent mean giving a false impression of consistent and confident output. Moreover, the performance of deep-learning based methods can degrade even more with OOD data. These effects have been well documented in other image-related regression applications recently, such as parameter mapping Gyori et al. (2022). So far, they have not been considered in IQT and image enhancement although similar effects are likely to arise. Additional problems, particularly in deep learning, can arise from over-fitting and under-fitting which can further add to bias in estimates particularly for examples that are over/under-represented in the training data. Moreover, state-of-the-art IQT models, specifically deep neural networks, are generally designed for a static and closed world Krizhevsky et al. (2017); He et al. (2015). The models are trained under the assumption that the input distribution at test time will be the same as the training distribution. In real world MRI data, however, deep-learning-based techniques effectiveness diminishes when applied to images that differ significantly from the training data set Gu et al. (2019). Although various approaches have been developed to tackle this issue, such as training networks to handle multiple types of degradation Soh et al. (2020); Xu et al. (2020); Zhang et al. (2018a); Zhou and Susstrunk (2019) and making models less sensitive to degradation through iterative optimisations Shocher et al. (2018); Gu et al. (2019), it is also crucial to enhance the robustness of the network structure. Sparse representation (SRep) using dictionary learning is an unsupervised learning framework that assumes a given signal is sparse in some domain (Wavelets, Fourier, discrete cosine transform, etc.). SRep has proven robust to noise and redundancy in the data, where supervised deep learning algorithms encounter problems Elad (2010). In the IQT context, low and high-quality dictionaries ({\mathbf{D}}_{\ell}, and {\mathbf{D}}_{h} respectively) can be trained using a sparse representation model using pairs of low- and high-quality volumes. Subsequently, the sparse representation of a low-quality block, in terms of the low-quality dictionary {\mathbf{D}}_{\ell}, can be directly used to recover the corresponding high-quality block using the high-quality dictionary {\mathbf{D}}_{h}. As such, low-quality or high-quality volume patches are represented as a linear combinations of atoms drawn from a dictionary. SRep has been successfully applied to many other related inverse problems in image processing, such as denoising Li et al. (2012); Elad and Aharon (2006), restoration Zhang et al. (2014); Li et al. (2012), image quality assessment Liu et al. (2017, 2018, 2024, 2019), outlier or anomaly detection Eldaly (2018); Eldaly et al. (2019), image reconstruction Eldaly and Alexander (2024); Eldaly et al. (2025), and super resolution Yang et al. (2010). In a convex optimisation framework, training and testing samples are forced to follow the observation model of the imaging system on hand. Therefore, any new unseen test samples (either InD or OOD) will follow this model, which can avoid the “regression to the mean” problems observed with supervised regression models, often observed in OOD data. On the other hand, in supervised deep learning, Dong et al. Dong et al. (2014) replaced the dictionary learning using sparse representation steps described above with a multilayered convolutional neural network to take advantage of the powerful capability of deep learning. As such, the low and high-quality dictionaries are implicitly acquired through network training. Various methods have been proposed to improve the performance of this approach such as in Kim et al. (2016); Lim et al. (2017); Tai et al. (2017); Zhang et al. (2018b). However, most of these studies, follow the same formality as in Dong et al. (2014) from a general perspective, where all the processes in the sparse-coding-based methods are replaced by a multilayered network. Recently, deep dictionary learning Tariyal et al. (2016) is proposed to take advantage of both transductive and inductive nature of dictionary learning and deep learning, respectively, and is very well suited where there is a scarcity of training data. While dictionary learning focuses on learning “basis” and “features” by matrix factorisation, deep learning focuses on extracting features via learning “weights” or “filter” in a greedy layer by layer fashion. Deep dictionary learning has been applied to various problems including recognition Tang et al. (2020); Sharma et al. (2017), image inpainting Deshpande et al. (2020), super resolution Huang and Dragotti (2018); Zhao et al. (2017), classification Majumdar and Singhal (2017); Majumdar and Ward (2017); Manjani et al. (2017), and load monitoring Singh and Majumdar (2017). In this work, in contrast to existing IQT models in the literature, we propose two novel IQT algorithms, from which one is an example of unsupervised learning while the other is an example of blended supervised and unsupervised learning. The first approach is based on a sparse representation model and dictionary learning, which we call IQT-SRep. In this approach, low and high-quality dictionaries can be trained using a sparse representation model using pairs of low- and high-quality volumes. Subsequently, the sparse representation of a low-quality block, in terms of the low-quality dictionary, can be directly used to recover the corresponding high-quality block using the high-quality dictionary. The second approach is based on deep dictionary learning which we call IQT-DDL. This approach explicitly learns high-quality dictionary through network training. The main network predicts the high-quality dictionary coefficients, and the weighted sum of the dictionary atoms generates a high-quality output. This approach differs fundamentally from traditional deep-learning methods, which typically employ upsampling layers within the network. The upsampling process in our IQT approach is efficient since pre-generated high-quality dictionary serves as a magnifier during inference. Additionally, the main network no longer needs to retain pixel-level information in the high-quality space, enabling it to focus solely on predicting the dictionary coefficients. The main advantages of these two novel formulations are that they are robust to super resolve heavily OOD test data, and they are well suited where there is a scarcity of training data. We demonstrate the two models using experiments from a low-field MRI application and compare the results with the recently proposed state-of-the-art supervised deep learning approach Lin et al. (2022). As such, the main contributions of this paper can be summarised as follows. 1. We propose two new formulations of the IQT technique, from which one is an unsupervised learning based (IQT-SRep), and one is based on a combination of both supervised and unsupervised learning (IQT-DDL). Both of these formulations have never been previously applied to the IQT problem in literature. 2. The IQT-SRep approach is based on sparse representation and dictionary learning model and assumes that a given low- or high-quality volume patch can be represented as a linear combination of atoms drawn from a dictionary that is trained using training examples of pairs of low- and high-quality volume patches. This requires training of a pair of coupled dictionaries using a sparse representation model using pairs of low- and high-quality volumes. 3. The IQT-DDL approach is based on a combination of supervised and unsupervised learning using deep dictionary learning. This approach assumes that a given low- or high-quality volume patch can be represented as a non-linear combination of atoms drawn from a dictionary that is trained using training examples of pairs of low- and high-quality volume patches. 4. We demonstrate the performance of the model using experiments from a low-field MRI application, using both InD and OOD data, and compare with the state-of-the-art supervised deep learning IQT method, for low-field MRI enhancement. The remaining sections of the paper are organised as follows. Section 2 formulates the problem of IQT using three learning techniques; the formulations that we propose here for IQT-SRep and IQT-DDL are described in detail, and finally, the supervised deep learning approach proposed in Lin et al. (2022) is briefly presented for comparison. Experiments conducted using a low-field MRI application synthesised using data from the human connectome project (HCP) are presented in Section 3. A general discussion is then presented in 4. Conclusions and future work are finally reported in Section 5."
https://arxiv.org/html/2411.05873v1,Poor Man’s Training on MCUs: A Memory-Efficient Quantized Back-Propagation-Free Approach,"Back propagation (BP) is the default solution for gradient computation in neural network training. However, implementing BP-based training on various edge devices such as FPGA, microcontrollers (MCUs), and analog computing platforms face multiple major challenges, such as the lack of hardware resources, long time-to-market, and dramatic errors in a low-precision setting. This paper presents a simple BP-free training scheme on an MCU, which makes edge training hardware design as easy as inference hardware design. We adopt a quantized zeroth-order method to estimate the gradients of quantized model parameters, which can overcome the error of a straight-through estimator in a low-precision BP scheme. We further employ a few dimension reduction methods (e.g., node perturbation, sparse training) to improve the convergence of zeroth-order training. Experiment results show that our BP-free training achieves comparable performance as BP-based training on adapting a pre-trained image classifier to various corrupted data on resource-constrained edge devices (e.g., an MCU with 1024-KB SRAM for dense full-model training, or an MCU with 256-KB SRAM for sparse training). This method is most suitable for application scenarios where memory cost and time-to-market are the major concerns, but longer latency can be tolerated.","On-device training, that is training deep neural networks (DNN) on edge devices, enables a DNN model pre-trained on cloud to improve itself on newly observed data and adapt to cross-domain or out-of-domain distribution shifts after edge deployment. It also allows the model to adapt to user personalization locally, which protects user privacy over sensitive data (e.g., healthcare and financial data). As physic-informed machine learning has been increasingly used for safety-critical decision-making in autonomous systems, there has been also growing interest in on-device fine-tuning or end-to-end training. In federated learning, a machine learning model also needs to be trained periodically on each local edge node, then updated on a global centralized server. Backward propagation (BP) [1] is used in almost all neural network training frameworks for gradient computation. BP is actually a reverse-mode automatic differentiation (AD) [2, 3] approach implemented based on the information of a computational graph. While a forward-mode AD is suitable for computing the gradient of a single-input multiple-out function, BP is more suitable for a multiple-input (i.e., many network parameters) and single-output (i.e., training loss) function. With sophisticated AD packages, operating systems, and compilers, BP can be called with just one command (e.g., loss.backward() in PyTorch) on a CPU- or GPU-based desktop or cloud computing platform. This has greatly simplified the development and deployment of modern neural network models. However, training a neural network on resource-constrained edge hardware [e.g., a microcontroller unit (MCU), FPGA or photonic platform] is completely different from the training task on a desktop or cloud platform, due to the limited hardware resources and software support. Specifically, implementing a standard BP-based training framework on edge devices are often prevented by three major challenges: • Memory Challenge. Edge devices like MCU have a very limited run-time memory (e.g., STM32F746 with only 256-KB user SRAM, or STM32H7B3 with 1024-KB user SRAM). This budget is often below the memory requirement of storing all network parameters, making full-model BP-based training impossible for most realistic cases. By choosing tailored network models (e.g., MCUNet [4]), using real-quantized graphs and a co-designed lightweight back-end (e.g., the TinyEngine [4, 5]), one may perform edge inference with a low memory cost (e.g., 96 KB for the MCUNet-in1 model [4]). However, the memory cost of a full-model BP-based training (e.g., 7.4 MB for MCUNet-in1) is far beyond the memory capacity. Existing training methods on MCU update only a small subset of model parameters (e.g., only the last layer [6, 7], bias vectors [8] to reduce the memory cost, yet leads to significant (e.g., ¿10%) accuracy drop. Sparse update [4, 9]) could narrow this gap, yet requires computation-intensive searches and compilation-level optimization on cloud. • Precision Challenge. Low-precision quantized computation is often utilized on digital edge hardware (e.g., MCU and FPGA) to reduce latency, memory cost, and energy consumption. However, low-precision operations pose great challenges for BP-based training. BP was originally designed for the gradient computation of smooth functions. Thus, error-prone approximation techniques such as straight-through estimators [10] are required to handle non-differentiable functions in quantized neural network training. The errors introduced by these approximation techniques increase as hardware precision reduces. They also propagate and accumulate through different layers, leading to dramatic accuracy drop, unstable training behaviors, or even divergence [11, 12]. • Time-to-Market Challenge. While BP can be done on CPU or GPU with just one line of code (e.g., loss.backward() in PyTorch), implementing it on edge devices can be very challenging. Due to the lack of automatic differentiation packages [2] and sophisticated operating systems on edge platforms, designers often have to implement the math and hardware of gradient computation manually. On some platforms (e.g., integrated photonics), novel devices must be invented and fabricated to perform BP [13]. This error-prone process needs numerous debugs and design trade-offs. As a result, designing edge training hardware is more time-consuming than designing inference hardware. For instance, our own experience shows that an experienced FPGA designer can design a high-quality inference accelerator within one week, yet it takes over one year to implement an error-free training accelerator on FPGA. This long time to market is often unacceptable in the industry due to the fast evolution of AI models. Paper Contributions. The above challenges motivate us to ask the following question: {mdframed} [userdefinedwidth=5.8inch, align=center] Can we make the edge training hardware design as easy and memory-efficient as inference hardware design? In this paper, we show that the answer is affirmative, with the assumption that memory budget and time to market are given higher priority over runtime latency. Our key idea is to completely bypass the complicated BP implementation by proposing a quantized zeroth-order (ZO) method to train a real-quantized neural network model on MCU. This training method only uses quantized forward evaluations to estimate gradients. As a result, we can use a similar memory cost of inference to achieve full-model training under the tiny memory budget of an MCU. This quantized ZO training framework can be used as a plug-and-play tool added to quantized inference hardware, therefore the design complexity and time to market can be dramatically reduced. Our specific contributions are briefly summarized below: 1. ZO Quantized Training for Edge Devices. We propose a BP-free training framework via quantized zeroth-order optimization to enable full-model and real-quantized training on MCUs under extremely low memory budget (e.g., 256-KB SRAM for sparse training or 1024-KB SRAM for dense training). This framework enjoys low memory cost and easy implementation. Furthermore, it shows better accuracy than quantized BP-based training in low-precision (e.g., INT8) settings since no error-prone straight-through estimator is needed. 2. Convergence Improvement. ZO training suffers from slow convergence rates as the number of training variables increases. Previous assumption of low intrinsic dimensionality [14] or coordinate-wise gradient estimation [15] does not work in on-device training. To improve the training convergence, we propose a learning-rate scaling method to stabilize each training step. We also employ a few dimension-reduction methods to improve the training convergence: (i) a generic layer-wise gradient estimation strategy that combines weight perturbation and node perturbation for ZO gradient estimation, (ii) a sparse training method with task-adaptive block selection to reduce the number of the trainable parameters. 3. MCU Implementation. We implement the proposed BP-free training framework on an MCU (full-model training on an STM32H7B3 with 1024-KB user SRAM, and sparse training on an STM32F746 with 256-KB user SRAM). A quantized inference engine is easily converted to a training engine, with only an additional control unit, a temporary gradient buffer, and a pseudo-random number generator. To our best knowledge, this is the first framework to enable full-model training under such a tiny memory budget (STM32H7B3 with 1024-KB user SRAM). 4. Experimental Validation. We conduct extensive experiments on adapting a pre-trained image classification model to unseen image corruptions and fine-grained vision classification datasets. On adapting image corruptions, our BP-free training outperforms current quantized BP-based training with an average 6.4% test accuracy improvement. Our method can also match the performance of back-propagation training on fine-grained vision classification datasets. Figure 1: (a): Overview of BP-free training framework. A quantized inference engine is easily converted to a training engine by adding control unit and repeatedly calling the inference accelerator. (b): Training memory comparison of different training methods. The numbers are measured with MCUNet-in1 [4], batch size 1, and resolution 128\times 128. Our Key idea is summarized in Fig. 1 (a). As demonstrated in Fig. 1 (b), our method is the only solution to enable full-model training on commodity-level MCU (e.g., STM32H7B3 with 1024-KB user SRAM) without auxiliary memory. This memory cost is the minimum to enable full-model training (478 KB model parameters plus 190 KB peak inference memory), 5.46\times more memory-efficient than the memory cost of quantized BP, and >400\times more memory efficient than BP-based training in PyTorch which includes back-end memory overhead. BP-free sparse training further reduces the memory cost to fit a smaller budget (e.g., 256-KB SRAM)."
https://arxiv.org/html/2411.05863v1,Exploring the Feasibility of Affordable Sonar Technology: Object Detection in Underwater Environments Using the Ping 360,"This study explores the potential of the Ping 360 sonar device, primarily used for navigation, in detecting complex underwater obstacles. The key motivation behind this research is the device’s affordability and open-source nature, offering a cost-effective alternative to more expensive imaging sonar systems. The investigation focuses on understanding the behaviour of the Ping 360 in controlled environments and assessing its suitability for object detection, particularly in scenarios where human operators are unavailable for inspecting offshore structures in shallow waters. Through a series of carefully designed experiments, we examined the effects of surface reflections and object shadows in shallow underwater environments. Additionally, we developed a manually annotated sonar image dataset to train a U-Net segmentation model. Our findings indicate that while the Ping 360 sonar demonstrates potential in simpler settings, its performance is limited in more cluttered or reflective environments unless extensive data pre-processing and annotation are applied. To our knowledge, this is the first study to evaluate the Ping 360’s capabilities for complex object detection. By investigating the feasibility of low-cost sonar devices, this research provides valuable insights into their limitations and potential for future AI-based interpretation, marking a unique contribution to the field.","In recent years, there has been a growing emphasis on exploring underwater environments. Traditionally, tasks such as inspection, maintenance, and object retrieval were performed by human divers, often at significant physical and mental risk [1]. However, advances in technology have led to the development of Remotely Operated Vehicles (ROVs) and Autonomous Underwater Vehicles (AUVs), which have transformed ocean exploration by automating complex tasks and reducing human risks [2]. Despite these advancements, underwater navigation and object detection remain challenging due to the limitations of optical sensors. Water turbidity, light attenuation, and scattering severely degrade visual data quality, making traditional cameras unreliable in sub-sea operations [3]. While short-range improvements in visual data quality have been achieved, alternative sensing technologies are needed to overcome these limitations [4]. Sonar technology has emerged as a viable solution for underwater detection and navigation, effectively penetrating murky waters and low-light conditions through the use of sound waves [5][6]. There are two primary types of sonar systems: single-beam and multi-beam. Multi-beam sonar systems, which generate detailed 3D maps and provide extensive spatial coverage, offer superior resolution and accuracy. However, their high cost limits accessibility for many applications [7]. The Ping 360 sonar, developed by Blue Robotics, presents an affordable alternative as a single-beam system designed primarily for navigation. Capable of performing 360° scans, it offers potential for underwater exploration and obstacle avoidance [8]. However, its ability to perform more advanced tasks, such as complex object detection, remains under explored. Accurate object detection is critical for autonomous underwater vehicles operating without human intervention, where the reliable identification of underwater structures is essential. This study aims to investigate the feasibility of using the Ping 360 sonar for underwater object detection in real-world scenarios. Existing research has explored sonar-based object detection, particularly through imaging sonar techniques. However, these approaches often fail to account for the fundamental differences between sonar and optical imagery. Single-beam sonar systems like the Ping 360 generate acoustic representations of underwater environments, which are sometimes converted into image-like outputs. While this conversion can improve detection accuracy, it oversimplifies the inherent complexities of sonar data, such as non-homogeneous resolution, speckle noise, acoustic shadowing, and reverberation, as noted by Karimanzira et al. [9]. While the Ping 360 is an affordable option—about one-fifth the cost of multi-beam systems—it faces challenges such as noise interference and shadowing, which complicate object detection [10]. Despite these limitations, single-beam systems are practical for smaller-scale or budget-conscious operations. Kim et al. suggest that advanced image processing techniques can help mitigate these limitations, though they may still struggle in noisy or low-resolution environments [11]. In more complex underwater environments, such as turbid or structurally intricate coastal habitats, the resolution of the Ping 360 cannot match that of high-frequency multi-beam sonars, which offer real-time, camera-like visuals. However, McKay et al. showed that fine-tuning machine learning models for sonar data can improve detection accuracy, even with simpler systems like the Ping 360 [9]. Still, these models often rely on high-quality sonar data, which is not always realistic for single-beam systems. Our study addresses these challenges by evaluating the Ping 360 sonar’s potential for underwater object detection in real-world settings, moving beyond its traditional navigation role. Many existing studies focus on imaging sonar or synthetic data generation, such as Jiang et al.’s CycleGAN approach, which converts sonar data into image-like formats [12]. While these methods produce strong results in controlled environments, they often overlook the complexities of sonar data, such as variable resolution, acoustic noise, and shadowing. Forcing sonar data into image-based frameworks risks misinterpreting its unique characteristics, which are crucial for real-world detection accuracy. Recent studies frequently apply image processing and AI models to sonar data [13][14][15][16]. While these models may perform well in controlled environments, they often struggle in real-world conditions where sonar’s acoustic properties must be accounted for. Moreover, manual annotation processes are often misrepresented, leading to errors in sonar data interpretation and artificially inflated model performance metrics, as noted by Zhao et al. [12]. Factors such as object material, size, and positioning, which significantly impact detection reliability, are rarely considered in detail. Our study focuses on re-purposing the Ping 360; the navigation-sonar; for more complex underwater object detection tasks. Rather than developing AI solutions, we evaluate the device’s feasibility and limitations in environments where traditional image-based methods fail, such as in low-visibility or turbid waters. We emphasize the importance of treating, and interpreting sonar data based on its acoustic properties, rather than forcing it into an image-processing framework. By examining challenges such as surface reflections, noise, and acoustic shadows, we provide a realistic evaluation of the Ping 360’s capabilities for complex detection tasks. This research, being the first of its kind, offers a unique contribution to the field. Research Questions The primary research questions (RQ) addressed in this study are as follows: • RQ1: What are the main challenges in interpreting navigation sonar data in cluttered or reflective environments? • RQ2: How effective is manual annotation of sonar data in improving object detection when used with advanced segmentation AI models such as UNet? • RQ3: Can low-cost sonar devices like Ping 360 typically used for navigation, be effectively repurposed for complex underwater object detection? Contributions In line with finding out the answers to those questions, this research makes several key contributions: • For the first time, we present a detailed investigation into the feasibility of using the Ping 360 sonar device for object detection in underwater environments. • We are releasing the raw collected data from the Ping 360 sonar to enable future researchers to explore its potential usability, especially for those who may not have access to the device due to its cost, despite it being relatively affordable. • We provide insights into the challenges of sonar data interpretation, including surface reflections, object shadows, and the importance of careful data annotation for machine learning. • We developed a segmentation dataset from manually annotated sonar images and demonstrated that the Ping 360 sonar’s object detection performance is highly dependent on data pre-processing and careful interpretation, particularly when used with machine learning models like the U-Net segmentation algorithm."
https://arxiv.org/html/2411.05831v1,"To Ask or Not to Ask? 
Detecting Absence of Information in Vision and Language Navigation","Recent research in Vision Language Navigation (VLN) has overlooked the development of agents’ inquisitive abilities, which allow them to ask clarifying questions when instructions are incomplete. This paper addresses how agents can recognize “when” they lack sufficient information, without focusing on “what” is missing, particularly in VLN tasks with vague instructions. Equipping agents with this ability enhances efficiency by reducing potential digressions and seeking timely assistance. The challenge in identifying such uncertain points is balancing between being overly cautious (high recall) and overly confident (high precision). We propose an attention-based instruction-vagueness estimation module that learns associations between instructions and the agent’s trajectory. By leveraging instruction-to-path alignment information during training, the module’s vagueness estimation performance improves by around 52% in terms of precision-recall balance. In our ablative experiments, we also demonstrate the effectiveness of incorporating this additional instruction-to-path attention network alongside the cross-modal attention networks within the navigator module. Our results show that the attention scores from the instruction-to-path attention network serve as better indicators for estimating vagueness.","Vision Language Navigation (VLN) is the task of enabling a robot to navigate an environment based on a given instruction. There has been substantial research in VLN in the recent past [25]. The majority of previous research has focused on training large neural models for VLN task using synthetic datasets like R2R [2] that contained step-by-step instructions and Reverie [16] that had high-level instructions, among others. All these models operate under the assumption that agents are designed to act independently without human intervention. Even when dealing with abstract or high-level instructions, agents are trained to explore and make decisions they deem best for progressing towards their goals. There is no provision for indicating uncertainty or indecision about the next course of action. Typically, designers impose an upper time limit in these approaches, prompting agents to cease operations after a predefined number of moves, regardless of goal achievement. When a robot transitions to a real-world environment, it often encounters instructions from various individuals, each with unique communication styles. Adapting to this diversity poses a challenge in generalizing across different instructing styles. While some individuals may prefer concise instructions, others might delve into details but present them in a disorganized manner. A second person unfamiliar with the instructor may find it challenging to follow the instructions easily. In such situations, humans typically seek clarification by asking questions like, “I’m now in front of the kitchen. Where should I go next?”. Implementing a similar ability is a desired feature in intelligent robots to tackle uncertainty due to vagueness in the input instruction. When a robotic agent is posed with an unclear instruction, it may get stuck at some point, or it may keep exploring, taking very long paths before reaching the goal, or it may never reach the goal. Rather than waiting for the robot to encounter failure, it is preferable if the robot can be proactive and seek help when it is approaching failure. We are interested in the task of Under-specified Vision Language Navigation (ULN) as outlined in [8]. ULN introduced a nuanced modification to the VLN task by incorporating a new dimension of information completeness. In this paper, we focus on the task of instruction-vagueness estimation within the context of ULN. Instruction-vagueness (IV) estimation assesses the uncertainty arising from vague input instructions in the decision-making process of a VLN agent. In essence, an IV estimation module addresses the VLN agent’s query: “Do I possess enough information to proceed with my next move or do I seek assistance?”. A key requirement for this module is to find the right balance (see Figure 1) between being overly cautious and frequently seeking help, which guarantees task completion (high recall), and being overly confident, potentially resulting in excessive exploration without finishing the task (high precision). The contributions of this paper are as follows: • We introduce an attention-based instruction-vagueness (IV) estimation module and integrate it into an existing VLN model. The module takes the instruction, the path followed thus far, and the proposed next move as its inputs and decides at each time step whether to follow the VLN model’s suggestion or to request assistance. • We introduce a pre-training task that helps identify important parts of the instructions needed for predicting actions. By incorporating this instruction to path alignment information into the Instruction-Vagueness (IV) module, we significantly improve its ability to detect points of uncertainty, enhancing precision-recall balance."
https://arxiv.org/html/2411.05825v1,SurfGNN: A robust surface-based prediction model with interpretability for coactivation maps of spatial and cortical features,"Current brain surface-based prediction models often overlook the variability of regional attributes at the cortical feature level. While graph neural networks (GNNs) excel at capturing regional differences, they encounter challenges when dealing with complex, high-density graph structures. In this work, we consider the cortical surface mesh as a sparse graph and propose an interpretable prediction model—Surface Graph Neural Network (SurfGNN). SurfGNN employs topology-sampling learning (TSL) and region-specific learning (RSL) structures to manage individual cortical features at both lower and higher scales of the surface mesh, effectively tackling the challenges posed by the overly abundant mesh nodes and addressing the issue of heterogeneity in cortical regions. Building on this, a novel score-weighted fusion (SWF) method is implemented to merge nodal representations associated with each cortical feature for prediction. We apply our model to a neonatal brain age prediction task using a dataset of harmonized MR images from 481 subjects (503 scans). SurfGNN outperforms all existing state-of-the-art methods, demonstrating an improvement of at least 9.0% and achieving a mean absolute error (MAE) of 0.827±0.056 in postmenstrual weeks. Furthermore, it generates feature-level activation maps, indicating its capability to identify robust regional variations in different morphometric contributions for prediction.","There is growing evidence that brain development/aging trajectories and developments of brain disorders both could be traced on the cerebral cortex [1]. A prevalent approach for characterizing cerebral cortex is to reconstruct the cortical surface and measure the morphological features, such as cortical thickness, surface area, sulcal depth, myelin content, etc. [2] Currently, an important application of cortical features is to predict phenotypes, such as age [3, 4], sex[5], and brain disease states[6] using machine learning methods, and it could help explore important biomarkers about the cortex evolutional process and diagnose brain disorders. For surface-based analysis, early approaches solely focused on vertex features without considering the topological structure of the surface mesh[8]. More recent techniques utilized GNN-based networks to collectively examine the node features and topological architecture of the cortical surface, due to the graph-like characteristics of the surface mesh[9]. However, managing cortical surface meshes that contain a vast number of vertices presents substantial computational difficulties in graph analysis. A common approach to address this is down-sampling the surface mesh, significantly reducing the vertex count before model learning [3, 4, 5]. Nevertheless, this could either diminish the prediction accuracy of the model or reduce its capacity to yield meaningful and interpretable results. Another commonly employed approach for cortical surface manipulation works within a spherical framework [10, 11, 12, 13]. It collects node features from the brain’s surface, sequentially down-samples them, adhering to the hierarchical spherical architecture of the cortical surface, and eventually combines them for prediction. These models are efficient in managing large scale graphs with a high density of nodes. However, they couldn’t flexibly identify the best sub-graph structures or important nodes that contribute best to the prediction task. This might be important because different regions exhibit diverse responses to various predictive demands[17]. In addition to exploring the spatial heterogeneity at the global level, researches to date have seldom focused specifically on the feature-level heterogeneity. That is, different features may exhibit different spatial patterns in prediction tasks. Every cortical feature corresponds to a distinct macro- or micro- structure of the cerebral cortex. Observations during rapid developmental stages[17], the aging process[20], or under pathological conditions[21] have demonstrated that the regional variation in different cortical features could serve as distinct biomarkers for varying brain conditions. Enabling the separate manipulation of each cortical feature within the model, before integrating them for prediction analysis, might significantly boost the model’s performance. We hypothesize that by allowing the autonomous expression of each feature, the model could capture more nuanced and impactful information for prediction tasks. It could help disaggregate the contribution of different cortical features to the prediction, enabling spatial-feature-level interpretations of the model for each subject. A further challenge faced by surface-based models is their interpretability. It involves exploring the diverse characteristics of the cortical surface and identifying biomarkers associated with specific phenotypes. Post-hoc saliency-based methods are widely used, which aim to pinpoint the most impactful input features that contribute to a prediction task by examining the gradients or activations within the network in relation to a specific input[22]. Nevertheless, these methods have limitations[23, 24] and may not always hold for neuroimaging and neuroscience research, where available data are typically small-sized and much more complex [22, 25]. Another strategy is to develop a deep learning model with inherent self-interpretability[26]. This entails creating an end-to-end framework that facilitates the identification of detailed explanatory factors, thereby improving the extraction of discriminative representations and leading to more accurate outcomes. Previous research in this area, such as SiT[18] and NeuroExplainer[25], has shown promising results. Motivated by these findings, creating a surface-based prediction model with built-in interpretability emerges as a significant and promising area of research. To fulfill the outlined requirements and tackle the previously mentioned challenges, inspired by the GNN-based networks and the spherical frameworks, we propose a Surface Graph Neural Network (SurfGNN) as a self-interpretable prediction model. The entire framework of SurfGNN consists of topology-sampling learning (TSL) and region-specific learning (RSL) structures for each cortical feature, and a score-weighted fusion (SWF) structure across all features for prediction. We assess our model within the context of a brain age prediction task. Predicting brain age from structural brain neuroimaging data poses challenges akin to those faced in various neuroimaging applications, serving as a foundation for developing and testing deep learning algorithms. Furthermore, this task has gained attention due to its potential clinical and biological significance [57]. To summarize our contributions as follows: 1. We formulate a graph analysis process comprising TSL and RSL structures, extending from low-level to high-level surface meshes characterized by higher and fewer numbers of vertices, respectively. The TSL efficiently performs sampling on sparse graphs, preserving the overall brain topological shape, while the RSL effectively conducts in-depth graph analysis, distinguishing the varied impacts of different brain regions on prediction. 2. We propose a novel score-weighted fusion mechanism to amalgamate node information derived from individual cortical features within the graph learning framework. This mechanism also facilitates the generation of node scores, providing interpretable results that are specific to each feature. 3. We apply the SurfGNN to a neonatal brain age prediction task on a dataset with morphological features including cortical thickness, sulcal depth, and gray matter/white matter (GM/WM) intensity ratio. Our model outperforms state-of-the-art approaches on it. For each cortical feature, we build the spatial maps based on the node score."
https://arxiv.org/html/2411.05824v1,Navigating Distribution Shifts in Medical Image Analysis: A Survey,"Medical Image Analysis (MedIA) has become indispensable in modern healthcare, enhancing clinical diagnostics and personalized treatment. Despite the remarkable advancements supported by deep learning (DL) technologies, their practical deployment faces challenges due to distribution shifts, where models trained on specific datasets underperform across others from varying hospitals, regions, or patient populations. To navigate this issue, researchers have been actively developing strategies to increase the adaptability and robustness of DL models, enabling their effective use in unfamiliar and diverse environments. This paper systematically reviews approaches that apply DL techniques to MedIA systems affected by distribution shifts. Unlike traditional categorizations based on technical specifications, our approach is grounded in the real-world operational constraints faced by healthcare institutions. Specifically, we categorize the existing body of work into Joint Training, Federated Learning, Fine-tuning, and Domain Generalization, with each method tailored to distinct scenarios caused by Data Accessibility, Privacy Concerns, and Collaborative Protocols. This perspective equips researchers with a nuanced understanding of how DL can be strategically deployed to address distribution shifts in MedIA, ensuring diverse and robust medical applications. By delving deeper into these topics, we highlight potential pathways for future research that not only address existing limitations but also push the boundaries of deployable MedIA technologies.","Medical image analysis (MedIA) [1] has become a cornerstone of modern healthcare, playing a critical role in enhancing diagnostics [2, 3, 4], patient monitoring [5], and treatment planning [6]. With the advent of high-resolution imaging technologies and the increasing complexity of medical data, the application of advanced computational tools has become indispensable. Deep learning (DL) technologies [7, 8, 9, 10], in particular, have revolutionized MedIA by enabling automated and accurate analyses of medical images [11, 12]. These technologies leverage large datasets to train models that can recognize patterns with a precision often surpassing human capabilities [13]. The integration of DL in MedIA not only speeds up diagnostic processes but also offers the potential for personalized healthcare through more accurate patient-specific assessments. However, the application of deep learning techniques in MedIA faces substantial challenges, primarily due to distribution shifts. These shifts occur because the training data (known as source data) used to develop DL models often come from highly controlled environments or specific populations. When deployed in varied medical settings – like different hospitals, population regions, and time periods – these models encounter data that differ significantly in aspects such as imaging modalities [14], scanning protocols [15], patient populations [16], and temporal changes [17]. These variations expose the models to novel, out-of-distribution patterns (referred to as target data) that they have not been trained to recognize, impairing their ability to generalize effectively; this compromised performance in turn undermines the reliability and effectiveness of DL-based diagnostics. Therefore, addressing these distribution shifts is crucial for the effect and reliable deployment of DL technologies in diverse medical environments. To this end, this survey focuses on investigating DL-based MedIA under the challenges posed by distribution shifts. In recent years, the research community has actively developed strategies to enhance the adaptability and robustness of DL models. These strategies aim to mitigate the impact of data distribution shifts across diverse medical settings [18, 19]. In real-world healthcare, the successful deployment of DL technologies often encounters various operational constraints that directly leads to different data distribution shift scenarios. These constraints typically stem from several key factors: - Data Accessibility: This aspect concerns the availability of comprehensive datasets for training DL models. The breadth and quality of accessible data impacts how well a model can be trained to handle varied medical conditions, determining the difficulty level of managing the potential data distribution shifts. - Privacy Concerns: Given the sensitive nature of medical data, privacy concerns [20] revolve around the protection of patient information. These considerations often limit the sharing of medical data among different healthcare institutions, creating data silos that exacerbate the potential data distribution shifts. - Collaborative Protocols: Collaboration among healthcare institutions enables collective efforts to improve diagnostic models across diverse settings. By adhering to different protocols, various collaborative methods [21, 22] have been developed while meeting specific requirements to alleviate the potential distribution shifts. Building on these practical considerations while deploying DL models, we categorize existing efforts to manage distribution shifts in MedIA into a hierarchy from simple to hard: - Joint Training: This approach is feasible when both the source and target data are accessible and there is no privacy concerns. This scenario often occurs when multiple health institutions agree to share their own data, facilitating joint model training [23, 24] and thereby enhancing model adaptability across diverse settings. - Federated Learning: When multiple institutions seek to cooperate without exposing their distinct datasets due to privacy concerns, federated learning [25] offers a powerful solution. It enables collaborative model improvements across different institutions by training models locally on each dataset and aggregating the learned models without centralizing data storage. - Fine-tuning: When synchronous collaborations are not allowed for addressing data distribution shifts with privacy concerns, fine-tuning [26, 27] emerges as an effective remedy. This involves using a well pre-trained model and then fine-tuning it on new datasets to transfer learned knowledge to unfamiliar domains. - Domain Generalization: When data from unseen domains that require model adaptation is inaccessible or unknown, training a model that is generalizable enough to withstand distribution shifts is essential [28, 29]. This involves preparing for unforeseen challenges by developing models that can generalize from the data currently available for training to any potential new environments. In this survey, we present a nuanced understanding of how deep learning can be strategically deployed to address distribution shifts in MedIA, facilitating the development of diverse and robust applications. While a few surveys have also explored the impact of distribution shifts on MedIA and summarized solutions, our work stands apart in several critical ways. For instance, [18] primarily focuses on domain adaptation (DA) within MedIA, categorizing existing methods based on the degree of DL model supervision while [19] emphasizes Domain Generalization (DG) and organizes existing methods according to common MedIA workflows. Although DA and DG are significant topics with profound impacts on MedIA, these surveys [18, 19] concentrate on the technical aspects of existing approaches, treating MedIA primarily as an application domain. Their classifications, rooted in the intricacies of DL techniques, often overlook the real-world medical constraints that give rise to different distribution shift scenarios. Consequently, they fail to provide a detailed, step-by-step guide addressing the impact of medical data variations. Unlike them, our approach is grounded in the practical, operational constraints faced by healthcare institutions, examining current DL techniques in light of the real factors affecting MedIA under distribution shifts. Moreover, while some surveys have explored these issues within medical contexts, they often restrict their discussions to specific scenarios (e.g., heart/lung/brain), such as [30, 31, 32, 33], lacking a comprehensive exploration of distribution shifts within MedIA. Our survey addresses these gaps by offering direct and actionable strategies for deploying DL models under the unique operational constraints encountered in real-world applications. This not only serves as a practical guide for medical professionals on employing deep learning to tackle genuine medical challenges but also underscores the transformative potential of DL technologies in MedIA. By highlighting operational constraints and providing tailored solutions, our survey deepens the understanding and broadens the application of deep learning in MeIA, thereby enhancing both the field and the integration of artificial intelligence in healthcare. TABLE I: Definition of mathematical notations. Notation Definition Notation Definition \mathcal{X},\mathcal{Z},\mathcal{Y} Input, feature, output space P(\cdot) Prior Knowledge \mathbf{x},\mathbf{z},y Input, feature, label variables \mathcal{L}(\cdot,\cdot) Loss function \mathcal{D}_{t} Target Domain q_{\theta}(\cdot) Predictive function \mathcal{D}_{s} Source Domain \mathcal{M}(\cdot) Manipulation function p(\cdot) Probability distribution f(\cdot) Feature mapping function I-A Problem Definition In this section, we formalize the distribution shift problem with notations defined in Table I for easy reading. A domain \mathcal{D} is a joint distribution p(x,y) defined on the input-output space \mathcal{X}\times\mathcal{Y}, where random variables x\in\mathcal{X} and y\in\mathcal{Y} denote the input data and the output label, respectively. We typically deal with two distinct datasets, known as the source and target domains. The Source Domain \mathcal{D}_{s}=\{(x,y)\sim p_{s}(x,y)\}. comprises medical images x such as X-rays or MRI scans, each paired with a label y that might be categorical information regarding disease diagnosis or the segmentation mask. The Target Domain \mathcal{D}_{t}=\{(x,y)\sim p_{t}(x,y)\} originates from a different but related distribution to that of the source. For instance, they might come from different medical imaging devices or patient populations. Note that for both source and target distributions, p_{s}(x,y)=p_{s}(x)p_{s}(y|x) and p_{t}(x,y)=p_{t}(x)p_{t}(y|x). We take the standard covariate shift assumption as Distribution Shift, i.e., p_{s}(y|x)=p_{t}(y|x) and p_{s}(x)\neq p_{t}(x). In this situation, the model q_{{\theta}}(y|x) solely trained on the source domain cannot well represent the true, domain-invariant distribution p(y|x). Therefore, a variety of research concentrates on adjusting q_{{\theta}}(y|x) to maximize its predictive performance on the target distribution."
https://arxiv.org/html/2411.05821v1,"Benchmarking Vision, Language, & Action Models on Robotic Learning Tasks","Vision-language-action (VLA) models represent a promising direction for developing general-purpose robotic systems, demonstrating the ability to combine visual understanding, language comprehension, and action generation. However, systematic evaluation of these models across diverse robotic tasks remains limited. In this work, we present a comprehensive evaluation framework and benchmark suite for assessing VLA models. We profile three state-of-the-art VLM and VLAs —GPT-4o, OpenVLA, and JAT—across 20 diverse datasets from the Open-X-Embodiment collection, evaluating their performance on various manipulation tasks. Our analysis reveals several key insights: (1) current VLA models show significant variation in performance across different tasks and robot platforms, with GPT-4o demonstrating the most consistent performance through sophisticated prompt engineering, (2) all models struggle with complex manipulation tasks requiring multi-step planning, and (3) model performance is notably sensitive to action space characteristics and environmental factors. We release our evaluation framework and findings to facilitate systematic assessment of future VLA models and identify critical areas for improvement in the development of general-purpose robotic systems.","The development of generalizable robotic systems remains a central challenge in machine learning and robotics. While recent advances have enabled impressive capabilities in controlled settings, learned policies frequently fail to generalize beyond their training distribution. This limitation manifests across multiple dimensions: models struggle to respond appropriately to novel task instructions [4, 23], handle variations in object positions and orientations [3], adapt to changing lighting conditions or partially obscured scenes [6], interact with previously unseen objects, or maintain performance in the presence of distracting objects [25, 22]. This brittleness presents a significant barrier to deploying learned robotics systems in unconstrained real-world environments. Concurrent advances in foundation models, particularly in vision and language domains, offer a compelling potential solution to these challenges. Web-scale training has enabled models to realize SotA capabilities in visual recognition [15, 17], complex reasoning about object-agent interactions [2, 8, 24], code generation [5], and multimodal understanding. These models exhibit precisely the kinds of robust generalization capabilities that traditional robotics approaches have found challenging to achieve. The semantic reasoning, problem-solving, and visual interpretation capabilities demonstrated by these models would be tremendously valuable for developing generalist robots capable of performing diverse tasks in dynamic real-world environments. This direction aligns with an emerging trend in machine learning regarding the advantages of unified neural sequence models. Such models continue to show performance improvements even at the frontier of data, compute, and model scale [11, 10]. Moreover, historical trends suggest that generic models capable of effectively leveraging computation tend to eventually supersede specialized domain-specific approaches [21]. Unified sequence models offer several compelling advantages: they eliminate the need for hand-crafted policy architectures with domain-specific inductive biases, can leverage diverse training data through sequence serialization, and demonstrate consistent improvements with increased scale. However, significant challenges remain in applying these models to robotics. While foundation models are typically trained on billions of tokens and images from the web, collecting comparable quantities of robotic interaction data remains infeasible in the near term [7, 13]. Additionally, the direct application of foundation models to robotics tasks presents fundamental technical challenges: these models excel at semantic reasoning and high-level understanding, but robots require precise, grounded, low-level control actions, such as Cartesian end-effector commands. Recent work has attempted to bridge this gap by incorporating language models (LLMs) and vision-language models (VLMs) into robotics systems (Ahn et al., 2022; Driess et al., 2023; Vemprala et al., 2023). However, many of these approaches restrict foundation models to high-level planning, effectively using them as sophisticated state machines that parse commands into primitive actions, which are then executed by separate low-level controllers that cannot leverage the rich semantic knowledge encoded in the foundation models. More recent research has explored bootstrapping robotics representations using pretrained language and vision-language models [19, 18, 12]. Applying these modules in components in planning systems has also been explored [8, 20]. A promising direction has emerged in the form of vision-language-action models (VLAs), which often involve extending different kinds of pretrained foundation models for robotics by pretraining [3], cotraining [22] or fine-tuning [14, 4, 16] visually-conditioned language models to control actions for robots. These models have demonstrated notable transfer to new task scenarios, a compelling first step on the path towards generally useful policies for robots and agents. As these models continue to evolve, there is a critical need for systematic evaluation of their capabilities across both their intended multimodal training domains and out-of-distribution scenarios. Our primary contributions in this paper are: • Detailed profiling results for an initial set of VLM, VLA, and emerging ""generalist"" models, providing insights into their capabilities and limitations • Analysis of generalization • A systematic set of evaluation splits and metrics specifically designed for robotics learning tasks in the widely-used OpenX Dataset • A general framework for mapping VLMs to other modality classes, with particular emphasis on action spaces • Open-source software infrastructure for downloading, managing, and utilizing the benchmark data Through this work, we aim to provide the robotics learning community with robust tools and methodologies for assessing and comparing these emerging approaches, facilitating progress in this rapidly evolving field and helping to bridge the gap between foundation models and practical robotics applications. Importantly, this is the first foray into a new large scale generalist action model benchmark, which we discuss in the context of Future Work."
