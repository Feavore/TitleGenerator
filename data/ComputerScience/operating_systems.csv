URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.08300v1,EDM: An Ultra-Low Latency Ethernet Fabric for Memory Disaggregation,"Achieving low remote memory access latency remains the biggest challenge in realizing memory disaggregation over Ethernet inside the datacenter. We present EDM that tries to overcome this challenge using two key ideas. First, while the existing network protocols for remote memory access over Ethernet, such as TCP/IP and RDMA, are implemented on top of Ethernet’s MAC layer, EDM takes a rather radical approach of implementing the entire network protocol stack for remote memory access within the Physical layer (PHY) of the Ethernet. This overcomes fundamental latency and bandwidth overheads imposed by the MAC layer, especially for small memory messages. Second, EDM implements a centralized, fast, in-network traffic scheduler for memory traffic within the PHY of the Ethernet switch. Inspired by the classic Parallel Iterative Matching (PIM) algorithm, the scheduler dynamically reserves bandwidth between compute and memory nodes by creating virtual circuits in the switch’s PHY, thus eliminating the queuing delay and layer 2 packet processing delay at the switch for memory traffic, with high bandwidth utilization. Our FPGA testbed shows that EDM’s network fabric incurs a latency of only \sim300 ns for remote memory access in an unloaded network, which is an order of magnitude lower than state-of-the-art Ethernet-based solutions such as RoCEv2 and comparable to the emerging PCIe-based solutions such as CXL. Larger-scale network simulations show that even at high network loads, EDM’s latency is within 1.3\times its unloaded latency.","Memory disaggregation is a computing architecture where compute and memory are physically separate blades of resources connected via a network fabric such as the Ethernet. Memory disaggregation promises high compute density, fine-grained memory pooling and provisioning, and elastic memory scaling. Hence, not surprisingly, memory disaggregation has garnered significant interest in recent years, both in industry (hp-the-machine, ; intelrackscale, ; huawei, ) and in academia (osdi16:disaggregated, ; shoal, ; legos, ; mind, ). As the Ethernet link bandwidth reaches 400 Gbps, with Tbps Ethernet on the horizon (terabit-eth, ), prior works (osdi16:disaggregated, ; mind, ) have argued that the Ethernet bandwidth inside datacenters is already plentiful to carry both the memory and traditional IP and storage traffic with minimal bandwidth contention. However, despite the promise of plentiful bandwidth, remote memory access latency remains the key bottleneck in realizing memory disaggregation over Ethernet. As a result, there have been several proposals for alternate fabrics (cxl, ; novakovic14sonuma, ; infiniband, ) for memory disaggregation inside datacenters, most recently, PCIe-based CXL.mem fabric (cxl, ), that promise ultra-low latency for remote memory access. However, such fabrics scale poorly in terms of bandwidth, distance, cost and power compared to Ethernet (§ 2.2). Furthermore, even their latency scales poorly with network load (§ 4.3) due to ineffective mechanisms to handle fabric congestion (aurelia, ). Hence, an ultra-low latency Ethernet fabric, if realized in practice, has the potential to enable scalable, high bandwidth and low latency memory disaggregation at low cost and power. Using Ethernet as the interconnect between the processor and the memory introduces two sources of latency for memory access (Figure 1)—(i) the processor/memory to NIC interconnect latency, and (ii) the Ethernet fabric latency, which includes the latency of the host network protocol stack for remote memory access over the Ethernet (e.g., TCP/IP, RDMA) and the latency of the Ethernet switching network. The traditional PCIe interconnect between the NIC and processor/memory can have latency of the order of \mus (pcie_perf, ). However, in the past several years, various new interconnect specifications (cxl, ; ccix, ; nvlink, ; opencapi, ; ucie, ) have been proposed that either replace or build upon the PCIe physical layer, to dramatically reduce the processor/memory to NIC latency. Most notably, PCIe-based Compute Express Link (CXL.io) (cxl, ) point-to-point peripheral interconnect latency can be of the order of 100 ns (li2023pond, ). The demand for lower latency for cloud services has also prompted a tighter integration of processor and memory with the network controller, that promises to reduce the processor/memory to NIC latency to sub-100 ns. Such designs exist both in academia (e.g., nanoPU (nanopu, ), soNUMA (novakovic14sonuma, ), FAME-1 RISC-V RocketChip SoC (firesim, )) and industry (e.g., Intel Omni-Path (omnipath, )). Further, cloud providers are increasingly offloading processing from CPUs to accelerators, such as FPGAs, for cost efficiency and higher performance (microsoft-fpga, ; gft, ; disagg-nf, ). Such accelerators (stratix10, ; stratixv, ; netfpgasume, ) have processor, memory, and network controller integrated on the same motherboard, thus reducing the processor/memory to NIC latency to as few as 10s of nanoseconds (§ 4.1). Figure 1. A generic architecture for Ethernet-based memory disaggregation. In light of the above (and ongoing (cc-nic, )) architectural and system optimizations to reduce the processor/memory to NIC latency, the Ethernet fabric becomes the primary source of latency for remote memory access. Intra-server (or local) memory access latency typically varies from few 10s of nanoseconds to few 100s of nanoseconds (dram-latency-2, ; dram-latency-1, ), depending upon factors such as memory access pattern and memory location relative to the processor. However, in existing Ethernet-based networks, even with just a single Ethernet switch and highly optimized host network protocol stacks for remote memory access (e.g., RDMA over Converged Ethernet (RoCEv2) (rocev2, )), the Ethernet fabric latency can be as high as few \mus (osdi16:disaggregated, ) even in an unloaded network, which is already an order of magnitude higher than the local memory access latency. And at higher network loads, this baseline latency may easily increase by another order of magnitude due to network queuing and congestion (shoal, ). Given this, we present EDM (Ethernet Disaggregated Memory), which is an ultra-low latency Ethernet fabric for memory disaggregation inside datacenters. Akin to prior works (mind, ), EDM targets a rack or cluster scale memory disaggregation, with a single Ethernet switch connecting hundreds of compute and memory blades. To achieve ultra-low latency with high bandwidth utilization, EDM uses two key design ideas, as discussed below. First, we note that existing Ethernet-based network protocol stacks for remote memory access, such as TCP/IP and RDMA, are implemented on top of Ethernet’s Media Access Control (MAC) layer. Unfortunately, MAC layer imposes fundamental latency and bandwidth overheads for small memory messages due to its operation at frame granularity. In particular, MAC layer imposes a minimum frame size of 64 B and an Inter-frame gap (IFG) of at least 12 bytes. This results in high bandwidth overhead for small memory messages, such as a remote memory read request that only contain the control information for reading from remote memory, e.g., a 64-bit (8 B) remote memory address (§ 2.4). Further, MAC layer does not allow inter-frame preemption. As a result, a small, latency-sensitive memory message cannot preempt the transmission of a large Ethernet frame carrying, e.g., IP data, resulting in significant latency overhead (§ 2.4). To overcome these challenges, EDM employs a rather radical approach of completely bypassing the MAC layer and implementing the entire network protocol stack for remote memory access inside the Physical Coding Sublayer (PCS) of Ethernet’s Physical layer (PHY) at both the host (§ 3.2.1) and the switch (§ 3.2.2). The key insight is that PCS operates at the granularity of 66-bit blocks, that can be leveraged to both reduce the bandwidth overhead for small memory messages as well as enable very fine-grained multiplexing of memory messages and non-memory Ethernet frames (e.g., IP) for lower latency. Further, IFG is accessible inside PCS, which one could repurpose to carry memory messages. EDM’s network stack for remote memory access runs in parallel with the traditional Ethernet network stack for IP and storage traffic. The applications (on the compute node) and the memory controller (on the memory node) sending/receiving memory messages interface directly with EDM in the PHY. EDM encodes/decodes the memory messages to/from a series of 66-bit PHY blocks, all the while ensuring compliance with the Ethernet PHY standard. Furthermore, EDM is the first system to enable intra-frame preemption (§ 3.2.3), by multiplexing the transmission of memory messages and non-memory Ethernet frames at the granularity of 66-bit PHY blocks, while ensuring the higher sublayers in PCS at the receiver still receive the PHY blocks of an Ethernet frame in contiguous clock cycles as required by the Ethernet standard. The second key idea in EDM is to implement a centralized traffic scheduler for memory traffic in the Ethernet switch’s PHY (§ 3.1.1). The scheduler takes as input the current memory traffic demand matrix and implements the classic Parallel Iterative Matching (PIM) (pim, ) to dynamically reserve bandwidth between compute and memory nodes by creating virtual circuits in the switch’s PHY. This proactively ensures no queuing and layer 2 packet processing delay at the switch for memory traffic, while guaranteeing high bandwidth utilization. Furthermore, to achieve near-optimal latency under bandwidth contention, EDM augments PIM with priority-based scheduling, such as SRPT (srtf, ). Implementing EDM’s scheduler in practice requires overcoming two key challenges. First, acquiring the traffic demand matrix for memory traffic accurately and with low overhead. For this, EDM leverages the unique request-reply nature of remote memory reads, where the read request, containing the remote memory address the number of bytes to be read (as required by the memory controller interface, e.g., DDR4), implicitly provides a demand estimation for the corresponding read reply message. Further, by implementing the scheduler in the switch, EDM is able to intercept the read request messages inline and extract the demands without incurring any extra bandwidth or latency overhead. For remote writes, however, EDM does incur bandwidth and latency overhead of sending explicit demand messages to the switch before sending the actual write data. However, EDM manages to keep the bandwidth overhead to minimal through techniques such as batching, and the latency overhead of RTT/2 is nominal at rack or cluster scale. The second challenge is to design a scheduler that can schedule at line rate with small scheduling latency. In a naive implementation, each iteration of the priority-based PIM would take O(log(n)) clock cycles per destination port (in parallel), where n is the number of demand messages per destination port. In contrast, EDM is able to compute this in constant number of clock cycles, owing to EDM’s novel hardware design (§ 3.1.2), that intelligently trades-off hardware resource for time, by using a combination of constant time ordered list data structure that allows for parallel reads, comparisons, and writes, and a fast priority encoder. We implement EDM’s design on FPGAs by extending the PHY of standard 25G Ethernet (§ 4.1). Using a small network testbed of EDM-capable FPGA-based switch and NICs, we show that EDM only incurs a latency of \sim300 ns in an unloaded network, for both remote memory reads and writes (§ 4.2). The read (write) latency is 3.7\times (1.9\times), 6.8\times (3.4\times), and 12.6\times (6.4\times) lower than the latency of raw Ethernet (standard Ethernet MAC + PHY only), RDMA over Converged Ethernet (RoCEv2), and hardware offloaded TCP/IP network stacks respectively. Furthermore, EDM’s latency is comparable to both an intra-server two hop NUMA (dram-latency-1, ) as well as PCIe-based CXL fabric with a single switch hop (li2023pond, ) (while being more scalable and cost efficient than CXL (§ 2.2)). Using larger-scale network simulations over a wide variety of disaggregated network traffic traces from real applications, we show that even at high network loads, EDM’s average latency is within 1.3\times its unloaded latency, and over 7\times lower than CXL (§ 4.3), whose underlying flow control mechanism fails to handle fabric congestion effectively (aurelia, )."
https://arxiv.org/html/2411.06980v1,: Unleashing Storage Hardware-Software Co-design,"NVMe SSD hardware has witnessed widespread deployment as commodity and enterprise hardware due to its high performance and rich feature set. Despite the open specifications of various NVMe protocols by the NVMe Express group and NVMe being touted as the new language of storage, there is a complex labyrinth of software abstractions to program the underlying hardware. The myriad storage I/O paths such as POSIX storage API, ad-hoc OS mechanisms, and userspace I/O libraries have different syntax and semantics that complicate software development and stand in the way of mass adoption and evolution of the NVMe ecosystem. To unify the diverse I/O storage paths, we built xNVMe that exposes a single message-passing API to support both asynchronous and synchronous communication with NVMe devices. xNVMe provides various command sets to support diverse storage I/O paths in different OS (e.g., Linux, FreeBSD, Windows, and MacOS) and userspace libraries (e.g., SPDK) with minimal overhead. xNVMe is an Open Source project and has gained traction amongst various industry stakeholders. In this paper, we elaborate on the lessons that we have learned in the project during its evolution. We also provide some ongoing and future work planned for the project. We hope the database and storage systems community can join in the effort to both extend xNVMe and leverage it as a building block for innovative co-design of storage systems on modern NVMe hardware.","The past decade has witnessed the widespread evolution and deployment of NAND flash memory as commodity and enterprise storage hardware due to its high bandwidth and low latency. The growth of NAND flash SSDs has necessitated the birth of NVMe (Non-volatile memory express) access technology to sidestep the performance limitations of the SATA interface. The current global NVMe technology market share stands at 54.1 billion US$ and is speculated to grow to 412 billion US$ in 2031 (nvm, 2024b). To coordinate the evolution and interoperability of NVMe technologies, the NVM Express working group was formed to create open specifications that could be implemented by hardware and software vendors (nvm, 2024a). Despite the open specifications of NVMe hardware, the challenge of software abstractions to program NVMe hardware remains and is growing with its popularity. Classically, the POSIX storage abstractions (pread, pwrite) had been the holy grail of programmability, stability, and portability to hide the underlying hardware complexity. However, the rise of diverse NVMe hardware has created a fissure in this perfect world. The need for a low-overhead, asynchronous programming model to leverage the high performance of modern NVMe hardware has created multiple complex I/O storage stacks differing from the original POSIX API. For the Linux kernel, there is the POSIX aio_* APIs, libaio, and io_uring storage interfaces(Joshi et al., 2024) that applications can program against. These interfaces differ widely in their API, semantics, and performance. The landscape of storage interfaces gets even more complicated if you factor in different OS. The complexity of the storage I/O stack grows if you factor in support for newer SSD technologies such as ZNS SSDs, FDP SSDs, KV SSDs, and computational storage, to name a few. Either the rich-feature set is hidden behind the block layer interface of the OS or exposed via a userspace I/O stack such as SPDK, custom vendor libraries that are a thin shim over an NVMe device driver. This complexity and fragmentation of various storage I/O paths is unfortunate and creates unnecessary barriers in the adoption of the storage I/O paths by application stacks. The unnecessary complexity also stifles cooperation between software application designers and hardware vendors. As the usage of GPUs and hardware accelerators grows to support AI workloads, a similar API fragmentation is occurring for storage devices that require direct and efficient access by accelerators (Markussen et al., 2020; Qureshi et al., 2023). xNVMe was envisaged to fill the programmability gap for NVMe storage technologies by creating a single unified API that applications can program against to flexibly multiplex the desired storage I/O path with minimal overhead (Lund et al., 2022). Instead of creating yet another abstraction for storage, xNVMe provides a single message passing API for interacting with NVMe devices along with support for various storage I/O paths using this API. Since storage systems are not locked into the API and semantics of a specific I/O path, they can flexibly experiment with different storage I/O paths based on need. xNVMe currently supports various I/O paths in the Linux kernel e.g., libaio, io_uring and the classic POSIX abstractions of pread and pwrite. It also supports other OS e.g., FreeBSD and Windows and userspace I/O stack such as SPDK. xNVMe is an Open Source project (xnv, 2024a) that became publicly available in 2019. It began as an experimental platform for emerging NVMe interfaces (e.g., Open-Channel SSDs (Bjørling et al., 2017)) and matured over time, gaining traction among academic researchers and industry practitioners. In this experience cum vision paper, we present our experiences and lessons learned during the project that has shaped its evolution. We outline the reasons behind its existence in Section 3 and some of the ongoing and future work in the project in Section 5. Our aim with xNVMe is to lower the entry barrier of building innovative data-intensive systems that leverage features of modern NVMe hardware. By providing I/O storage independence and an Open Source collaboration environment, we hope the project can foster co-design of data-intensive software systems and NVMe hardware than what is currently possible."
