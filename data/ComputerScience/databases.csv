URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10342v1,"EHRs Data Harmonization Platform, an easy-to-use shiny app based on  for harmonizing and deriving clinical features","Electronic health records (EHRs) contain important longitudinal information on individuals who have received medical care. Traditionally, EHRs have been used to support a wide range of administrative activities such as billing and clinical workflow, but, given the depth and breadth of clinical and demographic data they contain, they are increasingly being used to provide real-world data for research. Although EHR data have enormous research potential, the full realization of that potential requires a data management strategy that extracts from large EHR databases, that are collected from a range of care settings and time periods, well-documented research-relevant data that can be used by different researchers. Having a common well-documented data management strategy for EHR will support reproducible research and sharing documentation on research variables that are derived from EHR variables is important to open science. In this short paper, we describe the EHRs Data Harmonization Platform. The platform is based on an easy to use web app a publicly available at https://poxotn-arian-aminoleslami.shinyapps.io/Arian/ and as a standalone software package at https://github.com/ArianAminoleslami/EHRs-Data-Harmonization-Platform, that is linked to an existing R library for data harmonization called recodeflow. The platform can be used to extract, document, and harmonize variables from EHR and it can also be used to document and share research variables that have been derived from those EHR data. We provide an example of how this platform is being used to create an environment where multiple research teams can access well-documented data from multiple EHR data sources to conduct reproducible research and where they can share research data sets and the research variables they have created to support open science. We also use a publicly available data set to demonstrate some of the key functions of the platform. We believe this platform can support the use of EHR data for high quality, reproducible research.","Electronic health records. Electronic health records (EHRs) are information systems that collect, store and provide data on healthcare to individuals in digital or electronic format [1]. EHR data are typically collected at the time of healthcare encounters or when services are provided to individuals. These EHR data are collected in multiple care settings and across time and can be linked at the individual level to provide an aggregate description of care for an individual across time and service provider. Historically, these data have been used to support administrative functions such as billing, clinical information sharing and workflow management, but EHRs are increasingly being used to support health and health services research [2, 3]. The data in EHRs typically include basic demographic facts such as date of birth and sex at birth, as well a data on diagnoses, treatments, results of clinical investigations, and outcomes such as hospital discharge or death. Information in EHR is collected at the individual level and using unique patient identifiers it is possible to create trajectories of multiple aspects of care and outcomes over time. These real-world data from EHRs could have enormous value to researchers who are interested in understanding health and disease, in identifying disease trajectories and evaluating disease risks and treatments using modern statistical and machine learning techniques [2] Using electronic health records for research. The process of making EHR data available for research can be divided into two main steps. The first step is the selection and description of the specific variables that the data custodian or owner makes available from the EHRs for research purposes and the second step is the process through which individual research teams select and modify variables that are made available to them. The first step allows the data custodians to select data elements that they feel are sufficiently well characterized and accurate enough to support the scientific research and importantly allows them to explicitly deal with concerns about protecting the privacy of individuals whose person health information is contained in the EHRs [4]. Typically, this aspect limits access to data elements such as date of birth or address that could identify individuals but can be extended to cover opportunities for re-identification through several variables none of which individually is identifying but that can be in combination. This process typically results in the creation of data dictionaries that provide labels and descriptions of the available EHR data elements. The data are often provided in different files from different care settings with different dictionaries. For example, hospital inpatient data in one file and outpatient prescription drug data in another file. The second step in using EHR data for scientific purposes involves selecting the variables from those data sources that are relevant to a specific research project, and creating or deriving research study variables from the available data. This step may vary from research team to research team. For example, a research team focused on breast cancer might focus on using diagnostic codes from hospital inpatient data that identify individuals with different types of breast cancer and then linking to outpatient prescription drug data to identify the specific breast cancer treatments they receive, while a research team looking at emergency room waiting times might focus on data from emergency rooms and define waiting time as the time between registration and assessment. Both teams are drawing from the same publicly available data sources but are drawing on different data elements and creating or deriving research variables from those data elements. An important aspect of multiple research teams making use of common data sources is open science, the notion that science will progress faster if data and knowledge are shared [5]. We have developed a software platform that can support open science in the context of research using publicly available EHR data. The software platform draws on the data dictionaries that are provided to researchers who are accessing EHR data as the input to a process that allows different research teams to use a similar approach to documentation and harmonization of multiple EHR data sources so that the process for data selection and variable derivation can be standardized and shared. EHRs Data Harmonization Platform. The EHRs Data Harmonization Platform is an easy to use publicly available Shiny app that draws on an existing R library: recodeflow. The R library recodeflow was developed as an extension of cchsflow[6, 7] and itself relies on sjmisc [8]. The platform creates shareable documentation of EHR data extraction and derivation that can not only support efforts to make research reproducible, but also will allow researchers to share strategies for data extraction and variable derivation. recodeflow and its components. Doug Manuel and colleagues [6] originally developed cchsflow as a standalone R package and they published it in the Comprehensive R Archive Network (CRAN) [7]. Starting from that specific case focused on Canadian Community Health Survey (CCHS) surveys’ data, the same team developed a general version of this software library, called recodeflow, that is intended for any survey dataset and any EHR dataset. The authors released this R library in CRAN, too [9]. recodeflow package is designed to enhance the reproducibility and standardization of data recoding processes. It leverages two critical components: the “variable details sheet” and the “variable sheet”. The “variable sheet” serves as a repository for essential metadata, including variable labels, which are crucial for accurately interpreting data. This sheet ensures that descriptive information about each variable is consistently applied throughout the analysis, promoting clarity and consistency. The “variable details sheet” also plays a pivotal role in recoding the dataset, as it contains valuable information about variable categories. By harnessing the insights stored in these two spreadsheets, recodeflow empowers researchers to systematically and transparently recode their data, thereby enhancing the rigor and reproducibility of their analytical workflows. We provide a description of essential information stored in each of the spreadsheets along with examples showing how the spreadsheets are filled and should be interpreted in Supplementary Figure S1 and Figure S2. How does the platform utilize recodeflow and contributes in EHRs data harmonization? Doug Manuel and his team built chsflow and recodeflow mainly on sjmisc [10], a popular R package aimed at recoding, dichotomizing or grouping variables, and for general data transformation [8]. sjmisc is available both in CRAN and in Conda [11]. recodeflow performs variable harmonization and derivation, but does not provide a user interface: only software developers proficient in R can use it effectively. Our platform not only helps in creating the above-mentioned spreadsheets in a user-friendly environment, but also gives the opportunity to users to implement the recoding process on their datasets by taking simple steps. It also documents all essential information (such as the functions’ codes to create the derived variables and their names) and therefore, other researchers can reproduce an already existing work by only uploading the required documentation to the app. To be more specific, non-recoded data can be imported to the app with various format such as CSV, SAS7BDAT, RDS, and SQLite. There are also options to handle large datasets to be imported to the app in smaller chunks. Users can create a details sheet from scratch using the basic transformations available in recodeflow (for example, renaming a variable, creating a categories out of a continuous variable, etc.) or creating more complicated derived variables that has more than one components and needs functions to be coded. The platform then uses the information stored in the created spreadsheets to perform the curation on the dataset. The advantage of this standard approach is that once other users want to perform the same curation on a dataset, they don’t need to create everything from scratch. These spreadsheets could be shared with other users, and they can upload them to the platform, modify them if needed, connect their non-curated database and reproduce the same curation on their database. The platform gives the flexibility to the users to save the curated database in various formats. In line with the principles of open science [12], a range of R software libraries and programs are available for data harmonization. retroharmonize, specializes in survey data harmonization by creating a reusable metadata object that supports the generation of data dictionary, amongst other applications[13, 14]. DataHarmonizer is an online platform that was created for genomics data but it can be used for other data. DataHarmonizer [15] builds on a new emerging open data model, LinkML that supports ontologies and transformations to other databases [16]. dxpr integrates and preprocessing electronic health records, focusing on diagnostic and procedure codes [17]. Our EHR Data Harmonization Platform aims to make data transformation as easy as possible with a web-based interface to facilitate tasks that are common for all three applications, including data loading, common transformations, facilitating the generation and organization of R code for new, more complex derived transformation, and display of summary data and metadata. Other software programs aimed at harmonizing EHRs data exist [18, 19, 12], but none of them has become a standard tool in the medical environments worldwide. This study. We organize the rest of this manuscript as follows. After this Introduction, we first provide a case study of the use of the platform to support multiple research teams accessing a new EHR data resource for COVID-19 pandemic research (section 2). We then use a publicly available data set to demonstrate specific functions of the platform (section 3). Finally, we outline a discussion and some conclusions in the section (section 4)."
https://arxiv.org/html/2411.10005v1,Analyzing Performance Characteristics of PostgreSQL and MariaDB on NVMeVirt,"The NVMeVirt paper analyzes the implication of storage performance on database engine performance to promote the tunable performance of NVMeVirt. They perform analysis on two very popular database engines, MariaDB and PostgreSQL. The result shows that MariaDB is more efficient when the storage is slow, but PostgreSQL outperforms MariaDB as I/O bandwidth increases. Although this verifies that NVMeVirt can support advanced storage bandwidth configurations, the paper does not provide a clear explanation of why two database engines react very differently to the storage performance.To understand why the above two database engines have different performance characteristics, we conduct a study of the database engine’s internals. We focus on three major differences in Multi-version concurrency control (MVCC) implementations: version storage, garbage collection, and index management. We also evaluated each scheme’s I/O overhead using OLTP workload. Our analysis identifies the reason why MariaDB outperforms PostgreSQL when the bandwidth is low.","The NVMeVirt is a versatile software-defined virtual NVMe device. Because the NVMeVirt supports advanced storage configurations, it can be used for database engine analysis and allows us to estimate the performance of database engines on future storage devices. In NVMeVirt paper (Kim et al., 2023), the authors conducted an evaluation on PostgreSQL (Group, 2023b) and MariaDB (Foundation, 2023) with OLTP workload using sysbench (Kopytov, 2023). They measured various performance metrics while running the benchmark. The result indicates that MariaDB and PostgreSQL react differently to the storage performance. MariaDB fully utilizes the I/O bandwidth up to 500 MiB/s. However, I/O bandwidth utilization remains around 600 MiB/s even when the storage device provides higher bandwidth. On the other hand, PostgreSQL fully utilizes the I/O bandwidth up to 1,000 MiB/s, and the performance is saturated at approximately 1,800 MiB/s. The I/O bandwidth affects both database engines’ performance, but PostgreSQL is much more sensitive than MariaDB. From the evaluation, the authors conclude that PostgreSQL is more promising on modern storage devices, whereas MariaDB is more efficient when the storage is low. The result verifies the tunable performance of NVMeVirt, but the problem is that the paper does not provide a clear explanation of what features of database engine internals make such differences. In this paper, we analyze the implication of storage performance on database engine performance focusing on database engine internals. We aim to provide a clear explanation of why PostgreSQL is more sensitive to I/O bandwidth than MariaDB. The contributions of this work are as follows: • We perform experiments for both OLTP and OLAP workloads and analyze the different performance characteristics (Section 2). • We analyze what differences in database engine internals make PostgreSQL more sensitive to I/O bandwidth compared to MariaDB (Section 3). • We evaluated different MVCC schemes using OLTP workloads (Section 4)."
https://arxiv.org/html/2411.09997v1,DBenVis: A Visual Analytics System for Comparing DBMS Performance via Benchmark Programs,"Database benchmarking is an essential method for evaluating and comparing the performance characteristics of a database management system (DBMS). It helps researchers and developers to evaluate the efficacy of their optimizations or newly developed DBMS solutions. Also, companies can benefit by analyzing the performance of DBMS under specific workloads and leveraging the result to select the most suitable system for their needs. The proper interpretation of raw benchmark results requires effective visualization, which helps users gain meaningful insights. However, visualization of the results requires prior knowledge, and existing approaches often involve time-consuming manual tasks. This is due to the absence of a unified visual analytics system for benchmark results across diverse DBMSs. To address these challenges, we present DBenVis, an interactive visual analytics system that provides efficient and versatile benchmark results visualization. DBenVis is designed to support both online transaction processing (OLTP) and online analytic processing (OLAP) benchmarks. DBenVis provides an interactive comparison view, which enables users to perform an in-depth analysis of performance characteristics across various metrics among different DBMSs. Notably, we devise an interactive visual encoding idiom for the OLAP benchmark to represent a query execution plan as a tree. In the process of building a system, we propose novel techniques for parsing meaningful data from raw benchmark results and converting the query plan to a D3 hierarchical format. Through case studies conducted with domain experts, we demonstrate the efficacy and usability of DBenVis.","1 Related Work We introduce the research areas related to DBenVis, primarily focusing on two domains: benchmark result visualization and query plan visualization. Our work gets inspiration from existing studies and attempts to overcome some of the limitations in these areas. 1.1 Benchmark Result Visualization Benchmark result visualization is a key aspect in the analysis of database performance. Several popular libraries are used in the visualization process, each with unique features and requirements. Matplotlib [6] is among the most popular Python libraries aimed at creating interactive visualizations. D3 [4] is one of the most famous web-based JavaScript libraries for visualization. Numerous other visualization libraries exist, including Seaborn [10] and FusionCharts. Although these libraries serve as a powerful tool for visualization, the challenge is that they require users to preprocess the raw benchmark results. This requires users to have prior knowledge of result data and the library, making it time-consuming. HammerDB [1] is an open-source database benchmarking tool that allows loading and evaluating DBMS performance using TPC benchmarks and facilitates result visualization. However, its utility is constrained since it supports only a limited range of benchmarks and database systems. These limitations highlight the need for more versatile and user-friendly tools in database benchmark visualization. 1.2 Query Plan Visualization The query planner in DBMS generates possible plans for submitted queries and examines each of these possible execution plans, ultimately selecting the execution plan that is expected to run the fastest. The selection of the right plan to match the query structure and the properties of the data is critical for good performance, so the system includes a complex planner that tries to choose the good plan. EXPLAIN command is used to see what query plan is chosen. The query plan generated from EXPLAIN structure follows a tree format, with scan nodes at the bottom level of the tree. For queries that require joining, aggregation, sorting, or other operations, additional nodes are present to execute these operations. There are only a few query plan visualizers available. The PostgreSQL Explain Visualizer (PEV) [3] provides an interactive interface for visualizing query plans in a tree format. Users can engage with the visualization through clicking, zooming, and panning interactions. Although not as comprehensive as PEV, MySQL does provide its own visualization tool for explaining plans. This tool facilitates a basic understanding of query execution plans within the MySQL environment. Additionally, as far as we surveyed, no query plan visualizer is available for MariaDB. This notable absence highlights the need for a unified visualizer to compare and analyze query plans across various DBMSs."
https://arxiv.org/html/2411.08874v1,A Decidable Case of Query Determinacy: Project-Select Views,Query determinacy is decidable for project-select views and a project-select-join query with no self joins—as long as the selection predicates are in a first-order theory for which satisfiability is decidable.,"1 Summary To enforce a view-based access-control policy, the Blockaid system checks a property on SQL views and queries that generalizes query determinacy under set semantics [9, § 4.2]. A set \mathbf{V} of queries (which we will call “views”) determines a query Q if and only if \mathbf{V}(\mathbf{I})=\mathbf{V}(\mathbf{I}^{\prime}) implies Q(\mathbf{I})=Q(\mathbf{I}^{\prime}) [8].111We may think of \mathbf{V}(I) as a database instance that, for each V\in\mathbf{V}, maps the relation name V to the relation V(\mathbf{I}). Then the formula \mathbf{V}(\mathbf{I})=\mathbf{V}(\mathbf{I}^{\prime}) means \forall\,V\in\mathbf{V}:V(\mathbf{I})=V(\mathbf{I}^{\prime}). Checking query determinacy is a hard problem—it is undecidable even for CQ (conjunctive query) views and CQ queries [3, 4]. It is shown to be decidable in simpler scenarios—for example, (1) for MCQ (monadic-conjunctive-query) views and CQ queries [8, Theorem 5.16], and (2) for a single path-query view and a CQ query [8, Theorem 5.20]. But these decidability results are too limited to be applied to view-based access-control for practical web applications. Here, we discuss another case where query determinacy is decidable: for select-project views, and a select-project-join query with no self joins—as long as the selection predicates are not too complex. To be clear, this result is still quite limited, for the simple reason that real-world views often have joins. But it is a step forward in our search for a larger class of views and queries, encompassing more real-world use cases, for which query determinacy is decidable."
https://arxiv.org/html/2411.08203v1,: abstractions and differential caching for efficient data pre-processing,"Data pre-processing pipelines are the bread and butter of any successful AI project. We introduce a novel programming model for pipelines in a data lakehouse, allowing users to interact declaratively with assets in object storage. Motivated by real-world industry usage patterns, we exploit these new abstractions with a columnar and differential cache to maximize iteration speed for data scientists, who spent most of their time in pre-processing – adding or removing features, restricting or relaxing time windows, wrangling current or older datasets. We show how the new cache works transparently across programming languages, schemas and time windows, and provide preliminary evidence on its efficiency on standard data workloads.","Scans do not repeat themselves, but they often rhyme. (almost) Mark Twain As the already large market for analytics, Business Intelligence and Artificial Intelligence keeps increasing [1], the community is once again re-discovering the pivotal role of data preparation pipelines for the success of any data-driven initiative [2]. In recent years, the lakehouse architecture [3] and, more generally, the decoupling of data and compute became the standard for unified data processing at the enterprise scale: a pre-processing pipeline takes the shape of a directed acyclic graph (DAG), in which nodes are transformations, i.e. functions from dataframe(s) [4] to dataframe which represent the cleaning, aggregation and simplification logic when going from “raw” to “cleaned” dataframes for downstream consumption, e.g. training a Machine Learning model (Fig. 1). As a fast feedback loop is generally recognized as the hallmark of successful data initiatives [5, 6], it is imperative that data scientists can easily experiment with different languages, libraries, and data subsets. Notwithstanding the industry interest in solving the problem [7], popular pipeline frameworks (e.g. [8] [9]) are mostly designed for asynchronous machine execution (e.g. batch jobs at night), and not for iterative work, leaving data scientists to work on small samples and then hand off the project for production refactoring, or roll their own productivity abstractions and constantly re-inventing the wheel. In this short paper, we discuss how to design pipeline abstractions to optimize for user interactivity, and then dive deeper on a columnar cache design that significantly improves performance in iterated workflows. In particular, our main contributions are the following: 1. we introduce Bauplan as a pipeline tool, and discuss its syntax and semantics to run DAGs on top of object storage; in particular, we highlight the distinction between dataframes as logical abstractions vs. dataframes as physical operations; 2. we identify data movement as a primary source of latency for data workloads, and argue that scans over object storage are the atomic building blocks for a heterogeneous set of operations; 3. we share design choices and preliminary results for our differential cache for cloud tables; our cache works transparently across dataset versions, sets of projections and overlapping filters. Finally, we showcase its efficacy with preliminary quantitative benchmarks: compared to baseline, the proposed design allows the system to read up to 30% fewer bytes. Importantly, as our solution is built with open source lakehouse formats in mind (Apache Parquet [10], Apache Iceberg [11])), our insights can be used to improve other cloud-first systems with minimal adaptations. Figure 1: A sample multi-language, cloud data pipeline. The pipeline takes raw data in object storage (S3) to a final training dataset, by going through intermediate steps that wrangle dataframes into progressively cleaner data assets."
https://arxiv.org/html/2411.08041v1,GraphAide: Advanced Graph-Assisted Query and Reasoning System,"Curating knowledge from multiple siloed sources that contain both structured and unstructured data is a major challenge in many real-world applications. Pattern matching and querying represent fundamental tasks in modern data analytics that leverage this curated knowledge. The development of such applications necessitates overcoming several research challenges, including data extraction, named entity recognition, data modeling, and designing query interfaces. Moreover, the explainability of these functionalities is critical for their broader adoption.The emergence of Large Language Models (LLMs) has accelerated the development lifecycle of new capabilities. Nonetheless, there is an ongoing need for domain-specific tools tailored to user activities. The creation of digital assistants has gained considerable traction in recent years, with LLMs offering a promising avenue to develop such assistants utilizing domain-specific knowledge and assumptions.In this context, we introduce an advanced query and reasoning system, GraphAide, which constructs a knowledge graph (KG) from diverse sources and allows to query and reason over the resulting KG. GraphAide harnesses both the KG and LLMs to rapidly develop domain-specific digital assistants. It integrates design patterns from retrieval augmented generation (RAG) and the semantic web to create an agentic LLM application. GraphAide underscores the potential for streamlined and efficient development of specialized digital assistants, thereby enhancing their applicability across various domains.","Large Language Models (LLMs) Brown et al. (2020) represent the cutting edge of generative artificial intelligence (GenAI) and machine learning research and development. They have been adopted at an extraordinary rate across a range of disciplines, including scientific research, engineering, economics, and social sciences. By enabling domain experts to utilize pre-trained, ready-to-use models, LLMs have democratized the application development process, catalyzing innovations in both scientific and technological domains. The development of LLM-based applications remains a dynamic area of research, offering superior performance in tasks such as summarization, correlation, and inference across various input sources Zhang et al. (2024). Knowledge Graphs (KGs) are formal representations of key concepts as entities and the relationships between them Berners-Lee et al. (2023); Bizer et al. (2023). KGs serve as powerful tools that can be employed in diverse user environments as foundational reference knowledge sources. However, constructing these KGs presents substantial challenges due to the scale of input data, the heterogeneity of use-case-specific concepts, and the costs associated with KG construction. LLMs have demonstrated impressive accuracy in extracting relevant entities and forming relationships, significantly contributing to KG construction. A major challenge in developing accurate and consistent capabilities based on LLMs is hallucination Yao et al. (2023), which occurs when LLMs generate non-existent facts in response to user queries. The memorization of training data by LLMs and the subsequent reliance on corpus-based heuristics are significant factors contributing to hallucinations McKenna et al. (2023). Addressing this issue is crucial for the reliability and trustworthiness of LLM-derived applications. Retrieval-augmented generation (RAG) Lewis et al. (2020) is a well-established design pattern aimed at mitigating hallucination in Large Language Models (LLMs) by providing additional context during the generation of responses to user queries. This additional context is domain-specific and acts as grounding for the model’s generative capabilities. A foundational RAG-based system employs a vector database to store embeddings of a domain-specific knowledge corpus and utilizes semantic similarity algorithms to retrieve the context relevant to the user query. This context is then combined with the user query and forwarded to the LLM, serving as a guardrail for its generation process. The RAG-based approach has demonstrated efficacy in reducing hallucination by constraining the LLM’s generative output to the localized region of the provided context. However, a vector search-based method is constrained by the semantic similarity between the query and the corpus. It fails to leverage the structural context, such as the relationships between documents within the corpus, their metadata, or the associated reasoning. Knowledge Graphs (KGs) offer a robust solution to these limitations by efficiently storing domain-specific information and establishing relationships between information sources dispersed across documents in the corpus. KGs can also significantly enhance various components of an LLM-based application, including retrieval, summarization, and inference. By integrating KGs into the design pattern, the system can utilize both the semantic and structural context, thereby improving the accuracy and consistency of the generated responses. This comprehensive approach enables a more nuanced understanding and utilization of the knowledge corpus, offering a sophisticated strategy to further reduce hallucination and enhance the application’s overall performance."
https://arxiv.org/html/2411.08647v1,"The Galactica database: an open, generic and versatile tool for the dissemination of simulation data in astrophysics","The Galactica simulation database is a platform designed to assist computational astrophysicists with their open science approach based on FAIR (Findable, Accessible, Interoperable, Reusable) principles. It offers the means to publish their numerical simulation projects, whatever their field of application or research theme and provides access to reduced datasets and object catalogs online. The application implements the Simulation Datamodel IVOA standard.To provide the scientific community indirect access to raw simulation data, Galactica can generate, on an ”on-demand” basis, custom high-level data products to meet specific user requirements. These data products, accessible through online WebServices, are produced remotely from the raw simulation datasets. To that end, the Galactica central web application communicates with a high-scalability ecosystem of data-processing servers called Terminus by means of an industry-proven asynchronous task management system. Each Terminus node, hosted in a research institute, a regional or national supercomputing facility, contributes to the ecosystem by providing both the storage and the computational resources required to store the massive simulation datasets and post-process them to create the data products requested on Galactica, hence guaranteeing fine-grained sovereignty over data and resources.This distributed architecture is very versatile, it can be interfaced with any kind of data-processing software, written in any language, handling raw data produced by every type of simulation code used in the field of computational astrophysics. Its generality and versatility, together with its excellent scalability makes it a powerful tool for the scientific community to disseminate numerical models in astrophysics in the exascale era.","The Amsterdam call for Open Science in 2016 started to promote public access to both the scientific publications and the data obtained with public funds. This call has been implemented at national level all over the world in recent years in the form of Open Science plans and programs to encourage the effective sharing of publications and research data. The first international framework on open science, the UNESCO Recommendation on Open Science (UNESCO, 2021), was adopted by 193 countries attending UNESCO’s General Conference in 2021. In recent years, data publication and reuse has become a key requirement demanded by both funding agencies and resource (computation or observation time) allocation committees. In the field of astrophysics, a number of initiatives have been taken to disseminate scientific data, mostly in astronomy and to a lesser extent, for numerical models in computational astrophysics. Online science platforms with astronomical data have been made available in the past few decades, e.g. the Sloan Digital Sky Survey (York et al., 2000), the Dark Energy Survey Science Portal (The Dark Energy Survey Collaboration, 2005; Fausti Neto et al., 2018) to provide the scientific community with access to theses surveys. In the same manner, similar dedicated science portals are currently being developed for observational data that will be produced by the Euclid space telescope (Laureijs et al., 2011), the Square Kilometer Array (SKA) telescope (Lazio, 2009) or the Vera Rubin Observatory (Ivezić et al., 2019). If the dissemination of astronomical data has been greatly facilitated by the standardization of file format, for example the FITS format (Wells et al., 1981), or the MeasurementSet (Kemball & Wieringa, 2000) standard in radio astronomy, computational astrophysicists lack a standardized data model to enable them to exchange their numerical simulation data in a uniform way. Nevertheless, a few simulation projects have released their data to the community, many of them in the cosmology field: e.g. the MultiDark (Prada et al., 2012) and Bolshoi (Klypin et al., 2011) simulations hosted on the CosmoSim database, the galaxy cluster merger catalog (ZuHone et al., 2018) hosted on the yt Hub, the Illustris project (Vogelsberger et al., 2014; Nelson et al., 2019), the web portal for cosmological hydrodynamical simulations (Ragagnin et al., 2017), CosmoHub (Carretero et al., 2017; Tallada et al., 2020) or the Theoretical Astrophysical Observatory (Bernyk et al., 2016). Some projects even attempted to combine infrastructures in the field of turbulence studies, e.g. the Johns Hopkins Turbulence Database (Li et al., 2008), the Catalogue for Astrophysical Turbulence Simulations (Burkhart et al., 2020) or in the field of star formation (the StarFormat database) or interstellar medium (ISMDB). One of the major collaborative efforts led to the release of the generic Django-Daiquiri framework (Galkin et al., 2020, and references therin), a web application developed with the aim of being deployed for each project to host the data produced by the numerical simulations conducted in the project. It greatly lowered the technical barrier individual research groups have to overcome to publish their data on a web application, but unfortunately the required technical expertise and maintenance expenses are still out of reach for a majority of small individual projects led by computational astrophysicists."
https://arxiv.org/html/2411.08599v1,XiYan-SQL: A Multi-Generator Ensemble Framework for Text-to-SQL,"To tackle the challenges of large language model performance in natural language to SQL tasks, we introduce XiYan-SQL, an innovative framework that employs a multi-generator ensemble strategy to improve candidate generation. We introduce M-Schema, a semi-structured schema representation method designed to enhance the understanding of database structures. To enhance the quality and diversity of generated candidate SQL queries, XiYan-SQL integrates the significant potential of in-context learning (ICL) with the precise control of supervised fine-tuning. On one hand, we propose a series of training strategies to fine-tune models to generate high-quality candidates with diverse preferences. On the other hand, we implement the ICL approach with an example selection method based on named entity recognition to prevent overemphasis on entities. The refiner optimizes each candidate by correcting logical or syntactical errors. To address the challenge of identifying the best candidate, we fine-tune a selection model to distinguish nuances of candidate SQL queries. The experimental results on multiple dialect datasets demonstrate the robustness of XiYan-SQL in addressing challenges across different scenarios. Overall, our proposed XiYan-SQL achieves the state-of-the-art execution accuracy of 89.65% on the Spider test set, 69.86% on SQL-Eval, 41.20% on NL2GQL, and a competitive score of 72.23% on the Bird development benchmark. The proposed framework not only enhances the quality and diversity of SQL queries but also outperforms previous methods.Keywords LLM, Text-to-SQL, NL2SQL","The ability to convert natural language queries into structured query language (SQL) through natural language to SQL (NL2SQL) technology represents a significant advancement in making complex datasets more accessible. It greatly facilitates both non-expert and advanced users in extracting valuable insights from extensive data repositories [2, 15, 24, 27, 6, 10, 13, 29, 20, 19, 23, 22]. Recent advancements in large language models (LLMs) have significantly enhanced the efficacy and accuracy of NL2SQL applications. There are generally two approaches for NL2SQL solutions based on LLMs: prompt engineering [3, 5, 17, 18], and supervised fine-tuning (SFT) [9]. Prompt engineering leverages the intrinsic capabilities of the model by optimizing prompts to generate diverse SQL queries. Prompt engineering has demonstrated promising results in NL2SQL using zero-shot [3] or few-shot prompting [28, 5, 18]. This type of approach typically employs closed-source models with enormous parameters, such as GPT-4 [1] and Gemini 1.5 [26], which present significant potential and powerful generalization capability. However, most methods rely on multi-path generation and selecting the best option utilizing self-consistency, resulting in significant inference overheads. Approaches based on SFT seek to fine-tune models with much smaller parameter sizes on the NL2SQL task to produce more controllable SQL queries, such as CodeS [9]. Nevertheless, due to their limited parameters, these methods struggle to perform complex NL2SQL reasoning and transfer to databases within a new domain. In this technical report, we propose XiYan-SQL, a novel NL2SQL framework that employs a multi-generator ensemble strategy to enhance candidate generation. XiYan-SQL combines prompt engineering and the SFT method to generate candidate SQL queries with high quality and diversity. To enhance high quality, we take advantage of the high controllability of SFT and utilize a range of training strategies to specifically fine-tune models to generate candidates with different preferences. We introduce a two-stage multi-task training approach, which first activates the model’s fundamental SQL generation capabilities, and subsequently transitions to a model with enhanced semantic understanding and diverse stylistic preferences. To enhance diversity of generated candidates and capability of generating complex SQL queries, we utilize in-context learning to prompt LLMs. We propose to extract the skeleton of the questions by masking the named entities with common special tokens and using skeleton similarity to select and organize useful examples. Then, each generator is followed by a refiner to correct logical or syntactical error based on execution results or error information. Finally, a selection agent is required to select the best option. Most existing works use self-consistency, but the most consistent result is not always the correct case. So we propose to fine-tune a model to understand and identify the subtle differences among candidates and pick the final response. Additionally, to enhance LLMs for better understanding of the database schema, we propose a new schema representation method named M-Schema. Inspired by MAC-SQL Schema [28], M-Schema presents the hierarchical structure between databases, tables, and columns in a semi-structured form. We revised MAC-SQL Schema by adding data types and resulting in a more compact and clear format. We conduct experiments to compare the impact of different schema representations on NL2SQL performance. In comparison to DDL Schema and MAC-SQL Schema, LLMs using M-Schema demonstrate superior performance. We present comprehensive evaluations on both relational and non-relational databases, specifically focusing on prominent systems such as SQLite, PostgreSQL, and nGQL. XiYan-SQL demonstrates remarkable performance across a range of benchmarks, achieving the state-of-the-art performance on the Spider [32], SQL-Eval, and NL2GQL [33] datasets with 89.65%, 69.86%, and 41.20% execution accuracy, respectively. In the context of the more challenging Bird [10] benchmark, XiYan-SQL also achieves a competitive score of 72.23%. The impressive results achieved on various challenging NL2SQL benchmarks not only validate the effectiveness of our approach but also demonstrate its significant potential for broader applications in NL2SQL translation tasks. XiYan-SQL can be accessed from https://bailian.console.aliyun.com/xiyan. We also release the source code for connecting to the database and building M-Schema at https://github.com/XGenerationLab/M-Schema."
https://arxiv.org/html/2411.08077v1,DBgDel: Database-Enhanced Gene Deletion Framework for Growth-Coupled Production in Genome-Scale Metabolic Models,"When simulating metabolite productions with genome-scale constraint-based metabolic models, gene deletion strategies are necessary to achieve growth-coupled production, which means cell growth and target metabolite production occur simultaneously. Since obtaining gene deletion strategies for large genome-scale models suffers from significant computational time, it is necessary to develop methods to mitigate this computational burden. In this study, we introduce a novel framework for computing gene deletion strategies. The proposed framework first mines related databases to extract prior information about gene deletions for growth-coupled production. It then integrates the extracted information with downstream algorithms to narrow down the algorithmic search space, resulting in highly efficient calculations on genome-scale models. Computational experiment results demonstrated that our framework can compute stoichiometrically feasible gene deletion strategies for numerous target metabolites, showcasing a noteworthy improvement in computational efficiency. Specifically, our framework achieves an average 6.1-fold acceleration in computational speed compared to existing methods while maintaining a respectable success rate. The source code of DBgDel with examples are available on https://github.com/MetNetComp/DBgDel.","I INTRODUCTION Computational approaches are essential in many metabolic engineering applications [1, 2, 3, 4, 5]. A representative example is computational strain design, which relies on mathematical models to simulate a microorganism’s metabolic processes and the production of target metabolites. In genome-scale metabolic engineering simulations, the constraint-based model is one of the most popular models. This model typically comprises two components: (1) a metabolic network and (2) gene-protein-reaction (GPR) rules. A metabolic network serves as the backbone of the whole model. It contains chemical reactions that define the transformation relationships between metabolites. Enzymatic proteins encoded by genes catalyze the chemical reactions within cells, associating these reactions with specific genes. Accordingly, GPR rules employ Boolean functions to represent these relationships between genes, proteins, and reactions. In this manner, a metabolic network and GPR rules can model the metabolic mechanisms of a specific microorganism. Furthermore, a metabolic network can be modulated by influencing its reactions. Specifically, we can ”turn off” certain reactions by deleting genes that encode the required enzymes. This operation of deleting genes to reshape metabolic networks is the core idea of metabolic engineering. During simulation in constraint-based models, the primary objective is to achieve growth-coupled production for target metabolites. Growth-coupled production means cell growth is bonded to the synthesis of target metabolites in the metabolic process of microorganisms. Coupling cell growth with the production of target metabolites is necessary. The reason is in industrial practices, microorganism genotypes with higher cell growth are more likely to persist in the culture through repeated passaging. However, in the natural metabolic state of most microorganisms, only a limited number of metabolites meet the criteria for growth-coupled production. Therefore, achieving growth-coupled production for most target metabolites requires calculating gene deletion strategies to reshape the metabolic network [6]. But this task is far from trivial. Calculating gene deletion strategies demands huge computational resources, especially when simulating genome-scale models with complex metabolic networks and GPR rules involving many genes. Several methods have been proposed to address this issue. Among the existing approaches [7, 8, 9, 10, 11, 12, 13, 14] for calculating gene deletion strategies in growth-coupled production, one of the most efficient is the elementary flux vector-based method [15]. Its basic idea is to identify a minimal set of reactions in the flow where cell growth compels the production of the target metabolite. Specifically, the elementary flux vector-based method determines a non-decomposable flux distribution that encompasses (1) the cell growth reaction and (2) the target metabolite production reaction, then the reactions not utilized by the flux distribution are eliminated. This method has demonstrated its capability to compute reaction deletions for growth-coupled production for most target metabolites in common model microorganisms like E. coli and S. cerevisiae [15]. Furthermore, the elementary flux vector-based method has been successfully extended and applied to calculate the growth-coupled production of valuable metabolites, including itaconic acid and 2,3-butanediol in E. coli [16], as well as indigoidine in P. putida [17]. More recently, Tamura et al. introduced gDel_minRN [18], a tool to calculate gene deletion strategies. It operates by maximizing the number of repressed reactions, thereby extracting essential core components for growth-coupled production. Experimental validations have demonstrated that in comparison with other methods, gDel_minRN stands out as one of the most effective approaches currently available for calculating gene deletions. However, despite numerous efforts, these approaches exhibit a common shortcoming. All of the present methods employ de novo calculation strategies when deriving gene deletion strategies for various target metabolites. This approach overlooks the shared information among different target metabolites, potentially leading to an excessive number of unnecessary repeated calculations. On the other side, efforts have been made to establish databases on gene deletion strategies to address the growing demand for computational experiment data in strain design. A noteworthy example is the MetNetComp [19] database. It is a web-based system that offers information on gene deletion strategies for growth-coupled production in constraint-based metabolic networks. MetNetComp encompasses a vast repository of over 85,000 gene deletion strategies for metabolites across various constraint-based models from different species. While the advent of such databases has significant potential to foster new research paradigms in the field, the exploration of their utility, especially concerning computational gene deletion strategies, is still in its infancy. Therefore, it would be desirable to develop a method to combine existing database resources to alleviate the computational burden and efficiently calculate gene deletion strategies. However, extracting information that contributes to efficient computation from related databases and effectively integrating it into algorithms is not a straightforward task. In this study, the authors propose DBgDel, a novel database-enhance framework for calculating gene deletion strategies for growth-coupled production on genome-scale models. DBgDel extracts information on gene deletion from the related database to boost the computational efficiency of downstream algorithms. DBgDel comprises two steps: (1) STEP 1, extracting the gene set from a gene deletion database that corresponds to the essential core components for growth-coupled production for a given constraint-based model; and (2) STEP 2, using an extended version of gDel_minRN that incorporates the gene set extracted from the STEP 1 as the initial gene pool to narrow down the algorithmic search space. In the computational experiments, we compared DBgDel with gDel_minRN [18], GDLS [7], and optGene [20]. Besides the aforementioned algorithm gDel_minRN, GDLS and optGene are also the most widely used software to derive gene deletion strategies, which are available in the COBRA Toolbox [21]. All these methods were applied to three constraint-based models including e_coli_core [22], iMM904 [23], and iML1515 [24]. e_coli_core is a constraint-based model that contains the only essential part of the metabolism of E. coli; iMM904 and iML1515 are genome-scale constraint-based models of S. cerevisiae and E. coli, respectively; The average elapsed time of DBgDel for e_coli_core, iMM904, and iML1515 were 1.12s, 79.92s, and 431.75s, respectively, which were substantially faster than gDel_minRN, GDLS and optGene. In the meantime, DBgDel achieves the success ratio for e_coli_core, iMM904, and iML1515 at 60.0%, 11.5%, and 51.2%, which were substantially better than GDLS and optGene, and closely similar to gDel_minRN . To the best of our knowledge, this study is the first attempt to extract information from pre-existing knowledge in gene deletion databases and integrate it with algorithms to derive new gene deletion strategies. Furthermore, in comparison to existing approaches, our proposed method achieves a decent tradeoff between the success rate and time-consuming: it attains noteworthy enhancements in computational efficiency, all while maintaining a high success rate. The remaining sections of this paper are organized as follows: Section II-A formularizes several fundamental concepts in this study; Section II-B and II-C describes the main problem of this study mathematically and illustrates it with a small example; Section III illustrates the proposed framework DBgDel with a small example and provides the corresponding pseudo-code; Section IV-A describes the basic experiment setting; Section IV-B describes the experiment results: (1) the performance comparison of DBgDel, gDel_minRN, GDLS, and optGene for e_coli_core, iMM904, and iML1515, (2) the performance comparison of DBgDel based on different initial gene pools, including Predicted-G_{\text{remain}} genes as the default setting, G_{\text{remain}} genes, and growth essential genes; Section V analyzes the results of the experiments, evaluates the performance of DBgDel and other methods, and discusses future work."
https://arxiv.org/html/2411.07490v1,: A Method to Extract Event Logs for Object-Centric Processes,"Real-world processes involve multiple object types with intricate interrelationships. Traditional event logs (in XES format), which record process execution centred around the case notion, are restricted to a single-object perspective, making it difficult to capture the behaviour of multiple objects and their interactions. To address this limitation, object-centric event logs (OCEL) have been introduced to capture both the objects involved in a process and their interactions with events. The object-centric event data (OCED) metamodel extends the OCEL format by further capturing dynamic object attributes and object-to-object relations. Recently OCEL 2.0 has been proposed based on OCED metamodel. Current research on generating OCEL logs requires specific input data sources, and resulting log data often fails to fully conform to OCEL 2.0. Moreover, the generated OCEL logs vary across different representational formats and their quality remains unevaluated. To address these challenges, a set of quality criteria for evaluating OCEL log representations is established. Guided by these criteria, Dirigo is proposed—a method for extracting event logs that not only conforms to OCEL 2.0 but also extends it by capturing the temporal aspect of dynamic object-to-object relations. Object-role Modelling (ORM), a conceptual data modelling technique, is employed to describe the artifact produced at each step of Dirigo. To validate the applicability of Dirigo, it is applied to a real-life use case, extracting an event log via simulation. The quality of the log representation of the extracted event log is compared to those of existing OCEL logs using the established quality criteria.","Process mining has gained popularity for its ability to analyse and improve business processes in an evidence-based manner [1]. Its key input—the event log—captures multidimensional time sequence data characterising process execution. Traditionally, event logs are captured in XES format [14], represented as tables where each row is an event capturing the execution of an activity related to a single object (the case), and each column specifies an event attribute. Real-world processes involve multiple object types, where the relations between objects and interactions between objects and process events are complex [1]. The traditional event log format is restricted to a single-object perspective and thus cannot capture the complex behaviour of real-world processes [1]. To address this limitation, object-centric event logs (OCEL 1.0) [9] have been introduced to capture the objects involved in a process and their interactions with events. The object-centric event data (OCED) meta-model [1] further extends this by introducing dynamic object attributes and relations between objects. Recently, based on OCED, which is considered “an early version of the OCEL 2.0 metamodel” [7], OCEL 2.0 was released. Among the many studies [6, 23, 21] focused on object-centric process mining, only a few have proposed methods for generating OCEL, and these often require specific input data sources like ERP [4, 5] and blockchain systems [22]. Some work proposes methods to extract OCEL from knowledge graphs [28, 18] or traditional XES event logs [25]. Moreover, these studies rarely discuss the quality of the generated logs. Some methods do not always capture all necessary object-to-object relations and for some methods, at times, the semantics of those object-to-object relations that are captured are not entirely clear. In this paper, we address existing research gaps by proposing a novel method, namely Dirigo111Dirigo means “I guide” in Latin., to extract event logs for object-centric processes. Our method is guided by the widely adopted Design Science research methodology [24] and adheres to the design principles of operating at a conceptual level and maintaining generality. We employ Object-role Modelling (ORM) [13], a conceptual data modelling technique, to describe the artifact produced at each step of Dirigo. As such, the method is not limited to specific input data sources and is independent of any particular systems, tools, or domains. The resulting OCEL log schema not only conforms to OCEL 2.0 [7], but also extends it by capturing the temporal aspect of dynamic object relationships and explicitly capturing resources as event attributes. To develop the Dirigo method, we first establish a set of quality criteria for evaluating OCEL log representations. These criteria guide the design of the Dirigo method to ensure the generation of high-quality log representations. To validate the applicability of the proposed method, we apply it to a real-life use case, and extract an event log via simulation. We then assess the quality of the log representation of the extracted event log in comparison to those of existing OCEL logs, using the established quality criteria. The extracted event log from the real-life use case, along with the simulation model and evaluation experiments, are made publicly available to ensure transparency and replicability. The remainder of this paper is structured as follows. Sect. 2 introduces key concepts in object-centric processes. Sect. 3 reviews related work. Sect. 4 outlines the research methodology, design principles, and established quality criteria for evaluation. Sect. 5 details the design of the proposed method Dirigo. Sect. 6 presents the evaluation of the generated log using the established quality criteria. Sect. 7 summarises our findings and suggests directions for future research."
https://arxiv.org/html/2411.07589v1,Overhead-free User-side Recommender Systems,"Traditionally, recommendation algorithms have been designed for service developers. But recently, a new paradigm called user-side recommender systems has been proposed. User-side recommender systems are built and used by end users, in sharp contrast to traditional provider-side recommender systems. Even if the official recommender system offered by the provider is not fair, end users can create and enjoy their own user-side recommender systems by themselves. Although the concept of user-side recommender systems is attractive, the problem is they require tremendous communication costs between the user and the official system. Even the most efficient user-side recommender systems require about 5\times more costs than provider-side recommender systems. Such high costs hinder the adoption of user-side recommender systems. In this paper, we propose overhead-free user-side recommender systems, RecCycle, which realizes user-side recommender systems without any communication overhead. The main idea of RecCycle is to recycle past recommendation results offered by the provider’s recommender systems. The ingredients of RecCycle can be retrieved “for free,” and it greatly reduces the cost of user-side recommendations. In the experiments, we confirm that RecCycle performs as well as state-of-the-art user-side recommendation algorithms while RecCycle reduces costs significantly.","Recommender systems have been used in many web services (Linden et al., 2003; Geyik et al., 2019). It was estimated that 35 % of purchases on Amazon and 75 % of watches on Netflix came from recommender systems (MacKenzie et al., 2013). Recommender systems are indispensable both for businesses and users. Although traditional recommender systems aim only at conversion, many fine-grained demands for recommender systems have emerged. Users may want to receive fair recommendations (Kamishima et al., 2012a; Biega et al., 2018; Milano et al., 2020) or serendipitous recommendations (Chen et al., 2021; Anderson et al., 2020; Steck, 2018; Mladenov et al., 2020; Zheng et al., 2018), or users may want recommender systems to be transparent (Sinha and Swearingen, 2002; Balog et al., 2019) and steerable (Green et al., 2009; Balog et al., 2019). For example, on LinkedIn, recruiters may want to receive account recommendations that are fair in terms of gender and race to avoid (implicit) discrimination. A citizen who gathers information for election may want to receive both Republican and Democrat news equitably to avoid filter bubbles (Pariser, 2011). Cinema enthusiasts may want to receive recommendations that involve minor movies instead of popular movies that enthusiasts already know. However, there are too many kinds of demands, and the service provider cannot cope with all of them. Besides, service provider may not implement such functionalities on purpuse. For example, some service providers may intentionally choose to increase short-term conversions instead of caring the fairness of the platform. If the service provider does not implement fair recommender systems, users are forced to use unfair ones or quit the service. It has been considered that users have little ability to change the recommendations. In most cases, the only option available to the user is to wait until the service implements the functionality. Green et al. (2009) also pointed out that “If users are unsatisfied with the recommendations generated by a particular system, often their only way to change how recommendations are generated in the future is to provide thumbs-up or thumbs-down ratings to the system.” User-side recommender systems (Sato, 2022b) offer a proactive solution to this problem. Users can build their own (i.e., private, personal, or user-side) recommender systems to ensure recommendations are made fairly and transparently. Since the system is built by the user, it can be customized to meet specific criteria they want and add the functionalities they want. User-side recommender systems realize ultimate personalization. Table 1. Properties of user-side recommender systems. The definitions of these properties are shown in Section 4.4. Postprocessing (PP) applies postprocessing directly to the official recommender system, which is not sound when the list does not contain some sensitive groups (See also Section 4.1). PP PrivateRank (Sato, 2022b) PrivateWalk (Sato, 2022b) ETP (Sato, 2022d) Consul (Sato, 2022d) RecCycle (ours) Consistent ✓ ✓ ✗ ✓ ✓ ✓ Sound ✗ ✓ ✓ ✓ ✓ ✓ Local ✓ ✗ ✓ ✗ ✓ ✓ Overhead-free ✓ ✗ ✗ ✗ ✗ ✓ The concept of user-side recommender systems looks similar to steerable (Green et al., 2009) (or scrutable (Balog et al., 2019)) recommender systems at first glance. Steerable recommender systems also allow users to control the recommendation results. However, the key difference is that steerable systems are implemented by the service provider, while user-side recommender systems are built by the users themselves. What to steel is chosen by the service provider in traditional steerable recommender systems. If the recommender system in use is not steerable in the way they want, users cannot enjoy steerability and must wait for the service provider to implement it. By contrast, user-side recommender systems allow users to make the system steerable, even if the service provider implemented only a standard non-steerable system. Although user-side recommender systems are attractive, building them is challenging. End users do not have access to the data stored in the service’s database, unlike the developers employed by the service provider. Most modern recommender systems rely on user log data and/or item features to make recommendations. At first glance, it seems impossible to build an effective recommender system without such data. Sato (2022b) addressed this problem by using the official recommender systems provided by the target web service. Although the official recommender systems are black-box and possibly unfair, Sato’s methods turn them into fair and transparent ones on the user’s side by combining multiple outputs. However, existing user-side recommender systems issue multiple queries to the official (possibly unfair) recommender system to build a single (fair) recommendation list. In other words, these methods trade communication costs with fairness. The drawback of this approach is the communication cost. Even the most efficient user-side recommender systems, Consul, require 5 queries to build a recommendation list (Sato, 2022d). This means that Consul loads the service 5 times more. Such a high communication cost causes problems. First, the service provider may disfavor and prohibit such users’ activities to mitigate the load on the service. Second, end users cannot afford to pay the high API cost. Third, such systems are not suitable for real-time applications due to the response time of the multiple queries. We advocate that the communication cost between the end user and the service is crucial for effective user-side recommender systems. An ideal user-side system works as if it were an official system. The recommendation list should be shown to the user at the same time as the official system. However, existing user-side recommender systems require additional queries and thus require more loading time than the official system, which leads to a poor user experience. We propose overhead-free user-side recommender systems, RecCycle (recommendation + recycle), to address this problem. The main idea of RecCycle is to recycle past recommendation results presented by the provider’s recommender systems when the user uses the system as usual. These recommendation results used to be discarded once shown on the page. Sometimes, these recommendations are just shown on the page and do not catch the attention of the user due to the position and/or timing of the presentation. RecCycle “recycles” these information to create new recommendations on the user’s side. These information can be used “for free”, i.e., without any additional communication cost. All of the computation for RecCycle is done locally. RecCycle is so communication efficient that it can realize real-time user-side recommendations, and the user can enjoy the recommendations as if they were shown by the official system. RecCycle can be combined with existing user-side recommender systems. We will elaborate on the premise of RecCycle in the following sections. As a special case, we show that RecCycle can be combined with Consul (Sato, 2022d), which leads to consistent, sound, local, and overhead-free user-side recommender systems (Table 1). In the experiments, we confirm that RecCycle performs as well as state-of-the-art user-side recommendation algorithms while RecCycle reduces costs significantly. The contributions of this study are as follows: • We propose overhead-free user-side recommender systems, RecCycle, for the first time. • We show that RecCycle is consistent, sound, and local, as well as overhead-free. • We empirically validate that RecCycle performs as well as state-of-the-art user-side recommendation algorithms while RecCycle reduces costs significantly. • We deploy RecCycle in a real-world X (Twitter) environment and confirm that users can realize their own recommender system with specified functionalities they call for using RecCycle."
https://arxiv.org/html/2411.07267v1,A Survey on Data Markets,"Data is the new oil of the 21st century. The growing trend of trading data for greater welfare has led to the emergence of data markets. A data market is any mechanism whereby the exchange of data products including datasets and data derivatives takes place as a result of data buyers and data sellers being in contact with one another, either directly or through mediating agents. It serves as a coordinating mechanism by which several functions, including the pricing and the distribution of data as the most important ones, interact to make the value of data fully exploited and enhanced. In this article, we present a comprehensive survey of this important and emerging direction from the aspects of data search, data productization, data transaction, data pricing, revenue allocation as well as privacy, security, and trust issues. We also investigate the government policies and industry status of data markets across different countries and different domains. Finally, we identify the unresolved challenges and discuss possible future directions for the development of data markets.","Data is considered an invaluable resource in the digital economy. The last decades have witnessed the explosive growth of data. As raw material for acquiring knowledge and developing products, data generates value in an indirect way. After remodeling the commercial perspective of data, data is directly monetized like other material commodities nowadays. Individuals and organizations extensively trade datasets and derived data products. In this new vision, data is no longer the enabler of products, but also the product itself. Governments around the world are seizing this new opportunity. For example, the Chinese government unveiled a guideline to improve the market-based allocation of data factors, which is the first to list data as a production factor following land, labor, capital, and entrepreneurship (China, 2020). The United States created the Federal Data Strategy Action Plan aimed at leveraging data as a strategic asset (States, 2019). Driven by the tides of data monetization, data markets have emerged. Data markets, a nascent interdiscipline of computer science and economics, are growing rapidly and evolving in myriad research directions. The history of data markets can be traced back to 1986. A seminal work by Admati and Pfleiderer (1986) studies a market where traders purchase information from a monopolistic seller. The information they trade is the data endowed or produced by individual agents. To the best of our knowledge, the term “data market” is put forward by Keenan (2008) in 2008 for the first time in the literature. They propose to exchange spatial data collected by geographic information systems in the market. In 2011, Balazinska et al. (2011) present a vision of a more general data market where commodities are derivative data products. They outline key challenges in a relational cloud data market for the database research community. Since then, data markets have experienced rapid development. Koutris et al. (2012) design the first query-based data market; Deep and Koutris (2017) propose a scalable and flexible pricing framework for relational queries; Agarwal et al. (2019) design the first two-sided marketplace for trading training data directly; Chen et al. (2019a) introduce the first model-based data market; and more recently Liu et al. (2021b) propose the first end-to-end (model-based) data market involving the interactions among sellers, brokers, and buyers. With the growing demand for data transactions, many data marketplaces have sprung up, such as AWS Data Exchange (AWS, [n. d.]), Dawex (DAWEX, [n. d.]), BDEX (BDE, [n. d.]), Factual (Fac, [n. d.]), and Snowflake (Sno, [n. d.]). Data marketplaces are online transaction locations or exchanges that facilitate the buying and selling of data products. They are authorized to host data products and conduct data transactions for the benefit of stakeholders. We propose a definition of the data market in this survey as follows. A data market is any mechanism whereby the exchange of data products including datasets and data derivatives (such as query results and trained models) takes place as a result of data buyers and data sellers being in contact with one another, either directly or through mediating agents. The data market serves as a coordinating mechanism by which several functions, including the pricing and the distribution of data as the most important ones, interact to make the value of data fully exploited and enhanced. In data markets, the life chain of data covers the process of data search, productization, monetization in pricing and transaction, and finally destruction. Trading data products naturally raises privacy, security, and trust concerns, and faces regulatory barriers to achieving compliance and traceability. Whether in academia or industry, there are rich explorations on designing data markets, where different data markets vary from each other in terms of data products, underlying functions, and market mechanisms. In this article, we present a comprehensive survey of this important and emerging direction from the aspects of data search, data productization, data transaction, data pricing, revenue allocation as well as privacy, security, and trust issues. We also investigate the government policies and industry status of data markets across different countries and different domains. Finally, we identify the unresolved challenges and discuss possible future directions for the development of data markets. 1.1. Related Surveys The existing surveys on data markets can be generally categorized based on the scope: (1) surveys on academic research (Thomas and Leiponen, 2016; Driessen et al., 2022; Liang et al., 2018; Abbas et al., 2021), (2) surveys on industry status (Schomm et al., 2013; Li et al., 2018a; Kennedy et al., [n. d.]; Azcoitia and Laoutaris, 2022), and (3) surveys on data pricing (Muschalle et al., 2012; Fricker and Maksimov, 2017; Zhang and Beltrán, 2020; Pei, 2022; Cong et al., 2022; Zhang et al., 2023a; Miao et al., 2023; Chi et al., 2023). Surveys on academic research. Efforts (Thomas and Leiponen, 2016; Abbas et al., 2021; Driessen et al., 2022; Liang et al., 2018) have been made to survey academic research for data markets within the whole lifecycle. Thomas and Leiponen (2016) provide managers with a literature review on the commercialization of big data. From a managerial and commercial perspective, they introduce six business models in the data ecosystem, led by data suppliers, data managers, data custodians, application developers, service providers, and data aggregators. Based on this taxonomy, they discuss the characteristics of the data ecosystem and conclude with the challenges faced by managers and corresponding guidelines in trading big data including pricing and privacy concerns which we reinforce in this paper. Abbas et al. (2021) examine 133 academic articles using a Service-Technology-Organization-Finance (STOF) model. They find that the existing literature on data marketplaces is primarily dominated by technology studies. Driessen et al. (2022) present a statistical analysis of works related to data markets up until 2021, discuss practical application areas for data markets, categorize the problems of designing data markets, and find corresponding solutions in the literature. Liang et al. (2018) use 4V (Volume, Velocity, Variety, and Value) to define big data and survey the lifecycle of trading big data, including data pricing, data trading, and data protection, for each of which they review corresponding issues and models. The above works (Thomas and Leiponen, 2016; Abbas et al., 2021; Driessen et al., 2022; Liang et al., 2018) mainly focus on techniques for trading data, while our survey covers state-of-the-art literature for trading general data products, including raw data and its derivatives such as queries, statistical inferences, and machine learning models. Moreover, our survey comprehensively covers key issues in main procedures in data markets from data search to data destruction. Surveys on industry status. There are four works (Schomm et al., 2013; Li et al., 2018a; Kennedy et al., [n. d.]; Azcoitia and Laoutaris, 2022) conducting industry surveys on data marketplaces. Schomm et al. (2013) present an initial survey of data marketplaces and data vendors by investigating 46 data suppliers from twelve dimensions (type, time frame, domain, data origin, pricing model, data access, data output, language, target audience, trustworthiness, size of vendor, and maturity) up until Summer 2012. Li et al. (2018a) introduce policies of China for developing data markets and discuss concerns and research opportunities including preprocessing, pricing, security, privacy, and verifiability. Azcoitia and Laoutaris (2022) investigate 180 entities which trade data on the Internet, summarize different business models, and discuss open challenges. Kennedy et al. ([n. d.]) introduce different types of data marketplaces and describe data transaction lifecycle from the perspective of buyers and sellers. They also interview buyers and sellers to understand the current status and challenges of online data marketplaces in 2022. The above works (Schomm et al., 2013; Li et al., 2018a; Kennedy et al., [n. d.]; Azcoitia and Laoutaris, 2022) provides an understanding of data marketplaces through practical investigations of entities and marketplaces. In contrast, our survey not only examines mainstream data marketplaces worldwide but also provides a list of government policies. Surveys on data pricing. There have been several surveys (Muschalle et al., 2012; Fricker and Maksimov, 2017; Zhang and Beltrán, 2020; Pei, 2022; Cong et al., 2022; Zhang et al., 2023a; Miao et al., 2023) specializing in data pricing, a subtopic that receives the most attention in data markets. Muschalle et al. (2012) investigate seven established vendors for their potential market situations, pricing approaches, and trends. Fricker and Maksimov (2017) report a literature survey of 18 papers regarding several research questions, including the maturity and targets of pricing models, types of data products, and pricing mechanisms. Zhang and Beltrán (2020) review novel data pricing studies and categorize data pricing methods based on data granularity and privacy. Pei (2022) starts with the economics of data pricing and reviews pricing models based on a set of fundamental principles. He also discusses the differences between digital products and data products, and the corresponding pricing methods. Very recently, Cong et al. (2022) survey data pricing methods in machine learning pipelines, including pricing raw data sets, pricing data labels, and pricing in collaborative machine learning models. Zhang et al. (2023a) categorize and review pricing methods for queries from the aspects of market structure, privacy notion, query type, and pricing method. Miao et al. (2023) classify data pricing techniques into three strategies and analyze thirteen pricing models. Chi et al. (2023) outline the fundamental concepts of data pricing, categorize data pricing strategies into query-based and privacy-based approaches, and offer an overview of data pricing from a data science standpoint. In addition to covering other data market functions, our survey includes a comprehensive analysis of data pricing that examines both revenue allocation for allocating compensations to data sellers and data product pricing for pricing data products to data buyers and their interactions. Furthermore, it systematically reviews emerging game-theoretic approaches to data pricing for the first time. In summary, while existing surveys approach data markets from either academic or industry perspective, our survey provides a comprehensive and general review of data markets covering both academic research and industry status including government policies across representative countries and domains. We also discuss the differences between data and other production factors and the corresponding impact on the design of data markets. While existing surveys investigate a few significant challenges in data markets, we study the interaction between key entities, summarize important desiderata for designing a well-functioning data market, and review techniques regarding data search, productization approaches, pricing mechanisms, data transactions, privacy concerns, etc, based on a formal framework as in Figure 2. Contributions. We present a comprehensive survey of data markets in both academia and industry. The purpose of this survey is to delve into subtopics of data markets in terms of computer science while covering mechanisms, regulations, and challenges in economics, law, and governance. The main contributions of this survey are summarized as follows. • Identify the unique properties of data and discuss the difference between data markets and other markets for the four production factors (land, labor, capital, and entrepreneurship). • Introduce the framework of data markets, formalize the abilities and restrictions of key roles, and illustrate the main procedures in the operations of data markets. • Present important desiderata for well-functioning data markets. • Summarize various methods of data search for various purposes, including crowdsourced dataset collection, dataset discovery in databases, data discovery in machine learning, and general dataset search. • Introduce various approaches to data productization based on versioning and data market categories. • Outline advertising strategies for data sellers and data purchase methods for data buyers in data transactions. • Review different approaches for revenue allocation and data product pricing, along with game-theoretic pricing methods. • Describe possible attacks on privacy preservation, fairness, profitability, and traceability from dishonest entities and corresponding solutions. • Present guidelines and regulations and investigate actual data marketplaces in representative countries and domains. • Discuss various open challenges and emerging directions for future research. Figure 1. The structure of the survey. 1.2. Structure of The Survey Figure 1 shows the structure of the survey. Section 2 first presents a diagram that shows different problems affecting data markets and their relationship. Section 3 summarizes important desiderata for building well-functioning data markets. Section 4 reviews methods of data search including crowdsourced dataset collection, dataset discovery in databases, dataset discovery in machine learning, and general dataset search. Section 5 describes techniques for data productization. Section 6 overviews strategies of both data buyers and data sellers in data transactions. Section 7 reviews studies investigating data product pricing and game-theoretic pricing. Section 8 reviews studies investigating revenue allocation. Section 9 deals with issues related to dishonest participants in untrusted data markets. Section 10 provides government policies and industry status in representative countries and regions. Section 11 draws a conclusion and discusses open challenges and opportunities for future work."
https://arxiv.org/html/2411.06392v1,LSMGraph: A High-Performance Dynamic Graph Storage System with Multi-Level CSR,"The growing volume of graph data may exhaust the main memory. It is crucial to design a disk-based graph storage system to ingest updates and analyze graphs efficiently. However, existing dynamic graph storage systems suffer from read or write amplification and face the challenge of optimizing both read and write performance simultaneously. To address this challenge, we propose LSMGraph, a novel dynamic graph storage system that combines the write-friendly LSM-tree and the read-friendly CSR. It leverages the multi-level structure of LSM-trees to optimize write performance while utilizing the compact CSR structures embedded in the LSM-trees to boost read performance. LSMGraph uses a new memory structure, MemGraph, to efficiently cache graph updates and uses a multi-level index to speed up reads within the multi-level structure. Furthermore, LSMGraph incorporates a vertex-grained version control mechanism to mitigate the impact of LSM-tree compaction on read performance and ensure the correctness of concurrent read and write operations. Our evaluation shows that LSMGraph significantly outperforms state-of-the-art (graph) storage systems on both graph update and graph analytical workloads.","Real-time analysis of large-scale dynamic graph data has become a key requirement in various fields, extensively applied in recommendation systems (Li et al., 2023; Guo et al., 2017; Wang et al., 2018; Tan et al., 2020), fraud detection (Ye et al., 2021; Wang et al., 2020a; Fan et al., 2021), community management (Xie et al., 2013; Raghavan et al., 2007), network monitoring (Kumar and Huang, 2019; Khalil et al., 2016; Akoglu et al., 2015) and so on. Designing an efficient dynamic graph storage system capable of rapid data storage and effective real-time graph analysis is increasingly crucial and challenging. Such a system must not only support fast real-time graph analysis, but also efficiently ingest updates, especially in scenarios where graphs are updated frequently, such as social networks, e-commerce, and smart city management (Kumar and Huang, 2019; Seo et al., 2013; Zhang et al., 2017; Zhu et al., 2020). Figure 1. An example of a graph storage system working in an e-commerce platform, where users or items represent vertices and the interactions between users and items are regarded as edges of the graph. Real-world Example. Consider a graph storage system that stores the user-item graphs on a large-scale e-commerce platform, such as Alibaba Taobao (tao, 2024). Fig. 1 shows a toy example of the graph storage system, where users or items represent vertices, and the interactions between users and items, such as favorite and buy, are regarded as edges of the graph. The system should maintain graph updates and support graph analysis tasks such as product recommendation and fraud detection. However, the massive number of users and their frequent behaviors bring huge challenges to the graph storage system. First, Taobao has approximately 400 million daily active users (Li, 2023), each generating an average of 10 behavioral data records per day (Zhu et al., 2018). This means that Taobao generates approximately 46,000 behavioral data records per second. Therefore, the storage system must support fast data ingestion. Second, the average size of each behavioral data is approximately 31 bytes (Zhu et al., 2018). Given the high data generation rate, a 1 TB RAM will be exhausted in less than 9 days, i.e., 1\text{ TB}/(4\times 10^{8}\text{ users}\times 10\text{ bh./day}\times 31\text% { bytes})\approx 8.9\text{ days}, where bh. refers to customer behaviors such as buying and favoriting. Therefore, the storage system needs to efficiently digest updates using limited memory and persist them on disks. In this paper, we default to using SSDs as persistent storage, as they provide a good balance between storage capacity, price, and performance. Third, graph data needs to be analyzed using graph algorithms, such as using PageRank for product recommendations (Li et al., 2023; Guo et al., 2017) and Label Propagation for fraud detection (Ye et al., 2021; Wang et al., 2020a). Therefore, the storage system needs to support fast queries for these graph analysis algorithms. Existing Disk-based Dynamic Graph Storages. A naive method is to extend the existing memory-based dynamic graph storage systems (Zhu et al., 2020; Feng et al., 2021) to disk-based ones by the utilization of automatic memory and disk mapping tools like mmap (Crotty et al., 2022). When updates occur on the graph data stored on disk, these systems automatically load the graph data into memory using mmap to perform in-place graph updates. However, this strategy leads to significant read/write amplification, although it makes data operations easier. This is because even if only one edge is updated, mmap must read the entire data page from the disk into memory and then write back the entire page after the updates. On the other hand, there are some dynamic graph storage systems specifically designed for disks, e.g., LLAMA (Macko et al., 2015) and GraphSSD (Matam et al., 2019), which use Compressed Sparse Rows (CSR) (Saad, 2003) to improve read performance since CSR is friendly to both random and sequential access to graphs due to its efficient offset index and compactness. However, the compactness of CSR leads to a large amount of data movement when inserting new data, which decreases graph update performance. Although LLAMA overcomes data movement by generating new CSRs in batches, a large number of CSR fragments are generated as updates continuously arrive. To access all neighbors of a vertex, it is required to read data from multiple CSRs, which results in a lot of random I/Os and decreases graph query performance. Finally, some graph databases, such as NebulaGraph (Neb, 2024) and Dgraph (dgr, 2024), leverage Log-Structured Merge-tree (LSM-tree) (O’Neil et al., 1996a) designed primarily for key-value stores to improve write performance. NebulaGraph (Neb, 2024) takes each vertex or edge as a key, and the properties of the vertex or edge as the value. However, when Nebula accesses all the edges of a vertex, the non-contiguous storage of these edges results in a large number of random reads, which decreases read performance. Dgraph (dgr, 2024) takes each vertex as a key, and the associated edges as the value. However, when Dgraph updates an edge of a vertex, it requires updating the whole edge block (as a value), which results in severe write amplification. This deficiency is more pronounced for vertices with more edges. In summary, these works suffer from problems in reading or writing as they ignore the properties of the graphs. Dilemma. It is challenging to design a graph storage system for persistent storage since there is a dilemma in optimizing read and write performance. In terms of optimizing write performance, a log structure is preferred with only sequential writes (e.g., LSM-tree (O’Neil et al., 1996a)). However, due to the random arrival order of updated edges, the edges of a vertex can be dispersed throughout the whole log-structured storage, which decreases the read performance when accessing all the edges of a vertex. In terms of optimizing read performance, a compact and sorted structure (e.g., CSR (Saad, 2003)) is preferred with efficient indexing and sequential reads. However, it requires significant overhead due to the data movement to maintain compactness when edges are inserted or deleted. Insight. LSM-tree (O’Neil et al., 1996b) is a log structure known for its excellent write performance, implemented by caching recent writes in memory and then performing sequential writes to persistent storage. On the contrary, CSR (Saad, 2003) is a widely used graph storage structure with excellent graph query performance due to its continuous storage of edges for each vertex and efficient vertex/edge index. It is worthwhile to design a novel graph storage structure by combining LSM-tree and CSR so that the structure can simultaneously leverage high performance in the writing of LSM-tree and high performance in the reading of CSR. Our solution. Based on the above insight, we propose LSMGraph, a novel dynamic graph storage system that takes advantage of both LSM-tree and CSR. From a memory perspective, we design an efficient memory cache structure for graph data, MemGraph, which can not only rapidly ingest graph updates but also effectively persist LSM-style data structures to disk. From a disk perspective, we propose organizing the graph data into multiple levels similar to the LSM-tree, and each level maintains a part of the graph data in CSR format. When the storage of the i-th level is full, a compaction is performed in the background, asynchronously merging the CSR of the i-th level into the next level. In this way, the online continuous data movement overhead of CSR is replaced by the offline periodical compaction overhead of LSM-tree. It is noteworthy that although the edges of a vertex are stored continuously in each CSR, they may span multiple CSRs across different levels, which results in the need to locate their position in multiple files and decreases read performance. To enhance read performance, we design a multi-level index that records the positions of each vertex’s edges on multiple levels to avoid many random searches. Additionally, we design a vertex-grained version control mechanism to mitigate the compaction overhead and allow concurrent read/write operation during compaction. To sum up, our contributions are summarized as follows. \bullet A novel dynamic graph storage system LSMGraph that leverages the write performance of LSM-tree and the read performance of CSR (Section 3). \bullet A memory cache structure MemGraph that manages the cached graph updates efficiently before flushing them to disk (Section 4.1). \bullet A multi-level index that supports fast reading of a vertex’s edges from multiple CSRs on different levels (Section 4.2). \bullet A vertex-grained version control mechanism that maximizes read and write performance during CSR compaction, while simultaneously guaranteeing the correctness of reading and writing (Section 4.3). \bullet A high-performance implementation and a comprehensive evaluation to verify the efficiency of our LSMGraph (Section 5). Experiments show that for graph update, LSMGraph achieves an average speedup of 36.12\times over LiveGraph, 2.85\times over LLAMA, and 8.07\times over RocksDB. For graph analysis, LSMGraph achieves an average speedup of 24.4\times over LiveGraph, 3.1\times over LLAMA, 30.8\times over RocksDB, and 6.6\times over MBFGraph."
https://arxiv.org/html/2411.06243v1,Towards Establishing Guaranteed Error for Learned Database Operations,"Machine learning models have demonstrated substantial performance enhancements over non-learned alternatives in various fundamental data management operations, including indexing (locating items in an array), cardinality estimation (estimating the number of matching records in a database), and range-sum estimation (estimating aggregate attribute values for query-matched records). However, real-world systems frequently favor less efficient non-learned methods due to their ability to offer (worst-case) error guarantees — an aspect where learned approaches often fall short. The primary objective of these guarantees is to ensure system reliability, ensuring that the chosen approach consistently delivers the desired level of accuracy across all databases. In this paper, we embark on the first theoretical study of such guarantees for learned methods, presenting the necessary conditions for such guarantees to hold when using machine learning to perform indexing, cardinality estimation and range-sum estimation. Specifically, we present the first known lower bounds on the model size required to achieve the desired accuracy for these three key database operations. Our results bound the required model size for given average and worst-case errors in performing database operations, serving as the first theoretical guidelines governing how model size must change based on data size to be able to guarantee an accuracy level. More broadly, our established guarantees pave the way for the broader adoption and integration of learned models into real-world systems.","Recent empirical results show that learned models perform many fundamental database operations (e.g., indexing, cardinality estimation) more efficiently than non-learned methods, providing significant speed-ups and space savings (Galakatos et al., 2019; Kraska et al., 2018; Ferragina & Vinciguerra, 2020; Zeighami et al., 2023; Kipf et al., 2018). Nevertheless, the lack of theoretical guarantees on their performance poses a significant hurdle to their practical deployment, especially since the non-learned alternatives often provide the required theoretical guarantees (Agarwal et al., 2013; Pătraşcu & Thorup, 2006; Hellerstein et al., 1997; Bayer & McCreight, 1970). Such guarantees are needed to ensure the reliability of the learned operations across all databases at deployment time, that is, to ensure consistent performance of the learned model on databases where the learned model had not been apriori evaluated. Thus, similar to existing worst-case bounds for non-learned methods, a guarantee is needed that a learned operation will achieve the desired accuracy level on all possible databases. Providing such a guarantee depends on how large the learned model is (e.g., number of parameters of a neural network), the desired accuracy level, and the size and dimensionality of the underlying databases. This paper takes the first step towards a theoretical understanding of the relationship between these factors for three key database operations, offering theoretical bounds on the required model size to achieve a desired accuracy on all possible databases of a certain size and dimensionality when using learned models to perform the operation. Specifically, the three operations studied in this paper are (1) indexing: finding an item in an array, (2) cardinality estimation: estimating how many records in a database match a query, and (3) range-sum estimation: estimating the aggregate value of an attribute for the records that match a query. We focus on numerical datasets and consider axis-aligned range queries for cardinality and range-sum estimation (i.e., queries that ask for the intersection of ranges across dimensions). Typical learned approaches to the above database operations take a function approximation view of the operations. Let f(q) be a function that takes a query, q, as an input, and outputs the answer to the query calculated from the database. For instance, in the case of cardinality estimation, f(q) will be the number of records in the dataset that match the query q (and f(q) can be similarly defined for indexing and range-sum estimation). At training time, a model, \hat{f}(q;\theta) (e.g., a neural network) is trained to approximate f. Training is done using supervised learning, where training labels are collected for different queries by performing the queries on the database using an existing method (e.g., for cardinality estimation, by iterating over the database and counting how many records match a query). At test time, the models are used to obtain estimates directly (e.g., by performing a forward pass of a neural network), providing \hat{f}(q;\theta) as an estimate to the answer to a query q. For indexing, where the exact location of the query in the array is needed (not an estimated location returned by the model), a local search around the model estimate is performed to find the exact answer. Such learned approaches are currently state-of-the-art, with experimental results showing significantly faster query time and lower storage space when using learned methods compared with non-learned methods for indexing (Kraska et al., 2018; Ferragina & Vinciguerra, 2020; Ding et al., 2020), cardinality estimation (Kipf et al., 2018; Negi et al., 2023) and range-sum estimation (Zeighami et al., 2023). Furthermore, recent results also show theoretical advantages to using learned models (Zeighami & Shahabi, 2023; Ferragina et al., 2020; Zeighami et al., 2023), most significantly, with Zeighami & Shahabi (2023) showing the existence of a learned index that can achieve expected query time of O(\log\log n) under mild assumptions on the data distribution, asymptotically better than the traditional O(\log n) of non-learned methods such as binary search. However, there has been no theoretical understanding of the required modeling choices, such as the required model size, for the learned approaches to provide an error guarantee across databases. Without any theoretical guidelines, design choices are made through empirical hyperparameter tuning, leading to choices with unknown performance guarantees at deployment time. Database Operation Worst-Case Error Average-Case Error (Uniform Dist.) Average-Case Error (Arbitrary Dist.) Indexing \frac{n}{2\epsilon+1}\log(1+\frac{(2\epsilon+1)u}{n}) Theorem 1 (\sqrt{n}-2)\log(1+\frac{1}{2\epsilon}) Theorem 2 (\sqrt{n}-2)\log(1+\frac{1}{2\epsilon}) Theorem 5 Cardinality Estimation \frac{n}{2\epsilon+1}\log(1+\frac{(2\epsilon+1)u^{d}}{n}) Theorem 1 ({\sqrt{n}-2})\log(1+\frac{\sqrt{n}^{d-1}}{4^{d(d+1)}\epsilon^{d}}-\frac{1}{% \sqrt{n}}) Theorem 3 X Lemma 2 Range-Sum Estimation \frac{n}{2\epsilon+1}\log(1+\frac{(2\epsilon+1)u^{d}}{n}) Theorem 1 ({\sqrt{n}-2})\log(1+\frac{\sqrt{n}^{d-1}}{4^{d(d+1)}\epsilon^{d}}-\frac{1}{% \sqrt{n}}) Corollary 1 X Lemma 2 Table 1: Our bounds on required model size in terms of data size, n, dimensionality, d, tolerable error, \epsilon, and domain size, u. Each column shows the result when \epsilon is the tolerable error for the specified error scenario. X: No non-trivial bound possible. Base of \log is 2. 1.1 Our Results In this paper, we present the first known bounds on the model size needed to achieve a desired accuracy when using machine learning to perform indexing, cardinality estimation and range-sum estimation. We provide bounds on the required model size, defined as the smallest possible size for a model to achieve error at most \epsilon on all d-dimensional datasets of size n. We measure model size in terms of number of bits required to store a model (which translates to the number of model parameters by considering the storage precision for the parameters). We refer to \epsilon as the tolerable error parameter, which denotes the maximum error that can be tolerated in the system. We thoroughly study the required model size in two different scenarios, namely when considering the worst-case and average-case error. That is, \epsilon can be provided in terms of worst-case or average-case error across queries that can be tolerated for all databases (i.e., worst-case across databases). Table 1 summarizes our main results, which we further discuss considering the two error scenarios in turn. First, suppose our goal is to answer all possible queries with error at most \epsilon across all d-dimensional datasets of size n. The results in the second column of Table 1, summarizing our Theorem 1 in Sec 3.1, provide a lower bound on the required model size to achieve this. For example, for indexing, to be able to guarantee error at most \epsilon on all possible queries and datasets of size n, one must use a model whose size exceeds \frac{n}{2\epsilon+1}\log(1+\frac{(2\epsilon+1)u}{n}). Notably, the bounds depend on the domain size u, which is the number of possible values the records in the database can take, implicitly assuming a finite data domain. We show in Lemma 1 in Sec. 3.1 that this is necessary: no model with finite size can answer queries with a bounded worst-case error on all possible datasets with infinite domain (this result is mostly of theoretical interest, since data stored in a computer always has finite domain). In the second scenario, our goal is to answer queries with average error of at most \epsilon on all d-dimensional datasets of size n. Assuming the queries are uniformly distributed, the third column in Table 1, summarizing Theorem 2, 3 and Corollary 1 in Sec. 3.2, presents our lower bounds on the required model size. Our bounds in this scenario show a weaker dependency on data size and tolerable error parameter compared with the worst-case error scenario, and as expected, suggest smaller required model size. Interestingly, bounds do not depend on the domain size and hold when the data domain is the set of real numbers, showing a significant difference between model size requirements when considering the two scenarios. Thus, our results formally show that robustness guarantees (i.e., guarantees on worst-case error) must come at the expense of larger model sizes. Furthermore, the results in the last column of Table 1, summarizing our Theorem 5 and Lemma 2 in Sec. 3.3, show that we can extend our results to arbitrary query distribution (compared with uniform distribution) in the case of indexing without affecting the bounds. However, for cardinality and range-sum estimation, we show in Lemma 2 that when relaxing our assumption on data distribution, one can construct arbitrarily easy distribution to answer queries from, so that no non-trivial lower bound on the model size can be obtained (surprisingly, this is not possible for learned indexing). Finally, not presented in Table 1, for average-case error, we complement our lower bounds on the required model size with corresponding upper bounds, showing tightness of our results. Theorem 2-5 show that our lower bounds are tight up to an O(\sqrt{n}) factor, asymptotically in data size. 1.2 discussion In practice, model size is often set in practice to a fixed value or through hyperparamter tuning (Lu et al., 2021; Kraska et al., 2018; Kipf et al., 2018; Zeighami et al., 2023) without taking data size into account111At least explicitly, as hyperparameter tuning can implicitly account for data size. Our results show that model size indeed needs to depend on data size to be able to guarantee any fixed error. More specifically, for practical purposes, our results can be interpreted in two ways. In the first interpretation, given a model size and data size, our results provide a lower bound on the worst-case possible error. This bound shows what error can be guaranteed by a model of a certain size (and how bad the model can get) after it is deployed in practice. This is important, because datasets change in practice and our bound on error help quantify if a model of a given size can guarantee a desired accuracy level when the dataset changes. Experiments in Sec. 4 illustrate that this bound on error is meaningful, showing that models achieve error values close to what the bound suggests. In the second interpretation, our results provide a lower bound on the required model size to achieve a desired accuracy level across datasets. This shows how large the model needs to be, to guarantee the desired accuracy, and has significant implications for resource management in database systems. For instance, it helps a cloud service provider decide how much resources it needs to allocate for models to be able to guarantee an accuracy level across all its database instances. Overall, our results are information theoretic, showing that it is not possible for any model to contain enough information to answer queries on all datasets accurately if they contain less than the specified number of bits. The bounds are obtained by considering the parameters of a model as a data representation, and showing bounds on the required size of any data representation to achieve a desired accuracy when performing the specific operations. Our proofs provide a novel exploration of the function approximation view of database operations, connecting combinatorial properties of datasets with function approximation concepts. Specifically, we prove novel bounds on packing and metric entropy of the metric space of database query functions to prove the bounds, which are particularly challenging to obtain for the average-case error. In Sec.A, we discuss various possible extensions of our results to queries with joins, other aggregation function such as min/max/avg. and other error metrics not considered in this paper."
https://arxiv.org/html/2411.06102v1,SiriusBI: Building End-to-End Business Intelligence Enhanced by Large Language Models,"The rapid advancement of AI technologies, particularly Large Language Models (LLMs), is establishing a new paradigm for Business Intelligence (BI). Despite the emergence of pioneering work in enhancing BI systems with LLMs, we have identified the following three issues when deployed in real industrial scenarios: interaction limitations, performance bottlenecks, and functionality deficiencies.In this paper, we present SiriusBI, an end-to-end business intelligence system that is designed to address the three issues simultaneously. First, we propose an intelligent and application-oriented module called multi-round dialogue with querying, which aims to overcome the prevalent interaction limitations in current BI solutions. Next, to mitigate the performance bottlenecks caused by scenario migration, we introduce two SQL generation methods that strike a balance between accuracy and deployment costs. Finally, to tackle the practical challenges posed by functionality deficiencies, we develop an end-to-end workflow that covers the entire BI process, ensuring that SiriusBI delivers a robust and complete set of functionalities.As an independent cloud service in Tencent’s data platform, SiriusBI has been applied across Tencent’s finance, advertising, and cloud sectors, providing services to dozens of enterprise clients. Experiments on real-world datasets and practical applications in industrial BI scenarios demonstrate the practicality and effectiveness of SiriusBI. Remarkably, SiriusBI achieves remarkable accuracy rates of 97% in SQL generation for Tencent Finance, 89% for Tencent Advertisement, and 91% for Tencent Cloud.","Business Intelligence (BI) (Negash, 2004; Wieder and Ossimitz, 2015) is a crucial application scenario in the data field, comprising a comprehensive suite of methodologies, tools, and infrastructures designed to collect, integrate, analyze, and present an organization’s raw data to generate actionable insights for informed decision-making. The primary objective of BI is to generate actionable insights that facilitate informed decision-making. BI systems are extensively utilized across various sectors, including finance (Nuseir, 2021), environment (Hegger and Saraceni, 2024), and social media (Chen et al., 2024a; Rivera, 2024), which significantly enhances the decision-making process through the provision of real-time analytics and reporting capabilities. A typical BI system comprises several key components: a data management system that stores, processes, and aggregates vast amounts of data; analytics algorithms that transform this data into actionable insights; and visualization tools that present the information in intuitive and user-friendly formats. These components collaboratively function to streamline business processes, enhance customer experiences, reduce operational costs, and mitigate risks. Recent advancements in LLMs have sparked significant interest in ChatBI (Lian et al., 2024) — an integrated BI system enhanced by conversational AI. Concurrently, the demand for a fully integrated and efficient ChatBI solution is surging, driven by the need of a more intuitive and accessible mode of data interaction. This evolution promises to transform how users engage with data, making insights more readily available and actionable. To meet the growing demand for big data analysis and decision-making in BI, the data community has proposed numerous effective approaches. While NL2SQL is the core task in most BI systems, traditional methods often rely on the schema-based approaches (Hristidis and Papakonstantinou, 2002; Luo et al., 2007; Zeng et al., 2016; Hristidis et al., 2003) or parsing-based approaches (Iyer et al., 2017; Wang et al., 2017; Li and Jagadish, 2014; Popescu et al., 2004) to generate SQL queries from natural language. Recently, there has been a growing trend in implementing NL2SQL powered by LLMs, utilizing either prompt engineering (Pourreza and Rafiei, 2023; Wang et al., 2023) or model training techniques (Li et al., 2024; Pourreza and Rafiei, 2024). However, these methods typically focus on Single-Round Dialogue (SRD) scenarios, which limits their effectiveness in real-world applications where user interactions are typically more complex. To address the limitations of NL2SQL in BI contexts, the NL2BI (Lian et al., 2024) task has been introduced. This task aims to generate SQL queries based on Multi-Round Dialogues (MRD), allowing for a comprehensive understanding of user intentions. However, when implementing existing work in industrial BI systems and applying it in real-world scenarios, we identified the following three challenges: Figure 1. Demonstration of multi-round user requests. Compared with SiriusBI, SRD misses the omitted year information in conversations, while MRD fails to identify the user’s ambiguous intent. C1: Interaction Limitations. ChatBI aims to leverage MRD to assist users in data analysis, thereby lowering the barrier to data usage and enhancing overall data utilization efficiency. However, existing public solutions still face challenges in supporting MRD due to real-world interaction modes. On one hand, much of the research (Wang et al., 2023; Talaei et al., 2024; Li et al., 2024; Pourreza and Rafiei, 2024, 2023) in the community concentrates on enhancing the performance of the NL2SQL task. While this has advanced the field to some extent, the interaction mode remains restricted to SRD, which imposes strict constraints on how users express their needs. This limitation ultimately undermines the primary objective of ChatBI systems: to facilitate universally accessible data analysis. On the other hand, leading technology teams in industry have introduced ChatBI (Lian et al., 2024) that support MRD and offered feasible methods for implementing multi-round interactions. Nevertheless, these solutions also impose limitations on user interaction modes in practical applications, catering only to fixed question-answer formats. Furthermore, they tend to be less effective in BI scenarios that require deep domain expertise. C2: Performance Bottlenecks. The NL2SQL task is crucial for BI because it directly impacts the accuracy of data retrieval during analysis. In practical industrial scenarios, existing NL2SQL methods often exhibit suboptimal performance when applied across diverse contexts. For example, a LLM fine-tuned on data specifically annotated for the financial domain performs poorly when applied to the advertising domain. The process of preparing and standardizing data for each specialized field, followed by fine-tuning, incurs significant costs, which are often impractical in industrial applications. C3: Functionality Deficiencies. Traditional BI systems are typically comprehensive solutions for data analysis, encompassing multiple interrelated modules such as data storage, SQL generation, and data insights. However, existing LLM-based BI systems fall short in providing essential functionalities, including efficient intent understanding, adaptive domain-specific knowledge, and intelligent data insights. For example, Mac-SQL (Wang et al., 2023) and CHESS (Talaei et al., 2024) primarily focus on NL2SQL tasks, while Lian et al. (2024) propose using the Apache SuperSet(Apa, [n.d.]) tool directly after NL2SQL, neglecting the components of domain knowledge management module and deep data insight. Unlike traditional BI systems, users expect LLMs to enhance the entire ChatBI workflow, which drives a technological paradigm shift that improves both application efficiency and user experience. A purely NL2SQL-based approach or scenario-constrained MRD functionality is inadequate to meet the real-world needs of BI applications. This gap presents an opportunity to develop a more comprehensive and intelligent ChatBI system. To address the aforementioned challenges, we propose SiriusBI, which implements an end-to-end solution for BI scenarios. This system leverages the capabilities of LLMs to empower various submodules, thereby enhancing both efficiency and user experience in data analysis. Specifically, for the issue of interaction limitations (C1), we introduce the MRD-Q (Multi-Round Dialogue with Querying) module. Unlike the MRD functionality proposed by Lian et al. (2024), MRD-Q incorporates an intent clarification module that disambiguates user queries through follow-up questions. This approach enables the system to accurately identify the user’s true intent, even when the initial query is flawed or ambiguous, thereby facilitating precise responses, as shown in Figure 1. Furthermore, we employ a Retrieval-Augmented Generation (RAG) framework that leverages both business-domain knowledge and metadata, ensuring our system’s adaptability across various domains. Consequently, this reduces barriers for users and significantly enhances the overall user experience. For the issue of performance bottlenecks (C2), we propose two implementation strategies in SQL generation. First, for domain-specific NL2SQL tasks, we introduce an efficient and automated process for constructing training data to improve the performance of LLMs. Second, for scenarios that require transferability, we present a two-step solution that integrates a Semantic Model with a SQL Model. This approach leverages domain-specific knowledge and metadata, ensuring high accuracy in SQL generation without the need of additional training. Furthermore, it reduces the costs associated with transferring between different scenarios and enhances the generalization of NL2SQL across various domains. For the issue of functionality deficiencies (C3), SiriusBI provides a comprehensive end-to-end workflow. Unlike existing works that mainly focus on the NL2SQL task and the expansion of MRD modules, SiriusBI introduces two essential modules specifically designed for ChatBI : the Knowledge Management module and the Data Insights module. The knowledge management module is responsible for storing, processing, and utilizing domain-specific expertise and data to support the specialized knowledge requirements of various submodules. Meanwhile, the Data Insights module transcends traditional data visualization and report generation by providing critical key functionalities such as automatic data preparation, complex task planning, and the application of insight tools. These capabilities provide users with more comprehensive and in-depth support for their data analysis needs. Contributions. The main contributions of this paper can be summarized as follows: • We propose SiriusBI, an end-to-end LLM-based ChatBI system designed to provide better data analysis services and lower the usage threshold for users. To the best of our knowledge, SiriusBI is the first comprehensive end-to-end ChatBI system that simultaneously addresses the aforementioned three challenges. So far, SiriusBI has been applied across Tencent’s finance, advertising, and cloud sectors, providing services to dozens of enterprise clients. • To extend the interaction modes of ChatBI systems, we introduce MRD-Q, a dialogue system designed to ask clarifying questions when user queries are unclear or ambiguous. This improves the accuracy of intent understanding, lowers the barrier to user entry, and ultimately enhances the overall user experience. • To enhance the transferability of ChatBI systems across different industrial scenarios, we propose two implementation approaches for SQL generation: single-step generation and two-step generation. The two-step generation method significantly improves the transferability of ChatBI and provides a new paradigm for SQL generation in the industry. • Extensive experiments on real-world datasets and industrial BI scenarios demonstrate the practicality and effectiveness of SiriusBI. Notably, when applied in industry, SiriusBI achieves remarkable accuracy rates of 97% in SQL generation for Tencent Finance, 89% for Tencent Advertisement, and 91% for Tencent Cloud."
https://arxiv.org/html/2411.07009v1,Hierarchical Conditional Tabular GAN for multi-tabular synthetic data generation,"The generation of synthetic data is a state-of-the-art approach to leverage when access to real data is limited or privacy regulations limit the usability of sensitive data. A fair amount of research has been conducted on synthetic data generation for single-tabular datasets, but only a limited amount of research has been conducted on multi-tabular datasets with complex table relationships. In this paper we propose the algorithm HCTGAN to synthesize multi-tabular data from complex multi-tabular datasets. We compare our results to the probabilistic model HMA1. Our findings show that our proposed algorithm can more efficiently sample large amounts of synthetic data for deep and complex multi-tabular datasets, whilst achieving adequate data quality and always guaranteeing referential integrity. We conclude that the HCTGAN algorithm is suitable for generating large amounts of synthetic data efficiently for deep multi-tabular datasets with complex relationships. We additionally suggest that the HMA1 model should be used on smaller datasets when emphasis is on data quality.","Data is ubiquitous in today’s modern society, and being able to opt-out of sharing your data with services is made more and more difficult each day. Nevertheless, we have seen great initiatives and regulations in the last decade, like the GDPR [9] in the EU and PIPA [16] in South Korea, whose goals are to enforce standards for individuals data privacy. But providing greater privacy standards for individuals does not come cheap for parties that aim to leverage the data, as new processes for using the data has to be established, and a lot of the data is now rendered unusable or inaccessible for development and analytical initiatives. Synthetic data is today being actively researched within the field of machine learning, and is an approach to leverage in businesses where usability of real data is limited. Furthermore, it has showed to be an effective method to use for speeding up internal data processes whilst minimizing privacy risks [24, 34]. Tabular data is a de-facto standard used throughout the industry to store data in a column and row format. Typically, each row represents an object and each column represents an attribute of the object. The rows are usually identified by a unique identifier, ID, which is stored as a column in the table and is referred to as a primary key of the table. If data is stored in multiple tables which are connected, this is referred to as relational data, or multi-tabular data. To represent a relationship between two tables, the child table has to contain a reference to some object in the parent table. This is accomplished by introducing a new column in the child table which stores so called foreign keys. The foreign keys reference an object in the parent table, more specifically, they reference the parents primary key. Parent tables can have multiple children, and vice versa for children, which means that there could be highly complex relationships in a multi-tabular database. Machine learning research on synthetic data has thus far showed promising results, but the large majority of that research only deals with non-relational data [38, 37, 27]. Hence, there is a lack of established research on multi-tabular algorithms, and the little research that does exist mainly proposes to use probabilistic models. However, these models do not scale well for largely connected multi-tabular datasets, neither in the training nor in the sampling process. Furthermore, there is no guarantee that the generated synthetic data has referential integrity between tables [3]. Apart from probabilistic models, plenty of research has been conducted on Generative Adversarial Networks (GANs) for synthetic data generation, but mainly on single table datasets [13]. The proposed GAN models are an attractive choice whenever sensitive data is to be synthesized, because the generator never operates on any real data, and these models can be extended to incorporate various levels of differential privacy [4, 36, 7, 33]."
https://arxiv.org/html/2411.06980v1,: Unleashing Storage Hardware-Software Co-design,"NVMe SSD hardware has witnessed widespread deployment as commodity and enterprise hardware due to its high performance and rich feature set. Despite the open specifications of various NVMe protocols by the NVMe Express group and NVMe being touted as the new language of storage, there is a complex labyrinth of software abstractions to program the underlying hardware. The myriad storage I/O paths such as POSIX storage API, ad-hoc OS mechanisms, and userspace I/O libraries have different syntax and semantics that complicate software development and stand in the way of mass adoption and evolution of the NVMe ecosystem. To unify the diverse I/O storage paths, we built xNVMe that exposes a single message-passing API to support both asynchronous and synchronous communication with NVMe devices. xNVMe provides various command sets to support diverse storage I/O paths in different OS (e.g., Linux, FreeBSD, Windows, and MacOS) and userspace libraries (e.g., SPDK) with minimal overhead. xNVMe is an Open Source project and has gained traction amongst various industry stakeholders. In this paper, we elaborate on the lessons that we have learned in the project during its evolution. We also provide some ongoing and future work planned for the project. We hope the database and storage systems community can join in the effort to both extend xNVMe and leverage it as a building block for innovative co-design of storage systems on modern NVMe hardware.","The past decade has witnessed the widespread evolution and deployment of NAND flash memory as commodity and enterprise storage hardware due to its high bandwidth and low latency. The growth of NAND flash SSDs has necessitated the birth of NVMe (Non-volatile memory express) access technology to sidestep the performance limitations of the SATA interface. The current global NVMe technology market share stands at 54.1 billion US$ and is speculated to grow to 412 billion US$ in 2031 (nvm, 2024b). To coordinate the evolution and interoperability of NVMe technologies, the NVM Express working group was formed to create open specifications that could be implemented by hardware and software vendors (nvm, 2024a). Despite the open specifications of NVMe hardware, the challenge of software abstractions to program NVMe hardware remains and is growing with its popularity. Classically, the POSIX storage abstractions (pread, pwrite) had been the holy grail of programmability, stability, and portability to hide the underlying hardware complexity. However, the rise of diverse NVMe hardware has created a fissure in this perfect world. The need for a low-overhead, asynchronous programming model to leverage the high performance of modern NVMe hardware has created multiple complex I/O storage stacks differing from the original POSIX API. For the Linux kernel, there is the POSIX aio_* APIs, libaio, and io_uring storage interfaces(Joshi et al., 2024) that applications can program against. These interfaces differ widely in their API, semantics, and performance. The landscape of storage interfaces gets even more complicated if you factor in different OS. The complexity of the storage I/O stack grows if you factor in support for newer SSD technologies such as ZNS SSDs, FDP SSDs, KV SSDs, and computational storage, to name a few. Either the rich-feature set is hidden behind the block layer interface of the OS or exposed via a userspace I/O stack such as SPDK, custom vendor libraries that are a thin shim over an NVMe device driver. This complexity and fragmentation of various storage I/O paths is unfortunate and creates unnecessary barriers in the adoption of the storage I/O paths by application stacks. The unnecessary complexity also stifles cooperation between software application designers and hardware vendors. As the usage of GPUs and hardware accelerators grows to support AI workloads, a similar API fragmentation is occurring for storage devices that require direct and efficient access by accelerators (Markussen et al., 2020; Qureshi et al., 2023). xNVMe was envisaged to fill the programmability gap for NVMe storage technologies by creating a single unified API that applications can program against to flexibly multiplex the desired storage I/O path with minimal overhead (Lund et al., 2022). Instead of creating yet another abstraction for storage, xNVMe provides a single message passing API for interacting with NVMe devices along with support for various storage I/O paths using this API. Since storage systems are not locked into the API and semantics of a specific I/O path, they can flexibly experiment with different storage I/O paths based on need. xNVMe currently supports various I/O paths in the Linux kernel e.g., libaio, io_uring and the classic POSIX abstractions of pread and pwrite. It also supports other OS e.g., FreeBSD and Windows and userspace I/O stack such as SPDK. xNVMe is an Open Source project (xnv, 2024a) that became publicly available in 2019. It began as an experimental platform for emerging NVMe interfaces (e.g., Open-Channel SSDs (Bjørling et al., 2017)) and matured over time, gaining traction among academic researchers and industry practitioners. In this experience cum vision paper, we present our experiences and lessons learned during the project that has shaped its evolution. We outline the reasons behind its existence in Section 3 and some of the ongoing and future work in the project in Section 5. Our aim with xNVMe is to lower the entry barrier of building innovative data-intensive systems that leverage features of modern NVMe hardware. By providing I/O storage independence and an Open Source collaboration environment, we hope the project can foster co-design of data-intensive software systems and NVMe hardware than what is currently possible."
https://arxiv.org/html/2411.06799v1,Structuring the Processing Frameworks for Data Stream Evaluation and Application,"The following work addresses the problem of frameworks for data stream processing that can be used to evaluate the solutions in an environment that resembles real-world applications. The definition of structured frameworks stems from a need to reliably evaluate the data stream classification methods, considering the constraints of delayed and limited label access. The current experimental evaluation often boundlessly exploits the assumption of their complete and immediate access to monitor the recognition quality and to adapt the methods to the changing concepts. The problem is leveraged by reviewing currently described methods and techniques for data stream processing and verifying their outcomes in simulated environment. The effect of the work is a proposed taxonomy of data stream processing frameworks, showing the linkage between drift detection and classification methods considering a natural phenomenon of label delay.","From the applied artificial intelligence perspective, the most important factor of data is its representation. Even the shortest peek into the reality shows us that the dominating modality of the contemporary internet is a media stream – either in the form of YouTube clips, Netflix movies, or TikTok and Instagram infinite information cascades [1] – since out of 5.45 billion of worldwide internet users, nearly 95% consume social media [2]. Each of those media streams can be, and de facto is, processed [3] by the machine learning environment as a data stream [4]. Data streams and concept drift Data stream is defined as an ordered collection of objects flowing into the system in time. The objects may be observed separately – in online processing – or aggregated into groups – in batch processing [5]. A typical phenomenon visible in the potentially infinitely incoming data stream is the time-varying probability distribution of represented problem [6]. It indicates that any classification model used to generalize knowledge and infer decisions in the streaming environment depends on the knowledge acquired so far, and on changes yet to come. Such changes are generally categorized as concept drifts, most often divided into the real and virtual ones [7]. Real concept drifts are easy to define by directly impacting the measured quality of a model, whereas virtual drifts can be hindered and possibly never lead to any degeneration of predictive system [8]. Some guidelines for the design of such systems even recommend relying solely on the impact of real drifts, ignoring the possible delayed cost factor induced by detectable changes of an initially virtual nature that eventually reach the status of real drifts [9]. Figure 1 shows an average recognition accuracy of a data stream with synthetic gradual drift [10], in which the first phase of a concept change does not yet impact the classification accuracy. Meanwhile, the class distribution shift starts prior to a concept change. The presented example shows that considering both the real and virtual concept drift is significant for the overall system recognition quality. Figure 1: Averaged classification accuracy over 100 replications of a synthetic data stream with the real drift – in which the initial 30 chunks do not yet show the loss of recognition quality, despite the proceeding concept transition indicated by a shift in class centroids The minor axes of concept drift taxonomy lean on drift dynamic (static, gradual, and incremental drift) [10], the prior class probability changes [11], or the concept recurrence [12]. Classification of data streams An incremental adaptation to changes in non-stationary data streams is a necessary element of effective data stream processing [13]. State-of-the-art solutions for such a problem often follow the paradigms of classifier ensembles [5]. In such a system, instead of one intelligent agent, the task is performed by the ensemble of independent models, whose decisions are integrated during fusion stage [14]. There are various strategies that enable incremental learning in ensemble approaches dedicated to data stream processing. One can mention the canonical method of Streaming Ensemble Algorithm (sea) [15], in which classifiers are trained on separate data batches and added to the ensemble, where they are then no longer updated. Another approach was used in the Dynamic Weighted Majority (dwm) algorithm [16], which follows incremental learning of ensemble members. Depending on the learning strategy used by the ensemble method, the techniques will be able to adapt to changing environments at different rates. Currently, the most competent data stream classification methods use in-build algorithms as the base classifiers of homogeneous ensembles organized in parallel architecture with a dynamic fuser and pruning criterion connected with a drift detector. Classifier ensembles offer an accurate path for stably improving the general predictive power of the in-build methods, such as Concept-adapting Hoeffding Tree [17] or Multilayer Perceptron [18]. As identified in the literature, there are hybrid processing methods that do not rely on any internal drift detection module – their processing strategy is denoted as passive adaptation or continuous rebuild [19, 20, 21]. On the contrary, active adaptation or triggered rebuild is visible in methods that compose a unit dedicated to the detection of a concept drift [22, 23]. A drift detector is an important agent in the architecture of classifier ensembles of this type. If its internal state exceeds the activation threshold, the whole ensemble does experience the occurrence of a concept drift alert [24]. Drift detection and delayed labeling The drift detection module plays a vital role in hybrid methods with active adaptation. However, over the years, a vast pool of solutions stating an independent tool was proposed, dedicated directly to detecting changes in the data distribution [25]. Such solutions – denoted as concept drift detectors – monitor different data characteristics to provide a binary decision if the concept drift occurred or if the concept remained stable. Drift detectors follow two canonical taxonomy branches, following the systematical axis of real and virtual drifts [26] as explicit or implicit methods [27]. The explicit drift detectors will rely directly on the quality of the monitored recognition model, while implicit ones will use other data characteristics. In terms of label access, all explicit methods will be supervised, while some of another category – unsupervised. It is worth noting that there exist implicit detectors that require labels in order to monitor conditional data distribution [28]. In the data streams, where the data distribution is time-dependent, we will use an indice i to denote a position of sequences of features X and labels y, describing the samples incoming in the continuous data stream. Most often, concept drift detection methods utilize a processing technique identified as moving window mechanism – objects are analyzed for some time, depending on the structure of a given solution. Such a processing technique is a cornerstone of modern methods – to take as an example the Adaptive Windowing (adwin) method [29], used for years as a default drift detector in newly designed ensemble data stream classifiers, such as Leveraging Bagging [22] and Robust Online Self-adjusting Ensemble [23]. Most often, the proposed architectures of methods assume the instant presence of a label and, consequently, the instant possibility of establishing the current predictive power of a model. In practical applications, a label is assigned to the sample after some time [30]. This property of a data stream, often overlooked due to limited computer simulation resources, is denoted as a label delay [31]. It states that the description of data – that is, labels, but also the indicator of the system’s recognition quality – is not guaranteed to be present at the moment of initial sample occurrence i [32]. In practical applications, the value y_{i} may, and probably will, be provided after X_{i} when the time delay \delta has passed. As this work focuses on processing protocols in terms of label requirements, the existing drift detection methods will be categorized into three groups: 1. Supervised drift detector DD_{s} will require class labels to be present. It detects a concept change by processing a data batch or sample X_{i} and a corresponding set of labels y_{i}. DD_{s}=f(X_{i},y_{i}) (1) In such a case, X_{i} may activate the supervised drift detector only as soon as its label is observed by the system after the delay of a label presence. The statistic on which the decision of most state-of-the-art supervised detectors like Drift Detection Method (ddm), Early Drift Detection Method (eddm) or Adaptive Windowing (adwin) is purely related to the recognition quality. Such explicit methods will require labels y_{i} to compare them with predictions y^{\prime}_{i} returned by the classifier for data X_{i}. 2. Unsupervised drift detector DD_{u} will only require the presence of data X_{i} to provide information about the concept drift occurrence. DD_{u}=f(X_{i}) (2) The particular benefit resulting from using an unsupervised drift detection method is the independence of the label access. The cost and time delay of labels is currently viewed as a significant limitation of data stream processing methods [33], where both the velocity and the volume of data are significant [34]. Among the unsupervised drift detection approaches, one can distinguish the techniques that monitor the data distribution using statistical tests [35], using one-class classifiers [27] or that monitor the shifts of class centroids [36]. 3. Partially unsupervised drift detector DD_{p} is a particular case of unsupervised drift detector that requests the labels y exclusively in the event of a concept change. The method operates similarly to DD_{u} at a regular mode by only processing data features X_{i}. However, it is possible that the detector requires labels y_{i}, which makes it operate similar to DD_{s} in certain states of a system – directly after the drift detection. Therefore, depending on the current state, the detection can be performed in two modes: supervised or unsupervised. DD_{p}=f(X_{i},y_{i}) (3) DD_{p}=f(X_{i}) (4) This category is composed of methods that require to be re-calibrated after the drift occurrence. Compared with DD_{s}, the label request may be sent sporadically only when the concept drift is detected and the model rebuild is required. The methods that monitor the properties of the underlying classification model – other than the classification quality reserved for explicit and supervised solutions – include many approaches from this group. State-of-the-art drift detectors employ monitoring the density of samples near the decision boundary [33], the support function of classification model [37], or the explainability of its decisions [38]. Contribution and Motivation This work looks at the current methods and processing protocols from a big picture perspective, considering the real-world mechanisms that need to be employed for incremental learning and the implications of such mechanisms on the classification quality of state-of-the-art methods. The main contributions of the presented work are: 1. The presentation of four structured frameworks presenting the paradigms of data stream processing when using different categories of drift detection methods. 2. Consideration of unavoidable label delay phenomenon in the presented processing protocols. 3. The experimental evaluation of state-of-the-art in-build methods coupled with different types of drift detectors, showing the implications of different processing schemes. 4. Comparison of existing methods in a context of various label delay times and concept drift frequencies. 5. Consideration of an abstract Oracle drift detector in the experiments, limiting the uncertainty related to the correct drift detection by existing methods. 6. Extending the standard evaluation of (a) classification quality while additionally taking into account (b) the number of label requests and (c) the frequency of classifier training. The proposed contributions will support the proper and fair experimental evaluation of methods, allowing for considering the impact of label cost, their delay, and the cost of model rebuild in the event of a concept change. Those matters are often neglected in comparing methods presented in the research but remain significant in real-world applications [31, 39, 40]."
