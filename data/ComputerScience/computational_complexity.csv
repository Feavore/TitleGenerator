URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10093v1,Bounded degree QBF and positional games,"The study of \SAT and its variants has provided numerous \NP-complete problems, from which most \NP-hardness results were derived. Due to the \NP-hardness of \SAT, adding constraints to either specify a more precise \NP-complete problem or to obtain a tractable one helps better understand the complexity class of several problems. In 1984, Tovey proved that bounded-degree \SAT is also \NP-complete, thereby providing a tool for performing \NP-hardness reductions even with bounded parameters, when the size of the reduction gadget is a function of the variable degree. In this work, we initiate a similar study for \langQBF, the quantified version of \SAT. We prove that, like \SAT, the truth value of a maximum degree two quantified formula is polynomial-time computable. However, surprisingly, while the truth value of a 3-regular 3-\SAT formula can be decided in polynomial time, it is \PSPACE-complete for a 3-regular \langQBF formula. A direct consequence of these results is that Avoider-Enforcer and Client-Waiter positional games are \PSPACE-complete when restricted to bounded-degree hypergraphs. To complete the study, we also show that Maker-Breaker and Maker-Maker positional games are \PSPACE-complete for bounded-degree hypergraphs.","The \NP-completeness of \SAT established by Cook [Coo71] marked the beginning of the study of computational hardness. To better understand these problems and explore the boundary between tractable and \NP-hard problems, several variants of \SAT have been introduced, providing proofs that many problems are \NP-complete, even when restricted to specific instances. The most commonly studied variant is 3-\SAT, where \SAT is restricted to clauses of size at most 3. However, other variants also appear frequently, depending on the context, such as planar 3-\SAT or 1-in-3-\SAT. For examples of such variants and reductions requiring specific conditions on the formulas, we refer the reader to Karp’s article [Kar72] and Papadimitriou’s book [Pap94]. In \PSPACE, the central problem from which most reductions are derived is 3-\langQBF, the quantified version of 3-\SAT, i.e., quantified Boolean formulas restricted to clauses of size at most 3. In this paper, we adopt the two-player game interpretation of \langQBF proposed by Stockmeyer and Meyer [SM73] when they proved its \PSPACE-completeness. Let \psi=Q_{1}x_{1},\dots,Q_{n}x_{n}\varphi be a quantified Boolean formula, where for each 1\leq i\leq n, Q_{i}\in\{\exists,\forall\} and \varphi is a quantifier-free Boolean formula over x_{1},\dots,x_{n}. The \langQBF game played on \psi involves two players, Satisfier and Falsifier, as follows: for i=1 to n, if Q_{i}=\exists, Satisfier assigns a truth value to x_{i}, and if Q_{i}=\forall, Falsifier assigns a truth value to x_{i}. Once all variables have been assigned values, a valuation \nu for x_{1},\dots,x_{n} is obtained, and Satisfier wins if and only if \nu satisfies \varphi. Let \varphi be a Boolean formula, and let x be a variable in \varphi. We denote by d(x) the degree of x in \varphi, i.e., the number of clauses in which x appears. We also denote by \Delta(\varphi) the degree of \varphi, which is the maximum degree of any variable in \varphi. A formula \varphi is said to be k-regular if every variable in \varphi has degree k. If c\in\varphi is a clause, we denote by |c| its size, which is the number of literals in c. The rank of a formula \varphi is the largest size of any clause in \varphi. A formula \varphi is said to be k-uniform if all its clauses have size exactly k. This paper focuses on bounded-degree formulas, i.e., formulas in which each variable appears in a bounded number of clauses. Bounded-degree \SAT has been studied since 1984, when Tovey [Tov84] proved that \SAT remains \NP-complete when restricted to formulas with a maximum degree of 3, but is polynomial-time solvable for formulas with a maximum degree of 2 or for 3-regular 3-uniform formulas. We initiate here the study of bounded-degree \langQBF. For integers r,d, we denote by r-\langQBF-d the set of Boolean formulas in which the clauses have size at most r and where each variable appears in at most d clauses. If either r or d is unspecified, there are no restrictions on them. This study is motivated by the fact that \langQBF is a central problem for \PSPACE hardness reductions, and restricted versions of it can lead to \PSPACE-hard problems on specific instances. The motivation for this study primarily stems from the desire to understand bounded-degree positional games. Positional games were introduced by Hales and Jewett [HJ63] in 1963, and the study of their complexity began in 1978 when Schaefer proved that the Maker-Breaker version is \PSPACE-complete [Sch78]. In this convention, two players, Maker and Breaker, alternate claiming the vertices of some hypergraph \mathcal{H}. Maker wins if the vertices she manage to claim all the vertices of some hyperedge of \mathcal{H}, otherwise, Breaker wins. More recently, other conventions have been proven to be \PSPACE-complete, including Maker-Maker [Bys04], Avoider-Avoider [FGM+15, BH19], Avoider-Enforcer [GO23], and Client-Waiter [GOTT24]. However, all these proofs focused on the rank of the hypergraph (i.e., the size of its largest hyperedge) and did not attempt to optimize its maximum degree. For a general overview of the complexity of positional games, we refer the reader to Oijid’s Ph.D. thesis [Oij24]. Recent studies on positional games focus on games played on graphs, where certain graph structures are considered as the winning sets [Sla00, DGPR20, DGI+23, BFM+23]. In most of these studies, both the rank and the degree of the vertices of the hypergraph play important roles in the reductions provided. Bounding the degree of vertices, in conjunction with the rank, would enable reductions to be restricted to bounded-degree graphs. This result is particularly relevant within the parameterized complexity framework, as bounded-degree graphs have locally bounded treewidth. Bonnet et al. [BGL+17] showed that positional games parameterized by the number of moves are \FPT on locally bounded treewidth graphs, which indicates that tractability really requires the number of moves to be treated as a parameter. In this paper, we prove in Section 2 that 2-\langQBF can be solved in quadratic time, and that, unlike the unquantified version, 3-\langQBF-3 is already \PSPACE-complete. In Section 3, we explore the computational complexity of positional games restricted to bounded-degree hypergraphs. As a direct consequence of the \PSPACE-hardness of 3-\langQBF-3, we prove that Avoider-Enforcer and Client-Waiter games are \PSPACE-complete. However, since the reduction for Maker-Breaker games generates vertices of unbounded degree, we prove separately that Maker-Breaker games are \PSPACE-complete when restricted to hypergraphs of rank 12 and maximum degree 5. As a consequence, we answer an open question from Bagan et.al. [BDG+24], proving that the Maker-Breaker Domination game is \PSPACE-complete, even restricted to bounded degree graphs, and we prove that Maker-Maker games are also \PSPACE-complete restricted to bounded degree hypergraphs."
https://arxiv.org/html/2411.09619v1,Hardness Amplification via Group Theory,"We employ elementary techniques from group theory to show that, in many cases, counting problems on graphs are almost as hard to solve in a small number of instances as they are in all instances. Specifically, we show the following results.Boix-Adserà et al., (2019) showed in FOCS 2019 that given an algorithm A computing the number of k-cliques modulo 2 that is allowed to be wrong on at most a \delta=O\left(1/(\log k)^{k\choose 2}\right)-fraction of n-vertex simple undirected graphs in time T_{A}(n), we have a randomized algorithm that, in O\left(n^{2}+T_{A}(nk)\right)-time, computes the number of k-cliques modulo 2 on any n-vertex graph with high probability. Goldreich, (2020) improved the error tolerance to a fraction of \delta=2^{-k^{2}}, making 2^{O\left(k^{2}\right)}-queries to the average-case solver in O\left(n^{2}\right)-time. Both works ask if any improvement in the error tolerance is possible. In particular, Goldreich, (2020) asks if, for every constant \delta<1/2, there is an \tilde{O}\left(n^{2}\right)-time randomized reduction from computing the number of k-cliques modulo 2 with a success probability of greater than 2/3 to computing the number of k-cliques modulo 2 with an error probability of at most \delta.In this work, we show that for almost all choices of the \delta 2^{n\choose 2} corrupt answers within the average-case solver, we have a reduction taking \tilde{O}\left(n^{2}\right)-time and tolerating an error probability of \delta in the average-case solver for any constant \delta<1/2. By “almost all”, we mean that if we choose, with equal probability, any subset S\subset\{0,1\}^{n\choose 2} with |S|=\delta 2^{n\choose 2}, then with a probability of 1-2^{-\Omega\left(n^{2}\right)}, we can use an average-case solver corrupt on S to obtain a probabilistic algorithm.Inspired by the work of Goldreich and Rothblum, (2018) in FOCS 2018 to take the weighted versions of the graph counting problems, we prove that if the \textit{Randomized Exponential Time Hypothesis}(\rETH) is true, then for a prime p=\Theta\left(2^{n}\right), the problem of counting the number of unique Hamiltonian cycles modulo p on n-vertex directed multigraphs and the problem of counting the number of unique half-cliques modulo p on n-vertex undirected multigraphs, both require exponential time to compute correctly on even a 1/2^{n/\log n}-fraction of instances. Meanwhile, simply printing 0 on all inputs is correct on at least a \Omega\left(1/2^{n}\right)-fraction of instances.","Average-case complexity or typical-case complexity is the area of complexity theory concerning the difficulty of computational problems on not just worst-case inputs but on the majority of inputs (Ben-David et al., , 1992; Bogdanov and Trevisan, , 2021; Levin, , 1986). The theory of worst-case hardness, specifically in the context of proving conditional or unconditional lower bounds for a function f, attempts to prove that for every fast algorithm A, there is an input x such that the output of the algorithm A on the input x differs from f(x). However, for applications such as cryptography (Impagliazzo, , 1995), it is not just sufficient that f is hard on some input. It should have a good probability of being hard on an easily samplable distribution of inputs. Possibly the most famous and instructive example of this is the Discrete Logarithm Problem (\DLP), which has been proved by Blum and Micali, (1982) to be intractable for polynomial-time algorithms to compute correctly on even a 1/h(n)-fraction of inputs for any polynomial h, for all sufficiently large n, if it is intractable for polynomial-time randomized algorithms in the worst-case. Given a generator g of a field \mathbb{Z}_{p} of prime size and an input y\in\mathbb{Z}_{p}, the \DLP asks to find x\in\mathbb{Z}_{p} such that g^{x}\equiv y\pmod{p}. Suppose that there is an algorithm A computing the \DLP correctly on a 1/h(n)-fraction of inputs; then, for any y\in\mathbb{Z}_{p}, we can attempt to compute x as follows: Pick a random x^{\prime}\in\mathbb{Z}_{p} and ask A to compute x^{\prime\prime} such that g^{x^{\prime\prime}}\equiv yg^{x^{\prime}}\pmod{p}; we verify if this is true, and if so, we return x=x^{\prime\prime}-x^{\prime}, else we repeat. We will likely obtain the correct answer in h(n) repetitions with a probability of at least 1-1/e, giving us a polynomial-time randomized algorithm for the problem. Blum and Micali, (1982) use this to construct pseudorandom generators (Vadhan, , 2012), algorithms that generate random-looking bits. Algorithms that generate pseudorandomness have many applications: For the quick deterministic simulation of randomized algorithms, for generating random-looking strings for secure cryptography, for zero-knowledge proofs (Goldreich et al., , 1991; Goldreich, , 2006), and many others. This hardness result for the \DLP is a special case of rare-case hardness, a term coined by Goldreich and Rothblum, (2018), which refers to computational problems where algorithms with a specific time complexity cannot be correct on even an o(1)-fraction of inputs. Cai et al., (1999) proved more such hardness results for the permanent, building on a long line of work, showing intractability results for computing the permanent (Valiant, , 1979; Gemmell and Sudan, , 1992; Feige and Lund, , 1996). Most average-case hardness and rare-case hardness results are shown, similar to the case of the \DLP, by proving that the tractability of some function f on a small fraction of inputs implies the tractability over all inputs of some function h that is conjectured to be intractable. Most existing techniques use error correction over polynomials to achieve such hardness amplifications (Ball et al., , 2017; Lund et al., , 1992; Gemmell and Sudan, , 1992). Recently, Asadi et al., (2022) introduced tools from additive combinatorics to prove rare-case hardness results for matrix multiplication and streaming algorithms, revealing new avenues for complexity-theoretic research. A more recent application of additive combinatorics shows that in quantum computing, all linear problems have worst-case to average-case reductions (Asadi et al., , 2024). In this paper, we intend to show that group theory is a powerful tool for achieving hardness amplification for graph problems. We emphasize that we are far from the first to apply group theory to graphs in the context of theoretical computer science (Babai, , 2006; Luks, , 1982). The breakthrough quasipolynomial-time algorithm of Babai, (2016) for the graph isomorphism problem is a tour de force in the application of group theory to graph theoretic computational problems. Our thesis is that group theory is also a powerful tool in the theory of average-case and rare-case complexity for graph problems. 1.1 Counting k-Cliques Modulo 2 One area that has gained much attention in the past few decades is the paradigm of “hardness within \P” (Vassilevska Williams, , 2015). In particular, for practical reasons, it is not just important to us that a problem is in \P, but also that it is “quickly computable”, in one sense of the phrase. Traditionally, in complexity theory, “quickly computable” has been used interchangeably with polynomial-time computable. However, considering the vast data sets of today, with millions of entries, O\left(n^{15}\right)-time complexity algorithms are not practical. Many works have gone into showing conditional lower bounds as well as tight algorithms (Abboud and Williams, , 2014; Williams, , 2018; Williams and Williams, , 2018). Another practical application, which arguably motivated many works on fine-grained average-case hardness, starting with that of Ball et al., (2017), is the idea of a proof of work. A proof of work (Dwork and Naor, , 1993) is, informally, a protocol where a prover intends to prove to a verifier that they have expended some amount of computational power. The textbook example for an application is combatting spam emails: Forcing a sender to expend some resource for every email sent makes spamming uneconomical. The idea is to have a prover compute an f(x) for some function f, which both the prover and verifier know, and an input x of the verifier’s choice. In particular, the verifier needs a distribution of inputs for which f is expected to take some minimum amount of time to compute for any algorithm. Another condition is that the distribution should be easy to sample from f and should not be too hard. We do not want to make sending emails impossible. This is where average-case fine-grained hardness for problems computable in polynomial-time comes into the picture. Many other such works have explored these ideas further (Boix-Adsera et al., , 2019; Dalirrooyfard et al., , 2020; Goldreich and Rothblum, , 2018; Asadi et al., , 2022). We also emphasize that these works are not only important for practical reasons but also give insights into the structure of \P itself. 1.1.1 Background One problem that has become a central figure in this paradigm of “hardness within \P” is the problem of counting k-cliques for a fixed k, or k fixed as a parameter. Many works have explored the fine-grained complexity of variants of this problem and many others (Dalirrooyfard et al., , 2020; Goldreich and Rothblum, , 2018; Goldreich, , 2023). One interesting direction is to explore how the difficulty of computing the number of k-cliques modulo 2 correctly on some fraction of graphs (say 0.75) relates to the complexity of computing this number correctly for all inputs with high probability. This is simultaneously a counting problem, a class of problems for which many average-case hardness results are known, and a decision problem, where finding worst-case to average-case reductions is more complicated. Boix-Adserà et al., (2019) explore this question for general hypergraphs, and for the case of simple undirected graphs, they show that if there is an algorithm A computing the number of k-cliques modulo 2 correctly on over a 1-\Omega\left(1/(\log k)^{k\choose 2}\right)-fraction of instances, then we can obtain an algorithm that computes the number of k-cliques modulo 2 correctly on all inputs in O\bigg{(}(\log k)^{k\choose 2}(T_{A}(nk)+(nk)^{2})\bigg{)}-time, where T_{A}(m) is the time taken by A on input graphs with m vertices. They do this by reducing the problem of counting the number of k-cliques on n vertices graphs to the problem of counting k-cliques on k-partite graphs where each partition has n vertices. The k-partite setting is reduced to the problem of computing a low-degree polynomial, where the average-case hardness is obtained. Goldreich, (2020)333They call it t-cliques, possibly to emphasize that t is a parameter. improves the error tolerance from O\left((\log k)^{-{k\choose 2}}\right)=2^{-\Omega\left(k^{2}\log\log k\right)} to 2^{-k^{2}} and simplifies the reduction. They make 2^{O\left(k^{2}\right)} queries and use O\left(n^{2}\right)-time. More specifically, they construct a new polynomial such that one of the evaluations gives us our answer of interest. They use the crucial insight that the sum of a low-degree polynomial (degree less than the number of variables) over all inputs in \mathbb{Z}_{2} is 0, and hence summing over all inputs other than the one of interest gives us our answer. They make 2^{k\choose 2}-1 correlated queries, each of which is uniformly distributed over the set of all simple undirected n vertex graphs. Using the union bound, if the fraction of incorrect instances in the average-case solver is 2^{-k^{2}}, the probability of error for the reduction is bounded by 2^{k\choose 2}/2^{k^{2}}=o(1). Both Boix-Adserà et al., (2019) and Goldreich, (2020) ask whether there are similar worst-case to average-case reductions tolerating more substantial error. In particular, Goldreich, (2020) asks whether there is a randomized reduction taking \tilde{O}(n^{2}) time from computing the number of k-cliques modulo 2 on any graph with a success probability of larger than 2/3 to computing the number of k-cliques modulo 2 on a (1/2+\epsilon)-fraction of instances for any arbitrary constant \epsilon>0. 1.1.2 Our Results First, we define a random experiment O^{H_{n}}_{c}. Definition 1. The Random Experiment O^{H_{n}}_{c}. Given any set \mathbb{D} and a function H_{n}:\{\,0,1\,\}^{n\choose 2}\to\mathbb{D} defined over n-vertex simple undirected graphs that is invariant under graph isomorphism, the random experiment O^{H_{n}}_{c} selects a set S\subset\{\,0,1\,\}^{n\choose 2} of size c2^{n\choose 2} with uniform probability and gives an oracle O that correctly answers queries for computing H_{n} on the set S. The other answers of O can be selected adversarially, randomly, or to minimize the time complexity, T_{O}, of the fastest deterministic algorithm implementing it. In Section 8, we will prove the following results, crucially relying on these functions or problems being invariant under graph isomorphism. Our results hold regardless of O’s answers to \overline{S}. Theorem 1. For any k\in\mathbb{N} (not necessarily a constant), given an \epsilon=\omega\left(n^{3/2}/\sqrt{n!}\right), given an oracle O sampled from O^{H_{n}}_{1/2+\epsilon}, where H_{n}:\{\,0,1\,\}^{n\choose 2}\to\mathbb{D} is any function defined over n-vertex undirected simple graphs that is invariant under graph isomorphism and can be computed in O\left(n^{8+o(1)}/\epsilon^{4+o(1)}\right)-time given the number of k-cliques in the graph, then with a probability of at least 1-2^{-\Omega\left(n^{2}\right)} over the randomness of O^{H_{n}}_{1/2+\epsilon}, we have an algorithm that, with access to O computes H_{n} with a high probability in time O\left(\left(n^{8+o(1)}/\epsilon^{2+o(1)}+T_{O}\right)/\epsilon^{2}\right), where T_{O} is the time complexity of a hypothetical algorithm simulating the oracle O. Informally, this says that for almost all subsets S of \{\,0,1\,\}^{n\choose 2} with |S|=(1/2+\epsilon)2^{n\choose 2}, an algorithm that computes H_{n} correctly on the set S is nearly as hard, computationally speaking, as computing H_{n} correctly on all instances with a randomized algorithm with high probability. Due to work of Feigenbaum and Fortnow, (1993), and Bogdanov and Trevisan, (2006), under the assumption that the \PH does not collapse, this is a near-optimal result for non-adaptive444This is when the inputs of the queries we make to O do not depend on any of the answers. Another interpretation is that the inputs to query on O must be decided before making any queries to it. querying when no other assumptions are made of H_{n}. In particular, when applied to the \NP-complete problem of deciding whether a simple undirected graph with n vertices has a clique of size \lfloor n/2\rfloor, \HALF, we show a non-adaptive polynomial-time reduction from computing \HALF over any instance to computing \HALF correctly on S for almost all S\subset\{\,0,1\,\}^{n\choose 2} with |S|=(1/2+\epsilon)2^{n\choose 2} for any \epsilon=1/\poly(n). If this reduction can be extended to show this for all S, instead of almost all, \PH would collapse to the third level (Feigenbaum and Fortnow, , 1993; Bogdanov and Trevisan, , 2006). We also show the following result, making progress on the open problem of Goldreich, (2020). Theorem 2. Given any constants k>2 and \epsilon>0, with a probability of at least 1-2^{-\Omega\left(n^{2}\right)} over the randomness of sampling O from O^{H_{n}}_{1/2+\epsilon}, where H_{n} is the function counting the number of k-cliques modulo 2 in an n-vertex undirected simple graph, we have an \tilde{O}\left(n^{2}\right)-time randomized reduction from counting k-cliques modulo 2 on all instances to counting k-cliques modulo 2 correctly over the 1/2+\epsilon-fraction of instances required of O. Moreover, this reduction has a success probability of greater than 2/3. Whereas Goldreich, (2020) asks whether, for every \epsilon>0, there is a randomized reduction in \tilde{O}\left(n^{2}\right)-time, with a success probability of larger than 2/3, from computing the number of k-cliques modulo 2 to computing the number of k-cliques modulo 2 correctly on any subset S\subset\{\,0,1\,\}^{n\choose 2} with |S|=(1/2+\epsilon)2^{n\choose 2}. We answer in the affirmative, not for all such subsets S, but for almost all such subsets S. We stress that while our result significantly improves the error tolerance from the previous state of the art of 2^{-k^{2}} of Goldreich, (2020) to 1/2-\epsilon for “almost all S”-type results, the error tolerance of Goldreich, (2020) is still state of the art for “all S.” This introduces a tradeoff between error tolerance and universality of instances, where we have a sharp gain in error tolerance at the cost of universality of instances. 1.2 Worst-Case to Rare-Case Reductions for Multigraph Counting Problems It has been believed for decades that there are no polynomial-time algorithms for \NP-hard problems, at least with sufficient faith in the conjecture that \P\neq\NP. In the past decade, efforts have been made to determine the exact complexities of \NP-hard problems. The world of fine-grained complexity attempts to create a web of reductions analogous to those of made by Karp reductions (Karp, , 1972), except the margins are “fine”. One such connection, proved by Williams, (2005) is that if the Orthogonal Vectors (\OV) problem has an n^{2-\epsilon}-time algorithm for dimension d=\omega(\log n), then the Strong Exponential Time Hypothesis (\SETH) (Calabro et al., , 2009) is false. Not only do minor algorithmic improvements under the framework of fine-grained complexity imply faster algorithms for many other problems, they can also prove structural lower bounds. Williams, (2013), in pursuit of the answer to the question, “What if every \NP-complete problem has a slightly faster algorithm?,” proved that faster than obvious satisfiability algorithms for different classes of circuits imply lower bounds for that class. Soon after, by showing that there is a slightly better than exhaustive search for \ACC circuits555More concretely, Williams, (2014) showed that for \ACC circuits of depth d and size 2^{n^{\epsilon}} for any 0<\epsilon<1, there is a satisfiability algorithm taking 2^{n-n^{\delta}} time for some \delta>0 depending on \epsilon and d., Williams, (2014) showed that \NEXP\not\subset\ACC. Currently, the fastest algorithm for computing the number of Hamiltonian cycles on digraphs takes O^{*}\left(2^{n-\Omega(\sqrt{n})}\right)-time due to Li, (2023). Some other algorithmic improvements upon O^{*}(2^{n}) (including the parameterized cases) are due to Björklund et al., (2019), Björklund and Williams, (2019), and Björklund, (2016). 1.2.1 Background Cai et al., (1999) proved that the permanent of an n\times n matrix over \mathbb{Z}_{p}, a polynomial that, in essence counts the number of cycle covers of a multigraph modulo p is as hard to evaluate correctly on a 1/\poly(n)-fraction of instances in polynomial-time as it is to evaluate over all instances in polynomial-time. Here, they used the list decoder of Sudan, (1996) to show a worst-case to rare-case reduction: A reduction using a polynomial-time algorithm that evaluates the polynomial on an o(1)-fraction of instances to construct a polynomial-time randomized algorithm that computes the permanent over this field over any input with high probability. Goldreich and Rothblum, (2018) consider the problem of counting the number of t-cliques in an undirected multigraph. A t-clique is a complete subgraph of t vertices (K_{t}). They give an \left(\tilde{O}\left(n^{2}\right),1/\polylog(n)\right)-worst-case to rare-case reduction from counting t-cliques in n-vertex undirected multigraphs to counting t-cliques in undirected multigraphs generated according to a specific probability distribution. That is, given an oracle O that can correctly count the number of t-cliques in undirected multigraphs generated according to a certain probability distribution on at least a 1/\polylog(n)-fraction of instances, in \tilde{O}\left(n^{2}\right)-time, using the oracle O, we can count the number of t-cliques correctly in n-vertex undirected multigraphs with a success probability of at least 2/3. Combined with the work of Valiant, (1979) and Cai et al., (1999), they also show 1/2^{o(n)}-hardness for computing the permanent in a setup similar to the one in our work. Given a constant-depth circuit C_{L} for verifying an \NP-complete language L, Nareddy and Mishra, (2024) created a generalized certificate counting function, f^{\prime}_{L,p}:\mathbb{Z}_{p}^{n+2n^{c}}\to\mathbb{Z}_{p}, where p is a prime and n^{c} is the certificate size for L. Further, using an appropriate set of functions, f^{\prime}_{L,p}, they prove that for all \alpha>0, there exists a \beta>0 such that the set of functions f^{\prime\prime}_{L,\beta} is 1/n^{\alpha}-rare-case hard to compute under various complexity-theoretic assumptions. There are two observations in the works of Nareddy and Mishra, (2024). 1. The set of functions, f^{\prime\prime}_{L,\beta}, is artificially generated using the circuit C_{L}. 2. Proving f^{\prime}_{L,p} to be rare-case hard for any \NP-complete language L seems infeasible using their work. 1.2.2 Our Results In contrast to the above observations, our contributions in this paper are as follows. 1. From the problem description itself, we construct a generalized certificate counting polynomials, f^{\prime}_{L,p}, for two natural \NP-complete languages, which look more natural as compared to the above “artificially” generated functions, f^{\prime\prime}_{L,\beta}. The first problem counts the number of Hamiltonian cycles in a directed multigraph over \mathbb{Z}_{p}. The second problem counts the number of \lfloor n/2\rfloor-cliques in n-vertex undirected multigraphs over \mathbb{Z}_{p}. 2. We prove rare-case hardness results for the above two “natural” problems (f^{\prime}_{L,p}) for a prime p=\Theta\left(2^{n}\right) by exploiting their algebraic and combinatorial structures. Assuming the Randomized Exponential Time Hypothesis (\rETH) (Dell et al., , 2014), the conjecture that any randomized algorithm for 3\SAT on n variables requires 2^{\gamma n} time for some \gamma>0, we show the following results. Theorem 3. Unless \rETH is false, counting the number of unique Hamiltonian cycles modulo p on an n-vertex directed multigraph requires 2^{\gamma n}-time for some \gamma>0 even to compute correctly on a 1/2^{n/\log n}-fraction of instances for a prime, p=\Theta\left(2^{n}\right). Theorem 4. Unless \rETH is false, counting the number of unique cliques of size \lfloor n/2\rfloor modulo p on an n-vertex undirected multigraph requires 2^{\gamma n}-time for some \gamma>0 even to compute correctly on a 1/2^{n/\log n}-fraction of instances for a prime, p=\Theta(2^{n}). Meanwhile, for both problems, simply printing 0 all the time, without even reading the input, in this setting yields the correct answer on at least an \Omega\left(1/2^{n}\right)-fraction of instances. Using “weighted” versions of counting problems in Goldreich and Rothblum, (2018) inspired our choice to use multigraphs. By “unique,” in the example of triangle counting, we take a choice of three vertices and compute the number of “unique” triangles between them by multiplying the “edge weights.” This multiplicative generalization, precisely to count the number of choices of one edge between any two vertices in our subgraph structure, is what we mean when we say “unique cliques” or “unique Hamiltonian cycles.” Our results extend the results obtained for the permanent (Valiant, , 1979; Feige and Lund, , 1996; Cai et al., , 1999; Dell et al., , 2014; Björklund and Williams, , 2019; Li, , 2023) to these two problems. This provides heuristic evidence that significantly improving algorithms for these two problems might be infeasible. Under \rETH, one needs exponential time to marginally improve the O(1)-time algorithm of always printing 0. 1.3 Techniques This paper’s central unifying theme is using group theoretic arguments to obtain our results. In particular, a small subset of elementary arguments gives great mileage for both results. Moreover, the arguments made in both results complement each other in the following ways. 1. For the hardness amplification achieved for counting k-cliques modulo 2 on an n-vertex simple undirected graph, the most important tool for us from the theory of group actions, is the Orbit Stabilizer Theorem (Lemma 4). In the context of graphs, we can interpret this as saying that the automorphism group of a simple undirected n-vertex graph U_{n}, \Aut\left(U_{n}\right), the subgroup of S_{n} such that permuting the vertices and edges of U_{n} by a permutation \pi\in\Aut\left(U_{n}\right) conserves the adjacency matrix of U_{n}, is related to the isomorphism class \mathcal{C}_{n} of distinct666We say that two n-vertex graphs are different if their adjacency matrices are different. graphs isomorphic to U_{n} as \left|\Aut\left(U_{n}\right)\right|\left|\mathcal{C}_{n}\right|=n!. As will be seen in section 1.3.1, this is the most important insight for us, along with the result of Pólya, (1937) and Erdős and Rényi, (1963) that almost all graphs have a trivial automorphism group. 2. For our results obtained for counting problems on multigraphs, our objects of algebraic study are the functions themselves. Our protagonists, weighted counting functions on multigraphs, form a vector space, also a group under addition. The space of weighted counting functions that are invariant under graphs isomorphism forms a subspace. We provide a valuable classification of these functions based on conjugacy class structure, and use these results. In particular, the intention of our usage of group theory here is to, given oracle access to a function f under some constraints, find out if it computes our function of interest. First, for both problems, we test whether f is a counting function on multigraphs, then we check if it is invariant under graph isomorphism, and finally, check whether f is our function of interest. 1.3.1 For Counting k-Cliques Modulo 2 The following “key ideas” are helpful to keep in mind while going through the technical details of this work. The Fraction of Correct Answers Over Large Isomorphism Classes is Usually not Too Far From the Expected Fraction of Correct Answers Over the Oracle When we sample O from O^{H_{n}}_{1/2+\epsilon}, we can think of the correct fraction of instances as being distributed over the isomorphism class partitions of O. While our proof in Section 8.1 formalizes this fact using the Chernoff bounds (Mitzenmacher and Upfal, , 2005), as is usually the case with tail bounds, the intuitive picture to have in mind is the central limit theorem. As n grows, for large isomorphism classes \mathcal{C}_{n}, the random variable representing the fraction of correct instances resembles a normal distribution centered at 1/2+\epsilon. As n grows, for sufficiently large isomorphism classes, almost all the weight of the distribution is concentrated between 1/2+\epsilon/2 and 1/2+3\epsilon/2. The fact that the weight in the region [0,1/2+\epsilon] is small, in fact exponentially low, is useful to us. In fact, using the union bound, we show that for sufficiently large n, all sufficiently large (say \left|\mathcal{C}_{n}\right|\geq n^{3}) isomorphism classes have a correctness fraction greater than 1/2+\epsilon over O. Almost All Graphs Belong to Isomorphism Classes of the Largest Possible Size In their work, Pólya, (1937) and Erdős and Rényi, (1963) showed that almost all n-vertex undirected simple graphs have a trivial automorphism group. More specifically, if we randomly sample a graph U_{n} uniformly from the set of all undirected simple graphs with n vertices, with a probability of 1-{n\choose 2}2^{-n-2}(1+o(1)), \left|\Aut\left(U_{n}\right)\right|=1. Due to our version of the orbit stabilizer theorem (Lemma 4), this means that almost all graphs belong to an isomorphism class of size n!. Graphs With Very Large Automorphism Groups are Easy to Count Cliques Over One can imagine that with a graph whose automorphism group is of “almost full size,” perhaps when seen on a logarithmic scale, counting k-cliques is easy. With a highly symmetric graph, if we have a k-clique, we have many others in predictable positions. It is also likely that the number of k-cliques in this graph is represented by a small arithmetic expression consisting of binomial coefficients. For progress on the problem of Goldreich, (2020), for sufficiently large n, we classify all graphs with n vertices whose automorphism group is of size \omega\left(n!/n^{3}\right). For sufficiently large n, there are only twelve non-isomorphic graphs of this type. All of them either have an independent set with n-2 vertices or a clique containing n-2 vertices. Also, six of these classes have zero k-cliques for k>2, five have the number of k-cliques described by an arithmetic expression containing one binomial coefficient, and only one has its k-clique count as the difference between two binomial coefficients. Keeping this intuition in mind, the paradigm for our reduction is as follows: 1. Check if our graph, U_{n}, belongs to an isomorphism class that is large enough to have good probabilistic guarantees of having a 1/2+\epsilon/2-fraction of correctness over the randomness of O^{H_{n}}_{1/2+\epsilon}. In particular, this size threshold grows as \Theta\left(n^{2}/\epsilon^{2}\right). 2. If the isomorphism class is large enough, then with a very high probability over the randomness of O^{H_{n}}_{1/2+\epsilon}, this class has at least a 1/2+\epsilon/2-fraction of correctness over O. We sample random permutations \pi from S_{n} and permute the vertices and edges of U_{n} accordingly to obtain a graph U^{\prime}_{n} isomorphic to U_{n}. We query O on the input U^{\prime}_{n} and note down the answer. We repeat this process O\left(1/\epsilon^{2}\right) times and take the majority answer. Due to the Chernoff bound, once again, if we do have a 1/2+\epsilon/2-fraction of correctness within the isomorphism class for O, this is correct with high probability over the randomness of the algorithm. 3. If the isomorphism class is small, the graph is highly symmetric, and we count the number of k-cliques ourselves. We execute this paradigm differently for a constant \epsilon>0 and for an \epsilon varying as a function of n. For a Constant \epsilon>0. When this is the case, notice that our critical threshold for isomorphism class size is O\left(n^{2}\right). Due to the orbit stabilizer theorem (Lemma 4) for graphs, this means that the automorphism group of every graph U_{n} with isomorphism class size O\left(n^{2}\right) has \left|\Aut\left(U_{n}\right)\right|=\Omega\left(n!/n^{2}\right)=\omega\left(n!% /n^{3}\right). In Section 8.2.1, we will prove in Lemma 23 that for sufficiently large n, the following are the only kinds of graphs with automorphism group of size \omega\left(n!/n^{3}\right). 1. K_{n} and its complement. 2. K_{n} with one edge missing and its complement. 3. K_{n-1} with an isolated vertex and its complement. 4. K_{n-1} with one vertex of degree 1 adjacent to it and its complement. 5. K_{n-2} with two isolated vertices and its complement. 6. K_{n-2} with two vertices adjacent to each other and its complement. In \tilde{O}\left(n^{2}\right)-time, by checking each case, we can tell whether U_{n} is isomorphic to any of these graphs and quickly compute the number of k-cliques if so. If U_{n} is not isomorphic to any of these, then, due to the orbit stabilizer theorem for graphs (Lemma 4), its isomorphism class size is above the critical threshold, and we can query on O for answers. For an \epsilon Varying as a Function of n. When \epsilon varies as a function of n, the procedure here varies since obtaining a complete classification of graphs whose automorphism class is above the size threshold is impractical. Let t(n)=O\left(n^{2}/\epsilon^{2}\right) be the threshold isomorphism class size in this case. We estimate whether the automorphism class of U_{n} is larger than n!/t(n) or smaller than n!/t(n)^{1+\alpha} for some \alpha>0. We can do this by taking nt(n) random permutations \pi from S_{n} and counting how often permuting the vertices and edges of the graph U_{n} as specified by \pi gives us the same adjacency list as U_{n}. If the automorphism group is larger than n!/t(n), then with high probability, this count is larger than n/2. If the automorphism group size is smaller than n!/t(n)^{1+\alpha}, then this is very likely to be less than n/2; hence, we decide based on comparing this number to n/2. The algorithm to count k-cliques on the symmetric case is also different since we no longer have a convenient classification of graphs anymore. In particular, we first attempt to list all (at most t(n)) distinct graphs isomorphic to U_{n}. We can do this by picking n^{2}t(n) random permutations \pi from S_{n} and permuting U_{n} according to \pi. If this is a graph we have not yet seen, then we add it to the list. With high probability, we will have seen all graphs. In each of these graphs, we count how many cases the first k vertices form a k-clique. As shown in Section 8.3, the number of k-cliques in this graph is a simple function of this number. When the isomorphism class is of size above the critical threshold, we can, of course, use the querying procedure to O and obtain good probabilistic guarantees over the randomness of O^{H_{n}}_{1/2+\epsilon}. 1.3.2 For the Rare-Case Hardness of Counting on Multigraphs We will discuss the overview of the proof for the problem of counting Hamiltonian cycles on directed multigraphs. The techniques to prove the analogous results counting the number of unique cliques of size \lfloor n/2\rfloor are very similar. \ETH-Hardness of Computing the Number of Hamiltonian Cycles Modulo p on a Directed Multigraph Note that due to the O(n+m)-space reduction from 3\SAT on n variables and m clauses to the problem of deciding whether there is a clique of size \lfloor n/2\rfloor in an undirected multigraph (Appendix A) or deciding whether there is a Hamiltonian cycle in a directed multigraph, along with the Sparsification Lemma of Impagliazzo et al., (2001) (Lemma 6), neither of these problems should have 2^{o(n)}-time algorithms under the Exponential Time Hypothesis (\ETH) (Impagliazzo and Paturi, , 2001), the hypothesis that 3\SAT on n variables requires 2^{\gamma n}-time for some \gamma>0. We show, due to a randomized reduction from the decision problems (Lemmas 7 and 8) that we cannot count for growing p, the number of unique cliques of size \lfloor n/2\rfloor in an undirected multigraph or Hamiltonian cycles in a directed multigraph in 2^{o(n)}-time under \rETH; however, since the algorithm for 3\SAT would be randomized in the case of an algorithm for these problems. Hardness Amplification Using the STV List Decoder The STV List Decoder of Sudan et al., (2001) (Lemma 2) is a potent tool for error correction. Formally, we speak more about it in Section 2.3, but in essence, given an oracle that is barely, but sufficiently correct on some polynomial f of degree at most d, the STV list decoder gives us some number of machines M computing polynomials of degree at most d, one of which is our function of interest. We use this list decoder to obtain a probabilistic algorithm correct on all inputs from an algorithm that is correct on a small, vanishing fraction of instances. We are not the first to use the STV list decoder to prove hardness results. Our usage of it is inspired by its usage in Goldreich and Rothblum, (2018). Goldenberg and Karthik, (2020) shows one more such application of this tool to amplify hardness. Identifying the Correct Machine. On the problem of amplifying from a barely correct algorithm, we use the STV list decoder (Lemma 2), which gives us some machines M, all of which compute polynomials of degree upper bounded by the degree of our function of interest. So, we iterate through each machine and test whether it computes our function of interest. A rough outline of this test is as follows. 1. Given a machine M, we first test whether it computes a “valid” multigraph counting function. The techniques we use here are the pigeonhole principle based techniques for counting Hamiltonian cycles and interpolation techniques for counting half-cliques. 2. Given that the function is promised to compute a “valid” multigraph counting function, how do we know if it is invariant under graph isomorphism? The test relies on straightforward ideas: Lagrange’s theorem (Herstein, , 1975), the idea for finite groups that the order of a subgroup H (of G) must divide the order of G and the somewhat silly fact that the smallest integer larger than 1 is 2. Suppose we have a counting function H_{n,p} on multigraphs. Let \Pi\left(H_{n,p}\right) be the subgroup of S_{n} such that permuting the vertices and edges to the input graph of H_{n,p}, for any input graph, does not change the output. If H_{n,p} is invariant under graph isomorphism, then \Pi\left(H_{n,p}\right) is S_{n}. However, if \Pi\left(H_{n,p}\right) is not S_{n}, then it is at most half the size of S_{n}. Indeed, this is precisely the insight we use. We pick a random graph and a random permutation from S_{n}. For sufficiently large n, the probability that the function H_{n,p} does not change throughout this operation is close to \left|\Pi\left(H_{n,p}\right)\right|/|S_{n}|. If H_{n,p} is indeed invariant under graph isomorphism, then \left|\Pi\left(H_{n,p}\right)\right|/|S_{n}|=1 and otherwise, \left|\Pi\left(H_{n,p}\right)\right|/|S_{n}|\leq 1/2, and we reject with a probability of roughly 1/2. 3. In this step, we try to identify our functions of interest, guaranteed that the machine computes an invariant function under graph isomorphism. For both problems, we classify all graph counting functions based on insight from conjugacy classes and use that to our advantage. For the problem of counting Hamiltonian cycles, the insight is that this function is the only one that places zero weight on any cycle cover other than the Hamiltonian cycles. In the case of counting half-cliques, the argument is more complicated."
https://arxiv.org/html/2411.09597v1,Rare-Case Hard Functions Against Various Adversaries,"We say that a function is rare-case hard against a given class of algorithms (the adversary) if all algorithms in the class can compute the function only on an o(1)-fraction of instances of size n for large enough n. Starting from any \NP-complete language, for each k>0, we construct a function that cannot be computed correctly on even a 1/n^{k}-fraction of instances for polynomial-sized circuit families if \NP\not\subset\PPOLY and by polynomial-time algorithms if \NP\not\subset\BPP - functions that are rare-case hard against polynomial-time algorithms and polynomial-sized circuits. The constructed function is a number-theoretic polynomial evaluated over specific finite fields. For \NP-complete languages that admit parsimonious reductions from all of \NP (for example, \SAT), the constructed functions are hard to compute on even a 1/n^{k}-fraction of instances by polynomial-time algorithms and polynomial-sized circuit families simply if \P^{\SHARPP}\not\subset\BPP and \P^{\SHARPP}\not\subset\PPOLY, respectively. We also show that if the Randomized Exponential Time Hypothesis (RETH) is true, none of these constructed functions can be computed on even a 1/n^{k}-fraction of instances in subexponential time. These functions are very hard, almost always.While one may not be able to efficiently compute the values of these constructed functions themselves, in polynomial time, one can verify that the evaluation of a function, s=f(x), is correct simply by asking a prover to compute f(y) on targeted queries.","For decades, complexity theory has focused chiefly on worst-case hardness, from the original proofs of Cook, (1971) and Levin, (1973) that the satisfiability language (\SAT) is \NP-complete to Karp, (1972) showing the following year that many natural languages are \NP-complete as well. These languages are not solvable by deterministic polynomial-time algorithms if \P\neq\NP. However, for many applications, cryptography being the foremost, we want better guarantees of hardness than just worst-case hardness. It is not enough for our cryptographic protocols, that for every algorithm, there is some instance that is hard. This motivates the need for “rare-case” hardness. Suppose we can guarantee that for some problem, for any reasonably fast algorithm, the algorithm only outputs the correct answer on an o(1)-fraction of instances. In that case, we can be assured that, for large enough n, any instance we randomly generate will probably not be solvable by a reasonably fast adversary. The phrase “rare-case hardness” is inspired by its usage by Goldreich and Rothblum, (2018) on counting t-cliques, where they show that counting cliques of a specific size in a graph is hard for even a 1/\polylog(n)-fraction of instances if it is in the worst case. Similar work has been done to show that some variants of k-clique are as hard in the average-case as they are in the worst case (Dalirrooyfard et al., , 2020; Boix-Adsera et al., , 2019). Similar results have been shown by Kane and Williams, (2019) for the orthogonal vectors (\OV) problem against \AC^{0} formulas under certain worst-case hardness assumptions. They have shown the existence of a distributional \OV problem that can be solved by o(n^{2})-sized \AC^{0} circuits for a 1-o(1)-fraction of instances. As a motivational example, consider the problem of multiplying two n-bit numbers (\MULT_{n}). Harvey and van der Hoeven, (2021) have proved that \MULT_{n} can be solved in O(n\log n)-time on a multitape Turing machine (MTM). We can say that \MULT_{n} is easy for the set of O(n\log n)-time MTMs, since there exists at least one MTM that solves \MULT_{n} correctly over all instances with parameter n. It is an open problem whether there exists an O(n)-time MTM which correctly solves \MULT_{n} on all instances with parameter n (Afshani et al., , 2019). Now we ask the question: what is the largest fraction of instances an O(n)-time MTM can solve \MULT_{n}? If the answer to this question is 1, then we say that \MULT_{n} is easy for the set of O(n)-time MTMs. If the answer is a constant, we say that \MULT_{n} is average-case hard (Ball et al., , 2017) for the set of O(n)-time MTMs. Finally, if the answer is a negligible fraction that tends to 0 as n tends to infinity, we say that \MULT_{n} is rare-case hard (formally defined in Section 3) for the set of O(n)-time MTMs. Another famous and instructive example of rare-case hardness is the usage of the Discrete Logarithm Problem (\DLP) in the pseudorandom generator of Blum and Micali, (1982), depending on the worst-case hardness of the \DLP. The \DLP asks whether given a prime p of n-bits, a multiplicative generator g of \mathbb{Z}^{*}_{p}, and l\in\mathbb{Z}^{*}_{p}, to find r such that g^{r}\equiv l\mod p. Suppose for any k>0, we have a polynomial-time algorithm (an oracle O) solving the \DLP on a 1/n^{k}-fraction of instances for n-bit primes. We have a simple worst-case to rare-case reduction (formally defined in Section 3) - given l, simply generate r^{\prime} at random and ask O for the answer to the \DLP for l\cdot g^{r^{\prime}}. If O returns r, check if g^{r-r^{\prime}}\equiv l\mod p, and return r-r^{\prime} if so. Otherwise, we will repeat this process. We are expected to find the answer in n^{k} queries, giving us a probabilistic algorithm. Due to this, we have that if the \DLP is not solvable by randomized polynomial-time algorithms, then no randomized or deterministic algorithm solves the \DLP on a 1/n^{k}-fraction of instances for any k, giving us a one-way function. However, we would also like to construct families of hard problems that are hard due to many weak conjectures and hypotheses, which, when scaled down to asymptotically small input sizes, can also give us protocols such as proof of work (Dwork and Naor, , 1993), that are hard to solve almost all the time, but always very quick to verify333Say, under some conjecture, taking \Omega(n^{2})-time for a prover to solve, but \polylog(n)-time to verify.. In this paper, we show that we can construct infinite families of such rare-case hard functions using \NP-complete languages as our starting point. The constructed functions are number-theoretic polynomials evaluated over \mathbb{Z}_{p} for certain primes p. These families also have polynomial-time interactive proof systems where the prover and verifier only need to communicate inputs and outputs to the constructed function for a verifier to be convinced. In fact, the interactive proof system is used within the reduction. Interestingly, we can look at any reduction as an interactive proof with varying degrees of trust. Many-one polynomial-time reductions for \NP-completeness fully trust the prover and take the prover’s word as gospel. Here, since our hypothetical algorithm is correct only sometimes, we do not trust it fully but employ some tools to help extract more truth from an oracle that tells the truth only sometimes. A notable work that uses a verification protocol as a reduction is by Shamir, (1992) that proves \IP=\PSPACE. We use a modified version of the sumcheck protocol as proposed by Lund et al., (1992). We use a theorem of Sudan et al., (2001) that Goldreich and Rothblum, (2018) use to error-correct to go from an algorithm that is correct on a small fraction of instances to a randomized algorithm that is correct with very high probability on any instance. As with this paper, and most other works on average-case hardness (Ball et al., , 2017), we leverage the algebraic properties of low-degree polynomials over large fields to show that if such a polynomial is “sufficiently expressive”, in that it can solve a problem we believe to be hard in the worst-case with a small number of evaluations of the polynomial, we can error-correct upwards from the low-correctness regime to solve our problem that is conjectured to be hard with a very high probability. The remainder of the paper is organized as follows. Section 2 gives the preliminaries. Section 3 gives an overview of our results. Section 4 describes the generalized certificate counting polynomials. Section 5 gives an oracle sumcheck protocol over \mathbb{Z}_{p}. Section 6 gives a method to reconstruct the certificate counting polynomials over \mathbb{Z}. Section 7 proves the main results of the paper. Finally, we conclude in Section 8."
https://arxiv.org/html/2411.09646v1,"Reducing Stochastic Games 
to Semidefinite Programming","We present a polynomial-time reduction from max-average constraints to the feasibility problem for semidefinite programs. This shows that Condon’s simple stochastic games, stochastic mean payoff games, and in particular mean payoff games and parity games can all be reduced to semidefinite programming.","There are two important clusters of computational problems that are not known to be in the complexity class P: the first is related to numeric computation, and contains the sums-of-square-roots problem [GGJ76, EHS24], the Euclidean shortest path problem, PosSLP [ABKPM09], and the feasibility problem for semidefinite programs [NN94, Ram97]. The second cluster is related to tropical geometry, and contains for instance the model checking problem for the propositional \mu-calculus [EJ91, EJS93], parity games [Mar75, Mos91], mean payoff games [EM79], and/or scheduling [MSS04], stochastic mean payoff games [AM09], and simple stochastic games [Con92]. So far, no polynomial-time reduction from a problem of one of the clusters to a problem from the other cluster was known. We show that all of the mentioned problems from the second cluster can be reduced in polynomial time to the feasibility problem for semidefinite programs from the first cluster. Semidefinite programming is a generalisation of linear programming with many important algorithmic applications in discrete and continuous optimisation, both from a theoretical and a practical perspective [GLS94, BTN01, Las09]. However, already the feasibility problem for semidefinite programs is not known to be in P. By Ramana’s duality [Ram97], the problem is either in the intersection of NP and coNP, or outside of the union of NP and coNP. The semidefinite feasibility problem falls into the existential theory of the reals, which is known to be in PSPACE [Can88]. However, it has been observed by Khachiyan that the smallest feasible solution to an SDP might be of doubly exponential size (see, e.g., [PK97]), which is an obstacle for the polynomial-time algorithmic methods known for linear programming. In fact, it is possible to reduce the PosSLP problem to semidefinite programming [TV08]. PosSLP has been introduced in [ABKPM09], motivated by the ‘generic task of numerical computation’. For example, the sums-of-square-roots problem, which is a numeric computational problem not even known to be in NP and the barrier for polynomial-time tractability for numerous problems in computational geometry, such as the Euclidean shortest path problem, has a polynomial-time reduction to PosSLP [ABKPM09]. Mean payoff games are turn-based two-player games on graphs [EM79, ZP96]; it is a famous open problem in theoretical computer science whether there is a polynomial-time algorithm to decide for a given graph which of the players has a winning strategy. Finding the winning region in mean payoff games is polynomial-time equivalent to various other computational problems in theoretical computer science, for instance scheduling with and-or-constraints [MSS04], the max-atoms problem [BNRC08], as well as solvability of max-plus systems and tropical linear feasibility [ABG07, AGG12]. The latter can be seen as the tropical analog of testing feasibility of linear inequalities (which for classical geometry is known to be in P, e.g. via the ellipsoid method [Kha79, GLS94]). Furthermore, there is a simple reduction from parity games to mean payoff games by using the priorities of the parity game with a suitably large basis [Pur95]. Parity games are polynomial-time equivalent to the model-checking problem of the propositional \mu-calculus [EJS93], which has been called ‘the most important logics in model checking’ [BW18]. For parity games, a quasipolynomial algorithm has been found recently [CJK+22]. Subsequent work indicates that the various quasipolynomial methods that have been obtained lately for parity games [LPSW22, JMT22, LB20] do not extend to mean payoff games [CFGO22]. Mean payoff games can be generalised to stochastic mean payoff games (sometimes called 2\frac{1}{2}-player games), where the graph might contain stochastic nodes. If the strategy of one of the two players in such a game is fixed, the game turns into a Markov decision process, for which polynomial-time algorithms based on linear programming are known [Put05]. Stochastic mean payoff games are equivalent under polynomial-time Turing reductions to Condon’s simple stochastic games [AM09], which are known to be in the intersection of NP and coNP [Con92] (we may even assume that the simple stochastic games are stopping, see the discussion in Section 2.2.) There are many other variants of games that all have the same complexity, see [AM09]. Stochastic mean payoff games can also be reduced to max-plus-average constraints, which are still in NP \cap coNP [BM18]. Figure 1: Computational problems that are not known to be in P and not known to be NP-hard, and their relationship. Arcs indicate polynomial-time (many-one) reductions, the dotted arc indicates a polynomial-time Turing reduction. We introduce a fragment of max-plus-average constraints, which we call max-average constraints. There is a polynomial-time reduction from stopping simple stochastic games to max-average constraints (Section 3). Max-average constraints have the advantage that they can be further reduced to the feasibility problem for semidefinite programs. This implies that all the mentioned computational problems from the second cluster can be reduced to semidefinite programming. See Figure 1 for an overview of the mentioned computational problems and their relationship. The reduction uses a similar idea as the reduction of Schewe [Sch09] to reduce parity games and mean payoff games to linear programming, which is, however, not a proper polynomial-time reduction. The idea is to replace constraints of the form x_{0}=\max(x_{1},\dots,x_{k}) by x_{0}=\log_{b}(b^{x_{1}}+\dots+b^{x_{k}}) for a sufficiently large b. Following [AGS18], we extend this idea to max-average constraints and obtain a proper polynomial-time reduction to non-Archimedean SDPs. From there, we use bounds from quantifier elimination results to translate non-Archimedean SDPs to real SDPs. However, we still have the obstacle that these SDPs involve coefficients of doubly exponential size, which means that they have exponential representation size and do not lead to a polynomial-time reduction. We overcome this difficulty by using the duality of semidefinite programming to find small expressions that define coefficients of doubly exponential size, which combined with the above ideas leads to a polynomial-time reduction from max-average constraints to (real) SDPs. Our results imply that if there is a polynomial-time algorithm for SDPs, then all the mentioned problems in the second cluster can be solved in polynomial time as well. Conversely, our reduction can be used to translate interesting families of instances for (stochastic) mean payoff games or parity games into SDP instances, which might yield interesting families of instances for (algorithmic approaches to) semidefinite programming. For linear programming, this idea turned out to be very fruitful: certain instances of parity games have been used to prove exponential lower bounds for several famous pivoting rules for the simplex method in linear programming [FHZ11, Fri11, FHZ14, AF17, DFH23]. However, these results were not based on a general polynomial-time reduction from parity games to linear programming (which is not known!). We also hope that algorithmic ideas for solving games might generalise to (certain classes of) SDPs. Our results also show that two large research communities (convex real algebraic geometry, and more generally convex optimisation on the one hand, and verification and automata theory on the other hand) were working on closely related computational problems and might profit from joining forces in the future. 1.1 Related work There are multiple works that reduce games to continuous optimization problems. As mentioned above, one-player mean payoff games (Markov decision processes) can be reduced to linear programming [Put05], which implies that they can be solved in polynomial time. For the case of two-player deterministic mean payoff games, Schewe [Sch09] proposed a reduction to linear programs with coefficients of exponential representation size. Such programs can also be interpreted as linear programs over non-Archimedean ordered fields [ABGJ15, ABGJ14, ABGJ21]. The idea of Schewe was generalized to stochastic games in [BEGM17], where the authors prove that stochastic mean payoff games can be encoded by convex programs with constraints of exponential encoding size. Such programs can be solved by the ellipsoid method in pseudo-polynomial time provided that the number of stochastic nodes is fixed. The non-Archimedean approach was generalized from deterministic games to stochastic games in the work [AGS18], which proves that stochastic mean payoff games can be reduced to semidefinite programs over non-Archimedean fields. The paper [AGS18] serves as a basis for our results. These reductions express mean payoff games as convex optimization problems, but these problems are “non-standard” in the sense that they either use constraints of exponential encoding size or are expressed over non-Archimedean fields. There is yet another line of work deriving polynomial-time reductions from mean payoff games to classical optimization problems which are not convex. In particular, [Con93] expressed simple stochastic games as non-convex quadratic programs. This reduction is further studied in [KRSW22]. Another reduction to (generalized) complementarity problems with P-matrices was proposed in [GR05, SV06] and extended in subsequent works [JS08, FJS10, HIJ13]. Such linear complementarity problems form a subclass of quadratic programs and belong to the complexity class UniqueEOPL [FGMS20]. However, they are in general non-convex and can therefore not be expressed by semidefinite programs."
https://arxiv.org/html/2411.08183v1,Locally Sampleable Uniform Symmetric Distributions,"We characterize the power of constant-depth Boolean circuits in generating uniform symmetric distributions. Let f\colon\{0,1\}^{m}\to\{0,1\}^{n} be a Boolean function where each output bit of f depends only on O(1) input bits. Assume the output distribution of f on uniform input bits is close to a uniform distribution \mathcal{D} with a symmetric support. We show that \mathcal{D} is essentially one of the following six possibilities: (1) point distribution on 0^{n}, (2) point distribution on 1^{n}, (3) uniform over \{0^{n},1^{n}\}, (4) uniform over strings with even Hamming weights, (5) uniform over strings with odd Hamming weights, and (6) uniform over all strings. This confirms a conjecture of Filmus, Leigh, Riazanov, and Sokolov (RANDOM 2023).","Despite being one of the simplest models of computation, \mathsf{NC^{0}} circuits (i.e., Boolean circuits of constant depth and bounded fan-in) elude a comprehensive understanding. Even very recently, the model has been the subject of active research on the range avoidance problem [33, 19, 18], quantum advantages [5, 47, 6, 48, 26], proof verification [17, 4, 25], and more. Pertinent to this paper is the study of the sampling power of \mathsf{NC^{0}} circuits. While the general problem was considered at least as early as [24], interest in the \mathsf{NC^{0}} setting has seen a strong uptick lately [38, 28, 7, 15, 41, 42, 20, 11, 43, 16, 26, 34]. At a high level, it considers what distributions can be (approximately) produced by simple functions on random inputs. More formally, let f(\mathcal{U}^{m}) denote the distribution resulting from applying an \mathsf{NC^{0}} function f\colon\{0,1\}^{m}\to\{0,1\}^{n} to a random string drawn from \mathcal{U}^{m}, the uniform distribution over \{0,1\}^{m}. Typically, m is viewed as being arbitrarily large and n is the parameter of interest. Then the goal is to analyze the distance between f(\mathcal{U}^{m}) and some specific distribution. Aside from being inherently interesting, this question has also played a crucial role in applications ranging from data structure lower bounds [38, 28, 7, 42, 11, 43, 26] to pseudorandom generators [37, 28, 7] to extractors [39, 15, 40, 14, 12] to coding theory [34]. One recurring class of distributions in this line of work is uniform symmetric distributions (i.e., uniform distributions over a symmetric support). Indeed, these are precisely the distributions that arise in an elegant connection to succinct data structures (see [38, Claim 1.8]), for example. Moreover, this seemingly simple class is already rich enough to allow surprisingly powerful results. For example, \mathsf{NC^{0}} circuits can sample the uniform distribution over the preimage \mathrm{PARITY}^{-1}(0) (and \mathrm{PARITY}^{-1}(1)), despite a celebrated result of Håstad [22] proving that more powerful \mathsf{AC^{0}} circuits require an exponential number of gates to compute \mathrm{PARITY}. Perhaps more surprisingly, the strategy to sample a uniform random string with even Hamming weight is extremely simple: map the uniform random bits x_{1},\ldots,x_{n} to x_{1}\oplus x_{2},x_{2}\oplus x_{3},\ldots,x_{n}\oplus x_{1} [2, 8]. A number of notable prior results already rule out specific distributions from being accurately sampled by \mathsf{NC^{0}} circuits. For example, let \mathcal{D}_{k} denote the uniform distribution over all n-bit strings of Hamming weight k. The influential early paper of [38] showed that such shallow circuits could not accurately sample \mathcal{D}_{k} for k=\Theta(n) under certain assumptions about the input length or accuracy tolerance; recent works [16, 43, 26] have eliminated the need for these assumptions. Additionally, a number of results are known for uniform symmetric distributions over multiple Hamming weights, such as the case of exclusively tail weights [16], all weights divisible by q for fixed 3\leq q\ll\sqrt{n} [26], and all weights above n/2 [17, 38, 16] (see also [48]). Despite much effort, the previous body of work proceeds in a somewhat ad-hoc fashion, with techniques tailored to rule out specific cases. However, an exciting recent work by Filmus, Leigh, Riazanov, and Sokolov [16] gave the following bold conjecture about the capabilities of \mathsf{NC^{0}} circuits for sampling distributions, unifying prior results. Conjecture 1.1. Let f\colon\{0,1\}^{m}\to\{0,1\}^{n} be computable by an \mathsf{NC^{0}} circuit. If f(\mathcal{U}^{m}) is \varepsilon-close (in total variation distance) to a uniform symmetric distribution and n is sufficiently large, then f(\mathcal{U}^{m}) is O(\varepsilon)-close to one of the following six distributions: • Point distribution on 0^{n}. • Point distribution on 1^{n}. • Uniform distribution over \left\{0^{n},1^{n}\right\}. • Uniform distribution over strings with even Hamming weights. • Uniform distribution over strings with odd Hamming weights. • Uniform distribution over all strings. All six distributions can be sampled (exactly) by functions whose output bits each depend on at most two input bits. Hence one may informally view the conjecture as asserting that more input dependencies do not substantially increase the ability of \mathsf{NC^{0}} circuits to generate uniform symmetric distributions. In this work, we confirm the conjecture of [16] as follows. Theorem 1.2 (Consequence of Theorem 4.1). Let d be a fixed constant, and suppose f\colon\{0,1\}^{m}\to\{0,1\}^{n} is computable by an \mathsf{NC^{0}} circuit of depth at most d. If f(\mathcal{U}^{m}) is \varepsilon-close (in total variation distance) to a uniform symmetric distribution and n is sufficiently large, then f(\mathcal{U}^{m}) is O(\varepsilon)-close to one of the six distributions in 1.1. Note that this result is optimal up to the implicit constant in O(\varepsilon). We include a more thorough discussion of our result’s tightness in Section 4, where we present a quantitative version of Theorem 1.2 parametrized by the locality (i.e., number of input bits each output bit depends on) of f. The following corollary is immediate. Corollary 1.3. For sufficiently large n, the only uniform symmetric distributions over \{0,1\}^{n} exactly sampleable by \mathsf{NC^{0}} functions are the six distributions in 1.1. As a contrasting example to the limitation given by Theorem 1.2, consider the next simplest class of circuits commonly studied: \mathsf{AC^{0}}. Up to some exponentially small error, they are able to sample the uniform distribution over permutations of [n] [29, 21]. Thus by sampling 1^{w}0^{n-w} for the appropriate distribution over weights w accepted (or rejected) by a symmetric function f, one can apply a randomly sampled permutation to output the uniform distribution over f^{-1}(1) (or f^{-1}(0)) [38, Lemma 4.3]. Paper Organization. We provide a proof overview of Theorem 1.2 in Section 2. Preliminary definitions and results are given in Section 3. The full proof of our main result is in Section 4, with some technical proofs deferred to the appendices."
https://arxiv.org/html/2411.08792v1,An alignment problem,"This work concerns an alignment problem that has applications in many geospatial problems such as resource allocation and building reliable disease maps. Here, we introduce the problem of optimally aligning k collections of m spatial supports over n spatial units in a d-dimensional Euclidean space. We show that the 1-dimensional case is solvable in time polynomial in k, m and n. We then show that the 2-dimensional case is NP-hard for 2 collections of 2 supports. Finally, we devise a heuristic for aligning a set of collections in the 2-dimensional case.","This work concerns the problem of aligning collections of spatial supports which share a common set of spatial units. For example, Figures 1a and 1b each depicts a collection of four supports (green, yellow, orange, and blue) which shares a common set of 16 spatial units (rectangular blocks). The goal is to swap units from one support to another within each collection (change the colors of blocks) until the collections are identical, i.e., are aligned, as depicted in Figure 1c. Note that there are many different ways to align the supports, i.e., the alignment depicted in Figure 1c is not unique. With this in mind, it would be preferable to align such collections using the minimum number of (possibly weighted) swaps. This optimization problem is easy in some cases, and (NP-) hard in others. In general, the alignment problem is on a set U of n spatial units, each unit u\in U having a population count p(u) within a certain spatial boundary, disjoint from other units. These spatial units can represent census tracts or ZIP code tabulation areas. Constructing maps, e.g., choropleth maps, which reflect certain rates within a population, such as cancer incidence, provides an intuitive way to portray the geospatial patterns of such rates. This can provide decision support in public health surveillance, which can aid officials to form the appropriate policies. Building such a map at the level of an individual unit can produce misleading results due to small populations in some units, resulting in statistically unstable rates. To remedy this issue, sets s\subseteq U of contiguous units are aggregated to create larger spatial supports with adequate population counts to ensure a stable rate calculation, as depicted in Figure 1a. (a) (b) (c) Figure 1: Collections (a) \mathcal{S}=\{s_{1},s_{2},s_{3},s_{4}\} and (b) \mathcal{T}=\{t_{1},t_{2},t_{3},t_{4}\} of spatial supports over the set U=\{u_{1,1},u_{1,2},\dots,u_{4,4}\} of 16 spatial units. An alignment (c) of \mathcal{S} and \mathcal{T}. The red dots mark the units (u_{2,1},u_{3,1},u_{1,2},u_{2,2},u_{3,2},u_{2,3},u_{2,4}) on which \mathcal{S} and \mathcal{T} disagree. Suppose a certain rate, e.g., prostate cancer incidence, can be mostly explained by a factor such as age. In this case, we want to create several maps, which represent each age stratum in order to more clearly portray this factor in determining such rate. For the sake of illustration, suppose that \mathcal{S} and \mathcal{T}, depicted in Figures 1a and 1b are two such maps, represented as collections of supports over U. In \mathcal{S}, the populations p_{\mathcal{S}}(u) of each unit u in s_{1},s_{2},s_{3},s_{4} are 20, 20, 10, 15, respectively—e.g., p_{\mathcal{S}}(u_{1,1})=20 (u_{1,1}\in s_{1}), and p_{\mathcal{S}}(u_{4,4})=10 (u_{4,4}\in s_{3})—while the populations p_{\mathcal{T}}(u) of each unit u in t_{1},t_{2},t_{3},t_{4} are 15, 15, 12, 20, respectively. In this way, the total population in any support (of \mathcal{S} or \mathcal{T}) is 60. We want to consolidate the information across these maps onto a single map, however, and this requires to align their collections of supports. To align collections of supports is to modify the supports of all collections, in terms of the units they contain, such that the resulting supports remain contiguous, and the resulting collections are identical. This can be viewed as “swapping” units between neighboring supports until the desired alignment is reached. For example, Figure 1c depicts an alignment of collections \mathcal{S} and \mathcal{T}. Such an alignment is obtained from \mathcal{S} by swapping u_{1,2} and u_{2,2} from s_{4} (blue) to s_{1} (green), u_{2,3} from s_{3} (orange) to s_{1} (green), and u_{2,4} from s_{3} (orange) to s_{4} (blue). The alignment is obtained from \mathcal{T} by swapping u_{2,1} and u_{3,1} from t_{2} (yellow) to t_{1} (green), and u_{3,2} from t_{3} (orange) to t_{2} (yellow). Since any collection of contiguous supports—including supports that may not be currently present, e.g., a hypothetical s_{5}—is an alignment, it is desirable to produce an alignment that minimizes the maximum number of changes in any one collection. Since \mathcal{S} and \mathcal{T} disagree on 7 units (u_{2,1},u_{3,1},u_{1,2},u_{2,2},u_{3,2},u_{2,3},u_{2,4}, annotated with the red dots in Figure 1c), one collection must have at least 4 changes (the other collection having 3 changes), hence the alignment depicted in Figure 1c satisfies this criterion. This need to adjust leads to a notion of a distance, d(\mathcal{S},\mathcal{T}), between a pair \mathcal{S} and \mathcal{T} of collections of spatial supports, namely the number of swaps needed to transform \mathcal{S} into \mathcal{T}—this is simply the number of units on which the pair of collections disagree. Here, d(\mathcal{S},\mathcal{T})=7, and since an alignment is just another collection, if we denote the alignment of Figure 1c as collection \mathcal{A} of supports, then d(\mathcal{S},\mathcal{A})=4, and d(\mathcal{T},\mathcal{A})=3. Note that this distance is symmetric. The units of the different collections being swapped contain populations, however. Hence each swap has an associated cost, namely the population p_{\mathcal{C}}(u) of the unit u in the collection \mathcal{C} being swapped. For example, in \mathcal{S}, swapping u_{1,2} from s_{4} (blue) to s_{1} (green) costs p_{S}(u_{1,2})=15. This idea leads to a notion of a weighted distance d_{w}(\mathcal{S},\mathcal{A}) between a pair \mathcal{S} and \mathcal{A} of collections of spatial supports, or the overall cost of the swaps needed to transform \mathcal{S} into \mathcal{A}. Here, d_{w}(\mathcal{S},\mathcal{A})=p_{S}(u_{1,2})+p_{S}(u_{2,2})+p_{S}(u_{2,3})+p_% {S}(u_{2,4})=15+15+10+10=50, while d_{w}(\mathcal{T},\mathcal{A})=p_{T}(u_{2,1})+p_{T}(u_{3,1})+p_{T}(u_{3,2})=15% +15+12=42. Note that this weighted distance is not symmetric, i.e., d_{w}(\mathcal{S},\mathcal{T})\neq d_{w}(\mathcal{T},\mathcal{S}) in general. So, more precisely, it is desirable to produce an alignment \mathcal{A} that minimizes the maximum weighted distance d_{w}(\mathcal{C},\mathcal{A}) between any collection \mathcal{C} of spatial supports and \mathcal{A}. After careful inspection, no alignment can achieve such a weighted distance less than 50, hence the alignment depicted in Figure 1c satisfies this weighted criterion as well. We formalize the alignment problem as follows. Problem 1 (The Alignment Problem). Input: A base set U=\{u_{1},\dots,u_{n}\} of units over some Euclidean geospatial region and set \mathscr{C}=\{\mathcal{C}_{1},\dots,\mathcal{C}_{k}\} of collections of spatial supports. Each unit u\in U has population p_{\mathcal{C}}(u) from p_{\mathcal{C}}:U\rightarrow\mathbb{N}, specific to each collection \mathcal{C}\in\mathscr{C}. Each \mathcal{C}\in\mathscr{C} is a collection \{s_{\mathcal{C}}^{1},\dots s_{\mathcal{C}}^{m}\} of contiguous supports such that: (a) s\subseteq U for each s\in\mathcal{C}; (b) s\cap t=\emptyset for any pair s,t\in\mathcal{C} of distinct supports; and (c) \bigcup_{s\in\mathcal{C}}s=U. Output: A collection \mathcal{A} of contiguous supports which satisfies properties (a–c) above, such that \max\{d_{w}(\mathcal{C},\mathcal{A})~{}|~{}\mathcal{C}\in\mathscr{C}\} is minimized. Note that properties (a–c) ensure that the set of supports partitions the base set U. That is, (a) supports contain sets of contiguous units, (b) pairs of distinct supports are disjoint, and (c) the supports cover the base set U. In Section 2, we show that if the Euclidean geospatial region, that the set U of units is over, is 1-dimensional, then Problem 1—which we will refer to as the Alignment Problem, when the context is clear—is solvable in time polynomial in k, m and n. In Section 3, we show that if the geospatial region is 2-dimensional—which is the typical case in this context of constructing age-adjusted maps—then the Alignment Problem is NP-hard, even in the case of 2 collections, each with 2 supports. Finally, in Section 4, we outline a heuristic for the Alignment Problem in the 2-dimensional case. Section 5 concludes the paper and outlines future work."
https://arxiv.org/html/2411.07972v1,A Zero-Knowledge PCP Theorem,"We show that for every polynomial q^{*} there exist polynomial-size, constant-query, non-adaptive PCPs for \mathsf{NP} which are perfect zero knowledge against (adaptive) adversaries making at most q^{*} queries to the proof. In addition, we construct exponential-size constant-query PCPs for \mathsf{NEXP} with perfect zero knowledge against any polynomial-time adversary. This improves upon both a recent construction of perfect zero-knowledge PCPs for \#\mathsf{P} (STOC 2024) and the seminal work of Kilian, Petrank and Tardos (STOC 1997).","The PCP theorem [AroraS98, ALMSS92] states that for any language in \mathsf{NP}, there exists a polynomial-size proof that can be checked probabilistically by reading only a constant number of bits from the proof; or, succinctly, \mathsf{NP}\subseteq\mathsf{PCP}[\log n,1]\;, where \mathsf{PCP}[r,q] is the class of all languages that admit a PCP verifier that uses O(r) random bits and reads O(q) bits of the proof (note that logarithmic randomness complexity implies polynomial proof length). While PCPs originated from the study of zero-knowledge proofs [GoldwasserMR89], there seems to be an intrinsic tension between the two notions: PCPs achieve locality by encoding NP witnesses in a manner that spreads global information throughout the proof, whereas zero-knowledge proofs aim to hide all information except for the validity of the statement. Moreover, PCPs are fundamentally non-interactive objects, whereas interaction is often crucial for zero knowledge. Indeed, one must take care in even defining zero knowledge PCPs (ZK-PCPs), as it is impossible to achieve non-trivial zero knowledge against a malicious verifier that reads the entire proof. In their seminal work on ZK-PCPs, Kilian, Petrank and Tardos [KilianPT97] identify two regimes of interest: (a) Polynomial-size PCPs that are zero-knowledge against a verifier that makes at most a fixed polynomial number of queries q^{*} to the PCP. In this regime, they construct ZK-PCPs for NP with polylogarithmic query complexity. We refer to q^{*} as the “query bound”. (b) Exponential-size PCPs that are zero-knowledge against any polynomial-time verifier. In this regime, they construct ZK-PCPs for NEXP with polynomial query complexity. However, these constructions fall short of a “zero-knowledge PCP theorem” in a fundamental way: the query complexity is polylogarithmic, as opposed to O(1), which is a characteristic property of the PCP theorem. A major obstacle to achieving O(1) query complexity via the [KilianPT97] approach is that the technique used to obtain zero knowledge leads to an inherently adaptive honest verifier (i.e., which makes multiple rounds of queries to the proof). Known query-reduction methods apply only to non-adaptive PCPs. Aside from its theoretical interest, non-adaptivity is crucial for some applications [IshaiWY16]. Finally, these constructions only achieve statistical zero knowledge (SZK-PCP) and not perfect zero knowledge (PZK-PCP). Building on techniques developed in [BenSassonCFGRS17, ChiesaFGS18, ChenCGOS23], a recent work [GurOS2024] constructed exponential-size PZK-PCPs with polynomially many non-adaptive queries for \#\mathsf{P}. Our first result shows that such PZK-PCPs exist for \mathsf{NEXP}, with constant query complexity. Theorem 1. There exist PCPs for \mathsf{NEXP} of exponential length with a non-adaptive verifier that reads O(1) bits of the proof, which are perfect zero-knowledge against any efficient adversary. Our second result “scales down” the above to obtain polynomial-size non-adaptive PZK-PCPs for \mathsf{NP} with constant query complexity.111For our definition of the class \mathsf{PZK}\text{-}\mathsf{PCP}, see Definition 2.12. Theorem 2 (“Zero-knowledge PCP theorem”). For any q^{*}\leq 2^{\mathrm{poly}(n)}, there exist PCPs for \mathsf{NP} of length \mathrm{poly}(q^{*},n) with a non-adaptive verifier that reads O(1) bits of the proof, which are perfect zero-knowledge against any adversary reading at most q^{*} bits of the proof; i.e., \mathsf{NP}\subseteq\mathsf{PZK}\text{-}\mathsf{PCP}[\log n,1]\;. 1.1 Techniques PZK-PCPs for nondeterministic computation. This paper builds on the prior work of [GurOS2024], which proves a weaker version of 1 that only captures (decision) \#\mathsf{P}, by constructing non-adaptive PZK-PCPs for the sumcheck problem. Readers familiar with the PCP literature may wonder why this construction does not lead immediately to a ZK-PCP for NEXP; indeed, the first construction of a PCP for NEXP is essentially a reduction to sumcheck [BabaiFLS91]. However, as we discuss below, even with a PZK-PCP for the sumcheck problem, the BFLS construction is not zero knowledge. We start by briefly reviewing the BFLS construction, which is a PCP for the \mathsf{NEXP}-complete problem \mathsf{Oracle}\textsf{-}\mathsf{3SAT}, defined as follows. Definition 1 (Oracle 3-SAT). Let B\colon\{0,1\}^{r+3s+3}\to\{0,1\} be a 3-CNF. We say that B is implicitly satisfiable if there exists A\in\{0,1\}^{s}\to\{0,1\} such that for all z\in\{0,1\}^{r},b_{1},b_{2},b_{3}\in\{0,1\}^{s}, B(z,b_{1},b_{2},b_{3},A(b_{1}),A(b_{2}),A(b_{3}))=1. Let \mathsf{Oracle}\textsf{-}\mathsf{3SAT} be the language of implicitly satisfiable 3-CNFs. The BFLS PCP consists of two parts. First, the multilinear extension of the witness A; i.e., a multilinear polynomial \hat{A} over some finite field \mathbb{F} such that \hat{A}(x)=A(x) for all x\in\{0,1\}^{s}. Second, a sumcheck PCP222Strictly speaking, a PCP of proximity; we will ignore this distinction for this overview. for the following claim333To mitigate some technical issues with soundness, the actual construction uses a slightly different summand polynomial.: \sum_{\begin{subarray}{c}z\in\{0,1\}^{r}\\ b_{1},b_{2},b_{3}\in\{0,1\}^{s}\end{subarray}}\hat{B}(z,b_{1},b_{2},b_{3},\hat% {A}(b_{1}),\hat{A}(b_{2}),\hat{A}(b_{3}))=2^{r+3s}~{}, (1) where \hat{B} is an arithmetisation of the circuit B. A natural first step is to use the zero-knowledge sumcheck PCP of [GurOS2024] in place of the standard sumcheck PCP, which hides hard-to-compute information about partial sums. However, this does not yet yield a ZK-PCP, because the first part of the construction contains an encoding of the witness A, which violates the zero-knowledge condition. To hide A, we would like to use the sumcheck commitment scheme, introduced by [ChiesaFGS22] (see also [ChiesaFS17]) for their construction of an interactive PCP (IPCP) for \mathsf{NEXP}. A sumcheck commitment to the polynomial \hat{A}(\vec{X}) is a random polynomial C(\vec{X},\vec{Y}) of individual degree d^{\prime}\geq 2 in each Y-variable such that \sum_{c\in\{0,1\}^{k}}C(\alpha,c)=\hat{A}(\alpha) for all \alpha\in\mathbb{F}^{s}. It was shown in [ChiesaFGS22] that this commitment perfectly hides \hat{A} against all adversaries that make fewer than 2^{k} queries to C. The construction of [BabaiFLS91] uses the sumcheck protocol to reduce checking (1) to three random queries to \hat{A}. Building on this idea, the ZK-IPCP construction of [ChiesaFGS22] uses the interactive sumcheck protocol to open the sumcheck commitment to \hat{A} at those points. Finally, to ensure that those three evaluations do not leak information about the witness, \hat{A} is chosen to be a random multiquartic—rather than the unique multilinear—extension of A. However, the strategy above strongly relies on the interactivity of the ZK-IPCP. Indeed, observe that if we were to “unroll” this interaction into a PCP, we would simply write down all of \hat{A}! We must therefore establish (1) without opening the sumcheck commitment. That is, we would like to directly check: \sum_{\begin{subarray}{c}z\in\{0,1\}^{r}\\ b_{1},b_{2},b_{3}\in\{0,1\}^{s}\end{subarray}}\hat{B}\left(z,b_{1},b_{2},b_{3}% ,\sum_{c\in\{0,1\}^{k}}C(b_{1},c),\sum_{c\in\{0,1\}^{k}}C(b_{2},c),\sum_{c\in% \{0,1\}^{k}}C(b_{3},c)\right)=2^{r+3s}~{}, To do this, we first “pull out” the three inner summations. There are various ways this can be achieved; we follow a linearisation approach. First observe that (assuming \hat{A} takes boolean values on \{0,1\}^{s}), the LHS of (1) is equal to \sum_{a_{1},a_{2},a_{3}\in\{0,1\}}\sum_{\begin{subarray}{c}z\in\{0,1\}^{r}\\ b_{1},b_{2},b_{3}\in\{0,1\}^{s}\end{subarray}}\hat{B}(z,b_{1},b_{2},b_{3},a_{1% },a_{2},a_{3})\cdot\prod_{i=1}^{3}(\hat{A}(b_{i})-(1-a_{i}))~{}, since the product expression “zeroes out” any term of the sum for which some a_{i}\neq\hat{A}(b_{i}). Next, observe that for any b_{1},b_{2},b_{3},a_{1},a_{2},a_{3}, provided that \mathbb{F} has characteristic different from 2, \prod_{i=1}^{3}(\hat{A}(b_{i})-(1-a_{i}))=\prod_{i=1}^{3}\sum_{c\in\{0,1\}^{k}% }\left(C(b_{i},c)-\frac{1-a_{i}}{2^{k}}\right)=\sum_{c_{1},c_{2},c_{3}\in\{0,1% \}^{k}}\prod_{i=1}^{3}\left(C(b_{i},c_{i})-\frac{1-a_{i}}{2^{k}}\right)~{}. Taken together, we see that checking (1) is equivalent to checking \sum_{c_{1},c_{2},c_{3}\in\{0,1\}^{k}}\sum_{a_{1},a_{2},a_{3}\in\{0,1\}}\sum_{% \begin{subarray}{c}z\in\{0,1\}^{r}\\ b_{1},b_{2},b_{3}\in\{0,1\}^{s}\end{subarray}}\hat{B}(z,b_{1},b_{2},b_{3},a_{1% },a_{2},a_{3})\prod_{i=1}^{3}\left(C(b_{i},c_{i})-\frac{1-a_{i}}{2^{k}}\right)% =2^{r+3s}~{}, which can be proven in zero knowledge using the [GurOS2024] ZK-PCPP for sumcheck. In particular, the PCPP simulator requires only polynomially many evaluations of the summand to simulate polynomially many queries to the proof. By setting k=\omega(\log n) we can simulate those evaluations by lazily simulating C as a uniformly random polynomial (via an algorithm of [BenSassonCFGRS17]), and then evaluating the summand directly. The query complexity of the verifier is \mathrm{poly}(n). Our result for \mathsf{NP} is obtained in a similar way, scaling the parameters appropriately. In particular, for a given adversary query bound q^{*}, we can set k=O(\log q^{*}). The query complexity of the verifier is then \mathrm{poly}(\log n,\log q^{*}). We also point out that the honest prover in our construction for \mathsf{NP} is efficient, given a valid witness as input. Proof composition and zero knowledge. In order to obtain constant query ZK-PCPs for \mathsf{NP}, we would like to apply the proof composition paradigm [BenSassonGHSV06] to our ZK-PCPs. This involves composing a robust outer PCP with an inner PCP of proximity to obtain a PCP which inherits the randomness complexity of the former and query complexity of the latter. To do this, we first need to strengthen our ZK-PCPs to satisfy robust soundness (i.e., the local view of the verifier must be far from an accepting view, with high probability). Then we show that proof composition preserves the zero knowledge of the outer PCP. To the best of our knowledge, this is the first composition theorem for ZK-PCPs. Our first step is to obtain robust ZK-PCPs for \mathsf{NP} with polylogarithmic query complexity. While the PZK-PCP of [GurOS2024] (upon which our PZK-PCP builds) is an algebraic construction, it does not have constant robust soundness as written. We present a modification of the [GurOS2024] PCP for sumcheck, following the “query bundling” approach of [BenSassonGHSV06], that has constant robust soundness. The key challenge is to show that this modification preserves zero knowledge. In fact, we will show that a much more general class of “local” transformations preserve zero knowledge. This class includes not only query bundling, but also the subsequent steps of alphabet reduction and proof composition. To capture this class formally, we define a new notion, locally computable proofs. Definition 2 (Locally computable algorithms (informal; see Definition 3.1)). Let \mathcal{A} and \mathcal{A}_{0} be randomized algorithms. We say that \mathcal{A} is \ell-locally computable from \mathcal{A}_{0} if there exists an efficient, deterministic oracle algorithm f making at most \ell queries to its oracle such that, for every input x, the following two distributions are identically distributed: \mathcal{A}(x),\quad(f^{\pi_{0}}~{}|~{}\pi_{0}\leftarrow\mathcal{A}_{0}(x)). We show that if the PCP prover algorithm \mathcal{P} is \ell-locally computable from a zero-knowledge PCP prover \mathcal{P}_{0}, then provided \ell is asymptotically smaller than the query bound on the ZK-PCP, \mathcal{P} inherits the zero knowledge guarantee of \mathcal{P}_{0}. Intuitively, this is true because if a proof \pi is locally computable from a proof \pi_{0}, and \pi_{0} is zero knowledge, then we can apply f to the simulator for \pi_{0} to obtain a simulator for \pi. This notion is surprisingly versatile, and allows us to prove zero knowledge in an array of distinct settings: • Robustification of [GurOS2024]. We show that our modified [GurOS2024] construction is locally computable from the original construction, and thus inherits zero knowledge. • Alphabet reduction. Recall that alphabet reduction allows us to transform a robust PCP over a large alphabet into a boolean PCP, while maintaining robustness. It is performed by encoding each symbol of the PCP with a good error correcting code. More formally, if \pi is a distribution over robust PCP proofs over the alphabet \{0,1\}^{a} for some a\in\mathbb{N}, and \mathsf{ECC}\colon\{0,1\}^{a}\to\{0,1\}^{b} is a systematic binary error-correcting code of constant relative distance and rate, then we can obtain a robust boolean PCP by defining a new proof \tau(\alpha)\ {:=}\ \mathsf{ECC}(\pi(\alpha)) for every proof index \alpha, and writing \tau over the alphabet \{0,1\}. Then \tau is 1-locally computable from \pi by the following function: f^{\pi}(\alpha,i)=\mathsf{ECC}(\pi(\alpha))_{i}, where i\in[b]. We note that prior work on alphabet reduction for ZKPCPs [HazayVW22] required a much more complex construction because their ZKPCP achieves only a quadratic gap between the honest verifier’s query complexity and the query bound. • Proof composition. Recall that composition of an outer PCP system (\mathcal{P}_{\text{out}},\mathcal{V}_{\text{out}}) for \mathcal{L} with an inner PCP of proximity (\mathcal{P}_{\text{in}},\mathcal{V}_{\text{in}}) for circuit evaluation proceeds as follows. Let \pi_{\text{out}}\leftarrow\mathcal{P}_{\text{out}}. For every choice r\in\{0,1\}^{r_{\text{out}}} of \mathcal{V}_{\text{out}}’s randomness, the composed prover computes \mathcal{V}_{\text{out}}’s query set Q(r) and an “inner proof” \pi_{r}\ {:=}\ \mathcal{P}_{\text{in}}(\mathcal{V}_{\text{out}},\pi_{\text{out% }}|_{Q(r)}), which attests to the fact that if \mathcal{V}_{\text{out}} performed its verification of \pi_{\text{out}} using randomness r, then it would have accepted. Each inner proof is a function of at most \ell_{\text{out}} many locations of \pi_{\text{out}}, where \ell_{\text{out}} is the query complexity of \mathcal{V}_{\text{out}}. The composed proof is given by (\pi_{\text{out}},(\pi_{r})_{r\in\{0,1\}^{r_{\text{out}}}}). Hence the composed proof is \ell_{\text{out}}-locally computable from \pi_{\text{out}}, by the following function: f^{\pi_{\text{out}}}(\mathcal{O},i)=\begin{cases}\pi_{\text{out}}(i)~{}&\text{% if}~{}\mathcal{O}=\pi_{\text{out}}\\ \mathcal{P}_{\text{in}}(\mathcal{V}_{\text{out}},\pi_{\text{out}}|_{Q(r)})_{i}% ~{}&\text{if}~{}\mathcal{O}=\pi_{r},~{}\text{for some}~{}r\in\{0,1\}^{r_{\text% {out}}}\end{cases}. Note that the composed PCP is locally computable from the outer PCP, so only the outer PCP needs to be zero knowledge to ensure the composed PCP is zero knowledge. Therefore we can employ the existing (non-ZK) PCP of proximity for circuit evaluation of [BenSassonGHSV06] as the inner PCPP. 1.2 Open problems This work shows that for any polynomial q^{*}, any language in NP has a polynomial-sized proof that can be probabilistically checked by probing only O(1) bits, but where any set of q^{*} bits carries no information about the witness. This can be viewed a zero-knowledge PCP theorem that matches parameters of the original PCP theorem [AroraS98, ALMSS92]. Since then, stronger versions of the PCP theorem have been shown. It is tempting to ask whether zero-knowledge PCPs can match the strongest constructions of standard PCPs. In particular, one of the most immediate open questions is whether it is possible to obtain ZK-PCPs with nearly-linear length. Optimising the proof length of PCPs received much attention for decades after the first proof of the PCP theorem, where the current state of the art achieves quasilinear proof length [BS08, Din07]. We ask whether the same can be obtained for zero-knowledge PCPs. Open Problem 1 (Short ZK-PCPs). Do there exist O(1)-query ZK-PCPs for \mathsf{NP} with proof length \tilde{O}(n)? We remark that our algebraic zero-knowledge techniques are based on Reed-Muller arithmetisation and the sumcheck protocol, whereas (non-ZK) constructions of quasilinear length PCPs are based on Reed-Solomon arithmetisation and combinatorial gap amplification, hence new ideas are necessary for such strengthening of our theorem. Even more ambitiously, one could ask whether it is possible to transform any construction of a (non-ZK) PCP to a ZK-PCP while preserving its parameters. Open Problem 2 (PCP to ZK-PCP transformation). Is there a black-box transformation that imbues a PCP construction with zero knowledge? We note that prior to [GurOS2024], all works on ZK-PCPs (see below) followed this approach, but those transformations either introduce adaptivity or only achieve weak ZK guarantees, and none preserve query complexity. Similiar transformations are known, e.g., for multi-prover interactive proofs [BenOrGKW88] and their quantum analogues [GriloSY19, MastelS24] whereas in other models, such as in zero-knowledge streaming interactive proofs [cormode2023streaming], we have a zero knowledge sumcheck protocol but no generic transformation is known. We remark that our techniques make whitebox use of the structure of the [BabaiFLS91] PCP, and rely strongly on the [GurOS2024] ZK-PCP for sumcheck; hence, obtaining a generic transformation would require new ideas. The quantum PCP conjecture. Finally, we highlight a connection between zero-knowledge PCPs and one of the most imporant open problems in quantum complexity theory: the quantum PCP (QPCP) conjecture. Most classical constructions of PCPs rely on an encoding of the \mathsf{NP} witness via a locally-testable and (relaxed) locally-decodable code [BenSassonGHSV06, gur2020relaxed]. Even though there is growing evidence that good quantum LTCs may exist [aharonov2015quantum, leverrier2022towards, anshu2023nlts, dinur2024expansion], quantum codes cannot be locally decodable due to the no-cloning theorem. This is one of the main barriers towards applying algebraic and coding-theoretic techniques to QPCPs. Quantum codes are fundamentally tied to zero knowledge. It is a well-known fact that the erasure of a subset of qubits of a codeword is correctable if and only if the reduced density matrix on that subset is independent of the encoded state. In other words, roughly speaking, a quantum code has good distance if and only if it satisfies a quantum analogue of the PZK property for PCPs. Thus PZK-PCPs are perhaps the closest classical analogue of QPCPs, and studying them may help to shed light on the QPCP conjecture. 1.3 Related work The first zero-knowledge PCPs appeared in the work of Kilian, Petrank and Tardos [KilianPT97]. Later works [IshaiMS12, IshaiSVW13, IshaiW14, IshaiMSX15] simplified this construction, and extended it to PCPs of proximity and the closely related notion of zero-knowledge locally testable codes (LTCs). These constructions rely on an adaptive honest verifier, and hence it is unclear how to use proof composition to improve their parameters. [IshaiMS12, IshaiMSX15] showed that PCPs which are zero knowledge against any efficient adversary and where the proof oracle is described by a polynomial-sized circuit exist only for languages in \mathsf{SZK}. Another line of work, motivated by cryptographic applications, focuses on obtaining SZK-PCPs for NP with a non-adaptive honest verifier from leakage resilience. These results come with caveats, achieving either a weaker notion of zero knowledge known as witness indistinguishability [IshaiWY16], or simulation against adversaries making only quadratically many more queries than the honest verifier [HazayVW22]. See [Weiss22] for a survey of this line of work. In related models that allow for interaction, zero-knowledge proofs are easier to construct. We know that \mathsf{PZK\text{-}MIP}=\mathsf{MIP} (=\mathsf{NEXP}) [BenOrGKW88], where \mathsf{MIP} is the class of languages with a multi-prover interactive proofs. The quantum analogue of this result, \mathsf{PZK\text{-}MIP}^{*}=\mathsf{MIP}^{*} (=\mathsf{RE}), is also known to hold [ChiesaFGS22, GriloSY19, MastelS24]. The constructions in this work draw inspiration from a similar result for interactive PCPs (IPCPs) [KalaiR08], an interactive generalisation of PCPs (and special case of IOPs): \mathsf{PZK\text{-}IPCP}=\mathsf{IPCP}=\mathsf{NEXP} [BenSassonCFGRS17, ChiesaFS17]."
https://arxiv.org/html/2411.07966v1,"Feasibly Constructive Proof of Schwartz-Zippel Lemma 
and the Complexity of Finding Hitting Sets","The Schwartz-Zippel Lemma states that if a low-degree multivariate polynomial with coefficients in a field is not zero everywhere in the field, then it has few roots on every finite subcube of the field. This fundamental fact about multivariate polynomials has found many applications in algorithms, complexity theory, coding theory, and combinatorics. We give a new proof of the lemma that offers some advantages over the standard proof.First, the new proof is more constructive than previously known proofs. For every given side-length of the cube, the proof constructs a polynomial-time computable and polynomial-time invertible surjection onto the set of roots in the cube. The domain of the surjection is tight, thus showing that the set of roots on the cube can be compressed. Second, the new proof can be formalised in Buss’ bounded arithmetic theory {\mathsf{S^{1}_{2}}} for polynomial-time reasoning. One consequence of this is that the theory {\mathsf{S^{1}_{2}}}+\mathsf{dWPHP}({\mathsf{PV}}) for approximate counting can prove that the problem of verifying polynomial identities (PIT) can be solved by polynomial-size circuits. The same theory can also prove the existence of small hitting sets for any explicitly described class of polynomials of polynomial degree.To complete the picture we show that the existence of such hitting sets is equivalent to the surjective weak pigeonhole principle \mathsf{dWPHP}({\mathsf{PV}}), over the theory \mathsf{S^{1}_{2}}. This is a contribution to a line of research studying the reverse mathematics of computational complexity (cf. Chen-Li-Oliveira, FOCS’24). One consequence of this is that the problem of constructing small hitting sets for such classes is complete for the class APEPP of explicit construction problems whose totality follows from the probabilistic method (Kleinberg-Korten-Mitropolsky-Papadimitriou, ITCS’21; cf. Korten, FOCS’21). This class is also known and studied as the class of Range Avoidance Problems (Ren-Santhanam-Wang, FOCS’22).","We study constructive proofs of the well-known Schwartz-Zippel Lemma, and their applications in complexity theory. This statement is sometimes referred to as the “Schwartz-Zippel-DeMillo-Lipton Lemma” following [47, 42, 15]. However, this result goes back to Øystein Ore, 1922 [34] (over finite fields) and was subsequently rediscovered by other authors; see [3] who unearthed the history of this lemma. Due to this complicated history, it is sometimes called “The Polynomial Identity Lemma”. Theorem 1.1 (Schwartz-Zippel Lemma). Let \mathbb{F} be a field, let \overline{x} be a set of n indeterminates, let S\subseteq\mathbb{F} be a finite subset of field elements, and let P(\overline{x}) be a multivariate polynomial in the indeterminates \overline{x} with coefficients in the field \mathbb{F} and maximum individual degree at most d. Then, either every point in \mathbb{F}^{n} is a root of P(\overline{x}), or the number of roots in S^{n} is at most d\cdot n\cdot|S|^{n-1}. In particular, the number of roots in S^{n} is either |S|^{n} or at most d\cdot n\cdot|S|^{n-1}.111Our version of the lemma is for maximum individual degree and is closely related to Zippel’s version of the lemma in [47]. Zippel’s bound, also for maximum individual degree, is slightly tighter than the one stated here, namely: |S|^{n}\cdot(1-(1-d/|S|)^{n})\leq d\cdot n\cdot|S|^{n-1}. There is also a better known version of the lemma for total degree D, where the bound is D\cdot|S|^{n-1}. Note that d\leq D\leq d\cdot n, so the bound for total degree is never further than a factor of n from our bound. In the regime where |S| is bigger than n and d, which is the setting of most applications, all bounds are equally useful and yield similar conclusions. See [30] and the discussion there, for a comparison of the bounds, and for a discussion on how tight they are. The Schwartz-Zippel Lemma is a cornerstone in randomised algorithms and the use of randomness in computing, with a wide range of applications in computational complexity, coding theory, graph algorithms, and algebraic computation. The lemma shows that evaluating a non-zero polynomial on inputs chosen randomly from a large enough set is likely to find an input that produces a non-zero result. This offers a fast test with good guarantees for checking if a polynomial is identically zero. While the lemma provides significant efficiency advantages in its applications, it is based on an existential statement—the existence of many non-roots for non-zero polynomials—without offering a deterministic way to find these non-roots or easily witness their absence. Accordingly, the standard textbook proof of the lemma, which goes by induction on the number of variables, although simple, does not reveal a feasible constructive argument. Specifically, it treats multivariate polynomials as potentially exponential sums of monomials. This non-constructive nature of the lemma is one reason why providing feasible constructive proofs has been challenging (cf. [28]). Although different in nature, feasible constructivity in proofs and algorithms often go hand in hand, either informally or, at times, formally through translation theorems between proofs and computation. This work presents a new proof of the Schwartz-Zippel Lemma that fits within the framework of feasibly constructive proofs in a precise manner. We then demonstrate several applications of this proof, both in feasible constructive mathematics and in computational complexity, as we explain in the following. Organisation of the introduction. In Section 1.1 we discuss the motivation behind seeking feasibly constructive proofs in general, and specifically the approach taken here to carry out the new proof within the formal logic framework of bounded arithmetic, which are formal theories accompanying and complementing the development of complexity theory. In Section 1.2 we describe (in the “meta-theory”) the new feasibly constructive proof of Schwartz-Zippel. For those interested to first see the new proof, it is presented at the end of Section 1.2. We precede it with an exposition that aims to explain the intuition behind the new proof. In Section 1.3 we discuss applications of the new proof to bounded arithmetic, where our new argument helps to settle the open problem of formalising the Schwartz-Zippel Lemma in bounded arithmetic. Specifically, in Section 1.3.2 we discuss how to prove the existence of small hitting sets and hence formalise Polynomial Identity Testing (PIT) in the theory. In Section 1.3.3 we describe a “reversal” theorem, in which the dual weak pigeonhole principle—namely, the statement that a function cannot map surjectively a small domain onto a large range—is shown to be equivalent to the existence of small hitting sets. And in Section 1.3.4 we show that this reversal theorem implies that finding hitting sets is complete for the class of Range Avoidance Problems (aka {\mathsf{APEPP}}). 1.1 The Quest for Feasibly Constructive Proofs While existence proofs, particularly those establishing the presence of certain combinatorial objects, are very useful—for instance, in probabilistic combinatorics, where one seeks to identify objects with certain properties that are as large or small as possible, such as expanders or combinatorial designs—it is widely acknowledged that constructive arguments, which provide explicit methods for constructing these objects, are often even more fruitful. This is especially significant in algorithmic contexts, where the utility of the objects (e.g., expanders) depends on their “explicitness”, meaning that they must be feasibly computable, typically in polynomial time. In computational complexity theory, the notions of constructivity and explicitness are equally critical. Fundamental questions about separating complexity classes hinge on explicit languages, such as the satisfiability problem (SAT), and whether SAT can be solved by polynomial-size circuits. For random, non-explicit languages, the analogous problems are almost trivially resolved: a simple counting argument shows that there exist non-explicit Boolean functions that cannot be computed by Boolean circuits of polynomial-size. A related but distinct approach to feasible constructivity in complexity theory is found in the framework of bounded arithmetic, which is the field that studies the computational complexity of concepts needed to prove different statements. In this setting, one aims to formalise constructivity in a more rigorous and systematic manner. Bounded arithmetic is a general name for a family of weak formal theories of arithmetic (that is, natural numbers, and whose intended model is \mathbb{N}). These theories are characterized by their axioms and language (set of symbols), starting from a basic set of axioms providing the elementary properties of numbers. Each bounded arithmetic theory possesses different additional axioms postulating the existence of different sets of numbers, or different kinds of induction principles. Based on its specific axioms, each theory of bounded arithmetic proves the totality of functions from different complexity classes (e.g., polynomial-time functions). We can typically consider such theories as working over a logical language that contains the function symbols of that prescribed complexity class. In this sense, proofs in the theory use concepts from a specific complexity class, and we can say that the theory captures “reasoning in this class” (e.g., “polynomial-time reasoning”). In the current work we shall start with a standard naturally written constructive proof (in the “meta-theory”) of the Schwartz-Zippel Lemma (Section 1.2), following the first, less formal approach, to constructivity. We then show how the new proof fits in the formal approach to feasible constructivity of bounded arithmetic. We moreover exemplify the usual benefits of such proofs by showing applications both in bounded arithmetic and computational complexity. Background on theories of bounded arithmetic, their utility and applications. While the first theory for polynomial-time reasoning was the equational theory \mathsf{PV} considered by Cook [14], bounded arithmetic goes back to the work of Parikh [35] and Paris-Wilkie [36]. In a seminal work, Buss [8] introduced other theories of bounded arithmetic and laid much of the foundation for future work in the field. Using formal theories of bounded arithmetic is important for several reasons. First, it provides a rigorous framework to ask questions about provability, independence, and the limits of what can be proved by which means and arguments. The quest for “barriers” in computational complexity—namely, the idea that some forms of arguments are futile as a way to solve the fundamental hardness problems in complexity—such as Relativisation by Baker, Gill and Solovay [6], Natural Proofs by Razborov and Rudich [40] or Algebrisation by Aaronson and Wigderson [1], has played an important role in complexity theory. Nevertheless, the language of formal logic provides a more systematic framework for such a project (cf. [2, 19] and the recent work [11]). In that sense, bounded arithmetic allows us to identify suitable logical theories capable of formalising most contemporary complexity theory, and determine whether the major fundamental open problems in the field about lower bounds are provable or unprovable within these theories. Furthermore, bounded arithmetic serves as a framework in which the bounded reverse mathematics program is developed (in an analogy to Friedman and Simpson reverse mathematics program [44]). In this program, one seeks to find the weakest theory capable of proving a given theorem. In other words, we seek to discover what are the axioms that are not only sufficient to prove a certain theorem, but rather are also necessary. Special theorems of interest are those of computer science and computational complexity theory. The motivation is to shed light on the role of complexity classes in proofs, in the hope to delineate, for example, those concepts that are needed to progress on major problems in computational complexity from those that are not. For instance, it has been identified that apparently most results in contemporary computational complexity can be proved using polynomial-time concepts (e.g., in \mathsf{PV}) (cf. [38]), and it is important to understand whether stronger theories and concepts are needed to prove certain results. Accordingly, recent results in bounded arithmetic [11] seek to systematically build a bounded reverse mathematics of complexity lower bounds, in particular, as these are perhaps the most fundamental questions in complexity. This serves to establish complexity lower bounds as “fundamental mathematical axioms” which are equivalent, over the base theory, to rudimentary combinatorial principles such as the pigeonhole principle or related principles. Indeed, we will show that the (upper bound) statement about the existence of small hitting sets is equivalent to the (dual weak) pigeonhole principle. (It is interesting to note that the existence of explicit small hitting sets, which implies efficient PIT, is also a disguised lower bound statement as shown by Kabanets and Impagliazzo [23]; though we do not attempt to formalise or pursue this direction in the current work.) Another advantage of bounded arithmetic comes in the form of “witnessing” theorems. These are results that automatically convert formal proofs (of low logical-complexity theorems, namely, few quantifier alternations) in bounded arithmetic to feasible algorithms (usually, deterministic polynomial-time ones). Witnessing theorems come in different flavours and strength, and recent work show the advantage of this method in both lower and upper bounds [10, 18, 29]. Moreover, the somewhat forbidding framework of bounded arithmetic forces one to think algorithmically from the get go, optimising constructions. This resulted in new arguments to existing results, which proved very useful in complexity, beyond the scope of bounded arithmetic. A celebrated example is Razborov’s new coding argument of the Håstad’s switching lemmas [17], which emerged as work in bounded arithmetic [39]. (Intriguingly, our new argument for Schwartz-Zippel will also be based on a coding argument.) Randomness and feasibly constructive proofs. One central part of complexity that was challenging to fit into bounded arithmetic is that of random algorithms. Randomness usually entails thinking of probability spaces of exponential size (e.g., the outcome of n coin flips), and so cannot be directly used in most bounded arithmetic theories which cannot state the existence of exponential size sets. Initial work by Wilkie (unpublished) [26, Theorem 7.3.7], taken as well by Thapen [45], and systematically developed in a series of works of Jeřábek (cf. [21]), concerned how to work with randomness in bounded arithmetic. Nevertheless, one of the most well-known examples for using randomness in computing, the question of formulating Schwartz-Zippel and polynomial identity testing in particular, was left open due to the exponential nature of the standard argument (i.e., the need to treat polynomials as sums of exponential many monomials; see the first paragraph in Section 1.2). For example, Lê and Cook in [28] list this as an open question (where they settled for formalising a special case of the Schwartz-Zippel Lemma). The formalisation of Schwartz-Zippel Lemma and PIT in bounded arithmetic and its consequences we present, hopefully exemplify several of the benefits of bounded arithmetic described above. It serves to fill in the missing link in the formalisation of complexity of randomness; it produces a new (coding) argument of Schwartz-Zippel lemma that may be of independent interest; it establishes the existence of hitting sets as equivalent to the dual weak pigeonhole principle, and thus provides further examples of the “axiomatic” nature of building blocks in complexity theory; lastly, it has consequences to computational complexity, by showing that hitting sets are complete for the class of range avoidance problems. 1.2 New Proof of the Schwartz-Zippel Lemma The standard Schwartz-Zippel proof proceeds by induction on the number of variables n but is not a feasibly constructive proof. It is non-constructive in the sense that each inductive step involves stating properties of objects that are of exponential size. For example, in step i of the induction, the proof states that the current polynomial on n-i+1 variables can be rewritten into a univariate polynomial in the last variable, with coefficients taken from the ring of polynomials in the first n-i variables. The non-constructive character of the proof stems then from the fact that there is no (known) efficient way of iterating this rewriting n times, unless the polynomial is given in explicit sum of monomials form. Note, however, that the sum of monomials form is typically of exponential size. Moshkovitz [32] provided an alternative proof of the Schwartz-Zippel Lemma, but only for finite fields. The proof in [32] does not use explicit induction, but it is anyway unclear how to make it strictly constructive in our sense, namely how to identify polynomial-time algorithms for the concepts used in the proof, and then use these to formalise the whole argument in a relatively weak theory. We refer to Moshkovitz’ proof again later in this section to compare it with our approach. Towards our new proof. For the sake of exposition, let us begin with two natural but failed attempts to a new proof. In what follows, let P(\overline{x}) be a polynomial over the field \mathbb{F}, with n variables and maximum individual degree d, and let {\overline{a}}\in\mathbb{F}^{n} be a given non-root; P({\overline{a}})\neq 0. Let S\subseteq\mathbb{F} be a finite subset of the field. In the first attempt, we try to cover the cube S^{n} with at most n\cdot|S|^{n-1} lines, each emanating from the non-root {\overline{a}}. These lines are the subsets of \mathbb{F}^{n} of the form \{{\overline{a}}+t\cdot{\overline{b}}:t\in\mathbb{F}\} for some {\overline{b}}\in\mathbb{F}^{n}. For each such line L, note that P retricted to L, defined as P_{L}(t):=P({\overline{a}}+t\cdot{\overline{b}}), is a non-zero univariate polynomial of degree at most d. It is non-zero because it evaluates to P({\overline{a}})\not=0 at t=0, and it has degree at most d because it is a linear restriction of P. Therefore, by the fundamental theorem for (univariate) polynomials, each such line has at most d roots, and since the lines cover S^{n}, we count at most d\cdot n\cdot|S|^{n-1} roots in S^{n} in total, and we are done. The problem with this approach is that it is not always possible to cover |S|^{n} with at most n\cdot|S|^{n-1} lines emanating from a single point {\overline{a}}. A simple counterexample can be found already at dimension n=2 with S=\{0,1,\ldots,q-1\} and {\overline{a}}=(0,0): the 2q-1 points of the form (i,q-1) or (q-1,i) with i\in S require each its own line emanating from the origin, and the remaining q^{2}-2q-2 points cannot be covered with one more line. In the second attempt, we want to cover the cube S^{n} again with at most n\cdot|S|^{n-1} lines, but now we try with parallel lines. For example we could consider the set of axis-parallel lines of the form \{(c_{1},\ldots,c_{k-1},t,c_{k+1},\ldots,c_{n}):t\in\mathbb{F}\} with c_{j}\in S for all j\not=k, for some fixed k=1,\ldots,n. The problem with this approach now is that it is not clear that all such lines will go through some non-root of P. It is tempting to consider some sets of parallel lines that are more related to the given non-root {\overline{a}}, such as the set of lines of the form \{{\overline{b}}+t\cdot{\overline{a}}:t\in\mathbb{F}\} whose gradient is {\overline{a}}. This is indeed the approach taken by Moshkovitz in [32], but as far as we can see this does not work for arbitrary non-roots {\overline{a}}, and works only for a specific kind of non-roots that seem hard to find in the first place. Our approach. We are now ready to explain the two new ideas that we use in our proof, and how they overcome the obstacles of the previous two attempts. First, instead of covering the roots in S^{n} with n\cdot|S|^{n-1} lines where the polynomial is non-zero, we are going to encode each root in S^{n} with one in n\cdot|S|^{n-1} such lines, together with an additional number i in 1,\ldots,d. The lines we use to encode the roots are not necessarily pair-wise parallel, though each line will be parallel to one of the axes. Second, to actually find this line, our proof uses a hybrid-type argument. Concretely, to encode the root {\overline{c}}, we start at a line through {\overline{a}} and end at a line through {\overline{c}}. Along the way, the process travels across at most n axis-parallel lines of the cube S^{n}, changing the dimension of travel at each step. The hybrid-argument is used to preserve the property that the restriction of P to the current line is still a non-zero polynomial. The exact details of how this is done are explained below. New proof of Schwartz-Zippel lemma: Let {\overline{a}}=(a_{1},\ldots,a_{n})\in\mathbb{F}^{n} be such that P({\overline{a}})\not=0, and let S be a finite subset of the field \mathbb{F}. Each vector {\overline{c}}=(c_{1},\ldots,c_{n})\in S^{n} of field elements in S can be encoded with n numbers, each from 1 to |S|, by identifying each c_{i} with its position in an arbitrary ordering of the finite set S. Our goal is to encode the roots of P in S^{n} with shorter codewords. To achieve this we will use only the fact that we know a non-root {\overline{a}}\in\mathbb{F}^{n}. Let {\overline{c}}=(c_{1},\ldots,c_{n})\in S^{n} be such that P({\overline{c}})=0. Find the minimal index k between 1 and n such that P(c_{1},\dots,c_{k-1},a_{k},a_{k+1},\dots,a_{n})\neq 0 while P(c_{1},\dots,c_{k-1},c_{k},a_{k+1},\dots,a_{n})=0. Such a k must exist since by assumption P({\overline{a}})\not=0 and P({\overline{c}})=0. Observe that, if we are given both {\overline{a}} and {\overline{c}}, then finding k is easy by looping through the coordinates from left to right. The argument hinges on the following observation: Knowing this k allows us to encode, given c_{1},\dots,c_{k-1},c_{k+1},\dots,c_{n}, the field element c_{k}\in S by a single number i between 1 and d. Therefore, we can use i, together with k and the positions of c_{1},\ldots,c_{k-1},c_{k+1},\ldots,c_{n} in the fixed ordering of S, as a code for {\overline{c}}. This shows that the set of roots in S^{n} can be encoded using only d\cdot n\cdot|S|^{n-1} numbers instead of the trivial |S|^{n} bound, concluding the argument. In detail, consider the root {\overline{c}}=(c_{1},\dots,c_{k-1},c_{k},c_{k+1}\dots,c_{n}), where k is the minimal index as above. We encode {\overline{c}} by the n-1 elements c_{1},\dots,c_{k-1},c_{k+1},\dots,c_{n} from S, together with k and a second index i such that c_{k}\in S is the i-th root in S of the univariate polynomial Q(t)=P(c_{1},\dots,c_{k-1},t,a_{k+1},\dots,a_{n}). This encoding works because given the numbers k and i, and the elements (c_{1},\dots,c_{k-1},c_{k+1}\dots,c_{n}) we can recover {\overline{c}}. Indeed, by the choice of k and i, the univariate polynomial Q(t) is non-zero and has degree at most d, which means that it has no more than d roots in \mathbb{F}. By looping through S we can find its i-th root in S which, by construction, is c_{k}. The fact that each root of P in S^{n} is coded by n-1 of its components and the two positive integers k and i finishes the proof as it means that the total number of such roots is bounded above by the number of all possible such codes: d\cdot n\cdot|S|^{n-1}. (0,0,0) Illustration of the encoding process. Refer to the cube on the right. The x,y,z axes are represented by the vertical dimension, and the two horizontal dimensions, respectively. The given non-root is the red dot \overline{a}=(1,0,0). The root to encode is the blue dot \overline{c}=(3,2,1). The process starts at the red line (t,0,0) through \overline{a}, parallel to the x axis. Replacing the first component a_{1} by c_{1}, we check if P(c_{1},a_{2},a_{3})=0. If not, we travel along the red line for c_{1}-a_{1}=2 units to reach the green line (3,t,0), parallel to the y axis. Next we check if P(c_{1},c_{2},a_{3})=0. If not, we travel along the green line for c_{2}-a_{2}=2 units to reach the blue line (3,2,t), parallel to the z axis. We test now if P(c_{1},c_{2},c_{3})=0, which checks, because {\overline{c}} is a root. The journey ends here. In this example it took us k=3 steps to reach the root. Note that this was the first k in 1,\ldots,n that caused P to vanish on the hybrid (c_{1},\ldots,c_{k-1},c_{k},a_{k+1},\ldots,a_{n}). The blue line, call it L, is the one we use to encode the root {\overline{c}}. To encode it, we use the index of c_{k} as a root of the univariate polynomial P_{L}(t) obtained by restricting P to this line. If this index is i in 1,\ldots,d, then we encode the root \overline{c} by (i,k,(c_{1},\ldots,c_{k-1},c_{k+1},\ldots,c_{n})). In the example, if the index i happens to be 1, then we use (1,3,(3,2)). Note that this encoding depends on the given non-root \overline{a}, but any non-root serves the purpose of encoding all roots in S^{n}. 1.3 Applications 1.3.1 Schwartz-Zippel Lemma in the Theory We begin with a short informal description of the theories, language and axioms we shall use. \mathsf{PV}: This is the language with a function symbol for every polynomial-time function, with its meaning specified by the equations that define it via Cobham’s bounded recursion on notation. \mathsf{S^{1}_{2}}: The first level of Buss’ family of theories [8] for basic number theory whose definable functions are precisely the polynomial-time functions. It contains basic axioms for properties of numbers (e.g., associativity of product), together with a polynomial induction axiom for \mathsf{NP}-predicates. The extension of {\mathsf{S^{1}_{2}}} with all {\mathsf{PV}}-symbols and the Cobham equations as axioms is denoted by {\mathsf{S^{1}_{2}}}({\mathsf{PV}}). The theory has the same theorems as {\mathsf{S^{1}_{2}}} in the base language (see [8]), and it is customary to abuse notation and still call it {\mathsf{S^{1}_{2}}} instead of the heavier {\mathsf{S^{1}_{2}}}({\mathsf{PV}}). \mathsf{dWPHP}(\mathsf{PV}): The set of dual weak pigeonhole principle axioms \mathsf{dWPHP}(f), for every polynomial-time function symbol f\in{\mathsf{PV}}. This axiom states the simple counting principle that a function f cannot map surjectively a domain of size N to a range of size 2N or more; namely, there is a point in the set of size 2N that is not covered by f. Wilkie (unpublished; see [26, Theorem 7.3.7]) observed the connection between this principle and randomness in computation (within bounded arithmetic): roughly speaking, when f has a small domain but much larger co-domain, with high probability a point in the co-domain will not be covered by f. Hence, the ability to pick such a point is akin to witnessing this probabilistic argument. \mathsf{S^{1}_{2}}+\mathsf{dWPHP}(\mathsf{PV}): \mathsf{S^{1}_{2}} (indeed {\mathsf{S^{1}_{2}}}({\mathsf{PV}}); see above), augmented with the axioms \mathsf{dWPHP}(f) for every polynomial-time function symbol f\in{\mathsf{PV}}. This is a theory that can serve as a basis for probabilistic reasoning (close to Jeřábek’s theory for approximate counting [21]; cf. [20]). With this notation we can now state the form of Schwartz-Zippel Lemma that we prove. Here, and in the rest of this introduction, let [q] denote the set \{1,\ldots,q\}. Theorem 1.2 (Schwartz-Zippel Lemma in \mathsf{S^{1}_{2}}; informal, see Theorem 3.9). Let P be a polynomial of degree d, given as an algebraic circuit, with integer coefficients and n variables. Then, either P is zero everywhere on \mathbb{Z}, or for every positive integer q there is a (polynomial-time) function f that given any non-root {\overline{a}}=(a_{1},\ldots,a_{n})\in\mathbb{Z}^{n} with P({\overline{a}})\not=0, returns a function f({\overline{a}}):{\sf codes}\to{\sf roots} that maps the set of codes surjectively onto the set of roots in the cube [q]^{n}, and |{\sf codes}|\leq d\cdot n\cdot q^{n-1}. Here the codes and the function f are defined according to the encoding scheme in Section 1.2. Since given a non-root {\overline{a}} the function f({\overline{a}}) is (provably) surjective onto the roots in the cube [q]^{n}, the number of roots of P in the cube is at most the number of codes, or |{\sf codes}|\leq d\cdot n\cdot q^{n-1}. To actually reason in the theory about the size of exponential-size sets like {\sf codes} we could have chosen to invoke approximate counting (based on Jeřábek’s theories [21], which would require the inclusion of the dual weak pigeonhole principle \mathsf{dWPHP}). However, we opt not to do this for the Schwartz-Zippel Lemma. Rather, we will show that this formulation of the Schwartz-Zippel Lemma, together with the \mathsf{dWPHP}(\mathsf{PV}) axiom, suffices to apply the lemma in its standard applications, such as PIT and finding small hitting sets (see below). En route to the proof of Theorem 1.2, we prove in the theory {\mathsf{S^{1}_{2}}} one half of the Fundamental Theorem of Algebra (FTA). This is the fundamental theorem of univariate polynomials stating that every non-zero polynomial of degree d with complex coefficients has exactly d complex roots. The theorem naturally splits into two halves: the half that states that there are at least d roots, and the half that states that there are at most d roots. While the at least statement relies on special properties of the complex numbers, the proof of the at most statement relies only on the fact that univariate polynomials over a field admit Euclidean division. In particular, the at most statement holds also for polynomials over any subring of a field; e.g., the integers. Theorem 1.3 (Half of Fundamental Theorem of Algebra in \mathsf{S^{1}_{2}} informal; see Lemma 3.4). Every non-zero univariate polynomial of degree d with integer coefficients has at most d roots on (every finite subset of) \mathbb{Z}. While the underlying idea of this proof is standard, it is somewhat delicate to carry out the argument in {\mathsf{S^{1}_{2}}} because we need to keep track of the bit-complexity of the coefficients that appear along the way in the computations. It is well-known that certain widely-used algorithms working with integers or rational numbers, including Gaussian Elimination, could incur exponential blow-ups in bit-complexity if careless choices were made in their implementation; cf. [16]. We also note that Jeřábek [22] formalised Gaussian Elimination over rationals in {\mathsf{S^{1}_{2}}}, and proved also the same half of the FTA that we prove, but only for finite fields, where exponential blow ups cannot occur. 1.3.2 Existence of Hitting Sets and PIT in the Theory For a field \mathbb{F} and a set of algebraic circuits \mathscr{C} over \mathbb{F} with n variables, we say that a set H\subseteq\mathbb{F}^{n} is a hitting set for \mathscr{C} if for every non-zero polynomial P in \mathscr{C} there exists a point {\overline{a}}\in H such that P({\overline{a}})\neq 0. In other words, if P is non-zero, H ‘hits’ it. Hitting sets are important because when they are explicit and small they allow for derandomization of PIT: running through the full hitting set one can test if a given algebraic circuit is the zero polynomial or not. By Theorem 1.2, the theory {\mathsf{S^{1}_{2}}} proves (by means of a surjective map) that every non-zero n-variable algebraic circuit with small degree d has relatively few roots in [q]^{n}. By a counting argument (or the union bound, cf. [43, Theorem 4.1]), it follows that for any given bounds d and 2^{m} on the degree and the number of circuits in the class \mathscr{C}, there is a set H\subseteq[q]^{n} of {\mathsf{poly}}(n,d,m) points, with q={\mathsf{poly}}(n,d), that intersects the set of non-roots of every non-zero circuit in the class. This set H is thus a hitting set of polynomial size, and we say it is a hitting set for \mathscr{C} over [q]. We show that this counting argument can now be formalised in the theory {\mathsf{S^{1}_{2}}}+\mathsf{dWPHP}({\mathsf{PV}}). Theorem 1.4 (Small Hitting Sets Exist in {\mathsf{S^{1}_{2}}}+\mathsf{dWPHP}({\mathsf{PV}}); informal, see Theorem 4.4). For every class \mathscr{C} of algebraic circuits with integer coefficients that is definable in the theory, with n variables, polynomial degree, and polynomial size, there exists a polynomial-size hitting set for \mathscr{C} over [q] with q={\mathsf{poly}}(n). The argument in the theory makes two uses of the axiom \mathsf{dWPHP} and is roughly as follows. We begin by showing that, if q is sufficiently large but polynomial, then a non-zero polynomial with n variables and polynomial degree always has non-root {\overline{a}} in [q]^{n}. To see this, recall the function f({\overline{a}}):{\sf codes}\to{\sf roots} from Theorem 1.2, that given a polynomial P and a non-root {\overline{a}} of P, surjectively maps all codes of roots to the roots of P. Note that the set {\sf roots} is a subset of [q]^{n}, which has size q^{n}, and recall that the set {\sf codes} has size at most d\cdot n\cdot q^{n-1}, where d is the degree of P. Thus, when q\geq 2dn, the \mathsf{dWPHP} axiom applies to show that there exists a point {\overline{a}}_{0} in [q]^{n} that is not in the range {\sf root} of f({\overline{a}}). This {\overline{a}}_{0} is thus a non-root of P in the set [q]^{n}, like we wanted. Next we show how to use this fact to get a hitting set with a second application of the \mathsf{dWPHP} axiom. Let \mathscr{C} be a class of algebraic circuits with n variables, syntactic degree at most d, and size at most s. Consider the function \begin{array}[]{llllll}g&:&\mathscr{C}\times[q]^{n}\times\textsf{codes}^{r}&% \to&\textsf{roots}^{r}\\ &&(P,{\overline{a}},{\overline{c}}_{1},\ldots,{\overline{c}}_{r})&\mapsto&(f({% \overline{a}})({\overline{c}}_{1}),\ldots,f({\overline{a}})({\overline{c}}_{r}% )),\end{array} (1) where {\overline{c}}_{1},\ldots,{\overline{c}}_{r} are candidate codes, each from the code-set codes of size n\cdot d\cdot q^{n-1}, and {\overline{a}} is a potential non-root of P in [q]^{n}. The parameter r should be sufficiently big, but polynomial. Then, it follows by construction and the fact proved in the previous paragraph that any point outside the range of g is a hitting set, since it will have a non-root for every algebraic circuit in \mathscr{C}. To find the point outside the range of g we invoke the \mathsf{dWPHP} axiom, using again the assumption that q\geq 2dn. One immediate consequence of Theorem 1.4 is that the theory {\mathsf{S^{1}_{2}}}+\mathsf{dWPHP}({\mathsf{PV}}) proves that the problem of verifying polynomial identities {\mathsf{PIT}} can be solved by polynomial-size Boolean circuits, so is in {\mathsf{P\text{/}poly}}. We read this as adding evidence to the claim that the theory is sufficiently powerful to prove most contemporary results in complexity theory. In particular, it adds interest to the question of proving that the major lower bound conjectures of computational complexity are consistent with {\mathsf{S^{1}_{2}}}+\mathsf{dWPHP}({\mathsf{PV}}) and stronger theories; see [27, 10, 5] for more on this line of work. 1.3.3 Contribution to Reverse Mathematics of Complexity Theory The fact that the dual weak pigeonhole principle suffices to prove the existence of small hitting sets raises a natural question: Is it also necessary? A positive answer would provide a combinatorial characterization of the algebraic statement that small hitting sets exist. It would also shed light on the role or the necessity of the probabilistic method in the usual proof of this existential statement. We show how to achieve a version of these two goals. We define a formal scheme of hitting sets axioms called {\mathsf{HS}}({\mathsf{PV}}). We follow two provisos. First, in view of the generality of Theorem 1.4, we define the axiom scheme to contain one axiom for each definable class \mathscr{C} of algebraic circuits; the axiom states that each slice \mathscr{C}_{n}, consisting of the circuits of \mathscr{C} with n variables, has small hitting sets. Second, in the definition of the axiom for \mathscr{C}, we need to decide whether to let it state the existence of hitting sets of unspecified but polynomial size, or to let it state the existence of hitting sets of some specified polynomial size. The bound established in Theorem 1.4 is actually of the form {\mathsf{poly}}(m,n) where m is the logarithm of the number of circuits in the n-th slice, and {\mathsf{poly}}(m,n) refers to a fixed polynomial of m and n. This dependence on m is common in most proofs of existence by the union bound. While the claim that hitting sets of any possibly larger but unspecified size exist would of course be also true, it turns out that asking the axiom to provide a hitting set of some fixed polynomial bound seems crucial in the proof of necessity of \mathsf{dWPHP}({\mathsf{PV}}) that we are after. We chose the latter because this is what is sufficient, and it is still natural. These two provisos motivate the following definition (informal; see Definition 5.1): \mathsf{HS}(\mathsf{PV}): The set of hitting set axioms {\mathsf{HS}}(g), for every g\in{\mathsf{PV}}. This states that if g defines a class \mathscr{C} with its n-th slice having 2^{m} algebraic circuits with n variables, polynomial degree, and polynomial size, then there is a hitting set for \mathscr{C} over [q] of size {\mathsf{poly}}(m,n), with q={\mathsf{poly}}(n). With the right definitions in place we can state the theorem that characterizes the proof-theoretic strength of the existence of small hitting sets: Theorem 1.5 (Reverse Mathematics of Hitting Sets; informal, see Theorem 5.2). The axioms schemes \mathsf{dWPHP}({\mathsf{PV}}) and {\mathsf{HS}}({\mathsf{PV}}) are provably equivalent over the theory {\mathsf{S^{1}_{2}}}. As remarked earlier, the sufficiency claim follows from Theorem 1.4. To prove the necessity we show how to use a hitting set to find a point outside the range of any given polynomial-time function f:[N]\to[2N]. To do this we design a class \mathscr{C}_{f} of N many low-degree algebraic circuits each vanishing on the appropriate representation of a point in the image of f. We do so in such a way that a hitting set for \mathscr{C}_{f} will correspond to an element in [2N]\setminus\mathrm{Img}(f), completing the proof. To make this actually work we need to use a technique known as amplification, which goes back to the work of Paris-Wilkie-Woods [37]. The same kind of technique was discovered even earlier, in cryptography, to build pseudorandom number generators from hardcore bits; see the work of Blum-Micali [7]. The details of this argument can be found in Section 5 and Appendix A. 1.3.4 Application to Computational Complexity and Range Avoidance Problem The proof-sketch we gave for Theorem 1.5 reveals a two-way connection between the computational problem of finding hitting sets and the so-called Range Avoidance Problem, or {\mathsf{Avoid}}. The latter problem asks to find a point outside the range of a given function f:[N]\to[2N]. In recent years, {\mathsf{Avoid}} has been studied with competing names. It was first studied by Kleinberg-Korten-Mitropolsky-Papadimitriou [24], calling it 1-{\mathsf{Empty}}, and later by Korten [25], renaming it {\mathsf{Empty}}. Those works defined it as the canonical complete problem for a complexity class of total search problems they called {\mathsf{APEPP}}. Ren-Santhanam-Wang [41] studied it too, calling it {\mathsf{Avoid}}, in their range avoidance problem approach to circuit lower bounds. Our new coding-based proof of the existence of hitting sets shows that its associated search problem is in {\mathsf{APEPP}}. The proof of Theorem 1.5 yields its completeness in the class: Theorem 1.6 (Completeness of Finding Hitting Sets; informal, see Theorem 1.6). The total search problem that asks to find witnesses for the hitting set axioms of the scheme {\mathsf{HS}}({\mathsf{PV}}) is {\mathsf{APEPP}}-complete under {\mathsf{P}}^{{\mathsf{NP}}}-reductions. Perhaps not too surprisingly, the proof of Theorem 1.6 is almost identical to the proof of Theorem 1.5. Indeed, the necessity for the {\mathsf{NP}}-oracle in the reductions comes (only) from the use of the amplification technique in the proof, as in earlier uses of this method; cf. [25]. A handful of complete problems for {\mathsf{APEPP}} were known before, and some required {\mathsf{P}}^{{\mathsf{NP}}}-reductions too (cf. [24, 25]). But, to our knowledge, none of these complete problems related to the problem of constructing hitting sets for algebraic circuits. Here we used the new constructive proof of the Schwartz-Zippel Lemma to find an example of this type."
https://arxiv.org/html/2411.07473v1,Nearly-Linear Time Seeded Extractors with Short Seeds,"Seeded extractors are fundamental objects in pseudorandomness and cryptography, and a deep line of work has designed polynomial-time seeded extractors with nearly-optimal parameters. However, existing constructions of seeded extractors with short seed length and large output length run in time \Omega(n\log(1/\varepsilon)) and often slower, where n is the input source length and \varepsilon is the error of the extractor. Since cryptographic applications of extractors require \varepsilon to be small, the resulting runtime makes these extractors unusable in practice.Motivated by this, we explore constructions of strong seeded extractors with short seeds computable in nearly-linear time O(n\log^{c}n), for any error \varepsilon. We show that an appropriate combination of modern condensers and classical approaches for constructing seeded extractors for high min-entropy sources yields strong extractors for n-bit sources with any min-entropy k and any target error \varepsilon with seed length d=O(\log(n/\varepsilon)) and output length m=(1-\eta)k for an arbitrarily small constant \eta>0, running in nearly-linear time, after a reasonable one-time preprocessing step (finding a primitive element of \mathds{F}_{q} with q=\operatorname{poly}(n/\varepsilon) a power of 2) that is only required when k<2^{C\log^{*}\!n}\cdot\log^{2}(n/\varepsilon), for a constant C>0 and \log^{*}\! the iterated logarithm, and which can be implemented in time \operatorname{polylog}(n/\varepsilon) under mild conditions on q. As a second contribution, we give an instantiation of Trevisan’s extractor that can be evaluated in truly linear time in the RAM model, as long as the number of output bits is at most \frac{n}{\log(1/\varepsilon)\operatorname{polylog}(n)}. Previous fast implementations of Trevisan’s extractor ran in \widetilde{O}(n) time in this setting. In particular, these extractors directly yield privacy amplification protocols with the same time complexity and output length, and communication complexity equal to their seed length.","Seeded randomness extractors are central objects in the theory of pseudorandomness. A strong (k,\varepsilon)-seeded extractor is a deterministic function \mathsf{Ext}\colon\mathopen{}\mathclose{{}\left\{{0,1}}\right\}^{n}\times% \mathopen{}\mathclose{{}\left\{{0,1}}\right\}^{d}\to\mathopen{}\mathclose{{}% \left\{{0,1}}\right\}^{m} that receives as input an n-bit source of randomness X with k bits of min-entropy111A random variable X has k bits of min-entropy if \Pr[X=x]\leq 2^{-k} for all x. Min-entropy has been the most common measure for the quality of a weak source of randomness since the work of Chor and Goldreich [CG88]. and a d-bit independent and uniformly random seed Y, and outputs an m-bit string \mathsf{Ext}(X,Y) that is \varepsilon-close in statistical distance to the uniform distribution over \mathopen{}\mathclose{{}\left\{{0,1}}\right\}^{m}, where \varepsilon is an error term, even when the seed Y is revealed. Besides their most direct application to the generation of nearly-perfect randomness from imperfect physical sources of randomness (and their inaugural applications to derandomizing space-bounded computation [NZ96] and privacy amplification [BBCM95]), seeded extractors have also found many other surprising applications throughout computer science, particularly in cryptography. For most applications, it is important to minimize the seed length of the extractor. A standard application of the probabilistic method shows the existence of strong (k,\varepsilon)-seeded extractors with seed length d=\log(n-k)+2\log(1/\varepsilon)+O(1) and output length m=k-2\log(1/\varepsilon)-O(1), and we also know that these parameters are optimal up to the O(1) terms [RT00]. This motivated a deep line of research devising explicit constructions of seeded extractors with seed length as small as possible spanning more than a decade (e.g., [NZ96, SZ99, NT99, Tre01, TZS06, SU05]) and culminating in extractors with essentially optimal seed length [LRVW03, GUV09]. In particular, the beautiful work of Guruswami, Umans, and Vadhan [GUV09] gives explicit strong extractors with order-optimal seed length d=O(\log(n/\varepsilon)) and output length m=(1-\eta)k for any constant \delta>0, and follow-up work [DKSS13, TU12] further improved the entropy loss k+d-m. The extractors constructed in these works are explicit, in the sense that there is an algorithm that given x and y computes the corresponding output \mathsf{Ext}(x,y) in time polynomial in the input length. A closer look shows that the short-seed constructions presented in the literature all run in time \Omega(n\log(1/\varepsilon)), and often significantly slower. In cryptographic applications of extractors we want the error guarantee \varepsilon to be small, which means that implementations running in time \Omega(n\log(1/\varepsilon)) are often impractical. If we insist on nearly-linear runtime for arbitrary error \varepsilon, we can use strong seeded extractors based on universal hash functions that can be implemented in O(n\log n) time (e.g., see [HT16]), have essentially optimal output length, but have the severe drawback of requiring a very large seed length d=\Omega(m). These limitations have been noted in a series of works studying concrete implementations of seeded extractors, with practical applications in quantum cryptography in mind [MPS12, FWE+23, FYEC24]. For example, Foreman, Yeung, Edgington, and Curchod [FYEC24] implement a version of Trevisan’s extractor [Tre01, RRV02] with its standard instantiation of Reed–Solomon codes concatenated with the Hadmadard code, and emphasize its excessive running time as a major reason towards non-adoption.222The reason why these works focus on Trevisan’s extractor is that this is the best seeded extractor (in terms of asymptotic seed length) that is known to be secure against quantum adversaries [DPVR12]. Instead, they have to rely on extractors based on universal hash functions, which, as mentioned above, are fast but require very large seeds. This state of affairs motivates the following question, which is the main focus of this work: Can we construct strong (k,\varepsilon)-seeded extractors with seed length d=O(\log(n/\varepsilon)) and output length m=(1-\eta)k computable in nearly-linear time, for arbitrary error \varepsilon? Progress on this problem would immediately lead to faster implementations of many cryptographic protocols that use seeded extractors. 1.1 Our Contributions We make progress on the construction of nearly-linear time extractors. Seeded extractors with order-optimal seed length and large output length. We construct nearly-linear time strong seeded extractors with order-optimal seed length and large output length for any k and \varepsilon, with the caveat that they require a one-time preprocessing step whenever k=O(\log^{2}(n/\varepsilon)). This preprocessing step corresponds to finding primitive elements of finite fields \mathds{F}_{q} with q=\operatorname{poly}(n/\varepsilon), which, as we discuss below, is reasonable in practical applications. More precisely, we have the following result. Theorem 1. For any constant \eta>0 there exists a constant C>0 such that the following holds. For any positive integers n and k\leq n and any \varepsilon>0 satisfying k\geq C\log(n/\varepsilon) there exists a strong (k,\varepsilon)-seeded extractor \mathsf{Ext}\colon\{0,1\}^{n}\times\{0,1\}^{d}\to\{0,1\}^{m} with seed length d\leq C\log(n/\varepsilon) and output length m\geq(1-\eta)k. Furthermore, • if k\geq 2^{C\log^{*}\!n}\cdot\log^{2}(n/\varepsilon), then \mathsf{Ext} is computable in time \widetilde{O}(n), where \widetilde{O}(\cdot) hides polylogarithmic factors in its argument and \log^{*}\! denotes the iterated logarithm; • if k<2^{C\log^{*}\!n}\cdot\log^{2}(n/\varepsilon), then \mathsf{Ext} is computable in time \widetilde{O}(n) after a preprocessing step, corresponding to finding a primitive element of \mathds{F}_{q} with q=\operatorname{poly}(n/\varepsilon) a power of 2.333In full rigor, the preprocessing step corresponds to finding primitive elements of O(\log\log n) fields \mathds{F}_{q} with orders q\leq\operatorname{poly}(n/\varepsilon), each a power of 2. This O(\log\log n) term has negligible influence on the complexity of this preprocessing step. Note that we can find such a primitive element in time \operatorname{polylog}(n/\varepsilon) if q\leq\operatorname{poly}(n/\varepsilon) is a power of 2 and we know the factorization of q-1, but we do not know how to do that in time \widetilde{O}(\log q). More precisely, given the factorization of q-1 we can test whether a given \alpha\in\mathds{F}_{q} is primitive in time \operatorname{polylog}(q) by checking whether \alpha^{\frac{q-1}{p}}\neq 1 for all prime factors p of q-1. We can exploit this in various ways. If we are fine with using randomness in the one-time preprocessing stage, then we can sample an element of \mathds{F}_{q} uniformly at random, test whether it is primitive, and repeat if not. If we insist on a deterministic algorithm, then we can combine the testing procedure with algorithms of Shoup [Sho90] or Shparlinski [Shp92] which identify in time \operatorname{polylog}(q) a subset of size \operatorname{polylog}(q) in \mathds{F}_{q} that is guaranteed to contain a primitive element. For an alternative faster randomized algorithm, see [DD06]. 1 follows from combining modern condensers with short seeds (namely, the lossless condenser of Kalev and Ta-Shma [KT22] and the lossy Reed-Solomon-based condenser of Guruswami, Umans, and Vadhan [GUV09]) with a careful combination and instantiation of classical recursive approaches developed by Srinivasan and Zuckerman [SZ99] and in [GUV09]. It readily implies, among other things, an \widetilde{O}(n)-time privacy amplification protocol where only O(\log(n/\varepsilon)) bits need to be communicated over the one-way authenticated public channel and almost all the min-entropy can be extracted (after a reasonable one-time preprocessing step if the min-entropy bound k is very small). A new non-recursive construction. As a conceptual contribution which may be of independent interest, we present a new “non-recursive” construction of extractors with seed length O(\log(n/\varepsilon)) and output length (1-\eta)k that is computable in nearly-linear time when k>\operatorname{polylog}(1/\varepsilon) and avoids the complicated recursive procedures from [SZ99, GUV09]. We believe this to be a conceptually better approach towards constructing seeded extractors, and we discuss it in more detail in the technical overview. Faster instantiations of Trevisan’s extractor. One of the most widely-used explicit seeded extractors is Trevisan’s extractor [Tre01, RRV02]. While by now we have extractors with better parameters, one of its main advantages is that it is one of the few examples of extractors, and in a sense the best one, which are known to be quantum proof.444An extractor is quantum proof if its output is close to uniform even in the presence of a quantum adversary that has some (bounded) correlation with X. A bit more formally, \mathsf{Ext} is quantum-proof if for all classical-quantum state \rho_{XE} (where E is a quantum state correlated with X) with H_{\infty}(X|E)\geq k, and a uniform seed Y, it holds that \rho_{\mathsf{Ext}(X,Y)YE}\approx_{\varepsilon}\rho_{U_{m}}\otimes\rho_{Y}% \otimes\rho_{E}. See [DPVR12] for more details. Trevisan’s extractor uses two basic primitives: combinatorial designs (when more than one output bit is desired), and binary list-decodable codes. A standard instantiation of such suitable codes goes by concatenating a Reed-Solomon code with a Hadamard code, and this is also what is considered in [FWE+23, FYEC24]. As they also observe, this gives a nearly-linear time construction when the output length m=1. In fact, by leveraging fast multipoint evaluation, one can also get a nearly-linear time construction for any output length m\leq\frac{n}{\log(1/\varepsilon)}, although this was not noted in previous works.555For a rigorous statement on fast multipoint evaluation, see Lemma 2.1. Our main contribution in this direction is an alternative instantiation of Trevisan’s extractor that can be computed in truly linear time on a RAM in the logarithmic cost model, for any output length m\leq\frac{n}{\log(1/\varepsilon)\cdot\operatorname{polylog}(n)}. Theorem 2. There exists an instantiation of Trevisan’s extractor, set to extract m bits with any error \varepsilon>0, that is computable in: 1. Time O(n)+m\log(1/\varepsilon)\cdot\operatorname{polylog}(n) after a preprocessing step running in time \widetilde{O}(m\log(n/\varepsilon)), on a RAM in the logarithmic cost model. In particular, there exists a universal constant c, such that whenever m\leq\frac{n}{\log(1/\varepsilon)\cdot\log^{c}(n)}, the instantiation runs in time O(n), without the need for a preprocessing step. 2. Time \widetilde{O}(n+m\log(1/\varepsilon)) in the Turing model. We note that one interesting instantiation of the above theorem is when Trevisan’s extractor is set to output k^{\Omega(1)} bits for k=n^{\Omega(1)}. In this setting, Trevisan’s extractor requires a seed of length O\mathopen{}\mathclose{{}\left(\frac{\log^{2}(n/\varepsilon)}{\log(1/% \varepsilon)}}\right), and, as long as \varepsilon is not too tiny, we get truly-linear runtime. 1.2 Other Related Work Besides the long line of work focusing on improved constructions of explicit seeded extractors and mentioned in the introduction above, other works have studied randomness extraction in a variety of restricted computational models. These include extractors computable by streaming algorithms [BRST02], local algorithms [Lu02, Vad04, BG13, CL18], AC0 circuits [GVW15, CL18, CW24], AC0 circuits with a layer of parity gates [HIV22], NC1 circuits [CW24], and low-degree polynomials [ACG+22, AGMR24, GGH+24]. Moreover, implementations in various restricted computational models of other fundamental pseudorandomness primitives such as k-wise and \varepsilon-biased generators, that often play a key role in constructions of various types of extractors, have also been independently studied (see [HV06, Hea08, CRSW13, MRRR14] for a very partial list). As mentioned briefly above, some works have also focused on constructing seeded extractors computable in time O(n\log n) motivated by applications in privacy amplification for quantum key distribution. Such constructions are based on hash functions, and are thus far restricted to \Omega(m) seed length. The work of Hayashi and Tsurumaru [HT16] presents an extensive discussion of such efforts. We also mention that nearly-linear time extractors with very short seed, in the regime k=n^{\Omega(1)} and \varepsilon=n^{-o(1)}, were given in [DMOZ22], with applications in derandomization. 1.3 Technical Overview In a nutshell, we obtain 1 by following two standard high-level steps: 1. We apply a randomness condenser with small seed length O(\log(n/\varepsilon)) to the original n-bit weak source X to obtain an output X^{\prime} that is \varepsilon-close to a high min-entropy source. 2. We apply a seeded extractor tailored to high min-entropy sources with small seed length O(\log(n/\varepsilon)) to X^{\prime} to obtain a long output that is \varepsilon-close to uniform. To realize this approach, we need to implement each of these steps in nearly-linear time \widetilde{O}(n) (possibly after a reasonable one-time preprocessing step). We briefly discuss how we achieve this, and some pitfalls we encounter along the way. Observations about nearly-linear time condensers. In order to implement Item 1, we need to use fast condensers with short seeds. Luckily for us, some existing state-of-the-art constructions of condensers already satisfy this property, although, to the best of our knowledge, this has not been observed before. We argue this carefully in Section 3.3. For example, the “lossy Reed-Solomon condenser” from [GUV09] interprets the source as a polynomial f\in\mathds{F}_{q}[x] of degree d\leq n/\log q and the seed y as an element of \mathds{F}_{q}, and outputs \mathsf{RSCond}(f,y)=(f(y),f(\zeta y),\dots,f(\zeta^{m^{\prime}}y)), for an appropriate m^{\prime} and field size q, with \zeta a primitive element of \mathds{F}_{q}. Evaluating \mathsf{RSCond}(f,y) corresponds to evaluating the same polynomial f on multiple points in \mathds{F}_{q}. This is an instance of the classical problem of multipoint evaluation in computational algebra, for which we know fast and practical algorithms (e.g., see [vzGG13, Chapter 10] or Lemma 2.1) running in time \widetilde{O}((d+m^{\prime})\log q)=\widetilde{O}(n), since d\leq n/\log q and if m^{\prime}\leq n/\log q. A downside of this condenser is that it requires knowing a primitive element \zeta of \mathds{F}_{q} with q=\operatorname{poly}(n/\varepsilon). As discussed above, if we know the factorization of q-1 and q is a power of 2, then we can find such a primitive element in time \operatorname{polylog}(q). Beyond that, having access to such primitive elements, which only need to be computed once independently of the source and seed, is reasonable in practice. Therefore, we may leave this as a one-time preprocessing step. The lossless “KT condenser” from [KT22] has a similar flavor. It interprets the source as a polynomial f\in\mathds{F}_{q}[x] and the seed y as an evaluation point, and outputs \mathsf{KTCond}(f,y)=(f(y),f^{\prime}(y),\dots,f^{(m^{\prime})}(y)), for some appropriate m^{\prime}. The problem of evaluating several derivatives of the same polynomial f on the same point y (sometimes referred to as Hermite evaluation) is closely related to the multipoint evaluation problem above, and can also be solved in time \widetilde{O}(n).666Interestingly, recent works used other useful computational properties of the KT condenser. Cheng and Wu [CW24] crucially use the fact that the KT condenser can be computed in NC1. Doron and Tell [DT23] use the fact that the KT condenser is logspace computable for applications in space-bounded derandomization. Evaluating the KT condenser does not require preprocessing. On the other hand, it only works when the min-entropy k\geq C\log^{2}(n/\varepsilon) for a large constant C>0, where n is the source length and \varepsilon the target error of the condenser. The “ideal” approach to seeded extraction from high min-entropy sources. We have seen that there are fast condensers with short seeds. It remains to realize Item 2. Because of the initial condensing step, we may essentially assume that our n-bit weak source X has min-entropy k\geq(1-\delta)n, for an arbitrarily small constant \delta>0. In this case, we would like to realize in time \widetilde{O}(n) and with overall seed length O(\log(n/\varepsilon)) what we see as the most natural approach to seeded extraction from high min-entropy sources: 1. Use a fresh short seed to transform X into a block source Z=Z_{1}\circ Z_{2}\circ\cdots\circ Z_{t} with geometrically decreasing blocks, where \circ denotes string concatenation. A block source has the property that each block Z_{i} has good min-entropy even conditioned on the values of blocks Z_{1},\dots,Z_{i-1}. 2. Perform block source extraction on Z using another fresh short seed. Due to its special structure, we can extract a long random string from Z using only the (small) seed length associated with extracting randomness from the smallest block Z_{t}, which has length O(\log(n/\varepsilon)). The classical approach to Item 2 where we iteratively apply extractors based on universal hash functions with increasing output lengths to the blocks of Z from right to left is easily seen to run in time \widetilde{O}(n) and requires a seed of length O(\log(n/\varepsilon)) if, e.g., we use the practical extractors of [TSSR11, HT16]. Therefore, we only need to worry about realizing Item 1. A standard approach to Item 1 would be to use an averaging sampler to iteratively sample subsequences of X as the successive blocks of the block source Z, following a classical strategy of Nisan and Zuckerman [NZ96] (improved by [RSW06, Vad04]). We do know averaging samplers running in time \widetilde{O}(n) (such as those based on random walks on a carefully chosen expander graph). However, this approach requires a fresh seed of length \Theta(\log(n/\varepsilon)) per block of Z. Since Z will have roughly \log n blocks, this leads to an overall seed of length \Theta(\log^{2}n+\log(1/\varepsilon)), which is too much for us. Instead, we provide a new analysis of a sampler based on bounded independence, that runs in time \widetilde{O}(n) and only requires a seed of length O(\log(n/\varepsilon)) to create the entire desired block source. We give the construction, which may be of independent interest, in Section 3.2. The caveat of this “block source creator” is that it only works as desired when the target error \varepsilon\geq 2^{-k^{c}} for some small constant c>0. Combining these realizations of Items 1 and 2 yields the desired \widetilde{O}(n)-time extractor with order-optimal seed length O(\log(n/\varepsilon)) and output length (1-\eta)n for arbitrary constant \eta>0, provided that \varepsilon\geq 2^{-k^{c}}. See Theorem 5.1 for the formal statement. Getting around the limitation of the ideal approach. We saw above that combining the ideal approach to seeded extraction from high min-entropy sources with the new analysis of the bounded independence sampler yields a conceptually simple construction with the desired properties when the error is not too small (or alternatively, whenever the entropy guarantee is large enough). However, we would like to have \widetilde{O}(n)-time seeded extraction with O(\log(n/\varepsilon)) seed length and large output length for all ranges of parameters. To get around this limitation of our first construction, it is natural to turn to other classical approaches for constructing nearly-optimal extractors for high min-entropy sources, such as those of Srinivasan and Zuckerman [SZ99] or Guruswami, Umans, and Vadhan [GUV09]. These approaches consist of intricate recursive procedures combining a variety of combinatorial objects, and require a careful analysis.777In our view, these approaches are much less conceptually appealing than the “ideal” approach above. We believe that obtaining conceptually simpler constructions of fast nearly-optimal extractors that work for all errors is a worthwhile research direction, even if one does not improve on the best existing parameters. However, we could not find such an approach that works as is, even when instantiated with \widetilde{O}(n)-time condensers and \widetilde{O}(n)-time hash-based extractors. In particular: • The GUV approach [GUV09] gives explicit seeded extractors with large output length and order-optimal seed length for any min-entropy requirement k and error \varepsilon. However, its overall runtime is significantly larger than \widetilde{O}(n) whenever \varepsilon is not extremely small (for example, \varepsilon=2^{-k^{\alpha}} for some \alpha\in(0,1/2) is not small enough). • The SZ approach [SZ99] can be made to run in time \widetilde{O}(n) and have large output length when instantiated with fast condensers, samplers, and hash-based extractors, but it is constrained to error \varepsilon\geq 2^{-ck/\log^{*}\!n}, where \log^{*} is the iterated logarithm. Fortunately, the pros and cons of the GUV and SZ approaches complement each other. Therefore, we can obtain our desired result by applying appropriately instantiated versions of the GUV and SZ approaches depending on the regime of \varepsilon we are targeting. 1.4 Future Work We list here some directions for future work: • Remove the preprocessing step that our constructions behind 1 require when k<C\log^{2}(n/\varepsilon). • On the practical side, develop concrete implementations of seeded extractors with near-optimal seed length and large output length. In particular, we think that our non-recursive construction in Section 5.1 holds promise in this direction. 1.5 Acknowledgements Part of this research was done while the authors were visiting the Simons Institute for the Theory of Computing, supported by DOE grant # DE-SC0024124. D. Doron’s research was also supported by Instituto de Telecomunicações (ref. UIDB/50008/2020) with the financial support of FCT - Fundação para a Ciência e a Tecnologia and by NSF-BSF grant #2022644. J. Ribeiro’s research was also supported by Instituto de Telecomunicações (ref. UIDB/50008/2020) and NOVA LINCS (ref. UIDB/04516/2020) with the financial support of FCT - Fundação para a Ciência e a Tecnologia."
https://arxiv.org/html/2411.07400v1,Multiparty Communication Complexity of Collision-Finding,"We prove an \Omega(n^{1-1/k}\log k\ /2^{k}) lower bound on the k-party number-in-hand communication complexity of collision-finding. This implies a 2^{n^{1-o(1)}} lower bound on the size of tree-like cutting-planes proofs of the bit pigeonhole principle, a compact and natural propositional encoding of the pigeonhole principle, improving on the best previous lower bound of 2^{\Omega(\sqrt{n})}.","The pigeonhole principle asserting that there is no injective function f:[m]\to[n] for m>n is a cornerstone problem in the study of proof complexity. It is typically encoded as unsatisfiable conjunctive form formula (CNF), henceforth denoted \mathrm{PHP}^{m}_{n}, on the variables y_{i,j}, each of which is an indicator that “pigeon” i is mapped to “hole” j. It is well known that any refutation of \mathrm{PHP}^{n+1}_{n} using resolution proofs requires size 2^{\Omega(n)} [DBLP:journals/tcs/Haken85] and the same asymptotic bound holds for all m that are O(n) [DBLP:journals/tcs/BussT88]. On the other hand, if we allow our proof system to reason about linear inequalities (for example using cutting-planes proofs), then it is easy to see that refuting \mathrm{PHP}^{n+1}_{n} becomes easy – indeed, there exist polynomial size refutations of \mathrm{PHP}^{n+1}_{n}. Despite the pigeonhole principle having short cutting-planes refutations, the related clique-coloring formulas, which state that a graph cannot have both k-cliques and k-1-colorings, requires exponential-size cutting-planes refutation [Pudlak97].111Lower bounds for restricted cutting-planes refutations of these formulas were earlier shown in [implagliazzo-pitassi-urquart-treelike-cp, DBLP:conf/stoc/BonetPR95] The clique-coloring formula can be viewed as a kind of indirect pigeonhole principle: The k nodes of the clique correspond to the pigeons and k-1 colors correspond to the holes, but the representation of possible mappings is quite indirect. It is natural to wonder about the extent to which indirection is required for the pigeonhole principle to be hard for cutting-planes reasoning. As part of studying techniques for cutting-planes proofs, Hrubeš and Pudlák [hrubres-pudlak] considered a very natural compact and direct way of expressing the pigeonhole principle, known as the bit or binary pigeonhole principle222This encoding of the pigeonhole principle was introduced in [DBLP:journals/jsyml/AtseriasMO15].. The bit pigeonhole principle analog of \mathrm{PHP}^{m}_{n} (henceforth denoted \mathrm{BPHP}^{m}_{n}) has m\log n variables x_{i,j} for i\in[m],j\in[\log n] and the principle asserts that, when we organize these variables as an m\times[\log n] matrix, the rows of the matrix all have distinct values. \mathrm{BPHP}^{m}_{n} is the following CNF formula: for each i\neq j\in[m], include the clauses of a CNF encoding that x_{i}\neq x_{j}. One can achieve this by including a clause for each \alpha\in\{0,1\}^{\log n} expressing that x_{i}\neq\alpha\vee x_{j}\neq\alpha. The end result is a CNF with \binom{m}{2}n clauses of size 2\log n. Using techniques related to those of [Pudlak97], Hrubeš and Pudlák [hrubres-pudlak] showed that \mathrm{BPHP}^{m}_{n} requires cutting-planes refutations of size 2^{\Omega(n^{1/8})} for any m>n, proving that even a very direct representation of the pigeonhole principle is hard for cutting-planes proofs. Their arguments, like those of Pudlák, also apply to any proof system that has proof lines consisting of integer linear inequalities with two antecedents per inference that are sound with respect to 01-valued variables; such proofs are known alternatively as semantic cutting-planes proofs or \Th(1) proofs [DBLP:journals/siamcomp/BeamePS07]. Recently, Dantchev, Galesi, Ghani, and Martin [binary-encoding-comb-princip] exhibited a 2^{\Omega(n/\log n)} lower bound on the size of any general resolution refutation of \mathrm{BPHP}^{m}_{n} for all m>n. In fact, they showed that \mathrm{BPHP}^{m}_{n} requires proofs of size 2^{\Omega(n^{1-\varepsilon})} for a more powerful class of proof systems that extend resolution by operating on k-DNFs (known as \mathrm{Res}(k) proofs) for k\leq\log^{1/2-\varepsilon^{\prime}}n. (Note that any sound proof system operating on DNFs requires size at least 2^{n^{\Omega(1)}} to refute of \mathrm{PHP}^{n+1}_{n} [DBLP:journals/cc/PitassiBI93, DBLP:journals/rsa/KrajicekPW95, DBLP:conf/focs/Hastad23].) In addition, [binary-encoding-comb-princip] showed that \mathrm{BPHP}^{m}_{n} has no refutations in the Sherali-Adams proof system [DBLP:journals/siamdm/SheraliA90] of size smaller than 2^{\Omega(n/\log^{2}n)}. Finally, just as \mathrm{PHP}^{m}_{n} has polynomial-size Sum-of-Squares refutations [grigoriev-hirsch-pasechnik], Dantchev et al. showed that \mathrm{BPHP}^{m}_{n} has polynomial-sized Sum-of-Squares refutations. Given the large lower bounds for resolution, \mathrm{Res}(k), and Sherali-Adams refutations of \mathrm{BPHP}^{m}_{n}, it is natural to ask the extent to which the sub-exponential lower bounds can be improved for cutting-planes proofs; how close to a 2^{\Omega(n)} lower bound is possible? While the general question is still open, there has been progress towards this question for the restricted class of tree-like refutations. Tree-like proofs require that any time an inequality is used, it must be re-derived (i.e., the underlying graph of deductions is a tree); the polynomial-size cutting-planes refutations of \mathrm{PHP}^{n+1}_{n} can be made tree-like. In contrast, Itsykson and Riazanov [DBLP:journals/eccc/ItsyksonRiazanov] showed that \mathrm{BPHP}^{m}_{n} requires tree-like cutting-planes refutations of size 2^{\Omega(\sqrt{n})} when m\leq n+\sqrt{n}. Our main result pushes this bound almost to its limit. Specifically, we prove that any tree-like semantic cutting-planes refutation of \mathrm{BPHP}^{m}_{n} requires size 2^{n^{1-o(1)}} whenever m\leq n+2^{2\sqrt{\log n}-2}. In order to show this, we utilize a well-known connection between tree-like refutations and communication complexity. While the results of [DBLP:journals/eccc/ItsyksonRiazanov] for cutting planes relies on two-party communication complexity (and number-on-forehead multiparty communication for other results that we mention below), our stronger results are based on multiparty number-in-hand communication. In particular it is based on a similar natural collision-finding communication problem \coll^{k}_{m,\ell}, in which each player p\in[k] in the number-in-hand model receives an input in x^{(p)}\in[\ell]^{m}, and their goal is to communicate and find a pair i\neq j\in[m] such that x^{(p)}_{i}=x^{(p)}_{j} for all players p\in[k]. Such a communication problem is well-defined (in the sense that such a pair i,j exists) when m>\ell^{k}. This collision-finding problem is intimately related to the unsatisfiable \mathrm{BPHP}^{m}_{n} formula via the following natural search problem associated with any unsatisfiable CNF formula: Given unsatisfiable CNF \varphi, the associated search problem \mathrm{Search}_{\varphi} takes as input a truth assignment \alpha to the variables of \varphi and requires the output of the index of a clause of \varphi that is falsified by \alpha. In particular the connection follows by considering a natural k-party number-in-hand communication game that we denote by \mathrm{Search}_{\varphi}^{k} wherein the assignment \alpha to the variables of \varphi is evenly distributed among the k players. and the players must communicate to find an an answer for \mathrm{Search}_{\varphi}(\alpha). It is not hard to see that if we have a communication protocol solving \mathrm{Search}_{\mathrm{BPHP}^{m}_{n}}^{k}(\alpha) then such a protocol also solves \coll^{k}_{m,n^{1/k}} on input \alpha. Our main technical result is a lower bound on \coll^{k}_{m,n^{1/k}} that holds even when we allow randomized protocols. Theorem 1.1. The randomized number-in-hand communication complexity of \coll^{k}_{m,n^{1/k}} is \Omega(n^{1-1/k}\log k\ /2^{k}) whenever n+1\leq m\leq n+2^{k-2}n^{1/k}. We pause here to note that this bound is nearly tight. There is a deterministic protocol wherein the first player sends a subset of coordinates of size \lceil m/n^{1/k}\rceil in which their inputs are all equal. This requires \log\binom{m}{m/n^{1/k}}\lesssim(m/n^{1/k})\log(m/n^{1/k}) bits; when m\approx n, this is O(n^{1-1/k}\log(n^{1/k}))=O(n^{1-1/k}\log n\ /k) bits of communication. Player two then announces a subset of these coordinates on which they are equal of size \lceil m/n^{2/k}\rceil. The players can continue in this manner until they have found a collision (which is guaranteed by the pigeonhole principle). Note that the amount of communication is handled by a geometric series, and is dominated by the first term, which results in communication O(n^{1-1/k}\log n\ /k). This shows that up to logarithmic factors and a factor of 2^{k}, Theorem 1.1 is tight. We state here a simplified corollary333When m is somewhat larger, we can obtain somewhat weaker lower bounds. of Theorem 1.1 which formalizes our lower bounds for cutting-planes refutations of \mathrm{BPHP}^{m}_{n}. Theorem 1.2. Any tree-like semantic cutting-planes refutation of \mathrm{BPHP}^{m}_{n} requires size 2^{n^{1-2/\sqrt{\log n}-o(1/\sqrt{\log n})}} when m\leq n+2^{2\sqrt{\log n}-2}. We remark that Itsykson and Riazanov [DBLP:journals/eccc/ItsyksonRiazanov] utilized the same connection between communication and proof complexity to achieve their results. They were also interested in a k-party number-on-forehead version of \coll^{k}_{m,\ell} (in particular, in their version, the matrices are added rather than concatenated), which leads to weaker lower bounds in stronger proof systems \Th(k-1) that manipulate degree k-1 polynomial inequalities. Itsykson and Riazanov also left as an open problem whether their bounds for \mathrm{Search}_{\mathrm{BPHP}^{m}_{n}} could be extended to the regime of the“weak” pigeonhole principle when m=n+\Omega(n). Göös and Jain [goos-jain] first answered this in the affirmative, giving an \Omega(n^{1/12}) lower bound on the randomized communication complexity of \coll^{2}_{2n,n^{1/2}}. Yang and Zhang [YangZhang] subsequently improved this to an \Omega(n^{1/4}) bound, which is tight for randomized computation. On the other hand, the results of Hrubeš and Pudlák [hrubres-pudlak] imply a size lower bound for all m>n of 2^{\Omega(n^{1/8})} for the two-party deterministic DAG-like communication complexity of \mathrm{Search}^{2}_{\mathrm{BPHP}^{m}_{n}}, which is an incomparable model."
https://arxiv.org/html/2411.07374v2,Low Degree Local Correction Over the Boolean Cube,"In this work, we show that the class of multivariate degree-d polynomials mapping \{0,1\}^{n} to any Abelian group G is locally correctable with \widetilde{\mathcal{O}}_{d}((\log n)^{d}) queries for up to a fraction of errors approaching half the minimum distance of the underlying code. In particular, this result holds even for polynomials over the reals or the rationals, special cases that were previously not known. Further, we show that they are locally list correctable up to a fraction of errors approaching the minimum distance of the code. These results build on and extend the prior work of the authors [ABP+24] (STOC 2024) who considered the case of linear polynomials (d=1) and gave analogous results.Low-degree polynomials over the Boolean cube \{0,1\}^{n} arise naturally in Boolean circuit complexity and learning theory, and our work furthers the study of their coding-theoretic properties. Extending the results of [ABP+24] from linear polynomials to higher-degree polynomials involves several new challenges and handling them gives us further insights into properties of low-degree polynomials over the Boolean cube. For local correction, we construct a set of points in the Boolean cube that lie between two exponentially close parallel hyperplanes and is moreover an interpolating set for degree-d polynomials. To show that the class of degree-d polynomials is list decodable up to the minimum distance, we stitch together results on anti-concentration of low-degree polynomials, the Sunflower lemma, and the Footprint bound for counting common zeroes of polynomials. Analyzing the local list corrector of [ABP+24] for higher degree polynomials involves understanding random restrictions of non-zero degree-d polynomials on a Hamming slice. In particular, we show that a simple random restriction process for reducing the dimension of the Boolean cube is a suitably good sampler for Hamming slices. Thus our exploration unearths several new techniques that are useful in understanding the combinatorial structure of low-degree polynomials over \{0,1\}^{n}.","In this paper, we consider the local correction of low-degree polynomial functions over groups evaluated over \{0,1\}^{n} and give polylogarithmic query local correctors for every constant degree. This extends and generalizes previous work of the authors [ABP+24] who considered and solved the analogous problem for the linear (i.e., d=1) case. We define some of the basic terms and review the previous work before describing the challenges in strengthening to higher degrees and the new tools used to overcome them. Low degree polynomials over groups. The main objects of interest in this paper are polynomial functions mapping \{0,1\}^{n} to an Abelian group G. Here a function f is a polynomial of degree at most d if it can be expressed as \sum_{S\subseteq[n]:|S|\leq d}c_{S}\prod_{i\in S}x_{i}, where the product is over the integers and the coefficients c_{S} come from the Abelian group G. We denote the space of polynomial functions of degree at most d by \mathcal{P}_{d}(\{0,1\}^{n},G) (which we compress to \mathcal{P}_{d} when G and n are known). The standard proof of the Ore-DeMillo-Lipton-Schwartz-Zippel lemma naturally extends to polynomials over groups. It proves that two different degree d polynomials disagree on at least \delta_{d}:=2^{-d} fraction of the domain (if d<n), and thus form natural classes of error-correcting codes. This paper explores the corresponding correction questions focusing on locality. A special case that is already of interest is when the group G is the group of real numbers (or rationals) - a setting where relatively few codes are shown to exhibit local correction properties. Local correction of polynomials. Informally, the local correction problem is that of computing, given oracle access to a function f:\{0,1\}^{n}\to G and a point \mathbf{a}\in\{0,1\}^{n}, the value P(\mathbf{a}) of the nearest degree d polynomial P to the function f at the point \mathbf{a}, while making few oracle queries to f. More formally, for functions f,g:\{0,1\}^{n}\to G, let \delta(f,g) denote the fraction of points from the domain where they differ. We say f is \varepsilon-close to g if \delta(f,g)\leq\varepsilon and \varepsilon-far otherwise. For a given G, we say that that \mathcal{P}_{d} is (\delta,q)-locally correctable if for every n there is a probabilistic algorithm that, for every function f:\{0,1\}^{n}\to G that is \delta=\delta(n)-close to some polynomial P\in\mathcal{P}_{d}(\{0,1\}^{n},G) and for every \mathbf{a}\in\{0,1\}^{n}, outputs P(\mathbf{a}) with probability at least 3/4 while making at most q=q(n) queries to f. One of the main quests of this work is to give non-trivial upper bounds on the query complexity q for which \mathcal{P}_{d} is (\Omega_{d}(1),q)-locally correctable. List correction of codes. Note that (\delta,q)-locally correctability of \mathcal{P}_{d} requires that \delta is less than half the minimum distance of the space, i.e., \delta<\delta_{d}/2. To go beyond one usually resorts to the notion of list-decoding; and in the local setting, to notions like “local list-decoding” and “local list correction”. Roughly list-decoding allows the decoder to output a small list of words with the guarantee that all codewords within a given distance are included in the output. Formally we say \mathcal{P}_{d} is (combinatorially) (\delta,L)-list correctable if for every f:\{0,1\}^{n}\to G there are at most L degree-d polynomials P satisfying \delta(f,P)\leq\delta.111The algorithmic version would require the list of nearby polynomials to be algorithmically recoverable from f. We don’t consider this notion in this work but move ahead to the harder “local list correction” problem. Unlike the unique decoding problem where the maximum \delta such that a code is uniquely correctable up to \delta errors is well understood, the list-decoding radius for higher values of L is not well-understood. A natural question that we study here (for the first time in this generality) is: What is the largest \delta such that \mathcal{P}_{d} is (\delta,\mathcal{O}_{d}(1))-list correctable? We refer to this largest value of \delta as the list-decoding radius of \mathcal{P}_{d}. Local list correction of codes. Local list correction is the notion of list decoding combined with the notion of local correction. Formalizing this definition is a bit more subtle and was first done in [STV01]. The notion allows the decoder to work in two phases — a preprocessing phase with q_{1}=q_{1}(n) queries to the function f, that outputs up to L algorithms \phi_{1},\ldots,\phi_{L} and a query phase, where given \mathbf{a}\in\{0,1\}^{n} each algorithm \phi_{i} makes q_{2}=q_{2}(n) queries to f and outputs \phi_{i}(\mathbf{a}). We say that \mathcal{P}_{d} is (\delta,q_{1},q_{2},L)-local list correctable if for every function f and polynomial P\in\mathcal{P}_{d} that are \delta-close, there is a decoder as above such that one of its outputs includes P with high probability (say 3/4). See Definition 2.2.3 for a formal definition. The final goal of this paper is to locally list-correct \mathcal{P}_{d} using non-trivially small number of queries (in both the preprocessing and query phases) where the fraction of errors approaches the list-decoding radius. 1.1 Motivation and previous work Local decoding of polynomials over finite fields has played a central role in computational complexity and in particular in breakthrough results like IP=PSPACE and the PCP theorem. While most of these results consider functions over the entire multivariate domain (i.e., \mathbb{F}^{n}), low-degree polynomials over \{0,1\}^{n} do arise quite naturally in complexity theory, notably in circuit complexity capturing classes like \mathrm{AC}^{0} [Raz87, Smo87] and \mathrm{ACC} [BHLR19], and in learning theory. Many of these results exploit basic distance properties of multivariate polynomials as given by the Ore-DeMillo-Lipton-Schwartz-Zippel lemma (see Theorem 2.2.1). This lemma roughly says that the space of degree-d polynomial functions mapping S^{n} to a field \mathbb{F} where S\subseteq\mathbb{F} is finite form an error-correcting code of relative distance d/|S| when d<|S|, and |S|^{-d/(|S|-1)} when d\geq|S|. The special case of S=\mathbb{F} is extensively studied and heavily used, e,g., in PCPs and cryptography. In this setting, the lemma can also be made algorithmic, with the first such instance handling the special case of \mathbb{F}_{2} dating back to the works of Reed and Muller [Ree54, Mul54]. More recently, local list correction algorithms were discovered in the works of Goldreich and Levin [GL89] for linear polynomials, Sudan, Trevisan and Vadhan [STV01] for higher-degree polynomials over large fields, Gopalan, Klivans and Zuckerman [GKZ08] for higher-degree polynomials over \mathbb{F}_{2}, and Bhowmick and Lovett [BL18] for polynomials over any small finite field. The case of general S however has not received much attention and is mostly unexplored. This was first highlighted in a relatively recent work of Kim and Kopparty [KK17] who gave polynomial time (but not local) unique-decoding algorithms correcting errors up to half the minimum distance. Their work exposed the fact that many other algorithmic and even some coding-theoretic questions were not well understood when S\neq\mathbb{F}, and our work aims to fill some gaps in knowledge here. Another motivation for our work is the design of locally correctable codes over the reals. A series of works [BDYW11, DSW14, DSW17] has exposed that there are no known (\Omega(1),\mathcal{O}(1))-locally correctable codes over the reals of arbitrarily large dimensions. The underlying challenge here leads to novel questions in incidence geometry. Roughly the goal here is to design a finite set of points T\subseteq\mathbb{R}^{n} such many pairs of points in T are contained in constant-dimensional “degenerate” subspaces, where a q-dimensional subspace is said to be degenerate if it contains q+1 points from T. Till recently no sets that possessed this property with q=o(n) were known, and the recent results of [ABP+24] may be viewed as showing that the set T=\{0,1\}^{n}\subseteq\mathbb{R}^{n} has \widetilde{O}(\log n) dimensional subspaces covering most pairs of points of T. Local correctability of degree d polynomials would translate to showing that moment vectors222d-moment vector of \mathbf{v}\in\{0,1\}^{n} is a vector in \{0,1\}^{\binom{n}{d}} which is an evaluation vector of \mathbf{v} on all multilinear monomials of degree \leq d. of \{0,1\}^{n} (viewed as vectors in \mathbb{R}^{N} for N=\mathcal{O}(n^{d})) also exhibit a similar property, thus adding to the body of sets in \mathbb{R}^{N} that show non-trivial degeneracies. Turning to previous work on local correction of polynomials over grids, the local correction question when S=\{0,1\} was first explored by Bafna, Srinivasan and Sudan [BSS20], who mainly presented a lower bound of \widetilde{\Omega}(\log n) on the number of queries to recover even when d=1 and from some o(1) fraction of errors and \mathbb{F}=\mathbb{R}. On the positive side, for fields of characteristic p, they gave an \mathcal{O}_{d,p}(1) query algorithm to recover from \Omega_{d,p}(1) fraction of errors. This left the case for large and zero characteristic fields open. The recent work of the authors [ABP+24] investigated the case of general fields, and more generally, polynomials over Abelian groups (i.e., \{0,1\}^{n}\to G), for the special case of d=1. For this setting, they consider all three questions posed in the previous section, namely the (unique) local correction limit, the list-decoding radius, and the local list correction problem. In this setting where distinct degree 1 polynomials disagree with each other on at least half the domain, they show that up to 1/4 fraction of errors can be uniquely locally corrected with \widetilde{\mathcal{O}}(\log n) queries. They further show that the list-decoding radius approaches 1/2 and that there are \widetilde{\mathcal{O}}(\log n) query algorithms to locally list correct \mathcal{P}_{1} for any fraction of errors bounded away from 1/2. Their work exposes a number of technical challenges in going beyond the d=1 case and we address those in this paper. 1.2 Technical challenges in extending beyond the linear case We start by reviewing the main ideas in [ABP+24] and outlining the challenges in the higher-degree extension. Their unique local corrector correcting up to half the minimum distance works in three steps: Given an oracle for a function f(\mathbf{x}) at a distance less than 1/4 from a linear polynomial P(\mathbf{x}), \blacktriangleright It first provides oracle access to a function f_{1}(\mathbf{x}) at any tiny but constant distance from P(\mathbf{x}), using \mathcal{O}(1) queries to f(\mathbf{x}). \blacktriangleright Then the algorithm provides oracle access to a function f_{2}(\mathbf{x}) at distance 1/\mathop{\mathrm{poly}}(\log n) from P(\mathbf{x}) while making \mathop{\mathrm{poly}}(\log\log n) queries to f_{1}(\mathbf{x}). \blacktriangleright Finally, the algorithm provides oracle access to the linear polynomial P(\mathbf{x}) making \mathcal{O}(\log n) queries to f_{2}. Composing the three steps gives the desired unique local corrector. The first two steps in their result are general enough to work for all degrees. The third step in the unique local corrector of [ABP+24] is the most significant one and does not extend immediately to the higher degree setting. [ABP+24] reduce this step to show that any point in \{0,1\}^{n} can be expressed as a linear combination of \widetilde{\mathcal{O}}(\log n) roughly balanced vectors333Hamming weight is very close to n/2. To extend their approach to higher degrees, we would need an analogous result for the degree \leq d-moment vectors of vectors in \{0,1\}^{n} but we are unable to find such an extension directly. Instead, as we elaborate further below, we manage to find a new path for this step, which results in a somewhat different proof even for the linear (d=1) case. Next, we turn to the combinatorial analysis of the list-decoding radius. For simplicity assume that the Abelian group G is a finite field \mathbb{F}_{p}. The analysis in the linear case [ABP+24] splits into two cases: the low444The precise constants here are not important, as the analysis works as long as the p\leq\mathcal{O}(1). characteristic case (p\leq 3) and the high characteristic case (p>5). The former case is handled via a suitable version of the Johnson bound. In the latter setting, a key insight used in [ABP+24] for the linear case is that non-sparse linear polynomials tend to be non-zero with very high probability, i.e. anti-concentration of non-sparse linear polynomials. In the higher degree setting, both analyses become much more involved. In the low characteristic case, the Johnson bound no longer yields the right answer. For the high characteristic case, the primary obstacle is understanding the anti-concentration statement for non-sparse low-degree polynomials. Generalizing the result above to all Abelian groups involves multiple steps in [ABP+24] - they extend the latter approach above to groups where every element has a sufficiently high order (specifically order at least 5). Then they consider groups where every element has order a power of 2 or 3 separately and analyze these special cases; and finally use some “special intersection properties” of the agreement sets555Agreement set of a polynomial P and a function f is defined as the subset of \{0,1\}^{n} on which P and f agree. of different polynomials with any given function to apply a counting tool from the work of Dinur, Grigorescu, Kopparty, and Sudan [DGKS08a] to combine these different steps. While many of the steps extend to the higher degree setting (sometimes needing heavier machinery) the final step involving “special intersection properties” simply does not work in our setting. (Roughly the difference emanates from the fact that the probability that two linearly independent degree-1 functions vanish at a random point is at most 1/4, which is the square of the probability for a single degree-1 function. This fails for degree 2.) Overcoming this barrier leads to further new challenges in the higher-degree case. The local list-correction algorithm for degree-1 polynomials of [ABP+24] is inspired by the local list-correction algorithm of Reed-Muller codes from [STV01]. The high-level idea is to start with a function f(\mathbf{x}) that is (1/2-\varepsilon)-close to a set \mathsf{List} of linear polynomials and produce a small list of oracles such that each polynomial in \mathsf{List} is within a small constant distance to an oracle from the list, at which point the unique local corrector becomes applicable. This ‘error-reduction’ step involves choosing a random subcube \mathsf{C} of \{0,1\}^{n} (as defined in Section 2 below) of sufficiently large but constant dimension k and doing a brute-force list decoding on \mathsf{C} to find a list \mathsf{List}^{\prime}: it is not hard to argue that the restricted version P^{\prime} of each polynomial P\in\mathsf{List} appears in the list \mathsf{List}^{\prime} with high probability (this needs the combinatorial bound mentioned above). To complete the error-reduction, they need to decode P at a random point \mathbf{b}\in\{0,1\}^{n}. This is done by repeating the above brute-force algorithm with the subcube \mathsf{C}^{\mathbf{b}} ‘spanned’ by \mathsf{C} and \mathbf{b}: informally, this is the smallest subcube spanned by \mathsf{C} and \mathbf{b} and has dimension 2k (see Section 5.2.1 for details.) They now obtain a new list of polynomials \mathsf{List}^{\prime\prime} and need to isolate the polynomial P^{\prime\prime} corresponding to P in this list to get P(\mathbf{b}). Here, [ABP+24] uses the fact that we know the restriction of P to the subcube \mathsf{C} inside \mathsf{C}^{\mathbf{b}}. The bad event in this case is that there are two polynomials in \mathcal{L}^{\prime\prime} that disagree on \mathbf{b} but agree on \mathsf{C}. Bounding the probability of this event is the key step in the analysis of [ABP+24] and is done by giving a complete understanding of which kinds of distinct polynomials over \mathsf{C}^{\mathbf{b}} can collapse to the same polynomial over \mathsf{C}. This kind of understanding seems difficult to obtain for higher degrees, and we need to develop a new analysis for bounding the probability of the bad event in this setting. We now turn to our results before elaborating on the techniques used to overcome the challenges. 1.3 Our main results Briefly, our results provide poly-logarithmic query algorithms for unique and list-decoding to the maximal fraction of errors that are allowed. Specifically, the unique decoding algorithm works up to half the distance. We also establish that the list-decoding radius approaches the distance (as the list size tends to infinity) and give matching local algorithms. We give our specific theorems below. Theorem 1.3.1 (Local correction algorithms for \mathcal{P}_{d} up to the unique decoding radius). For every Abelian group G and for every constant \varepsilon>0, the space \mathcal{P}_{d} has a (\delta,q)-local correction algorithm where \delta=\frac{1}{2^{d+1}}-\varepsilon and q=\widetilde{\mathcal{O}}_{\varepsilon}(\log n)^{d}. We show that if all its elements of the group have a constant order, then the query complexity of the local correction algorithm can be brought down from \widetilde{\mathcal{O}}_{d}((\log n)^{d}) to a constant (i.e., independent of n). Specifically, we say that an Abelian group G is a torsion group if all its elements have finite order, and the exponent of a torsion group is the least common multiple of the orders of all the elements. While [BSS20] shows this only for groups underlying fields of constant characteristic and for some constant error, we extend their proof to all groups of constant exponent and error up to the unique-decoding radius. Theorem 1.3.2. If G is an Abelian torsion group of exponent M, then for every \varepsilon>0, \mathcal{P}_{d} has a (\delta,q)-local correction algorithm where \delta=\frac{1}{2^{d+1}}-\varepsilon and q=\mathcal{O}_{M,\varepsilon}(1). As noted earlier, some dependence on n is needed even when the degree is 1 and G is a field of large characteristic (or characteristic 0), as an \Omega(\log n/\log\log n) lower bound was shown in this setting by earlier work of Bafna, Srinivasan, and Sudan [BSS20] (and shown to be tight up to \mathop{\mathrm{poly}}(\log\log n) factors in [ABP+24]). Our upper bound above is thus optimal to within polynomial factors (for constant d). However, we do not know if the query complexity can be improved to, say, \tilde{\mathcal{O}}_{d}(\log n) for degree d. We also extend the above algorithm from Theorem 1.3.1 to the list decoding regime. For this, we first establish a bound on the list-decoding radius. As far as we know, the following result was not known before for G being any group other than the field \mathbb{F}_{2}. Theorem 1.3.3 (Combinatorial list decoding bound for \mathcal{P}_{d}). For every Abelian group G and for every constant \varepsilon>0, the space \mathcal{P}_{d} over any Abelian group G is (1/2^{d}-\varepsilon,\exp(\mathcal{O}_{d}(1/\varepsilon)^{\mathcal{O}(d)})-list correctable. The above is tight in the sense that the number of codewords at distance 1/2^{d} can depend on both n and the size of the group G (and is infinite when G is infinite). We do not know if the dependence on \varepsilon is tight. Note that for the setting where d=1, [ABP+24] gives a polynomial bound in terms of 1/\varepsilon. Our bound as stated above is exponential and while we can see a path to improving this to a quasi-polynomial, we don’t see a polynomial upper bound using the proofs of this paper even when d=1. Finally, we state our local list correction result. Theorem 1.3.4 (Local list correction for \mathcal{P}_{d}). For every Abelian group G and for every \varepsilon>0, the space \mathcal{P}_{d} is (1/2^{d}-\varepsilon,\mathcal{O}_{\varepsilon}(1),\widetilde{\mathcal{O}}_{% \varepsilon}(\log n)^{d},\exp(\mathcal{O}_{d}(1/\varepsilon)^{\mathcal{O}(d)}))-locally list correctable. Specifically, there is a randomized algorithm \mathcal{A} that, when given oracle access to a polynomial f and a parameter \varepsilon>0, outputs with probability at least 3/4 a list of randomized algorithms \phi_{1},\ldots,\phi_{L} (L\leq\exp(\mathcal{O}_{d}(1/\varepsilon)^{\mathcal{O}(d)})) such that the following holds. For each P\in\mathcal{P}_{d} that is (1/2^{d}-\varepsilon)-close to f, there is at least one algorithm \phi_{i} that, when given oracle access to f, computes P correctly on every input with probability at least 3/4. The algorithm \mathcal{A} makes \mathcal{O}_{\varepsilon}(1) queries to f, while each \phi_{i} makes \widetilde{\mathcal{O}}_{\varepsilon}((\log n)^{d}) queries to f. Remark 1.3.5. If G is an Abelian torsion group of exponent M, \mathcal{P}_{d} is (1/2^{d}-\varepsilon,\mathcal{O}_{\varepsilon}(1), \mathcal{O}_{M,\varepsilon}(1), \exp(\mathcal{O}_{d}(1/\varepsilon)^{\mathcal{O}(d)}))-locally list-correctable. This follows in a similar manner as Theorem 1.3.4, except we replace the generic local corrector in the unique-decoding regime with that given by Theorem 1.3.2. 1.4 Technical tools In the process of proving our main results, we prove several lemmas that we believe are independently interesting. In the proof of Theorem 1.3.1, the main step is to construct, for any \mathbf{a}\in\{0,1\}^{n}, a distribution \mathcal{D}_{\mathbf{a}} over (\{0,1\}^{n})^{q} such that the marginal distribution of each point is close to the uniform distribution over \{0,1\}^{n}, and for any degree-d polynomial P, P(\mathbf{a}) can be computed via the evaluations of P on a sample from \mathcal{D}_{\mathbf{a}}. Constructing such a distribution \mathcal{D} reduces to the following problem of constructing a geometric set with some nice algebraic properties. We discuss in Section 3 how such a set leads to the distribution \mathcal{D}_{\mathbf{a}}. Question 1. Find two parallel hyperplanes in k dimensions that are \varepsilon-close in Euclidean distance such that every non-zero degree-d multilinear polynomial is non-zero on the points of the Boolean cube \{0,1\}^{k} lying between the two hyperplanes. The ‘closeness’ parameter \varepsilon plays a crucial role in the efficiency of the local correction algorithm. It is easy to see (and folklore) that we can get \varepsilon=1/k^{\Omega(1)}. However, we show that we can obtain a construction where \varepsilon is exponentially small in k. This is done in Lemma 3.2.1. The proof of the combinatorial list decoding bound is broken down into two cases depending on the order of elements in the group. The first case is when all elements have order larger than a prime p_{0}(d) (a constant dependent on d) and the second case is when the group is a product of p-groups for p<p_{0}. For the first case, the key step is an understanding of the anti-concentration properties of low-degree non-sparse polynomials. More precisely, we have the following question. Question 2. Let P(x_{1},\ldots,x_{n}) be a polynomial of degree d with at least s non-zero monomials. Then how large can \Pr_{\mathbf{a}\sim\{0,1\}^{n}}{[P(\mathbf{a})=0]} be? If we take P=x_{1}x_{2}\cdots x_{d-1}\cdot L(x_{d},\ldots,x_{n}) where L is a linear polynomial with s monomials, we see that P vanishes with probability approximately 1-2^{-(d-1)}-\mathcal{O}(1)/\sqrt{s} over (say) the reals. In Lemma 4.1.1, we build on known anti-concentration results [Erd45, MNV16] show that this is essentially the worst possible in groups with no elements of small order. In the second case, an important step in our proof is a tail inequality for events defined by the vanishing of low-degree polynomials over a field. Let Q_{1}(\mathbf{x}),\ldots,Q_{t}(\mathbf{x}) be t degree-d polynomials on the same variable set. We know that each is non-zero on a random input with probability at least 2^{-d} and hence that the expected number of polynomials vanishing at a uniformly random point \mathbf{a}\sim\{0,1\}^{n} is at most (1-2^{-d})\cdot t. This leads to the following question. Question 3. Suppose we have a collection of t degree-d polynomials. Can we bound the probability that more than (1-2^{-d}+\varepsilon)\cdot t many of these polynomials vanish at a random point \mathbf{a} in \{0,1\}^{n}? Clearly, we cannot get a strong tail bound unless we impose some ‘independence’ constraints on the polynomials (for example, we cannot hope for a strong bound if all the polynomials are in the linear span of a small number of polynomials). We show that we can get a Chernoff-style tail bound under the constraint that the ‘leading monomials’ (under a suitable ordering) of these polynomials are pairwise disjoint. This is done in Lemma 4.2.1 using the ‘Footprint bound’ [GH00] (essentially a tool from commutative algebra) and an idea due to Panconesi and Srinivasan [PS97]. We build on this result to prove an optimal bound on the list-decoding radius for degree-d polynomials over small finite fields \mathbb{F}_{p} (and more generally over groups that are products of p-groups, where p<p_{0}). In the setting when we are working with polynomials mapping \mathbb{F}_{p}^{n} to \mathbb{F}_{p}, this was done in the works of [GKZ08, BL18] via an involved mixture of algebraic and analytic techniques. Unfortunately, these do not seem to be applicable here: one significant reason that appears again and again in our work is that we cannot restrict a given function to an arbitrary subspace in our ambient space since the domain \{0,1\}^{n} does not have this algebraic structure. Instead we use other combinatorial techniques such as the Sunflower lemma in conjunction with the above tail bound to obtain the stated result. For local list correction, we follow the algorithm of [ABP+24] modulo changes in parameters to handle higher degree polynomials. The main innovation involves analyzing the behavior of degree-d polynomials on a Hamming slice (points with a fixed Hamming weight) after a random process of reducing the dimension. In particular, assume that we have a fixed degree-d polynomial R(x_{1},\ldots,x_{2k}) in 2k dimensions such that R is non-zero on the Hamming slice of weight k. We now choose a random subcube \mathsf{C} by pairing the 2k variables at random into k pairs and identifying the variables in each pair. We would like to upper bound the probability that R is zero on the cube \mathsf{C} by a function that goes to 0 with k.666One might hope to prove such a statement under the weaker assumption that R is simply a non-zero multilinear polynomial of degree d. Unfortunately, the simple example R=x_{1}+\cdots+x_{2k} over the group G=\mathbb{F}_{2} shows that such a statement is not possible even in the degree-1 case. We do this by addressing the following two questions. The first question is on how the density of any fixed set on a slice changes under the aforementioned random process. In this work, we are particularly interested in the middle slice, i.e. points of Hamming weight k in \{0,1\}^{2k}. Question 4. For any fixed subset S of the middle slice, how does the density of the set S\cap\mathsf{C} (as a subset of the middle slice of \mathsf{C}) compare with the density of S? In Lemma 5.1.1 we show that the density is almost preserved under the random process. In other words, this random process is a good sampler for subsets of the middle slice. To prove Lemma 5.1.1, we show that certain kinds of Johnson graphs are good spectral expanders. The second question is on a quantitative estimate of the number of non-zero points of a degree-d polynomial on a Hamming slice. This is a natural question, but has not been addressed before as far as we know. Question 5. For a degree-d polynomial R which is non-zero on a Hamming slice, on how many points of the Hamming slice is it non-zero? We give a simple lower bound on the number of non-zero points for a degree-d polynomial on the Hamming slice by modifying the proof of the Ore-DeMillo-Lipton-Schwartz-Zippel lemma. We show it in Lemma 5.1.6."
https://arxiv.org/html/2411.07602v1,Circuit Complexity Bounds for RoPE-based Transformer Architecture,"Characterizing the express power of the Transformer architecture is critical to understanding its capacity limits and scaling law. Recent works provide the circuit complexity bounds to Transformer-like architecture. On the other hand, Rotary Position Embedding (\mathsf{RoPE}) has emerged as a crucial technique in modern large language models, offering superior performance in capturing positional information compared to traditional position embeddings, which shows great potential in application prospects, particularly for the long context scenario. Empirical evidence also suggests that \mathsf{RoPE}-based Transformer architectures demonstrate greater generalization capabilities compared to conventional Transformer models. In this work, we establish a tighter circuit complexity bound for Transformers with \mathsf{RoPE} attention. Our key contribution is that we show that unless \mathsf{TC}^{0}=\mathsf{NC}^{1}, a \mathsf{RoPE}-based Transformer with \mathrm{poly}(n)-precision, O(1) layers, hidden dimension d\leq O(n) cannot solve the arithmetic problem or the Boolean formula value problem. This result significantly demonstrates the fundamental limitation of the expressivity of the \mathsf{RoPE}-based Transformer architecture, although it achieves giant empirical success. Our theoretical framework not only establishes tighter complexity bounds but also may instruct further work on the \mathsf{RoPE}-based Transformer.","Recently, Large Language Models (LLMs), such as GPT-4 [1], Claude [3], Llama [55], and more recently, OpenAI’s o1 [60], have exhibited remarkable potential to revolutionize numerous facets of daily life, including conversational AI [40], AI agents [75, 21], search capabilities [60], and AI assistants [38, 23], among others. One of the most significant emergent capabilities of LLMs is their proficiency in handling long-context information, which is essential for effectively processing complex documents such as academic papers, official reports, and legal texts. LLMs also have demonstrated exceptional capabilities in tackling long-context tasks, such as zero-shot summarization [14, 79] and sustaining coherent, extended conversations [76, 56]. The o1 model from OpenAI [60] represents a major advancement in this field. By leveraging Chain-of-Thought (CoT) reasoning [74, 37] and incorporating Retrieval Augmented Generation (RAG) [48, 26], it showcases a level of expertise comparable to PhD-level problem solving, with both techniques heavily relying on extensive contextual understanding. Large Language Models (LLMs) are primarily built upon the Transformer architecture [69], which uses the self-attention mechanism as its core component. Given this foundational structure, an important question arises: what computational primitives can the components of the Transformer implement, and what problems can the entire system solve collectively? These questions are crucial for interpreting Transformers in a principled manner, understanding the potential limitations of their reasoning capabilities, and fostering trust in deployed Transformer-based systems. To address the aforementioned questions and to investigate the expressiveness of transformers, prior research has made significant strides. Recent studies, such as [58], have established two key results concerning both non-uniform and \mathsf{L}-uniform settings: first, any depth-d transformer with c\log n-precision can be simulated by a threshold circuit family with constant depth; second, such a transformer can also be simulated by a \mathsf{L}-uniform threshold circuit family of constant depth. Further advancing these findings, [57] demonstrate that \mathsf{DLOGTIME}-uniform \mathsf{TC}^{0} circuits are capable of simulating softmax-attention transformers. Building on this foundation, [16] refine these results by increasing the accuracy of approximation. They enhance the precision for softmax-attention transformers from O(\log n) to O(\operatorname{poly}(n)), confirming that these transformers fall within the \mathsf{DLOGTIME}-uniform \mathsf{TC}^{0} class. Additionally, they show that a softmax-attention transformer with an absolute error bound of 2^{-O(\operatorname{poly}(n))} is also contained within \mathsf{DLOGTIME}-uniform \mathsf{TC}^{0}. On the other hand, first introduced by [61], Rotation Position Embedding (\mathsf{RoPE}) enhances Transformers by encoding both absolute and relative positional information through a rotation matrix, enabling greater sequence length flexibility, improved attention mechanism efficiency, and better performance on long-text tasks, exemplified by \mathsf{RoPE}-based language models that can summarize an entire book in a single pass. Due to these advantageous properties, \mathsf{RoPE} has been widely adopted in numerous empirical studies [20, 9, 12]. However, despite its considerable success, the underlying mechanisms of \mathsf{RoPE} remain largely unknown, posing an intriguing mystery in the field. A natural question arises: Does \mathsf{RoPE} enhance the expressiveness of the Transformer-based Large Language Model? This work aims to address this question from the perspective of circuit complexity, taking a step forward in theoretically understanding the underlying mechanisms of \mathsf{RoPE}. We present a rigorous theoretical investigation of \mathsf{RoPE}-based Transformers, establishing fundamental limits on their computational power. Our core approach involved a systematic examination of the circuit complexity for each component of the \mathsf{RoPE}-based architecture, from the basic trigonometric functions to the complete attention mechanism. Ultimately, we prove that these models can be simulated using uniform \mathsf{TC}^{0} circuits. Furthermore, we show that unless \mathsf{TC}^{0}=\mathsf{NC}^{1}, \mathsf{RoPE}-based Transformers with \operatorname{poly}(n)-precision, O(1) layers, and a hidden dimension d\leq O(n) are unable to solve either arithmetic formula evaluation or Boolean formula value problems. This finding is significant because it uncovers fundamental expressivity limitations of \mathsf{RoPE}-based architectures, even though they have shown empirical success in modern language models. Beyond Merrill and Sabharwal [58, 57] and Chiang [16], our contribution are summarized as follows: • We prove that unless \mathsf{TC}^{0}=\mathsf{NC}^{1}, \mathsf{RoPE}-based Transformer with \operatorname{poly}(n)-precision, constant-depth, \operatorname{poly}(n)-size can be simulated by a \mathsf{DLOGTIME}-uniform \mathsf{TC}^{0} circuit family (Theorem 4.8). • We prove that unless \mathsf{TC}^{0}=\mathsf{NC}^{1}, a \mathsf{RoPE}-based Transformer with \operatorname{poly}(n)-precision, O(1) layers, hidden dimension d\leq O(n) cannot solve the arithmetic formula evaluation problems (Theorem 5.8). • We prove that unless \mathsf{TC}^{0}=\mathsf{NC}^{1}, a \mathsf{RoPE}-based Transformer with \operatorname{poly}(n)-precision, O(1) layers, hidden dimension d\leq O(n) cannot solve the Boolean formula value problem (Theorem 5.9). Roadmap. In Section 2, we review the related work. In Section 3, we introduce some important computation concepts and Transformer definitions essential for the subsequent sections. In Section 4, we give the circuit complexity result of \mathsf{RoPE}-based Transformers. In Section 5, we give our hardness results. In Section 6, we summarizes our theoritical results."
https://arxiv.org/html/2411.07275v1,The Equivalence Problem of E-Pattern Languages with Regular Constraints is Undecidable,"Patterns are words with terminals and variables. The language of a pattern is the set of words obtained by uniformly substituting all variables with words that contain only terminals. Regular constraints restrict valid substitutions of variables by associating with each variable a regular language representable by, e.g., finite automata. Pattern languages with regular constraints contain only words in which each variable is substituted according to a set of regular constraints. We consider the membership, inclusion, and equivalence problems for erasing and non-erasing pattern languages with regular constraints. Our main result shows that the erasing equivalence problem—one of the most prominent open problems in the realm of patterns—becomes undecidable if regular constraints are allowed in addition to variable equality.","A pattern is a finite word consisting of symbols from a finite set of letters \Sigma=\{a_{1},...,a_{\sigma}\}, also called terminals, and from an infinite set of variables X=\{x_{1},x_{2},...\} with \Sigma\cap X=\emptyset. It is a natural and compact device to define formal languages. Words consisting of only terminal symbols are obtained from patterns by a substitution h, a terminal preserving morphism which maps all variables from a pattern to words over the terminal alphabet. The language of a pattern consists of all words obtainable from that pattern by substitutions. We differentiate between two kinds of substitutions. Originally, pattern languages introduced by Angluin [1] only consisted of words obtained by non-erasing substitutions that required all variables to be mapped to non-empty words. Thus, those languages are also called NE-pattern languages. Later, so called erasing-/extended- or just E-pattern languages have been introduced by Shinohara [24]. In these, substitutions are also allowed to map variables to the empty word. Consider, for example, the pattern \alpha:=x_{1}\mathtt{a}\mathtt{b}x_{2}x_{2}. Then, by mapping x_{1} to \mathtt{a}\mathtt{a}\mathtt{a} and x_{2} to \mathtt{b}\mathtt{a} with a substitution h, we obtain the word h(\alpha)=\mathtt{a}\mathtt{a}\mathtt{a}\mathtt{a}\mathtt{b}\mathtt{b}\mathtt{% a}\mathtt{b}\mathtt{a}. If we consider the E-pattern language of \alpha, we could also map x_{2} to the empty word \varepsilon with a substitution h^{\prime} which also maps x_{1} to \mathtt{a}\mathtt{a}\mathtt{a} and obtain h^{\prime}(\alpha)=\mathtt{a}\mathtt{a}\mathtt{a}\mathtt{a}\mathtt{b}. Due to its practical and simple definition, patterns and their corresponding languages occur in numerous areas regarding computer science and discrete mathematics, including unavoidable patterns [14, 17], algorithmic learning theory [1, 5, 25], word equations [17], theory of extended regular expressions with back references [9], and database theory [7, 23]. The main problems regarding patterns and pattern languages are the membership problem (and its variations [10, 11, 6]), the inclusion problem, and the equivalence problem in both the erasing (E) and non-erasing (NE) cases. The membership problem determines if a word belongs to a pattern’s language. This problem is NP-complete for both E- and NE-pattern languages [1, 14]. The inclusion problem asks if one pattern’s language is included in another’s. Jiang et al. [15] showed that it is generally undecidable for E- and NE-pattern languages. Freydenberger and Reidenbach [8], and Bremer and Freydenberger [2] proved its undecidability for all alphabets with size \geq 2 in both E- and NE-pattern languages. The equivalence problem tests if two patterns generate the same language. It is trivially decidable for NE-pattern languages [1]. Whether its decidable for E-pattern languages is one of the major open problems in the field [15, 21, 20, 19, 22]. However, for terminal-free patterns, the inclusion and equivalence problems in E-pattern languages have been characterized and shown to be NP-complete [15, 4]. The decidability of the inclusion problem for terminal-free NE-pattern languages remains unresolved, though. Various extensions to patterns and pattern languages have been introduced over time. Some examples are the bounded scope coincidence degree, patterns with bounded treewidth, k-local patterns, and strongly-nested patterns (see [3] and references therein). Koshiba [16] introduced so called typed patterns to enhance the expressiveness of pattern languages by restricting substitutions of variables to types, i.e., arbitrary recursive languages. This has recently been extended by Geilke and Zilles [12] who introduced the notion of relational patterns and relational pattern languages. We consider a specific class of typed- or relational patterns called patterns with regular constraints. Let \mathcal{L}_{Reg} be the set of all regular languages. Then, we say that a mapping r:X\rightarrow\mathcal{L}_{Reg} is a regular constraint that implicitly defines languages on variables x\in X by L_{r}(x)=r(x). Let \mathcal{C}_{Reg} be the set of all regular constraints. A patterns with regular constraints (\alpha,r_{\alpha})\in(\Sigma\cup X)^{*}\times\mathcal{C}_{Reg} is a pattern which is associated with a regular constraint. A substitution h is r_{\alpha}-valid if all variables are substituted according to r_{\alpha}. The language of (\alpha,r_{\alpha}) is defined analogously to pattern languages with the additional requirement that all substitutions must be r_{\alpha}-valid. This paper examines erasing (E) and non-erasing (NE) pattern languages with regular constraints. The membership problem for both is NP-complete, while the inclusion problem is undecidable for the general and terminal-free versions. This immediately follows from known results. The main finding of this paper is that the equivalence problem for erasing pattern languages with regular constraints is indeed undecidable."
https://arxiv.org/html/2411.06498v1,Barriers to Complexity-Theoretic Proofs that Achieving AGI Using Machine Learning is Intractable,"A recent paper [VRGA+24] claims to have proved that achieving human-like intelligence using learning from data is intractable in a complexity-theoretic sense. We point out that the proof relies on an unjustified assumption about the distribution of (input, output) pairs in the data. We briefly discuss that assumption in the context of two fundamental barriers to repairing the proof: the need to precisely define “human-like,” and the need to account for the fact that a particular machine learning system will have particular inductive biases that are key to the analysis. Another attempt to repair the proof, by focusing on subsets of the data, faces barriers in terms of defining the subsets.","In [VRGA+24] a claim is made that the authors “formally prove [in the paper that] creating systems with human(-like or -level) cognition (“AGI” for short, for the purposes of this paper) is intrinsically computationally intractable.” Here, we show that the paper falls short of formally proving the claim. We identify a key unproven premise that underlies the proof: that the distribution \mathcal{D} of tuples (s,b), with s denoting “situation” and b denoting “[human] behavior” in response to s can be an arbitrary (polytime-computable) distribution. If \mathcal{D} is a model of human behavior, both the marginal distribution of s and the conditional distribution P_{\mathcal{D}}(b|s) are in fact highly structured. For example, if s encodes natural images, the marginal distribution P_{\mathcal{D}}(s) would need to account for the hierarchical structure of natural images [SO01]. If P_{\mathcal{D}}(b|s) models human chess moves, the distribution would need to account (among other things) for the rules of chess. This means that many \mathcal{D}’s can be ruled out a-priori. As we argue below, the fact that, in the authors’ proof, \mathcal{D} denotes both the distribution of situation-behavior tuples and an arbitrary distribution means that the authors did not prove what they set out to prove. We argue that two critical issues must be resolved when attempting to repair the proof (although we make no claim that the proof could be repaired). • “Human-like” behavior must be defined precisely . • The fact that while an arbitrary function is not learnable due to No-Free-Lunch-Theorem-style results, some structured functions can be learned with appropriate inductive biases must be considered. Another attempt to repair the proof by focusing on subsets of the data also face a barrier. The paper is organized as follows. in Section 2 we introduce the Ingenia Theorem of [VRGA+24], along with the necessary context. In Section 3 we point out what we believe to be a central flaw in the proof. In Section 4, we identify what we see as the challenges that a correct version of the proof would have to overcome. We illustrate that not having met one of the challenges leaves the current proof vulnerable to a reductio ad absurdum argument (Section 4.1.1)."
https://arxiv.org/html/2411.07030v1,Hyperplanes Avoiding Problem and Integer Points Counting in Polyhedra,"In our work, we consider the problem of computing a vector x\in\operatorname{\mathbb{Z}}^{n} of minimum \norm{\cdot}_{p}-norm such that a^{\top}x\not=a_{0}, for any vector (a,a_{0}) from a given finite set \operatorname{\mathcal{A}}\subseteq\operatorname{\mathbb{Z}}^{n}. In other words, we search for a vector of minimum norm that avoids a given finite set of hyperplanes, which is natural to call as the Hyperplanes Avoiding Problem. This problem naturally appears as a subproblem in Barvinok-type algorithms for counting integer points in polyhedra. More precisely, it appears when one needs to evaluate certain rational generating functions in an avoidable critical point.We show that:With respect to \norm{\cdot}_{1}, the problem admits a feasible solution x with \norm{x}_{1}\leq(m+n)/2, where m=\abs{\operatorname{\mathcal{A}}}, and show that such solution can be constructed by a deterministic polynomial-time algorithm with O(n\cdot m) operations. Moreover, this inequality is the best possible. This is a significant improvement over the previous randomized algorithm, which computes x with a guaranty \norm{x}_{1}\leq n\cdot m. The original approach of A. Barvinok can guarantee only \norm{x}_{1}=O\bigl{(}(n\cdot m)^{n}\bigr{)};The problem is \operatorname{N\!P}-hard with respect to any norm \norm{\cdot}_{p}, for p\in\bigl{(}\operatorname{\mathbb{R}}_{\geq 1}\cup\{\infty\}\bigr{)}.As an application, we show that the problem to count integer points in a polytope \operatorname{\mathcal{P}}=\{x\in\operatorname{\mathbb{R}}^{n}\colon Ax\leq b\}, for given A\in\operatorname{\mathbb{Z}}^{m\times n} and b\in\operatorname{\mathbb{Q}}^{m}, can be solved by an algorithm with O\bigl{(}\nu^{2}\cdot n^{3}\cdot\Delta^{3}\bigr{)} operations, where \nu is the maximum size of a normal fan triangulation of \operatorname{\mathcal{P}}, and \Delta is the maximum value of rank-order subdeterminants of A. It refines the previous state-of-the-art O\bigl{(}\nu^{2}\cdot n^{4}\cdot\Delta^{3}\bigr{)}-time algorithm.","Let \operatorname{\mathcal{A}}\subseteq\operatorname{\mathbb{Z}}^{n+1} be a set of pairs (a,a_{0}) with a\in\operatorname{\mathbb{Z}}^{n}\setminus\{\operatorname{\mathbf{0}}\} and a_{0}\in\operatorname{\mathbb{Z}}, and denote m:=\abs{\operatorname{\mathcal{A}}}<\infty. Consider the system \begin{cases}a^{\top}\cdot x\not=a_{0},\quad\forall(a,a_{0})\in\operatorname{% \mathcal{A}}\\ x\in\operatorname{\mathbb{Z}}^{n}.\end{cases} (HyperplanesAvoiding) The system (HyperplanesAvoiding) has infinitely many solutions, and it is interesting to find solutions having small norm (we are mainly interested in the \norm{\cdot}_{1}-norm). The latter motivates the following problem, which is natural to call the Hyperplanes Avoiding Problem: \displaystyle\norm{x}_{p}\to\min \displaystyle\begin{cases}a^{\top}\cdot x\not=a_{0},\quad\forall(a,a_{0})\in% \operatorname{\mathcal{A}}\\ x\in\operatorname{\mathbb{Z}}^{n}.\end{cases} (p-HyperplanesAvoiding) In other words, we are just trying to find an integer vector of the smallest norm that does not lie in any of the m given hyperplanes. It is also interesting to consider the Homogeneous forms of the system (HyperplanesAvoiding) and problem (p-HyperplanesAvoiding), when a_{0}=0 for any (a,a_{0})\in\operatorname{\mathcal{A}}. In this case, we are trying to find an integer vector of the smallest norm that does not lie in any of the m given (n-1)-dimensional subspaces. 1.1 Motivation: The integer Points Counting in Polyhedra The problem (p-HyperplanesAvoiding) naturally appears as a subproblem in algorithms for integer points counting in polyhedra. Let us give a brief sketch of how it appears. Consider a rational polytope \operatorname{\mathcal{P}} defined by a system of linear inequalities. The seminal work of A. Barvinok [5] (see also [3, 4]) proposes an algorithm to count the number of points inside \operatorname{\mathcal{P}}\cap\operatorname{\mathbb{Z}}^{n}, which is polynomial for a fixed dimension. His approach is based on a representation of \operatorname{\mathcal{P}}\cap\operatorname{\mathbb{Z}}^{n} via some rational generating function. More precisely, the Barvinok’s algorithm computes a set of indices \operatorname{\mathcal{I}}, and for each i\in\operatorname{\mathcal{I}}, it computes a number \epsilon^{(i)}\in\operatorname{\mathbb{Z}} and vectors v^{(i)},u_{1}^{(i)},\dots,u_{n}^{(i)}\in\operatorname{\mathbb{Z}}^{n} such that \sum\limits_{x\in\operatorname{\mathcal{P}}\cap\operatorname{\mathbb{Z}}^{n}}z% ^{x}=f_{\operatorname{\mathcal{P}}}(z):=\sum\limits_{i\in\operatorname{% \mathcal{I}}}\epsilon^{(i)}\cdot\frac{z^{v^{(i)}}}{\bigl{(}1-z^{u_{1}^{(i)}}% \bigr{)}\cdot\ldots\cdot\bigl{(}1-z^{u_{n}^{(i)}}\bigr{)}}. (1) Here, the notation z^{x} means z^{x}=z_{1}^{x_{1}}\cdot\ldots\cdot z_{n}^{x_{n}}. The right-hand-side of (1), i.e. the function f_{\operatorname{\mathcal{P}}}(z), is called the short rational generating function of \operatorname{\mathcal{P}}\cap\operatorname{\mathbb{Z}}^{n}. Since the left part of (1) is a finite sum, the point z=\operatorname{\mathbf{1}} is an avoidable critical point of f_{\operatorname{\mathcal{P}}}(z). Therefore, \abs{\operatorname{\mathcal{P}}\cap\operatorname{\mathbb{Z}}^{n}}=\lim\limits_% {z\to\operatorname{\mathbf{1}}}f_{\operatorname{\mathcal{P}}}(z). (2) One possible approach to find this limit, is to compute a vector c\in\operatorname{\mathbb{Z}}^{n} such that c^{\top}u^{(i)}_{j}\not=0, for any i\in\operatorname{\mathcal{I}} and j\in\left\{1,\dots,n\right\}. Note that c is a solution of the system (HyperplanesAvoiding) with \operatorname{\mathcal{A}}=\bigl{\{}u^{(i)}_{j}\bigr{\}}, and m=\abs{\operatorname{\mathcal{A}}}=(n+1)\cdot\abs{\operatorname{\mathcal{I}}}. Using the substitution z_{i}\to e^{\tau\cdot c_{i}}, the function f_{\operatorname{\mathcal{P}}}(z) transforms to the function \hat{f}_{\operatorname{\mathcal{P}}}(\tau), depending on the single complex variable \tau, defined by \hat{f}_{\operatorname{\mathcal{P}}}(\tau)=\sum\limits_{i\in\operatorname{% \mathcal{I}}}\epsilon^{(i)}\cdot\frac{e^{\langle c,v^{(i)}\rangle\cdot\tau}}{% \bigl{(}1-e^{\langle c,u_{1}^{(i)}\rangle\cdot\tau}\bigr{)}\cdot\ldots\cdot% \bigl{(}1-e^{\langle c,u_{n}^{(i)}\rangle\cdot\tau}\bigr{)}}. (3) Now, since \hat{f}_{\operatorname{\mathcal{P}}} is analytical, the limit (2) just equals to the [\tau^{0}]-term of the Tailor’s series for \hat{f}_{\operatorname{\mathcal{P}}}(\tau): \abs{\operatorname{\mathcal{P}}\cap\operatorname{\mathbb{Z}}^{n}}=\lim\limits_% {\tau\to 0}\hat{f}_{\operatorname{\mathcal{P}}}(\tau)=[\tau^{0}]\hat{f}_{% \operatorname{\mathcal{P}}}. We note that it is preferable to calculate the vector c satisfying c^{\top}u^{(i)}_{j}\not=0 with the smallest possible norm, because it will reduce the size of the numbers \langle c,v^{(i)}\rangle and \langle c,u_{j}^{(i)}\rangle, which in turn will speed up practical computations. However, the norm of c does not affect the computational complexity in terms of the number of operations. It only reduces the variable sizes. Assuming that the polyhedron \operatorname{\mathcal{P}} is defined by a system Ax\leq b, for given A\in\operatorname{\mathbb{Z}}^{m\times n} and b\in\operatorname{\mathbb{Q}}^{m}, the computational complexity of the Barvinok’s algorithm in terms of operations number can be bounded by \nu\cdot\bigl{(}O(\log\Delta)\bigr{)}^{n\ln n}, (4) where \nu is the maximum size of a normal fan triangulation of \operatorname{\mathcal{P}}, and \Delta is the maximum value of the rank-order subdeterminants of A. Thus, finding a good solution to (HyperplanesAvoiding) has only effect on the variable sizes of the Barvinok’s algorithm. However, there is an alternative algorithmic approach to integer point counting, which allows obtaining complexity bounds of the type \operatorname{poly}(\nu,n,\Delta). It was developed in a series of works [10, 11, 8, 9, 7].111For the latest perspective see [9], for the parametric case see [7], the paper [8] is a correction of [11]. In this alternative approach, the norm of the solution to (HyperplanesAvoiding) is a multiplicative factor in the bound on its computational complexity. More precisely, the following result was obtained in [9]. Proposition 1 (D. Gribanov, I. Shumilov, D. Malyshev & N. Zolotykh [9]) Assume that, for any collection \operatorname{\mathcal{A}} of vectors of size m, there exists a solution x of the system (HyperplanesAvoiding) with \norm{x}_{1}\leq L(m,n). Assume additionally that such x can be calculated for free. Then the number \abs{\operatorname{\mathcal{P}}\cap\operatorname{\mathbb{Z}}^{n}} can be calculated with O\bigl{(}\nu\cdot L(\nu\cdot n,n)\cdot n^{2}\cdot\Delta^{3}\bigr{)}\quad\text{% operations}. It was shown in [9] that L(m,n)\leq n\cdot m, and such x can be constructed by a randomized polynomial-time algorithm with O(n\cdot m) operations. It means that the counting complexity can be estimated by O\bigl{(}\nu^{2}\cdot n^{4}\cdot\Delta^{3}\bigr{)}. In the current paper, we show that L(m,n)\leq(m+n)/2, and such x can be constructed by a deterministic O(n\cdot m)-time algorithm. The latter yields the counting complexity O\bigl{(}\nu^{2}\cdot n^{3}\cdot\Delta^{3}\bigr{)}, which is the main application of our results. Additionally, we hope that our result can significantly accelerate the evaluation part of the Barvinok-type algorithms. To this end, we propose some experimental results showing that the new algorithm constructs solutions of significantly lower norm than random sampling in a cross-polytope, see Section Experimental Evaluation. Finally, we note that this paper is not considering the dual-type algorithms for counting integer points in polyhedra. A great survey of this approach could be found in the book [12] of J. Lasserre. 1.2 Complexity Model Assumptions All the algorithms that are considered in our work correspond to the Word-RAM computational model. In other words, we assume that additions, subtractions, multiplications, and divisions with rational numbers of the specified size, which is called the word size, can be done in O(1)-time. In our work, we chose the word size to be equal to some fixed polynomial on the input size of the corresponding computational problem. More precisely, considering the problem (p-HyperplanesAvoiding), we assume that the input size is bounded by n\cdot m\cdot(1+\lceil\log_{2}\alpha\rceil), where \alpha is the maximum absolute value of coordinates of a, for a\in\operatorname{\mathcal{A}}. 1.3 Main Results and Related Work Let us summarize our results below. 1. With respect to \norm{\cdot}_{1}, the problem (p-HyperplanesAvoiding) admits a feasible solution x with \norm{x}_{1}\leq(m+n)/2, where m=\abs{\operatorname{\mathcal{A}}}, and we show that such solution can be constructed by a deterministic polynomial-time algorithm with O(n\cdot m) operations, see Theorem 2.3 of Section Approximate Solution via Combinatorial Nullstellensatz. The inequality is the best possible, see the discussion afterward. This is a significant improvement over the previous O(n\cdot m)-time randomized algorithm of [9], which computes x with a guaranty \norm{x}_{1}\leq n\cdot m. In contrast, the original approach of A. Barvinok searches x in the form x=(1,t,t^{2},\dots,t^{n-1}). Since, for each a\in\operatorname{\mathcal{A}}, a^{\top}x=\sum_{i=1}^{n}a_{i}\cdot t^{i-1} is a polynomial of degree at most n-1, there exists a suitable t with t\leq n\cdot m. However, this reasoning can guaranty only \norm{x}_{1}=O\bigl{(}(n\cdot m)^{n}\bigr{)}. 2. For any p\in\bigl{(}\operatorname{\mathbb{R}}_{\geq 1}\cup\{\infty\}\bigr{)}, the problem (p-HyperplanesAvoiding) is \operatorname{N\!P}-hard with respect to any norm \norm{\cdot}_{p}, even in its homogeneous form. See Theorem 3.1 of Section Computational Complexity of the Exact Solution; 3. We show that the problem to calculate the value \abs{\operatorname{\mathcal{P}}\cap\operatorname{\mathbb{Z}}^{n}} for a polyhedron \operatorname{\mathcal{P}} defined by the system Ax\leq b, for agiven A\in\operatorname{\mathbb{Z}}^{m\times n} and b\in\operatorname{\mathbb{Q}}^{m}, can be solved with O\bigl{(}\nu^{2}\cdot n^{3}\cdot\Delta^{3}\bigr{)} operations, where \nu is the maximum size of a normal fan triangulation of \operatorname{\mathcal{P}}, and \Delta is the maximum value of rank-order subdeterminants of A. It refines the O\bigl{(}\nu^{2}\cdot n^{4}\cdot\Delta^{3}\bigr{)}-time algorithm of [9]. See Subsection Motivation: The integer Points Counting in Polyhedra, more specifically, see the discussion alongside Proposition 1; It is easy to see that the guaranty \norm{x}_{1}\leq(m+n)/2 on an existing solution x of the system (HyperplanesAvoiding) is the best possible. Proposition 2 There exists a family of systems (HyperplanesAvoiding) such that \norm{x}_{1}\geq(m+n)/2 for any solution x. Proof Fix some positive integer k. The desired system consists of the constraints x_{i}\not=j, for any i\in\left\{1,\dots,n\right\} and j\in\left\{-k,\dots,k\right\}. So, the total number of constraints is m=(2k+1)\cdot n. It is easy to see that \abs{x_{i}}\geq k+1, for any i\in\left\{1,\dots,n\right\} and any solution x of the system. Therefore, \norm{x}_{1}\geq(k+1)\cdot n=(m+n)/2. However, for the homogeneous form of the system (HyperplanesAvoiding), the asymptotics of the solution quality with respect to the parameter m can be slightly improved. This observation is based on the following result of I. Bárány, G. Harcos, J. Pach, & G. Tardos [2]. Let \operatorname{\mathbb{B}}_{1} be the unit ball with respect to \norm{x}_{1} and g(r) be a minimal number of subspaces needed to cover all points of the set r\cdot\operatorname{\mathbb{B}}_{1}\cap\operatorname{\mathbb{Z}}^{n}. Theorem 1.1 (I. Bárány, G. Harcos, J. Pach, & G. Tardos [2]) There exist absolute constants C_{1} and C_{2} such that C_{1}\cdot\frac{1}{n^{2}}\cdot r^{\frac{n}{n-1}}\leq g(r)\leq C_{2}\cdot 2^{n}% \cdot r^{\frac{n}{n-1}}. Note that the original work [2] contains a more general result concerning arbitrary convex bodies in \operatorname{\mathbb{R}}^{n}, albeit with a worse dependence on n. The Theorem 1.1 is a straightforward adaptation of the original proof to the case of \operatorname{\mathbb{B}}_{1}. As a corollary, it follows that the system (HyperplanesAvoiding) always has a solution with an asymptotics that is slightly better in m, but worse in n. Corollary 1 The system (HyperplanesAvoiding) has a solution x, such that \norm{x}_{1}=O\bigl{(}n^{2}\cdot m^{\frac{n-1}{n}}\bigr{)}. At the same time, the theorem implies that solutions of significantly smaller norm do not exist in general. In particular, it implies that our constructive bound \norm{x}_{1}\leq(m+n)/2 is almost optimal with respect to m even in the homogeneous case. Corollary 2 There exists a system (HyperplanesAvoiding) such that, for any solution x, \norm{x}_{1}=\Omega\bigl{(}\frac{1}{2^{n}}\cdot m^{\frac{n-1}{n}}\bigr{)}."
https://arxiv.org/html/2411.06569v1,Randomized Black-Box PIT for Small Depth +-Regular Non-commutative Circuits,"In this paper, we address the black-box polynomial identity testing (PIT) problem for non-commutative polynomials computed by +-regular circuits, a class of homogeneous circuits introduced by Arvind, Joglekar, Mukhopadhyay, and Raja (STOC 2017, Theory of Computing 2019). These circuits can compute polynomials with a number of monomials that are doubly exponential in the circuit size. They gave an efficient randomized PIT algorithm for +-regular circuits of depth 3 and posed the problem of developing an efficient black-box PIT for higher depths as an open problem.We present a randomized black-box polynomial-time algorithm for +-regular circuits of any constant depth. Specifically, our algorithm runs in s^{O(d^{2})} time, where s and d represent the size and the depth of the +-regular circuit, respectively. Our approach combines several key techniques in a novel way. We employ a nondeterministic substitution automaton that transforms the polynomial into a structured form and utilizes polynomial sparsification along with commutative transformations to maintain non-zeroness. Additionally, we introduce matrix composition, coefficient modification via the automaton, and multi-entry outputs—methods that have not previously been applied in the context of black-box PIT. Together, these techniques enable us to effectively handle exponential degrees and doubly exponential sparsity in non-commutative settings, enabling polynomial identity testing for higher-depth circuits. Our work resolves an open problem from [AJMR19].In particular, we show that if f is a non-zero non-commutative polynomial in n variables over the field \mathbb{F}, computed by a depth-d +-regular circuit of size s, then f cannot be a polynomial identity for the matrix algebra \mathbb{M}_{N}(\mathbb{F}), where N=s^{O(d^{2})} and the size of the field \mathbb{F} depending on the degree of f. Our result can be interpreted as an Amitsur-Levitzki-type result [AL50] for polynomials computed by small-depth +-regular circuits.","The non-commutative polynomial ring, denoted by \mathbb{F}\mathopen{\langle}X\mathclose{\rangle}, over a field \mathbb{F} in non-commuting variables X, consists of non-commuting polynomials in X. These are just \mathbb{F}-linear combinations of words (we call them monomials) over the alphabet X=\{x_{1},\ldots,x_{n}\}. Hyafil [Hya77] and Nisan [Nis], studied the complexity of non-commutative arithmetic computations, in particular the complexity of computing the determinant polynomial with non-commutative computations. Non-commutative arithmetic circuit families compute non-commutative polynomial families in a non-commutative polynomial ring \mathbb{F}\mathopen{\langle}X\mathclose{\rangle}, where multiplication is non-commutative (i.e., for distinct x,y\in X, xy\neq yx). We now recall the formal definition of non-commutative arithmetic circuits. Definition 1.0.1 (Non-commutative arithmetic circuits). A non-commutative arithmetic circuit C over a field \mathbb{F} is a directed acyclic graph such that each in-degree 0 node of the graph is labelled with an element from X\cup\mathbb{F}, where X=\{x_{1},x_{2},\ldots,x_{n}\} is a set of noncommuting variables. Each internal node has fan-in two and is labeled by either (+) or (\times) – meaning a + or \times gate, respectively. Furthermore, each \times gate has a designated left child and a designated right child. Each gate of the circuit inductively computes a polynomial in \mathbb{F}\mathopen{\langle}X\mathclose{\rangle}: the polynomials computed at the input nodes are the labels; the polynomial computed at a + gate (respectively \times gate) is the sum (respectively product in left-to-right order) of the polynomials computed at its children. The circuit C computes the polynomial at the designated output node. An arithmetic circuit is a formula if the fan-out of every gate is at most one. Designing an efficient deterministic algorithm for non-commutative polynomial identity testing is a major open problem. Let f\in\mathbb{F}\mathopen{\langle}X\mathclose{\rangle} be a polynomial represented by a non-commutative arithmetic circuit C. In this work, we assume that the polynomial f is given by a black-box access to C, meaning we can evaluate the polynomial f on matrices with entries from \mathbb{F} or an extension field. Note that the degree of an n-variate polynomial computed by the circuit C of size s can be as large as 2^{s} and the sparsity, i.e., the number of non-zero monomials, can be as large as n^{2^{s}}. For example, the non-commutative polynomial (x+y)^{2^{s}} has degree 2^{s}, doubly exponential sparsity 2^{2^{s}}, and has a circuit of size O(s). The classical result of Amitsur-Levitzki [AL50] shows that a non-zero non-commutative polynomial f of degree 2d-1 does not vanish on the matrix algebra \mathcal{M}_{d}(\mathbb{F}). Bogdanov and Wee [BW05] have given an efficient randomized PIT algorithm for non-commutative circuits computing polynomials of degree d=poly(s,n). Their algorithm is based on the result of Amitsur-Levitzki [AL50], which states the existence of matrix substitutions M=(M_{1},M_{2},\ldots,M_{n}) such that the matrix f(M_{1},M_{2},\ldots,M_{n}) is not the zero matrix, where the dimension of the matrices in M depends linearly on the degree d of the polynomial f. Since the degree of the polynomial computed by circuit C can be exponentially large in the size of the circuit, their approach will not work directly. Finding an efficient randomized PIT algorithm for general non-commutative circuits is a well-known open problem. It was highlighted at the workshop on algebraic complexity theory (WACT 2016) as one of the key problems to work on. Recently, [AJMR19] gave an efficient randomized algorithm for the PIT problem when the circuits are allowed to compute polynomials of exponential degree, but the sparsity could be exponential in the size of the circuit. To handle doubly-exponential sparsity, they studied a class of homogeneous non-commutative circuits, that they call +-regular circuits, and gave an efficient deterministic white-box PIT algorithm. These circuits can compute non-commutative polynomials with the number of monomials doubly exponential in the circuit size. For the black-box setting, they obtain an efficient randomized PIT algorithm only for depth-3 +-regular circuits. In particular, they show that if a non-zero non-commutative polynomial f\in\mathbb{F}\mathopen{\langle}X\mathclose{\rangle} is computed by a depth-3 +-regular circuit of size s, then f cannot be a polynomial identity for the matrix algebra \mathbb{M}_{s}(\mathbb{F}) for a sufficiently large field \mathbb{F}. Finding an efficient randomized PIT algorithm for higher depth +-regular circuits is listed as an interesting open problem. We resolve this problem for constant depth +-regular circuits. In particular, we show that if f\in\mathbb{F}\mathopen{\langle}X\mathclose{\rangle} is a non-zero non-commutative polynomial computed by a depth-d +-regular circuit of size s, then f cannot be a polynomial identity for the matrix algebra \mathbb{M}_{N}(\mathbb{F}), with N=s^{O(d^{2})} and the size of the field \mathbb{F} depends on the degree of polynomial f. This resolves an open problem given in [AJMR19]. We note that we get a black-box randomized polynomial time black-box PIT algorithm for constant depth +-regular circuits. Our results We consider the black-box PIT problem for +-regular circuits. These are a natural subclass of homogeneous non-commutative circuits and these circuits can compute polynomials of exponential degree and a double-exponential number of monomials. Recall that a polynomial f is homogeneous if all of its monomials have the same degree. The syntactic degree is inductively defined as follows: For a leaf node labeled by a variable, the syntactic degree is 1, and 0 if it is labeled by a constant. For a + gate, its syntactic degree is the maximum of the syntactic degree of its children. For a \times gate, its syntactic degree is the sum of the syntactic degree of its children. A circuit is called homogeneous if all gates in the circuit compute homogeneous polynomials. Now we recall the definition and some observations of +-regular circuits from [AJMR19]. Definition 1.0.2 (+-regular circuits [AJMR19]). A non-commutative circuit C, computing a polynomial in \mathbb{F}\mathopen{\langle}X\mathclose{\rangle}, where X=\{x_{1},x_{2},\ldots,x_{n}\}, is +-regular if it satisfies the following properties: 1. The circuit is homogeneous. The + gates are of unbounded fanin and \times gates are of fanin 2. 2. The + gates in the circuit are partitioned into layers (termed +-layers) such that if g_{1} and g_{2} are + gates in the same +-layer then there is no directed path in the circuit between g_{1} and g_{2}. 3. All gates in a +-layer have the same syntactic degree. 4. The output gate is a + gate. 5. Every input-to-output path in the circuit goes through a gate in each +-layer. 6. Additionally, we allow scalar edge labels in the circuit. For example, suppose g is a + gate in C whose inputs are gates g_{1},g_{2},\ldots,g_{t} such that \beta_{i}\in\mathbb{F} labels edge (g_{i},g),i\in[t]. If polynomial P_{i} is computed at gate g_{i},i\in[t], then g computes the polynomial \sum_{i=1}^{t}\beta_{i}P_{i}. The +-depth, denoted by d^{+}, refers to the number of + layers in C. The + layers in circuit C are numbered from the bottom upwards. For i\in[d], let \mathcal{L}_{i}^{+} represent the i-th layer of addition (+) gates, and let \mathcal{L}_{i}^{\times} represent the i-th layer of multiplication (\times) gates that are inputs to the addition gates in \mathcal{L}_{i}^{+}. It’s important to note that all gates in \mathcal{L}_{i}^{\times} and \mathcal{L}_{i}^{+} have the same syntactic degree. The sub-circuit in C between any two consecutive addition layers \mathcal{L}_{i}^{+} and \mathcal{L}_{i+1}^{+} consists of multiplication gates and is denoted by \Pi^{*}. The inputs of this sub-circuit come from layer \mathcal{L}_{i}^{+}. Let \mathcal{L}_{i+1}^{\times} consist of all output gates of this sub-circuit, where 1\leq i\leq d-1. Note that all the gates of \mathcal{L}_{i+1}^{\times} are product gates. It is important to note that this sub-circuit depth, which is the number of gates in any input-to-output gate path in the sub-circuit, can be arbitrary and is only bounded by the size of the circuit C. The bottom-most \times-layer \mathcal{L}_{1}^{\times} can be assumed without loss generality to be the input variables and gates in \mathcal{L}_{1}^{+} compute homogeneous linear forms. Remark 1.0.1. If the top layer is \Pi^{*} (i.e., the output gate is a \times gate), we can add an extra + gate at the top with the + gate having a single input (i.e., fan-in 1). This ensures that the top layer is a \Sigma layer. If the bottom layer is \Pi^{*}, then for each input variable, we can add a sum gate having a single input. This will increase the circuit size by at most n+1, where n is the number of input variables. This allows us to assume that in +-regular circuits, both the top and bottom layers are \Sigma layers. This is done to simplify the analysis. The size of the +-regular circuit is the number of gates in the circuit. As noted earlier, the non-commutative polynomial (x+y)^{2^{s}} can be computed by a depth-3 +-regular circuit, denoted by \Sigma\Pi^{*}\Sigma, of size O(s) using repeated squaring. This circuit consists of two addition layers, namely \mathcal{L}_{1}^{+},\mathcal{L}_{2}^{+} and two multiplication layers, namely \mathcal{L}_{1}^{\times},\mathcal{L}_{2}^{\times}. The multiplication layer \mathcal{L}_{1}^{\times} consists of only the two input gates labeled by x and y respectively. The addition layer \mathcal{L}_{1}^{+} consists of only one addition gate computing the homogeneous linear form (x+y). The multiplication layer \mathcal{L}_{2}^{\times} consists of only one gate computing the polynomial (x+y)^{2^{s}}. The addition layer \mathcal{L}_{2}^{+} consists of only the ouptut gate computing the polynomial (x+y)^{2^{s}}. In this example, the top-most addition gate (i.e., the output gate) essentially has one input. The main result of the paper is the following theorem. Theorem 1. Let f be a non-commutative polynomial of degree D over X=\{x_{1},\ldots,x_{n}\} computed by a +-regular circuit of depth d and size s. Then f\not\equiv 0 if and only if f is not identically zero on the matrix algebra \mathbb{M}_{N}(\mathbb{F}), with N=s^{O(d^{2})} and \mathbb{F} is sufficiently large. For degree D non-zero non-commutative polynomial f, the classical Amitsur-Levitzki [AL50] theorem guarantees that f does not vanish on the matrix algebra \mathcal{M}_{\frac{D}{2}+1}(\mathbb{F}). If D=2^{\Omega(s)}, this gives us an exponential time randomized PIT algorithm [BW05], where s is the size of the circuit computing f. If the sparsity of the polynomial, i.e., the number of non-zero monomials, is doubly exponential, then the main result of [AJMR19] gives only an exponential time randomized PIT algorithm as their matrix dimension depends on the logarithm of the sparsity. This above theorem demonstrates that if the polynomial f is computed by a +-regular circuit of size s and depth o(\sqrt{s}/\log s), we can determine if f is identically zero or not using a 2^{o(s)} time randomized PIT algorithm, which is exponentially faster than the existing methods. In particular, if depth is O(1) then our algorithm runs in polynomial time. It is important to note that the number of product gates (within each \Pi^{*} layers) in any input-to-output path can be arbitrary and is only bounded by the circuit size s. We note that [AJMR19] presented a white-box deterministic polynomial-time PIT for arbitrary depth +-regular circuits. For the small-degree case, [RS05] provided a white-box deterministic polynomial-time PIT for non-commutative ABPs, while [FS13, AGKS15] have shown a quasi-polynomial-time black-box PIT algorithm for non-commutative ABPs. 1.1 Outline of the Proofs: High-level Idea In the rest of the paper, we will use ”n.c.” as an abbreviation for ”non-commutative”. We first explain the main ideas behind the randomized PIT algorithm for depth-5 +-regular circuits, as this is the major bottleneck. Consider an n.c. polynomial f over X=\{x_{1},\ldots,x_{n}\} computed by a \Sigma\Pi^{*}\Sigma\Pi^{*}\Sigma circuit of size s. The polynomial f, computed by a \Sigma\Pi^{*}\Sigma\Pi^{*}\Sigma circuit of size s, can be written as follows: f=\sum_{i\in[s]}\prod_{j\in[D_{2}]}Q_{ij}. (1) Here, the degree of each Q_{ij}, i\in[s],j\in[D_{2}], is denoted by D_{1} and can be computed by a \Sigma\Pi^{*}\Sigma circuit of size at most s. As the size of the circuit is s, the output gate’s fan-in is bounded by s. The syntactic degree D of the circuit can be expressed as D=D_{1}\times D_{2}. In general, both D_{1} and D_{2} can be exponential in s. Note that each Q_{ij} is a polynomial computed at layer \mathcal{L}_{2}^{+}. One natural idea is that since each Q_{ij} can be computed by a \Sigma\Pi^{*}\Sigma circuit, we can try to use the known result of depth-3 +-regular circuits [AJMR19] and convert the given polynomial f into a commutative polynomial, and then perform the randomized PIT using the Polynomial Identity Lemma for commutative polynomials (also known as the DeMillo-Lipton-Schwartz-Zippel Lemma [DL78, Zip79, Sch80]). Recall that in [AJMR19], the given polynomial f is computed by a depth-3 +-regular circuit of size s. That is, f is a sum of products of homogeneous linear forms. Formally, f=\sum\limits_{i=1}^{s}P_{i}, where for all i\in[s],P_{i}=L_{i,1}\cdots L_{i,D} and D could be exponential in s. They show that there exists an index set I\subseteq[D] of size at most s-1 such that by considering only those linear forms positions indexed by I as n.c. and the remaining as commutative, the non-zeroness of f is preserved. This fact is crucially used in their black-box identity-testing algorithm for depth-3 +-regular circuits. In our depth-5 setting, that is, when f=\sum\limits_{i\in[s]}\prod\limits_{j\in[D_{2}]}Q_{ij}, it is only natural to wonder if there exists such a small index. Note that the number of Q_{ij} polynomials in each product, denoted by D_{2}, can be exponential, in general. It is generally impossible to have a small index set of polynomial size. This is because if the index set is only polynomial in size, then as a result, no variables in some of the Q_{ij} are considered non-commutative. Thus, these Q_{ij} are considered commutative, possibly resulting in the commutative polynomial becoming 0. We cannot consider a n.c. polynomial as commutative and still maintain non-zeroness, in general. To resolve this problem, we convert the n.c. polynomial into a commutative polynomial in several steps, utilizing the fact that a +-regular circuit computes it. 1.1.1 Step 1: Transforming the Polynomial for Improved Structure In this step, we show that the polynomial can be converted into a more structured polynomial at the cost of introducing some spurious monomials. We show that in each Q_{ij} there is a small index set such that by considering only those homogeneous linear forms appearing in that Q_{ij} as n.c. and the remaining as commutative, the non-zeroness of f is preserved. The index sets are encoded using n.c. variables. This results in an exponential-sized index set. This is because the number of terms in the product, denoted by D_{2}, can be exponential in general and each term in the product has a small index set. This fact is formalized in Lemma 4.1.1. However, this fact alone will not be sufficient to design an identity-testing algorithm for depth-5 +-regular circuits. This is because, to choose a small index set from each Q_{ij}, one would have to know the boundary between Q_{ij} and Q_{i,j+1} (i.e., the position at which Q_{ij} ends and Q_{i,j+1} begins), for any i\in[s],j\in[D_{2}-1]. To know where Q_{ij} ends and Q_{i,j+1} begins exactly, the algorithm will have to keep track of the degree (length) of Q_{ij}. This is not feasible as the degree of Q_{ij} could be exponential in s. This is one of the main challenges in designing a black-box identity-testing algorithm for depth-5 +-regular circuits and is one of the main differences between depth-3 and depth-5 +-regular circuits (small depth +-regular circuits, in general). To address this issue, we first convert each Q_{ij} into a more structured n.c. polynomial, which we refer to as a k-ordered power-sum polynomial (see Definition 4.0.1), where k is the number of variables. This new n.c. polynomial has the property that we can consider it as a commutative polynomial preserving non-zeroness (Claim 4.0.1). Note that when the variables in any n.c. linear form is considered commutative, then non-zeroness is preserved. However, this may not be true for n.c. polynomials, in general. For example, the polynomial (xy-yx) is 0 if x and y are commuting variables. This property (Claim 4.0.1) alone will not let us consider the whole polynomial as commutative. This is because we are dealing with the sum of the product of Q_{ij} polynomials and the number of terms in the product in general could be exponential. Thus, we can not use a fresh set of variables for each k-ordered power-sum polynomial as this will have exponentially many new variables, in general. Because of this, we use the same k variables to convert each Q_{ij} in the product to a k-ordered power-sum polynomial, instead of using a fresh set of k variables for each Q_{ij}. If we simply consider the resulting product as commutative, then variables that belong to k-ordered power-sum polynomials of different Q_{i,j_{1}} and Q_{i,j_{2}} in the product, could mix (i.e., exponents of the same variable appearing in different k-ordered power-sum polynomials gets added). As a result, we cannot guarantee that the resulting commutative polynomial will preserve non-zeroness. To address this issue, we will not be considering the modified polynomial as commutative at this stage. Instead, we will transform each Q_{ij} polynomial into a more structured n.c. polynomial, which we denote by \hat{Q}_{ij}, by introducing a new set of n.c. variables. We show that this transformation preserves non-zeroness. The substitution automaton we use for the first step generates some spurious monomials along with a structured polynomial (Claim 4.1.4). The spurious monomials are produced because we cannot definitively identify the boundaries of each Q_{ij} polynomial. We can consider spurious monomials as noise generated by our approach. Let F_{1} be the sum of all spurious monomials produced and \hat{f}_{1} be the structured polynomial resulting from this transformation. We will prove that \hat{f}_{1}+F_{1}\not\equiv 0 (see Lemma 4.1.1 and Claim 4.1.5). The key to this proof lies in demonstrating that the spurious monomials possess a distinctive property, allowing us to differentiate them from the structured part. After completing Step 1, we can transform the polynomial computed by the depth-5 circuit into a combination of a structured part and a spurious part. One of the outcomes of this first step is that we can efficiently identify the boundaries of the \hat{Q}_{ij} polynomials in the structured part, particularly using a small automaton, even though this process introduces some spurious monomials. A similar concept of boundary also applies to the monomials in the spurious part, and we show that these boundaries can also be identified using a small automaton. Another outcome is that each \hat{Q}_{ij} polynomial can be treated as commutative without resulting in a zero polynomial. These two outcomes are crucial for the remaining steps of our method. 1.1.2 Step 2: Product Sparsification In the second step, we demonstrate that within the polynomial \hat{f}_{1} if we treat a small number of the \hat{Q}_{ij} polynomials as n.c. while considering the rest as commutative, non-zeroness is preserved. A key aspect of this step is the ability to treat each \hat{Q}_{ij} polynomial as commutative. Although treating n.c. polynomials as commutative while preserving non-zeroness is generally not feasible, in Step 1, we transformed f into a sum of a structured part \hat{f}_{1} and a spurious part F_{1}. The n.c. polynomials \hat{Q}_{ij} within \hat{f}_{1} can be treated as n.c. without resulting in a zero polynomial (as established in Claim 4.0.1). We then show that the n.c. polynomial obtained after this transformation remains non-zero (see Lemma 4.2.1), a result we refer to as product sparsification. We call this sparsification because we reduce the number of n.c. \hat{Q}_{ij} polynomials in each product, which can be exponential, to a small number of them while preserving non-zeroness. It is important to note, however, that the sum of the exponents of the n.c. variables (a.k.a n.c. degree) in this new polynomial can still be generally exponential. It’s important to highlight that this product sparsification step affects both the structured part \hat{f}_{1} and the spurious part F_{1} of the polynomial obtained after Step 1. Let \hat{f}_{2} and F_{2} represent the polynomials derived from \hat{f}_{1} and F_{1} respectively as a result of this product sparsification step. We first consider the product sparsification of the n.c. polynomial \hat{f}_{1}. We will return to \hat{f}_{2}+F_{2} later. 1.1.3 Step 3: Commutative Transformation In the third and final step, we show that the sum of products of a small number of structured \hat{Q}_{ij} n.c. polynomials can be transformed into a commutative polynomial while preserving non-zeroness (see Lemma 4.3.1). As mentioned earlier, the n.c. degree of the polynomial obtained after Step 2 can be exponential in general. However, we show that this exponential degree n.c. polynomial can be converted into a commutative polynomial using only a small number of new commutative variables. It is noteworthy that there is no known method to convert a general exponential degree n.c. polynomial into a commutative polynomial with just a small number of commutative variables while preserving non-zeroness. Such transformations are known only when the number of non-zero monomials is bounded single-exponentially (a.k.a sparsity of the polynomial) or when the polynomial is computed by a \Sigma\Pi^{*}\Sigma circuit [AJMR19]. The key to our transformation lies in the structured nature of the \hat{Q}_{ij} polynomials, each of which requires only a small number of new commutative variables. Since the number of n.c \hat{Q}_{ij} polynomials is small, we conclude that we have to introduce only a small number of new commutative variables for this transformation while ensuring non-zeroness is maintained. It’s important to highlight that this commutative transformation step also affects both the structured part \hat{f}_{2} and the spurious part F_{2} of the polynomial derived after Step 2. Let \hat{f}^{(c)}_{3} and F^{(c)}_{3} represent the polynomials derived from \hat{f}_{2} and F_{2} respectively as a result of this commutative transformation step. We are ready to consider the sum of the structured part and spurious part now. 1.1.4 Efficient Coefficient Modifications Both Steps (2) and (3) will be applied to the structured polynomial \hat{f}_{1} and the spurious polynomial F_{1} obtained after Step 1. Let \hat{f}^{(c)}_{3} and F^{(c)}_{3} denote the respective commutative polynomials obtained after applying these two steps to \hat{f}_{1} and F_{1}. Since the same commutative variables are used, it is possible for monomials in \hat{f}^{(c)} and F^{(c)} to cancel each other. If \hat{f}^{(c)}_{3}+F^{(c)}_{3}\neq 0, we have successfully transformed the exponential degree n.c. polynomial into a commutative polynomial using only a small number of commutative variables while preserving non-zeroness. We can then evaluate the resulting commutative polynomial \hat{f}^{(c)}_{3}+F^{(c)}_{3} by randomly chosen matrices for non-zeroness. On the other hand, if \hat{f}^{(c)}_{3}+F^{(c)}_{3}=0, we know that the transformations in Steps (2) and (3) carried out only on the structured part \hat{f}_{1} obtained from Step (1) ensure that \hat{f}_{3}^{(c)}\neq 0, which implies that F_{3}^{(c)}\neq 0. If a group of n.c. monomials in \hat{f}_{1} transform into a commutative monomial m with coefficient \alpha_{m} in \hat{f}_{3}^{(c)} after Steps (2) and (3) then another group of n.c. monomials in F transformed into the same commutative monomial m with the coefficient -\alpha_{m} in F_{3}^{(c)}. To address this cancellation issue, we show that we can carefully modify the coefficients of at least one such group of n.c. monomials in the polynomial \hat{f}_{1}+F_{1} obtained after Step 1 before executing Steps (2) and (3). The key to this coefficient modification lies in the fact that the spurious monomials introduced in Step (1) have a distinctive property, enabling us to differentiate them from the structured part using a small automaton. We show that such a coefficient modification preserves non-zeroness (see Lemma 4.4.1). We then establish that if we apply Steps (2) and (3) on this newly modified polynomial, we get a commutative polynomial while preserving non-zeroness. Though our transformation of a non-commutative polynomial into a commutative one involves modifying the monomial coefficients using substitution matrices obtained from an automaton, it does not turn a zero polynomial into a non-zero one. Since we apply the same matrix substitution for each occurrence of a given variable, monomials that cancel before the transformation will continue to cancel afterward, as they are affected in the same way. This property is crucially used in applying our method inductively for higher depths. 1.1.5 Matrix Compositions Each of these steps (Steps (1)-(3) and coefficient modification step) is performed using a small substitution automaton, which defines a substitution matrix for each n.c. variable. Throughout the process, we obtain four different sets of matrices. It’s important to note that the matrices used in each step evaluate a n.c. polynomial obtained from the previous step. As our model is black-box, it is not possible to evaluate the polynomial in this way. We need a single matrix substitution for each n.c. variable. We show that the substitution matrices used across these four steps can be combined into a single matrix for each n.c. variable (see Lemma 3.0.1). This lemma on matrix composition enables us to establish the existence of small substitution matrices for testing non-zeroness in a sequence of steps, which is a key novelty and contribution of this work. This enables us to develop an efficient randomized polynomial identity testing (PIT) algorithm for depth-5 +-regular circuits (see Theorem 2). We further extend this idea to higher depths to develop an efficient randomized PIT algorithm (see Theorem 5)."
https://arxiv.org/html/2411.06944v1,Finite Variable Counting Logics with Restricted Requantification,"Counting logics with a bounded number of variables form one of the central concepts in descriptive complexity theory. Although they restrict the number of variables that a formula can contain, the variables can be nested within scopes of quantified occurrences of themselves. In other words, the variables can be requantified. We study the fragments obtained from counting logics by restricting requantification for some but not necessarily all the variables.Similar to the logics without limitation on requantification, we develop tools to investigate the restricted variants. Specifically, we introduce a bijective pebble game in which certain pebbles can only be placed once and for all, and a corresponding two-parametric family of Weisfeiler-Leman algorithms. We show close correspondences between the three concepts.By using a suitable cops-and-robber game and adaptations of the Cai-Fürer-Immerman construction, we completely clarify the relative expressive power of the new logics.We show that the restriction of requantification has beneficial algorithmic implications in terms of graph identification. Indeed, we argue that with regard to space complexity, non-requantifiable variables only incur an additive polynomial factor when testing for equivalence. In contrast, for all we know, requantifiable variables incur a multiplicative linear factor.Finally, we observe that graphs of bounded tree-depth and 3-connected planar graphs can be identified using no, respectively, only a very limited number of requantifiable variables.","Descriptive complexity is a branch of finite model theory that essentially aims at characterizing how difficult logical expressions need to be in order to capture particular complexity classes. While we are yet to find or rule out a logic capturing the languages in the complexity class P, there is an extensive body of work regarding the descriptive complexity of problems within P. Most notably, there is the work of Cai, Fürer, and Immerman [3] which studies a particular fragment of first-order logic. This is the fragment \mathsf{C}^{k} in which counting quantifiers are introduced into the logic, but the number of variables is restricted to being at most k. The seminal result in [3] shows that this logic fails to define certain graphs up to isomorphism, which in turn proves that inflationary fixed-point logic with counting IFP+C fails to capture P. Although the fragment \mathsf{C}^{k} restricts the number of variables, it is common for variables to be reused within a single logical formula. In particular, variables can be nested within scopes of quantified occurrences of themselves. In other words, they can be requantified. In our work, we are interested in understanding what happens if we limit the ability to reuse variables through requantification. In fact, we may think of reusability as a resource (in the vein of time, space, communication, proof length, advice etc.) that should be employed economically. It turns out that the ability to limit requantification provides us with a more detailed lens into the landscape of descriptive complexities within P, much in the fashion of fine-grained complexity theory. Results and techniques Let us denote by \mathsf{C}^{(k_{1},k_{2})} the fragment of first-order logic with counting quantifiers in which the formulas have at most k_{1} variables that may be requantified and at most k_{2} variables that may not be requantified. First, we show that many of the traditional techniques of treating counting logics can be adapted to the setting of limited requantification. Specifically, it is well known that there is a close ternary correspondence between the logic \mathsf{C}^{k}, the combinatorial bijective k-pebble game, and the famous (k-1)-dimensional Weisfeiler-Leman algorithm [3, 22, 26]. We develop versions of the game and the algorithm that also have a limit on the reusability of resources. For the pebble game, a limit on requantification translates into pebbles that cannot be picked up anymore, once they have been placed. For the Weisfeiler-Leman algorithm, the limit on requantification translates into having some dimensions that “cannot be reused”. In fact the translation to the algorithmic viewpoint is not as straightforward as one might hope at first. Indeed, we do not know how to define a restricted version of the classical Weisfeiler-Leman algorithm that corresponds to the logic \mathsf{C}^{(k_{1},k_{2})}. However, we circumvent this problem by employing the oblivious Weisfeiler-Leman algorithm (OWL). This variant is often used in the context of machine learning. In fact, Grohe [17] recently showed that k+1-dimensional OWL is in fact exactly as powerful as k-dimensional (classical) WL. We develop a resource-reuse restricted version of the oblivious algorithm and prove equivalence to our logic. Indeed, we formally prove precisely matching correspondences between the limited requantification, limited pebble reusability, and the limited reusable dimensions (Theorem 3.6). Next, we conclusively clarify the relation between the logics within the two-parametric family \mathsf{C}^{(k_{1},k_{2})}. We show that in most cases limiting the requantifiability of a variable strictly reduces the power of the logic. We argue that no amount of requantification-restricted variables is sufficient to compensate the loss of an unrestricted variable. However, these statements are only true if at least some requantifiable variable remains. In fact, exceptionally, \mathsf{C}^{(1,k_{2})} is strictly less expressive than \mathsf{C}^{(0,k^{\prime}_{2})} whenever k_{2}^{\prime}>2k_{2} (Theorem 4.7). To show the separation results, we adapt a construction of Fürer [13] and develop a cops-and-robber game similar to those in [12, 19]. In this version, some of the cops may repeatedly change their location, while others can only choose a location once and for all. Using another graph construction, we rule out various a priori tempting ideas concerning normal forms in \mathsf{C}^{(k_{1},k_{2})}. To this end we show that formulas in the logics can essentially become as complicated as possible, having to repeatedly requantify all of the requantifiable variables an unbounded number of times, before using a non-requantifiable variable (Corollary 4.10). In terms of the pebble game, it seems a priori unclear when an optimal strategy would employ the non-reusable pebbles. However, the corollary says that in general one has to conserve the non-reusable pebbles for possibly many moves until a favorable position calls for them. Having gone through the technical challenges that come with the introduction of reusability, puts us into a position to discuss the implications. Indeed, as our main result, we argue that our finer grained view on counting logics through restricted requantification has beneficial algorithmic implications. Specifically, we show that equivalence with respect to the logic \mathsf{C}^{(k_{1},k_{2})} can be decided in polynomial time with a space complexity of O\bigl{(}n^{k_{1}}\log n\bigr{)}, hiding quadratic factors depending only on k_{1} and k_{2} (Theorem 5.8). This shows that while the requantifiable variables each incur a multiplicative linear factor in required space, the restricted variables only incur an additive polynomial factor. In particular, equivalence with respect to the logics \mathsf{C}^{(0,k_{2})} can be decided in logarithmic space. To show these statements, we leverage the fact that, because non-requantifiable variables cannot simultaneously occur free and bound, the \mathsf{C}^{(k_{1},k_{2})}-type of a variable assignment does not depend on the \mathsf{C}^{(k_{1},k_{2})}-type of assignments which disagree regarding non-requantifiable variables. Moreover, we use ideas from an algorithm of Lindell, which computes isomorphism of trees in logarithmic space [31] to implement the iteration steps of our algorithm. Generally, we believe the new viewpoint may be of interest in particular for applications in machine learning, where the WL-hierarchy appears to be too coarse for actual applications with graph neural networks (see for example [1, 2, 32, 41]). In the process of the space complexity proof, we also show that the iteration number of the resource-restricted Weisfeiler-Leman algorithm described above is at most (k_{2}+1)n^{k_{1}}-1 (Corollary 5.3). Justifying the new concepts of restricted reusability, we observe that there are interesting graph classes that are identified by the logics \mathsf{C}^{(k_{1},k_{2})}. We argue that \mathsf{C}^{(0,d+1)} identifies all graphs of tree-depth at most d (Theorem 6.3) and that \mathsf{C}^{(2,2)} identifies all 3-connected planar graphs (Theorem 6.8). Outline of the paper After briefly providing necessary preliminaries (Section 2) we formally introduce the logics \mathsf{C}^{(k_{1},k_{2})}, the pebble game with non-reusable pebbles, the (k_{1},k_{2})-dimensional oblivious Weisfeiler-Leman algorithm, and prove the correspondence theorem between them (Section 3). We then relate the power of the logics to each other and rule out certain normal forms (Section 4). We then analyze the space complexity (Section 5) and finally provide two classes of graphs that are identified by our logics (Section 6). Further related work In addition to the references above, let us mention related investigations. Over time, a large body of work on descriptive complexity has evolved. For insights into fundamental results regarding bounded variable logics, we refer to classic texts [25, 26, 34, 35]. However, highlighting the importance of the counting logics \mathsf{C}^{k}, let us at least mention the Immerman-Vardi theorem [24, 40]. It says that on ordered structures, least fixed-point logic LFP captures P. Since LFP has the same expressive power as IFP+C on ordered structures, also IFP+C, whose expressive power is closely related to the expressive power of the logics \mathsf{C}^{k}, captures P. We should also mention the work of Hella [22] introducing the bijective k-pebble game which forms the basis for our resource restricted versions. (Counting logics on graph classes) Because of the close correspondence between the logic \mathsf{C}^{k} and the (k-1)-dimensional Weisfeiler-Leman algorithm, our investigations are closely related to the notion of the Weisfeiler-Leman dimension of a graph defined in [15]. Given a graph G this is the least number of variables k such that \mathsf{C}^{k+1} identifies G. In particular, on every graph class of bounded Weisfeiler-Leman dimension, the corresponding finite variable counting logic captures isomorphism. Graph classes with bounded Weisfeiler-Leman dimension include graphs with a forbidden minor [14] and graphs of bounded rank-width (or equivalently clique width) [20], which in both cases is also shown to imply that IFP+C captures P on these classes. For a comprehensive survey we refer to [27]. Our observations for planar graphs follow from techniques bounding the number of variables required for the identification of planar graphs [28]. Other recent classes not already captured by the results on excluded minors and rank-width include, for example, some polyhedral graphs [29], some strongly regular graphs [4], and permutation graphs [21]. (Logic and tree decompositions) In [9] and independently [8] it was shown that \mathsf{C}^{k}-equivalence is characterized by homomorphism counts from graphs of tree-width at most k-1. Likewise, homomorphism counts from bounded tree-depth graphs characterize equivalence in counting logic of bounded quantifier-rank [16]. Recently, these results were unified to characterize logical equivalence in finite variable counting logics with bounded quantifier-rank in terms of homomorphism counts [12]. (Space complexity) Ideas underlying Lindell’s logspace algorithm for tree isomorphism have also been used in the context of planar graphs [6] and more generally bounded genus graphs [10]. Similar results exist for classes of bounded tree-width [5, 11]. (Further recent results) Let us mention some quite recent results in the vicinity of our work that cannot be found in the surveys mentioned above. Regarding the quantifier-rank within counting logics, there is a recent superlinear lower bound [19] improving Fürer’s linear lower bound construction [13]. Further, very recent work on logics with counting includes results on rooted unranked trees [23] and inapproximability of questions on unique games [36]. Finally, there has been a surge in research on descriptive complexity within the context of machine learning (see [17, 37, 38])."
https://arxiv.org/html/2411.06904v1,The Equivalence Problem of E-Pattern Languages with Length Constraints is Undecidable,"Patterns are words with terminals and variables. The language of a pattern is the set of words obtained by uniformly substituting all variables with words that contain only terminals. Length constraints restrict valid substitutions of variables by associating the variables of a pattern with a system (or disjunction of systems) of linear diophantine inequalities. Pattern languages with length constraints contain only words in which all variables are substituted to words with lengths that fulfill such a given set of length constraints. We consider membership, inclusion, and equivalence problems for erasing and non-erasing pattern languages with length constraints. Our main result shows that the erasing equivalence problem, one of the most prominent open problems in the realm of patterns-becomes undecidable if length constraints are allowed in addition to variable equality. Additionally, it is shown that the terminal-free inclusion problem-another prominent open problem in the realm of patterns-is also undecidable in this setting.It is also shown that considering regular constraints, i.e., associating variables also with regular languages as additional restrictions together with length constraints for valid substitutions, results in undecidability of the non-erasing equivalence problem. This sets a first upper bound on constraints to obtain undecidability in this case, as this problem is trivially decidable in the case of no constraints and as it has unknown decidability if only regular- or only length-constraints are considered.","A pattern is a finite word consisting only of symbols from a finite set of letters \Sigma=\{\mathtt{a}_{1},...,\mathtt{a}_{\sigma}\}, also called terminals, and from an infinite set of variables X=\{x_{1},x_{2},...\} such that we have \Sigma\cap X=\emptyset. It is a natural and compact device to define formal languages. From patterns, we obtain words consisting only of terminals using a substitution h, a terminal preserving morphism that maps all variables in a pattern to words over the terminal alphabet. The language of a pattern is the set of all words obtainable from that pattern using arbitrary substitutions. We differentiate between two kinds of substitutions. In the original definition of patterns and pattern languages introduced by Angluin [1], only words obtained by non-erasing substitutions are considered. Here, all variables are required to be mapped to non-empty words. The resulting languages are called non-erasing (NE) pattern languages. Later, so called erasing-/extended- or just E-pattern languages have been introduced by Shinohara [27]. Here, variables may also be substituted by the empty word \varepsilon. Consider, for example, the pattern \alpha:=x_{1}\mathtt{a}x_{2}\mathtt{b}x_{1}. Then, if we map x_{1} to \mathtt{a}\mathtt{b} and x_{2} to \mathtt{b}\mathtt{b}\mathtt{a}\mathtt{a} using a substitution h, we obtain the word h(\alpha)=\mathtt{a}\mathtt{b}\mathtt{a}\mathtt{b}\mathtt{b}\mathtt{a}\mathtt{% a}\mathtt{b}\mathtt{a}\mathtt{b}. Considering the E-pattern language of \alpha, we could also map x_{1} to the empty word and obtain any word in the language \{\mathtt{a}\}\cdot\Sigma^{*}\cdot\{\mathtt{b}\}. Due to its practical and simple definition, patterns and their corresponding languages occur in numerous areas in computer science and discrete mathematics. These include, for example, unavoidable patterns [14, 18], algorithmic learning theory [1, 5, 28], word equations [18], theory of extended regular expressions with back references [9], or database theory [7, 26]. There are three main decision problems regarding patterns and pattern languages. Those are the membership problem (and its variations [10, 11, 6]), the inclusion problem, and the equivalence problem. All are considered in the erasing (E) and non-erasing (NE). The membership problem determines if a word belongs to the language of a pattern. This problem has been shown to be NP-complete for both, erasing- and non-erasing, pattern languages [1, 14]. The inclusion problem determines whether the language of one pattern is a subset of the language of another pattern. It has been shown to be generally undecidable by Jiang et al. in [15]. Freydenberger and Reidenbach [8] as well as Bremer and Freydenberger [2] improved that result and showed that it is undecidable for all bounded alphabets of size |\Sigma|\geq 2 for both erasing and non-erasing pattern languages. The equivalence problem asks whether the languages of two patterns are equal. For NE-pattern languages, this problem is trivially decidable and characterized by the equality of patterns up to a renaming of their variables [1]. The decidability of the erasing case is one of the major open problems in the field [15, 23, 22, 21, 24]. For terminal-free patterns, however, i.e., patterns without any terminal letters, the inclusion problem as well as the equivalence problem for E-pattern languages have been characterized and shown to be NP-complete [15, 4]. What remains unresolved, though, is the decidability of the inclusion problem of terminal-free NE-pattern languages. Over time, various extensions to patterns and pattern languages have been introduced, either, to obtain additional expressibility due to some practical context or to get closer to an answer for the remaining open problems. Some examples are the bounded scope coincidence degree, patterns with bounded treewidth, k-local patterns, or strongly-nested patterns (see [3] and references therein). Koshiba [16] introduced so called typed patterns that restrict substitutions of variables to types, i.e., arbitrary recursive languages. Geilke and Zilles [12] extended this recently to the notion of relational patterns and relational pattern languages. In [20], a special form of typed patterns has been considered, i.e., patterns with regular constraints. Here, variables may be restricted by arbitrary regular languages and the same variable may occur more than once. It has been shown that this notion suffices to obtain undecidability for both main open problems regarding pattern languages, i.e., the equivalence of E-pattern languages and the inclusion of terminal-free NE-pattern languages. Another natural extension other than regular constraints is the notion of length constraints. Here, instead of restricting the choice of words for the substitution of variables, length constraints just restrict the lengths of substitution of variables in relation to each other. In the field of word equations, length constraints have been considered as a natural extension for a long time and, e.g., answering the decidability of the question whether word equations with length constraints have a solution, is a long outstanding problem (see, e.g., [17] and the references therein). In this paper, we consider that natural extension on patterns, resulting in the class of patterns called patterns with length constraints. In general, we say that a length constraint \ell is a disjunction of systems of linear (diophantine) inequalities over the variables of X. We denote the set of all length constraints by \mathcal{C}_{Len}. A pattern with length constraints (\alpha,\ell_{\alpha})\in(\Sigma\cup X)^{*}\times\mathcal{C}_{Len} is a pattern associated with a length constraint. We say that a substitution h is \ell_{\alpha}-valid if all variables are substituted according to \ell_{\alpha}. Now, the language of (\alpha,\ell_{\alpha}) is defined analogously to pattern languages but restricted to \ell_{\alpha}-valid substitutions in the erasing- and non-erasing cases. We examine erasing (E) and non-erasing (NE) pattern languages with length constraints. It can be shown that the membership problem for both cases in NP-complete. The inclusion problem is shown to be undecidable in both cases, too, notably even for terminal-free patterns, which is a difference to the decidability of the inclusion problem in the erasing case for patterns without any constraints, and which is an answer to a problem which is still open for non-erasing patterns without constraints. The main result of this paper is the undecidability of the equivalence problem for erasing pattern languages with length constraints in both cases, terminal-free and general, giving an answer to a problem of which the decidability has been an open problem for a long time in the case of no constraints. The final result shows that regular constraints and length constraints combined suffice to show undecidability of the equivalence problem for non-erasing pattern languages, a problem that is trivially decidable in case of no constraints and still open in the cases of just regular- or just length-constraints."
https://arxiv.org/html/2411.06383v2,Program Analysis via Multiple Context Free Language Reachability,"Context-free language (CFL) reachability is a standard approach in static analyses, where the analysis question (e.g., is there a dataflow from x to y?) is phrased as a language reachability problem on a graph G wrt a CFL \mathcal{L}. However, CFLs lack the expressiveness needed for high analysis precision. On the other hand, common formalisms for context-sensitive languages are too expressive, in the sense that the corresponding reachability problem becomes undecidable. Are there useful context-sensitive language-reachability models for static analysis?In this paper, we introduce Multiple Context-Free Language (MCFL) reachability as an expressive yet tractable model for static program analysis. MCFLs form an infinite hierarchy of mildly context sensitive languages parameterized by a dimension d and a rank r. Larger d and r yield progressively more expressive MCFLs, offering tunable analysis precision. We showcase the utility of MCFL reachability by developing a family of MCFLs that approximate interleaved Dyck reachability, a common but undecidable static analysis problem.Given the increased expressiveness of MCFLs, one natural question pertains to their algorithmic complexity, i.e., how fast can MCFL reachability be computed? We show that the problem takes O(n^{2d+1}) time on a graph of n nodes when r=1, and O(n^{d(r+1)}) time when r>1. Moreover, we show that when r=1, even the simpler membership problem has a lower bound of n^{2d} based on the Strong Exponential Time Hypothesis, while reachability for d=1 has a lower bound of n^{3} based on the combinatorial Boolean Matrix Multiplication Hypothesis. Thus, for r=1, our algorithm is optimal within a factor n for all levels of the hierarchy based on the dimension d (and fully optimal for d=1).We implement our MCFL reachability algorithm and evaluate it by underapproximating interleaved Dyck reachability for a standard taint analysis for Android. When combined with existing overapproximate methods, MCFL reachability discovers all tainted information on 8 out of 11 benchmarks, while it has remarkable coverage (confirming 94.3\% of the reachable pairs reported by the overapproximation) on the remaining 3. To our knowledge, this is the first report of high and provable coverage for this challenging benchmark set.","Static analysis via language reachability. Static analyses are a standard approach to determining program correctness, as well as other useful properties of programs. They normally operate by establishing an approximate model for the program, effectively reducing questions about program behavior to algorithmic questions about the model. One popular type of modeling in this direction is language reachability, where the program abstraction is via an edge-labeled graph G (Reps, 1997; Reps et al., 1995). Language reachability is a generalization of standard graph reachability, parameterized by a language \mathcal{L}. Intuitively, given two nodes u, v, the problem asks to determine whether v is reachable from u via a path whose sequence of labels produce a string that belongs to \mathcal{L}. Such a path captures program executions that relate the objects represented by v and u (e.g., control-flow between two program locations or data-flow between two variables). The role of \mathcal{L} is normally to increase the analysis precision by filtering out paths that represent spurious program executions, owing to the approximate nature of G. Context-free language reachability. Language-reachability based static analyses are most frequently phrased with respect to context free languages (CFL), known as CFL reachability, which have various uses. For example, they are used to increase the precision of interprocedural analyses (aka whole-program analyses), where modeled executions cross function boundaries, and the analysis has to be calling-context sensitive111It might sound paradoxical that a context-free language makes the analysis context-sensitive, but this is just a naming coincidence. “Context sensitivity” refers to calling contexts, and the CFL simulates the call stack.. A CFL \mathcal{L} captures that a path following an invocation from a caller function \mathtt{foo}() to a callee \mathtt{tie}() must return to the call site of \mathtt{foo}() when \mathtt{tie}() returns (as opposed to some other function \mathsf{bar}() that also calls \mathtt{tie}()). This approach is followed in a wide range of interprocedural static analyses, including data-flow and shape analysis (Reps et al., 1995), type-based flow analysis (Rehof and Fähndrich, 2001) and taint analysis (Huang et al., 2015), to name a few. In practice, widely-used tools, such as Wala (Wal, 2003) and Soot (Bodden, 2012), equip CFL-reachability techniques to perform the analysis. Another common use of CFLs is to track dataflow information between composite objects in a field-sensitive manner. Here, a CFL \mathcal{L} captures a dataflow between variables x and y if, for example x flows into z.f (i.e., field f of composite object z), and z.f itself flows into y (as opposed to z.g flowing into y). This approach is standard in a plethora of pointer, alias and data dependence analyses (Lhoták and Hendren, 2006; Reps, 1997; Chatterjee et al., 2018; Sridharan et al., 2005; Sridharan and Bodík, 2006; Lu and Xue, 2019). The need for context-sensitive models. Although CFLs offer increased analysis precision over simpler, regular abstractions, there is often a need for more precise, context sensitive models. For example, almost all analyses are designed with both context and field sensitivity, as this leads to obvious precision improvements (Milanova, 2020; Lhoták and Hendren, 2006; Sridharan and Bodík, 2006; Späth et al., 2019). The underlying language modeling both sensitivities is the free interleaving of the two corresponding CFLs, which is not a CFL, and is commonly phrased as interleaved Dyck reachability. Unfortunately, interleaved Dyck reachability is well-known to be undecidable (Reps, 2000). Since it is desirable to maintain at least some precision of each type, there have been various approximations that generally fall into two categories: (i) apply some kind of k-limiting, which approximates one of the CFLs by a regular (or even finite) language (the most common approach), or (ii) solve for each type of sensitivity independently (Späth et al., 2019), possibly in a refinement loop (Ding and Zhang, 2023; Conrado and Pavlogiannis, 2024). Observe that both cases essentially fall back to context-free models, forgoing the desirable precision of context sensitivity for which the analysis was designed in the first place. This leads to a natural question: Are there natural, efficient (polynomial-time), and practically precise, context-sensitive approximations for interleaved Dyck reachability? Multiple Context Free Languages. One of the most natural generalizations of CFLs towards mild context sensitivity is that of Multiple Context Free Languages (MCFLs) (Seki et al., 1991). These languages are generated by the corresponding Multiple Context Free Grammars (MCFGs), which form a hierarchy of expressiveness parameterized by a dimension d and rank r (concisely denoted as d\text{-}\operatorname{MCFG}(r)). Intuitively, an MCFG in d dimensions performs simultaneous context-free parsing on d substrings of a word, and can thus capture bounded context-sensitivity between these substrings. The rank r limits the number of non-terminals that can appear in a single production rule. MCFGs have received considerable attention, as they are regarded as a realistic formalism for natural languages (Clark, 2014), while several popular classes of formal languages fall into specific levels in this hierarchy, e.g., CFLs are MCFLs of dimension 1, and Tree Adjoining Languages (TALs) (Joshi, 1987) and Head Languages (Pollard, 1984) fall in dimension 2 (Seki et al., 1991). Despite the context sensitivity, for each r\geq 2, d\text{-}\operatorname{MCFL}(r) forms a full abstract family of languages (AFL – closed under homomorphism, inverse homomorphism, intersection with regular sets, union, and Kleene closure) (Rambow and Satta, 1999), with decidable membership and emptiness (Vijay-Shanker et al., 1987). As such, they form an elegant class of mildly context sensitive languages that are amenable to algorithmic treatment. In a static analysis setting, language reachability with MCFLs has the potential to yield higher modeling power. Moreover, this power is utilized in a controllable way, owing to the higher expressivity along the MCFL hierarchy. However, neither (i) the modeling power of MCFL reachability (what can MCFLs express in a program analysis setting?) nor (ii) the algorithmic question (how fast can we solve MCFL reachability?) have been studied. This paper addresses these questions, by (1) designing a family of MCFLs for approximating the common static analysis problem of interleaved Dyck reachability, with remarkable coverage in practice, and (2) developing a generic algorithm for MCFL reachability (for any dimension d and rank r), as well as proving fine-grained complexity lower bounds for the problem. The following motivating example illustrates the problem setting and our approach. 1.1. Motivating Example In a standard dataflow analysis setting, the task is to identify pairs of variables x, y such that the value of x may affect the value of y. This is achieved by following def-use chains in the program P. The program model is a dataflow graph G, where nodes represent variables, and an edge x\to y results from an instruction of the form x=f(y) (for some uninterpreted function f). To address the common issue of high false positives, the analysis must be both context-sensitive and field-sensitive. For example, consider the program P in Fig. 1, and the dataflow graph G in Fig. 2(a). {mdframed} [backgroundcolor=black!7!white,rightline=false,leftline=false,linewidth=0.25mm,] ⬇ 1pair tie(int x,int y){ 2 pair p; 3 p.first = x; 4 p.second = y; 5 return p; 6} ⬇ 1void foo() { 2 int a = 2; 3 int b = 3; 4 pair q = tie(a,b); 5 int c = q.first; 6 return; 7} ⬇ 6void bar() { 7 int d = 5; 8 pair r = tie(d,7); 9 int e = r.second; 10 return; 11} Figure 1. A program P containing three functions tie(), foo(), bar() and composite objects of type pair. adbxyp\text{tie}_{\text{ret}}qrce{\color[rgb]{0.2549019607843137,0.2117647058823529,0.9823529411764706}% \definecolor[named]{pgfstrokecolor}{rgb}{% 0.2549019607843137,0.2117647058823529,0.9823529411764706}(_{10}}{\color[rgb]{0.2549019607843137,0.2117647058823529,0.9823529411764706}% \definecolor[named]{pgfstrokecolor}{rgb}{% 0.2549019607843137,0.2117647058823529,0.9823529411764706}(_{16}}{\color[rgb]{0.2549019607843137,0.2117647058823529,0.9823529411764706}% \definecolor[named]{pgfstrokecolor}{rgb}{% 0.2549019607843137,0.2117647058823529,0.9823529411764706}(_{10}}{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}[_{% 1}}{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}[_{% 2}}{\color[rgb]{0.2549019607843137,0.2117647058823529,0.9823529411764706}% \definecolor[named]{pgfstrokecolor}{rgb}{% 0.2549019607843137,0.2117647058823529,0.9823529411764706})_{10}}{\color[rgb]{0.2549019607843137,0.2117647058823529,0.9823529411764706}% \definecolor[named]{pgfstrokecolor}{rgb}{% 0.2549019607843137,0.2117647058823529,0.9823529411764706})_{16}}{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}]_{% 1}}{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}]_{% 2}} (a) A graph G modeling context-sensitive and field-sensitive data flow in program P from Fig. 1. efghijk{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}[_{% 1}}{\color[rgb]{0.2549019607843137,0.2117647058823529,0.9823529411764706}% \definecolor[named]{pgfstrokecolor}{rgb}{% 0.2549019607843137,0.2117647058823529,0.9823529411764706}(_{100}}{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}]_{% 1}}{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}[_{% 1}}{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}]_{% 1}}{\color[rgb]{0.2549019607843137,0.2117647058823529,0.9823529411764706}% \definecolor[named]{pgfstrokecolor}{rgb}{% 0.2549019607843137,0.2117647058823529,0.9823529411764706})_{100}}{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}]_{% 1}} (b) A subgraph of the uranai benchmark in a taint analysis for Android. Figure 2. Two graphs modeling context and field sensitivity through edge labels. Context sensitivity. Let us momentarily ignore edge labels in G. We have a path b\rightsquigarrow e, signifying a dataflow from b to e. This, however, does not correspond to a valid program execution: the path goes through the call of function tie() from foo() (where b is declared), but when tie() returns, the execution continues on foo, rather than bar() where e is declared. Call-context sensitivity is achieved by modeling call sites using parenthesis labels, and only considering reachability as witnessed by paths that produce a properly balanced parenthesis string. Formally we require that the label of the path forms a string that belongs to the Dyck language over parentheses (which is a CFL). Now, the path b\rightsquigarrow e is invalid, since {\color[rgb]{0.2549019607843137,0.2117647058823529,0.9823529411764706}% \definecolor[named]{pgfstrokecolor}{rgb}{% 0.2549019607843137,0.2117647058823529,0.9823529411764706}(_{10}} (along the edge b\xrightarrow{{\color[rgb]{% 0.2549019607843137,0.2117647058823529,0.9823529411764706}\definecolor[named]{% pgfstrokecolor}{rgb}{0.2549019607843137,0.2117647058823529,0.9823529411764706}% (_{10}}}y) does not match {\color[rgb]{0.2549019607843137,0.2117647058823529,0.9823529411764706}% \definecolor[named]{pgfstrokecolor}{rgb}{% 0.2549019607843137,0.2117647058823529,0.9823529411764706})_{16}} (along the edge \text{tie}_{\text{ret}}\xrightarrow{{\color[rgb]{% 0.2549019607843137,0.2117647058823529,0.9823529411764706}\definecolor[named]{% pgfstrokecolor}{rgb}{0.2549019607843137,0.2117647058823529,0.9823529411764706}% )_{16}}}r), thus the analysis avoids reporting this false positive. Field sensitivity. With parentheses modeling call-context sensitivity, consider the path d\rightsquigarrow e. The parenthesis string along this path is {\color[rgb]{0.2549019607843137,0.2117647058823529,0.9823529411764706}% \definecolor[named]{pgfstrokecolor}{rgb}{% 0.2549019607843137,0.2117647058823529,0.9823529411764706}(_{16}}{\color[rgb]{% 0.2549019607843137,0.2117647058823529,0.9823529411764706}\definecolor[named]{% pgfstrokecolor}{rgb}{0.2549019607843137,0.2117647058823529,0.9823529411764706}% )_{16}}, which is balanced, representing the fact that call contexts are respected. However, in P there is no dataflow from d to e, this time due to unmatched fields: x is assigned to \mathit{p.first}, and although there is a dataflow from p to r, e gets assigned \mathit{r.second}. Field-sensitivity is achieved by modeling object fields using (square) bracket labels, and only considering reachability as witnessed by paths that produce a properly balanced bracket string. Formally we require that the label of the path forms a string that belongs to the Dyck language over brackets. Now, the path d\rightsquigarrow e is invalid, since {\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}[_{% 1}} (along the edge x\xrightarrow{{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{% rgb}{0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.2% 4}[_{1}}}p) does not match {\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}]_{% 2}} (along the edge r\xrightarrow{{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{% rgb}{0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.2% 4}]_{2}}}e), thus the analysis avoids reporting this false positive. Context and field sensitivity, simultaneously. To capture both context and field sensitivity, the analysis must decide reachability via paths that are well-balanced wrt both parentheses and brackets. However, these two types of symbols can be interleaved in an arbitrary way. For example, out of all 6 possible source-sink pairs \{a,d,b\}\times\{c,e\}, the only real dataflow is from a to c, witnessed by a path producing the string {\color[rgb]{0.2549019607843137,0.2117647058823529,0.9823529411764706}% \definecolor[named]{pgfstrokecolor}{rgb}{% 0.2549019607843137,0.2117647058823529,0.9823529411764706}(_{10}}{\color[rgb]{% 0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{rgb}{0.24,0.24,0.24}% \pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}[_{1}}{\color[rgb]% {0.2549019607843137,0.2117647058823529,0.9823529411764706}\definecolor[named]{% pgfstrokecolor}{rgb}{0.2549019607843137,0.2117647058823529,0.9823529411764706}% )_{10}}{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}]_{% 1}}. As the corresponding reachability problem is undecidable (Reps, 2000), existing techniques focus on overapproximating interleaved Dyck reachability, mostly by some context-free model. This implies that these analysis results may still contain false positives in terms of reachability in the dataflow graph. Illustration on a real benchmark. To further illustrate the challenge, consider the dataflow graph in Fig. 2(b), which is a subgraph of a common taint analysis for Android (Huang et al., 2015). From an overapproximation standpoint, consider the potential reachability from e to j. Notice that there are valid context-sensitive paths and valid field-sensitive paths e\rightsquigarrow j; these are, respectively e\xrightarrow{{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{% rgb}{0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.2% 4}[_{1}}}g\xrightarrow{{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{% pgfstrokecolor}{rgb}{0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}% \pgfsys@color@gray@fill{0.24}]_{1}}}i\xrightarrow{{\color[rgb]{% 0.2549019607843137,0.2117647058823529,0.9823529411764706}\definecolor[named]{% pgfstrokecolor}{rgb}{0.2549019607843137,0.2117647058823529,0.9823529411764706}% )_{100}}}j\qquad\text{and}\qquad e\xrightarrow{{\color[rgb]{0.24,0.24,0.24}% \definecolor[named]{pgfstrokecolor}{rgb}{0.24,0.24,0.24}% \pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}[_{1}}}g% \xrightarrow{{\color[rgb]{% 0.2549019607843137,0.2117647058823529,0.9823529411764706}\definecolor[named]{% pgfstrokecolor}{rgb}{0.2549019607843137,0.2117647058823529,0.9823529411764706}% (_{100}}}h\xrightarrow{{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{% pgfstrokecolor}{rgb}{0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}% \pgfsys@color@gray@fill{0.24}]_{1}}}f\xrightarrow{{\color[rgb]{0.24,0.24,0.24}% \definecolor[named]{pgfstrokecolor}{rgb}{0.24,0.24,0.24}% \pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}[_{1}}}e% \xrightarrow{{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{% rgb}{0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.2% 4}[_{1}}}g\xrightarrow{{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{% pgfstrokecolor}{rgb}{0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}% \pgfsys@color@gray@fill{0.24}]_{1}}}i\xrightarrow{{\color[rgb]{% 0.2549019607843137,0.2117647058823529,0.9823529411764706}\definecolor[named]{% pgfstrokecolor}{rgb}{0.2549019607843137,0.2117647058823529,0.9823529411764706}% )_{100}}}j Because of the presence of both paths, an overapproximation algorithm may fail to conclude that e does not reach j through a path that is simultaneously context and field-sensitive. In fact, even newer overapproximation methods such as (Ding and Zhang, 2023) indeed report that e reaches j, thereby producing a false positive. From an underapproximation standpoint, consider the reachability from e to k, witnessed by the path e\xrightarrow{{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{% rgb}{0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.2% 4}[_{1}}}g\xrightarrow{{\color[rgb]{% 0.2549019607843137,0.2117647058823529,0.9823529411764706}\definecolor[named]{% pgfstrokecolor}{rgb}{0.2549019607843137,0.2117647058823529,0.9823529411764706}% (_{100}}}h\xrightarrow{{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{% pgfstrokecolor}{rgb}{0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}% \pgfsys@color@gray@fill{0.24}]_{1}}}f\xrightarrow{{\color[rgb]{0.24,0.24,0.24}% \definecolor[named]{pgfstrokecolor}{rgb}{0.24,0.24,0.24}% \pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.24}[_{1}}}e% \xrightarrow{{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{pgfstrokecolor}{% rgb}{0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}\pgfsys@color@gray@fill{0.2% 4}[_{1}}}g\xrightarrow{{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{% pgfstrokecolor}{rgb}{0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}% \pgfsys@color@gray@fill{0.24}]_{1}}}i\xrightarrow{{\color[rgb]{% 0.2549019607843137,0.2117647058823529,0.9823529411764706}\definecolor[named]{% pgfstrokecolor}{rgb}{0.2549019607843137,0.2117647058823529,0.9823529411764706}% )_{100}}}j\xrightarrow{{\color[rgb]{0.24,0.24,0.24}\definecolor[named]{% pgfstrokecolor}{rgb}{0.24,0.24,0.24}\pgfsys@color@gray@stroke{0.24}% \pgfsys@color@gray@fill{0.24}]_{1}}}k Observe that the path is non-simple, as we have to traverse the cycle once to obtain a valid string. Moreover, the string interleaves parentheses with brackets, which means that it cannot be captured in a single Dyck language involving both parentheses and brackets. In this work we demonstrate that MCFLs are an effective and tractable context-sensitive language formalism for underapproximating interleaved Dyck reachability that yields good approximations for real-world benchmarks. 1.2. Summary of Results To benefit readability, we summarize here the main results of the paper, referring to the following sections for details. We relegate all proofs to the Appendix. 1. MCFL reachability as a program model. We introduce MCFL reachability as an expressive, yet tractable, context-sensitive formalism for static analyses. Parameterized by the dimension d and rank r, d\text{-}\operatorname{MCFL}(r) yields an infinite hierarchy of progressively more expressive models that become Turing-complete in the limit. We illustrate the usefulness of MCFL reachability by using it to under-approximate the (generally, undecidable) problem of interleaved Dyck reachability, which is the standard formulation of a plethora of static analyses. In particular, for each d\geq 1, we obtain a d\text{-}\operatorname{MCFL}(2) that achieves increased precision as d increases (i.e., it discovers more reachable pairs of nodes), and becomes complete (i.e., it discovers all reachable pairs) in the limit of d\to\infty. Our MCFL formulation is, to our knowledge, the first non-trivial method that approximates the reachability set from below, thus having no false positives. Although underapproximations are less common in static analyses, they have many uses, such as excluding false positives (Psalm, 2024; Hicken, 2023), reporting concrete witnesses, acting as a tool for bug-finding (Le et al., 2022; Bessey et al., 2010) and performing “must” analyses (Godefroid et al., 2010; Xu et al., 2009; Smaragdakis and Balatsouras, 2015). Our underapproximation, when paired with existing overapproximate methods, allows limiting the set of potentially false negatives dramatically, and even find a fully-precise answer (as often is the case in our experiments). 2. MCFL reachability algorithm. We develop a generic algorithm for solving d\text{-}\operatorname{MCFL}(r) reachability, for any value of d and r. Our algorithm generalizes the existing algorithms for CFL reachability (Yannakakis, 1990) and TAL reachability (Tang et al., 2017). In particular, we establish the following theorem. {restatable} theoremthmupperbound All-pairs d\text{-}\operatorname{MCFL}(r)-reachability given a grammar \mathcal{G} on a graph G of n nodes can be solved in (1) O(\operatorname{poly}(|\mathcal{G}|)\cdot\delta\cdot n^{2d}) time, if r=1, where \delta is the maximum degree of G, and (2) O(\operatorname{poly}(|\mathcal{G}|)\cdot n^{d(r+1)}) time, if r>1. As CFLs and TALs are 1\text{-}\operatorname{MCFL}(2) and 2\text{-}\operatorname{MCFL}(2), respectively, Section 1.2 recovers the known bounds of O(n^{3}) and O(n^{6}) for the corresponding reachability problems. We also remark that the simpler problem of d\text{-}\operatorname{MCFL}(r) membership is solved in time O(n^{d(r+1)}) time on strings of length n (Seki et al., 1991). Section 1.2 states that reachability is no harder than membership, as long as the current bounds hold, for bounded-degree graphs (\delta=O(1)) or when r>1. 3. MCFL membership and reachability lower bounds. Observe that the bounds in Section 1.2 grow exponentially on the dimension d and rank r of the language. The next natural question is whether this dependency is tight, or it can be improved further. Given the role of MCFL reachability as an abstraction mechanism, this question is also practically relevant. For example, consider a scenario where a 3-dimensional MCFL is used in a static analysis setting, but the analysis is too heavy for the task at hand. The designer faces a dilemma: “should we attempt to improve the analysis algorithm, or should we find a simpler model, e.g., based on a 2-dimensional MCFL?”. A proven lower bound resolves this dilemma in favor of receding to 2 dimensions222Of course, one should also look for heuristics that offer practical speedups. We touch on this in Section 8.. We prove two such lower bounds based on arguments from fine-grained complexity theory. First, we study the dependency of the exponent on the dimension d. For this, we fix r=1 and arbitrary d, for which the membership problem, as well as the reachability problem on bounded-degree graphs, takes O(n^{2d}) time. We establish a lower-bound of n^{2d} based on the Strong Exponential Time Hypothesis (SETH). {restatable} theoremthmovhard For any integer d and any fixed \epsilon>0, the d\text{-}\operatorname{MCFL}(1) membership problem on strings of length n has no algorithm in time O(n^{2d-\epsilon}), under SETH. Section 1.2 is based on a fine-grained reduction from Orthogonal Vectors. The k-Orthogonal Vectors (OV) problem asks, given a set of m\cdot k Boolean vectors, to identify k vectors that are orthogonal. The corresponding hypothesis k-OVH states that this problem cannot be solved in O(m^{k-\epsilon}) time, for any fixed \epsilon>0 (it is also known that SETH implies k-OVH (Williams, 2005)). Section 1.2 is obtained by proving that a d\text{-}\operatorname{MCFL}(1) can express the orthogonality of 2d vectors. This implies that the dependency 2d in the exponent of Section 1.2 cannot be improved, while for r=1 our reachability algorithm is optimal on sparse graphs. Second, note that, on dense graphs (i.e., when \delta=\Theta(n)), the bound in Section 1.2 Item 1 is a factor n worse than the lower bound of Section 1.2. Are further improvements possible in this case? To address this question, we focus on the case of d=1, for which this upper bound becomes O(n^{3}). We show that the problem has no subcubic combinatorial algorithm based on the combinatorial Boolean Matrix Multiplication Hypothesis (BMMH). {restatable} theoremthmtrianglehard For any fixed \epsilon>0, the single-pair 1\text{-}\operatorname{MCFL}(1)-reachability problem on graphs of n nodes has no algorithm in time O(n^{3-\epsilon}) under BMMH. Hence, the \delta factor increase in the complexity cannot be improved in general, while Section 1.2 is tight for d=1 and r=1, among combinatorial algorithms. 4. Implementation and experimental evaluation. We implement our algorithm for MCFL reachability and run it with our family of d\text{-}\operatorname{MCFL}(2)s on standard benchmarks of interleaved Dyck reachability that capture taint analysis for Android (Huang et al., 2015). To get an indication of coverage, we compare our underapproximations with recent overapproximations. Remarkably, our underapproximation matches the overapproximation on most benchmarks, meaning that we have fully sound and complete results. For the remaining benchmarks, our underapproximation is able to confirm (94.3\%) of the taint information reported by the overapproximation. To our knowledge, this is the first report of such high, provable coverage for this challenging benchmark set."
