URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10444v1,Balancing Passenger Transport and Power Distribution: A Distributed Dispatch Policy for Shared Autonomous Electric Vehicles,"Shared autonomous electric vehicles can provide on-demand transportation for passengers while also interacting extensively with the electric distribution system. This interaction is especially beneficial after a disaster when the large battery capacity of the fleet can be used to restore critical electric loads. We develop a dispatch policy that balances the need to continue serving passengers (especially critical workers) and the ability to transfer energy across the network. The model predictive control policy tracks both passenger and energy flows and provides maximum passenger throughput if any policy can. The resulting mixed integer linear programming problem is difficult to solve for large-scale problems, so a distributed solution approach is developed to improve scalability, privacy, and resilience. We demonstrate that the proposed heuristic, based on the alternating direction method of multipliers, is effective in achieving near-optimal solutions quickly. The dispatch policy is examined in simulation to demonstrate the ability of vehicles to balance these competing objectives with benefits to both systems. Finally, we compare several dispatch behaviors, demonstrating the importance of including operational constraints and objectives from both the transportation and electric systems in the model.","Advancements in shared mobility-on-demand services, vehicle automation, and electrification are reshaping the transportation landscape, enabling shared autonomous electric vehicles (SAEVs) to facilitate dynamic interaction between transportation networks and the electric grid [1, 2]. Such vehicles have the potential to reduce electricity demand and voltage fluctuation and improve reliability and resilience in the electric grid if charging and discharging schemes are collaboratively optimized [3, 4, 5, 6, 7]. However, though many studies on vehicle routing have neglected the impact of electric vehicles (EVs) on the grid by assuming infinite power availability [8, 9], research has shown that incorrectly managed EV charging could lead to power quality problems [10]. Appropriate control can negate these concerns and lead to benefits such as peak shaving and voltage and frequency control, particularly when vehicle-to-grid charging technology is employed [11]. Furthermore, integrating EVs with renewable energy sources has shown particular promise in helping align electricity demand and supply curves, increasing economic benefits, and improving resilience [12, 13]. As the SAEV fleet size must be large to serve peak hour demand, the incorporation of constraints related to the energy grid can enable additional uses for these vehicles when they are not needed for transportation. Beyond day-to-day operations, there are additional resilience benefits when strains are put on the electric grid [14]. Natural disasters can endanger the distribution system, causing equipment failures, blackouts, and even larger scale propagation of failures throughout the network [15]. In the event of electric outages, the extra battery capacity of SAEVs can be leveraged to continue providing power to affected areas, particularly critical facilities such as hospitals and shelters [16, 17, 14]. While some research has examined modifying bus routes to support grid restoration while maintaining transit operations [18], most work has focused on transportable energy storage systems (TESSs), which requires grid operators or government agencies to own and operate a fleet of dedicated vehicles that do not need to transport passengers but can be costly [3]. When SAEVs are used, the benefits to electric consumers must be weighed against the continued needs of transportation passengers who may continue to be reliant on the SAEV fleet after the disaster. This study introduces a dispatch policy that coordinates the SAEV system with the distribution network. In our framework, the grid operator manages power dispatch while the vehicle dispatcher optimizes vehicle logistics. Unlike previous research, we acknowledge that even during disruptions to the power grid, vehicles will continue to serve passengers. With this in mind, our proposed model ensures that the integrated efforts of the vehicle dispatcher and power grid operator achieve a balanced approach to maintaining essential transportation services, particularly for critical workers, and supporting end-user power needs during system disruptions [19, 20]. This novel cooperative approach allows for joint predictive control and modeling of vehicle and power flows, allowing vehicles to provide services to the electric grid while still maximizing passenger throughput [21, 5, 22, 23]. Furthermore, we advance our methodology by developing a distributed solution approach that enables local decision-making with minimal communication. This approach addresses scalability, stability, privacy, and resilience concerns more effectively. This heuristic, based on the alternating direction method of multipliers (ADMM) [24] achieves near-optimal, dynamic solutions quickly. Our findings from simulations on multiple networks illustrate the ability of the proposed dispatch policy to harmonize the competing demands of transportation and power grids, ultimately highlighting the critical need to integrate operational constraints and objectives from both sectors into the planning model."
https://arxiv.org/html/2411.10262v1,"Observer-Based Safety Monitoring of Nonlinear Dynamical
Systems with Neural Networks via Quadratic
Constraint Approach","The safety monitoring for nonlinear dynamical systems with embedded neural network components is addressed in this paper. The interval-observer-based safety monitor is developed consisting of two auxiliary neural networks derived from the neural network components of the dynamical system. Due to the presence of nonlinear activation functions in neural networks, we use quadratic constraints on the global sector to abstract the nonlinear activation functions in neural networks. By combining a quadratic constraint approach for the activation function with Lyapunov theory, the interval observer design problem is transformed into a series of quadratic and linear programming feasibility problems to make the interval observer operate with the ability to correctly estimate the system state with estimation errors within acceptable limits. The applicability of the proposed method is verified by simulation of the lateral vehicle control system.","Complex dynamical systems, such as autonomous vehicles and various cyber-physical systems (CPS), have been greatly benefiting from the fast advancement of artificial intelligence (AI) and machine learning (ML) technologies. Many new theories have been proposed on this basis, such as stable neural network controllers and observers (Wu \BOthers., \APACyear2014; Levin \BBA Narendra, \APACyear1992; L. Zhang \BOthers., \APACyear2017), adaptive neural network controllers (Takahashi, \APACyear2017; Niu \BOthers., \APACyear2020) and various neural network controllers (Hunt \BOthers., \APACyear1992). Real-time monitoring of these dynamical systems embedded with neural network components is essential to ensure the system’s safety. External inputs may have adversarial effects on the normal working state of the system; even with the most advanced neural networks, imperceptible perturbations in the input may lead to an erroneous result (Moosavi-Dezfooli \BOthers., \APACyear2017). In addition, these systems are highly susceptible to erroneous outputs if they are subjected to adversarial attacks, which can have serious safety consequences. Therefore, to ensure the security of dynamical systems embedded in neural networks, it is essential to develop a technique that can monitor the operational state of dynamical systems in real time. Most current approaches to safety or security verification take the form of offline computation. In general, verification using offline calculation requires a large amount of computational resources due to its high computational complexity. For example, for a type of neural networks with the activation function of rectified linear unit (ReLU), the safety verification problem can be represented as various complex computational problems. Based on polyhedral operations, a geometric computation method is proposed to obtain the exact output set of the neural network using ReLU activation function (Xiang, Tran, Rosenfeld\BCBL \BBA Johnson, \APACyear2018; Xiang \BOthers., \APACyear2017\APACexlab\BCnt2). Based on those results, the methods in (Tran, Manzanas Lopez\BCBL \BOthers., \APACyear2019; Tran, Musau\BCBL \BOthers., \APACyear2019) extended it by proposing a novel approach with the aid of a specific convex set representation called star sets, which greatly improved scalability. A mixed-integer linear programming (MILP) method to validate neural networks was proposed in (Lomuscio \BBA Maganti, \APACyear2017). The work (Dutta \BOthers., \APACyear2019) focuses on neural networks with ReLU activation functions; they used a Taylor-model-based flowpipe construction scheme and replaced the neural network feedback with a polynomial mapping approach for a small fraction of the input to obtain an over-approximated reachable set. In addition, this method can be extended to other activation units after processing by segmental linearization (Dutta \BOthers., \APACyear2018). The work (Xiang, Tran\BCBL \BBA Johnson, \APACyear2018) introduces a simulation-based approach to output reachability estimation for neural networks with common activation functions. This paper (Xiang \BOthers., \APACyear2021) takes the dynamic system embedded in the feedforward neural network named multilayer perceptrons (MLPs) as the research object, and develops a recursive algorithm with over-approximating the reachable set of the closed-loop system. The security verification of the system is achieved by checking the emptiness of the intersection between the insecure sets and the over-approximation of the reachable sets. It is worth noting that the open-loop computational structure of these offline methods makes them quite challenging to implement in online settings. On the other hand, offline methods are difficult to detect system security issues in a timely manner, and the system state and parameters may differ from run-time when offline. Therefore, developing an online security monitoring method is very important. For this reason, inspired by observer design theory, we propose an alternative solution to design closed-loop systems for run-time monitoring based on instantaneous measurements of the system. We resort to develop interval observers for dynamical systems with neural networks. The interval observer can estimate the upper and lower bounds of the operating state trajectory of the dynamical system in real-time, which can achieve real-time safety monitoring of the dynamical system (Efimov \BBA Raïssi, \APACyear2016; Chebotarev \BOthers., \APACyear2015; Bolajraf \BOthers., \APACyear2011; Cacace \BOthers., \APACyear2015; Y\BHBIw. Zhang \BOthers., \APACyear2020). As shown in (Xiang, \APACyear2021), unlike the general interval observer design approach, the observer gains as well as auxiliary neural networks have to be designed through a series of optimization problems to ensure that the interval observer can correctly estimate the upper and lower state bounds and a suitable estimation error. The design of the auxiliary neural networks in the interval observer is also necessary to simulate the behavior of the neural network in the original system for better state estimation. The work (D. Zhang \BOthers., \APACyear2020) applies interval observers to the safety monitoring of the state of charge (SOC) of lithium-ion batteries. The coupled equivalent circuit-thermal model is adopted in this paper, avoiding the complex structure and calculation caused by the traditional model with electrically and thermally coupled parallel connection of cells. The innovation of the work lies in considering cell heterogeneity as the uncertainty bounding functions and achieving the separation of the state number of interval observers from the number of parallel batteries. During the design of interval observers, it is challenging to apply classical control theory, such as Lyapunov theory, for analyzing dynamical systems embedded in neural network components due to the various types of nonlinear activation functions in neural networks. A popular approach is using quadratic constraints (QCs) to abstract the nonlinear activation functions in neural networks. The work (Anderson \BOthers., \APACyear2007) analyzes the stability of the feedback loop, including neural networks, by replacing the nonlinear and time-varying components of the neural networks with integral quadratic constraints (IQCs). Quadratic constraints are used to abstract the nonlinear activation functions and projection operators in neural network controllers in (Hu \BOthers., \APACyear2020), enabling the reachability analysis of closed-loop systems with neural network controllers. The approach in (Fazlyab \BOthers., \APACyear2022) uses quadratic constraints to abstract various properties of the activation function, such as bounded slope, monotonicity, and cross-layer repetition, thus formulating the safety verification problem for neural networks as the SDP feasibility problem. In addition, the characterization of the input-output of neural networks through quadratic constraints allows other issues to be solved, such as the input-output sensitivity analysis of neural networks (Xiang, Tran\BCBL \BBA Johnson, \APACyear2018), safety verification and robustness analysis (Fazlyab \BOthers., \APACyear2022), Lipschitz constant estimation of feedforward neural networks (Fazlyab \BOthers., \APACyear2019), etc. Synthesizing the previous discussions, the main contributions of this paper are as follows: (1) A global quadratic constraint formulation method for error dynamic systems is discussed; (2) A novel interval observer design method is proposed for the nonlinear dynamical systems with neural networks, and its core contribution is to abstract the nonlinear activation function of neural networks by the quadratic constraints method, so that some control theories applicable to linear systems can also be applied to the nonlinear dynamical systems with neural networks in this paper. The rest of the paper is organized as follows. In Section II, the system and problem formulation under discussion are presented. The main findings are given in Section III, where the design methods for quadratic constraints on the activation function and auxiliary neural networks are presented, and the design of the interval observer gains \underline{L} and \overline{L} is represented in the form of a series of convex optimization problems. The conclusion obtained is applied to a lateral control system for vehicles in Section IV. In Section V, conclusions and future research directions are given. Notations: In this paper, the notation \mathbb{R} represents real numbers, and \mathbb{R}_{+} is defined by \mathbb{R}_{+}=\{\tau\in\mathbb{R},\tau\geq 0\}. The notation \mathbb{R}^{n} represents the vector space of all n-tuples of real numbers, and \mathbb{R}^{n\times n} is the space of n\times n matrices with real entries. The superscript “T” denotes the matrix transpose. The block diagonal matrix is denoted by the symbol diag\{\cdots\}. The notation I_{n}\in\mathbb{R}^{n\times n} denotes the n-dimensional identity matrix. Given a matrix A\in\mathbb{R}^{m\times n}, \|A\| denote its Frobenius norm. For two vectors x_{1},x_{2}\in\mathbb{R}^{n} or matrices A_{1},A_{2}\in\mathbb{R}^{n\times n}, the relations x_{1}<x_{2} and A_{1}<A_{2} are interpreted elementwisly. The relation Q\succ 0 (Q\prec 0) means that Q\in\mathbb{R}^{n\times n} is positive (negative) definite. In addition, Q>0 (Q\geq 0) means that all elements in this matrix Q\in\mathbb{R}^{n\times n} are positive (nonnegative). \mathbb{M}_{n}\in\mathbb{R}^{n\times n} is defined as the collection of all n-dimensional Metzler matrices."
https://arxiv.org/html/2411.10243v1,"Style for IFAC Conferences & Symposia: Use Title Case for
Paper Title","These instructions give you guidelines for preparing papers for IFAC technical meetings. Please use this document as a template to prepare your manuscript. For submission guidelines, follow instructions on paper submission system as well as the event website.","This document is a template for LaTeX 2ε. If you are reading a paper or PDF version of this document, please download the electronic file ifacconf.tex. You will also need the class file ifacconf.cls. Both files are available on the IFAC web site. Please stick to the format defined by the ifacconf class, and do not change the margins or the general layout of the paper. It is especially important that you do not put any running header/footer or page number in the submitted paper.111 This is the default for the provided class file. Use italics for emphasis; do not underline. Page limits may vary from conference to conference. Please observe the page limits of the event for which your paper is intended."
https://arxiv.org/html/2411.10240v1,Efficient Neural Hybrid System Learning and Transition System Abstraction for Dynamical Systems,"This paper proposes a neural network hybrid modeling framework for dynamics learning to promote an interpretable, computationally efficient way of dynamics learning and system identification. First, a low-level model will be trained to learn the system dynamics, which utilizes multiple simple neural networks to approximate the local dynamics generated from data-driven partitions. Then, based on the low-level model, a high-level model will be trained to abstract the low-level neural hybrid system model into a transition system that allows Computational Tree Logic Verification to promote the model’s ability with human interaction and verification efficiency.","In recent years, the development of neural networks has received particular attention in various fields, including natural language processing Wang et al. (2023b), computer vision Stefenon et al. (2022), etc. The applications of neural networks in system identification hold significant promise for they provide a precise approximation of the dynamics while requiring no prior knowledge of the system’s mechanism. Neural networks serve as a predominant approach in machine learning, renowned for their exceptional ability to model complex phenomena with limited prior knowledge. Their proficiency in capturing intricate patterns in data offers valuable insights for dynamical system modeling, verification, and control. However, neural networks are opaque, limiting our ability to validate them solely from an input-output perspective. This opacity also renders neural network models vulnerable to perturbations Zhang et al. (2021),Yang et al. (2022). When it comes to applications in safety-critical scenarios, it requires time-consuming reachability analysis of the specific trajectories for verification, which poses challenges to real-time applications. According to Brix et al. (2023), the computational efficiency is highly related to the scale of the neural network model. This paper aims to promote the interpretability and computational efficiency of neural networks in dynamical system modeling by introducing a novel dual-level modeling framework. Specifically, our proposed approach will divide dynamical system modeling into two essential levels: the low-level neural hybrid system model and its high-level transition system abstraction. The low-level model is employed to precisely capture the system’s local behavior and enhance the computational efficiency with a parallel set of shallow neural networks aimed at approximating the local dynamics. Then the high-level transition model, which is an abstraction based on neural hybrid systems, can be obtained based on reachability analysis designed to capture relationships and transition patterns among system subspaces. The contributions of this paper are summarized as follows. • Maximum Entropy partitioning is applied to partition the system state space into multiple local subspaces, which allows analysis of the dynamics within local subspaces. • A concept of neural hybrid systems is proposed for distributed training and verification of a set of shallow neural networks, thereby enhancing computational efficiency. • A novel transition system abstraction method is proposed to investigate the transition relationships between local partitions, which will further enhance model interpretability. This paper is organized as follows: Preliminaries and problem formulations are given in Section II. The main result, the dual-level modeling framework, is given in Section III. In Section IV, modeling of the LASA data sets is given to illustrate the effectiveness of our proposed framework111The developed modeling tool and code for experiments are publicly available online at: https://github.com/aicpslab/Dual-Level-Dynamic-System-Modeling.. Conclusions are given in Section V. 1.0.1 Notations. In the rest of the paper, \mathbb{N} denotes the natural number sets, where \mathbb{N}^{\leq n} indicates \{1,2,\cdots,n\}, \mathbb{R} is the field of real numbers, \mathbb{B} is the set of the Boolean variables, \mathbb{R}^{n} stands for the vector space of n-tuples of real numbers, and \underline{X} and \overline{X} are the lower bound and upper bound of an interval X, respectively."
https://arxiv.org/html/2411.10166v1,Two-Stage Robust Optimal Operation of Distribution Networks using Confidence Level Based Distributionally Information Gap Decision,"This paper presents a confidence level-based distributionally information gap decision theory (CL-DIGDT) framework for the two-stage robust optimal operation of distribution networks, aiming at deriving an optimal operational scheme capable of addressing uncertainties related to renewable energy and load demands. Building on conventional IGDT, the proposed framework utilizes the confidence level to capture the asymmetric characteristics of uncertainties and maximize the risk-averse capability of the solution in a probabilistic manner. To account for the probabilistic consideration, the imprecise Dirichlet model is employed to construct the ambiguity sets of uncertainties, reducing reliance on precise probability distributions. Consequently, a two-stage robust optimal operation model for distribution networks using CL-DIGDT is developed. An iterative method is proposed to solve the model and determine the upper and lower bounds of the objective function. Case study demonstrates that the proposed approach yields a more robust and statistically optimized solution with required accuracy compared to existing method, contributing to a reduction in first-stage cost by 0.84%, second-stage average cost by 6.7%, and significantly increasing the reliability of the solution by 8%.","Distribution networks (DNs) are increasingly integrating high shares of renewable energy sources (RESs). The intermittent and volatile nature of RESs, along with the uncertainty in load demands, can affect both the economics and security of the networks [1]. For instance, failure to appropriately address uncertainty may increase operational costs due to actions such as load shedding, RESs curtailment, and the need for excessive reserves. Moreover, potential risks may incur, such as voltage violations and line overloading. These significant challenges to the optimal operation of DNs may undermine the transition to more sustainable energy systems. Accordingly, numerous studies have been conducted to manage the uncertainties associated with RESs and load demands, aiming to develop robust operational strategies that enhance the cost-effectiveness and reliability of DNs. Most research on the optimal operation of DNs under uncertainty relies on varying levels of information about the random variables: complete, partial, or none at all. Ref. [2] models the wind power loss and line overloading as chance constraints using a truncated normal distribution, establishing a two-stage stochastic programming (SP) model for unit commitment. Ref. [3] presents a two-stage stochastic dynamic economic dispatch model that effectively addresses wind power uncertainty. This model improves cost efficiency and reliability by pre-dispatching generator output to avoid network congestion in the first stage and re-dispatching resources after the realization of the wind scenarios. Ref. [4] incorporates multiple correlations of RESs through probability distributions (PDs) and scenario analysis into a multi-time scale SP framework. By managing slow- and fast-response resources to coordinate active and reactive power, the economical performance and the secure operation of the system are ensured under RESs uncertainty. SP features the need for PDs or uncertain scenarios, making its performance closely tied to the accuracy of PDs or the number of scenarios, which can be well guaranteed with an incredibly large amount of historical data. Robust optimization (RO) uses the boundary information of uncertainties, making it a practical choice for the optimal operation of DNs. However, solutions derived from RO tend to be conservative because they focus solely on the worst-case scenario within a predefined uncertainty set, ignoring any information on the uncertainties. To mitigate the conservatism, Ref. [5] searches the extreme scenarios to redesign traditional uncertainty sets within a two-stage RO framework, yielding a less conservative yet robust solution for determining the tap ratio of transformers and the capacity of switching capacitors. Similarly, Ref. [6] reconstructs the uncertainty set of wind power based on historical data, aiming to reduce conservatism in economic dispatch. In addition to RO, distributionally robust optimization (DRO) incorporates partial information on uncertainties to minimize operational cost over the worst-case PD within ambiguity sets constructed from historical data. This approach leverages available information on uncertainties, thereby reducing conservatism. For instance, Ref. [7] introduces a moment-based DRO to characterize renewable generation uncertainty for real-time power dispatch in DNs. Ref. [8] constructs an ambiguity set of wind power using Wasserstein-based DRO, followed by a chance constraint-based DRO model for reactive power dispatch. In Ref. [9], the imprecise Dirichlet model (IDM) is used to construct the ambiguity set of wind power, with a conditional value at risk (CVaR) indicator to identify the worst-case distribution, aiming to balance operational costs and the risks of wind curtailment in real-time energy dispatch. It is well-established that a larger uncertainty set (ambiguity set) increases robustness whereas reduces economic performance. Several critical challenges thus arise in the optimization process, such as the size of the predefined uncertainty set, the amount of operational cost budget allocated, as well as the trade-off between robustness and economic performance. Many of the aforementioned approaches fail to simultaneously address these challenges effectively. Information gap decision theory (IGDT) provides a robust decision-making framework for handling severe uncertainties without requiring any information or a predefined uncertainty set [10]. Risk-averse based-IGDT seeks to optimize the uncertainty set within a preset financial budget, which is particularly useful when there is a clear target, such as a desired budget for the operation of DNs. For example, Ref. [11] models the uncertainty of load demand with limited data to provide a robust optimal decision for IGDT-based three-phase optimal power flow. To address frequency excursions resulting from load and RESs fluctuations, an IGDT-based energy management system is proposed for islanded microgrids in Ref. [12]. Ref. [13] utilizes IGDT to find a robust operation scheme against uncertainties in wind power, solar power, and load for resiliency-oriented DNs. The envelop-bound uncertainty modeling of IGDT has extensively applied in DNs operation [11, 12, 13], network planning [14], electricity market offering strategy [15], etc., due to its capability to handle forecasted types of uncertainty sources [10]. Despite the advantages of IGDT, its uncertainty modelling does not incorporate specific information on uncertainties, and the real-valued symmetric uncertainty sets limit the approach to capture the asymmetric characteristics of uncertainties. The asymmetry of uncertainties primarily refers to scenarios that deviate significantly from the expected scenarios but still have a relatively high probability of occurrence, such as those found in the heavy tails of PDs. These scenarios can have substantial impacts when they occur. However, many existing uncertainty modelling approaches fail to account for these significant scenarios probabilistically, or neglect to incorporate the desired operational cost budget when defining uncertainty sets. While SP considers the probability of scenarios, as previously discussed, its accuracy is highly dependent on the distribution type and the size of the dataset. Base on the above discussion, we propose a novel uncertainty modelling approach that utilizes confidence level to capture the asymmetric characteristics of uncertain parameters and maximize risk-averse capability in a probabilistic manner within the IGDT framework, while adhering to a desired budget. The IDM is employed to construct the ambiguity sets, leveraging available data and mitigating the dependency on PDs. Consequently, the confidence level-based distributionally IGDT (CL-DIGDT) framework is developed for the two-stage robust optimal operation of DNs. An iterative method is proposed to solve the model and determine the upper and lower bounds of the objective function. Overall, the main contributions of this paper are as follows: • A two-stage DNs optimal operation model is constructed using CL-DIGDT. In the proposed framework, the confidence interval replaces the real-valued symmetric interval of IGDT, which captures the asymmetric characteristics of the uncertainties associated with RESs and load demands. Additionally, the newly defined objective function ensures the robustness of the solution by describing better the risk-averse capability in a probabilistic way, compared with the traditional IGDT. • IDM is utilized to construct the ambiguity sets for RESs and load demands, and integrated into the proposed optimization model. This method reduces reliance on precise PDs and leverages the available data at hand. • To solve the integrated model, an iterative method is employed to determine the upper and lower bounds of the objective function. The iterative process continues until the objective value converges to a specified accuracy, at which point the solution is deemed sufficiently precise."
https://arxiv.org/html/2411.10096v1,Neural Port-Hamiltonian Models for Nonlinear Distributed Control: An Unconstrained Parametrization Approach,"The control of large-scale cyber-physical systems requires optimal distributed policies relying solely on limited communication with neighboring agents. However, computing stabilizing controllers for nonlinear systems while optimizing complex costs remains a significant challenge. Neural Networks (NNs), known for their expressivity, can be leveraged to parametrize control policies that yield good performance. However, NNs’ sensitivity to small input changes poses a risk of destabilizing the closed-loop system. Many existing approaches enforce constraints on the controllers’ parameter space to guarantee closed-loop stability, leading to computationally expensive optimization procedures. To address these problems, we leverage the framework of port-Hamiltonian systems to design continuous-time distributed control policies for nonlinear systems that guarantee closed-loop stability and finite \mathcal{L}_{2} or incremental \mathcal{L}_{2} gains, independent of the optimzation parameters of the controllers. This eliminates the need to constrain parameters during optimization, allowing the use of standard techniques such as gradient-based methods. Additionally, we discuss discretization schemes that preserve the dissipation properties of these controllers for implementation on embedded systems. The effectiveness of the proposed distributed controllers is demonstrated through consensus control of non-holonomic mobile robots subject to collision avoidance and averaged voltage regulation with weighted power sharing in DC microgrids.","Distributed control of large-scale systems presents significant challenges even in seemingly basic scenarios due to the constrained flow of information in real-time. Particularly, Witsenhausen’s counter-example [1] demonstrates that even under apparently ideal conditions (i.e. linear dynamics, quadratic loss, and Gaussian noise), a nonlinear distributed control policy can outperform the best linear one. The work [2] has provided necessary and sufficient condition, namely, Quadratic Invariance (QI), under which a distributed optimal controller is linear and can be designed by solving a convex optimization problem. However, real-world systems often violate the QI assumption due to inherent nonlinearities, non-convex control costs, or privacy limitations [3]. This necessitates venturing beyond linear control and exploring highly nonlinear distributed control policies, such as those parametrized by deep Neural Networks (NNs). NNs have proved their capabilities in learning-enabled control [4, 5, 6, 3, 7, 8], and system identification [9, 10, 11, 12, 13, 14] of non-linear dynamical systems. Indeed, NN control has been applied in diverse contexts, such as robotics [4], epidemic models [15], safe path planning [6], and nonlinear consensus [16]. Existing approaches to NN control design also include modelling the system under control as a NN from data [17, 18, 19, 20]. Nevertheless, NNs can be susceptible to small changes in their inputs [21]. This fragility can readily lead to control policies jeopardizing closed-loop stability [11], thereby hindering their deployment in large-scale, safety-critical applications [6]. In this paper, we leverage the well-established port-Hamiltonian (pH) system framework [22] to provide an unconstrained parametrization of distributed control policies that are inherently endowed with a finite \mathcal{L}_{2} or an incremental \mathcal{L}_{2} (i\mathcal{L}_{2}) gain. This allows casting optimal control design into an unconstrained optimization problem that can be solved using standard gradient-based methods such as stochastic gradient descent or its variants. As a consequence, our approach eliminates the need for computationally expensive procedures, such as the projection of parameters or constrained optimization techniques, which are typically required to guarantee closed-loop stability [6]. Specifically, if both the underlying system to be controlled and the proposed controller satisfy the conditions of the small-gain theorem [22], our framework can ensure closed-loop stability both during and after the training. Moreover, the learned distributed policies are optimal in the sense that they strive to minimize an arbitrary nonlinear cost function over a finite horizon. Related work: NNs have shown promise in designing both static and dynamic distributed control policies for large-scale systems. Notably, Graph Neural Networks (GNNs) have achieved impressive performance in applications like vehicle flocking and formation flying [23, 24, 25, 26] thanks to their inherent scalable structure. However, guaranteeing stability with general GNNs remains challenging, often requiring restrictive assumptions like linear, open-loop stable system dynamics or sufficiently small Lipschitz constants [26]. Such limitations can be impractical, potentially leading to system failures during the training phase before an optimal policy can be found [27, 28]. Some remedies to rectify this problem include improving an initial known safe policy iteratively, while imposing the constraint that the initial region of attraction does not shrink [29, 30, 31], and leveraging integral quadratic constraints to enforce the closed-loop stability [32]. However, these approaches explicitly constrain the optimization parameters of NNs, which may lead to infeasibility or hinder the closed-loop performance. In contrast, our proposed method based on unconstrained parametrizations provides the same scalability as GNNs without imposing any constraints on the optimization parameters to guarantee closed-loop stability. Previous works also explored stable-by-design control based on mechanical energy conservation [33, 34], but these methods are limited to specific systems (e.g., with SE(3) dynamics). Our approach, instead, applies to a much wider range of nonlinear systems. Recently, the notion of unconstrained parametrization has emerged for learning-enabled control, where a controller is parametrized such that it satisfy specific constraints (e.g., semi-definite constraints) by design, i.e. without being constrained. This allows one to bypass computationally expensive a posteriori verification routines. Based on this approach, the framework of Recurrent Equilibrium Networks (RENs) has been proposed in [11]. RENs are a class of discrete-time nonlinear dynamical models that enjoy built-in stability and robustness properties. Notably, subsets of RENs can satisfy desired integral quadratic constraints regardless of their optimization parameters and ensure a finite \mathcal{L}_{2} gain. Despite their flexibility, RENs face some key limitations. Firstly, they consider dynamics that are dissipative with respect to quadratic storage functions, potentially limiting their expressiveness for complex systems. Secondly, the unconstrained parametrization approach in [11, 35] cannot be directly applied to distributed systems where sparsity patterns in weight matrices are crucial. In contrast, our framework based on pH models allows the use of arbitrary nonlinear storage functions to capture more complex dynamics. Additionally, it seamlessly integrates desired sparsity patterns into the optimization parameters, enhancing flexibility without compromising stability. Since our framework is based on pH systems, a limitation of our controllers is that they have the same number of inputs and outputs. This is not required for RENs. Building on RENs, the work [36] presents an unconstrained parametrization approach for interconnecting subsystems with finite \mathcal{L}_{2} gain, while guaranteeing the \mathcal{L}_{2} stability of the overall system. However, this approach is limited to quadratic storage functions for subsystems, constraining flexibility and generalization. The work [3] presented a distributed framework based on pH systems that ensure passivity by design. However, it assumes that subsystems are in the pH form as well. Moreover, it does not guarantee finite \mathcal{L}_{2} or i\mathcal{L}_{2} gains for the closed-loop system which is instead our main result. Unlike passivity, a finite \mathcal{L}_{2} gain guarantees stability even in the presence of external disturbances or modeling errors, which is crucial for safe operation in uncertain environments [22, 37]. Contributions: The main contributions of this paper can be summarized as follows: 1. We provide an unconstrained parametrization of a class of distributed controllers in the pH form that can seamlessly incorporate sparsity in their weight matrices and are inherently endowed with a finite \mathcal{L}_{2} or i\mathcal{L}_{2} gain. 2. Our approach overcomes the limitation of being restricted to specific storage functions (e.g., quadratic), enabling its application to a broader range of nonlinear control problems. 3. We illustrate how discrete gradient methods [38] can be leveraged to preserve the dissipative properties when continuous-time pH controllers are discretized in time for implementation purposes. 4. We demonstrate the effectiveness of our controllers on a benchmark consensus problem for non-holonomic agents subject to a collision avoidance constraint, and weighted power sharing and average voltage regulation in DC microgrids. Organization: Section II presents the problem formulation. We then introduce unconstrained parametrizations of distributed controllers using pH models that guarantee a finite \mathcal{L}_{2} gain (Section III), or a finite i\mathcal{L}_{2} gain (Section IV). Moreover, Section V discusses the discretization methods to preserve these gains for practical implementation. Finally, the performance evaluation of our proposed pH controllers is presented in Section VI, while Section VII concludes the paper. A preliminary version of this work appeared in [39], which is, however, substantially different from the present paper. Indeed, the following results were not provided: (i) the unconstrained parametrization of a finite i\mathcal{L}_{2} gain (ii) the use of dissipation-preserving discretization schemes for implementation purposes, and (iii) a detailed proof of the unconstrained parametrization of pH models with finite \mathcal{L}_{2} gains. General notation: The set of non-negative real numbers is \mathbb{R}_{+} and the standard Euclidean 2-norm is denoted by \|\cdot\|. The identity matrix of size n is denoted by I_{n}, a zero/null matrix of dimension n\times m is given by 0_{n\times m}, and \mathds{1} is a vector of all ones with an appropriate dimension. The maximal eigenvalue of a matrix {A} is represented by \bar{\lambda}({A}). We represent the set of \mathbb{R}^{n}-valued Lebesgue square-integrable functions by \mathcal{L}_{2}^{n}:=\{v:[0,\infty)\rightarrow\mathbb{R}^{n}|\|v\|_{2}^{2}:=% \int_{0}^{\infty}v(t)^{\top}v(t)dt<\infty\}. We omit the dimension n whenever it is clear from the context. Then, for any two v,w\in\mathcal{L}_{2}^{n}, we denote the \mathcal{L}_{2}^{n}-inner product as \langle v,w\rangle:=\int_{0}^{\infty}v(t)^{\top}w(t)dt. Define the truncation operator (P_{\mathcal{T}}v)(t):=v(t) for t\leq\mathcal{T}; (P_{\mathcal{T}}v)(t):=0 for t>\mathcal{T}, and the extended function space \mathcal{L}_{2e}^{n}:=\{v:[0,\infty)\rightarrow\mathbb{R}^{n}|P_{\mathcal{T}}v% \in\mathcal{L}_{2},\forall\mathcal{T}\in[0,\infty)\}. For any linear space \mathcal{U} endowed with a norm \|\cdot\|_{\mathcal{U}}, we define a Banach space \mathcal{L}_{2e}(\mathcal{U}) that consists of all measurable functions f:\mathbb{R}_{+}\mapsto\mathcal{U} such that \int_{0}^{\infty}\|f(t)\|^{2}_{\mathcal{U}}dt<\infty. Throughout this paper, a system will be specified by an input–output map \Sigma:\mathcal{L}_{2e}^{m}\rightarrow\mathcal{L}_{2e}^{p} satisfying \Sigma(0)=0. Given two systems \Sigma_{1} and \Sigma_{2}, the standard negative feedback configuration between them is denoted by \Sigma_{1}\|_{f}\Sigma_{2}, see Fig. 2. Let \mathcal{G}=(\mathcal{V},\mathcal{E}) be an undirected graph with nodes \mathcal{V}=\{1,\dots,N\} and edges \mathcal{E}, and let \mathcal{P}\in\{0,1\}^{N\times N} be the corresponding adjacency matrix. For a binary mask \mathcal{M}\in\{0,1\}^{m\times n}, we denote \bm{W}\in\texttt{blkSparse}(\mathcal{M}) if \bm{W} is a block matrix composed by m\times m blocks and \mathcal{M}_{i,j}=0\Rightarrow\bm{W}_{i,j}=0. \bm{A}=\texttt{blkdiag}(A_{i}) represents a block-diagonal matrix with matrices A_{0},A_{1},\dots,A_{i} on the diagonal. Preliminaries: Consider the following non-linear system \displaystyle\dot{x}(t) \displaystyle=f\left(x(t),u(t)\right), (1a) \displaystyle y(t) \displaystyle=h(x(t))\;, (1b) where x\in\mathcal{X}\subseteq\mathbb{R}^{n} is the state, u\in\mathcal{U}\subseteq\mathbb{R}^{m} is the input, and y\in\mathcal{Y}\subseteq\mathbb{R}^{p} is the output of the system \Sigma. Moreover, assume there exists a unique solution x(t) on the infinite time interval [0,\infty) of (3a) for all initial conditions x(0)\in\mathcal{X}, and u(\cdot)\in\mathcal{L}_{2e}(\mathcal{U}). We recall some important results and definitions that are used to obtain the main results of this paper. Definition 1 (Dissipativity, [22]) The system \Sigma is called dissipative w.r.t. to a supply rate s:\mathcal{U}\times\mathcal{Y}\mapsto\mathbb{R}, if there exists a smooth storage function V:\mathcal{X}\mapsto\mathbb{R}_{+}111For \mathcal{X}=\mathbb{R}^{n}, the storage function has to be radially unbounded, that is V(x)\rightarrow\infty, whenever \|x\|\rightarrow\infty [22, Theorem 3.2.4]. such that \dot{V}({x}(t))\leq s(u(t),y(t)),\quad\forall t\in\mathbb{R}_{+}\;, or equivalently, V({x}(\tau))-V({x}(0))\leq\int_{0}^{\tau}s_{i}(u(t),y(t))dt\;, for every input signal {u}(t)\in\mathcal{U}, output signal y(t)\in\mathcal{Y} and \tau\geq 0. Moreover, the choice of the supply rate leads to different notions of dissipativity, for instance, \bullet if p=m, and s(u(t),y(t))=u(t)^{\top}y(t), then system \Sigma is passive; \bullet if p=m, and s(u(t),y(t))=u(t)^{\top}y(t)-\epsilon\|y(t)\|, then system \Sigma is \epsilon-output strictly passive for \epsilon>0; \bullet for some non-negative constants \gamma,b, if s(u(t),y(t))=\frac{\gamma^{2}}{2}\|u(t)\|-\frac{1}{2}\|y(t)\|, then system \Sigma has a finite \mathcal{L}_{2} gain, i.e. \|y(t)\|\leq\gamma\|u(t)\|+b. Consider the interconnection \Sigma_{1}\|_{f}\Sigma_{2} of two dissipative systems \Sigma_{1}, and \Sigma_{2} in Fig. 2. Then, one can leverage the following result to ensure closed-loop stability. Theorem 1 ([22]) Consider the closed-loop system \Sigma_{1}\|_{f}\Sigma_{2} given in Fig. 2. \bullet (small gain condition) Assume the existence of the \mathcal{L}_{2} gains \mathcal{L}_{2}(\Sigma_{1})\leq\gamma_{1}, and \mathcal{L}_{2}(\Sigma_{2})\leq\gamma_{2}. Then, the closed-loop system \Sigma_{1}\|_{f}\Sigma_{2} is stable with an \mathcal{L}_{2} gain \gamma_{1}.\gamma_{2} if \gamma_{1}.\gamma_{2}<1; \bullet (strict output passivity) Assume that, for any e_{1}\in\mathcal{L}_{2e}(\mathcal{U}_{1}) and e_{2}=0, \Sigma_{1}:\mathcal{L}_{2e}(\mathcal{U}_{1})\rightarrow\mathcal{L}_{2e}(% \mathcal{Y}_{1}) is \epsilon_{1}-output strictly passive, and \Sigma_{2}:\mathcal{L}_{2e}(\mathcal{U}_{2})\rightarrow\mathcal{L}_{2e}(% \mathcal{Y}_{2}) is passive. Then for e_{2}=0, \Sigma_{1}\|_{f}\Sigma_{2} with input e_{1} and output y_{1} has an \mathcal{L}_{2}-gain \leq 1/\epsilon_{1}. We recall that \epsilon-output strict passivity also implies a finite \mathcal{L}_{2} gain not larger than 1/\epsilon [22]. While Theorem 1 guarantees \mathcal{L}_{2} stability of the closed-loop system, in several cases a stronger notion of stability, such as a finite i\mathcal{L}_{2} gain, is required [22]. Incremental stability [40] has garnered increasing interest in recent years due to its potential applications in the synchronization of chaotic systems [41, 42] and nonlinear circuits [43]. Moreover, incremental \mathcal{L}_{2} stability implies that any two trajectories must eventually asymptotically converge to each other, regardless of their initial conditions [44]. Since incremental stability is an incremental dissipativity property, we formally recall the latter notion. Definition 2 ( Incremental Dissipativity, [45]) The system \Sigma is called incrementally dissipative w.r.t. to a supply rate s_{\Delta}:\mathcal{U}\times\mathcal{U}\times\mathcal{Y}\times\mathcal{Y}% \mapsto\mathbb{R}, if there exists a smooth storage function V_{\Delta}:\mathcal{X}\times\mathcal{X}\mapsto\mathbb{R}_{+} such that for any two trajectories ({x},{u},{y}),(\tilde{{x}},\tilde{{u}},\tilde{{y}}) \dot{V}_{\Delta}({x}(t),\tilde{x}(t))\leq s_{\Delta}(u(t),\tilde{u}(t),y(t),% \tilde{y}(t)),\ \forall t\in\mathbb{R}_{+}\;, or equivalently, \displaystyle V_{\Delta}({x}(\tau),\tilde{x}(\tau)) \displaystyle-V_{\Delta}({x}(0),\tilde{x}(0)) \displaystyle\leq\int_{0}^{\tau}s_{\Delta}(u(t),\tilde{u}(t),y(t),\tilde{y}(t)% )dt\;, for every pair of input signals {u}(t),\tilde{u}(t)\in\mathcal{U}, output signals y(t),\tilde{y}(t)\in\mathcal{Y} and every \tau\geq 0. Moreover, the choice of supply rate leads to different notions of dissipativity, for instance, \bullet if p=m, and s_{\Delta}(u(t),\tilde{u}(t),y(t),\tilde{y}(t))=(u(t)-\tilde{u}(t))^{\top}(y(t% )-\tilde{y}(t)), then system \Sigma_{s} is incrementally passive; \bullet if p=m, and s_{\Delta}(u(t),\tilde{u}(t),y(t),\tilde{y}(t))=(u(t)-\tilde{u}(t))^{\top}(y(t% )-\tilde{y}(t))-\epsilon_{\Delta}\|y(t)-\tilde{y}(t)\|, then system \Sigma is \epsilon_{\Delta}-output strictly incrementally passive for \epsilon_{\Delta}>0; \bullet if s_{\Delta}(u(t),\tilde{u}(t),y(t),\tilde{y}(t))=\gamma^{2}_{\Delta}\|u(t)-% \tilde{u}(t)\|-\|y(t)-\tilde{y}(t)\|, then system \Sigma has a finite i\mathcal{L}_{2} gain, i.e. \|y(t)-\tilde{y}(t)\|\leq\gamma_{\Delta}\|u(t)-\tilde{u}(t)\| for a non-negative constant \gamma_{\Delta}, which is called the incremental gain. Note that \epsilon_{\Delta}-output strictly incremental passivity implies an i\mathcal{L}_{2} gain of {1}/{\epsilon_{\Delta}} [22, Proposition 2.2.22]. The following result about closed-loop properties parallels Theorem 1. Theorem 2 ([22]) Consider the closed-loop system \Sigma_{1}\|_{f}\Sigma_{2} given in Fig. 2. \bullet (incremental form of small gain condition) Assume the existence of the i\mathcal{L}_{2} gains \mathcal{L}_{2}^{\Delta}(\Sigma_{1})\leq\gamma_{\Delta,1}, and \mathcal{L}_{2}^{\Delta}(\Sigma_{2})\leq\gamma_{\Delta,2}. Then, the closed-loop system \Sigma_{1}\|_{f}\Sigma_{2} is stable with an i\mathcal{L}_{2} gain \gamma_{\Delta,1}.\gamma_{\Delta,2}<1; \bullet (strict output incremental passivity) Assume that, for any e_{1}\in\mathcal{L}_{2e}(\mathcal{U}_{1}) and e_{2}=0, \Sigma_{1}:\mathcal{L}_{2e}(\mathcal{U}_{1})\rightarrow\mathcal{L}_{2e}(% \mathcal{Y}_{1}) is \epsilon_{\Delta,1}-output strictly passive, and \Sigma_{2}:\mathcal{L}_{2e}(\mathcal{U}_{2})\rightarrow\mathcal{L}_{2e}(% \mathcal{Y}_{2}) is incrementally passive. Then, \Sigma_{1}\|_{f}\Sigma_{2} for e_{2}=0 with input e_{1} and output y_{1} has an i\mathcal{L}_{2}-gain \leq 1/\epsilon_{\Delta,1}. The conditions provided in Definition 2 for ensuring a finite i\mathcal{L}_{2} gain lead to Hamilton-Jacobi inequalities, which are nonlinear infinite-dimensional partial differential constraints that are difficult to satisfy [22]. Another approach, used in this paper, is to analyze the infinitesimal variation in the trajectories, as done in contraction theory [45]. To this end, we introduce the notion of differential dissipativity. Definition 3 (Differential Dissipativity, [45]) Consider the system \Sigma and its variational dynamics \displaystyle\delta\dot{x} \displaystyle=A(x,\delta x)\delta x+B(x,\delta x)\delta u, \displaystyle\delta y \displaystyle=C(x,\delta x)\delta x\;, where A(x,\delta x):=\frac{\partial}{\partial x}f\left(x,u\right), B(x,\delta x):=\frac{\partial}{\partial u}f\left(x,u\right), C(x,\delta x):=\frac{\partial}{\partial x}h\left(x\right) are matrix-valued functions. Then, \Sigma and \Sigma_{\delta} are called differentially dissipative w.r.t. to the supply function {s}_{\delta}:\mathcal{U}\times\mathcal{Y}\rightarrow\mathbb{R}, if there exists a smooth storage function V_{\delta}:\mathcal{X}\times\mathcal{X}\rightarrow\mathbb{R}^{+}, with V_{\delta}(\cdot,0)=0 such that \displaystyle V_{\delta}({x}(t_{1}),\delta{{x}}(t_{1})) \displaystyle-V_{\delta}({x}(t_{0}),\delta{{x}}(t_{0})) \displaystyle\leq\int_{t_{0}}^{t_{1}}s_{\delta}(\delta{u}(t),\delta{y}(t))dt for all t_{0},t_{1}\in\mathbb{R} with t_{0}\leq t_{1} and for all (\delta{x},\delta{u})."
https://arxiv.org/html/2411.10058v1,Unsupervised Congestion Status Identification Using LMP Data,"Having a better understanding of how locational marginal prices (LMPs) change helps in price forecasting and market strategy making. This paper investigates the fundamental distribution of the congestion part of LMPs in high-dimensional Euclidean space using an unsupervised approach. LMP models based on the lossless and lossy DC optimal power flow (DC-OPF) are analyzed to show the overlapping subspace property of the LMP data. The congestion part of LMPs is spanned by certain row vectors of the power transfer distribution factor (PTDF) matrix, and the subspace attributes of an LMP vector uniquely are found to reflect the instantaneous congestion status of all the transmission lines. The proposed method searches for the basis vectors that span the subspaces of congestion LMP data in hierarchical ways. In the bottom-up search, the data belonging to 1-dimensional subspaces are detected, and other data are projected on the orthogonal subspaces. This procedure is repeated until all the basis vectors are found or the basis gap appears. Top-down searching is used to address the basis gap by hyperplane detection with outliers. Once all the basis vectors are detected, the congestion status can be identified. Numerical experiments based on the IEEE 30-bus system, IEEE 118-bus system, Illinois 200-bus system, and Southwest Power Pool are conducted to show the performance of the proposed method.","The locational marginal prices (LMPs) are the foundation of modern electricity markets. They reflect the marginal operation and generation cost of the whole power system at each node [1, 2]. The data of LMPs are calculated and publicized by an independent system operator (ISO) or regional transmission organization (RTO) according to the price bid & offer collected in the day-ahead and real-time electricity market. The problem of security-constrained economic dispatch (SCED) is formulated and solved using linear programming or convex optimization tools. The system information, including network topology, transmission capacity, and line impedance, is included in the SCED and partially determines the distribution of LMP data. Generally, detailed market structure information (e.g., topology and line parameters) is private due to security concerns. Thus, the market participants may not have access to such information. Researchers aim to study the publicized LMP data with additional system information in data-driven approaches. In [3], Geng et al. proposed a multi-class support vector machine (SVM)-based approach to fit the LMP-load coupling. The LMP in a certain power system was modeled as a function of loads at all nodes. The distribution of LMPs in the load space was analyzed and fitted precisely using an SVM. Kekatos et al. aimed to recover network topology from LMP data in [4]. The low-rank feature of the LMP data and sparsity of the topology adjacency matrix were utilized to formulate a convex optimization problem. It has been proved that as long as enough LMP data are given, the connectivity of certain lines could be approximately recovered and tracked. In [5], linear regression was used to recover the potential market structure based on the released LMP data and Lagrange multipliers of the inequality constraints in the SCED problem. However, in some electricity markets, the exact values of the Lagrange multipliers are not publicized. Recent research on LMP [3, 6] has revealed that if the network parameters and generation offers remain the same, the congestion component of the LMP data lies in a discrete space w.r.t. nodal loads. In other words, for the nodal loads that belong to the same system pattern region (SPR), the network congestion status and the congestion LMP vector remains the same when the generation offers are constant. A simple proof using multiparametric linear programming (MLP) was provided in [3], where the SPR was defined as the critical region (CR) in the theory of MLP. A much more rigorous proof and discussion on the SPRs of LMP under both linear and quadratic cases were given in [6]. Most previous works focus on learning the coupling between additional system information and LMP data. We want to utilize the aforementioned characteristics of the congestion LMP vectors and extract hidden messages from them without any additional system information (e.g., nodal load data or topology data, which are usually unavailable). The core work here is to identify the network congestion status/SPRs using only the publicized LMP data. If the SPR of the LMP is identified, the distribution of the congestion LMP vector in the high-dimensional Euclidean space is limited to certain subspaces, which could help in LMP forecasting [7, 8]. Additionally, through congestion status identification, we can know the borderline conditions of the system parameters as well as load and generator quotations. These results can help market participants enhance their understanding of the market structure and thus help to determine their bidding strategy. Some studies show the benefit of SPR/CR information in LMP forecasting and other applications. Zhou et al. exploited the structural aspects underlying the power market operations using SPR to reduce the congestion forecast error [8]. The convex hull of the zonal load was fitted in high-dimensional load space with the QuickHull algorithm. In [9], the short-term probabilistic LMP forecasting problem was considered with load uncertainty from a system operator perspective. CRs were used to map the uncertainty from the load space to the LMP space. Congestion status is also important for bidding in energy markets and financial transmission right (FTR) markets. For example, in [10] the impact of local transmission congestion on energy storage arbitrage opportunities was discussed. A bilevel imperfect competition model was proposed and the results indicated that the network congestion had a large impact on the energy storage profits. In [11], a strategic bidding method considering FTR and demand response was proposed for load serving entities. In [12], Lo Prete et al. studied the usage of unprofitable virtual transactions aiming at altering the network congestion status and improving the FTR revenue based on an equilibrium model. The information of congestion scenarios is fundamental in these models. In [13], an inverse optimization-based approach for estimating rivals’ production cost functions in electricity markets was proposed, and a discussion on its extension was given to take into account the network congestion status. A static generation offering behavior is usually assumed in the application of SPR/CR. Actually, the congestion LMP vector changes within the same subspace when the marginal generation offer fluctuates [14]. Thus, the discrete space that the LMP vector lies in is turned into a continuous space, and we cannot easily identify the congestion status from the LMP. In this paper, we propose an unsupervised method to cluster the LMP data into different subspaces in hierarchical ways with some efficient machine learning techniques. The efficacy of the method is proved by numerical experiments on the IEEE 30-bus system, IEEE 118-bus system, Illinois 200-bus system [15], and Southwest Power Pool (SPP) [16] with real market data. The contribution of this paper is threefold. 1. Problem modeling: We study the problem of identifying the congestion status of the LMP in a data-driven approach. The problem is modeled as recovering basis vectors for the congestion LMP data in high-dimensional Euclidean space. 2. Highly efficient solution framework: By applying efficient machine learning techniques, including principal component analysis, spectral clustering, and dual principal component pursuit in bottom-up and top-down hierarchical searching, the hidden subspaces in LMP data are identified one by one. The bottom-up algorithm can directly find the basis vectors that span the subspaces when there is no basis gap. The top-down algorithm can find the potential subspaces by detecting the norm vectors and addressing the basis gap. 3. Comprehensive experiments: Four numerical cases, including one with real market data, are presented to give a visualized procedure for the hierarchical searching methods. The cases verify the high accuracy and low time consumption of the proposed method. This paper is structured as follows. Section II describes the widely adopted LMP models and analyzes the distribution of congestion LMP vectors. Section III gives the proposed methodology and detailed algorithms. Three numerical cases are conducted in Section IV. Finally, Section V discusses the application of the proposed method and draws the conclusion."
https://arxiv.org/html/2411.10031v1,Enforcing Cooperative Safety for Reinforcement Learning-based Mixed-Autonomy Platoon Control,"It is recognized that the control of mixed-autonomy platoons comprising connected and automated vehicles (CAVs) and human-driven vehicles (HDVs) can enhance traffic flow. Among existing methods, Multi-Agent Reinforcement Learning (MARL) appears to be a promising control strategy because it can manage complex scenarios in real time. However, current research on MARL-based mixed-autonomy platoon control suffers from several limitations. First, existing MARL approaches address safety by penalizing safety violations in the reward function, thus lacking theoretical safety guarantees due to the black-box nature of RL. Second, few studies have explored the cooperative safety of multi-CAV platoons, where CAVs can be coordinated to further enhance the system-level safety involving the safety of both CAVs and HDVs. Third, existing work tends to make an unrealistic assumption that the behavior of HDVs and CAVs is publicly known and rationale. To bridge the research gaps, we propose a safe MARL framework for mixed-autonomy platoons. Specifically, this framework (i) characterizes cooperative safety by designing a cooperative Control Barrier Function (CBF), enabling CAVs to collaboratively improve the safety of the entire platoon, (ii) provides a safety guarantee to the MARL-based controller by integrating the CBF-based safety constraints into MARL through a differentiable quadratic programming (QP) layer, and (iii) incorporates a conformal prediction module that enables each CAV to estimate the unknown behaviors of the surrounding vehicles with uncertainty qualification. Simulation results show that our proposed control strategy can effectively enhance the system-level safety through CAV cooperation of a mixed-autonomy platoon with a minimal impact on control performance.","Connected and Automated Vehicles (CAVs) have demonstrated enormous potential to improve traffic flow [1, 2, 3, 4]. To realize such potential, significant research attention has been placed on the longitudinal control of CAVs within a platoon of vehicles, primarily aimed at optimizing fuel consumption [5, 6], improving string stability [7, 8, 9, 10, 11], addressing privacy issues [12, 13], and enhancing safety [14, 15, 16]. One widely recognized challenge associated with the longitudinal control of CAVs lies in the prolonged transition period with mixed autonomy, during which CAVs and human-driven vehicles (HDVs) coexist, as technological maturity and social acceptance can only evolve gradually. For the control of mixed-autonomy platoons, it is important to coordinate CAVs to improve the performance of the entire platoon, considering the behavior of HDVs. One promising scheme for mixed-autonomy platoon control is the leading cruise control (LCC) framework [7, 17, 8, 18, 11, 19]. Unlike traditional HDVs and CAVs with other longitudinal control schemes (e.g., Adaptive Cruise Control) that only follow their leading vehicles, LCC fundamentally revolutionizes the operation of vehicles by allowing the decision-making process of each CAV to utilize information from both preceding and following vehicles, thereby offering enormous potential to stabilize traffic flow and improve fuel efficiency. A number of control algorithms have been proposed to exploit such potential, whereby the key is (i) to efficiently coordinate CAVs within a platoon and (ii) to model the unknown and potentially heterogeneous behavior of HDVs. For example, Ref. [20] proposes a data-driven control scheme for a single CAV to achieve safe and optimal control in mixed traffic using data-enabled predictive control (DeePC), whereby the behavior of HDVs is implicitly captured via a Hankel matrix comprised of historical data [21]. However, DeePC requires solving a quadratic programming problem, which can be computationally expensive if the number of vehicles in the platoon becomes large. To reduce computation complexity, Ref. [17] utilizes decentralized DeePC, whereby each CAV computes its control input based on locally available data from its involved subsystem. Nevertheless, the construction of the Hankel matrix still assumes linear system dynamics, which may not be able to represent inherently nonlinear HDV behavior. Reinforcement Learning (RL)-based approaches appear to be a promising tool for LCC, which improves computational efficiency and is able to consider nonlinear HDV behavior. For example, Ref. [8] proposes a cooperative RL-based control scheme to enhance string stability and energy-saving performance by coordinating CAVs. Ref. [18] introduces a multi-agent reinforcement learning (MARL) approach, called Communication Proximal Policy Optimization, to enhance driving efficiency in mixed-autonomy platoons while reducing computational complexity. Ref. [11] presents a distributed RL-based controller that leverages the aggregated joint behaviors of HDVs to effectively manage the macroscopic features of a mixed-autonomy platoon and improve control efficiency. Compared to model-based controller, such RL-based approaches can more effectively coordinate multiple CAVs in mixed-autonomy platoons by operating in a decentralized manner, where each CAV relies on local observations while being trained in a centralized framework with a shared goal. Additionally, RL-based methods inherently use a data-driven approach to model the unknown behavior of HDVs, making them better suited to adapt to the nonlinear and unknown dynamics of HDVs. However, existing MARL-based controllers for mixed-autonomy platoons suffer from three limitations. First, existing MARL approaches for mixed-autonomy platoons lack theoretical safety guarantees. The aforementioned works typically consider safety by including a penalty term for safety violations in the reward function for training MARL agents [22], which cannot provide safety guarantees due to the black-box nature of RL. Second, few works have exploited the crucial benefits of multi-CAV platoons, i.e., cooperative safety, whereby multiple CAVs can be coordinated to enhance system-level safety, including their own safety and the safety of other HDVs. Third, existing work often assumes that the behavior of HDVs and other CAVs (except for the ego CAV) is explicitly known and rationale [7, 11, 15]. However, in real traffic scenarios, the behavior of human drivers and other CAVs may be unknown or affected by disturbances due to communication channel interference. Moreover, as introduced in [15], HDVs may exhibit irrational behavior, which may place other vehicles in the platoon in dangerous situations and thereby undermine system-level safety for both HDVs and CAVs. We aim to bridge the aforementioned research gaps by proposing a safe MARL framework for mixed-autonomy platoons. The theoretical foundation of the proposed framework builds on safe RL. Despite its rare application in mixed-autonomy platoon control, safe RL has attracted increasing research attention, especially in robotics. The main safe RL approaches include Constrained Markov Decision Process (CMDP) approaches [23], reachability analysis-based methods [24], and CBF-based approaches [25, 26, 27, 28]. In CMDP-based methods, such as [23], primal-dual techniques are used to constrain the MDP. However, CMDP approaches often lack rigorous theoretical guarantees for persistent safety. Reachability analysis-based approaches, e.g., [24], combine data-driven reachability analysis with a differentiable collision-checking component to ensure system safety, making them applicable in model-free settings while offering theoretical safety guarantees. Despite these strengths, reachability analysis-based methods face the challenge of the “curse of dimensionality”, rendering them computationally inefficient, especially in mixed-traffic environments involving multiple CAVs and HDVs. In this paper, we focus on CBF-based approaches thanks to the ease of combining CBF with MARL in the mixed-autonomy platoon control framework. Specifically, CBF can be used to construct a constraint on the control input, which becomes active only when the safety condition is violated, ensuring the algorithm’s real-time responsiveness without significantly influencing the effectiveness of the controller. Recently, Ref. [26] developed a CBF-quadratic programming (CBF-QP) problem that can compensate for the DRL action such that the compensated action is within a safe set. However, in this work, the CBF-QP is only considered as part of the environment without providing guidance on the training of the DRL model. In contrast, Ref. [28] integrated a CBF-QP-based differentiable safety layer into neural network-based controllers using differentiable QP layer [29], which can ensure safety while enabling backpropagation of the safety layer and thus facilitating the training of neural networks. However, to the best of our knowledge, CBF has rarely been combined with RL in the LCC context. Although our previous work [15] extends [28] and provides a safety guarantee by postprocessing RL actions with an optimization model based on CBF, it builds on single-agent RL that only controls a single CAV without considering the coordination of multiple CAVs. Statement of contribution. The contributions of this paper are three-fold. First, unlike existing methods that neglect the potential benefits of cooperation in enhancing system-level safety, our approach explicitly considers the notion of cooperative safety and characterizes it by a cooperative CBF, enabling CAVs to collaboratively improve the safety of the entire platoon. Second, we provide a safety guarantee to the MARL-based controller by integrating the CBF-based safety constraints into MARL through a differentiable quadratic programming (QP) layer [29], which converts the potentially unsafe RL actions to safe actions. Third, since the cooperation of CAVs involves estimating the decisions of other CAVs and HDVs in the platoon, we devise a conformal prediction module [30, 31] to explicitly quantify the uncertainty of these estimations and integrate it into the CBF-MARL framework to enhance the robustness against such uncertainties. To the best of our knowledge, the integration between conformal prediction and CBF has been rarely applied to ensure the safety and robustness of mixed-autonomy platoon control. The rest of the paper is organized as follows. Section II introduces the background knowledge about MARL and CBF. Section III presents the system dynamics and safety concerns of mixed-autonomy traffic. Section IV provides a detailed description of the proposed cooperative safe MARL-based controller. Section V performs simulations and analyzes the results. Section VI concludes the paper."
https://arxiv.org/html/2411.09990v1,Exploring the Influence of Residential Electric Vehicle Charging on Distribution System Hosting Capacity - A Case-Study in Arizona,"The installation of high-capacity fast chargers for electric vehicles (EVs) is posing a significant risk to the distribution grid as the increased demand from widespread residential EV charging could exceed the technical limits of the distribution system. Addressing this issue is critical, given that current infrastructure upgrades to enhance EV hosting capacity are both costly and time-consuming. Moreover, the inherent uncertainties associated with EV charging parameters make it challenging for power utilities to accurately assess the impact of EVs added to specific locations. To address these knowledge gaps, this study (a) introduces an algorithm to coordinate residential EV charging, and (b) proposes a comprehensive framework that evaluates all transformers within a feeder. The proposed method is applied to a real-world feeder, which includes 120 transformers of varying capacities. The results demonstrate that this approach effectively manages a substantial number of EVs without overloading any of the transformers, while also pinpointing locations that must be prioritized for future upgrades. This framework can serve as a valuable reference for utilities when conducting distribution system evaluations for supporting the growing EV penetration.","The global rise in Electric Vehicle (EV) adoption is primarily motivated by the urgent need to mitigate the greenhouse gas effects and reduce air pollution [1]. Traditional internal combustion engine (ICE) vehicles are considered as one of the major contributors to these environmental issues due to their emissions of greenhouse gasses and pollutants [2]. As such, the transition to EVs offers a promising solution for reducing the environmental footprint of transportation [3]. 1.1 Motivation The expected rise in EV adoption is prompting power utilities to prepare for a significant uptick in EV ownership. For instance, Salt River Project (SRP), a power utility in Arizona, anticipates supporting approximately one EV for every two households by 2035. In addition to environmental consciousness, this surge is being fueled by declining battery prices and various incentives, which has resulted in nearly a doubling of the number of personal EVs in recent years [4]. However, since 84% of EV owners in the United States (U.S.) charge their vehicles at home [5], this trend is underscoring the importance of managing residential charging in the EV ecosystem. 1.2 Related Works Several studies have been conducted to examine the impact of EVs on distribution systems and to propose solutions to manage the associated increase in load. Uncoordinated charging of EVs, such as charging at 16A, has been shown to escalate loading levels of distribution feeders and transformers, cause voltage deviations in residential networks, and necessitate infrastructure upgrades [6, 7, 8]. To address these challenges, [9] proposed a demand response mechanism based on transformer thermal loading to mitigate the negative effects without expanding the existing grid. Refs. [10, 11, 12] focused on transformer-side technical limits under high EV penetration and developed an optimization model for chargeable regions to maximize EV hosting capacity of distribution networks without causing technical issues. Similarly, studies in [13, 14, 15, 16, 17, 18, 19] address the specific challenge of integrating EV loads into distribution systems, with a mutual goal of assessing and enhancing hosting capacity. A key takeaway from these studies is that uncoordinated high-rate charging, particularly at 32A or higher, when adopted on a large scale in residential areas, would significantly worsen the identified issues. This highlights the need for conducting comprehensive investigations by distribution planners to quantify the impact of EV penetration and residential charging practices on the distribution system. Although each study has addressed important aspects of EV charging challenges, a reasonable model that thoroughly evaluates a feeder and quantifies its hosting capacity is still missing. Furthermore, the absence of methods that incorporate a wide range of influencing factors, such as EV owner behavioral patterns, is notable. This is more so because understanding these behavioral patterns is critical for accurately determining charging requirements. For example, studies that evaluate the impact of different charging power assuming uniform driving patterns for all EV owners may not reflect reality where owners have individual charging preferences, schedules, and commuting habits. In [20], the authors emphasized the importance of accounting for behavioral patterns in developing realistic EV load profiles and discussed the consequences of neglecting this factor. However, they assumed outdated chargers with fixed low charging power, whereas modern home chargers offer much higher power. This flexibility introduces a degree of freedom into the evaluation process that they did not consider. Additionally, factors such as the state-of-charge (SOC), play significant roles, further complicating the evaluation of charging impacts on the distribution transformer and eventually, the whole feeder. Treating these parameters independently, without considering their interactions, may not provide system planners with a comprehensive understanding of actual system conditions. Therefore, a holistic end-to-end approach is necessary to confidently assess the system, determine required upgrades, and prioritize locations to avoid potential overloading conditions. 1.3 Contributions of This Article This paper seeks to address the limitations identified in previous studies by proposing a comprehensive framework that allows utilities to effectively evaluate feeders and determine their maximum hosting capacity for EVs. The framework integrates real-world data and models through a novel formulation to provide accurate and reliable insights. One of the key strengths of this framework is its use of actual advanced metering infrastructure (AMI) data provided by SRP, rather than relying on synthetic data. By incorporating actual consumption patterns, the simulations deliver realistic insights into the grid’s capacity to handle the additional load from EVs. Furthermore, the framework leverages actual commuting data from U.S. drivers to accurately model EV owners’ behavioral patterns. This captures the realistic availability and unavailability of EVs for charging, which significantly influences the hosting capacity. In addition to using actual data, the framework also accounts for key influencing factors, including charger types and peak demand patterns. This allows the optimization model to create efficient charging schedules that significantly reduce grid stress. As a result, the proposed framework can mitigate the need for costly infrastructure upgrades, while delivering the required power without overloading the transformers. The proposed coordination model also identifies the optimal number of EVs that can be supported by each transformer. Given the stochastic nature of charging behaviors and grid conditions, the model associates the optimal number with a confidence rate, providing utilities with a reliable measure of system robustness. This approach not only considers individual factors, as previous studies have done, but also addresses the combined effects of multiple interacting variables, giving a more comprehensive understanding of EV integration into the grid. The holistic framework offers utilities practical insights into the EV hosting capacity of the distribution system. It facilitates the integration of a large number of EVs without requiring significant infrastructure upgrades, while also identifying critical locations where transformer replacements may be necessary due to concentrated charging demands. The key contributions of this work are as follows: 1. A framework is introduced to comprehensively evaluate the hosting capacity of distribution feeders, offering a practical solution for utilities. 2. The framework relies on real AMI data, ensuring accurate assessments of system behavior under various scenarios, as opposed to relying on statistical models. 3. The coordination model takes into account key factors such as EV availability, charger types, and peak demand. It optimizes charging schedules to minimize costs for customers while preventing overloading of the associated transformer. 4. The study provides utilities with a method to determine the maximum number of EVs that each transformer can reliably support, along with a confidence rate to account for the stochastic nature of the problem, giving a clearer understanding of system reliability and grid flexibility. By addressing these crucial areas, this work offers utilities a scalable and robust framework for assessing and managing EV hosting capacity in real-world distribution systems. 1.4 Organization of This Article The remainder of this paper is organized as follows: Section 2 discusses the parameters that are critical to modeling an EV load profile. Section 3 gives a detailed description of the developed optimization models. Section 4 presents the proposed end-to-end algorithm for assessing all transformers within a given distribution system. Section 5 showcases the effectiveness of the proposed method and discusses insights derived from the results. The conclusions are provided in Section 6."
https://arxiv.org/html/2411.09956v1,"A Secure Estimator with Gaussian 
Bernoulli Mixture Model","The implementation of cyber-physical systems in real-world applications is challenged by safety requirements in the presence of sensor threats. Most cyber-physical systems, in particular the vulnerable multi-sensor systems, struggle to detect the attack in observation signals. In this paper, we tackle this issue by proposing a Gaussian-Bernoulli Secure (GBS) estimator, which effectively transforms the assessment of sensor status into an optimal estimation problem concerning the system state and observation indicators. It encompasses two theoretical sub-problems: sequential state estimation with partial observations and estimation updates with disordered new observations. Within the framework of Kalman filter, we derive closed-form solutions for these two issues. However, due to their computational inefficiency, we propose the iterative approach employing proximal gradient descent to accelerate the estimation update. We conduct comprehensive experiments from three perspectives: computational efficiency, detection and estimation performance, and characterization of observation error. Our GBS estimator shows the improvements compared to other methods.","Cyber-Physical Systems (CPSs) are integrative frameworks that combine computing and communication capabilities to monitor or control entities within the physical world [1][2]. Over the past two decades, significant advancements in communication and computing technologies have catalyzed the wide applications for CPSs [3]. The advancement in wireless communication technologies has enabled CPSs to transmit real-time signals across expansive physical networks [4]. Furthermore, armed with advanced computational resources, CPS can optimize control strategies for complex systems online in real-time [5]. This technological evolution broadens the scope of CPSs, impacting various sectors such as medical devices [6], smart grids [7], and autonomous vehicles [8]. However, alongside the opportunities, CPSs face significant challenges, with security issues emerging as a paramount concern. A notable incident underscoring this concern involves the RQ-170, an American drone [9][10], which lost control in Iran due to a security breach. This security breach was executed through the manipulation of the drone’s GPS signal, causing the UAV to land in a strategically targeted area. The reliance of many CPSs on remote signal transmission over vulnerable networks opens them up to sensory attacks. These attacks, through the injection of malicious data [11] or disruption of information transmission [12], can significantly degrade a system’s performance, e.g., maximizing the estimation error covariance [13], amplifying the expected estimation error [14]. Against various sensory attacks, two types of detection methods are widely studied. One is the model-based defense mechanisms, e.g., control theory, game theory and cryptography. Detectors based on linear estimation theory, which analyze the Kalman innovation sequence, are widely applied in CPS, especially windows \chi^{2} detector [15], CUSUM detector [16] and MEWMA detector [17]. The framework of game theory also enables the design of the optimal defense policies. For instance, Basar et al. [5] considered the system balance between resilience and robustness, Li et al. [18] obtained the optimal DoS defense strategy by solving the associated Bellman equations, and Meira-Góes et al. [19] improved the efficiency of synthesized robust supervisors against sensor deception attacks. Additionally, with the idea of cryptography, Mo et al. [20] proposed the watermarking approach to authenticate the correct operation of a control system, while Kogiso et al. [21] incorporated the public key encryption scheme into the design of remote controller or observers. Another strategy involves using learning-based methods to identify CPS attacks. The graph neural network (GNN) approach proposed in [22] provided a solution for interpretation and localization of anomalies in secure water treatment, overcoming the challenges of high dimensionality and complexity. Besides, LSTM-RNNs were applied in [23] for learning the temporal behaviour of the data in CPSs and identifying sensor anomalies. Moreover, Wang et al. [24] utilized Deep Belief Network (DBF) to train an interval state predictor for pattern recognition in sensor security. Apart from detecting attacks, designing resilient state estimators played a critical role in mitigating sensor attacks. Mo et al. [25] studied the case of multiple measurements subject to sensor attacks, designing an optimal estimator by solving a minimax optimization problem. In addition, regarding secure estimation problem in noiseless systems, Chang et al. [26] formulated it as the classical error correction problem and provided the sufficient conditions for restoring the real states. Furthermore, Wang et al. [27] considered attacked sensor signals as general outliers, obtaining resilient estimates by identifying and eliminating these outliers. Previous detecting methods have two limitations. First, attack detectors and resilient state estimators are interdependently designed and the connection between them are overlooked. It is challenging to ensure the estimation performance while simultaneously detecting attacks. Second, occasional outliers and persistent malicious attacks are difficult to distinguish, as their Kalman innovation sequences may overlap. If we can separate these outliers from observations, the binary detection problem will become more accurate, thereby reducing the false alarm rate. To overcome these shortcomings, we initiatively formulate the potential anomalies and normal Gaussian noise as a Gaussian-Bernoulli mixture model by introducing observation indicators. By solving a dual-variable estimation problem, our proposed algorithm can simultaneously achieves resilient state estimation and attack detection. The main contributions of this article include: 1. We introduce, for the first time, the Gaussian-Bernoulli mixture model to model the observation model with potential attack risks. Under the mixture model, we propose a novel framework to integrate the attack detection and resilient estimation into a dual-variable optimization problem. 2. Within the framework of Kalman filter and RTS smoother, we derive analytical solutions for two subproblems: sequential state estimation with partial observations and estimation update with disordered new observations. 3. We propose an innovative iterative method that addresses the challenge of non-sequential observation changes. By using the previous estimation as the initial point and employing proximal gradient descent, it rapidly converges to the optimal estimation with updated observations. 4. We test the performance of the GBS estimator under various types and intensities of attacks, and observe a significant improvement compared to previous algorithms. The remainder of the paper is organized as follows. In Section II, we introduce the linear estimation as an application of Bayesian theory in Hidden Markov Chains. In Section III, we describe the model assumptions and the problem we aim to solve. In Sections IV and V, we discuss the core step of our algorithm: updating the state sequence estimation in response to disordered changes in the observation set. In Section VI, we propose our Gaussian-Bernoulli secure estimator and prove its convergence. In Section VII, we demonstrate our algorithm’s performance and compare it with other algorithms. In Section VIII, we conclude the paper. For clarity, this paper focuses on the single-sensor system, but the derivation process and conclusions are also applicable to multi-sensor systems. Notation: For S\in R^{n\times n},S\succ 0, let \|x\|_{S}=\sqrt{x^{T}Sx} denoting the S-weighted 2-norm of x. Denote the state sequence at interval T=[0,N] as x_{0:N}=\{x_{0},\cdots,x_{N}\}. The observation set is denoted as y_{0:N}=\{y_{0},\cdots,y_{N}\}. The observation indicators are denoted as p_{0:N}=\{p_{0},\cdots,p_{N}\}. Let \mathcal{O}\in 2^{\{0,\cdots,N\}} denotes the selected index set, and then y_{\mathcal{O}} denotes the selected observation set. \hat{x}_{i|\mathcal{O}} denotes the optimal estimation of the state x_{i} under the observation set y_{\mathcal{O}}. Let symbol \circ denoting the composition of functions."
https://arxiv.org/html/2411.09913v1,"A Graph-based Strategic Sensor 
Deployment Approach for k-coverage in WSN","This paper studies a graph-based sensor deployment approach in wireless sensor networks (WSNs). Specifically, in today’s world, where sensors are everywhere, detecting various attributes like temperature and movement, their deteriorating lifetime is indeed a very concerning issue. In many scenarios, these sensors are placed in extremely remote areas, where maintenance becomes challenging. As a result, it is not very wise to depend on a single sensor to obtain data from a particular terrain or place. Hence, multiple sensors are deployed in these places, such that no problem arises if one or few of them fail. In this work, this problem of intelligent placement of sensors is modelled from the graph theoretic point of view. We propose a new sensor deployment approach here, which results in lesser sensor density per unit area and less number of sensors as compared to the existing benchmark schemes. Finally, the numerical results also support our claims and provide insights regarding the selection of parameters that enhance the system performance.","With the rapid evolution of applications like Internet of Things (IoT), wireless sensors are everywhere. Wireless sensor network (WSN) is a reality today; the global IoT connections is forecast to reach around 38.9 billion in 2029 [1]. As a result, the lifetime of these sensors is a very critical issue and hence, they need to be regularly monitored and recharged. In this context, there has been research regarding efficient environment aware wireless transmission techniques, which extend the sensor lifetime by avoiding unwanted transmissions [2],[3]. However, in many applications, they are placed in very remote and challenging areas, where their maintenance is extremely challenging. Thus, it is not wise to rely solely on a single sensor placed in such areas for data. In graph theoretical terms, this is termed as 1-coverage, i.e., every point in the region is monitored or covered by at least one sensor. This leads to a more evolved concept of graph theory, which is k-coverage, i.e., all the points in a concerned ‘region’ is covered or monitored by at least k sensors. Note that, the value of k depends on the application at hand; a higher value of k implies that a higher degree of reliability is required, and vice-versa [4]. The simplest possible strategy to attain a certain coverage is the one, where we deploy all the available sensors in the region of concern, such that the defined set of targets are totally covered. However, as stated earlier, these sensors have a limited lifetime and hence, this method is not desirable from the application point of view [5]. Also, this leads to unwanted wastage of resources. The authors in [6] proposed a bound on the sensor density in order to cover a certain region of interest. The work in [7] proposed a sensor deployment strategy, where they used regular hexagons of a certain side length to cover the entire region and accordingly, place the sensors inside. In this context, we propose an intelligent graph-based sensor deployment strategy. Specifically, Section II introduces the essential terminology required throughout the work and also the respective system model. Section III discusses the proposed strategy by modeling the sensor deployment problem as a k-coverage problem, where it is assumed that the sensors are placed in the environment as a regular polygon tiling. To do so, we assess and examine various planar convex regular polygons, which possess the ability to tile the entire Euclidean plane. As hexagons prove to be the most efficient of the regular polygons, we assume hexagonal tiling. Based on hexagonal tiling of the region of interest, we strategically place the sensors inside a regular hexagon, and by employing this method for every hexagon, we cover the entire Euclidean plane. Moreover, we characterize the performance of the proposed strategy in terms of sensor density and number of sensors required to cover a certain region of interest. We show that our proposed strategy outperforms the state of the art deployment strategy [7] in terms of both sensor density and total number of sensors required. Finally, Section IV presents the numerical results and Section V concludes our work."
https://arxiv.org/html/2411.09783v1,Exploring the Use of Autonomous Unmanned Vehicles for Supporting Power Grid Operations,"This paper explores the use of autonomous unmanned vehicles for supporting power grid operations. With built-in batteries and the capability to carry additional battery energy storage, the rising number of autonomous vehicles can represent a substantial amount of capacity that is currently underutilized in the power grid. Unlike traditional electric vehicles which require drivers, the operations of autonomous vehicles can be performed without human intervention. To guide idle vehicles to support power grids autonomously, we propose a tractable optimization-based method for effectively integrating these “mobile batteries” into grid operations. During real-time operations, the vehicles are strategically routed to target locations to help maintain system power balance and reduce operating costs. Numerical studies have confirmed both the validity and scalability of the proposed algorithm for efficiently integrating autonomous vehicles into routine power system operations.","The rapid advancements in robotics, control theory, and artificial intelligence in recent years have positioned us closer than ever to the imminent era of autonomous unmanned vehicles (also known as self-driving or autonomous vehicles). Autonomous vehicles are uncrewed vehicles that are capable of independent operations without human intervention. These vehicles utilize cutting-edge sensors and complex algorithms to navigate, make real-time decisions and even interact with surroundings. The gradual expansion of autonomous vehicles is underway in many places across the United States and around the world, with expectations for further growth in the coming years. For example, Waymo has recently launched its self-driving robotaxi service for the general public in Los Angeles [1]. In addition, there are also plenty of autonomous vehicles under development, such as Tesla Cybercab and self-driving trucks (from Kodiak, Aurora, etc). One significant yet unexplored use of autonomous vehicles is utilizing their mobility and flexibility to provide essential support to power system operations. During idle time, these vehicles can be dispatched to target locations to help maintain power balance, provide frequency response, and support emergency operations. Given the increasing strain on the US power grid in recent years, these autonomous vehicles can serve as flexible resources to effectively alleviate the grid stress. Addressing this research problem could save billions of dollars by reducing power outages and enhancing grid resilience, without the necessity of major changes to the power grid infrastructure. While extensive research has examined the integration of electric vehicles (EVs) into power systems (e.g., [2, 3]), research on using autonomous vehicles to support grid operations has been very limited. Compared with traditional EVs, autonomous vehicles bring key advantages to power grid support, including enhanced operational autonomy, higher flexibility, and improved safety under challenging weather and road conditions. Some studies [4, 5] have explored the role of mobile energy storage units in grid operations, often in the form of trailer-mounted batteries that require separate transport. However, the specifications such as storage capacity, quantity, modeling, and availability, are generally different from those of autonomous vehicles. In this work, we aim to develop a framework for integrating autonomous vehicles in grid operations. The task involves comprehensive decision-making that balances the needs of power systems and the constraints of vehicles and transportation systems. To that end, we first present an optimal routing formulation, which enables each autonomous vehicle to quickly determine the best routes and costs to candidate locations. After that, we introduce a mixed-integer optimization model that integrates autonomous vehicles to assist power systems in maintaining power balance while minimizing the total operating cost. To tackle the nonlinearity in the optimization problem, we leverage McCormick relaxation to develop an exact and tractable reformulation, which enables real-time routing and dispatch of autonomous vehicles to support routine power grid operations. This paper is organized as follows. Section II introduces the optimal routing formulation for autonomous vehicles in a transportation network. Section III presents a tractable optimization formulation for integrating autonomous vehicles for grid operations. Section IV uses the IEEE 14-bus system for numerical simulation, with additional systems tested to evaluate the scalability of the algorithm. Section V concludes the paper and outlines directions for future research."
https://arxiv.org/html/2411.09717v1,Integrating Fuzzy Set Theory with Pandora Temporal Fault Trees for Dynamic Failure Analysis of Complex Systems,"Pandora temporal fault tree, as one notable extension of the fault tree, introduces temporal gates and temporal laws. Pandora Temporal Fault Tree(TFT) enhances the capability of fault trees and enables the modeling of system failure behavior that depends on sequences. The calculation of system failure probability in Pandora TFT relies on precise probabilistic information on component failures. However, obtaining such precise failure data can often be challenging. The data may be uncertain as historical records are used to derive failure data for system components. To mitigate this uncertainty, in this study, we proposed a method that integrates fuzzy set theory with Pandora TFT. This integration aims to enable dynamic analysis of complex systems, even in cases where quantitative failure data of components is unreliable or imprecise. The proposed work introduces the development of Fuzzy AND, Fuzzy OR, Fuzzy PAND, and Fuzzy POR logic gates for Pandora TFT. We also introduce a fuzzy importance measure for criticality analysis of basic events. All events in our analysis are assumed to have exponentially distributed failures, with their failure rates represented as triangular fuzzy numbers. We illustrate the proposed method through a case study of the Aircraft Fuel Distribution System (AFDS), highlighting its practical application and effectiveness in analyzing complex systems. The results are compared with existing results from Petri net and Bayesian network techniques to validate the findings.","Safety-critical systems find extensive application across various industries such as aerospace, automotive, and energy sectors. Failures within these systems possess the capacity to result in significant consequences for both human life and the environment. Thus, a rigorous analysis of system behavior is essential to ensure that these systems maintain a high level of reliability and functionality throughout their operational lifespan. Various techniques have been developed to analyze safety and evaluate system reliability, providing a comprehensive framework for identifying and addressing potential risks. Fault Tree Analysis (FTA)[1] is a widely used and well-established technique to compute the reliability of complex systems. In FTA[2], AND and OR gates are employed to graphically represent the logical relationships between system failure (top event) and its underlying root causes (basic events). This approach enables both qualitative and quantitative analyses, providing a comprehensive understanding of potential system failures. Qualitative analysis is a deductive process that commences with an identified system failure and iteratively works backward to uncover the underlying root causes. Subsequently, boolean logic is applied to minimize the fault tree and to obtain Minimal Cut Sets (MCS) [3], which represent the smallest combinations of component failures that can lead to system failure. Once MCSs are identified, quantitative analysis utilizes probabilistic data of the system’s components to compute the reliability of the system over a specified time. Quantitative analysis involves computing the probability of each MCS and summing them to obtain the TE probability[4]. While FTA is a popular system analysis method, it does have recognized limitations. A notable limitation of FTA is its restricted capability to assess the reliability of static systems only. Static systems are defined by a singular, unchanging mode of operation, whereas modern large-scale systems typically operate in multiple modes or phases, making them dynamic and adaptable. Modern complex systems possess a range of dynamic failure characteristics, including functional dependencies between events, prioritization of failure events and many more. To address this limitation, several extensions to static fault trees have been proposed, including Dynamic Fault Trees (DFTs) [5], Boolean logic Driven Markov Processes (BDMP) [6], and Pandora Temporal Fault Tree (TFT) [7]. In 1976, Fussell et al.[8] introduced the Priority-AND (PAND) Gate, igniting numerous studies and works focused on exploring its functionalities and applications. The PAND gate stands out among the standard gates due to its ability to impose an order on a set of events, enabling a fault tree to capture time-dependent structures. This feature allows analysts to consider the timing and order of events, enhancing the accuracy and completeness of FTA[9]. Martin Walker and Yiannis Papadopoulos [10] introduced the Priority-OR (POR) Gate, which has since been the subject of several developments in the field of temporal gates. Following this development, various case studies have been conducted using TFT to explore its practical applications and effectiveness [11] [12] [13]. Fuzzy set theory [14] has demonstrated effectiveness in solving problems when precise data is unavailable or inaccurate and in making decisions based on vague information [14] [15] [16]. Tanaka et al. [17] were the first to apply fuzzy set theory in FTA, modeling BE failure probabilities with trapezoidal fuzzy numbers and employing the fuzzy extension principle to estimate the TE probability. Subsequent extensive research on Fuzzy Fault Tree Analysis (FFTA) was carried out by Misra and Weber [15] & Liang and Wang [18]. In addition, fuzzy set theory combined with expert elicitation was utilized by Ching-Torng Lin and Mao-Jiun J. Wang [19] to assess the reliability of a robot drilling system. The application of FFTA has been used to assess the reliability of different systems. For example, Yuhua and Datao [20] employed FFTA to assess the likelihood of failure in an oil and gas transmission system. Deng-Feng Li [21] utilized FFTA based on intuitionistic fuzzy sets to analyze failures in printed circuit board assembly. Ferdous et al. [22] proposed a computer-aided fuzzy fault tree analysis method. S. Rajakarunakaran et al.[23] apply FFTA and expert elicitation method to evaluate risk in LPG refueling station. Hailong Yin et al.[24] evaluated the safety of natural gas storage tanks using a method called Similarity Aggregation Method based Fuzzy Fault Tree Analysis (SAM-FFTA). Singh et al.[25] developed \alpha-cut interval-based FFTA with Bayesian network for criticality analysis of submarine pipeline leakage. K. Singh et al.[25] developed \alpha-cut interval-based FFTA with Bayesian network for criticality analysis of submarine pipeline leakage. Kaushik M. and Kumar M. [26] proposed an integrated approach of intuitionistic fuzzy fault tree and Bayesian network analysis applicable to risk analysis of ship mooring operations. While a significant amount of research has focused on using fuzzy set theory in classical FTA to enable quantitative analysis, there has been limited exploration in extending this capability to TFTA. Recently, initial concepts regarding fuzzy set theory based Pandora TFTA were introduced by Kabir et. al.[27]. The existing article revealed gaps in the application of conventional formulas to temporal gates like PAND and POR. To overcome these challenges, we derived new formulas for PAND and POR gates and employed them to compute the TE probability. In this paper, we have developed an algorithm to compute the TE probability of TFTA using fuzzy failure rates. Here, we have taken failure rates as triangular fuzzy numbers. we have compared our results with other existing methods such as Petri net [28] and Bayesian network[29] [30]. We applied our algorithm to an AFDS and ranked each BE according to its impact on the probability of TE using fuzzy importance measures. The paper is structured as follows: Section 2 covers the basic definitions of fuzzy set theory and formulas for pandora Gates in classical set theory, and Section 3 extends those formulas in a fuzzy environment and ranking has been conducted using a Euclidean Distance-based Fuzzy Importance Measure (FIM). In Section 4, we present a case study of an AFDS where we compute the TE probability using the proposed algorithm and then we conclude the paper by comparing our results with existing methods such as Petri net-based solutions and Bayesian network-based solutions."
https://arxiv.org/html/2411.09712v1,Space-Air-Ground Integrated MEC-Assisted Industrial Cyber-Physical Systems: An Online Decentralized Optimization Approach,"Cloud computing and edge/fog computing are playing a pivotal role in driving the transformation of industrial cyber-physical systems (ICPS) towards greater intelligence and automation by providing high-quality computation offloading services to Internet of Things devices (IoTDs). Recently, space-air-ground integrated multi-access edge computing (SAGIMEC) is emerging as a promising architecture combining edge computing and cloud computing, which has the potential to be integrated with ICPS to accelerate the realization of the above vision. In this work, we first present an SAGIMEC-assisted ICPS architecture that incorporates edge computing and cloud computing through seamless connectivity supported by satellite networks to achieve determinism in connectivity, networked computing, and intelligent networked control. Then, we formulate a joint satellite selection, computation offloading, communication resource allocation, computation resource allocation, and UAV trajectory control optimization problem (\text{JSC}^{4}\text{OP}) to maximize the quality of service (QoS) of IoTDs. This problem considers both the dynamics and uncertainties of the system environment, as well as the limited resources and energy of UAVs. Given the complexity of \text{JSC}^{4}\text{OP}, we propose an online decentralized optimization approach (ODOA) to solve the problem. Specifically, \text{JSC}^{4}\text{OP} is first transformed into a real-time decision-making optimization problem (RDOP) by leveraging Lyapunov optimization. Then, to solve the RDOP, we introduce an online learning-based latency prediction method to predict the uncertain system environment and a game theoretic decision-making method to make real-time decisions. Finally, theoretical analysis confirms the effectiveness of the ODOA, while the simulation results demonstrate that the proposed ODOA outperforms other alternative approaches in terms of overall system performance.","With the rapid advancement of communication technologies and Internet of Things (IoT), industrial cyber-physical systems (ICPS) are increasingly becoming a crucial pillar in driving the transition of industry 4.0 towards intelligence and automation [1]. Specifically, the vision of ICPS is to integrate the physical processes of IoT devices (IoTDs) with the computational and communication capabilities of networks, thereby stimulating a wide range of intelligent applications that significantly enhance industrial production efficiency. However, a key challenge lies in the fact that enabling these intelligent applications typically requires handling a large volume of latency-sensitive and computation-heavy tasks, which conflicts with the limited computational resources and energy consumption of IoTDs. The integration of cloud computing and edge computing into ICPS has garnered significant attention as a viable solution, providing computing offloading services with high quality of service (QoS) for IoTDs. For example, Cao et al. [2] provided a comprehensive review of edge and edge-cloud computing-assisted ICPS architectures, in which the cloud computing and edge computing are combined into ICPS by deploying ground infrastructure. Li et al. [3] considered an industrial cyber-physical IoT system that utilizes the cloud data centers to achieve centralized control and processing. Hao et al. [4] proposed a softwarized-based ICPS architecture, where multiple terrestrial edge clouds are deployed to provide data analysis and processing for the ICPS. However, the aforementioned research on cloud or edge computing-assisted ICPS architectures heavily relies on the deployment of ground infrastructure, which may lead to the high deployment costs, limited coverage, and constrained application scenarios. For instance, in remote or disaster-stricken areas where the ground infrastructure is either nonexistent or unavailable, these architectures may struggle to operate effectively. To overcome the abovementioned challenges, space-air-ground integrated multi-access edge computing (SAGIMEC) is emerging as a promising architecture to provide edge and cloud computing services. Specifically, SAGIMEC is usually a three-tier computing architecture that integrates heterogeneous network components, including a terrestrial base station network, an aerial UAV network, and a space low earth orbit (LEO) satellite network [5]. First, thanks to the wide coverage of LEO satellites and flexible mobility of UAVs, SAGIMEC greatly expands the application scenarios and coverage of edge computing. Furthermore, the low transmission latency and seamless connectivity of LEO satellites enable SAGIMEC to effectively combine cloud-edge computing resources to improve the resource utilization. Therefore, SAGIMEC showcases substantial promise in propelling the evolution of ICPS. Several studies have investigated the SAGIMEC network. For example, to minimize the total system latency of the SAGIMEC network, Cheng et al. [6] jointly optimized the resource allocation and computation offloading. Fan et al. [7] formulated an optimization problem of joint resource allocation and computation offloading to reduce the system cost, which consists of the system energy consumption and system latency. To achieve higher system energy efficiency, Hu et al. [8] formulated the optimization problem of UAV trajectory control and resource allocation. However, the aforementioned studies above usually assume that the satellite link status can be accurately measured, which may be impractical because of the high-speed mobility, long-distance transmission, and time-varying network topology of satellite networks. Moreover, these studies usually formulate the optimization problems from the system perspective to consider indicators such as energy consumption and latency, which may neglect the QoS for IoTDs. Fully exploring the benefits of combining SAGIMEC with ICPS faces several fundamental challenges. i) Computation Offloading. The heterogeneity of the networks in SAGIMEC leads to an uneven distribution of resources. Moreover, different IoTDs usually have diverse computing requirements for resources. As a result, the heterogeneous resource distribution and IoTD requirements lead to the complexity of computation offloading decisions. ii) Satellite Selection. The dynamic topology of satellite networks leads to time-varying and uncertain satellite link conditions. Therefore, when multiple satellites are accessible, it is challenging to select appropriate satellites as relay nodes for the efficient use of cloud computing services based on satellite networks. iii) Resource Management. The tasks of IoTDs are often computation-heavy and latency-sensitive, imposing strict requirements on computing and communication resources. However, UAV networks usually have limited computing and spectrum resources. Therefore, the strict computing requirements make resource allocation difficult in the resource-constrained UAV network. iv) Trajectory Control. While the mobility of UAVs enhances the elasticity and flexibility of MEC, it also introduces important challenges related to UAV trajectory control. Furthermore, the limited battery capacity of UAVs leads to finite service time, which requires balancing both the service time of UAVs and the QoS of IoTDs. The abovementioned challenges necessitate efficient optimization of computation offloading, satellite selection, resource allocation, and UAV trajectory control. However, focusing on just one aspect of these components is insufficient to fully explore the advantages of SAGIMEC-assisted ICPS. First, these optimization variables are mutually coupled. For example, optimizing computation offloading requires the simultaneous consideration of satellite selection, resource allocation, and UAV location. Second, these optimization variables collectively determine the QoS of IoTDs. Therefore, these coupled optimization variables should be jointly optimized to fully exploit the performance of SAGIMEC-assisted ICPS, as it can effectively capture the intricate and coupling interactions and trade-offs among various optimization components. Consequently, we propose a novel online decentralized optimization approach (ODOA) that enables the joint optimization of computation offloading, satellite selection, communication resource allocation, computation resource allocation, and UAV trajectory control, to effectively improve the performance of SAGIMEC-assisted ICPS. Furthermore, compared to other approaches such as deep reinforcement learning (DRL), the proposed ODOA is more suitable for the considered system by leveraging the strengths of Lyapunov optimization, online learning, and game theory. Specifically, Lyapunov optimization is good for real-time decision-making without requiring direct knowledge of system dynamics and provides interpretability. Moreover, game theory can decentralized decision-making and guarantee existence of a solution, making the ODOA more scalable. Our main contributions are outlined as follows: • System Architecture. We propose an SAGIMEC-assisted ICPS architecture, where a UAV and a cloud computing center are seamlessly connected via a satellite network to facilitate high-quality computing offload services. Moreover, within this architecture, we consider the time-varying computing requirements of IoTDs, the energy and resource constraints of the UAV, as well as the dynamics and uncertainties of the satellite links to more accurately capture the real-world physical characteristics of SAGIMEC-assisted ICPS. • Problem Formulation. We formulate a joint satellite selection, computation offloading, communication and computation resource allocation, and UAV trajectory control optimization problem (\text{JSC}^{4}\text{OP}) to maximize the QoS of IoTDs. Moreover, we show that the formulated \text{JSC}^{4}\text{OP} is difficult to solve directly because it depends on future information and contains uncertain network parameters. In addition, we demonstrate that the \text{JSC}^{4}\text{OP} is non-convex and NP-hard. • Approach Design. Since the \text{JSC}^{4}\text{OP} is difficult to be directly solved, we propose the ODOA. Specifically, we first transform the \text{JSC}^{4}\text{OP} into a real-time decision-making optimization problem (RDOP) that only depends on current information by using the Lyapunov optimization. Then, for the RDOP, we propose an online learning-based latency prediction method to predict uncertain network parameters and a game theoretic decision-making method to make real-time decisions. • Performance Evaluation. The effectiveness and performance of the designed ODOA are confirmed through theoretical analysis and simulation experiments. In particular, the theoretical analysis proves that the ODOA not only satisfies the UAV energy consumption constraint, but also exhibits polynomial complexity. Additionally, the simulation results demonstrate that the ODOA outperforms other alternative approaches in terms of the overall system performance. The subsequent sections of this work are structured as follows. We introduce the relevant models and problem formulation in Section II. We detail the proposed ODOA and theoretical analysis in Section III. In Section IV, we demonstrate and discuss simulation results. Lastly, we present the conclusions in Section V. Figure 1: The proposed SAGIMEC-assisted ICPS architecture."
https://arxiv.org/html/2411.10431v1,Mitigating Parameter Degeneracy using Joint Conditional Diffusion Model for WECC Composite Load Model in Power Systems,"Data-driven modeling for dynamic systems has gained widespread attention in recent years. Its inverse formulation, parameter estimation, aims to infer the inherent model parameters from observations. However, parameter degeneracy, where different combinations of parameters yield the same observable output, poses a critical barrier to accurately and uniquely identifying model parameters. In the context of WECC composite load model (CLM) in power systems, utility practitioners have observed that CLM parameters carefully selected for one fault event may not perform satisfactorily in another fault. Here, we innovate a joint conditional diffusion model-based inverse problem solver (JCDI), that incorporates a joint conditioning architecture with simultaneous inputs of multi-event observations to improve parameter generalizability. Simulation studies on the WECC CLM show that the proposed JCDI effectively reduces uncertainties of degenerate parameters, thus the parameter estimation error is decreased by 42.1% compared to a single-event learning scheme. This enables the model to achieve high accuracy in predicting power trajectories under different fault events, including electronic load tripping and motor stalling, outperforming standard deep reinforcement learning and supervised learning approaches. We anticipate this work will contribute to mitigating parameter degeneracy in system dynamics, providing a general parameter estimation framework across various scientific domains.","Dynamic system modeling is a fundamental study across different scientific fields. Data-driven machine learning provides a new paradigm to model the system dynamics due to its potential of implementing more accurate and efficient simulations (Wang et al., 2023). This encompasses two fundamental forms: forward surrogation and inverse modeling. Forward surrogation predicts the system’s evolution from initial states, while inverse modeling deduces the model’s inherent properties from observation data (Kadeethum et al., 2021). Inverse modeling, which includes techniques such as parameter identification, plays a crucial role in understanding and emulating system dynamics. However, the inverse problem of system dynamics is complex and challenging due to parameter degeneracy and unidentifiability, where non-unique solutions exist that produce identical observation outputs (Lederman et al., 2021). In power systems, load modeling uses several types of models to represent the aggregation behavior of various end-user load devices in the distribution system (Kim et al., 2023). As the dynamic performance of end-user loads becomes increasingly complex with technological advances (NER, December 2016), the Western Electricity Coordinating Council (WECC) has developed the state-of-the-art composite load model (WECC CLM) (NER, December 2016; WEC, April 2021), which is capable of emulating more categories of load devices such as single-phase induction machines, power electronic-interfaced loads, as well as the distributed energy resources (DERs), which are being increasingly integrated into the power system. Though the structure of WECC CLM has been specified, this aggregated model is still like a ""grey box"", as the true values of the model parameters are not fully known. As a mathematical approximation, the parameters of WECC CLM cannot be tested on-site unlike the physical entity. The load survey is a direct approach to estimate the parameters. However, it is time-consuming and require high granularity for satisfactory accuracy. An alternative is to infer from measurement data under disturbances. The measurement-based approaches have been widely investigated by researchers. In (Wang and Wang, 2014), it’s formulated as a nonlinear optimization problem, which aims to find the optimum parameters that minimize the bias between the transient trajectories with estimated parameters and real measurements. The recursive least square (RLS) method is utilized to linearize the system model and identifies model parameters by minimizing the sum of the squares of the residuals in a recursive way. However, there are more than one hundred parameters and dozens of differential equations in WECC CLM, yielding a highly nonlinear and high-dimentional optimization problem with complex interaction among parameters. It is difficult to be competent in effectively solving this problem. Encouraged by the extraordinary capability of machine learning (ML) technology in solving complex tasks, researchers also investigate different kinds of ML-based methods for WECC CLM parameterization. In (Wang et al., 2020; Bu et al., 2020; Xie et al., 2021a), the parameter calibration problem is transformed to a markov decision process, and reinforcement learning (RL) is introduced to search for the best parameters. Some techniques, such as two-stage hierarchical framework, and evolutionary learning with sensitivity weight incorperation, are introduced to improve the accuracy. However, its optimality performance degenerates with the increase of action space and model complexity. In (Afrasiabi et al., 2023), a multi-residual deep learning structure is established to capture the spatial-temporal features and estimate the wide-area CLM parameters by learning the mapping between observations and model parameters. However, the supervised learning method fails to represent the one-to-many mapping between observations and parameters (Hu et al., 2023a). Different from the deterministic methods, generative models learn the underlying distributions of data and deduce probabilistic solutions. Based on Bayes’ theorem, several generative models, including generative adversarial network (GAN), conditional variational autoencoder (CVAE) and conditional masked autoregressive flow (CMAF) are also exploited to learn the posterior distribution of the parameters for WECC CLM (Khodayar and Wang, 2021; Khazeiynasab et al., 2022; Tan et al., 2024). However, one difficulty in practical application is ensuring the generalizability of parameters across different fault events. The CLM parameters carefully selected during one fault event may not achieve satisfactory performance in another fault. This is primarily due to the issue of parameter degeneracy, as previously discussed. Recent works with RL have explored training parameter identification agents using multiple events directly or adopting multi-task learning approaches (Hu et al., 2023b; Xie et al., 2021b). However, multi-event environments are inherently non-stationary, which can degrade the performance of the learning agents. Additionally, multi-task learning approaches risk negative transfer, where knowledge learned from one task hinders the learning of another. In recent years, generative artificial intelligence (AI) is taking center stage in the AI domain, with the emergence of a number of advanced generative models (Cao et al., 2023; Zhang et al., 2023). Diffusion probabilistic models employ a forward and reverse diffusion process, enabling them to accurately capture complex data distributions and embrace high-quality data generation (Ho et al., 2020; Du et al., 2023; Yang et al., 2023). The conditional structure enables diffusion models to flexibly control its generation process towards the expected style. In addition to data generation, diffusion models also have an outstanding performance in solving inverse problems such as image restoration with the ability to learn the intricate patterns and dependencies among data (Kawar et al., 2022; Daras et al., 2024). Motivated by the advancement of diffusion model and the need to address the multi-event challenge, we propose a novel parameter estimation framework named Joint Conditional Diffusion Model-based Inverse Problem Solver (JCDI). This framework learns the parameter posterior distributions through the forward and reverse diffusion process of diffusion models, simultaneoulsy considers the observations under different fault events. The main contributions of this work include: • By conducting global sensitivity analysis for WECC CLM, we reveal the sensitivity discrepancies under different fault events, especially when there are power electronic load tripping and motor stalling. • We develop a diffusion-based parameter estimation framework JCDI, with a transformer-based denoising network architecture, leveraging the diffusion model to capture complex distributions among parameter space, and produce a probabilistic solution considering the parameter degeneracy. • We propose a joint conditioning structure, which enables JCDI to infer parameters conditioned on transient trajectories under multiple fault events simultaneously. Therefore, the uncertainties of parameter estimation will be reduced with the increase of conditioned fault events. • We validate the effectiveness of JCDI in reducing estimation uncertainties of degenerated parameters and improving generalizability to different fault events. We also demonstrate the superiority of JCDI compared with existing parameter estimation approaches such as reinforcement learning and supervised learning."
https://arxiv.org/html/2411.09954v1,Reaching Resilient Leader-Follower Consensus in Time-Varying Networks via Multi-Hop Relays,"We study the problem of resilient leader-follower consensus of multi-agent systems (MASs) in the presence of adversarial agents, where agents’ communication is modeled by time-varying topologies. The objective is to develop distributed algorithms for the nonfaulty/normal follower agents to track an arbitrary reference value propagated by a set of leaders while they are in interaction with the unknown adversarial agents. Our approaches are based on the weighted mean subsequence reduced (W-MSR) algorithms with agents being capable to communicate with multi-hop neighbors. The proposed algorithms solve our resilient leader-follower consensus problem with agents possessing first-order and second-order dynamics. Moreover, we characterize tight necessary and sufficient graph conditions for the proposed algorithms to succeed in terms of the novel notion of jointly robust following graphs. Our graph condition is tighter than the sufficient graph conditions in the literature when agents use only one-hop communication (without relays). Using multi-hop relays, we are able to enhance robustness of leader-follower networks without increasing physical communication links and obtain further relaxed graph requirements for our algorithms to succeed. Numerical examples are given to verify the efficacy of the proposed algorithms.","\IEEEPARstart Over the past few decades, distributed consensus has emerged as a cornerstone of research in the fields of multi-agent systems (MASs) and distributed algorithms [3, 1, 2]. In such a problem, agents connected over a network try to reach consensus on a common value while interacting with only neighboring agents. Stemming from this concept, extensive applications and algorithms have been devised to overcome various industrial challenges [7, 5, 6, 4]. Concurrently, growing concerns over cyber security within MASs have amplified the significance of consensus protocols, especially in scenarios where adversarial agents induce failures or launch attacks, e.g., [9, 11, 8, 10]. Under this topic, the problems of resilient consensus have drawn much attention in areas of systems control, distributed computing, and cooperative robotics [14, 13, 12, 15, 16], where the nonfaulty, normal agents aim to reach consensus despite the possible misbehaviors of adversarial agents. A common goal in this setting is that normal agents arrive at the same value located within the convex hull of their initial states. However, for applications such as formation control and reliable broadcast, it is desirable that agents together track a specific value which is externally given and may be outside such a convex hull. Thus, it motivates us to extend resilient consensus algorithms for such objectives. A related problem in prior literature is the leader-follower consensus problem, where the goal is for normally behaving agents to come to an agreement on the reference value of a leader or a set of leaders [18, 17]. However, these works considered MASs without any adversarial agents, potentially rendering them vulnerable to random failures or deliberate attacks. Within the domain of distributed computing, considerable efforts have been dedicated to ensuring reliable broadcast [19] as well as the certified propagation algorithm (CPA) [20, 21]. In these works, the objective is for a secure leader to broadcast a reference value to all nodes in the network in the presence of adversarial agents. Additionally, there is a body of research addressing the problem known as resilient distributed estimation. For instance, the work [22] studied resilient parameter estimation, where certain reliable agents drive the errors of the remaining normal agents to the static reference value of zero. Moreover, the authors of [23] investigated a problem where the observation information of the system is resiliently transmitted from a group of source nodes to other nodes that cannot directly observe the system. In this paper, we develop distributed algorithms to tackle resilient leader-follower consensus in time-varying networks. In the literature, many efforts have been devoted to resilient consensus using the so-called mean subsequence reduced (MSR) algorithms [12, 13, 25, 24, 26]. In such algorithms, each normal agent disregards the most deviated states of neighbors to avoid being affected by possible faulty values from adversarial neighbors. Tight graph conditions on static (i.e., time-invariant) network structures guaranteeing the success of MSR algorithms have been derived for the class of malicious agents [13, 27, 14] as well as the class of Byzantine agents [12, 28]. Notably, [13] demonstrated that static networks utilizing MSR algorithms must adhere to a specific structural criterion, called graph robustness, to attain resilient consensus. However, the majority of these studies have been confined to static networks, i.e., communication topologies are fixed across iterations. However, in numerous applications of MASs with physical motions, e.g., formation control of drones and vehicle platoons, the underlying communication network may be time-varying due to limited communication ranges and temporal variations of communication channels [3, 29, 30]. Thus, there is a significant demand for investigating resilient leader-follower consensus in time-varying networks. For instance, the work [31] proved a sufficient condition for the sliding weighted-MSR (SW-MSR) algorithm from [32] to achieve resilient leader-follower consensus to arbitrary static reference values. It reduced the stringent connectivity requirements of MSR algorithms at each iteration. Later, [33] studied resilient leader-follower consensus in static networks with the leader in each network having a dynamic reference value. Meanwhile, several works relaxed the graph connectivity requirements for resilient consensus in static networks through multi-hop communication [15, 34, 28], which enables messages sent by an agent to reach beyond its direct neighbors through relays by middle agents [1, 35]. It can improve network resilience against adversaries without changing the original topology as shown in [15, 34, 36]. Motivated by these works, we are interested to investigate whether multi-hop relays could further help us to acquire a more relaxed condition for leader-follower consensus in time-varying networks. We summarize the contributions of this paper as follows. First, we characterize a necessary and sufficient graph condition for the Multi-hop Weighted-MSR (MW-MSR) algorithm to achieve resilient leader-follower consensus in time-varying networks. Consequently, the normal follower agents are able to track the reference value propagated by a set of leaders in the presence of Byzantine agents, which may also include adversarial leaders. Our graph condition is denoted by a novel notion of jointly robust following graphs with multi-hop communication. Compared to the SW-MSR algorithm [32, 31] storing neighbors’ values for the last certain time steps at each iteration, our approach utilizes neighbors’ values of only the current time step at each iteration. It is notable that even with one-hop communication, our graph condition is tighter than the ones in the resilient leader-follower consensus works with static reference values [31] as well as dynamic reference values [33]. Moreover, by increasing the number of relaying hops, our method can increase the graph robustness against adversaries without changing the network topology. Hence, our approach can tolerate more adversarial nodes compared to the one-hop MSR algorithms [13, 33, 11, 29] as well as the CPA-based methods [20, 21]. Moreover, numerical examples show that our method can achieve resilient leader-follower consensus in sparse time-varying networks where the algorithms in [31, 33] have difficulties. As a side result, we present that the tight graph condition for resilient leader-follower consensus under the malicious model is the same as the one for the Byzantine model, even though malicious agents are less adversarial. Second, we also deal with resilient leader-follower consensus in time-varying networks for agents with second-order dynamics and propose a multi-hop double-integrator position-based MSR (MDP-MSR) algorithm. This extension is vital since double-integrator dynamics are often used to characterize more accurate motions of agents in robotics; see, e.g., [37]. To the best of our knowledge, such a problem has not been investigated in the literature. Furthermore, we derive a necessary and sufficient graph condition for the MDP-MSR algorithm to handle this case. The condition is the same as the one for the MW-MSR algorithm. Moreover, we provide necessary properties for verifying whether network topologies meet our conditions or not. Both theoretical results and numerical examples verify that the proposed algorithm with multi-hop relays can improve the robustness against adversaries in static as well as time-varying networks for agents with second-order dynamics. Lastly, we apply the algorithm for achieving formation control in the leader-follower configuration in the presence of adversaries, which could serve as a basis for applications of, e.g., multi-robot manufacturing in complex industrial sectors. The rest of this paper is organized as follows. In Section II, we outline the problem settings. In Section III, we define the novel notion of joint robust following graphs with multi-hop communication. In Section IV, we derive a tight graph condition under which the MW-MSR algorithm guarantees resilient leader-follower consensus. In Section V, we introduce the MDP-MSR algorithm for MASs with second-order dynamics and provide tight graph conditions for the algorithm to achieve resilient leader-follower consensus in static and time-varying networks. In Section VI, we present numerical examples to verify the efficacy of our algorithms in sparse time-varying networks. Finally, we conclude the paper in Section VII. Compared to the preliminary version of this work [38], the current paper contains additional results for time-varying networks, the results for the secure leader, the results for second-order MASs, and extensive numerical examples."
https://arxiv.org/html/2411.09906v1,A Survey of Machine Learning-based Physical-Layer Authentication in Wireless Communications,"To ensure secure and reliable communication in wireless systems, authenticating the identities of numerous nodes is imperative. Traditional cryptography-based authentication methods suffer from issues such as low compatibility, reliability, and high complexity. Physical-Layer Authentication (PLA) is emerging as a promising complement due to its exploitation of unique properties in wireless environments. Recently, Machine Learning (ML)-based PLA has gained attention for its intelligence, adaptability, universality, and scalability compared to non-ML approaches. However, a comprehensive overview of state-of-the-art ML-based PLA and its foundational aspects is lacking. This paper presents a comprehensive survey of characteristics and technologies that can be used in the ML-based PLA. We categorize existing ML-based PLA schemes into two main types: multi-device identification and attack detection schemes. In deep learning-based multi-device identification schemes, Deep Neural Networks are employed to train models, avoiding complex processing and expert feature transformation. Deep learning-based multi-device identification schemes are further subdivided, with schemes based on Convolutional Neural Networks being extensively researched. In ML-based attack detection schemes, receivers utilize intelligent ML techniques to set detection thresholds automatically, eliminating the need for manual calculation or knowledge of channel models. ML-based attack detection schemes are categorized into three sub-types: Supervised Learning, Unsupervised Learning, and Reinforcement Learning. Additionally, we summarize open-source datasets used for PLA, encompassing Radio Frequency fingerprints and channel fingerprints. Finally, this paper outlines future research directions to guide researchers in related fields.","1.1 Background With the vigorous development of information technology promoted by academia and industry, wireless communication techniques have been widely applied in numerous fields, such as aviation navigation, radio and television, transportation, meteorology, fire prevention, flood control, as well as mobile communications [1]. According to forecasts, by the year 2025, it is estimated that there will be 7.49 billion mobile users worldwide111https://www.statista.com/statistics/218984/number-of-global-mobile-users-since-2010/. However, the misuse of wireless devices for illicit cybercriminal activities has been increasing. This can be attributed to the open and broadcast nature of wireless media, which makes it susceptible to various types of attacks [2]. For instance, malicious users exploit vulnerabilities in wireless networks to eavesdrop on transmitted data and obtain sensitive information such as personal data or trade secrets [3]. They may also deceive unsuspecting users by impersonating legitimate devices, tricking them into sharing sensitive information, or facilitating malicious operations [4]. Additionally, attackers can launch Jamming attacks that disrupt the communication between devices, leading to interruptions, data loss, or degraded communication quality [5]. Furthermore, Sybil attackers threaten the reputation and security of wireless networks or systems. These attackers create multiple false identities to manipulate network decision-making processes, monopolize resources, or interfere with the normal functioning of other users [6]. The above security threats have caused security threats to many application scenarios, and may even bring serious economic losses. For example, in vehicles ad hoc networks, the dependence on infrastructure, computing, dynamic characteristics and control technology makes its security threats increase [7, 8, 9]. For another example, the security threats of 6G come from the complexity of network architecture, the diversity of access devices, the surge of data traffic and new security threats[10]. Therefore, it is crucial for individuals and organizations to be aware of these risks and take appropriate measures to identify wireless devices and guarantee the wireless security. 1.2 Cryptography-based Upper-Layers Authentication Mechanisms Currently, authentication mechanisms in wireless communications are achieved through traditional cryptography-based algorithms at the upper-layers [11]. However, these methods are not applicable for emerging application scenarios, such as the Internet of Things (IoT), the sixth-generation (6G) wireless networks, Industrial Internet of Things (IIoT), and smart cities for the following limitations. 1. The cryptography-based authentication techniques are based on computational theories (such as algebraic geometry and discrete mathematics) and are realized with one basic assumption that attackers have limited computational capability [12]. However, this assumption has been gradually broken due to the rapid advancements in cryptanalysis algorithms and computational power [13]. If the root key is leaked, various attacks can compromise the identification system. For example, in the Internet of Vehicles (IoV), malicious nodes can employ Sybil attacks to transmit fake messages, such as incorrect route directions, disturbing networks and posing potential risks to passengers’ lives [14]. 2. Most cryptography-based approaches are vulnerable to replay attacks, where adversaries can recover the physical-layer bit stream and directly deliver the recovered signal to the legal receiver without modifying the upper-layers signaling or cracking the cryptographic algorithms [13]. For instance, an attacker may attempt to record transmitted signals from a legitimate transmitter earlier and subsequently replay the recorded signals to pass authentication. This can lead to the legitimate receiver failing to authenticate and disrupt normal communication [15]. 3. The cryptography-based algorithms necessitate the generation, distribution, and updating of keys, thereby increasing transmission latency [16]. Hence, they are not suitable for numerous latency-sensitive scenarios [17]. For example, health management within intelligent medicine requires patients’ self-management, emphasizing real-time self-monitoring, prompt feedback of health data, and timely medical intervention [18]. Additionally, real-time multivariable statistical system monitoring methods are extensively employed in chemical engineering, automobile production, agricultural monitoring, and other industrial sectors [19]. Failure to guarantee real-time performance may result in significant economic losses and security threats. 4. The cryptography-based identification methods introduce high communication overhead and complexity, particularly undesired for devices with limited computational and store resources, such as massive machine-type communications and Unmanned Aerial Vehicles (UAV) that are inherently power-limited and processing-restricted [20]. Moreover, due to diminishing compatibility as nodes increase, these approaches struggle to support the ultimate goal of IoT, real-time interaction between things, machines, and people [17]. Additionally, with 6G anticipated to support space-air-ground-sea integrated networks encompassing various terminals, divergent encryption and decryption methods between different network protocols pose challenges for devices in achieving swift handovers without service interruption [13]. Consequently, more robust and secure identity authentication approaches are required to effectively address the aforementioned limitations of the upper-layers security mechanisms, thus ensuring the wireless security. 1.3 Physical-Layer Authentication (PLA) As a complement of traditional security mechanisms, Physical-Layer Authentication (PLA) has recently been considered a powerful approach for verifying the identity of radio devices due to the below superiorities. 1. PLA is achieved based on physical-layer features, mainly including radio frequency (RF) fingerprints and channel fingerprints. Such physical-layer attributes are exploited from the communication links, devices, and location-related attributes, and it is challenging for adversaries to extract, imitate, and forge them [12]. In other words, they can provide unique identification signatures and endogenous security for legal devices [21]. 2. PLA is a lightweight approach that circumvents many upper-layer signaling processes [22]. In addition, since the access point has acquired the Channel State Information (CSI) of all legitimated users during the channel estimation phase, computational overhead is further reduced [23]. As a result, radio terminals with finite computing resources can perform effectively [24]. 3. PLA is highly compatible in heterogeneous coexistence environments [25]. Incompatible devices may not be able to decode each other’s upper-layer signaling, but they should be able to decode physical-layer bit-streams [12]. In earlier literature, PLA-based attack detection is accomplished by formulating a statistical hypothesis test, where the received signal is deemed illegitimate if the difference between the corresponding fingerprint and the reference fingerprint exceeds the threshold; otherwise, it is considered legitimate [26]. However, owing to the dynamic and random fluctuations of electromagnetic environments, the impact of noise, and the attackers’ concealment, it is becoming increasingly challenging for non-ML-based PLA methods to determine the theoretical optimal threshold [27]. More recently, Machine Learning (ML)-based PLA methods have attracted increased interest. Compared to non-ML-based PLA, ML-based PLA has the following advantages. 1. ML-based PLA is a data-driven method overcoming the challenges in modeling the uncertainty and unknown dynamics of wireless links. For example, for the industrial environments containing machine areas, mobile robot, inspection machine, assembly work cells, and stacking storage area, describing the mathematical expression of the estimated fingerprints of industrial terminals and determining the optimal threshold is not feasible. In this case, we can resort to ML to learn the distribution characteristics and design appropriate algorithms to realize authentication [28]. 2. ML-based PLA can realize adaptive threshold authentication. For example, for IoV or UAV scenarios where the channel environments are constantly varying dynamically, the threshold is not always optimal. To address this issue, the receiver can utilize ML algorithms to learn the time-varying physical-layer attributes and realize adaptive online authentication [17]. 3. ML-based PLA is a highly-universal approach without requiring much prior information. For example, with the help of ML techniques, RF fingerprints can be extracted for multi-device identification without the prior-information-dependent expert feature transformation, such as Short Time Fourier Transform (STFT), wavelet transform, and constellation diagram [29]. For another example, ML-based attack detection can be realized without knowing the prior information of attackers, such as the position and attack frequency [20, 30]. 4. ML-based PLA has higher scalability. Through Transfer Learning (TL) methods, the receiver can quickly identify the test signals of different equipment types in unknown radio environments with only a few training samples on the basis of a pre-training authentication model. In addition, ML-based PLA is an end-to-end authentication process with higher flexibility [31, 32]. 5. ML-based PLA has the potential to identify large-scale and even ultra-large-scale equipment. ML techniques, especially Deep Learning (DL) methods, are expert in learning high-dimensional features and classifying a large number of samples [33, 34]. In contrast, traditional non-DL approaches, such as feature engineering, can only identify about 100 devices, restricting the development of the Internet of Everything (IoE) [35]. According to the different types of authentication tasks, we categorize the existing ML-based PLA schemes into two categories: multi-device identification and attack detection. 1. Multi-Device Identification: Most of the state-of-the-art ML-based multi-device identification methods exploit DL techniques to extract the inherent and distinguishable characteristics of RF fingerprints. RF Fingerprint refers to the differences in signal characteristics caused by factors such as device hardware, antennas, and manufacturing processes in wireless communications. These characteristics are unique among devices, analogous to fingerprints in biometrics. Such dissimilarities make the radiation sources of the same model and batch have an inherent property that is different from other individuals [33]. Compared the traditional approaches, RF Fingerprint-based methods have the following advantages: no additional hardware required, high uniqueness, good real-time performance, and location tracking. DL-based methods can realize intelligent end-to-end identification, while the non-DL-based multi-device identification usually requires much prior information and expert feature transformation to manually set parameters. 2. Attack Detection: The ML-based attack detection usually considers the conventional “Alice-Bob-Eve” adversarial model and designs how to defend against spoofing attacks or replay attacks. With the help of ML techniques, the detection threshold can be determined automatically without knowing the channel parameters [36] or attackers’ information [20]. In contrast, the non-ML-based methods require setting the threshold manually, and it is challenging for the threshold to be adapted to dynamic channel environments. Mukherjee et al. [37] provide a survey of Physical Layer Security (PLS) in multiuser wireless networks, and the associated problem of PLA is also briefly discussed. To address the challenges in low reliability of authentication, Wang et al. [22] present several promising research areas and provide possible approaches of invoking PLA to reduce the latency. Liu et al. [38] summarize the fundamental theories of PLA, including confidentiality and authentication. Bai et al. [39] review the concepts, key techniques as well as future research trends of PLA. Xie et al. [40] give a literature survey on passive PLA and active PLA. The active PLA schemes modify the source message on purpose to provide additional identification characteristics, while the passive PLA schemes do not. Angueira et al. [41] present a survey on PLA techniques for ensuring the security in industry, including vulnerabilities, possible attacks, and PLA for factory automation. Xu et al. [42] provide a tutorial overview of RF fingerprints, including the taxonomy of RF fingerprints, authentication algorithms, and open research problems of fingerprint extraction. Fang et al. [28] envision ML-based PLA methods and provide intelligent authentication with a higher security level. The authors of [43, 44, 45] develop DL-based PLA schemes for indoor environments with multipath effects, WiFi scenarios, and near field communication (NFC). Jagannath et al. [46] present a tutorial of DL-based RFF techniques and provide a roadmap of potential research approaches in an illustrative way. Liu et al. [47] summarize ML-based identity authentication technologies for IoT devices from the viewpoint of passive surveillance agents and discuss various enabling techniques to secure the IoT. We provide a list of representative overview/survey/tutorial papers on PLA in Tab. LABEL:tab1. Table 1: List of Representative Overview/Survey/Tutorial Papers on PLA Ref. Publication Year/Type Major Contributions [13] 2022/Overview Overview different PLS mechanisms, explain the relationship among them and their characteristics, and further introduce several promising approaches to ensure the security. [22] 2016/Overview Review PLA techniques, analyze their limitations, provide three promising research areas in dealing with these issues, and further discuss feasible approaches of invoking PLA to reduce the latency. [28] 2019/Overview Envision novel PLA approaches based on ML and further introduce different ML paradigms for intelligent and continuous attack detection. [37] 2014/Survey Provide a comprehensive survey on PLS based on information-theoretic principles and briefly discuss PLA approaches based on hypothesis testing. [38] 2017/Survey Investigate the fundamental theories of PLS technologies, discuss various PLS techniques and corresponding challenges, and further suggest numerous solutions. [39] 2020/Survey Introduce the background, fundamentals, and attack models of PLA, and classify PLA methods into three typical architectures: channel information-based, RF feature-based, and identity watermarks-based. Potential research trends of PLA in multiuser communications are also discussed. [40] 2021/Survey Present a comprehensive survey on existing PLA schemes and categorize them into two categories: passive and active schemes. The related works are reviewed in detail. [41] 2022/Survey Give a literature survey on security aspects of industrial wireless communications from industry, academia, and standardization bodies. PLA techniques to defend against spoofing attacks are also reviewed. [42] 2016/Tutorial Provide a tutorial overview of RFF for enhancing the security of radio networks, including the taxonomy of RF fingerprints and several RFF algorithms. [43] 2019/Overview Review representative literature related to RF fingerprints and research difficulties of multipath effects in indoor radio environments, and further introduce an advanced identification framework based on DL. [44] 2020/Overview Review data augmentation approaches that attempt to overcome the drop in RFF accuracy when the channel is dynamically varying between training and testing sets, and further provide two data augmentation methods for enhancing the recognition accuracy. [45] 2021/Overview Discuss the feasibility of RF fingerprints used for recognizing NFC tags, implement a hardware testbed for extracting RF features, utilize DL algorithms for experiment, and give key technical challenges. [46] 2022/Tutorial Provide an elaborated tutorial of traditional and DL-based RFF approaches over the past two decades, including modulation recognition, protocol classification, and emitter identification. [47] 2022/Survey Give a survey on the existing techniques on the detection and identification of IoT devices from the perspective of ML, and provide challenges and future research directions for rogue device detection. 1.4 Contributions Although numerous researchers focus on ML-based PLA and harness its potential to bolster the identity security of wireless devices, it is astonishing to discover that a comprehensive overview of the state-of-the-art ML-based PLA and its core foundations remains elusive. Consequently, the primary impetus behind this paper is to offer a detailed survey of the characteristics and technologies that can be leveraged within the realm of ML-based PLA. Additionally, the applications of ML-based PLA approaches to various emerging radio communications have recently been proved. Therefore, it is prudent to review the latest cutting-edge ML-based PLA methodologies, which can unveil novel research avenues and directions for researchers in affiliated domains. In this paper, we propose a comprehensive taxonomy for ML-based PLA schemes. The contributions are summarized as follows. 1. Initially, we categorize the fingerprints utilized for PLA into two distinct groups: RF fingerprints and channel fingerprints, described as follows. (a) RF Fingerprints: These include phenomena such as Carrier Frequency Offset (CFO), In-phase/Quadrature (I/Q) imbalance, and phase noise, which mirror the hardware discrepancies among different devices. Even devices of the same model and batch exhibit unique RF fingerprints. (b) Channel Fingerprints: Encompassing parameters like Received Signal Strength (RSS) and Channel State Information (CSI), these indicators reflect the channel characteristics between transmitters and receivers. The dynamic, time-varying, and richly scattering channel environments furnish distinctive identifying traits for transmitters, known as channel fingerprints. 2. Subsequently, we classify ML-based PLA schemes into two primary categories: multi-device identification and attack detection. (a) PLA for multi-device identification: We compare the non-DL-based and DL-based multi-device identification methods to present the potential and superiority of DL techniques in identification, including not relying on expert feature transformation, end-to-end identification, better scalability, and identification for large-scale and ultra-large-scale devices. We divide the DL techniques for multi-device identification into the following sub-categories: Fully-Connected Neural Networks (FCNN), Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Attention mechanism, data augmentation, Complex-Valued Neural Networks (CVNN), Generative Adversarial Networks (GAN), and Autoencoders (AE). We further provide the architecture of the above-mentioned models and how to extract useful and valuable characteristics of fingerprints, especially raw I/Q fingerprints. Among the DL techniques, CNN is the most widely used model for identification, which are divided into five sub-categories: LeNet-like, AlexNet-like, VGG-like, GoogLeNet-like, and RseNet-like models. (b) PLA for attack detection: We compare the non-ML-based and ML-based attack detection approaches to present the advantages and advancement of ML technologies in attack detection, including intelligent determination of the optimal threshold and even threshold-free, less dependence on prior information of channel conditions and transmitters, exploitation of multi-fingerprints, and continuous protection. We divide the ML algorithms for attack detection into three sub-categories: Supervised Learning (SL), Unsupervised Learning (UL), and Reinforcement Learning (RL) algorithms. The SL-based methods require the fingerprints and corresponding labels to train the detection system with low false alarm rate and miss detection rate. In contrast, the UL-based approaches require no training fingerprints of attackers, which are more practical in actual wireless communication scenarios. Compared with the SL-based and UL-based methods, the RL-based schemes require no accurate inputs or outputs as well as precise parameter updates. The RL-based detection systems are usually modeled as the game between the legitimate receiver and attackers. 3. Acknowledging the paramount importance of data in demonstrating the efficacy of ML algorithms, we also summarize open-source datasets of fingerprints to serve as a reference for researchers in related fields. (a) The RF fingerprints are outlined based on the number of transmitters, type of receiver and transmitters, waveform, and frequency. (b) Conversely, the channel fingerprints are summarized according to the provider and channel environments. 4. In addition, we summarize the challenges of existing ML-based PLA schemes and point out the future research direction, including theory, method and practical application. 1.5 Organization Figure 1: Organization of the paper. As illustrated in Fig. 1, the rest of this paper is organized as follows. In Section 2, we provide the taxonomy of fingerprints as well as the comparison of non-ML-based and ML-based PLA. The DL-based PLA for multi-device identification and ML-based PLA for attack detection are comprehensively presented in Section 3 and Section 4, respectively. In Section 5, we summarize open-source datasets of fingerprints. Section 6 and Section 7 respectively show future research directions and conclusions. The acronyms used in this paper are listed in Tab. LABEL:tab2. Table 2: List of Acronyms Used in the Paper Abbreviations Full Name Abbreviations Full Name 6G The sixth-generation IoT Internet of Things ADS-B Automatic-dependent surveillance-broadcast IoV Internet of Vehicles AE Autoencoder KNN K-Nearest Neighbor AoA Angle of arrival LDA Linear Discriminant Analysis BN Batch Normalization LFDA Linear Fisher Discriminant Analysis CAA Chaotic Antenna Array LLRT Logarithmic likelihood ratio test CFO Carrier Frequency Offset LSTM Long Short-Term Memory CFR Channel Frequency Response LTE Long-Term Evolution CIR Channel Impulse Response MIMO Multiple Input Multiple Output CLRT Classical Likelihood Ratio Test ML Machine Learning CNN Convolutional Neural Network MSCNN Multi-Scale Convolutional Neural Network CSI Channel State Information NFC Near field communication CVNN Complex-Valued Neural Network OFDM Orthogonal Frequency Division Multiplexing CWD Choi-Williams Distribution PLA Physical-Layer Authentication DAC Digital-to-Analog Converter PLS Physical-Layer Security DL Deep Learning PSD Power spectral density DNN Deep Neural Network PUWS Physically unclonable wireless system DRL Deep Reinforcement Learning ReLU Rectified Linear Unit DT Decision Tree RF Radio Frequency EI Edge Intelligence RFF Radio Frequency Fingerprinting ELM Extreme Learning Machine RL Reinforcement Learning FCNN Fully-Connected Neural Network RNN Recurrent Neural Network FFT Fast Fourier Transform RSS Received Signal Strength FHSS Frequency hopping spread spectrum RSSI Received Signal Strength Indication FL Federated Learning RVNN Real-Valued Neural Network GAN Generative Adversarial Network SEI Specific Emitter Identification GCN Graph Neural Network SL Supervised Learning GLRT Generalized likelihood ratio test SNR Signal-Noise Ratio GMM Gaussian Mixture Model STFT Short Time Fourier Transform GP Gaussian Process SVM Support Vector Machine GPC Gaussian Process Classification TL Transfer Learning GPR Gaussian Process Regression UAV Unmanned Aerial Vehicle GRU Gated Recurrent Unit UL Unsupervised Learning HHT Hilbert-Huang Transform UWSN Underwater acoustic sensor network I/Q In-phase/Quadrature VAE Variational Autoencoder IAT Inter arrival time VANET Vehicular Ad Hoc Network IIoT Industrial Internet of Things WVD Wegener-Ville Distribution"
https://arxiv.org/html/2411.09812v1,Edge Caching Optimization with PPO and Transfer Learning for Dynamic Environments,"This paper addresses the challenge of edge caching in dynamic environments, where rising traffic loads strain backhaul links and core networks. We propose a Proximal Policy Optimization (PPO)-based caching strategy that fully incorporates key file attributes such as size, lifetime, importance, and popularity, while also considering random file request arrivals, reflecting more realistic edge caching scenarios. In dynamic environments, changes such as shifts in content popularity and variations in request rates frequently occur, making previously learned policies less effective as they were optimized for earlier conditions. Without adaptation, caching efficiency and response times can degrade. While learning a new policy from scratch in a new environment is an option, it is highly inefficient and computationally expensive. Thus, adapting an existing policy to these changes is critical. To address this, we develop a mechanism that detects changes in content popularity and request rates, ensuring timely adjustments to the caching strategy. We also propose a transfer learning-based PPO algorithm that accelerates convergence in new environments by leveraging prior knowledge. Simulation results demonstrate the significant effectiveness of our approach, outperforming a recent Deep Reinforcement Learning (DRL)-based method.","With the rapid growth of mobile applications and services, the demand for data has surged, placing significant pressure on backhaul links and core networks. The exponential rise in video streaming, cloud services, and Internet of Things (IoT) devices has led to an overwhelming number of content requests, often from centralized data centers. To address this, edge caching frequently requested data closer to users, has become a key strategy for reducing network congestion and minimizing transmission delays. By caching popular content locally, edge caching not only alleviates the load on core networks but also improves the overall user experience by reducing latency. Edge caching faces significant challenges in dynamic and unpredictable environments, where content popularity is not static and fluctuates based on factors like time, location, and social trends [1, 2, 3, 4]. A viral video, for instance, may experience a sudden spike in requests, followed by a rapid decline in demand. Similarly, request rates can change unpredictably, influenced by factors such as user mobility, device connectivity, and network congestion. For instance, during peak hours in a densely populated area, a surge in user activity may dramatically increase the request rate for certain content. On the other hand, during off-peak times, the request rate may drop significantly. Events like sports games or breaking news can also cause rapid shifts in content requests. When it comes to dynamic environments, most existing research primarily focuses on changes in content popularity trends while overlooking the equally important fluctuations in request rates. In real-world scenarios, Users’ behavior and usage patterns can shift over time, which directly influences the rate at which files are requested. For instance, during peak hours, when many users are accessing the network at once, demand for content rises sharply. This increased activity results in shorter intervals between file requests, as users require quick access to different resources. In contrast, during off-peak periods, when fewer users are online, the intervals between file requests tend to lengthen. With reduced demand for immediate access, users are less likely to request content frequently, leading to extended gaps between consecutive requests. Existing caching strategies, particularly those relying on fixed policies, often fail to account for these dynamic shifts, leading to inefficient use of cache resources. Therefore, the ability to learn new policies quickly and efficiently becomes crucial for maintaining high cache performance in such environments. Most of the existing work on reinforcement learning in dynamic environments assumes that changes occur gradually, giving the agent sufficient time to adapt its policy incrementally. This assumption, however, is not always valid, especially in real-world scenarios where sudden shifts in content popularity or request rates are common. In such cases, relying on gradual adaptation may lead to poor performance. Therefore, detecting when these abrupt changes happen becomes critical to maintaining high system efficiency. By quickly identifying the moment of change, essential steps can be taken to adjust the Reinforcement Learning (RL) policy and prevent the agent from making decisions based on outdated information. Detecting these shifts promptly allows the agent to recalibrate and ensure that caching decisions remain relevant. While training caching policies from scratch after every environmental change is one option, it is not practical. Learning a new policy from the ground up each time a shift occurs would result in significant delays, during which the system operates suboptimally. This can lead to increased latency and reduced cache efficiency. For this reason, it is important to converge to the optimal policy faster in the new environment, as this ensures the system can restore its high level of performance without unnecessary delays or inefficiencies. Building on this, we introduce a caching strategy based on PPO [5], a reinforcement learning (RL) technique known for its stability and efficiency. In our approach, we incorporate not only content popularity but also key file attributes such as size, lifetime, and importance to ensure that the cache is used optimally. In this work, we propose a mechanism for detecting changes in content popularity and request rates, designed to identify shifts in the environment quickly. This mechanism has two functions. The first function focuses on popularity change detection by leveraging cosine similarity between recent and past request patterns. This method effectively captures shifts in content popularity by comparing how closely current requests align with historical trends. The second function addresses request rate changes using a simple moving average over a defined window of time. By continuously monitoring and averaging request rates, it quickly detects deviations from the norm, signaling a change in the environment. Both functions are designed to detect changes rapidly, allowing the system to adapt swiftly and maintain optimal performance in dynamic scenarios. To quickly adapt to changing environments, we integrate transfer learning into the PPO algorithm by transferring the learned knowledge in the form of transitions to the new environment. In the new environment, we assign priorities to both the transferred state transitions and the new transitions collected from the current environment. These priorities are determined using a combination of the Temporal difference (TD) error associated with each transition and the difference between the average reward and the instant reward tied to that transition. By assigning higher priority to more relevant transitions, the caching agent is able to focus on learning from the most significant experiences, thus accelerating convergence. This approach is particularly advantageous in dynamic edge caching environments, where frequent changes demand swift adaptation, and leveraging prior knowledge minimizes the time and computational resources required for the agent to adjust to new conditions. In summary, our approach tackles the limitations of traditional caching strategies by dynamic change detection, and transfer learning. This results in a caching system that is flexible, efficient, and capable of adapting quickly to changing environments, ensuring optimal performance even in highly dynamic scenarios. The contributions of this paper are summarized as follows. • We introduce a mechanism with two functions specifically designed to rapidly detect changes in the environment. These functions are engineered to enhance the responsiveness of the system to dynamic shifts in content popularity and request rate, ensuring timely and accurate identification of environmental changes. • We propose a novel algorithm that combines transfer learning with PPO. This innovative approach facilitates accelerated adaptation to new environments by effectively leveraging previously acquired knowledge, thereby enhancing the algorithm’s ability to quickly adjust and optimize caching strategies in dynamic settings. • We conducted simulations where the environment underwent sudden changes, and our change detection approaches demonstrated remarkable speed in identifying these change points. Furthermore, our proposed transfer learning algorithm outperforms several benchmark methods, including learning from scratch, learning from demonstrations [6], and another recently published work [7]. This superiority highlights the effectiveness of our approach in rapidly adapting to new conditions and achieving better performance in dynamic environments. The remainder of the paper is structured as follows: Section II provides an overview of related work. Section III introduces our system model, while Section IV outlines our proposed caching algorithm. In Section V, we discuss our experimental setup and results, and Section VI concludes the paper. Table I summarises the most important notations used in this paper. TABLE I: Table of notations Symbol Description f_{r} the requested file F The total number of file types f_{f} file type f \lambda The request rate \eta Skewness of popularity distribution N length of the sliding window to store the history of the file requests \gamma The discount factor |\Lambda| The size of the replay buffer before change in the environment |\Lambda^{\prime}| The size of the replay buffer after the new environment \beta Controls the importance sampling d_{f} The number of requests for file type f over the last N requests l_{f} Lifetime of file type f i_{f} Importance of file type f z_{f} Size of file type f h^{f}(t) Freshness of file type f y_{f}(t) Utility of file type f b The files cached in the edge router’s cache \pi Policy \theta Weights of the actor network \theta^{\prime} Weights of the critic network s Current state s^{\prime} Next state r(t) The instant reward at time t \tau State transtition time r_{i} The instant reward of transition i r_{(n)} The immediate reward at the n^{th} step p_{i} The priority of sampling transition i P(i) The probability of sampling transition i L_{P} The size of the moving average window for popularity change detection L_{R} The size of the moving average window for request rate change detection th_{R} Threshold for the acceptable variance of the request rate th_{c} Threshold for the acceptable variance of the cosine similarity"
https://arxiv.org/html/2411.09787v1,ART-Rx: A Proportional-Integral-Derivative (PID) Controlled Adaptive Real-Time Threshold Receiver for Molecular Communication,"Molecular communication (MC) in microfluidic channels faces significant challenges in signal detection due to the stochastic nature of molecule propagation and dynamic, noisy environments. Conventional detection methods often struggle under varying channel conditions, leading to high bit error rates (BER) and reduced communication efficiency. This paper introduces ART-Rx, a novel Adaptive Real-Time Threshold Receiver for MC that addresses these challenges. Implemented within a conceptual system-on-chip (SoC), ART-Rx employs a Proportional-Integral-Derivative (PID) controller to dynamically adjust the detection threshold based on observed errors in real time. Comprehensive simulations using MATLAB and Smoldyn compare ART-Rx’s performance against a statistically optimal detection threshold across various scenarios, including different levels of interference, concentration shift keying (CSK) levels, flow velocities, transmitter-receiver distances, diffusion coefficients, and binding rates. The results demonstrate that ART-Rx significantly outperforms conventional methods, maintaining consistently low BER and bit error probabilities (BEP) even in high-noise conditions and extreme channel environments. The system exhibits exceptional robustness to interference and shows the potential to enable higher data rates in CSK modulation. Furthermore, because ART-Rx is effectively adaptable to varying environmental conditions in microfluidic channels, it offers a computationally efficient and straightforward approach to enhance signal detection in nanoscale communication systems. This approach presents a promising control theory-based solution to improve the reliability of data transmission in practical MC systems, with potential applications in healthcare, brain-machine interfaces (BMI), and the Internet of Bio-Nano Things (IoBNT).","Molecular communication (MC) or particle-based communication, a paradigm inspired by nature, has emerged as a promising solution to communicating with biological organisms where traditional communication methods have been shown to be ineffective [1, 2, 3, 4]. By leveraging biochemical mechanisms for the transmission of information, MC is also believed to play a vital role in the realization of the Internet of Everything (IoE) [5], particularly the Internet of Bio-Nano Things (IoBNT) [6, 7], and the concept of digital twins through the extension of connectivity to nanoscale and biological environments [8]. This unconventional bio-inspired technique encodes information with one or more types of information molecules (IM) at the transmitter end, which are then propagated to a receiver through various mechanisms such as channel diffusion [9, 10, 11, 12, 13], mimicking methods of communication commonly found in the natural world. As research in bioengineering and nanotechnology continues to advance, there are vast opportunities [14] in the realm of MC for the development of bio-nanoscale communication systems that could potentially revolutionize fields such as healthcare [11, 15, 16], nanomachines [3, 12, 16], and brain machine interfaces (BMI) [3, 14]. Microfluidic channel-based MC systems [11, 12, 17] have been the center of attention due to their controllable environment [18, 19], their ability to simulate biochemical intra-body communications [20], and their ability to test lab-on-a-chip technologies [21]. The system is also a suitable platform for testing various modulation, diffusion, and demodulation techniques of IM [13], making the microfluidic channel ideal for both fundamental research and exploration of potential practical applications. However, the microfluidic MC system still faces a wide variety of challenges that need to be addressed, particularly in detection and noise mitigation [12, 22], before it can be deployed for more practical use cases in the real world. The stochastic nature of diffusion in MC means that it is also susceptible to many of the same issues that affect traditional electromagnetic (EM) communications, particularly noise interference. This includes different types of noise, such as intersymbol interference (ISI) [23], environmental noise, binding noise, and Brownian noise [13], which can lead to high bit error rates (BER) and, as a result, affect the reliability of information transmission [24]. In addition to the complexity of the microfluidic environment in the MC channel, detection schemes such as maximum likelihood (ML) [25, 26] or optimal detection thresholds [27] alone may not always provide the most optimal performance under rapidly changing channel conditions and, as demonstrated in [28, 29, 30, 31], adaptive thresholding methods perform promisingly well. Although more recent and complex schemes such as machine learning (ML)-based adaptive thresholding could potentially provide better performance under varying channel conditions [32, 33, 34, 35, 36, 37, 38], computational costs, prior knowledge requirements of channel models, and large amounts of learning data requirement may outweigh the benefits in some scenarios [39], such as deployment on an intra-body nanomachine. Such a device would ideally require accurate, efficient, and simple detection schemes. To address these challenges, we propose a novel adaptive thresholding technique based on a Proportional-Integral-Derivative (PID) controller [40]. In this paper, it will be called an adaptive real-time threshold receiver (ART-Rx). PID controllers are already ubiquitous in various industrial settings and are known to be reliable, fast, and efficient. Unlike ML, it does not require any training or learning of channel models [40, 41]. Our proposed approach aims to utilize the benefits of PID controllers to adaptively adjust receiver detection thresholds in response to dynamical channel noise conditions. The closed-loop system will allow the receiver to adjust the detection threshold in discrete real-time based on various feedback parameters such as the BER of previously transmitted bits and channel environmental parameters. The implementation of ART-Rx tested in this paper with the Smoldyn simulator [42] employs a PID controller that adjusts the detection threshold by calculating the error between the observed peak receptor binding levels and a desired setpoint at each symbol interval. The suggested implementation can potentially improve the overall reliability and robustness of the molecular communication receiver in noisy microfluidic environments such as within the human body while remaining computationally efficient. The remainder of this paper is organized as follows. Section II provides a detailed overview of our ART-Rx implementation and the underlying assumptions. In Section III, we describe the simulation setup, including evaluation parameters and specifications, and present simulation results comparing ART-Rx performance to the statistically optimal detection threshold compared to as a benchmark under various noise levels and channel conditions. This section also includes a comprehensive performance analysis and discusses limitations of our approach, such as the impact of PID parameter tuning and challenges associated with real-world implementation. Finally, Section IV concludes the paper by summarizing key findings and outlines directions for future research on the ART-Rx system."
https://arxiv.org/html/2411.09582v1,Safety Filter for Robust Disturbance Rejection via Online Optimization,"Disturbance rejection in high-precision control applications can be significantly improved upon via online convex optimization (OCO). This includes classical techniques such as recursive least squares (RLS) and more recent, regret-based formulations. However, these methods can cause instabilities in the presence of model uncertainty. This paper introduces a safety filter for systems with OCO in the form of adaptive finite impulse response (FIR) filtering to ensure robust disturbance rejection. The safety filter enforces a robust stability constraint on the FIR coefficients while minimally altering the OCO command in the \infty-norm cost. Additionally, we show that the induced \ell_{\infty}-norm allows for easy online implementation of the safety filter by directly limiting the OCO command. The constraint can be tuned to trade off robustness and performance. We provide a simple example to demonstrate the safety filter.","This paper presents a safety filter for robust disturbance rejection via online optimization. Online convex optimization (OCO) is a broad set of methods that can be used for disturbance rejection. This includes classical techniques such as recursive least squares (RLS) (Section 2.2 of [1] or Section 9.4 of [2]) and other variants [3, 4, 5]. It also includes more recent regret-based formulations [6, 7, 8, 9, 10]. This is especially relevant in high-precision control applications such as satellite pointing where moving physical components cause disturbances that are neither purely stochastic or worst case [11, 4]. In these applications, OCO is used to learn the disturbance characteristics and compute a control command to reject the disturbance. However, the OCO is typically designed assuming perfect knowledge of the plant dynamics. This can lead to instability when there are small amounts of model uncertainty resulting in unsafe operating conditions. In the realm of safety critical control, a popular method of encoding safety constraints is by use of the control barrier function (CBF). This is relevant in autonomous vehicle and robotic applications where safety is tied to obstacle avoidance. These kinds of safety constraints can be accounted for by defining a safe region and constructing a corresponding CBF. The CBF effectively defines the set of safe control inputs that keep the system from entering unsafe regions. This can be implemented as a safety filter which minimally alters the baseline control input while imposing the CBF as a point wise in time constraint [12, 13]. Additional works on robust CBFs account for model uncertainties [14]. Our work focuses on designing a safety filter which can be implemented online for uncertain systems with OCO. We start with a motivating example where RLS is used for adaptive disturbance rejection. In this example, uncertainty causes the system to go unstable. This motivates the need for the safety filter design. We then describe a more general framework for systems with OCO which are subject to disturbance and uncertainty. Specifically, we consider the class of OCO that takes form as an adaptive FIR filter with time-varying coefficients. The safety filter has two competing objectives: robust stability and disturbance rejection performance. This combines robust control techniques and CBF methods for safety critical control. Our main contributions are the following. First, we use a scaled small gain condition and induced \ell_{\infty}-norm bounding property (Theorem 1 and Lemma 2 from [15], respectively) to define a safe (i.e. stable) set of FIR coefficients. The safe set is defined by a bound on the adaptive FIR filter that satisfies the scaled small gain (i.e. robust stability) condition. Second, we formulate the safety filter as a constrained minimization problem which computes the signal that minimally alters the unconstrained FIR filter output and restricts the FIR coefficients to the set of stable gains point wise in time. Third, we provide an explicit solution to the constrained minimization problem which can easily be implemented online without explicitly computing the optimal FIR coefficients. Lastly, we revisit the motivating example to demonstrate that the safety filter ensures both robust stability and disturbance rejection. Notation: Let \mathbb{N}_{+} and \mathbb{R}^{n} denote the set of nonnegative integers and real n\times 1 vectors, respectively. Discrete-time signals are given by vector-valued sequences, u:\mathbb{N}_{+}\to\mathbb{R}^{n}, where u_{t}\in\mathbb{R}^{n} is the value at time t. The \ell_{p}-norm of a signal u is defined as: \displaystyle\|u\|_{p} \displaystyle=\left(\sum_{t=0}^{\infty}\|u_{t}\|_{p}^{p}\right)^{1/p} (1) where \|u_{t}\|_{p}=\left(\sum_{i=1}^{n}|u_{t}(i)|^{p}\right)^{1/p} is the vector p-norm, and u_{t}(i) is the i^{\text{th}} entry of u_{t}. Let \ell_{p}^{n} denote the set of signals with finite \ell_{p}-norms, i.e. \ell_{p}^{n}=\{u:\|u\|_{p}<\infty\}. The superscript n is used to denote the dimension of the signal at any given time but may be dropped for simplicity. Let the set \ell_{pe}^{n}\subset\ell_{p}^{n} denote the subset of signals which have a finite \ell_{p}-norm on all finite time intervals, i.e. \ell_{pe}^{n}=\{u:\sum_{t=0}^{T}\|u_{t}\|_{p}^{p}<\infty,\,\forall\,T\in% \mathbb{N}_{+}\}. We refer to \ell_{p}^{n} and \ell_{pe}^{n} as the signal space and extended signal space, respectively. Let G:\ell_{pe}^{n}\to\ell_{pe}^{m} denote systems that map input signals u\in\ell_{pe}^{n} to output signals v\in\ell_{pe}^{m}. The induced \ell_{p}-norm of G is defined as: \displaystyle\|G\|_{p\to p} \displaystyle=\sup_{0\neq u\in\ell_{p}}\frac{\|v\|_{p}}{\|u\|_{p}}. (2) We use \|u\| and \|G\| to denote signal and system induced norms when the specific p-norm is not important. Additionally, we reserve capital letters for systems, matrices, and constants and lowercase letters for signals and vectors. Lastly, we use shorthand u_{i:j} to denote a subsequence of a signal u from time i to j: u_{i:j}={\small\left[\begin{smallmatrix}u_{i}\\ \vbox{\hbox{.}\hbox{.}\hbox{.}\kern-0.47998pt}\\ u_{j}\end{smallmatrix}\right]}."
https://arxiv.org/html/2411.09550v1,A small-gain criterion for 2-contraction of large scale interconnected systems,"Despite modular conditions to guarantee stability for large-scale systems have been widely studied, few methods are available to tackle the case of networks with multiple equilibria. This paper introduces small-gain like sufficient conditions for 2-contraction of large-scale interconnected systems on the basis of a family of upper-bounds to the L_{2} gains that arise from the gains computed on individual channels of the second additive variational equation. Such a condition guarantee the 2-additive compound of the system’s Jacobian to be exponentially contractive, thus implying convergence towards equilibria of the system’s solutions. The gains are obtained by solving suitable Linear Matrix Inequalities. Three interconnected Thomas’ systems are considered in order to illustrate the application of the theory and the degree of conservatism.","The prediction of the long term behavior of nonlinear dynamical systems is a challenging and a hard topic that has caught the interest of the scientific community for a long time. This is also strongly related with assessment of stability and instability properties of solutions of a dynamical system. During the years, this topic has been tackled from different points of view, giving rise of many complementary approaches such as Lyapunov-based analysis [18], Input-to-State Stability (ISS) [21], passivity [19], monotonicity [10], contraction theory [20], Incremental Stability [1], or extreme stability [26], just to name a few. In recent years, k-Contraction theory, based on the seminal paper by James Muldowney [16], has gained interest (see [12] and [5] for a recent survey on the topic). The idea relies on the connection between compound matrices and linear time-varying differential equations and, as a consequence, with nonlinear dynamical systems through the variational equation. In particular, the method imposes conditions on some matrix measures of the k-th additive compound of the Jacobian in order to quantify how k-dimensional perturbations with respect to initial conditions propagate along solutions. For k=1 we have the standard contraction property, while for k=2 we obtain Muldowney’s conditions, which means that area perturbations are contracting along solutions in the state space of the system thus ruling out, on convex domains, the presence of periodic orbits. It has been also shown how it is possible to verify k-Contraction property without computing k-th order compound matrices [8]. Exploiting the algebra of compound matrices, it has allowed establishing a connection of the 2-additive compound of the system’s Jacobian with its Lyapunov exponents [15],[14], providing a method to bound the maximal Lyapunov exponent and showing how it can be estimated by solving a certain number of Linear Matrix Inequalities. Furthermore, in [3], the 2-additive approach has been exploited to provide modular small-gain like conditions for 2-contraction of a system constructed as the feedback interconnection of two subsystems. This latter approach allows reducing the size of the LMI problem to be solved, from n\choose 2, with n dimension of the system, into three LMIs of dimension n_{1}\choose 2, n_{2}\choose 2 and n_{1}n_{2} respectively, where n_{1} and n_{2} are the dimensions of the two subsystems, n_{1}+n_{2}=n. Consideration of a large number of interconnected subsystems, known in the literature as large-scale interconnected systems, has gained interest in the research community due to its widespread application in different fields, e.g. neural networks [11], dynamic nonlinear networks with coupled and multiterminal resistors, inductors, and capacitors [7], biochemical networks [22], large-scale dynamic systems [13], swarm robotics [6]. This is a challenging topic due to the intrinsic complexity of finding conditions for global stability or convergence of solutions, especially in cases where the overall system exhibits multistability. In particular, sufficient conditions often arise from a stability analysis. Small-gain like criteria and input-output analysis have demonstrated to be a useful tool to tackle this problem. In [24] the author investigate how to decompose and the well-posedness of large scale systems, before treating the stability and instability problem with respect to different vector L_{p} norms. While, in [9], authors provide small-gain like conditions for stability of ISS interconnected systems (relaxing the need for linear gain functions). A further step in this direction is to consider a larger number of interconnected subsystems. Indeed, large-scale interconnected systems have gained a lot of interest in the research field due to their widespread application in different fields, e.g. neural networks [11], dynamic nonlinear networks containing coupled and multiterminal resistors, inductors, and capacitors [7], biochemical networks [22], large-scale dynamic systems [13], swarm robotics [6]. However, finding conditions for global stability or convergence of solutions is still a challenging problem, especially in cases where the overal system exhibits multistability. Among other approaches, small-gain like criteria and input-output analysis have demonstrated to be a useful tool to investigate stability of large-scale interconnected systems. For instance, a comprehensive input-output treatment of large scale systems can be found in [24], while [9] provides small-gain like conditions for ISS-stability of interconnected systems. In this paper, a system composed of more than two interconnected systems is considered. The goal is to extend the approach in [3] to this set-up, i.e., to provide small-gain like conditions ensuring 2-contraction of the overall system, thus enabling convergence towards equilibria, i.e., in a multistable setting. AS in [3], we look for conditions which can be verified by solving a finite number of LMIs, for which efficient software is available. The paper is structured as follows: Section 2 shows how the 2-additive compound matrix of a partitioned variational equation can be decomposed in a number of interconnected subsystems, whose equations can be written explicitly. For each subsystem a notion of L_{2} gain is introduced in Section 3, while Section 4 provides the main result on 2-contraction of the overall system in terms of a modular small-gain condition. Section 5 is devoted to the proof of the result, while an application example is discussed in Section 6. Finally, some conclusion and final remarks end the paper in Section 7."
https://arxiv.org/html/2411.09335v1,Experimental Demonstration of Remote Synchronization in Coupled Nonlinear Oscillator,"This study investigates remote synchronization in scale-free networks of coupled nonlinear oscillators inspired by synchronization observed in the brain’s cortical regions and power grid. We employ the Master Stability Function (MSF) approach to analyze network stability across various oscillator models. Synchronization results are obtained for a star network using linearization techniques and extended to arbitrary networks with benchmark oscillators, verifying consistent behavior. Stable synchronous solutions emerge as the Floquet multiplier decreases and the MSF becomes negative. Additionally, we demonstrate remote synchronization in a star network, where peripheral oscillators communicate exclusively through a central hub, drawing parallels to neuronal synchronization in the brain. Experimental validation is achieved through an electronic circuit testbed, supported by nonlinear ODE modeling and LTspice simulation. Future work will extend the investigation to arbitrary network topologies, further elucidating synchronization dynamics in complex systems.","Synchronization is a ubiquitous phenomenon observed in various natural and artificial systems. Pioneering work by Winfree explored synchronization in large populations of coupled oscillators [1]. Since then, the field has grown significantly, encompassing models from Kuramoto [2] and Winfree motivated by biological processes [3], such as brain waves [4], cardiac pacemaker cells [5], firefly flashing [6], and biochemical oscillations [7]. Remote synchronization is a more recent discovery [8, 9, 10, 11]. Bergner et al. investigated phase synchronization in star networks of non-identical Stuart-Landau oscillators [12], establishing necessary conditions and showing that fixed-amplitude systems cannot exhibit this behavior. Minati et al. further explored remote synchronization and pattern formation in larger rings of nonlinear electronic oscillators [13]. The growing interest in brain abnormalities has made remote synchronization in the brain a topic of significant interest. Motivated by this, Qin et al. investigated remote synchronization and pattern formation in star networks using the Kuramoto-Sakaguchi model [14], studying the role of structural connections between neurons in synchronizing cortical regions. All these study have been done for star network and bipartite networks but do not comment on the arbitrary network. This investigation aims to achieve remote synchronization in arbitrary networks of coupled nonlinear oscillators. To ensure a comprehensive exploration of this phenomenon, a multifaceted approach encompassing analytical, numerical, and experimental methods has been adopted. For the analytical component, the Floquet theory, the Master Stability Function approach, and the Gershgorin circle theorem have been employed to investigate the underlying mechanisms and conditions for remote synchronization."
https://arxiv.org/html/2411.09307v1,Model-Based Event-Triggered Implementation of Hybrid Controllers Using Finite-Time Convergent Observers,"In this paper, we explore the conditions for asymptotic stability of the hybrid closed-loop system resulting from the interconnection of a nonlinear plant, an intelligent sensor that generates finite-time convergent estimates of the plant state, and a controller node that receives opportunistic samples from the sensor node when certain model-based event-triggering conditions are met. The proposed method is endowed with a degree of separation, in the sense that the controller design is independent of the sensor design. This is achieved under mild regularity conditions imposed on the hybrid closed-loop system and the existence of persistently flowing solutions. We demonstrate the versatility of the method by implementing it on: 1) a sampled-data controller for regulation of linear plants; 2) a synergistic controller for attitude stabilization of rigid bodies. The effectiveness of these novel controllers is demonstrated through numerical simulations.","I-A Background Event-Triggered Control (ETC) has garnered significant interest in theoretical investigation due to its ability to reduce the data transmission rate compared to traditional periodic sampled-data control. Early works on ETC often assume full state feedback (cf. [1]). However, this assumption may not hold for practical applications, prompting the development of output-based ETC. This paper focuses on generalizing one of the output-based ETC approaches, termed continuous-time observer-based ETC (cf. [2]), to the hybrid systems domain and examines the conditions which guarantee closed-loop asymptotic stability, robustness, and non-Zeno solutions. One important class of continuous-time observer-based ETC schemes adopts the fundamental idea rooted in the framework of model-based ETC (cf. [3]). The idea is to exploit knowledge of the plant to integrate a synthetic model at both the sensor and controller nodes, between which a communication channel is monitored by event-triggering conditions. The model calculates control actions when the channel is idle and updates its state when the channel is active. One of the pioneer works in this direction [4] employs a fixed threshold to enable aperiodic transmission while ensuring ultimate boundedness of the state of a perturbed linear plant. The subsequent work [5] uses a relative threshold combined with periodic event detection to trigger transmission opportunistically and achieves global exponential stability and \ell_{2}-gain performance for the linear plant. The recent work [6] examines how a fixed threshold with or without periodic event generation affects the stability, robustness, and the inter-transmission time for a linear plant. Due to utilization of information during idleness of communication channel, these continuous-time observer-based model-based ETC schemes show superiority in reducing transmissions over output-based ones [7, 8, 9, 10] without an observer and over observer-based ones [11, 12, 13, 14] without a synthetic model, as witnessed by its application to distributed systems in [15], parameter-varying systems in [16], etc. I-B Motivations For most of the relevant works on continuous-time observer-based model-based ETC, transmissions occur incessantly even in the absence of disturbance (cf. [4, 5, 15, 16]). This is due to the fact that state estimates converge asymptotically, hence the event-triggering condition may continue to issue transmissions. It is plausible that the number of transmissions is finite if the estimate converges to the plant state within a finite time. Evidence supporting this can be found in [6], where significant savings in communication efforts are observed. Hence, incorporating finite-time convergent sensor dynamics into the framework of continuous-time observer-based model-based ETC is the core motivation of this paper. Hybrid systems encompass a combination of continuous-time and discrete-time dynamics that are described by differential and difference inclusions, respectively. Due to the sampling action by ETC, it becomes natural to model and analyze ETC systems under the framework of hybrid systems (cf. [17]). The major advantage of hybrid system models over other system models with impulsive dynamics is that, by meeting the hybrid basic conditions, asymptotic stability of a compact set is not only uniform but also robust (cf. [18, Theorem 7.12] and [18, Theorem 7.21]). Nevertheless, there are few works on continuous-time observer-based model-based ETC that adopt the hybrid system approach. Most of the aforementioned works use a piecewise linear system or input-to-output stability approach to model and analyze the interconnection of continuous-time linear system components (cf. [4, 5, 15, 16]). However, hybrid systems models are better suited to handle scenarios where some system components exhibit both continuous-time and discrete-time dynamics. For instance, impulsive updates of the finite-time convergent observer in [19] are treated within the hybrid system framework. The interconnection of a sampled-data controller with a hybrid observer in [20] is modeled and analyzed via hybrid tools. Therefore, generalizing the framework of continuous-time observer-based model-based ETC to the hybrid systems domain is our second motivation of this paper. The separation principle states that, if a stabilizing state feedback is fed with state estimates from a stabilizing observer, then resulting closed-loop system is stable (cf. [21]). This property is attractive from the design perspective since it allows for various combinations of controllers with observers. For linear systems, global separation can be achieved by decoupling the dynamics of the closed-loop system into independent controller and estimator components. For nonlinear systems, one can expect at most a local or semiglobal separation (cf. [22, 23]). A local separation principle for a class of hybrid systems in derived in [21]. However, it does not consider ETC while making a conservative stability assumption on the observer. In the context of continuous-time observer-based model-based ETC, the aforementioned works [15, 16, 20] fail in achieving a separation principle among system components. Notable exceptions are the works [4, 5, 6] on linear plants. For nonlinear plants and hybrid system components in this paper, we do not insist on achieving a separation principle. Rather, we pursue a separation of the controller design from the sensor design, namely the global asymptotic stabilizing capability of the controller maintains when incorporating the sensor. This is our third motivation of this paper. I-C Challenges and approaches We start by assuming that there exists a hybrid controller that asymptotically stabilizes a given compact set. Furthermore, we assume that this controller has continuous access to the state of the plant. However, in most control applications, the state must be reconstructed from sensor measurements, which is a process that presents many challenges, such as 1) sensor selection, namely determining the set of outputs that are required to estimate the state of the plant; 2) sensor limitations, such as quantization, sampling frequency, among others. In this paper, we consider a new type of sensors which are able to estimate the state of the plant in finite-time and that transmits these estimates to the controller when certain state-dependent criteria are met. We aim to study under which conditions the properties of the nominal state feedback controller are not compromised by the intrusion of this class of sensors into the control loop. This pursuit encounters two major challenges, which we approach with the help of hybrid tools: 1. One challenge is to relate stability properties of the nominal set to those of the target set. On one hand, the hybrid time domain of a solution to the nominal system may differ from that of a solution to the closed-loop system, even if the sensor issues perfect state estimates. On the other hand, the distance of a vector to the target set may not be a bounded function of the distance of the respective vector components to the nominal set. The work [21] gets around these issues by making an stability assumption on the closed-loop system constrained by perfect recovery of state estimates, which goes against our pursuit. In this paper, we borrow the notions of j-reparametrization and Lipschitz set-valued mappings to bridge the gap between stability properties of the nominal set and those of the target set. 2. The other challenge is to appropriately describe the stabilizing property of the hybrid sensor. The sensor has to accept inputs experiencing jumps (arising from event-triggered sampling) known as hybrid inputs (cf. [24]), but there lacks a notion of stability/attractivity of sets for hybrid systems with hybrid inputs (cf. [25]). The method [21, Assumption 9] takes the convex hull of the flow map with respect to the control input to get rid of the control input in the sensor dynamics. But this approach leads to a conservative stability assumption on the sensor, which can be hard to verify for practical examples. Another approach relies on the notion of complete uniform observability for a single-input single-output continuous-time plant (cf. [26, Assumption 2]). However, this approach only admits specific system dynamics and hence it remains unknown how to adapt the approach to our case of general hybrid dynamics. In this paper, we propose an alternative approach that describes the stabilizing hybrid sensor in terms of set attractivity and invariance. I-D Contributions Under the theory of hybrid systems, this paper proposes a novel continuous-time observer-based ETC framework that contributes on: 1. incorporating finite-time convergent sensor dynamics. Few attempts as such have been made in the literature with notable exceptions in [27, 28], but they do not follow the model-based ETC strategy; 2. enabling the consideration of nonlinear plants, hybrid controllers, hybrid sensors, and model-based event-triggering conditions. Such general system dynamics are rarely treated in the literature with notable exception in [20], whereas most relevant works consider specific system dynamics (cf. [4, 5, 6, 11, 12, 13, 14, 15, 16]); 3. separating the controller design from the sensor design. This has barely been achieved in the literature on hybrid systems with notable exceptions in [21, 26], but their approaches have limitations as mentioned above that makes it hard to extend to our setting. The separation property obtained in this paper makes our approach versatile in selecting the hybrid controller for the nominal nonlinear plant. Using the proposed framework, we design novel model-based event-triggered controllers for regulation of linear plants as well as attitude stabilization of rigid bodies, which, to the best of our knowledge, have never been considered in the literature. We reiterate that this paper does not develop particular structures for the hybrid controllers or sensors. Rather, it establishes rigorous conditions under which the target set is robustly asymptotically stable for the closed-loop system."
https://arxiv.org/html/2411.09254v1,Are the flows of complex-valued Laplacians and their pseudoinverses related?,"Laplacian flows model the rate of change of each node’s state as being proportional to the difference between its value and that of its neighbors. Typically, these flows capture diffusion or synchronization dynamics and are well-studied. Expanding on these classical flows, we introduce a pseudoinverse Laplacian flow system, substituting the Laplacian with its pseudoinverse within complex-valued networks. Interestingly, for undirected graphs and unsigned weight-balanced digraphs, Laplacian and the pseudoinverse Laplacian flows exhibit an interdependence in terms of consensus. To show this relation, we first present the conditions for achieving consensus in the pseudoinverse Laplacian flow system using the property of real eventually exponentially positivity. Thereafter, we show that the pseudoinverse Laplacian flow system converges to consensus if and only if the Laplacian flow system achieves consensus in the above-mentioned networks. However, these are only the sufficient conditions for digraphs. Further, we illustrate the efficacy of the proposed approach through examples, focusing primarily on power networks.","I INTRODUCTION Network science examines interconnected systems, where nodes represent entities and edges represent interactions. A key focus is on complex networks, which find applications in areas such as social, biological, and technological domains. In these applications, one is often interested in understanding the behavior of nodes (or agents) over time. This behavior is captured by Laplacian flows, where the evolution of node’s state is influenced by its interactions with neighboring nodes, providing insights into insights into dynamics like diffusion, synchronization, and influence propagation [1, 2, 3]. On the other hand, consensus algorithms focus on enabling a group of agents to reach agreement on a common value or state. Thus, these algorithms can be viewed as a specific case of Laplacian flows, where the Laplacian matrix governs the state evolution attains consensus. Over the last two decades, many important results for consensus algorithms were established by exploring algebraic, spectral, and combinatorial properties of the Laplacian matrices. For studies in this direction and beyond, see [4, 5, 6, 7, 8, 9, 10]. Despite the extensive research on Laplacian flows and their key role in consensus studies, most existing work focuses on “real-valued” Laplacian matrices. While effective in many applications, such assumption can be restrictive for emergin fields like distribution power networks, quantum dynamics, electrodynamics, and machine learning [11, 12, 13, 14]. As highlighted in [14], the Laplacian matrix in these contexts is complex-valued. We refer to such systems as complex-valued networks to distinguish them from complex networks. The dedicated session on complex-valued networks at SIAM’s 2024 Mathematics of Data Science conference underscores the growing importance of this emerging field. Distributed control algorithms for complex networks often employ Laplacian dynamics to study consensus in social networks [3]. Some works exploring Laplacian dynamics in complex-valued networks include [14, 15, 16, 17, 18]. In [15], the authors discuss modulus consensus using the concept of structural balance under the assumption of Hermitian adjacency matrices. Further, a spectral clustering algorithm in signed networks is proposed in [17] employing structural balance and anti-balance notions for Hermitian weight matrices. Few works employing eventually exponentially positivity property in the analysis of real-valued Laplacian flows are [19, 20, 21]. In contrast to the existing literature, we study the relationship between complex-valued Laplacians and their pseudoinverses in various types of network, such as signed and directed networks. We compute the Moore-Penrose pseudoinverse of the Laplacian matrix, corresponding to the network, to obtain the pseudoinverse Laplacian flows. We consider unsigned networks which find applications in power networks and non-uniform Kuramoto oscillators. To the best of our knowledge, we are the first to introduce and study pseudoinverse flow systems. Below, we summarize our key contributions: 1. Using the real eventually exponentially positivity property (rEEP) introduced in [18], we establish the necessary and sufficient conditions for achieving consensus in both complex-valued Laplacian and pseudoinverse Laplacian flows. We focus on cooperative and antagonistic (i.e., symmetric) complex-valued networks and weight-balanced digraphs. 2. We establish that the property of rEEP is equivalent to the marginal stability or semi-convergence of the Laplacian, as well as the pseudoinverse Laplacian flows. We use complex Perron-Frobenius theory to establish the real eventually exponential positivity of the matrices. Finally, we validate our mathematical findings on synthetic and IEEE benchmark power networks. In addition, throughout the paper we demonstrate our theoretical results using several toy network systems."
https://arxiv.org/html/2411.09237v1,Unsupervised Physics-Informed Neural Network-based Nonlinear Observer design for autonomous systems using contraction analysis,"Contraction analysis offers, through elegant mathematical developments, a unified way of designing observers for a general class of nonlinear systems, where the observer correction term is obtained by solving an infinite dimensional inequality that guarantees global exponential convergence. However, solving the matrix partial differential inequality involved in contraction analysis design is both analytically and numerically challenging and represents a long-lasting challenge that prevented its wide use. Therefore, the present paper proposes a novel approach that relies on an unsupervised Physics Informed Neural Network (PINN) to design the observer’s correction term by enforcing the partial differential inequality in the loss function. The performance of the proposed PINN-based nonlinear observer is assessed in numerical simulation as well as its robustness to measurement noise and neural network approximation error.","I INTRODUCTION Nonlinear observer design is a fundamental research area in control theory that is constantly attracting attention from researchers in the community. While general and systematic frameworks for state estimation of linear systems with global convergence guarantees are well-established in the literature [1, 2], nonlinear observer design still suffers from a lack of generality and global convergence guarantees. The literature abounds with various nonlinear observer methods such as high gain observers [3], immersion and invariance-based observers [4], observers based on geometric methods [5], those based on Linear Matrix Inequalities (LMI)s [6], algebraic estimators [7], approaches relying on an injective transformation into a larger latent space [8], and the well-known Extended Kalman Filter [9]. However, most of the above-mentioned observer designs rely heavily on the class of nonlinearity of the system or provide only local convergence guarantees. On the other hand, observer design based on contraction analysis is slowly regaining popularity since its first introduction in the late nineties in [10]. They offer a generic design framework that is suitable for a general class of smooth nonlinear systems while providing global exponential convergence guarantees. Contraction analysis was introduced in [11], drawing on concepts from continuum mechanics and differential geometry. It offers a distinct perspective on stability analysis compared to traditional methods like Lyapunov theory [12, 13, 14]. Rather than focusing on the convergence of a system to a target trajectory or equilibrium point, contraction analysis assesses stability by determining whether all trajectories of a system converge toward one another. In essence, a system is considered contracting if it eventually ”forgets” its initial conditions or any temporary disturbances, making this approach particularly well-suited for observer design. Indeed, the initial motivation behind contraction analysis was primarily related to the design of observers for nonlinear systems [15]. Numerous contraction-based nonlinear controllers and observers have since been proposed in the literature such as the work in [10, 16, 17, 11, 18, 19, 15]. Contraction analysis provides a systematic framework for designing nonlinear observers, leveraging sophisticated mathematical developments that ensure global exponential convergence. The observer’s correction term is determined by solving a Matrix Partial Differential Inequality (MPDI). However, despite the theoretical elegance of this approach, solving the MPDI presents significant analytical and numerical challenges that limit the use and implementation of contraction-based observers in their original form [20]. Therefore, we aim in the present paper to overcome this challenge by proposing a learning-based approach to design the contraction-based nonlinear observer’s gain satisfying the MPDI. Artificial Neural Networks (ANNs) have emerged as powerful tools in approximating solutions to ordinary and partial differential equations (ODEs and PDEs). By leveraging their ability to approximate complex nonlinear functions, neural networks can be trained to satisfy the differential equations governing physical phenomena directly. This approach, often referred to as Physics-Informed Neural Networks (PINNs), was pioneered by Raissi et al. [21], and incorporates the underlying physical laws into the learning process, allowing the network to learn solutions that are consistent with the known physics. The advent of automatic differentiation has further revolutionized this field by enabling the efficient computation of derivatives required in the training process. This capability facilitates the direct incorporation of differential equations into the neural network’s loss function, ensuring that the learned solutions not only fit the data but also satisfy the physical laws described by the differential equations. Physics-Informed Neural Networks (PINNs) have been extensively applied in modeling and parameter estimation of nonlinear dynamical systems. For instance, in [22], a Runge–Kutta-based PINN framework was proposed for parameter estimation and modeling of nonlinear systems where the physical loss improves the integral involved in the Runge-Kutta method, reducing the error in the learnable trajectories. From the perspective of this work, PINNs have been utilized in observer design for discrete-time and continuous-time nonlinear systems. The work in [23] introduced a PINN-based approach for designing nonlinear discrete-time observers, showing improved performance in state estimation without the need for explicit system models. Moreover, learning-based methods have been employed to design the KKL observer for autonomous nonlinear systems, as presented in [24, 25], and nonlinear systems subject to measurement delay [26, 27], where neural networks were trained to approximate a forward and inverse map involved in KKL observer design. The work in [28, 29] has shown that PINNs are effective in model predictive control, addressing practical challenges in the oil and gas industry, and accurately identified nonlinear dynamical systems. These findings emphasize the versatility of PINNs in system identification and observer design by embedding physical laws directly within neural network architectures, resulting in models that are both data-efficient and physically consistent, achieving robust generalization even for time horizons far beyond the training or operating points and maintaining resilience against additive noise. In the present paper, we propose an unsupervised learning approach to design a nonlinear observer for a general class of nonlinear systems based on the contraction analysis. The proposed approach relies on a Physics Informed Neural Network to enforce the contraction condition in the learning process. Based on the MDPI involved in the gain design of contraction-based nonlinear observer, we formulate an optimization problem by strategically designing the cost function of the Physics Informed Neural Network. Furthermore, we establish the robustness of the proposed learning-based nonlinear observer to the neural network approximation error and measurement noise and derive conditions ensuring exponential input-to-state stability. The present paper is organized as follows: Section II gives some background on contraction analysis and contraction-based nonlinear observers. The problem addressed in this paper is formulated in section III. Subsequently, we present the proposed unsupervised learning approach to design the observer’s gain in section IV, and analyze the robustness of the designed observer to the neural network approximation error and measurement noise in section V. The performance and robustness of the proposed observer is evaluated through two numerical examples in VI. Finally, a summary of the contributions and future work directions are provided in section VII. Notation. For a square matrix M, \operatorname{He}\{M\}=\frac{1}{2}(M+M^{T}) is the Hermitian Part of the matrix M. The class \mathcal{C}^{1} is the class of continuous functions with continuous first derivatives. The Euclidean norm of a vector u is denoted by \left\|{u}\right\|."
https://arxiv.org/html/2411.09177v1,Enhancing reinforcement learning for population setpoint tracking in co-cultures,"Efficient multiple setpoint tracking can enable advanced biotechnological applications, such as maintaining desired population levels in co-cultures for optimal metabolic division of labor. In this study, we employ reinforcement learning as a control method for population setpoint tracking in co-cultures, focusing on policy-gradient techniques where the control policy is parameterized by neural networks. However, achieving accurate tracking across multiple setpoints is a significant challenge in reinforcement learning, as the agent must effectively balance the contributions of various setpoints to maximize the expected system performance. Traditional return functions, such as those based on a quadratic cost, often yield suboptimal performance due to their inability to efficiently guide the agent toward the simultaneous satisfaction of all setpoints. To overcome this, we propose a novel return function that rewards the simultaneous satisfaction of multiple setpoints and diminishes overall reward gains otherwise, accounting for both stage and terminal system performance. This return function includes parameters to fine-tune the desired smoothness and steepness of the learning process. We demonstrate our approach considering an Escherichia coli co-culture in a chemostat with optogenetic control over amino acid synthesis pathways, leveraging auxotrophies to modulate growth.","The genetic engineering of microorganisms has enabled the manufacturing of a wide range of products, including chemicals, fuels, materials, and pharmaceuticals (Luo et al., 2021). Traditionally, bioproduction has relied on monocultures, harnessing the metabolic capabilities of a single species. However, engineering (large) metabolic pathways within a single cell can lead to metabolic burden, compromising cell growth and the process volumetric productivity. To alleviate this burden and enhance the overall process efficiency, metabolic pathways can be partitioned among multiple microbial species in a consortium (division of labor) (Roell et al., 2019). By distributing metabolic submodules across different populations, each species can specialize in the part of the pathway it is best suited for. It is worth noting that maintaining specific population levels within the consortium is essential for the optimization of the production process, as the concentration of each species directly influences the achievable volumetric productivity of the metabolic submodule it carries. Controlling population levels in microbial consortia, however, poses significant challenges. Due to the competitive exclusion principle (Kneitel, 2019), when multiple species compete for a single limiting resource, e.g., the carbon source, the species with the highest growth rate (fitness) will eventually outcompete and displace the others. Adjusting initial inoculation ratios can alleviate competitive exclusion in the short term, but it does not alter the long-term dynamics, making this approach unsuitable for prolonged cultivations, such as those in continuous bioreactors. Engineering endogenous interactions, like mutualism, is another strategy to address competitive exclusion by creating co-dependency among species, e.g., via engineering auxotrophies and cross-feeding relationships (Peng et al., 2024). However, the population dynamics are predetermined by the engineered interactions, limiting operational flexibility and adaptability. To overcome these limitations, external control mechanisms have been proposed to enhance operational flexibility and facilitate feedback control via externally tunable inputs. This approach allows the user to define different possible population setpoints according to process requirements, as opposed to having fixed engineered setpoints as in endogenous interactions. Conventional control strategies, such as proportional-integral-derivative (PID) controllers, have been proposed to regulate population ratios in bioreactors (Gutiérrez Mena et al., 2022). Although simple and useful in certain cases, PID controllers are inherently reactive, struggle with nonlinear system dynamics, and cannot handle system constraints, motivating more advanced control approaches. In the context of microbial consortia, model predictive control (MPC) offers a more advanced alternative by utilizing a system model to predict future behaviors and optimize control actions (Espinel-Ríos et al., 2023). However, its implementation can be challenging due to the difficulty in obtaining accurate mathematical models capturing system uncertainties. Reinforcement learning (RL) is a promising machine-learning control strategy where an agent (the controller) learns optimal control actions (process inputs) through interactions with the environment (the bioreactor system), without requiring an explicit system model. Previous studies have considered Q-learning, an action-value method, for setpoint tracking in microbial consortia with discrete bang-bang feeding control actions (Treloar et al., 2020). However, Q-learning involves deterministic policies and requires careful balancing of exploration and exploitation (often hard-coded via epsilon-greedy strategies). The value function in Q-learning, the expected cumulative reward, from which the optimal actions are computed upon solving an optimization problem, can be approximated, e.g., using neural networks. Q-learning may struggle to converge to an optimal policy if the value function is not properly approximated and/or if the optimization step poses numerical difficulties. This can be particularly challenging in continuous or high-dimensional action spaces. Thus, Q-learning may be more suitable for discrete actions. In this work, we consider RL based on the policy-gradient method (Petsagkourakis et al., 2020), which can address several of the limitations of Q-learning. Policy-gradient methods directly optimize the control policy, which can be approximated using, e.g., neural networks. This direct approach focused on the policy itself ensures convergence to at least a local optimum and naturally accommodates continuous input variables, enhancing operational flexibility as more of the input space can be explored and exploited. Furthermore, policy-gradient methods involve stochastic policies, which naturally balance the agent’s adaptive exploration-exploitation over time and are better suited for handling systems with high stochasticity, such as biological processes. A critical component of RL is the design of the return function, which guides the agent toward desired optimal behaviors. The inverse quadratic cost function, commonly used in optimal control problems, may offer good convergence properties when tracking a single setpoint in RL. However, in the context of microbial consortia, the latter function lacks a mechanism to directly incentivize the simultaneous satisfaction of multiple independent setpoint objectives. In other words, the agent may need to explore more extensively to discover a sweet-spot scenario where all setpoints are satisfied without prioritizing one over the other. This often results in a higher risk of suboptimal performance as the agent might oscillate between multiple individual objectives. To tackle this issue, we propose a novel return function based on multiplicative inverse saturation functions that can enhance multiple setpoint tracking performance. This reward structure ensures that maximum reward is achieved only when all setpoints are satisfied simultaneously and that improving individual setpoints while others remain off-target diminishes overall reward gains. This promotes a more balanced progression toward multiple targets and can guide the agent more precisely. The return function can be shaped to balance smoothness and steepness in the policy gradients and in the updates of its parameters."
https://arxiv.org/html/2411.09110v1,Information-Optimal Multi-Spacecraft Positioning for Interstellar Object Exploration,"Interstellar objects (ISOs), astronomical objects not gravitationally bound to the sun, could present valuable opportunities to advance our understanding of the universe’s formation and composition. In response to the unpredictable nature of their discoveries that inherently come with large and rapidly changing uncertainty in their state, this paper proposes a novel multi-spacecraft framework for locally maximizing information to be gained through ISO encounters with formal probabilistic guarantees. Given some approximated control and estimation policies for fully autonomous spacecraft operations, we first construct an ellipsoid around its terminal position, where the ISO would be located with a finite probability. The large state uncertainty of the ISO is formally handled here through the hierarchical property in stochastically contracting nonlinear systems. We then propose a method to find the terminal positions of the multiple spacecraft optimally distributed around the ellipsoid, which locally maximizes the information we can get from all the points of interest (POIs). This utilizes a probabilistic information cost function that accounts for spacecraft positions, camera specifications, and ISO position uncertainty, where the information is defined as visual data collected by cameras. Numerical simulations demonstrate the efficacy of this approach using synthetic ISO candidates generated from quasi-realistic empirical populations. Our method allows each spacecraft to optimally select its terminal state and determine the ideal number of POIs to investigate, potentially enhancing the ability to study these rare and fleeting interstellar visitors while minimizing resource utilization.","Interstellar objects (ISOs), astronomical objects traveling through space without any attachment to any system, present a unique opportunity to study various aspects of the universe, such as the formation and composition of other star systems, the origin of the universe, and possibly other forces at play in the expanses of space [1]. For example, one of the two ISOs observed to date [2, 3], named 1I/‘Oumuamua, visited us from the rough direction of the constellation Lyra exhibiting some unusual physical characteristics (a highly elongated shape, a lack of typical cometary volatiles, and a deviation from Keplerian trajectories [4, 5]), which might provide some clues into how our solar system and neighboring exoplanetary star systems are formed. Exploring these interstellar visitors in situ with dedicated spacecraft would offer an alternative means to acquire firsthand knowledge about interstellar space, which goes beyond what is obtained through remote observations via telescopes. Due to the hyperbolic nature of their orbit, ISOs only pass through the solar system once in their lifetime with inherently large and rapidly changing uncertainty in their state, often at high inclination and relative velocities [6]. This poses significant challenges in designing a guidance, navigation, and control (GNC) strategy for the ISO encounter, requiring fast response autonomous operations even with the limited computational capacity of spacecraft. These would involve some levels of approximations in the GNC policies for the sake of their onboard execution, which then introduces additional difficulties in formally quantifying the probability of a successful encounter. Furthermore, theoretical connections between this probability and our mission objective of maximizing the scientific return through the encounter (which we characterize by the amount of visual data collected by spacecraft cameras) still remain ambiguous. 1.1 Contributions This paper proposes a novel multi-spacecraft framework for locally maximizing the amount of information obtained through ISO encounters, with some probabilistic guarantees. Building on our previous work [7], we first derive a formal upper bound on the failure probability of the ISO encounter for hierarchical Itô stochastic nonlinear system, which models our spacecraft relative dynamics in the local-vertical local-horizontal (LVLH) frame [8, pp. 710-712] centered on the ISO with the large uncertainty in its state measurement. In order to achieve fast response autonomous operations even with the limited onboard computational capacity of the spacecraft, potential approximation errors in their control and estimation policies are explicitly considered when deriving the bound. The failure probability then enables the construction of an ellipsoid around the terminal position of the chief spacecraft, in which the ISO would be located at the terminal time with a finite probability. We further propose an approach to finding the terminal positions of the deputy spacecraft with respect to the chief spacecraft, optimally distributed around the ellipsoid to maximize the information we get from all the points of interest (POIs) in it. In particular, we utilize an information cost function characterized by the distance between the center of the ellipsoid and each spacecraft in the swarm and the visual coverage of the POIs, accounting for spacecraft positions and attitudes, camera specifications, and ISO position uncertainty, where the information here is defined as imaging data collected by spacecraft onboard cameras. Since this optimization only requires an upper bound for the ISO state uncertainty history over time, which we could obtain as in our previous work [6] with [9, 10], the optimal relative positions of the deputy spacecraft can be pre-computed offline, freeing onboard computational resources for some other highly autonomous tasks. Numerical simulations are performed to demonstrate the efficacy of our method using a quasi-realistic empirical population of ISOs [11, 9] with the empirical history of the navigation uncertainties [9, 10, 6, 7], where the optimization problems are solved using Nelder-Mead. Our first simulation examines the probability that a single spacecraft would be able to view the ISO, assuming an uncertainty ellipsoid with equal x, y, and z radii. As is expected from the aforementioned probability bound, the probability that the spacecraft views the ISO decreases as the ISO state uncertainty increases. Our second simulation empirically determines the optimal number of spacecraft to view as many POIs on the sphere as possible, where the size of the uncertainty sphere remains constant. It is demonstrated that a multi-spacecraft system both observes more POIs and results in a lower information cost than a singular spacecraft, however, an excess of spacecraft increases the overlap between the visual coverage of the system. 1.2 Related Work Previous studies have investigated the potential of ISO flyby missions for a single spacecraft, proposing viable methods for observing ISOs and other high-speed celestial objects [6, 7]. For example, it is rigorously shown that a control policy designed using the spectrally normalized deep neural network (SN-DNN) guarantees local, finite-time exponential boundedness of the expected spacecraft delivery error, assuming that the bounds for the estimation errors are known. These approaches would allow at least to encounter ISOs even under their large state uncertainty. However, the entirety of the ISO cannot be observed through a flyby as matching the spacecraft velocity with the ISOs’ is unrealistic with current propulsion technologies [9], making such missions inefficient for gathering valuable scientific data. Several numerical and theoretical methods have been proposed to circumvent these difficulties through the lens of information-based optimization. Previous works in this field with a swarm of spacecraft can be broken into two categories: gaining information from a body with a determined orbit, such as satellites or other spacecraft, and gaining information from an object with an undetermined orbit, such as celestial bodies. In [12], a passive observation technique is proposed for celestial bodies with known trajectories, whereas for the case of ISOs, an active approach should be taken to sufficiently capture information to address the large state uncertainty. Further, the algorithm utilized in the study only maximizes the amount of the celestial body covered by the field of view (FOV) of the spacecraft in the swarm. The overlap between spacecraft in the swarm must be minimized to make a more robust algorithm. This would prevent resources from being wasted by spacecraft redundantly examining the same area of the ISO. The collision avoidance techniques introduced in [13] utilize a minimum scalar distance that a spacecraft must be from each other. However, this constraint ignores any angular or directional constraints. With the highly uncertain trajectory of the ISO, it is crucial to minimize the amount of overlap in information gained by any two spacecraft. This consideration is crucial when optimizing information from bodies with controlled trajectories, such as during on-orbit inspection or servicing of satellites [14], [15]. These studies utilize an information cost to create a fuel and energy-efficient orbit around a target satellite. However, the information cost presented in these studies does not account for the spacecraft’s orientation or field of view. Instead, they determine a baseline distance for the spacecraft to start at and then algorithmically determine an orbit for the spacecraft to take. In the case of this study, orbiting the ISO would be infeasible again due to the ISO’s high relative velocity and the uncertainty of its dynamics. Some other studies focus on optimizing information from celestial bodies with uncontrolled trajectories; however, only comets and asteroids have been examined so far [16, 17]. In these studies, either the spacecraft that have to gain information are stationary, or the object they are observing is stationary and the spacecraft performs a flyby inspection of the celestial body. With an ISO, the spacecraft has to assume a formation in order to maximize the information gained, since the ISO’s state is uncertain and it never remains stationary. This paper proposes one solution to the issues listed above through the proactive deployment of multiple spacecraft, extending the ideas of information gain-based optimization. Hierarchical stochastic contraction also enables explicitly considering the state uncertainty of the ISO, having the spacecraft cooperate to gather visual data on all perspectives of the ISO locally optimally in one flyby mission."
https://arxiv.org/html/2411.09636v1,Nash equilibrium seeking for a class of quadratic-bilinear Wasserstein distributionally robust games,"We consider a class of Wasserstein distributionally robust Nash equilibrium problems, where agents construct heterogeneous data-driven Wasserstein ambiguity sets using private samples and radii, in line with their individual risk-averse behaviour. By leveraging relevant properties of this class of games, we show that equilibria of the original seemingly infinite-dimensional problem can be obtained as a solution to a finite-dimensional Nash equilibrium problem. We then reformulate the problem as a finite-dimensional variational inequality and establish the connection between the corresponding solution sets. Our reformulation has scalable behaviour with respect to the data size and maintains a fixed number of constraints, independently of the number of samples. To compute a solution, we leverage two algorithms, based on the golden ratio algorithm. The efficiency of both algorithmic schemes is corroborated through extensive simulation studies on an illustrative example and a stochastic portfolio allocation game, where behavioural coupling among investors is modeled.","A wide range of applications, from smart grids Saad1 and communication networks Scutari to social networks Acemoglu2013 can be modelled as a collection of self-interested interacting decision makers optimizing different cost functions under operational constraints. Game theory Basar1 provides the fundamental theoretical framework for analyzing such systems. Although investigating deterministic games can be adequate in some case studies Scutari , Paccagnan2017 , most real-world applications involve decision making under uncertainty, which stresses the need for the inclusion of stochasticity in the existing models. Several studies have explored uncertainty within a game theoretic context, based on particular assumptions on the probability distribution Kouvaritakis , Singh and/or the properties of the uncertainty sample space Aghassi2006 ; FukuSOCCP . When the probability distribution is unknown and distribution models are not an accurate description of the stochastic aspect of the problem, sampling-based or data-driven methods have shown strong potential for proposing robust solutions against uncertainty. Works such as Feleconf2019 ; Fele2021 ; Dario_Scenario ; fele-a ; Pantazis2020 ; mammarela2023 ; Pantazis2023_apriori design distribution-free approaches for data-driven Nash equilibria based on statistical learning techniques. More specifically, fele-a ; Pantazis2020 ; mammarela2023 ; Pantazis2023_apriori account for possible strategic perturbations around the Nash equilibrium. Separately, works based on Sample Average Approximation (SAA) techniques, such as Franci_2021 ; Franci_2021_merely , develop algorithms for finding Nash equilibria in stochastic settings by using expected values of cost functions. The works mentioned above constitute data-driven methods for stochastic equilibrium seeking. These works, however, do not account for ambiguity in the probability distribution, where the distribution itself may be uncertain within some known bounds. The challenge of ambiguity in the distributions becomes pronounced in multi-agent settings, where heterogeneous uncertainties affect the agents’ costs, often necessitating the consideration of different ambiguity sets, each representing the individual risk-averse nature of each agent. To account for distributional uncertainty, distributionally robust optimization (DRO) uses a so-called ambiguity set of possible probability distributions to make decisions robust against probabilistic variations within this set. Unlike scenario-based methods, which require many samples for robustness, DRO can perform well with limited data by adjusting the ambiguity set. DRO includes special cases like sample average approximation (SAA) and robust optimization (RO). At the same time, DRO can be less conservative than RO and offer better out-of-sample performance than SAA, making it especially useful in data-driven applications with limited data. Recently, Wasserstein ambiguity sets villani_topics_2016 , which use empirical data and the Wasserstein metric to measure distributional deviations, have gained attention. These sets are favored for penalizing horizontal shifts and providing finite-sample guarantees. Research has focused on convergence of empirical estimates in the Wasserstein distance Dereich ; mohajerin_esfahani_data-driven_2018 ; Dedecker1 ; Weed ; Weed_2 ; Fournier_2023 , as well as obtaining tractable reformulations of Wasserstein distributionally robust optimization problems mohajerin_esfahani_data-driven_2018 ; netessine_wasserstein_2019 ; Lotfi1 ; Lotfi2 . Extensions of those works include distributionally robust chance-constrained programs Chen2018 ; Hota2018 ; Alamo2024risk . Despite the considerable body of literature on DRO with Wasserstein ambiguity sets, the exploration of data-driven Wasserstein distributionally robust Nash equilibrium problems with heterogeneous uncertainty in the cost functions represents a notably underexplored topic. Most works in the literature consider moment-based methods or other measures of distance between distributions. For instance, Peng2021 considers a non-cooperative game with distributionally robust chance-constrained strategy sets applied to duopoly Cournot competition. The work Liu2018 develops distributionally robust equilibrium models based on the Kullback-Leibler (KL) divergence for hierarchical competition in supply chains. Other works mainly consider ambiguity in the constraints, such as the recent work Xia_elliptical_2023 , which studies a game with deterministic cost for each agent and distributionally robust chance constraints with the centre of the Wasserstein ambiguity set being an elliptical distribution; fabiani2023distributionally reformulates an equilibrium problem with a deterministic cost and distributionally robust chance-constraints as a mixed-integer generalized Nash equilibrium problem leveraging the results in Chen2023 . The contributions of this work with respect to the related literature are the following: (i) We study a class of heterogeneous data-driven Wasserstein distributionally robust games, where each agent’s ambiguity set is centered around an empirical probability distribution based on their individual data, while the Wasserstein radius is also set by each individual agent. We reformulate the original game as a robust Nash equilibrium problem and establish the connection between the distributionally robust and robust Nash equilibria of the corresponding problems. For this class of games, we demonstrate that the inner maximization can be solved without the use of epigraphic variables kuhn2019wasserstein , pantazis2023_DRG , which in a game-theoretic setting can lead to unshared coupling constraints. As such, our approach decreases computational complexity significantly. To the best of our knowledge, this is the first distributionally robust game-theoretic reformulation that leads to data-scalable results by leveraging the structure of the problem at hand. (ii) The robust Nash equilibrium problem is then reformulated as a variational inequality (VI). Unlike results of similar classes of problems in optimization Boskos_2024 , where the reformulated variational inequality is monotone under certain assumptions, the mapping corresponding to the game can be nonmonotone in general due to the heterogeneity of the agents’ ambiguity sets and costs. However, we show that this problem can be efficiently solved empirically using two algorithms: the adaptive golden ratio algorithm (aGRAAL) malitsky_golden_2020 and a hybrid version of this algorithm (Hybrid-Alg in Reza_2024 ). Notably, our numerical results show that in several cases, the convergence speed is close to linear, and increasing the number of samples does not slow down the convergence. Our results are then applied to a portfolio allocation game that takes into account market uncertainty and behavioural coupling of market participants."
https://arxiv.org/html/2411.09546v1,Architectural Exploration of Application-Specific Resonant SRAM Compute-in-Memory (rCiM),"While general-purpose computing follows Von Neumann’s architecture, the data movement between memory and processor elements dictates the processor’s performance. The evolving compute-in-memory (CiM) paradigm tackles this issue by facilitating simultaneous processing and storage within static random-access memory (SRAM) elements. Numerous design decisions taken at different levels of hierarchy affect the figure of merits (FoMs) of SRAM, such as power, performance, area, and yield. The absence of a rapid assessment mechanism for the impact of changes at different hierarchy levels on global FoMs poses a challenge to accurately evaluating innovative SRAM designs. This paper presents an automation tool designed to optimize the energy and latency of SRAM designs incorporating diverse implementation strategies for executing logic operations within the SRAM. The tool structure allows easy comparison across different array topologies and various design strategies to result in energy-efficient implementations. Our study involves a comprehensive comparison of over 6900+ distinct design implementation strategies for EPFL combinational benchmark circuits on the energy-recycling resonant compute-in-memory (rCiM) architecture designed using TSMC 28 nm technology. When provided with a combinational circuit, the tool aims to generate an energy-efficient implementation strategy tailored to the specified input memory and latency constraints. The tool reduces 80.9% of energy consumption on average across all benchmarks while using the six-topology implementation compared to baseline implementation of single-macro topology by considering the parallel processing capability of rCiM cache size ranging from 4KB to 192KB.","Cache memory remains one of the critical components in our computing system, enhancing overall performance by bridging the speed gap between the main memory (RAM) and the central processing unit (CPU). Besides, in recent years, static random access memory (SRAM)-based in-memory computing paved a promising direction to enable energy-efficient computation. However, the lack of design and automation tools to map computation on optimal SRAM architecture increases design time-to-market, resulting in higher engineering costs. This research resolves this issue by proposing an architectural exploration tool that efficiently maps logic computations to optimal cache architecture. Figure 1: (a) Conventional Von Neumann architecture, where an operation f is performed on data D within the CPU, incurs high data movement overhead, which can be reduced using (b) a CiM architecture, where f is computed directly within the memory, with the CPU primarily functioning as a control unit. Computing-in-memory (CiM) architectures have emerged as highly promising solutions for data-intensive applications. They minimize data movement, enhance computational capabilities, and improve the system’s overall energy efficiency by processing and storing data within cache memory. As shown in Figure 1 (a), the traditional Von Neumann architecture relies on data communication between the arithmetic logic unit (ALU) and cache memory through address and data buses. However, as the CPU performance is significantly higher than the memory performance, the Von Neumann architectures often create memory bottlenecks. CiM architectures, as shown in Figure 1 (b), mitigate the impact of large memory access latencies by performing the computations within the memory. By reducing data movement and exploiting parallelism within the memory, CiM architectures significantly enhance computational efficiency and performance. SRAM-based CiM architectures have been heavily investigated for performing various operations, such as matrix-vector multiplication (MVM) [1, 2], multiply-and-accumulate (MAC) operations [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], boolean logic operations [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30], and content-addressable memory (CAM) [31, 32, 33, 34, 35, 36] operations for fast searching operations. However, none presents a generic energy-saving architecture that spans across various applications. This work utilizes a novel series-resonance-based resonant CiM (rCiM) architecture that reduces dynamic power consumption by recycling the wasted energy during writing operations. This work proposes an agile architectural exploration tool to map various logical operations to an optimal SRAM macro cache size. The primary objective of the tool is to facilitate the development of novel energy-efficient SRAM-based energy-recycling rCiM implementations individually designed for specific boolean logical applications. In particular, the main contributions of the paper are as follows: • A novel resonant Compute-in-Memory (rCiM) structure that incorporates a series inductor to recycle energy dissipated during write operations. • An architectural exploration toolflow that integrates open-source synthesis tools (Berkeley-ABC [37] & YOSYS [38]) to identify the optimal SRAM configuration within a specified range of SRAM cache memory and map efficient logical operations tailored to an optimal rCiM macro size. • Comprehensive analysis of 6900+ distinct logical design implementations for EPFL combinational benchmark circuits [39] using 12 different SRAM topologies."
https://arxiv.org/html/2411.09320v1,AMARETTO: Enabling Efficient Quantum Algorithm Emulation on Low-Tier FPGAs,"Researchers and industries are increasingly drawn to quantum computing for its computational potential. However, validating new quantum algorithms is challenging due to the limitations of current quantum devices. Software simulators are time and memory-consuming, making hardware emulators an attractive alternative. This article introduces AMARETTO (quAntuM ARchitecture EmulaTion TechnOlogy), designed for quantum computing emulation on low-tier Field-Programmable gate arrays (FPGAs), supporting Clifford+T and rotational gate sets. It simplifies and accelerates the verification of quantum algorithms using a Reduced-Instruction-Set-Computer (RISC)-like structure and efficient handling of sparse quantum gates. A dedicated compiler translates OpenQASM 2.0 into RISC-like instructions. AMARETTO is validated against the Qiskit simulators. Our results show successful emulation of sixteen qubits on a AMD Kria KV260 SoM. This approach rivals other works in emulated qubit capacity on a smaller, more affordable FPGA.","In recent years, interest in quantum computing has achieved unique acceleration thanks to its potential in data-intensive applications. Nonetheless, the validation of new quantum computing algorithms is challenging due to the constraints imposed by current quantum devices. The production, administration, and upkeep of quantum hardware are exclusive domains of major corporations that grant access through cloud-based platforms, although usually with fees. Furthermore, the fidelity of outcomes can be substantially compromised by devices’ noise. Classical simulation remains the most popular solution for debugging, providing insights into the quantum state, hard to retrieve on real quantum hardware, but software simulation faces drawbacks, such as long execution times and high memory requirements, limiting scalability. Hence, exploring classical hardware platforms such as Field-Programmable Gate Arrays (FPGAs) holds significant promise. Indeed, hardware emulators are expected to outperform software-based counterparts in simulating quantum phenomena due to their ability to replicate the parallel nature of quantum computation more accurately. This work introduces AMARETTO (quAntuM ARchitecture EmulaTion TechnOlogy), a Reduced-Instruction-Set-Computer (RISC)-like architecture for quantum emulation on low-tier FPGAs, supporting Clifford+T and rotational gate sets. Validated using the Qiskit simulators, AMARETTO successfully emulated sixteen qubits on the AMD Kria KV260 SoM using a twenty-bit fixed-point numeric representation. This approach matches other works’ qubit capacity but with a smaller, more accessible FPGA. The article’s organization includes a review of quantum simulation on classical platforms and related work (Section II), details of the proposed architecture (Section III), results and validation methodology (Section IV), and conclusions with future perspectives (Section V)."
https://arxiv.org/html/2411.08999v1,Learning-Based Control Barrier Function with Provably Safe Guarantees: Reducing Conservatism with Heading-Aware Safety Margin,"We propose a learning-based Control Barrier Function to reduce conservatism in collision avoidance of car-like robots. Traditional CBFs often use Euclidean distance between robots’ centers as safety margin, neglecting headings and simplifying geometries to circles. While this ensures smooth, differentiable safety functions required by CBFs, it can be overly conservative in tight environments. To address this limitation, we design a heading-aware safety margin that accounts for the robots’ orientations, enabling a less conservative and more accurate estimation of safe regions. Since the function computing this safety margin is non-differentiable, we approximate it with a neural network to ensure differentiability and facilitate integration with CBFs. We describe how we achieve bounded learning error and incorporate the upper bound into the CBF to provide formal safety guarantees through forward invariance. We show that our CBF is a high-order CBF with relative degree two for a system with two robots whose dynamics are modeled by the nonlinear kinematic bicycle model. Experimental results in overtaking and bypassing scenarios reveal a 33.5\text{\,}\mathrm{\char 37\relax}\text{/} reduction in conservatism compared to traditional methods, while maintaining safety.Code: github.com/bassamlab/sigmarl","Control Barrier Functions are critical tools for ensuring safety in control systems, particularly for autonomous robots in dynamic environments. They provide a formal way to enforce safety constraints by rendering a designated safe set forward invariant [1]. In the context of car-like robots, such as Connected and Automated Vehicles, safety is critical. These robots must navigate complex environments and avoid collisions with other agents, while achieving their intended goals efficiently. Motion planning for car-like robots involves generating trajectories that are not only feasible concerning the robot’s dynamics but also safe w.r.t. the environment and other agents. Traditional Control Barrier Functions often simplify robots’ geometries to facilitate the computation of Control Barrier Function conditions or simplify safety estimation. A common simplification is to model the robots as circles, which allows for straightforward distance computation but ignores the robots’ actual shapes and orientations. While this simplifies the safety analysis and ensures the smoothness and differentiability required by CBFs, it can lead to overly conservative behaviors, especially in confined or densely populated environments. To address these limitations, we propose a learning-based Control Barrier Function that incorporates a heading-aware safety margin, inspired by Separating Axis Theorem [2] and Minimum Translation Vector [3], which we term Minimum Translation Vector-based safety margin. By accounting for car-like robots’ orientations and actual geometries, our method provides a more accurate estimation of safe regions. This approach reduces conservatism in collision avoidance, allowing them to navigate more efficiently without compromising safety. I-A Related Work Collision avoidance for car-like robots is a well-studied problem. While optimization-based approaches such as model predictive control are widely used [4, 5, 6, 7], their can be computationally intensive. ently, CBFs have gained attention for their forward invariance and formal safety guarantees. Traditional CBFs often simplify robot geometries as circles and define safety margins based on Euclidean distances between centers, which we refer to as the Center-to-Center-based safety margin. Work [8] uses Control Barrier Functions to ensure safety distance to a so-called avoidable set, which defines safe boundaries around round-shaped moving obstacles. Work [9] introduced safety barrier certificates for collision-free multi-robot systems using Control Barrier Functions, where each robot is modeled as a circle to simplify collision avoidance constraints. Study [10] extended CBFs to systems with high relative degrees, maintaining the circular approximation for robots. Further advancements include integrating learning into CBF frameworks with Center-to-Center-based safety margin. Work [11] uses off-the-center disks to avoid the conservatism in the Euclidean distance-based safety margins, where the deviation direction of the disks depends on the direction of the obstacles w.r.t. the lane center of the ego robot. In the domain of Connected and Automated Vehicles, studies [12] and [13] employed Center-to-Center-based safety margins within multi-agent Reinforcement Learning frameworks to ensure safety of the learned policies. They introduce another term to the longitudinal distance between Connected and Automated Vehicles to consider lane-changing behavior. Other similar works using Center-to-Center-based safety margin are [14, 15, 16]. While the circle approximation simplifies computations and ensures differentiability, it does not accurately capture the actual shape and orientation of car-like robots. This discrepancy can lead to overly conservative behaviors, limiting the robots’ ability to navigate efficiently in complex environments. To improve upon the circle approximation, some researchers have modeled robots or obstacles as ellipses (or ellipses in case of 3D space), which better represent their elongated shapes, despite that their distances cannot be easily computed. Work [17] proposes a conservative distance estimate between ellipsoids, which is shown to be an eigenvalue problem. Study [18] derives a closed-form expression that represents a distance metric of two ellipsoids in 3D space. In [19], robots and obstacles are represented by sets of ellipsoids and a point-world transformation is proposed to transform these ellipsoids to points, simplifying collision avoidance through customized navigation functions in the point world. The proposed transformation has been successfully applied in many other works such as [20]. Furthermore, some works use a mixture of circles and ellipses for shape approximation. This can happen by either approximating the ego robot with a circle and its surrounding robots with ellipses [21, 22] or conversely [23]. Note that only [18, 22] combine Control Barrier Functions. These approaches reduce conservatism compared to the pure circle-based approximation but still cannot fully capture the actual shape of car-like robots. I-B Paper Contributions The main contributions of this work are threefold: 1. We propose a non-differentiable, heading-aware safety margin based on Minimum Translation Vector that considers the headings and geometries of car-like robots, offering a less conservative and more accurate estimation of safe regions for collision avoidance. We train a differentiable neural network to learn it with estimable upper bound on approximation errors. 2. We establish a theorem providing our learning- and Minimum Translation Vector-based safety margin as a high-order Control Barrier Function with relative degree two for a system with two robots modeled by the nonlinear kinematic bicycle model. 3. We validate the theoretical findings through numerical simulations in overtaking and bypassing scenarios involving two car-like robots, demonstrating reduced conservatism compared to traditional Center-to-Center-based approach. Our work appears to be the first work in using Minimum Translation Vector-based safety margin to compute safety distance in Control Barrier Function. I-C Notation A variable x is annotated with a superscript x^{i} if it belongs to robot i. A relative state includes two letters in its superscript to indicate direction, e.g., x^{ji} denotes the relative x-position of robot j w.r.t. robot i. If the relative state is expressed in robot i’s ego perspective rather than in the global coordinate system, an underline is used, e.g., x^{j\underline{i}}. Vectors, such as state vector \bm{x} and control input vector \bm{u}, are bolded, and the dot product of two vectors \bm{a} and \bm{b} is denoted by \bm{a}\cdot\bm{b}. Time arguments of time-variant variables are omitted throughout the paper for simplicity. I-D Paper Structure Section II introduces preliminaries required for this work. Section III proposes our Minimum Translation Vector-based safety margin and its integration with Control Barrier Functions. Section IV discusses experimental results and limitations of our work. Section V draws conclusions and outlines future research directions."
https://arxiv.org/html/2411.08907v1,From Simulators to Digital Twins for Enabling Emerging Cellular Networks: A Tutorial and Survey,"Simulators are indispensable parts of the research and development necessary to advance countless industries, including cellular networks. With simulators, the evaluation, analysis, testing, and experimentation of novel designs and algorithms can be executed in a more cost-effective and convenient manner without the risk of real network service disruption. Additionally, recent trends indicate that the advancement of these Digital System Models (DSM), such as system-level simulators, will hold a pivotal role in advancing cellular networks by facilitating the development of digital twins. Given this growing significance, in this survey and tutorial paper, we present an extensive review of the currently available DSMs for 5G and beyond (5G&B) networks. Specifically, we begin with a tutorial on the fundamental concepts of 5G&B network simulations, followed by an identification of the essential design requirements needed to model the key features of these networks. We also devised a taxonomy of different types of 5G&B network simulators. In contrast to existing simulator surveys, which mostly leverage traditional metrics applicable to legacy networks, we devise and use 5G-specific evaluation metrics that capture three key facets of a network simulator, namely realism, completeness, and computational efficiency. We evaluate each simulator according to the devised metrics to generate an applicability matrix that maps different 5G&B simulators vis-à-vis the different research themes they can potentially enable. We also present the current challenges in developing 5G&B simulators while laying out several potential solutions to address the issues. Finally, we discuss the future challenges related to simulator design provisions that will arise with the emergence of 6G networks.","Computer-aided numerical simulations, or simulators, are used as a first-tier assessment tool to evaluate the diverse features of cellular networks [1]. For instance, cellular network operators traditionally rely on cellular network simulators such as Atoll [2], Planet [3], and Asset [4] to assist in the design, planning, and optimization stages of network rollout. Academic researchers, on the other hand, use simulators such as MATLAB [5], Vienna [6], and ns-3 [7] to design, analyze, and test new protocols, architectures, and features. Compared to alternative strategies such as analytical models, testbeds, and field trials, simulators are more practical when considering factors such as risk to real networks, benefits to industry, utilities to research and development, and resources needed to develop and perform, as summarized in Fig. 1. Compared to analytical modeling, simulators have the ability to generate results even for non-tractable mathematical problems. Although testbeds and field trials can provide a more realistic and practical assessment of wireless networks, simulators pose minimal risk to live cellular networks and require fewer resources to implement and develop. As cellular network technology evolves to enable an expanding number of emerging use cases and as the demand for mobile data grows at an unprecedented magnitude, simulators have become increasingly important for both industry and academia. For instance, simulators are being considered as a promising solution to address the data scarcity challenge in the cellular network domain [8]. However, amidst the increasing complexity and stringent requirements of future cellular networks, simulators must continue to evolve to remain relevant. Figure 1: Spider web diagram of different cellular network evaluation tools using a penta-prong metrics including practicality of the results, utility to industry, utility to academic R&D, risk to the live network, and required resources. Figure 2: Digital system model utilized as digital twin kernel. Recent trends suggest that to provide a more advanced method of network modeling, the cellular network industry is moving towards digital twins (DTs), which are virtual and up-to-date representations of a physical system (i.e., a cellular network) [9, 10]. This emphasis on DTs has gained traction within the cellular network community, particularly in shaping the foundation of 6G networks, as highlighted by several studies [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]. These collective endeavors signify a substantial shift towards leveraging DTs as a fundamental enabler in the evolution towards 6G networks. DTs are increasingly recognized to be valuable assets in addressing diverse challenges encountered within cellular networks, including resource allocation [23], energy efficiency [24], cellular edge networks [25, 26, 15], optimizing communication and computation costs associated with DTs [27] and the deployment of Open Radio Access Network (O-RAN) [19]. As DT closely mirrors a real cellular network, it serves as a reliable platform for developing and benchmarking new research directions accurately. Furthermore, DTs can serve as platforms for network management and optimization, mitigating the risk of service degradation. Leveraging DTs as the testing ground for modifications, such as parameter adjustments, provides a safeguard against potential disruptions, as changes are only implemented in the real network after successful validation within the digital twin. DTs are composed of three unique components: the physical product, the digital or virtual model of the product, and the interconnections represented by the data that reflects the current state of a live system, as shown in Fig. 2. While creating a digital copy of a real system is relatively straightforward for simple systems with low dynamicity, such as static machinery, the process becomes exceptionally challenging when dealing with complex and highly dynamic systems like cellular networks. In such cases, cellular network digital system models (DSMs) play a pivotal role. These DSMs take on a critical function in modeling the intricacies of cellular networks, including different network components (i.e., base station and antenna), radio propagation, protocols, user mobility, traffic patterns, and network performance, to name a few. The concept of evolving DSMs into comprehensive digital twins is exemplified in [28], wherein the authors showcase the potential of a state-of-the-art emulator to transition into a fully realized digital twin. A DSM is a versatile term that can encompass various simulation types, such as system-level or link-level simulators, as well as digital twins, depending on the specific features. For instance, DSM with high fidelity can become a kernel in developing live DT models of cellular networks [10, 29]. The distinction lies in the set of attributes that characterize a DSM as either a simulator or a digital twin. While there are overlaps between DSMs and DTs in terms of their functionalities, several differences exist between the two. These distinctions are summarized in Table I. DSMs, although functional independently, cannot be classified as DTs as they lack connectivity with the real network. The presence of a feedback loop between the digital representation and the real network is one of the major disparities between the two. This connection is pivotal for facilitating seamless data exchange between the DT and the real network. This link is imperative as DTs heavily rely on data-driven models to function [17] unlike simulators which rely more on pre-defined rules and deterministic models. The reliance of DTs on data-driven modeling is crucial for mitigating the complexity of modeling numerous real-time network functionalities. [11]. Consequently, this makes the requirements for efficient predictive modeling capabilities, more stringent in DTs than in simulators. The DTs of 6G wireless systems must rely on efficient AI schemes tailored for handling extensive datasets [9]. This underscores AI’s pivotal role as an enabler of DTs and is expected to be natively integrated into their design, unlike in simulators where its implementation remains optional [18]. Finally, in terms of visualization, DTs demand more advanced capabilities to provide richer representations of physical systems, such as 3D maps, buildings, vegetation, etc. Table I: Distinguishing factors between DSM and DT Digital Twins DSMs (i.e., simulators) Real-time interaction with real network Required Nonexistent Reliance on data-driven models High Low Predictive modeling capabilities High Low AI implementation Native Optional Visualization Advanced Less advanced Figure 3: Summary of available literature on simulator comparison in wireless networks. Inside the circles are the different domains of wireless networks. Inside the connected ellipses are the survey papers available in the literature. The outline of these ellipses represents the year of publication. The figures inside the square represent the number of simulators included in each survey paper. In this survey paper, these attributes are presented using a set of criteria that we call the ""iron triangle"" of cellular networks. The iron triangle encompasses three crucial factors: realism, completeness, and computational efficiency, which together play a critical role in determining whether DSMs can function as simulators or serve as digital twin kernels. By leveraging these criteria, our paper offers valuable insights into the current state of various DSMs and their progress towards becoming digital twin-ready by tackling the challenges posed by the iron triangle. Furthermore, our study aims to raise awareness about the challenges involved in developing these DSMs and explore potential solutions. We also conduct an analysis of future requirements for DSMs to meet the evolving demands of next-generation cellular networks. However, it is essential to clarify that our work does not cover the two-way feedback between the digital model and digital twin, as that aspect falls beyond the scope of this article. Instead, we focus on providing a comprehensive understanding of the key attributes and challenges associated with DSMs, contributing to the broader discourse on digital twin development in the cellular network domain. I-A Related Work Wireless networks (WN) have had a colossal impact on various sectors of human society, including communication, education, defense, security, healthcare, agriculture, and manufacturing, among others. It is a broad area of research and can be broken down into several sub-topics, as shown in Fig. 3. These specific domains include Wireless Sensor Network (WSN), Wireless Mesh Network (WMN), Cellular Network, Mobile Ad-hoc Network (MANET), and Vehicular Ad-hoc Network (VANET), which is a sub-category of MANET. The overwhelming demand to advance wireless communication has led to the development of several simulators that perform a broad array of functionalities. There are several comparative studies on simulators for WNs in the literature. Fig. 3 shows a summary of the related literature we assimilate in this survey paper. Currently, several surveys and comparative studies are focused on WN in general[30, 31, 32, 33, 34, 35]. Meanwhile, other works also exist highlighting the simulators for specific sub-categories of WN. For instance, [36, 37, 38, 39, 40, 41, 42] are surveys on simulators used for MANETs,[43, 44] are works dedicated to VANET simulators, while [45] confined the simulator discussions for WMN. Moreover, the bulk of the papers on simulator surveys and comparative studies are concentrated on WSN, with 11 papers as of this writing [46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56]. While some of these survey papers are comprehensive, including [50, 51, 49, 32, 30, 37, 41] wherein the authors evaluated 10 or more simulators, they can be deemed outdated with regards to emerging cellular networks as most were published between 2008 and 2015. Unlike other branches of WN, such as WSN, WMN, and MANETS, review papers dedicated to simulators pertinent to emerging cellular networks are scarce, as shown in Fig. 3. A review paper that is most related to the scope of our paper is [57], which offers a comparative study on simulators specific to 4G and 5G cellular networks. However, this review paper has a very limited scope as it considers only four simulators, namely, ns-3, OMNeT++, Riverbed Modeler, and NetSim. While there are other review papers available on cellular network simulators, such as those in [58, 59, 60, 61], they primarily concentrate on the specifications of 5G simulators rather than their evaluation. These papers discuss various requirements for simulators, such as reusability, scalability, flexibility, multiple levels of abstraction, parallel processing, and the integration of link-, system-, an d network-level simulators. However, it is worth noting that most of these requirements are not exclusive to 5G networks, and they provide only a cursory assessment of the challenges in developing 5G simulators. While [61] discusses the challenges associated with developing 5G simulators, the study is not comprehensive and does not offer insights into overcoming the challenges inherent in developing 5G&B simulators. Along with the aforementioned articles on cellular network simulators, there exist publications that delve into simulator development [1, 62, 63]. In such works, authors not only expound on the features and capabilities of their respective simulators but also perform comparative analyses with other 5G simulators, highlighting the strengths and advantages of their own tools. For instance, the authors in [62] compared their simulator with ten others with respect to supported features such as massive multi-input multi-output (MIMO), flexible numerology, random access procedure, millimeter wave (mmWave) propagation, and general information such as programming language and license type. Similarly, authors in [1] developed a simulator and compared it to existing simulators using almost similar metrics as [62] with the inclusion of cloud computing capabilities. By discussing its primary features, the authors in [63] juxtaposed the simulator they developed with six other simulators. Table II provides a comparative summary of the existing literature in tabular form, highlighting the distinctions between our work and other studies in this domain. Notably, our study stands out for its comprehensive analysis, which encompasses the broadest range of simulators, including industry-grade tools. Furthermore, our work is distinguished by the diversity of analyses performed, particularly our unique focus on how simulators can facilitate digital twin generation. Unlike previous studies that relied solely on traditional evaluation metrics, our research introduces new metrics, further setting it apart from existing literature. I-B Contributions The examination of the available literature on WN simulators reveals that, in comparison to other domains, cellular network simulators lag in the availability of review papers. Meanwhile, those that are now available are either insufficiently comprehensive, have limited discussion of simulator development challenges and potential solutions, or do not use 5G-specific evaluation measures. Given the shortcomings of the current relevant work on cellular network simulators, there is a dire need for a comprehensive survey that focuses on 5G&B network simulators. In summary, the main contributions of this work are as follows: 1. This paper provides a comprehensive survey of the current simulators for 5G&B networks. In contrast to current literature, which concentrates on general-purpose simulators (shown in Fig. 4), this survey covers simulators targeted for specific 5G use cases. Our discussion includes over 35 link-level, system-level, and network-level simulators. Moreover, this survey is the first to discuss current state-of-the-art, industry-grade commercial simulators. The presented analysis can assist researchers in selecting the proper simulation tool for their needs. 2. We provide potential strategies to reduce the complexity of simulator development by investigating simulator design requirements in light of the nuances and peculiarities of 5G&B communication. We also present several research topics that can be investigated in greater depth after these design requirements are met. 3. We present a new and insightful evaluation metric tailored specifically for 5G&B networks, aimed at discerning the operational status, degree of implementation fidelity, and underlying assumptions inherent to each simulator. This metric serves as a decisive benchmark, enabling the categorization of these DSMs into either potential candidates for enabling DTs or retaining their status as conventional simulators. In contrast to conventional and generic assessment methods, which often involve comparisons based on criteria such as graphical user interface, language platform, licensing model, and modularity, as depicted in Figure 4, our innovative metric focuses on a comprehensive evaluation framework composed of three pivotal criteria: realism, comprehensiveness, and computational efficiency. This refined metric is instrumental in not only gauging the capabilities of each simulator but also probing their applicability across different fields of research in 5G&B. 4. Simulators meeting the three mentioned criteria can be regarded as possessing high fidelity and good quality, however, they cannot be classified as DTs. Another essential aspect that must be present is the connection between the DSM and the real network. While this interconnection is beyond the scope of this paper, existing literature provides insights into what can be communicated through this link. Hence, we introduce a framework outlining the necessary steps to utilize DSM as a core component for generating DTs. This framework offers readers insights into the required information and the role of DSM in DT generation, while also emphasizing the significance of the three metrics. 5. We outline the challenges that may impede the development of realistic, comprehensive, and computationally efficient DSMs for 5G&B networks, preventing them from becoming DT kernels. This analysis is guided by insights from 5G&B network requirements, use cases, and enablers. It is essential to recognize these challenges in order to accelerate research and development toward more accurate and reliable DSMs and transform them into operational DTs rather than just conventional simulators. Furthermore, we identify potential approaches to addressing these challenges based on insights gleaned from academic literature and industrial practice. Finally, based on an array of related articles projecting what 6G will look like, we highlight the upcoming issues of designing 6G-specific DSMs. Table II: A Comparison of Existing Simulator Survey Papers Related Work Wireless Technology Number of Simulators Evaluated Year of Publication Comparison Metrics Used Development Requirements Development Challenges and Solutions Type of Simulators (Applicable only to CN Simulators) Digital Twin Considered Traditional New Link Level System Level Industry-grade NI Sarkar et al. [30] WN 10 2011 ✓ ✗ ✓ ✗ N/A N/A N/A ✗ S. Gupta et al. [35] WN 4 2013 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ S. Siraj et al. [33] WN 8 2012 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ MH Kabir et al. [32] WN 14 2014 ✓ ✗ ✓ ✗ N/A N/A N/A ✗ L. Ronit et al. [34] WN 7 2018 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ AS Toor et al. [31] WN 6 2017 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ H Sundani et al. [51] WSN 14 2011 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ C.P. Singh et al. [49] WSN 13 2008 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ M.Imran et al. [46] WSN 8 2010 ✓ ✗ ✓ ✗ N/A N/A N/A ✗ F. Yu et al. [53] WSN 7 2011 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ M.Korkalainen et al. [47] WSN 5 2009 ✓ ✗ ✓ ✗ N/A N/A N/A ✗ A. Stetsko et al. [56] WSN 4 2011 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ J.Lessmann et al. [48] WSN 4 2008 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ M Jevtic et al. [52] WSN 4 2009 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ B Musznicki et al. [50] WSN 28 2012 ✓ ✗ ✓ ✗ N/A N/A N/A ✗ M. A. Khan et al. [55] WSN 5 2014 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ S Yadav et al. [54] WSN 8 2012 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ L Hogie et al.[37] MANET 11 2006 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ MN Jambli et al.[41] MANET 15 2012 ✓ ✗ ✓ ✗ N/A N/A N/A ✗ J Malhotra et al. [39] MANET 8 2014 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ G. Chengetanai et al. [40] MANET 7 2015 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ A. Kumar et al. [36] MANET 6 2012 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ N. Bhatt et al. [42] MANET 4 2013 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ I. Dorathy et al. [38] MANET 8 2018 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ E Spaho et al. [43] VANET 5 2011 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ FJ Martinez et al. [44] VANET 4 2011 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ P Owczarek et al. [45] WMN 6 2014 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ C. Bouras et al. [57] CN 4 2020 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ P. K. Gkonis et al. [58] CN 8 2020 ✗ ✗ ✗ ✓ ✓ ✓ ✗ ✗ Y. Wang et al. [60] CN ✗ 2014 ✗ ✗ ✗ ✓ ✗ ✓ ✗ ✗ S. Cho et al. [61] CN ✗ 2017 ✗ ✗ ✓ ✓ ✓ ✓ ✗ ✗ S. M. A. Zaidi et al. [1] CN 14 2020 ✓ ✗ ✗ ✗ ✓ ✓ ✗ ✗ S. Martiradonna et. al [62] CN 11 2020 ✓ ✗ ✓ ✗ ✓ ✓ ✗ ✗ R. I. Tinini et al. [63] CN 7 2020 ✓ ✗ ✗ ✗ ✓ ✓ ✗ ✗ This survey & tutorial paper CN 35 TBD ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Figure 4: List of the simulators evaluated and comparison metrics used in relevant literature. I-C Paper Organization The structure of the paper, visualized in Fig. 5, is organized as follows: Section I provides the introduction and relevant work. Section II presents a tutorial focusing on the roles, strengths, and limitations of wireless network simulators. This section also includes an overview of the different types of simulators used in 5G&B networks, such as link-, system-, and network-level simulators, as well as the interplay between these simulators. In Section III, we discuss the key components of the 5G&B network and the impact of these components on the design requirements of the simulators. Section IV presents a taxonomy of the different types of simulators and their evaluation using traditional metrics. This section also includes a brief discussion of around 35 simulators that can be leveraged by both academia and industry for research, network planning, and optimization. Figure 5: Structure of the paper. We narrow down the discussion to DSMs consisting of system-level simulators and evaluate them in Section V. In this section, we present the new insightful metrics for simulator evaluation based on three factors: realism, completeness, and computational complexity. Section VI deals with the open challenges in the development of DSMs capable of functioning as DT kernels. Additionally, this section also presents several potential solutions to address the challenges. In Section VII, we extend the discussion of the challenges specific to 6G. Finally, Section VIII concludes the paper. To provide assistance to the readers, we provide a list of the acronyms used in this paper in Table III. Table III: List of Acronyms Acronym Description 3GPP 3rd Generation Partnership Project 4G Fourth Generation 5G Fifth Generation 5G&B Fifth Generation and Beyond 5GC 5G Core 5G PPP 5G Infrastructure Public Private Partnership AI Artificial Intelligence AMF Access and Mobility Management Function BLER Block Error Rate BS Base Station C-RAN Cloud Radio Access Network CAPEX Capital Expenditures CQI Channel Quality Indicator DSM Digital System Model EPC Evolved Packet Core FBMC Filter Bank Multicarrier FSPL Free Space Pathloss GIS Geographic Information Systems GW Gateway HARQ Hybrid Automatic Repeat Request HetNet Heterogeneous Network I2I Indoor–to-indoor InH Indoor-Hotspot IoT Internet of Things KPI Key Performance Indicator LOS Line-of-Sight LTE Long Term Evolution LUT Look-up Tables MAC Medium Access Control MANET Mobile Ad-hoc Network MCS Modulation and Coding Scheme ML Machine Learning MME Mobility Management Entity MU-MIMO Multi-user Multiple Input Multiple Output NFV Network Function Virtualization NLOS Non Line-of-Sight NR New Radio NSA Non-standalone O2I Outdoor–to-indoor O2O Outdoor–to-outdoor OFDM Orthogonal Frequency-division Multiplexing OPEX Operational Expenditures PDCP Packet Data Convergence Protocol PHY Physical Layer RAT Radio Access Technology RAN Radio Access Network RLC Radio Link Control RMa Rural Macro RRC Radio Resource Control RW Random Walk RWP Randon Waypoint SDN Software Defined Networking SINR Signal-to-Interference-plus-Noise Ratio SLAW Self-similar Least Action Walk SMa Suburban Macro SMF Session Management Function SA Standalone SUMO Simulation of Urban Mobility TIA Telecommunications Industry Association UAV Unmanned Aerial Vehicles UFe Urban Femto UMa Urban Macro UMi Urban Micro UPF User Plane Function VANET Vehicular Ad-hoc Network VoIP Voice Over IP WMN Wireless Mesh Network WSN Wireless Sensor Network ZF Zero Forcing"
https://arxiv.org/html/2411.08761v1,AI-Enhanced Inverter Fault and Anomaly Detection System for Distributed Energy Resources in Microgrids,"The integration of Distributed Energy Resources (DERs) into power distribution systems has made microgrids foundational to grid modernization. These DERs, connected through power electronic inverters, create a power electronics-dominated grid architecture, introducing unique challenges for fault detection. While external line faults are widely studied, inverter faults remain a critical yet underexplored issue. This paper proposes various data mining techniques for the effective detection and localization of inverter faults—essential for preventing catastrophic grid failures. Furthermore, the difficulty of differentiating between system anomalies and internal inverter faults within Power Electronics-Driven Grids (PEDGs) is addressed. To enhance grid resilience, this work applies advanced artificial intelligence methods to distinguish anomalies from true internal faults, identifying the specific malfunctioning switch. The proposed FaultNet-ML methodology is validated on a 9-bus system dominated by inverters, illustrating its robustness in a PEDG environment.","Today’s power systems are evolving with a focus on sustainability, efficiency, and resilience, driven largely by the development of microgrids and the advancement of inverter-based resources (IBRs). IBRs bring notable improvements in operational flexibility, reliability, and minimize energy loss; however, their integration introduces challenges across several critical dimensions. Managing diverse sources in PEDG systems, such as solar, wind, and battery storages that requires coordinated control to ensure balanced power distribution and system stability. Furthermore, PEDG systems rely on smart grid integration and extensive real-time data sharing, raising privacy concerns related to sensitive user information. Stability, planning, and resilience are also crucial considerations: integrating sources with varying dynamic behaviors risks fluctuations in voltage and frequency, which can potentially damage equipment. Strategic planning is required to meet growing demand while efficiently allocating resources. Additionally, as digital control becomes more prevalent, PEDG systems are increasingly vulnerable to cyber-attacks that could disrupt operations and impact system functionality. Inverter components within PEDG systems, particularly in renewable environments, face high levels of stress, making them susceptible to faults. Generally inverter faults include switch short circuits and open-switch faults. Short circuits arise from overvoltages or abnormal signals, causing large, rapid currents that may lead to open-switch faults if mitigated. Open-switch faults, caused by issues such as bond wire lifting and gate drive failures, distort three-phase currents and voltages, creating harmonic losses and stressing other components, ultimately compromising power quality. This vulnerability underscores the need for rapid, efficient fault detection and localization, especially as smart grid integration exposes inverter control to cyber threats. Recent advancements in big data processing now enable AI-based solutions that can effectively distinguish between internal faults and anomalous operations, making them essential for addressing these complex challenges. There has been limited research on inverter fault diagnosis methods, especially within extensive microgrid environments. Previous works, such as those by [1] and [2], have introduced open-circuit fault diagnosis in motor-driven systems, while [3] investigated open-circuit fault detection in a 7-level hybrid active neutral point clamped (7L-ANPC) multilevel inverter for the first time. More recent studies [4, 5, 6] have made advancements by applying data-driven approaches to diagnose open-switch faults. However, none of these studies have examined fault diagnosis in a fully integrated microgrid framework, where inverters are tested in interconnected settings with other loads and communication channels. In recent studies, various fault detection approaches have been proposed for microgrid systems, each focusing on different fault types, detection mechanisms, and implementation techniques. This paper addresses these gaps by proposing a machine-learning-based method for inverter fault detection and localization within a 9-bus microgrid system, including integrated Battery Energy Storage Systems (BESS) and solar PV units. Data collected from this microgrid configuration was used to train and validate machine learning models, including K-Nearest Neighbors (KNN) and other classification algorithms. This methodology facilitates rapid and accurate detection of inverter faults, enabling timely corrective action to ensure clean sinusoidal output for local and critical loads. The proposed approach can identify single and multiple switch faults in a three-phase three-leg voltage source inverter, as well as detect anomalies like false data injection scenarios—an essential consideration in the context of real-time grid operation. The following key contributions encapsulate the novelty and importance of this research: • The paper introduces a comprehensive approach to classify inverter faults within complex microgrid systems, enabling a detailed understanding of fault types and their operational impacts. • The proposed method differentiates between anomalies and hardware malfunctions, particularly in gate drive units. This distinction is critical for targeted responses, enhancing system security and reliability • Upon identifying a malfunction, the methodology accurately localizes the specific inverter fault, distinguishing between single-switch and multi-switch faults, facilitating efficient troubleshooting and maintenance. • Various data-driven models were evaluated for their performance in fault classification, leading to the selection of the most accurate model. This evaluation ensures the proposed approach is robust and reliable for practical applications. The remainder of this paper is structured as follows: Section II elaborates on the architecture of the microgrid framework and the fault classification approach. Section IV contains the simulation results and discussion, and Section V is the conclusion followed by acknowledgment as in section VI."
https://arxiv.org/html/2411.08754v1,Logic-based Knowledge Awareness for Autonomous Agents in Continuous Spaces,"This paper presents a step towards a formal controller design method for autonomous agents based on knowledge awareness to improve decision-making. Our approach is to first create an organized repository of information (a knowledge base) for autonomous agents which can be accessed and then translated into temporal specifications. Secondly, to develop a controller with formal guarantees that meets a combination of mission-specific objective and the specification from the knowledge base, we utilize an abstraction-based controller design (ABCD) approach, capable of managing both nonlinear dynamics and temporal requirements. Unlike the conventional offline ABCD approach, our method dynamically updates the controller whenever the knowledge base prompts changes in the specifications. A three-dimensional nonlinear car model navigating an urban road scenario with traffic signs and obstacles is considered for validation. Results show the effectiveness of the method in guiding the autonomous agents to the target while complying with the knowledge base and the mission-specific objective.","Autonomous agents are becoming popular in various application areas, including road driving, warehouse exploration, and space exploration [1]. The agents in these systems use sensors like cameras, lidar, radar, and GPS to detect and perceive their surroundings. The data from these sensors, along with the map information, must be represented in a way that the autonomous agents can comprehend. Therefore, The main challenges in these systems are (1) to form a formal specification by integrating the sensory data with the existing knowledge, and (2) to synthesize a controller that can provide formal guarantee over satisfaction of the specification. Our approach to address the first challenge is to create an organized repository that would enable autonomous agents to efficiently access available information, and then translate the information into the form of a temporal specification. In this context, we explore the field of knowledge representation techniques which support the construction of a knowledge base (a centralized or decentralized repository) that systematically stores information [2]. Knowledge awareness, on the other hand, refers to the ability of an intelligent system to understand, interpret, and utilize the information contained within the knowledge base [2, 3]. For instance, in a single-agent environment, such as an autonomous vehicle navigating a road with obstacles, the first step is to build a knowledge base. This knowledge base includes information on traffic regulations, the environmental map, operation limits of the vehicle, and any restrictions, all organized using appropriate knowledge representation techniques. In addition to the knowledge base, the autonomous agent must adhere to its mission-specific objective, defined as a reach-avoid specification that directs the actions of the agents. The integration of this mission-specific objective, the knowledge base, and sensory data creates a composite specification that directs the behavior of the agent in real time. While the reach-avoid specification and knowledge base remain fixed throughout a particular mission, this composite specification can be updated as new sensory data is received, allowing the agent to respond effectively to its environment by updating its control policy. Knowledge representation techniques have been extensively studied in the existing literature, with methods like predicate logic, frames, semantic networks, and production rules commonly employed [4, 5]. Foundational work has also explored logical, philosophical, and computational aspects [6] of knowledge representation. More recently, research has focused on description logics (DLs) as a framework for structured knowledge representation and reasoning, particularly useful in fields involving complex information and decision-making [7]. Within DLs, attributed concept language (\mathcal{ALC}) plays a central role as a fundamental subset and was initially introduced in [8]. \mathcal{ALC} provides a balance between expressive power and computational manageability [9]. However, \mathcal{ALC} cannot represent temporal properties because it lacks inherent support for temporal operators. This limitation makes \mathcal{ALC} unsuitable for applications that involve time-varying features [10, 11]. To capture time-dependent behaviors, a combination of \mathcal{ALC} with linear temporal logic (\mathcal{ALC}-LTL) is used to create a knowledge framework for autonomous agents, capitalizing on its strengths in efficiently representing structured knowledge [10]. In this work, we create a knowledge base as a organized repository using \mathcal{ALC}-LTL. The pictorial representation of our proposed scheme is shown in Fig. 1. At each time point t\in\mathbb{R}, during the operation of the autonomous agent, real-time sensory data are utilized to extract the relevant subset of information from the knowledge base, forming a temporal specification \psi_{kb}(t). This specification serves as a foundation for introducing knowledge awareness. In addition, the agent has a mission-specific objective \psi_{obj}, also represented in LTL. These two specifications are then combined to form a composite time-varying specification \psi_{comp}(t) that must be followed by the autonomous agent. Once this composite specification is established, we focus on the second challenge of synthesize a control input that ensures formal guarantees regarding its satisfaction. To achieve this, we consider agents whose dynamics are described using nonlinear set of differential equations with bounded disturbances. Formal controller design for continuous dynamical systems is challenging due to their continuous state space. One promising approach to tackle the formal controller synthesis problem for nonlinear dynamical systems is abstraction-based controller design (ABCD) [12, 13, 14, 15, 16, 17, 18]. ABCD schemes construct a finite abstraction of a dynamical system that has continuous state and input spaces, and solve a two-player graph game on the abstraction. Standard ABCD is used in an offline manner, that is the controller is synthesized with respect to a given static specification. However, this paper studies the use of ABCD in a setting wherein the composite specification \psi_{comp}(t) is subject to change. Therefore, the controller needs to be updated whenever there is a change in the composite specification. Figure 1: The overall scheme of the proposed approach. The decision-making process relies on three distinct inputs: the LTL formula derived from the knowledge base, the mission-specific objective of the autonomous agent, and the system dynamics. Motivating example. To illustrate how knowledge awareness can improve decision-making for autonomous agents, we consider a motivating example throughout this paper. This example as shown in Fig. 2, serves both as an illustration and eventually as an implemented case study. To this end, we consider an autonomous vehicle as an agent that aims to navigate from the start (initial) location to the target location, while avoiding the obstacles and respecting the traffic rules set by the traffic signs existing on the map. It is noted that the shortest path passes through the no-entry street signs, and hence must be prohibited. The autonomous vehicle begins without the knowledge of the locations of the no-entry sign, so it must navigate the map to determine a path from the start to the target, while avoiding obstacles and no-entry areas. Figure 2: A motivating example of the workspace consisting of an autonomous car, traffic signals, a target destination, and obstacles. Contributions. In this paper, we aim to design a controller integrated with a knowledge base for a nonlinear dynamical system, that provides formal correctness guarantees, and is guided by a mission-specific objective. We translate the knowledge base represented by description logic (DL) statements [19] into a linear temporal logic (LTL) [20] formula to serve as a specification. The combination of the mission specific objective and the specification from the knowledge base forms a composite specification, which is time-varying. This composite specification is then integrated with the controller synthesis process, which results in an adaptive control policy. For controller synthesis, we employ an abstraction-based controller design (ABCD) method, which can algorithmically construct a controller with formal correctness guarantees for systems with nonlinear dynamics, bounded disturbances, and general temporal specifications. As a case study, we consider a nonlinear 3-dimensional dynamical car model for which we use ABCD to synthesize a controller that fulfils the composite specification. Thus, our decision-making process relies on three distinct inputs: the LTL formula derived from the knowledge base, the mission-specific objective of the autonomous agent, and the dynamics of the autonomous agent. The overall scheme of the proposed method is shown in Fig. 1. Outline. The remainder of the paper is structured as follows, section II gives the description of the proposed scheme along with the necessary definitions, section III discusses the knowledge awareness formulation for autonomous driving case study, section IV presents the considered example, specification, and the formal synthesis method employed, along with the results. Concluding remarks with future directions are provided in section V."
https://arxiv.org/html/2411.08734v1,Recommender systems and reinforcement learning for building control and occupant interaction: A text-mining driven review of scientific literature,"The indoor environment significantly impacts human health and well-being; enhancing health and reducing energy consumption in these settings is a central research focus. With the advancement of Information and Communication Technology (ICT), recommendation systems and reinforcement learning have emerged as promising approaches to induce behavioral changes to improve the indoor environment and energy efficiency of buildings. This study aims to employ text-mining and Natural Language Processing (NLP) techniques to thoroughly examine the connections among these approaches in the context of building control and occupant interaction. The study analyzed around 27,000 articles from the ScienceDirect database, revealing extensive use of recommendation systems and reinforcement learning for space optimization, location recommendations, and personalized control suggestions. Although these systems are broadly applied to specific content, their use in optimizing indoor environments and energy efficiency remains limited. This gap likely arises from the need for interdisciplinary knowledge and extensive sensor data. Traditional recommendation algorithms, including collaborative filtering, content-based and knowledge-based methods, are commonly employed. However, the more complex challenges of optimizing indoor conditions and energy efficiency often depend on sophisticated machine learning (ML) techniques like reinforcement and deep learning. Furthermore, this review underscores the vast potential for expanding recommender systems and reinforcement learning applications in buildings and indoor environments. Fields ripe for innovation include predictive maintenance, building-related product recommendation, and optimization of environments tailored for specific needs, such as sleep and productivity enhancements based on user feedback. The study also notes the limitations of the method in capturing subtle academic nuances. Future improvements could involve integrating and fine-tuning pre-trained language models to better interpret complex texts.","1.1 Background The indoor environment significantly affects human health and well-being, with individuals spending on average approximately 86% of their day indoors [1, 2]. Addressing how to improve health and well-being in indoor environments while reducing energy consumption is a central question in contemporary research [3]. In recent years, smart building control has become an important topic in this regard, with extensive studies demonstrating its ability to significantly reduce energy usage while maintaining indoor comfort levels [4, 5, 6, 7]. However, despite these promising results, the adoption rate of smart control technologies remains relatively low [8]. The primary barriers to widespread adoption are the high initial costs and complexity of implementation [9]. Therefore, there remains a need to explore a more feasible, user-friendly, and cost-effective solution. In the context of the widespread adoption of Artificial Intelligence (AI), the Internet of Things (IoT) devices, and smart mobile devices, a viable approach is the behavior change facilitated by Information and Communication Technology (ICT) [10, 11]. This method leverages recommendation systems and smart devices to influence and change human behavior, thus improving indoor environmental quality and reducing energy consumption [10]. Central to this strategy is sophisticated recommendation algorithms that analyze user preferences and environmental data to provide personalized suggestions. These algorithms have shown success in various sectors, including mobile health, commonly referred to as Just-In-Time Adaptive Interventions (JITAI) in the domain [12], as well as in online shopping [13], entertainment [14, 15], and social networking [16], etc. In the field of building and built environments, several conventional literature reviews have been conducted recently. These reviews analyze current studies on recommendation systems in various areas, including building performance optimization [17, 18, 19], IoT [20, 21, 22], and smart cities [23, 24, 25, 26]. These systems are typically utilized to optimize indoor environments [27, 28] and enhance building energy efficiency [29, 30, 31, 32]. Predominantly, the methods involve leveraging real-time data and federated learning to train models that generate energy-saving suggestions [33, 34, 35], alongside integrating sensor data with user preference feedback to provide personalized recommendations [36, 37]. Furthermore, reinforcement learning-based recommender systems are widely utilized in this domain, due to their ability to handle multi-objective optimization tasks [38, 39]. For example, multi-agent deep reinforcement learning has been developed to optimize energy consumption, occupant comfort, and air quality simultaneously in commercial buildings [40]. These systems are typically reactive, meaning they respond to user inputs or environmental sensors to provide suggestions. However, proactive recommendation systems have also been explored, which aim to incrementally alter user habits during specific moments based on the theories of micro-moments and habit loops [41]. Additionally, due to the opacity of machine learning (ML) black box models, explainable AI has been used to provide explainable and personalized energy efficiency suggestions [42]. On an urban scale, the application of recommendation systems has also garnered interest, primarily to improve energy efficiency in smart grids [43, 44] and facilitate demand response initiatives [45]. These reviews cover the diverse applications of recommendation systems in various aspects of the built environment. However, due to the limitations of human analytical capacity, conventional literature reviews may not adequately handle the vast volume of publications, struggling to discern the subtle relationships among different studies, particularly in interdisciplinary fields [46, 47]. Typically, these reviews are restricted to analyzing between 80-100 publications. Given the complexity and interdisciplinarity of integrating recommendation systems within the built environment, a more sophisticated approach is essential to efficiently manage and interpret extensive literature datasets. To address these challenges, text mining-based literature review methods emerge as a key tool. 1.2 Text mining-based literature review Text mining-based literature reviews utilize data-driven technologies to systematically extract and analyze information from a large corpus of text, which is particularly advantageous in fields characterized by rapid technological advancements and extensive academic output [48]. Using natural language processing (NLP) and ML algorithms, text mining can efficiently process text data, including classification, clustering, and association, to identify trends, patterns, and emerging themes that might not be evident through traditional review methods [47, 49]. Previous applications of text mining in literature reviews have demonstrated promising results. Tools such as VOSviewer [50] are commonly used in bibliometric analysis and network visualizations, using paper metadata to create bibliometric networks and density maps between articles in various fields [51, 52, 53]. However, these tools are often constrained by graphical user interfaces (GUIs), which limit the user’s ability to modify or extend beyond the provided algorithms [47]. To address specific text mining needs, researchers have adopted advanced ML techniques such as topic modeling. For example, Latent Dirichlet Allocation (LDA) [54] and the Correlated Topic Model (CTM) [55] have been effectively used to model documents as mixtures of topics [56, 57, 58]. Similarly, [59] employs the BERTopic model [60] to efficiently extract and cluster topics, thus providing deeper insights into textual data. Recent advances in computational power and algorithmic sophistication have also led to the emergence of Large Language Models (LLMs), such as GPT [61] and LLaMA [62], offering an alternative to traditional topic modeling approaches [63]. Furthermore, word embedding technologies such as Word2Vec [64, 65] and GloVe [66] facilitate character-level analysis [47]. These are complemented by tools such as Python and Scikit-Learn [67], RapidMiner Studio [68], N-gram [69], and the Natural Language Toolkit (NLTK) [70] for pre or post-processing, offering a nuanced understanding of word relationships and topic significance [71, 72]. 1.3 Review of recommender systems for the built environment using text-mining This study aims to overcome the challenges and limitations in typical literature reviews while comprehensively examining the interrelationships between recommendation systems in the context of building control and occupant interaction. Specifically, this study utilizes data mining techniques proposed by Abdelrahman et al. [47] to explore the relationships among five distinct categories of keywords. These categories include algorithms, types of recommendation systems, input data, interventions/objectives, and platforms, and Figure 1 illustrates the relational network between these categories. This represents the first effort to apply text mining and NLP to review these elements systematically in the context of the built environment and recommendation systems. Figure 1: Overview of the categories of concepts analyzed in this text-mining analysis and their relationships with each other."
https://arxiv.org/html/2411.08678v1,Identification of Power Systems with Droop-Controlled Units Using Neural Ordinary Differential Equations,"In future power systems, the detailed structure and dynamics may not always be fully known. This is due to an increasing number of distributed energy resources, such as photovoltaic generators, battery storage systems, heat pumps and electric vehicles, as well as a shift towards active distribution grids. Obtaining physically-based models for simulation and control synthesis can therefore become challenging. Differential equations, where the right-hand side is represented by a neural network, i.e., neural ordinary differential equations, have a great potential to serve as a data-driven black-box model to overcome this challenge. This paper explores their use in identifying the dynamics of droop-controlled grid-forming units based on inputs and state measurements. In numerical studies, various NODEs structures used with different numerical solvers are trained and evaluated. Moreover, they are compared to the sparse identification of nonlinear dynamics (SINDy) method. The results demonstrate that even though SINDy yields more accurate models, NODEs achieve good prediction performance without prior knowledge about the system’s nonlinearities which SINDy requires to work best.","For a reliable operation of power grids, accurate dynamical models are of great importance. They enable simulations, control synthesis and prediction of future system behaviour. Tradionally, power systems were composed of a relatively small number of large-scale synchronous generators connected by high- or medium-voltage transmission lines, providing power to typically passive loads. In this context, physical-based modeling has been a viable approach. However, with the transition towards low carbon energy systems, this is no longer the case. The increasing share of small-scale distributed energy ressources, as well as the transition from consumer- to prosumer-based active distribution system, result in increasingly complex and partly uncertain dynamics. Unknown system parameters including inaccurate low-voltage grid impedances and dynamics of distributed actors render physically-based modeling impractical. With the wide deployment of sensors in power systems, data-driven methods for characterizing its dynamic behaviour appear promising. These rely dominantly on data and do not require detailed knowledge of the underlying processes or parameters. Modern power systems, especially active distribution grids, are subject to changes due to additions or removals of components as well as degradation, e.g., of storage units units. Data-driven methods could deal with this by retraining models with updated data or transfer learning. Data-driven system identification can be partitioned into gray- and black-box approaches. While the latter solely employ data, the former leverage some physical knowledge and data to identify system dynamics. Gray-box approaches have gained popularity in recent years. \Acppinn, for example, learn solutions to initial value problems based on data and a physical system description with possibly unknown parameters. The first application towards power systems was published in [1], where the rotor angle and frequency, as well as uncertain parameters of a single-machine-infinite-bus setup were learned. The authors of [2] propose a nearly Hamiltonian neural network, implicitely embedding energy conservation laws in the network architecture. By learning the Hamiltonian of the system and using automatic differentiation, the frequency dynamics of a single-machine-infinite-bus steup are identified with focus on fault scenarios. Moreover, SINDy has been used in [3] to learn the frequency and voltage dynamics for a microgrid under disturbances, e.g., load variations, based on state measurements. The authors of [4] use voltage measurements to learn the dynamics of a power grid for load variations and fault scenarios with SINDy. Lastly, the authors of [5] use a gray-box model for grid-forming units, called normal form, to identify the dynamics of a single grid forming inverter under fault conditions. Black-box approaches, on the other hand, rely solely on data. The proposed method in [6], for example, does not require physical knowledge about units or the grid, but makes use of the fact, that power systems follow a graph structure to predict post-fault trajectories online using only historic data. Similarily, the authors of [7] predict state trajectories based on historic data by making use of the Koopman operator. Since the introduction of NODEs in [8], they have been used in multiple fields and applications. One of them is black-box system identification which has been conducted, e.g., in [9] where the authors use them to model the input-output dynamics of different systems. In the power system domain, NODEs have also been employed for example in [10] and [11]. In [10], models are learned from portal measurements in order to create dynamic equivalents of the power system components which enable integrated transient simulation. In [11], post-fault frequency dynamics of grid-connected SGs are identified. It is important to note, that physics informed neural networks, as used in [1], learn solutions to initial value problems which includes the estimation of unknown parameters. Resulting solutions account for specific initial conditions and inputs. Consequently, the learned neural network (NN) can not be used for simulations in different settings [12]. Furthermore, they require detailed knowledge about the underlying differential equations in order to guide the learning process. Similarly, SINDy, as used in [3] and [4], requires good guesses about the nonlinear terms occuring in the system’s differential equations in order to work well. Another limitation of existing approaches, e.g. [10] and [5], is that only single components or dynamic equivalents, but not entire power system dynamics are identified. While purely data-driven approaches are successfully employed in [6, 7], they learn models for time-series prediction based on historic state measurements, rather then identifying continuous-time system dynamics. Finally, the approaches in [11] and [2] are limited to trajectory predictions in fault scenarios and and do not consider control inputs. This results in models that are not suitable for control synthesis and closed-loop simulation. Lastly, most publications provide little insights into what structures work well and what more general conclusions can be drawn. To our knowledge, only [10] conducts a limited, gridsearch-like method to find appropriate NN structures, but does not conduct an extensive hyperparameter optimization. The contributions of this paper are as follows: 1) We apply NODEs to identify the dynamics of an entire power system containing droop-controlled grid-forming units using input and state trajectories. This includes dynamics of the nodes, as well as their nonlinear coupling through the electrical network. We learn the dynamics from nodal measurements, enabling a prediction of voltage and frequency dynamics of the system. 2) While comparable studies focus on predicting trajectories of autonomous dynamical systems under fault scenarios, we aim to learn models that can be used for control synthesis and closed-loop simulation, i.e, input-output systems. 3) We draw a comparison between the performance of NODEs and SINDy for the system at hand and show that, even though SINDy is more accurate, good prediction accuracy can be achieved with NODEs without relying on physical knowledge about the system. 4) We conduct an extensive hyperparameter optimization to investigate the influence of the NN size and the activation functions on the accuracy of the models. We conclude that smaller networks with a continuous activation function are advantageous for the task at hand. 5) We investigate the use of different integration schemes for NODEs and show that using higher-order solvers does not necessarily result in more accurate predictions. The remainder of this paper is structured as follows. First, the power system model is presented. Then, the identification methods, i.e., NODEs and SINDy, are introduced. Consecutively, we describe the numerical experiments and conclude the paper with a discussion of the results and an outlook on future work."
https://arxiv.org/html/2411.08634v1,On the Application of Model Predictive Control to a Weighted Coverage Path Planning Problem,"This paper considers the application of Model Predictive Control (MPC) to a weighted coverage path planning (WCPP) problem. The problem appears in a wide range of practical applications, such as search and rescue (SAR) missions. The basic setup is that one (or multiple) agents can move around a given search space and collect rewards from a given spatial distribution. Unlike an artificial potential field, each reward can only be collected once. In contrast to a Traveling Salesman Problem (TSP), the agent moves in a continuous space. Moreover, he is not obliged to cover all locations and/or may return to previously visited locations. The WCPP problem is tackled by a new Model Predictive Control (MPC) formulation with so-called Coverage Constraints (CCs). It is shown that the solution becomes more effective if the solver is initialized with a TSP-based heuristic. With and without this initialization, the proposed MPC approach clearly outperforms a naive MPC formulation, as demonstrated in a small simulation study.","Many path planning applications in robotics try to find an optimal path while maximizing some sort of reward. The reward is commonly related to the proximity to given reference value, guiding the system via an artificial potential field (AFP), or keeping it away from unsafe areas via barrier functions. Model Predictive Control (MPC) casts this problem into a numerical optimization program, with an objective function and constraints. Over the past years, it has become a standard approach for path planning, due to its natural handling of general reward functions, system dynamics, and input and state constraints. Coverage Path Planning (CPP) aims for the system to cover an entire area of the state space, or as large a part of it as possible. Problem instances appear in many robotic applications, such autonomous lawn mowers, vacuum cleaners, agricultural robots, and arial / underwater drones used for inspection or surveillance. From the perspective of common MPC-based path planning, this means a uniform objective function. However, for CPP, the objective function changes dynamically in the sense that the reward at an already visited position, and some radius around it, drops to zero. Efficient motion patterns or policies are commonly used for CPP, including a boustrophedon (snake-like) path or straight driving with random reflection angles when hitting boundaries [1]. The main principle is, clearly, to minimize any overlaps in the track of the robot. This paper considers the extended problem of Weighted Coverage Path Planning (WCPP). The main difference to the CPP is that the objective function is not uniform, but weighted by a coverage priority. Problem instances of this also appear frequently, e.g., in search and rescue (SAR) or surveillance missions, where the goal is to find or detect a target whose probability of presence on a map is not uniformly distributed. Correspondingly, the motivational example for this paper is an unmanned aerial vehicle (UAV) with the task of finding a missing person in a given area as fast as possible. The coverage priority is indicated by a probability map, which serves as the reward function and represents a belief distribution integrating all available information at the current time, e.g., from sensor measurements, the observation of eye witnesses, or human behavioral models in the given map [2]. For practical applications, a probabilistic model could be used to dynamically update the belief distribution based on all available information, and thus guide the planning of the mission [3]. I-A Existing Literature CPP is about finding a path that covers all specified points or an entire area or region of interest while avoiding obstacles [4]. A comprehensive overview of CPP algorithms for robotic applications is given in [5]. The article covers both classical and heuristic-based algorithms. These two categories include a whole variety of basic approaches, such as AFPs, greedy search and graph search algorithms, and bio-inspired approaches, such as genetic algorithms or ant colony optimization. A key point in the discussion in [5] is that the resulting paths should be as smooth as possible. Namely, the avoidance of sharp turns in the path prevent premature wear of the robot’s components and it increases the efficiency, especially in UAV applications, where it advantageous for the average speed of the UAV to fly straight ahead or in smooth curves. In this spirit, the approach in [6] proposes a path search algorithm using ant colony optimization based on the Lin-Kernighan heuristic, followed by a smoothing step using a customized approach based on Fourier series. The resulting dynamically smooth trajectory is then fed to the UAV, which is operated by an MPC-based controller. Numerical optimization has become increasingly popular for path planning over the recent years, due to the availability of more powerful hardware and increasingly efficient solvers. MPC, in particular, has been successfully employed for collision-free path planning for autonomous road vehicles [7] and for UAVs [8]. MPC has been used in combination with AFPs [9, 10], and also for CPP with obstacles using a mixed-integer linear programming (MILP) formulation [11]. The area is covered by a uniform grid, where each cell defines a single way point. The way points are subsequently represented as discrete decision variables within the optimization problem, where the objective is to cover the maximum number of (equally weighted) way points. Related problems to the CPP include the Traveling Salesman Problem (TSP) and its variants, most notably the Orienteering Problem (OP) [12]. In contrast to the TSP, where all vertices must be visited and the goal is to minimize the traveled distance, the OP is concerned with maximizing the total reward collected within a limited time frame, without necessarily visiting all the vertices [13]. Both of these problems are known to be NP-hard [14]. Yet it is desirable, for many applications, to extend them further by including the dynamics of an agent in the problem formulation [15]. Recently, a mathematical framework to tackle this problem using techniques from optimal control has been proposed [15, 16]. In addition to the dynamics of an agent, this formulation also accounts for the movement of the sensors, i.e., in this case, a camera. However, the approach relies on nonsmooth calculus and considers only discrete regions of interest. I-B Contributions This paper proposes a new MPC approach for the WCPP problem, using coverage constraints (CCs). The proposed MPC formulation works with any agent moving governed by a nonlinear dynamic model and moving in a continuous space with obstacles. The reward function is arbitrary, but in contrast to an AFP, each reward can only be collected once. This is enforced by the use of quadratic constraints. In contrast to the TSP, the agent is not obliged (and may in fact be far from able to) cover all locations within the given prediction horizon. Furthermore, the paper shows that the solution becomes more effective if the MPC solver is initialized with a TSP-based heuristic. The heuristic is based on a set of key points, which are derived from a Gaussian mixture model that is used to approximate the reward function."
https://arxiv.org/html/2411.08618v1,Robust Optimal Power Flow Against Adversarial Attacks: A Tri-Level Optimization Approach,"In power systems, unpredictable events like extreme weather, equipment failures, and cyberattacks present significant challenges to ensuring safety and reliability. Ensuring resilience in the face of these uncertainties is crucial for reliable and efficient operations. This paper presents a tri-level optimization approach for robust power system operations that effectively address worst-case attacks. The first stage focuses on optimizing economic dispatch under normal operating conditions, aiming to minimize generation costs while maintaining the supply-demand balance. The second stage introduces an adversarial attack model, identifying worst-case scenarios that maximize the system’s vulnerability by targeting distributed generation (DG). In the third stage, mitigation strategies are developed using fast-response energy storage systems (ESS) to minimize disruptions caused by these attacks. By integrating economic dispatch, vulnerability assessment, and mitigation into a unified framework, this approach provides a robust solution for enhancing power system resilience and safety against evolving adversarial threats. The approach is validated using the IEEE-33 node distribution system to demonstrate its effectiveness in achieving both cost efficiency and system resilience.","I-A Background Power systems are essential for providing energy to modern society’s daily activities, industries, and services. However, they face increasing adversarial threats that challenge reliable operations. The integration of distributed energy resources (DERs) like wind and solar, though beneficial for sustainability, adds vulnerability due to inherent uncertainty, which complicates system stability and opens potential weaknesses for exploitation [1]. The growing risk of cyberattacks on critical infrastructure further threatens the control, communication, and integrity of power systems [2, 3]. Combined with extreme weather events like hurricanes, these threats pose severe risks of equipment failures and large-scale disruptions to energy delivery [4]. To address adversarial attacks on power systems, methods like security-constrained optimal power flow (OPF) provide foundational optimization approaches [2]. These techniques help develop defensive strategies to maintain system performance under challenging conditions. However, they often assume that attackers lack detailed knowledge of the system, potentially underestimating worst-case scenarios [5]. In reality, attackers with insights into system configurations and vulnerabilities may exploit weaknesses that go beyond the expected range of disruptions considered by traditional methods. I-B Related Work and Our Contributions Robust minimax optimization has been widely explored in power system planning and operations to address worst-case scenarios, proving effective in mitigating extreme weather impacts, renewable variability, load fluctuations, and other uncertainties [6]. For example, [7] developed a min-max framework for resilient distribution planning against natural disasters, and [8] applied it to battery storage systems for congestion management, though their model lacked real-time adaptability. Similarly, [9] proposed a two-stage min-max approach to address adversarial attacks on the EV charging market, impacting power system costs. However, these methods tend to be overly conservative, often leading to excessive reserve allocation [9], and they overlook the differing response times of power system devices, particularly the rapid response of battery systems in mitigating cyberattack impacts. To address the challenges outlined, we introduce a tri-level optimization framework for robust optimal power flow designed to counter adversarial attacks. The primary contributions of this paper are as follows: • We formulate the robust OPF problem as a three-stage process, encompassing (i) hourly economic dispatch scheduling, (ii) assessment of potential worst-case adversarial threats based on the current system state, and (iii) mitigation using fast-response energy storage systems (ESSs). Additionally, the state of charge (SOC) of the ESS is included in the model to optimize renewable energy use for charging, ensuring sufficient power availability for future needs. • We design a novel tri-level optimization structure that sequentially integrates system operation, worst-case adversarial threat assessment, and mitigation strategies. This approach ensures that the mitigation stage can effectively address potential worst-case adversarial threats by targeting the most vulnerable devices and amplifying constraint violations based on real-time system conditions. In this way, system resilience is maximized by neutralizing potential attacks, as typical threats are likely to be weaker than the anticipated worst-case scenarios. Numerical case studies show that worst-case attacks can severely disrupt power systems; however, our mitigation strategy can quickly restore system safety and enhance power system resilience. Stage 1: Distribution System OPF Minimize cost Equation: \min_{x\in\mathcal{X}}\ell_{\mathrm{base}}(x) s.t. f_{\text{base}}(x,v)=0, g_{\text{base}}(x,v)\leq 0 Stage 2: Adversarial Attack Maximize loss Equation: \max_{y\in\mathcal{Y}}\ell_{\mathrm{cont}}(x^{*},y)+\inf[g_{\mathrm{cont}}(x^{% *},y)] s.t. f_{\mathrm{cont}}(x^{*},y)=0 Stage 3: Attack Mitigation Mitigate violations Equation: \min_{z\in\mathcal{Z}}\ell(x^{*},y^{*},z)+\sup[g(x^{*},y^{*},z)] s.t. f(x^{*},y^{*},z)=0, g(x^{*},y^{*},z)\leq 0 Generation DispatchSystem ResilienceWorst-case Attack Figure 1: Methodology overview of the proposed tri-level optimization framework."
https://arxiv.org/html/2411.08413v1,Inference-Aware State Reconstruction for Industrial Metaverse under Synchronous/Asynchronous Short-Packet Transmission,"We consider a real-time state reconstruction system for industrial metaverse. The time-varying physical process states in real space are captured by multiple sensors via wireless links, and then reconstructed in virtual space. In this paper, we use the spatial-temporal correlation of the sensor data of interest to infer the real-time data of the target sensor to reduce the mean squared error (MSE) of reconstruction for industrial metaverse under short-packet transmission (SPT). Both synchronous and asynchronous transmission modes for multiple sensors are considered. It is proved that the average MSE of reconstruction and average block error probability (BLEP) have a positive correlation under inference with synchronous transmission scheme, and they have a negative correlation in some conditions under inference with asynchronous transmission scheme. Also, it is proved that the average MSE of reconstruction with inference can be significantly lower than that without inference, even under weak mean squared spatial correlation (MSSC). In addition, closed-form MSSC thresholds are derived for the superiority regions of the inference with synchronous transmission and inference with asynchronous transmission schemes, respectively. Adaptations of blocklength and time shift of asynchronous transmission are conducted to minimize the average MSE of reconstruction. Simulation results show that the two schemes significantly outperform the no inference case, with an average MSE reduction of more than 50\%.","Emerging industrial metaverse enables the real-time mapping and operation of physical industrial productions into virtual space, enhancing efficiency [2, 3, 4]. For example, the time-varying physical process states in real space are captured by multiple sensors via wireless links and then reconstructed in virtual space by real-time state estimation. The virtual space conducts virtualized operations through a digital platform, e.g., augmented reality (AR), and feeds precise instruction to real space. Accurate real-time state reconstruction is crucial to the effective operation of the industrial metaverse, but challenging due to the time-varying nature of physical processes and the delay and error in transmissions. Mean squared error (MSE) is one of the measures of state reconstruction error [5, 6, 7]. Many data in the industrial metaverse have limited data size (typically 20-250 bytes [8]). Hence, short packet transmission (SPT) is beneficial to reduce transmission delay and MSE of reconstruction [10, 11, 9]. Unlike the conventional transmission assuming infinite blocklength, SPT suffers from an inevitable block error probability (BLEP) due to limited coding capability, impairing the MSE performance of reconstruction [9]. The data in the industrial metaverse generally exhibit spatial-temporal correlation [12]. For example, the temperature sensor data at key positions of the hot blast stove in the steel smelting. Inference by exploiting the spatial-temporal correlation between data can help to improve the MSE performance of reconstruction under SPT. In addition, synchronous and asynchronous are typical transmission modes in the industrial metaverse. Synchronous mode transmits multiple sensors’ data simultaneously over different links, enabling a high probability of successful reception at the receiver. Asynchronous mode allows the sensors to separately transmit with a time shift, providing consecutive fresh data at the receiver. Both synchronous and asynchronous transmission modes have the potential to provide a superior MSE performance of reconstruction in inference. I-A Related Work The MSE of reconstruction with SPT has been investigated in [13, 14, 15, 16]. In [13], the average MSE of reconstruction was analyzed for a short-packet linear time-invariant (LTI) system with retransmissions. In [14], a non-orthogonal multi-access inspired hybrid automatic repeat request scheme was proposed to reduce the average and packet-level MSE of an LTI system with SPT. In [15], MSE and energy efficient joint optimization was studied under stability constraints. The MSE of reconstruction is related to the information freshness, which is measured by the age of information (AoI), defined by the elapsed time since the generation of the latest successfully received data [17]. In [16], transmission scheduling was studied to reduce MSE of reconstruction and AoI simultaneously. However, the work mentioned above on MSE of reconstruction with SPT ignored spatial-temporal correlation between data and did not consider inference. Inference is a promising approach to reducing the MSE of reconstruction, where the real-time data of the target sensor with outdated samples is estimated from the fresh samples of the spatially correlated sensors, reducing the AoI used for estimation. The MSE of reconstruction with inference has been widely investigated for distributed estimation systems, where multiple sensors are deployed at different locations to monitor the physical process states of interest [18, 19, 20, 21, 22, 23, 24, 25, 26]. In [18] and [19], the average MSE of reconstruction with inference was analyzed under orthogonal and coherent multiple-access channels (MAC), respectively. In [20], a correlation-aware adaptive access method was designed. In [21], a collaboration compression framework was proposed to reduce the average MSE of reconstruction. In [22], the optimal node number that achieves the best MSE-AoI tradeoff was derived. The work of [18, 19, 20, 21, 22] assumed no inference error, i.e., perfect spatial correlation between sensor data, which may not be applicable in practical distributed estimation systems. This was considered in [23, 24, 25, 26], where the distance-based spatial correlation model is adopted. In [23], the optimizations of sensor density and update rate were studied with inference for first-come first-served and last-come first-served service queuing models. In [24], an AoI-based transmission scheduling scheme was designed to reduce the average MSE of reconstruction. In [25], the optimal power allocation was studied to minimize the outage probability of MSE of reconstruction. The work in [26] showed that the inference alongside asynchronous transmission achieves a significant reduction of average MSE of reconstruction over the no inference case, with the assumption of no transmission error and delay. The SPT error and inference error have an important impact on the MSE performance of reconstruction. However, all the aforementioned work on MSE of reconstruction with inference [18, 19, 20, 24, 21, 22, 23, 25, 26] considered no SPT error. The relationship between the MSE of reconstruction and SPT error and inference error has not been analyzed for inference in the previous work [18, 19, 20, 24, 21, 22, 23, 25, 26]. Most existing work on MSE of reconstruction with inference has considered the synchronous transmission mode [18, 19, 20, 21, 22, 23, 24, 25]. Very little work has been conducted to study the ability of asynchronous transmission to assist inference and to enhance the MSE performance of reconstruction. In [26], a preliminary study for the inference with asynchronous transmission was presented in a two-source distributed estimation system. The quantitative MSE performance comparison between the inference with synchronous transmission and the inference with asynchronous transmission schemes was not provided in [26], thereby inhibiting the applicable conditions and preferences of the two schemes in the industrial metaverse. In addition, it is noteworthy that a shorter blocklength leads to a lower AoI at the cost of a higher BLEP, which may impair the MSE performance of reconstruction. Also, the length of time shift in the asynchronous transmission affects the tradeoff between the intra-period AoI and the inter-period AoI. In summary, the following open issues on MSE of reconstruction with SPT remain: 1. What is the relationship between the MSE of reconstruction and SPT error and inference error? 2. What are the preference regions of the inference with synchronous transmission and inference with asynchronous transmission schemes? 3. How to adapt the blocklength and time shift to minimize the average MSE of reconstruction? I-B Contributions Motivated by the above open issues, we investigate inference-aware state reconstruction for industrial metaverse under synchronous and asynchronous SPTs. We reveal important findings that the average MSE of reconstruction and average BLEP are positively correlated under inference with synchronous transmission scheme, and are negatively correlated in some conditions under inference with asynchronous transmission scheme. Also, the average MSE of reconstruction with inference can be significantly lower than that without inference, even under weak spatial correlation related to inference error. The main contributions are as follows. • A comprehensive analysis of the relationship between the average MSE of reconstruction and average BLEP and spatial correlation is presented. It is proved that the average MSE of reconstruction is mono-increasing with respect to average BLEP under inference with synchronous transmission scheme. It is decreasing first and then increasing with respect to average BLEP in some conditions under inference with asynchronous transmission scheme. Also, it is proved that the average MSE of reconstruction of both schemes is mono-decreasing with respect to the mean squared spatial correlation (MSSC), defined by the average of the squared spatial correlation between the target sensor and its correlated sensors. Particularly, the average MSE of reconstruction with inference is proved to be significantly lower than that without inference, even under weak MSSC and under weaker than the squared temporal correlation at a transmission period length. Furthermore, the upper and lower bounds of the average MSE of reconstruction with respect to the average BLEP and MSSC are proved and derived in closed form. • Preference regions of the inference with synchronous transmission and inference with asynchronous transmission schemes are analyzed, regarding the average MSE performance of reconstruction. Tight approximations of the MSSC thresholds are respectively derived for the superiority regions of the two schemes, which are shown to be mono-decreasing functions of average BLEP and saturate at low average BLEP. Also, closed-form expressions for the average MSE of reconstruction with respect to the MSSC, average received signal-to-noise ratio (SNR) and transmission period are derived. In general, the inference with synchronous transmission scheme is preferable with relatively weak MSSC, low-medium average received SNR and medium transmission period. The inference with asynchronous transmission scheme is desirable in a relatively strong MSSC, low average received SNR and long transmission period regime. • The blocklength and time shift are adapted to minimize the closed-form average MSE of reconstruction. The optimal blocklength of the inference with synchronous transmission scheme is derived in closed form. A joint time shift and blocklength optimization (JTSBO) algorithm is proposed for the inference with asynchronous transmission scheme, which significantly outperforms the existing approach with time shift optimization only [26]. The optimization results are shown to be near-optimal by simulation, with a much lower complexity than exhaustive search, and with an average MSE reduction of more than 50\% over the no inference case [13, 14, 15, 16]. The rest of the paper is organized as follows. Section II presents the system model. Section III presents the analysis of the average MSE of reconstruction for the inference with synchronous transmission and inference with asynchronous transmission schemes, and the analysis of the relationship between the average MSE of reconstruction and average BLEP and spatial correlation. Section IV presents the analysis of the preference regions of the two schemes in terms of the MSSC. Section V is dedicated to the adaptations of blocklength and time shift for the two schemes based on their average MSE analysis in Section III. Simulation results are given in Section VI, followed by the conclusion in Section VII. Notations: Throughout the paper, {{x}}\sim\mathcal{N}\left({{\text{\text{0}}},\sigma^{\text{2}}}\right) denotes that x is a Gaussian random variable with zero mean and variance of \sigma^{\text{2}}. Q(u)=\int_{u}^{\infty}({\text{\text{1}}}/\sqrt{{\text{2}}\pi}){e^{-{z^{\text{2% }}}/{\text{2}}}}dz is the Q-function. \mathbb{E}[\cdot] denotes expectation. \frac{{\partial f}}{{\partial x}} and \frac{{{\partial^{\text{2}}}f}}{{\partial{x^{\text{2}}}}} denote the first and second derivatives of f with respect to x, respectively. Figure 1: The inference-aware real-time state reconstruction system model in the industrial metaverse."
https://arxiv.org/html/2411.08329v1,Neural Network Certification Informed Power System Transient Stability Preventive Control with Renewable Energy,"Existing machine learning-based surrogate modeling methods for transient stability constrained-optimal power flow (TSC-OPF) lack certifications in the presence of unseen disturbances or uncertainties. This may lead to divergence of TSC-OPF or insecure control strategies. This paper proposes a neural network certification-informed power system transient stability preventive control method considering the impacts of various uncertainty resources, such as errors from measurements, fluctuations in renewable energy sources (RESs) and loads, etc. A deep belief network (DBN) is trained to estimate the transient stability, replacing the time-consuming time-domain simulation-based calculations. Then, DBN is embedded into the iterations of the primal-dual interior-point method to solve TSC-OPF. To guarantee the robustness of the solutions, the neural network verifier \alpha,\beta-CROWN to deal with uncertainties from RESs and loads is proposed. The yielded certification results allow us to further adjust the transient stability safety margin under the iterated TSC-OPF solution process, balancing system security and economics. Numerical results on a modified western South Carolina 500-bus system demonstrate that the proposed method can efficiently and quickly obtain the safety-verified preventive control strategy through RES curtailment and generator dispatch with only a slight increase in cost.","Neural networks (NNs) have achieved great success in various power system applications, such as renewable energy source (RES) forecasting [1], load forecasting [2], and stability estimation [3]. However, the reliability and robustness of NNs have raised concerns, as they are vulnerable to adversarial examples, meaning that imperceptible perturbations of test samples might unexpectedly change the NNs estimations [4, 5, 6]. For practical deployments of NNs in power systems, the inputs to NNs are typically measurements or forecasts. The measurement errors associated with different measurement devices can affect the estimations made by NNs [7, 8]. Moreover, measurement data may be subjected to false data injections and cyber-attacks [9, 10]. The adversarial samples generated in this way can be very close to true values but may lead to opposite NN estimations, such as misjudging an unstable scenario as stable, failing to trigger necessary controls, leading to system failures, blackouts, or even more severe accidents [11, 12]. In addition, the forecasts of RESs or loads have high uncertainty, and control strategies based on deterministic forecasts may be unsafe when executed. Therefore, robust estimation and control considering all perturbations are crucial. There has been some research on the robustness analysis of NNs, which can be categorized into adversarial sample generation, adversarial samples-based NNs’ robustness enhancement, and verification of NNs’ robustness. The core idea for generating adversarial samples is similar, which is to set neighborhoods with different norms based on the samples that need to be verified. The size of the neighborhood depends on the perturbation magnitude of the sample point. The adversarial samples are situated throughout the entire neighborhood, and the size of this neighborhood is determined by the perturbation amplitude of the samples. Then, these adversarial samples are used to retrain the NNs, enhancing their robustness. For example, [13] proposes a state-perturbation-based adversarial example and the false data injection attacks method can carry out attacks stealthy to both conventional bad data detectors and deep learning-based detectors. In [14], a signal-specific method and a universal signal-agnostic method are proposed, which can respectively generate perturbations to misclassify most natural signals with high probability and have a higher transfer rate of black-box attacks. In addition, adversarial training is adopted to defend systems against adversarial attacks in [14]. Robustness verification is divided into two types, one is based on Monte Carlo sampling (MCS) to find the largest neighborhood, where the sample being verified will not be misclassified. The larger neighborhood range corresponds to the higher robustness of the test sample. In [12], the MCS-based method is applied to calculate the Lipschitz constant within a norm ball to approximate the robust index of NNs. Similarly, [15] employs a randomized smoothing algorithm to transform any classifier, which performs well under Gaussian noise, into a new classifier that is certifiably robust against adversarial examples. Given that it is not feasible to precisely calculate the classification probabilities of the base classifier, and it is also challenging to accurately assess the prediction and robustness ability of the smoothed classifier, the MCS is employed for both tasks, ensuring success with arbitrarily high probability. Then, based on sampling or probabilistic methods, it’s impossible to guarantee with 100% certainty that all points within the neighborhood are reliably classified, leaving a risk of misclassification. Another category is analytical methods, for example, [16] calculates the norm distance from samples to the separating affine hyperplane as an index of robustness through the linearization of the model. However, for non-linear and non-convex models, achieving the optimal solution is challenging regardless of linearization. There may exist smaller distances to the separating affine hyperplane that lead to misclassification. [17] uses mixed-integer programming (MIP) to compute the minimum distance to a sample which changes the classification, where the ReLU function is piecewise linearized through binary variables. While MIP has proven effective in verifying the robustness of NNs, particularly those employing piecewise linear activation functions such as ReLU, they encounter considerable obstacles when applied to NNs featuring more complex and general nonlinearities. Moreover, MIP is highly time-consuming for verifying large networks. To rigorously ensure that the robustness within the p-norm ball of the test example is fully verified, and applicable to general activation functions, the CROWN is proposed. CROWN is an efficient bound propagation-based verification algorithm that backpropagates a linear inequality through the network, relaxing activation functions with linear bounds. Additionally, by adaptively selecting the linear approximation when computing certified lower bounds of minimum adversarial distortion, it can improve the certified lower bound [18]. \alpha-CROWN enhances the CROWN verifier by optimizing both intermediate and final layer bounds using variable \alpha, offering superior effectiveness over linear programming (LP) by more efficiently tightening intermediate layer bounds [19]. Then, \beta-CROWN extends the CROWN verifier by integrating ReLU split constraints in branch and bound (BaB) into the bound propagation process to optimize parameter \beta [20]. [21] extends BaB-based verification for non-ReLU and general nonlinear functions, achieving significant improvements in verifying NNs with non-ReLU activation functions, such as Transformer and long short-term memory networks. Furthermore, GPUs can effectively parallelize and accelerate bound propagation in the BaB process for \beta-CROWN and GCP-CROWN [22]. Preventive control aims to prepare the system for potential credible contingencies through generator dispatch and RES curtailment [23]. Transient stability constrained-optimal power flow (TSC-OPF) is widely used to find the optimal operating point under transient stability constraints, but it relies on time-consuming time-domain simulations (TDSs), making it challenging for online applications in large-scale power systems. Several alternatives to TDSs are proposed to accelerate the calculation, such as transient energy function method [24], extended equal-area criterion [25], trajectory sensitivity method [26], etc. Deep learning methods have been applied as surrogate models for TDSs to estimate power system transient stability, achieving significant acceleration from several to tens of seconds to less than 0.01 seconds [27]. Due to measurement or forecast errors, as well as cyber-attacks, the robustness of control strategies is critical. However, there is currently no fast and complete NN verification method for high-dimensional, nonlinear, and non-convex transient stability control problems. This paper proposes an NN robustness certification informed power system transient stability preventive control method. The main contributions are summarized as follows: 1. Deep belief networks (DBNs) are used to replace time-consuming TDSs in power systems transient stability estimation, significantly accelerating this process. Additionally, DBNs are analytically integrated into the primal-dual interior-point method (PDIPM), serving as surrogate models to facilitate solving TSC-OPF. 2. Due to measurement or forecast errors, as well as cyber-attacks, the uncertainty intervals of RESs and loads are constrained within a norm ball. To guarantee the robustness of the control solutions, the neural network verifier \alpha,\beta-CROWN is developed to address uncertainties from RESs and loads. The certification results allow further adjustment of the transient stability safety margin by iterative TSC-OPF process. The proposed method can effectively balance system security and economics while being scalable to large-scale systems. Based on our knowledge, this is also the first work on NN robustness certification-based TSC-OPF. The rest of the paper is organized as follows. Section II presents the TSC-OPF problem. Section III introduces the proposed NN robustness verification-based preventive control framework. Results are presented and analyzed in Section IV, and finally, Section V concludes the paper."
https://arxiv.org/html/2411.08199v1,System-Level Analysis for mm-Wave Full-Duplex Transceivers,"This paper conducts a comprehensive system-level analysis of mm-Wave full-duplex transceivers, focusing on a receiver employing a four-stage self-interference cancellation (SIC) process. The analysis aims to optimize the noise and linearity performance requirements of each transceiver block, ensuring that the self-interference (SI) signal does not compromise the receiver’s error vector magnitude (EVM) for an OFDM 64-QAM signal. Additionally, the necessary SIC for each stage is calculated to establish feasible noise and linearity specifications for a CMOS-based implementation. The resulting specifications are subsequently validated within a MATLAB Simulink environment, confirming the accuracy of the computed requirements for each block.","Full-Duplex wireless systems enable simultaneous transmission and reception over a single antenna, improving system compactness and offering substantial benefits in spectral efficiency, link capacity, and network latency reduction. By allowing both the transmitter (TX) and receiver (RX) to operate concurrently on the same frequency, these systems—referred to as in-band full-duplex (IBFD)—excel in supporting high data rates, particularly in densely populated network environments [1]. This concurrent operation provides significant advantages over traditional duplexing methods like frequency-division duplexing (FDD) and time-division duplexing (TDD). Beyond physical-layer enhancements, full-duplex technology delivers additional performance gains across higher network layers, as discussed in references [1], [2], [3], [4], [5], and [6]. However, a major design challenge is the suppression of the self-interference (SI) signal, which can be orders of magnitude stronger than the desired signal—potentially up to a million times—posing risks of receiver desensitization and severe signal-to-noise ratio (SNR) degradation. To address this, a single cancellation stage is typically insufficient; instead, a multi-stage self-interference cancellation approach is required, each stage providing targeted SI attenuation to meet system requirements. A detailed link-budget analysis is therefore essential, enabling the design of receiver components with realistic noise and linearity specifications that ensure the SI level is minimized to achieve the desired SNR. Furthermore, achieving high noise and linearity performance is challenging in CMOS technologies at mm-wave frequencies, necessitating meticulous optimizations to extract sensible values for each SIC stage and each block’s specifications. In this regard, this work presents a detailed system-level analysis of a mm-wave FD transceiver verified by modulation-based simulations in Simulink environment."
https://arxiv.org/html/2411.08190v1,Collision-Free Multi-Agent Coverage Control for Non-Cooperating Swarms: Preliminary Results,"The main contribution of this paper is a methodology for multiple non-cooperating swarms of unmanned aerial vehicles to independently cover a common area. In contrast to previous research on coverage control involving more than one swarm, this paper does not assume cooperation between distinct groups but considers them as entirely independent units following their own objectives. Using Voronoi tesselation, collision-free motion of agents within the same swarm has been proved before. However, as is shown in Example 1 of this paper, in the case of multiple swarms with inter-swarm but without intra-swarm collaboration, these guarantees do not hold. We address this issue by proposing an algorithm to achieve maximum coverage with multiple swarms while avoiding collisions between agents. Thus, the Optimal Reciprocal Collision Avoidance method used for safe navigation in multi-agent scenarios is adapted to suit the needs of Voronoi-based coverage control with more than one swarm. The functionality of the proposed technique is validated through Monte Carlo simulations.","Recent events of wildfire in Canada as well as other countries worldwide have risen concerns all around the globe. Fires do not only pose hazards to populations but also to firefighting teams. A strong need for reliable means of assistance arises to ensure security for communities as well as individuals while monitoring affected areas. In that sense, the use of Unmanned Aerial Vehicles (UAV) has gained importance in recent years for search and rescue missions as well as fire fighting and monitoring. In this context, it is common for UAVs to work in swarms aiming to collaboratively pursue a mutual goal. A prevalent example for such scenarios is the provision of maximum coverage over an area of interest with a group of multiple agents. Furthermore, applications demonstrating similar needs not only comprise planetary exploration with multiple swarms of vehicles competing to investigate the same area, but also drones belonging to different companies providing mobile coverage in remote areas. The problem of coverage control incorporates the planning of motion for each vehicle of the multi-agent system to conjointly achieve a common objective. Coverage control has been a widely studied subject during the past two decades. Using Lloyd’s iterative algorithm [1], Cortés et al. [2, 3] introduced the use of centroidal Voronoi partitions and proximity graphs for coverage control. Gradient descent algorithms to coordinate a group of agents are presented and guarantees for the convergence of the algorithms are studied. Applications of centroidal Voronoi tesselations are discussed in [4]. The coverage problem is formulated as an optimal control problem considering energy efficiency and conservation in [5]. Reference [6] addresses the same problem for discrete-time systems. Analytic expressions for the rate of change of the mass and the location of the center of mass of a Voronoi cell are presented in [7]. A common feature in the majority of published research in coverage control is the consideration of one single swarm of agents working as a team towards a common goal. However, multiple applications require several groups of UAVs to independently provide maximum coverage over the same area for various purposes, e.g. search and rescue and fire monitoring. Although in [8] more than one swarm of vehicles is incorporated into a multi-agent scenario, the objective is multi-agent formation rather than coverage control. Two years later, Sharifi investigated coverage control using a group of unmanned ground vehicles along with a swarm of UAVs in [9]. Yet, the two swarms collaborate with each other to cover the given area as a unit. In [10], the authors examined the use of more than one swarm of vehicles to cover larger areas in shorter time adopting a leader-follower approach. Similarly to reference [9], different swarms jointly work towards a common goal. The division of an overall unit of robots working towards one common goal into sub-groups of similar size to increase regional coverage speed and reduce the moving distances of the agents is proposed in [11]. As a first approach to ensure collision-free motion for agents of all swarms, the Velocity Obstacles (VO) method and its extensions, with a primary focus on Optimal Reciprocal Collision Avoidance (ORCA), are adapted in this work. A detailed description of the ORCA method is given in section II-D. The VO technique was first formalized in 1998 by Fiorini and Shiller [12] who introduced the selection of maneuvers to avoid static as well as moving obstacles based on operations within the velocity space. Assuming obstacles with constant velocities and directions of motion, the agents select their velocity outside the Velocity Obstacle (\mathcal{VO}), which is defined as the subset of the velocity space that results in collision at a future time. Reference [13] provides an overview of the VO method and its extensions. An extension of VO for multi-agent navigation considering reactive behavior of other agents under the assumption of similar avoidance strategies was proposed by Van Den Berg et al. in [14]. Further research on VO for multi-agent navigation includes the work of Guy et al. [15] and Van Den Berg et al. [16]. The former introduced truncated collision cones and formulated an optimization problem to extend the VO approach and ensure collision-free multi-agent motion. Furthermore, they include considerations for computational efficiency through data and thread-level parallelism. The authors of [16] adopted the idea of VO in the form of truncated cones for collision avoidance in multi-agent systems with independently operating robots. Collision-free motion is achieved by deriving half-spaces of permitted velocities so that a low-dimensional linear program can be solved to guarantee local collision avoidance. The main contribution of this paper is a collision-free algorithm for multiple non-cooperating swarms of UAVs to independently cover a common area. To the best of the author’s knowledge, it is the first investigation presenting coverage control of several non-collaborating multi-agent swarms operating within the same space. In contrast to previous work, distinct swarms act as entirely independent units without explicitly exchanging information. Besides the possibility of agents encountering each other while moving towards their desired positions, there exist cases where distinct swarms of vehicles aim to converge to the same configurations. Such a scenario is presented in example 1. Therefore, to avoid collisions between agents the ORCA method is adapted to suit Voronoi-based coverage control. Monte Carlo simulations validate the proposed algorithm. The structure of this paper is as follows. Section II introduces preliminary notions and definitions on Voronoi tesselation (section II-A), coverage control (section II-B), a motivating example (section II-C), and collision-free navigation using ORCA (section II-D). The proposed extension of coverage control to independent swarms of agents safely operating within a common area is introduced in section III. Section IV presents Monte Carlo simulations for a specific example. Conclusions follow in section V."
https://arxiv.org/html/2411.08161v1,Shaping Frequency Dynamics in Modern Power Systems with Grid-forming Converters,"In this paper, frequency dynamics in modern power systems with a high penetration of converter-based generation is analysed. A fundamental analysis of the frequency dynamics is performed to identify the limitations and challenges when the converter penetration is increased. The voltage-source behaviour is found as an essential characteristic of converters to improve the initial frequency derivative of Synchronous Generators (SGs). A detailed small-signal analysis, based on the system’s eigenvalues, participation factors and mode shapes, is then performed in a reduced system for different converter penetrations, showing that the flexibility of grid-forming (GFOR) converters as well as the system’s inertia reduction may lead to have a more controllable system frequency. First-order frequency responses can be programmed for high converter penetrations, when GFOR operation can impose their dominance over SGs. These results have been validated in the IEEE 118-bus system simulated in PSCAD.","Renewable energy generation is currently increasing in many countries to tackle the effects of climate change. Small islands, like El Hierro (Spain) with a peak load below 10 MW, have already achieved 100% of instantaneous renewable power during several days [1]. Larger islands like Ireland have securely operated the system with a renewable penetration around 75% and it is planned to increase this value to 95% by 2030 [2]. A renewable generation higher than 100% respect to the local demand has been achieved in Spain, Denmark and Portugal during some hours of the year [3, 4], while Italy and Germany have exceeded 80%. Swedish power system is planned to be fully based in renewable generation by 2040 [5], while Spain’s objective is set for 2050 [6]. Following these challenging targets, a massive amount of solar and wind power plants are being installed, whose capacity worldwide has been multiplied by eight in just a decade [7], displacing conventional power plants from the generation mix. Therefore, converter-based resources are replacing Synchronous Generators (SGs) in electrical networks, resulting in a deep transformation of power systems and their operation. Voltage-Source Converters (VSCs) are fully controllable devices that might provide higher flexibility to the system, as they do not follow SG’s mechanical laws. However, they do not have physical inertia and present a limited short-circuit current, which differs considerably from the conventional SG’s behaviour. The inertia reduction due to the SG replacement by power converters poses new challenges for the power system operation. System’s inertia is considered nowadays an essential parameter to ensure frequency stability [8, 9], as it determines the initial Rate of Change of Frequency (RoCoF) after a generation-load imbalance. Several Transmission System Operators (TSOs) estimate the system inertia to guarantee a minimum secure value [9, 10]. In some cases, the installation of synchronous condensers is also being considered to increase the inertia and short-circuit current of the system [11, 12]. On the other hand, faster active power response of converters can reduce the minimum inertia required to operate the system [13], changing the paradigm of the inertia needs in the system. In October 2020, AEMO required a primary frequency regulation to all converter-based generation [14], helping to improve the frequency deviations in the system. Figure 1 represents the probability of occurrence of the measured frequency in Australia in September 2020 and May 2023, revealing the reduction in the frequency deviations after including the converter-based generation into the frequency regulation. In addition, an increasing interest in STATCOMs with energy storage is also emerging as an alternative solution to mitigate the impact of load imbalances [15, 16, 17]. Figure 1: Measured frequency probability in Australia. Data from [18]. Although frequency deviations can be mitigated by including the converter-based generation and storage systems in the frequency regulation (as shown in the Australian system in Fig. 1), frequency dynamics during large transient events are also a concern, specially when the system’s inertia is reduced. In these cases, the frequency dynamics are more dependent on the support provided by converters, which is determined by their control structure. Grid-following (GFOL) VSCs can provide limited support, as their active power injection depends on the frequency measurement, resulting in a delayed response. Additional control strategies have been presented to improve the frequency support of GFOL converters considering also the frequency derivative [19]. However, the practical implementation of these control schemes can be challenging to apply. Additionally, it has been reported that high GFOL-VSC penetrations can result in system’s instability [20, 21]. To overcome such stability issues, several grid-forming (GFOR) control structures have been presented [22, 23, 24]. In contrast to the delayed GFOL’s response, GFOR VSCs can inject active power instantaneously after a load imbalance, which might help to mitigate the RoCoFs. Another important characteristic of GFOR converters is that their frequency no longer depends on physical laws, like the SG’s inertia, but just on their control algorithms. This fact opens a new scenario, where GFOR converters can shape the frequency dynamics in power-electronic-dominated power systems. It should also be noted that GFOR converters are self-synchronised units, so it is also necessary to ensure synchronism among all the generators in the system. Therefore, the selected control strategy will play a key role in the future frequency dynamics. Some publications have highlighted that GFOR converters can help to improve frequency dynamics in systems with high penetrations of power electronics. The synchronization mechanism between SGs and VSCs is analysed in [25], where the generation elements are simplified as second-transfer functions. Adjusting the damping component in GFOR converters can improve grid synchronization stability, especially for low-inertia systems. Similar conclusions are derived in [26], where aggregated models are used to capture the frequency dynamics. A qualitative analysis is provided in [27], comparing different synchronization control techniques for GFOR operation. However, a fundamental analysis on how the frequency can evolve in modern power system is not addressed in previous publications. An in-depth analytical and simulation-based study is provided in [28], where the impact of different droop variants on frequency dynamics is also analysed. It is concluded by means of non-linear simulations that the frequency dynamics progressively tend to a first-order response when the penetration of droop-based GFOR converters is increased. In this paper, the fundamentals on frequency dynamics are revisited. The impact of the GFOR operation is analysed for different converter penetrations and converter control parameters. The study is based on a small-signal analysis, providing details about the oscillation modes (frequency and damping), participation factors and mode shapes. This small-signal analysis allows to fully characterise the frequency dynamics, which are later validated via EMT simulations implemented in PSCAD. In particular, the following contributions are considered: • A comprehensive study about frequency dynamics in modern low-inertia power systems is provided, revealing that the VSC’s dominance can be an opportunity to improve frequency dynamics. It is demonstrated that high penetrations of GFOR converters allow to shape the frequency dynamics, which can be designed to follow the desired response, e.g. a first-order response. • The dominant modes of frequency dynamics are identified by small-signal analysis. These dominant modes have been classified as Synchronisation mode and Global mode, which fully describe the frequency behaviour in a power system with multiple generation units. • A sensitivity analysis of the GFOR VSC’s synchronisation control parameters is performed in a reduced system, identifying their impact on the frequency dynamics for different converter penetrations. • The results obtained in the reduced system are validated with a full non-linear model of the IEEE 118-bus system, implemented in PSCAD, confirming that it is possible to reshape the frequency dynamics also in large power systems with GFOR converters. • The voltage-source behaviour is shown as an essential characteristic from converters to improve frequency dynamics with high penetrations of power electronics."
https://arxiv.org/html/2411.08156v1,Optimal Constant Climb Airspeed with Variable Cost Index for All-electric Aircraft,"This paper presents for the first time an approach to minimize direct operational costs (DOC) for all-electric aircraft during the climb phase, introducing a time-varying cost index (CI). The CI is modeled as a dynamic parameter commanded by Air Traffic Control (ATC), allowing the aircraft to maintain a constant airspeed throughout the climb, while respecting the air traffic regulations. This paper also explores the implications of a time-varying CI on the determination of optimal airspeed and climbing time for all-electric aircraft. Additionally, it provides the necessary equations to calculate both the optimal climb airspeed and climb duration. The proposed methodology has been validated through a simulated scenario that reflects actual operational procedures. As a result, optimal values for climb airspeed, climbing time, and energy consumption have been established, paving the way for future applications of this methodology to advanced air mobility all-electric vehicles.","The interest for fully electric solutions is continuously increasing in the aviation community, especially in the past few years, due to their potential profitability and capability to emerge as forefront responses to greenhouse gas (GHG) emissions. The global electric aircraft market was valued at USD 7.91 billion in 2022 and it is projected to reach USD 50.86 billion by 2032 [PrecResearch2023]. From an environmental perspective, the aviation sector accounts for approximately 2.5% of the world’s current CO_{2} emissions [Ritchie2024]. At a first glance, it does not seem alarming, but only a small portion of the global population has access to aviation services. This indicates the potential impact in carbon dioxide emissions caused by aviation worldwide in a scenario of steadly increasing demand for domestic and international travel. The rapid urbanization of populated areas, the expanding awareness with new environmental concerns and the fast-paced rise in air travel demand are propelling initiatives that promote technological advance in transportation. Examples of such developments include autonomous systems and reliable alternatives to fossil-based fuels. In particular, the Advanced Air Mobility (AAM) concept intends to provide transportation services for people and cargo in an automated and cooperative fashion [FAA_Conops]. One of the main technical challenges faced by all-electric aviation is the limited energy density of the electrical batteries [Barzkar_Ghassemi2022], which restricts their operation in long-haul flights. One alternative for improving energy efficiency in aviation is to minimize overall direct operational costs (DOC) by flying the aircraft in the optimal airspeed that corresponds to the economy (ECON) mode as part of the flight plan in modern aircraft Flight Management Systems (FMS). The ECON speed is defined for any flight phase based on the cost index (CI), which is a trade-off parameter used in aviation characterized by the ratio of the costs associated to time of operation (crew salaries, maintenance procedures, costs incurred by prolonged delays or leasing of equipment) and the cost due to energy consumption (the cost to charge electrical batteries for all-electric aircraft). The minimization of DOC has been an active research topic since the introduction of FMS in the early 1980s. Initial research on the determination of optimal airspeed can be found in [Erzberger_Lee1980, Liden1985]. More recent work that deals with minimizing DOC typically uses optimal control theoretical results, such as the Hamilton-Jacobi-Bellman (HJB) equation or the Pontryagin’s Minimum Principle (PMP) to find optimal and suboptimal airspeed for fuel-powered aircraft [Villarroel_Rodrigues2016a, Fan_et_al2020, Villarroel_Rodrigues2016b] and for all-electric aircraft [Kaptsov_Rodrigues2017, Wang2020]. However, these references assume a constant CI throughout the flight phase for which the optimal airspeed has been computed. Some research considers CI as a parameter that might change during flight [Cook_et_al2009, Camacho_et_al2015, Prats_Torre_Delgado2022, Oliveira_et_al2023, Mori2022]. In [Silva_Akgunduz_Rodrigues2024], the authors introduce a time-varying CI that balances the strategic objectives of airlines with operational restrictions imposed by Air Traffic Control (ATC) as part of the cruise ECON speed computation. Nevertheless, to the best of the authors’ knowledge, none of the previous work in the open literature shows how ATC-driven changes in CI affect the computation of the optimal climb airspeed and climbing time for all-electric aircraft. This paper contrasts with the existing contributions as it demonstrates how a time-varying CI impacts the determination of the optimal climb airspeed and climbing time of all-electric aircraft for operations that require climbing in constant airspeed. The main contributions of this paper are: 1. The introduction of CI as a dynamic parameter in the formulation of the optimization problem to minimize DOC for all-electric aircraft in climb. The changes in CI are commanded by ATC to impose operational restrictions to the all-electric aircraft operation. 2. The equations that compute the optimal climb airspeed and time are provided for all-electric aircraft. The aircraft energy consumption is also determined in this paper. 3. The validation of the proposed methodology using a climb profile inspired by an actual operational procedure based on regulatory standards. Several operational procedures require constant airspeed climb, such as Noise Abatement Departure Procedures (NADP) and specific Climb Via Clearances. NADPs were created to alleviate the effects of noise caused by aircraft operation in terminal areas to communities that live nearby [FAA_Noise_Ab1993], by creating flight profiles that reduce exposure of individuals on the ground to the noise primarily caused by the aircraft’s engines. According to [ICAO_Noise_Ab2007], most of NADPs require constant airspeed during the majority of the aircraft climb phase. Climb Via Clearance procedures implement ATC requirements to make the aircraft climb using specific paths or waypoints with restrictions on altitude and speed. An example of a requirement of compliance with ATC clearance is the FAR 14 CFR 91.123 in the United States [14CFR_ATC_Clear]. In accordance with procedures that require constant airspeed in climb, [FAA_IFH2012] provides pilots with instructions on how to enter and maintain constant speed climb maneuvers. Although these procedures were initially proposed for fuel-powered aircraft, they could be adapted to all-electric aircraft, particularly within the controlled airspace, once these vehicles are fully integrated to the airspace system. This paper is organized as follows: Section II presents the methodology to perform the FMS initialization and the calculation of optimal climb airspeed and climbing time with variable CI for all-electric aircraft. Section III presents a simulated scenario and discussions about the observed results. Section IV concludes the paper."
https://arxiv.org/html/2411.08835v1,Goal-oriented Semantic Communication for Robot Arm Reconstruction in Digital Twin: Feature and Temporal Selections,"As one of the most promising technologies in industry, the Digital Twin (DT) facilitates real-time monitoring and predictive analysis for real-world systems by precisely reconstructing virtual replicas of physical entities. However, this reconstruction faces unprecedented challenges due to the ever-increasing communication overhead, especially for digital robot arm reconstruction. To this end, we propose a novel goal-oriented semantic communication (GSC) framework to extract the GSC information for the robot arm reconstruction task in the DT, with the aim of minimising the communication load under the strict and relaxed reconstruction error constraints. Unlike the traditional reconstruction framework that periodically transmits a reconstruction message for real-time DT reconstruction, our framework implements a feature selection (FS) algorithm to extract the semantic information from the reconstruction message, and a deep reinforcement learning-based temporal selection algorithm to selectively transmit the semantic information over time. We validate our proposed GSC framework through both Pybullet simulations and lab experiments based on the Franka Research 3 robot arm. For a range of distinct robotic tasks, simulation results show that our framework can reduce the communication load by at least 59.5% under strict reconstruction error constraints and 80% under relaxed reconstruction error constraints, compared with traditional communication framework. Also, experimental results confirm the effectiveness of our framework, where the communication load is reduced by 53% in strict constraint case and 74% in relaxed constraint case. The demo is available at: https://youtu.be/2OdeHKxcgnk.","As an emerging paradigm in industry, the Digital Twin (DT) is envisioned to enhance operational safety and reliability through integrating the physical world and digital world [1]. By reconstructing high-fidelity digital models of physical entities, DTs can comprehensively replicate real-world systems, thus enabling functionalities such as fault diagnosis [2], predictive maintenance [3], and optimal decision making [4]. However, the real-time reconstruction of digital models normally requires intensive communication resource, posing significant challenges to existing wireless networks [5]. This challenge becomes even more severe for digital robot arm reconstruction due to its high operating frequency and complex dynamics. It has been shown that a single control message can take up more than 100 bytes [6], and the throughput requirement for reconstructing an industrial-grade dual robot arm operated at 1ms scale in the DT can reach 138,500 bytes per second [7], not to mention its extension to large-scale industrial scenarios with thousands of robot arms operating simultaneously. Meanwhile, observations from existing testbed indicate that the frequent control message transmissions without considering the importance of data can lead to data congestion due to the buffer overflow at the receiver end [8]. Therefore, there is an urgent need to reduce the communication load for robot arm reconstruction in DTs. To tackle this, existing works mainly focused on designing the goal-oriented/task-oriented framework for DT reconstruction, with the aim of reducing communication cost while maintaining the reconstruction error in acceptable levels. In the context of robot arm DT, reconstruction error is commonly characterised by effectiveness-level performance metrics such as the mean square error (MSE) [9][10] and Euclidean Distance [11]. In [9], the authors implemented movement prediction algorithms at the DT side to compensate for packet loss and mitigate the Root-MSE (RMSE) between the digital robot arm trajectory and the ground truth. This method was further exploited to develop a communication and prediction co-design framework [10], where a deep reinforcement learning (DRL) algorithm jointly optimises the real-to-simulation updating frequency and the prediction window, to minimise the communication load subject to the MSE constraint on trajectory difference. Extending from [10], the authors in [11] defined the Euclidean distance between the positions and orientations of the physical robot and its DT as the effectiveness metric. They further reduced the communication load by introducing expert knowledge to the DRL algorithm and refining the reward design. The above task-oriented frameworks have shown improvements in real-time DT reconstruction, but there still exists two research gaps. First, the trajectory difference cannot comprehensively describe the DT reconstruction quality since the robot dynamics (e.g., velocity difference) might introduce deviations in the reconstruction process, this motivates us to refine the effectiveness-level design in this work. Second, these frameworks tend to transmit all generated messages without considering the significance and usefulness of their contents, i.e., the semantic-level problem is unsolved. To tackle the potential network congestion and increased latency caused by excessive message transmission in goal-oriented frameworks [12], the concept of semantic communication has been proposed[13]. By extracting and transmitting the semantic information behind the original bits, it can significantly reduce the length or transmission frequency of original messages. Prior works have applied semantic communication to a wide range of traditional data types, such as text [14], speech [15], image [16], and video [17]. Meanwhile, semantic communication has also been leveraged to filter and compress the control message or sensor data in reconstruction tasks [18, 19, 20, 21, 22]. The Age of Information (AoI) [18] and its variants (e.g., Age of Incorrect Information [19] and ultra-low AoI [20]) have been commonly used as the semantic metrics to evaluate the importance of the time-critical sensor data. They in essence characterise the data freshness, and the freshest data are typically assigned the highest priority. In [21], the authors quantified the semantic value of the transmitted messages by combining the AoI with the similarity between adjacent control messages to improve the communication efficiency for the unmanned aerial vehicle (UAV) control task. The authors in [22] were among the first to study semantic communication for DT robot arm reconstruction, where the authors optimised the communication by discarding the useless information based on the robot arm’s current motion. However, they only considered wired connection and their approach might cause the DT to lag behind the real-world. The aforementioned works [14, 15, 16, 17, 18, 19, 20, 21, 22] have explored the benefits of semantic communication extensively, but inevitably share a common limitation. That is, they ignored that the semantic value of messages is not only dependent on their context, but also closely coupled with the specific communication goal. To take the advantages of both the goal-oriented framework and semantic communication, an integrated goal-oriented semantic communication (GSC) framework was proposed in [23]. It jointly considers the semantic-level information and effectiveness-level performance metrics for multi-modal data in various tasks. This framework was further implemented in the context of UAV trajectory control [24]. The authors utilised a joint function of AoI and Value of information to identify the most important control and command data, with the GSC goal of minimising the trajectory MSE. Another GSC framework extending from [23] was proposed for the point cloud-based avatar reconstruction in the Metaverse [25], where only the critical nodes of the avatar skeleton graph are transmitted to minimise bandwidth usage. It can be seen that the GSC framework has been developed for various scenarios, but its application for robot arm reconstruction in DT has never been studied yet, where the communication efficiency needs further improvement, and both the effectiveness-level metrics for the reconstruction task as well as the semantic-level information remain unknown. Motivated by this, we propose a novel GSC framework for robot arm reconstruction in DT. Compared with the existing frameworks [9, 10, 11, 22], we not only analyse the specific contents of reconstruction messages to identify the most critical GSC information for transmission, but also perform temporal selection to transmit only the most important messages at critical moments without degrading the reconstruction quality. Additionally, we incorporate the velocity difference between the physical and digital robots into the effectiveness-level metrics, to ensure the DT and the physical world share the same dynamics during the reconstruction process. Our main contributions are summarised as follows: • We consider a real-time DT reconstruction task for a physical robot arm that performs three different tasks, including pick-and-place, pick-and-toss and push-and-pull tasks. The physical robot arm transmits reconstruction messages to the DT, with the goal to accurately replicate its dynamics, states, and real-time motions with reduced communication cost. • At the effectiveness-level, we jointly consider the impact of robot dynamics and kinematics, and define the goal-oriented performance metrics as the reconstruction error, which includes the joint angle error and joint velocity error of the robot arm. At the semantic-level, we reveal that the significance of different message contents (i.e., different features111We refer to features as different components of the reconstruction message in this work, not the general concept of features in machine learning.) changes based on robot’s current movement, with certain contents lacking semantic information at specific times as they do not affect the reconstruction accuracy. Meanwhile, we capture the temporal features of the reconstruction messages and show that dropping redundant or less useful messages barely affect real-time DT reconstruction. • We formulate the GSC goal as minimising the communication load under the DT reconstruction error constraints. To achieve this, we propose a GSC reconstruction framework that incorporates the effectiveness-level and semantic-level designs. Specifically, we develop a Feature Selection (FS) algorithm that can divide the robotic task into several phases (e.g., grasp and release), and only transmit message contents that contain GSC information for the current phase. Building upon this, a Proportional-Integral-Derivative-based Primal-dual Deep Q-Network (PPDQN) algorithm is designed to evaluate the importance of the reconstruction message at current time slot, and discard the redundant or less useful ones. • Our proposed framework is validated via both Pybullet simulations and experiments using the Franka Research 3 robot arm. For the three different robotic tasks, our proposed GSC framework can reduce the communication load by at least 59.5% under strict reconstruction error constraints and 80% under relaxed reconstruction error constraints in the simulations. Experimental results further validate our framework, as the communication load is reduced by 53% and 74% in the strict error constraints case and relaxed error constraint case, respectively. The rest of this paper is organised as follows: Section II presents the system model and the formulated problem. Section III and Section IV introduce the traditional DT reconstruction framework and our proposed GSC reconstruction framework. In Section V, the results of the simulations and physical experiments are presented to verify our proposed framework. Finally, we conclude our work in Section VI."
https://arxiv.org/html/2411.08759v1,Clutter-Aware Target Detection for ISAC in a Millimeter-Wave Cell-Free Massive MIMO System,"In this paper, we investigate the performance of an integrated sensing and communication (ISAC) system within a cell-free massive multiple-input multiple-output (MIMO) system. Each access point (AP) operates in the millimeter-wave (mmWave) frequency band. The APs jointly serve the user equipments (UEs) in the downlink while simultaneously detecting a target through dedicated sensing beams, which are directed toward a reconfigurable intelligent surface (RIS). Although the AP-RIS, RIS-target, and AP-target channels have both line-of-sight (LoS) and non-line-of-sight (NLoS) parts, it is assumed only knowledge of the LoS paths is available. A key contribution of this study is the consideration of clutter, which degrades the target detection if not handled. We propose an algorithm to alternatively optimize the transmit power allocation and the RIS phase-shift matrix, maximizing the target signal-to-clutter-plus-noise ratio (SCNR) while ensuring a minimum signal-to-interference-plus-noise ratio (SINR) for the UEs. Numerical results demonstrate that exploiting clutter subspace significantly enhances detection probability, particularly at high clutter-to-noise ratios, and reveal that an increased number of transmit side clusters impair detection performance. Finally, we highlight the performance gains achieved using a dedicated sensing stream.","Cell-free massive multiple-input multiple-output (MIMO) is a user-centric network infrastructure where a set of distributed access points serve multiple user equipments on the same time-frequency resources using joint processing techniques [1]. Over nearly a decade of research, it has been shown that cell-free massive MIMO improves both spectral and energy efficiency by providing macro diversity and enhanced interference management compared to traditional cellular setups. As a result, it has become one of the key technologies for sixth-generation (6G) wireless networks [2]. Recently, the cell-free massive MIMO architecture has also demonstrated potential benefits in integrated sensing and communications (ISAC) [3, 4]. ISAC allows for the efficient use of hardware and spectral resources through integration and coordination gains, compared to separate communication and sensing architectures [5]. Cell-free massive MIMO facilitates multi-static sensing through a central processing unit (CPU), to which the APs are connected via fronthaul links. Beyond cell-free massive MIMO and ISAC, another key 6G technology is reconfigurable intelligent surface (RIS)-aided communications. RISs have attracted significant attention from both academia and industry due to their ability to control challenging radio propagation environments intelligently. By using low-cost passive reflecting elements that induce controllable phase shifts in incoming waveforms, RISs can steer reflected signals toward desired locations while reducing interference at undesired points [6, 7]. Furthermore, RISs have emerged as a promising technology in ISAC applications, enhancing the available spatial degrees-of-freedom. In [8], the sum rate of communication UEs is maximized under a worst-case sensing signal-to-noise ratio (SNR) constraint. An often overlooked aspect in ISAC research is the effect of clutter on target detection, which can severely degrade system performance if not properly addressed. Recent studies, such as [9], illustrate the impact of clutter in monostatic ISAC settings, underscoring the significance of clutter awareness at the receiving APs for reliable target detection. RISCloud unitAP r\in\mathcal{R}TargetClutterAP t\in\mathcal{T}UE k\in\mathcal{K}\mathbf{g}_{k,t}\mathbf{H}_{t}\mathbf{g}_{\rm T}\mathbf{b}_{r} \overline{\mathbf{H}}_{t}\overline{\mathbf{g}}_{\rm T}\overline{\mathbf{b}}_{r} cluster Figure 1: Depiction of a cell-free massive MIMO ISAC network performing multi-static sensing. In this paper, we investigate the extent to which an RIS can enhance sensing capabilities when the direct channels between the transmitting APs and the target are obstructed. We also explore the need for precise channel state information (CSI) for target detection within a millimeter-wave (mmWave) channel architecture, as well as the requirement for sensing-specific streams. Our setup, illustrated in Fig. 1, evaluates the system’s ability to detect a potential target at a known location while meeting a minimum signal-to-interference-plus-noise ratio (SINR) for each UE. To this extent, we maximize the target’s signal-to-clutter-and-noise ratio (SCNR) using an alternating optimization (AO) algorithm that jointly optimizes transmit power allocation and the RIS phase-shift matrix. The APs assume a line-of-sight (LoS) structure for the sensing links, justified by the high path loss in mmWave frequencies, which typically makes LoS paths dominant over non-line-of-sight (NLoS) paths. A centralized precoding strategy is employed with regularized zero forcing (RZF) for communication streams. The sensing stream is projected onto the null space of the communication channels, which are assumed to be known at the APs. Assuming knowledge of the clutter subspace, target detection is conducted using a generalized likelihood ratio test (GLRT). THe numerical simulations reveal that clutter awareness significantly enhances the detection probability, particularly at high clutter-to-noise ratio (CNR). Furthermore, when sensing links contain many clusters concentrated in a narrow angular range, the LoS assumption becomes less effective, reducing detection probability. The results also highlight the importance of allocating dedicated sensing symbols to improve detection performance. Notation: \odot represents the Hadamard (element-wise) product. Boldface lowercase and uppercase letters denote vectors and matrices. The trace of the matrix \mathbf{X} is denoted by \text{Tr}(\mathbf{X}). \text{diag}(\mathbf{x}) represents the stacking of \mathbf{x} on the main diagonal of a matrix. \mathbf{I}_{N} represents an N\times N identity matrix. The notation \mathcal{CN}(0,\sigma^{2}) represents circularly symmetric complex Gaussian distribution with variance \sigma^{2}. By \mathcal{L}\setminus\{l\}, we denote the set obtained by removing the element l from the set \mathcal{L}. Given a matrix \mathbf{M}, \mathbf{M}^{\dagger} represents its Moore-Penrose pseudoinverse."
https://arxiv.org/html/2411.08144v1,Visual Tracking with Intermittent Visibility: Switched Control Design and Implementation,"This paper addresses the problem of visual target tracking in scenarios where a pursuer may experience intermittent loss of visibility of the target. The design of a Switched Visual Tracker (SVT) is presented which aims to meet the competing requirements of maintaining both proximity and visibility. SVT alternates between a visual tracking mode for following the target, and a recovery mode for regaining visual contact when the target falls out of sight. We establish the stability of SVT by extending the average dwell time theorem from switched systems theory, which may be of independent interest. Our implementation of SVT on an Agilicious drone [1] illustrates its effectiveness on tracking various target trajectories: it reduces the average tracking error by up to 45% and significantly improves visibility duration compared to a baseline algorithm. The results show that our approach effectively handles intermittent vision loss, offering enhanced robustness and adaptability for real-world autonomous missions. Additionally, we demonstrate how the stability analysis provides valuable guidance for selecting parameters, such as tracking speed and recovery distance, to optimize the SVT ’s performance.","The visual tracking task requires a pursuer to follow a moving target using only camera as a sensor. This capability is relevant for search and rescue, delivery, spacecraft docking, in-air-refuelling, navigation, and other autonomous missions. Visual tracking, also called visual pursuit, has been studied by robotics and aerospace researchers [2, 3, 4, 5] (see, other related works in Section V). A popular approach is for the pursuer’s controller to minimize the tracking error defined on image space. These methods are effective if the target is always visible, but cannot recover once it is lost from the pursuer’s camera view. The target may be lost because of motion blur, occlusions, or simply because it moves out of the pursuer’s camera frame. The latter is more likely as the pursuer nears the target and the camera’s viewable field becomes narrower. Thus, the two goals of maintaining visibility and gaining proximity can be conflicting. To tackle this challenge, we propose the design of a Switched Visual Tracker (SVT)—a mode-switching controller [6] that tracks the target when it is visible, in what we call a visual tracking mode, and maneuvers to regain visual contact when the target is lost, in a recovery mode. The design of SVT includes the logic for switching between these two modes. A switching controller for landing a drone on a moving vehicle was presented in [7]. That work, like ours, handles loss of visual contact of the moving target and showed interesting empirical results. To our knowledge, ours is the first to provide a stability analysis and connect the stability criteria with the control design parameter. There is extensive research on stability analysis of switched systems [6, 8, 9, 10]. Hespanha and Morse’s average dwell time theorem [8] gives a stability criterion in terms of the rate of energy (or Lyapunov function) decay in the individual modes (\lambda), the energy gains across the mode switches, and the rate of mode switches. To accommodate the analysis of SVT with recovery, we generalize this theorem to Theorem 1 which allows the system to be temporarily unstable, which leads to an additive (c) and multiplicative (\mu) increase in the Lyapunov functions. We show that, given a sufficiently long average dwell time in the stable modes, the system can still achieve asymptotic stability with respect to a set of states. For SVT this implies guaranteed tracking performance. We compare the effectiveness of SVT implemented on the Agilicious drone [1], with a baseline visual tracking controller. SVT improves the average tracking error by 45% and also significantly improves the fraction of time the target is visible. Further, we observe Theorem 1 can be used to guide the choice of various SVT parameters for improving the system’s performance with respect to tracking and visibility requirements. For example, according to Theorem 1, a higher tracking speed (v_{max}) increases the Lyapunov exponent \lambda, which reduces tracking by allowing smaller dwell time. We observe from experiments that increasing v_{max} from 0.1m/s to 1.0m/s indeed improves the average tracking error from 1.3m to 0.5m. A smaller recovery distance (d_{max}) reduces \mu and c (characterizing the unstable recovery). By reducing the recovery distance from 2.7m to 2.1m, experiments show the average tracking error improves 0.74m to 0.56m while target visibility improves from from 82.8 to 86.4%. In summary, our contribution are as follows: 1. The design of a Switched Visual Tracker (SVT) for the visual tracking problem, which effectively handles intermittent loss of target visibility. 2. A rigorous stability analysis of the SVT based on a modest extension of an existing switched system stability result. This provides guarantees on the tracking performance and guidelines for choosing parameters in the implementation of SVT. 3. An implemention of SVT on an Agilicous-based pursuer drone and comprehensive experimental evaluations with different target trajectories and design parameters."
https://arxiv.org/html/2411.07998v1,A Symmetry-Preserving Reduced-Order Observer,"A symmetry-preserving, reduced-order state observer is presented for the unmeasured part of a system’s state, where the nonlinear system dynamics exhibit symmetry under the action of a Lie group. The proposed observer takes advantage of this symmetry through the use of a moving frame that constructs invariant mappings of the measurements. Sufficient conditions for the observer to be asymptotically stable are developed by studying the stability of an invariant error system. As an illustrative example, the observer is applied to the problem of rigid-body velocity estimation, which demonstrate how exploiting the symmetry of the system can simplify the stabilization of the estimation error dynamics.","I INTRODUCTION Methods for designing state observers for nonlinear systems are limited and there are no general techniques that guarantee global convergence of the estimation error, as there are in the linear case [1, Ch. 15]. Provably effective state estimation strategies are inevitably limited to special classes of systems. Here, we leverage symmetries in a dynamical system’s structure to aid observer design and stability analysis. These so-called symmetry-preserving observers presented by Bonnabel et al. in [2] and [3] are adapted to systems whose dynamics are invariant under the action of a Lie group. The idea is to design an observer that is also invariant, i.e., for which the observer dynamics also preserve this Lie group symmetry. This approach allows the observer’s convergence properties to be analyzed more easily because of simplifications afforded by the system symmetry. Existing approaches to symmetry-preserving observers only consider the full-order case, however, in which the entire state of the system is estimated ([2, 3, 4]). In many scenarios, part of the system’s state may be known with negligible error or may be obtained as the output of an observer whose design is independent of the rest of the system’s state. For example, attitude observers for aircraft or spacecraft often do not rely on the rigid body’s translational dynamics ([5, 6]). Another example is the problem of wind estimation from aircraft motion ([7, 8, 9]), where the main goal is to obtain estimates of wind and air-relative velocity – not to re-estimate the aircraft’s position, attitude, and angular velocity. More generally, the problem of disturbance estimation falls into this category where the internal state of the system is known but the disturbance is not. In these scenarios, reduced-order observers, in the sense of Karagiannis et al. in [10] and [11], are of particular interest where only the unmeasured part of the system’s state is estimated. The aim of observer design is to render a particular set, characterized by zero state estimation error, positively invariant and globally asymptotically attractive. Here, we focus on reduced-order observers that are also symmetry-preserving. The remainder of this paper is organized as follows. Section II introduces the preliminary concepts that will be used in the development of a reduced-order, symmetry-preserving pre-observer in Section III. Next, sufficient conditions for the pre-observer to be an asymptotically stable observer are presented in Section IV. Finally, the main results are applied to the example of rigid-body velocity estimation in Section V followed by concluding remarks in Section VI."
https://arxiv.org/html/2411.07971v1,Optimal Control of Mechanical Ventilators with Learned Respiratory Dynamics,"Deciding on appropriate mechanical ventilator management strategies significantly impacts the health outcomes for patients with respiratory diseases. Acute Respiratory Distress Syndrome (ARDS) is one such disease that requires careful ventilator operation to be effectively treated. In this work, we frame the management of ventilators for patients with ARDS as a sequential decision making problem using the Markov decision process framework. We implement and compare controllers based on clinical guidelines contained in the ARDSnet protocol, optimal control theory, and learned latent dynamics represented as neural networks. The Pulse Physiology Engine’s respiratory dynamics simulator is used to establish a repeatable benchmark, gather simulated data, and quantitatively compare these controllers. We score performance in terms of measured improvement in established ARDS health markers (pertaining to improved respiratory rate, oxygenation, and vital signs). Our results demonstrate that techniques leveraging neural networks and optimal control can automatically discover effective ventilation management strategies without access to explicit ventilator management procedures or guidelines (such as those defined in the ARDSnet protocol).","Acute Respiratory Distress Syndrome (ARDS) is a clinical syndrome of acute hypoxemic respiratory failure due to lung inflammation (not caused by cardiogenic pulmonary edema), and requires emergency care to prevent further complications or death [matthay2024ardsdef]. A common approach to treating ARDS involves mechanical ventilation [matthay2019acute]. Mechanical ventilators (herein ‘ventilators’) are machines that artificially support a patient’s breathing by moving air into and out of their lungs according to a set of control inputs that define the pressure, time, fraction of inspired oxygen, etc. of the desired respiratory cycle [slutsky2000mechanical]. Our specific focus in this work is to illustrate that methods from the field of optimal control—supplemented with learning-based techniques—are capable of optimally and automatically choosing a ventilator’s inputs to effectively manage the condition of a patient with ARDS in real time. The contributions of this work are as follows: 1. A repeatable ventilator management task (formulated as a Markov Decision Process and built on top of the Pulse Physiology Engine [bray2019pulse]) for assessing the performance of ventilator management strategies. 2. Comparisons of optimal control policies, learning-based policies, and established ventilator management strategies (e.g. the ARDSnet protocol) on this task. 3. Evidence demonstrating that neural networks and optimal control can automatically discover effective ventilation management strategies without access to explicit ventilation procedures, recommendations, or guidelines. A key baseline in this work is the ARDSnet protocol, which is a strategy for setting ventilator actions to manage ARDS effectively [grasso2007ardsnet]. It uses a rules-based decision-making framework, calculating control inputs for a ventilator using mathematical formulas and lookup tables based on clinical trial data from the NIH and NHLBI. Other ventilator management protocols also exist, emphasizing specific outcomes such as protective ventilation (e.g. to avoid ventilator-induced lung injury [zou2024ventstratsprotect]). In addition to traditional protocols, methods for fully automating ventilator controls have been explored. By automating ARDS management, these techniques may offer more constant, fine-grained, and responsive care to patients with ARDS. Approaches include adaptive fuzzy proportional and integral controllers [naskar2023fuzzypi], the partially observable Markov decision process (POMDP) framework [li2019optimizing, kreutz2005pomdp], and reinforcement learning [peine2021development]. Our approach builds on these by integrating optimal control theory with learning-based techniques, enabling the discovery of effective ventilator strategies without explicit guidelines."
https://arxiv.org/html/2411.07927v1,Control-Oriented Models Inform Synthetic Biology Strategies in CAR T Cell Immunotherapy,"Chimeric antigen receptor (CAR) T cell therapy is revolutionizing the treatment of blood cancers. Mathematical models that can predict the effectiveness of immunotherapies such as CAR T are of increasing interest due to their ability to reduce the number of experiments performed and to guide the theoretical development of new therapeutic strategies. Following this rationale, we propose the use of control-oriented models to guide the augmentation of CAR T therapy with synthetic gene circuitry. Here we present an initial investigation where we adapt a previously developed CAR T model for control-oriented purposes. We then explore the impact of realistic alternative activation methods as control inputs to ensure effective tumor clearance.","Chimeric antigen receptor (CAR) T cells have shown remarkable therapeutic potential in the treatment of hematological cancers such as B cell leukemias [1] and lymphomas [2], [3]. CAR T cells are genetically engineered to interact with high specificity to tumor antigens [4]. Binding of the CAR to tumor target antigens leads to activation and proliferation of CAR T cells and tumor cell lysis [2]. Yet, CAR T therapies have encountered an array of different issues such as insufficient numbers of CAR T cells, immunosuppressive signaling, [5], and outgrowth of target antigen-negative or low tumor cells [6, 7]. A promising strategy to overcome these challenges is through augmentation of CAR T cells with additional synthetic gene circuitry [8, 9, 10]. Current synthetic biology control strategies are focused on achieving more specific activation of CAR T cells in presence of the tumor, as well as controlling therapeutic timing, location, and strength. [11]. The concept of control in CAR T immunotherapy is still removed from the classical control theory, the incorporation of which may provide important insights and tools to design more effective therapies, and this manuscript aims to bridge this theoretical gap. Two major classes of models have been previously developed to investigate CAR T cell behaviors: agent-based models (ABMs) and models using partial differential equations/ordinary differential equations (PDEs/ODEs). ABMs are computational methods that simulate the spatial and temporal behavior of every single agent/cell that interacts with the environment following specific rules [12]. This kind of model is based on simulations to capture a specific behavior of the system which can be difficult to describe with mathematical tools [13]. In contrast mathematical models based on PDEs and ODEs allow analyses that are impossible to make with ABMs or solve problems such as optimization and control. ODEs have been used in the framework of CAR T cells to describe the behavior of single cells [14], however, ODEs can also be applied to additional mechanistic levels in the study of CAR T cells. For instance, a specific CAR-T design at the cellular level will have effects on the entire system/population of CAR T cells and will act on a population of tumor cells. A previously developed model that provides a high-level behavior of tumor cell and CAR T cell populations for non-solid tumors is called CARTmath [15]. This model introduces three ODEs that describe dynamics and the interactions between the population of active CAR T cells, memory (non-active) CAR T cells, and tumor cells. This model mimics a predator-prey-like model where from the theoretical viewpoint tumor clearance cannot be achieved, which conflicts with real-world outcomes. Additionally, this model correctly demonstrates that the therapeutic success is highly dependent on input CAR T cell number. Since this model reduces all the effects of possible synthetic biology strategies in the design of the CAR T cells into a set of parameters, it is difficult to make a direct connection between the effectiveness of the therapy and potential synthetic gene circuitry designs. In this paper, we tackle the challenge of achieving tumor clearance from a control perspective. We demonstrate the potential effectiveness of an alternative CAR T cell activation strategy by modeling it as an external control input. This additional input complements the traditional activation mechanism driven by the presence of tumor cells. Our approach builds upon the model introduced in [15]. We propose a novel model and a backstepping analysis [16] to describe the effects of this control input on tumor clearance in terms of asymptotic stability. Additionally, we describe novel activation strategies for CAR T cells that are aligned with our model."
https://arxiv.org/html/2411.07887v1,Stochastic MPC for Finite Gaussian Mixture Disturbances with Guarantees,"This paper presents a stochastic model predictive control (SMPC) algorithm for linear systems subject to additive Gaussian mixture disturbances, with the goal of satisfying chance constraints. To synthesize a control strategy, the stochastic control problem is reformulated into an MPC problem. The reformulation begins by decoupling the mixture distribution and decomposing the system dynamics. Using stochastic simulation relations, we then redefine the stochastic control problem onto the resultant abstract system. Next, constraint tightening forms an MPC problem subject to finite disturbances. A branching control is introduced to solve the MPC problem. Finally, a controller refinement procedure determines a valid control strategy. Our contribution is an extension of the SMPC literature to accommodate Gaussian mixture disturbances while retaining recursive feasibility and closed-loop guarantees. We illustrate the retention of guarantees with a case study of vehicle control on an ill-maintained road.","Control theory, a fundamental discipline in engineering and applied mathematics, offers a broad spectrum of techniques, ranging from simple, intuitive methods to highly sophisticated and computationally intensive approaches [1]. Within this spectrum, stochastic model predictive control (SMPC) is a robust method for effectively managing chance constraints—set constraints that must be met with a specified probability—while addressing uncertainties within dynamical systems, often described by probability distributions [2]. SMPC has been effectively utilized in various applications, including vehicle path planning, air traffic control, building climate control, and operations research and finance [2]. SMPC approaches are typically classified into two broad categories: randomized and analytic approximation methods [3]. Randomized methods rely on generating realizations of disturbances, while analytic approximation methods generally seek to convert the stochastic control problem into a deterministic one, all while proving essential properties, including recursive feasibility. The literature on SMPC covers scenarios ranging from complete knowledge of system disturbances [4] to cases where only partial information is available [5]. Some approaches consider discrete distributions [6], while others handle continuous distributions [7]. For the subclass of convex unimodal continuous distributions, researchers have successfully reformulated stochastic control synthesis problems into deterministic ones [8, 9, 10, 11]. However, their approaches struggle when confronted with more complex distributions. Specifically, the assumption of convex unimodality is crucial for ensuring the closed-loop guarantees, though this assumption often fails in real-world scenarios. A more realistic model for such applications involves Gaussian mixtures, which can approximate any continuous distribution with arbitrary precision [12, 13]. Approximating distributions in the context of predictive control have been considered in [14] in which distributional robust data-enabled predictive control was used. Herein, a ball is constructed in the space of probability distributions centred around the uniform distribution of the sample set, which contains the actual distribution with a predefined probability [15]. The control strategy is developed under the assumption of the worst-case distribution scenario. Similarly, Gaussian mixtures in the context of predictive control have been considered in [16], which assumed non-linear dynamics subject to additive disturbances. Nevertheless, tractable implementation, deriving closed-loop guarantees, and establishing recursive feasibility remain open research problems. This paper aims to develop a control strategy for linear systems subject to additive Gaussian mixture disturbances with the purpose of satisfying chance constraints. The primary contribution of this paper is the extension of SMPC methods to handle additive Gaussian mixture distributions while preserving key properties such as recursive feasibility and closed-loop guarantees. To achieve this, we consider the approach depicted in Fig. 1. Herein, we reformulate the stochastic control synthesis problem into an MPC problem. To this aim, firstly, we fragment the system dynamics by employing disturbance decoupling, separating the mixture into discrete and continuous distribution components, and system decomposition, separating the dynamics into nominal and error parts. Next, the stochastic control problem is redefined onto the resultant abstract system through stochastic simulation relations. Afterwards, constraint tightening is utilized to obtain an MPC problem subject to finite disturbances. To solve the MPC problem, a branching control strategy is synthesized. Finally, by utilizing controller refinement, a valid control strategy on the original system can be obtained. Figure 1: An illustration of the stochastic MPC synthesis approach. The steps are given by: (1) redefine the control problem, (2) reformulate the control problem, (3) solve the MPC problem, and (4) refine the control strategy. The remainder of this paper is organized as follows. In Section II, the preliminaries and the problem statement are introduced. In Section III, the stochastic control synthesis problem is redefined onto the abstract system. In Section IV, the redefined control problem is reformulated into an MPC problem. In Section V, control implementation, recursive feasibility and closed-loop guarantees are established. Finally, Section VI considers a one-dimensional example of a vehicle maintaining position on an ill-maintained road."
https://arxiv.org/html/2411.07862v1,Iterative Learning Control with Mismatch Compensation for Residual Vibration Suppression in Delta Robots,"Unwanted vibrations stemming from the energy-optimized design of Delta robots pose a challenge in their operation, especially with respect to precise reference tracking. To improve tracking accuracy, this paper proposes an adaptive mismatch-compensated iterative learning controller based on input shaping techniques. We establish a dynamic model considering the electromechanical rigid-flexible coupling of the Delta robot, which integrates the permanent magnet synchronous motor. Using this model, we design an optimization-based input shaper, considering the natural frequency of the robot, which varies with the configuration. We proposed an iterative learning controller for the delta robot to improve tracking accuracy. Our iterative learning controller incorporates model mismatch where the mismatch approximated by a fuzzy logic structure. The convergence property of the proposed controller is proved using a Barrier Composite Energy Function, providing a guarantee that the tracking errors along the iteration axis converge to zero. Moreover, adaptive parameter update laws are designed to ensure convergence. Finally, we perform a series of high-fidelity simulations of the Delta robot using Simscape to demonstrate the effectiveness of the proposed control strategy.","Over the past decades, pick-and-place parallel robots have been widely used in food, electronics, packaging and other industries, due to their high speed and high acceleration motion capability [1, 2]. As typical representatives, Delta robots possess three translational degrees of freedom [3]. However, in order to reduce the weight of robots to achieve higher acceleration and decrease the energy costs, the links of the robots are made from lightweight materials, such as carbon fiber, which inevitably results in the deterioration of the residual vibration and eventually undermines the positioning accuracy of the robots. The occurrence of vibration makes it more difficult to achieve high performance trajectory tracking control and may cause instability in extreme circumstances. Therefore, designing an effective controller and suppressing the residual vibration are of great significance for the application of Delta robots as well as other parallel robots. Generally speaking, the approaches of vibration suppression can be divided into two categories, passive vibration suppression approaches [4, 5] and active vibration suppression approaches [6, 7, 8]. The passive approaches utilize additional damping materials to increase the damping ratio of the system. Although these kinds of methods can reduce the vibration effectively, the use of extra materials increases the manufacturing costs. On the contrary, the active approaches involve designing a control system to achieve trajectory tracking and vibration suppression simultaneously. The active approaches can be classified into feedback methods and feedforward methods. The feedback methods have to acquire real time vibration signals to form the closed-loop controllers [9, 10], which requires the installation of additional sensors to measure the vibration signals. The low-level controllers have to be changed and redesigned by the users, which may not be allowed for some commercial parallel robots. The feedforward methods do not change the existing low-level controllers but just modify the input reference trajectory based on the dynamic behavior of the systems, which are obviously easier for realization. Thus, controller design and residual vibration suppression can be addressed separately. Among the feedforward methods, the input shaping techniques are very common and useful tools to suppress the residual vibration [11, 12, 13]. The principle of input shaping techniques is to modify the reference trajectory, which results in a deviation between the original reference trajectory and the modified trajectory. Therefore it may not be suitable for those task conditions where the reference trajectory is designed strictly and cannot be adjusted. However, for all high-speed parallel robots including Delta robots used to carry out pick-and-place operations, only start and end points are crucial. In order to achieve satisfactory working accuracy, numerous control methods had been proposed for Delta robots and other parallel robots in the past decades, including sliding mode control [14, 15], robust control [16, 17], intelligent control [18, 19], etc. However, all these mentioned control strategies only depend on the error information of the current operation period. For those applications where robots repeatedly perform the same tasks, the historical error information is a valuable resource to improve the control performance. Previous research has illustrated that the trajectory has a profound impact on the vibration of Delta robots. Therefore, it is crucial to ensure that the robot can track the reference trajectory to prevent unexpected vibration. Delta robots are used to perform the pick-and-place operation, which is repetitive along the fixed trajectory and within the same duration time. Under this circumstance, iterative learning control (ILC) [20, 21] as a suitable control approach to achieve a high performance and high accuracy trajectory tracking, has been widely employed in various fields, such as high-speed trains, precision motion stages, etc [22, 23]. From the few existing applications of ILC on parallel / Delta robots, the restrictions of resetting condition and repetitive trajectory are overcome in the ILC proposed in [24]. It has been further developed in [25] to achieve an adaptive robust proportional-derivatives ILC strategy in which a robust term is introduced to compensate the repetitive and nonrepetitive disturbance. However, the above robust ILC methods for Delta robots require the norm of disturbance to satisfy certain relationships, which causes difficulties in their application. Moreover, the selection of control gains is complexly restricted due to the requirement of stability guarantee. Uncertainty compensation is in this case a suitable approach to address this problem, particularly when the uncertainty arises from model mismatch. A model mismatch compensation method is proposed in [23] for precision motion stages, in which the Gaussian process regression is used to predict the unknown model mismatch, and the error is compensated using a repetitive control approach. To improve modeling, the coupling between the mechanical and electrical systems, which profoundly influences the dynamic behavior of robots [26], needs to be accounted for. Thus, for completeness, the model of PMSM should be integrated into the dynamic model of Delta robots. Furthermore, in order to guarantee the working safety, the robots’ velocity needs to be constrained. To introduce state constraints in the control problem, an adaptive neural network control algorithm is proposed in [27] for a wheeled mobile robot with velocity constraints, in which the barrier Lyapunov function (BLF) is introduced to guarantee the velocity constraints. The derivation of convergence guarantees for the BLF is inspired by [27] in this paper. For parallel robots, especially for Delta robot, studying the effect of velocity constraints control is not common. The excessive speed not only requires more energy consumption but also increases the risks associated with the operation. Therefore, it is of great importance to take angular velocity constraints into account. In this paper, we propose a control strategy combining input shaping techniques and ILC for Delta robot with PMSM and angular velocity constraints. The main contributions of this work are listed as follows. 1. We establish an integrated dynamic coupling mathematical model of Delta robots, where the flexibility of joints and links, as well as the dynamics of PMSM are taken into account. Based on the established model, we design an optimal input shaper by two global optimization objectives, which can suppress the residual vibration effectively. 2. According to the concept of the singular perturbation method (SPM), we propose an adaptive mismatch-compensated iterative learning controller (AMCILC) for the rigid-body motion coordinates of a Delta robot. The iterative learning method can effectively address the repetitive tasks of Delta robots. We introduce the FLS to approximate the model mismatch including the damping term of PMSM. By using two designed adaptive iterative update laws, we employ a Barrier Composite Energy Function (BCEF) to prove the convergence property of the tracking errors, in which the barrier Lyapunov function can ensure the velocity constraints to be satisfied. 3. Based on Simscape’s multi physical domain coupling function, we establish a high-fidelity simulation model of a Delta robot system to verify the performance of the proposed input shaper-based adaptive mismatch-compensated iterative learning controller (IS-AMCILC). The rest of this paper is summarized as follows. The coupling dynamic mathematical model of a Delta robot is established and the optimal input shaper is designed in Section II. Then, in section III, the design and stability proof of IS-AMCILC are conducted in which the velocity constraints are taken into account. A series of simulations is performed to verify the effectiveness of the proposed control strategy in Section IV. Finally, conclusions are drawn in Section V."
https://arxiv.org/html/2411.07830v1,Singularity-Avoidance Control of Robotic Systems with Model Mismatch and Actuator Constraints,"Singularities, manifesting as special configuration states, deteriorate robot performance and may even lead to a loss of control over the system. This paper addresses the kinematic singularity concerns in robotic systems with model mismatch and actuator constraints through control barrier functions (CBFs). We propose a learning-based control strategy to prevent robots entering singularity regions. More precisely, we leverage Gaussian process (GP) regression to learn the unknown model mismatch, where the prediction error is restricted by a deterministic bound. Moreover, we offer the criteria for parameter selection to ensure the feasibility of CBFs subject to actuator constraints. The proposed approach is validated by high-fidelity simulations on a 2 degrees-of-freedom (DoFs) planar robot.","I INTRODUCTION Robots are becoming increasingly prevalent across various industries, such as robotic arms used in industrial production and parallel-mechanism based legged robots. Singularities, arising from specific geometric relationships between links, can cause robots to lose or gain one or more DoFs, potentially leading to a loss of control over the system. Therefore, avoiding singular configurations is crucial to ensure safe operation for robotic systems. Directly modifying the reference trajectories is an effective method to avoid singularities. For example, a linear weighting method based post-processing non-singular trajectory generation method was proposed for a 5-DoFs hybrid machining robot in [1]. An algorithm based on output twist screws was presented in [2] to address type II singularity in parallel mechanisms by modifying trajectories. However, the trajectory modification method lacks sufficient flexibility, as one has to repeat the process for different trajectories. Moreover, non-singular reference trajectories are unable to guarantee robots not entering singularity regions, due to the presence of control errors. In recent years, advancements in safe optimization enabled by CBFs offer a promising alternative solution to the singularity avoidance problem. CBFs are powerful tools for handling various constraints, which enable their application in numerous safety-critical fields [3]. For example, one can leverage multiple CBFs to coordinate connected and automated agents at intersections [4], where the collision avoidance CBFs and velocity CBFs have to be jointly feasible under input constraints. A CBFs design methodology is proposed in [5] for Euler-Lagrange systems with position, velocity and input constraints. CBFs can be also used to ensure the safety of learned models for control in robotic systems [6], and to impose safety-critical constraints in a continuous-time trajectory generation process [7]. In order to handle model mismatch, robust CBFs are developed by introducing a compensation term based on the bound of uncertainty [8]. GP-based learning methods are another suitable method to tackle model mismatch [9], as they provide a quantification of the prediction uncertainty, which could be used to obtain the corresponding bound [10, 11]. There is little research regarding CBFs in addressing singularity concerns. In [12], CBFs were utilized to tackle singularity problem in passivity-based control. However, the feasibility of CBFs in [12] is based on the assumption that joints can provide unbounded torques, which does not precisely correspond to the capabilities of motors in practice. In addition, model mismatch has not been addressed in [12]. This paper proposes a methodology for CBFs construction to address singularity avoidance problem in robotic systems with model mismatch and subject to actuator constraints. The primary contributions of this work are summarized as follows: (i) the theoretical guarantee of the feasibility of CBFs with model mismatch and actuator constraints is obtained, as well as the parameter selection criteria is provided, (ii) the model mismatch is learned using GP regression combined with a deterministic error bound, and (iii) the proposed approach is validated by high-fidelity 2 DoFs planar robot simulations on Simscape. Notation: \mathbb{R} and \mathbb{R}_{\geq 0} denote the set of real, non-negative real numbers, respectively. The Euclidean norm is denoted by \left\|\cdot\right\|. \mathbb{N}_{n} denoting the set of natural numbers \{1,\cdots,n\}. The matrix inequality A\leq B for matrices A and B means that the matrix B-A is positive semidefinite. e_{i} denotes the ith column of nth-order identity matrix I_{n}."
https://arxiv.org/html/2411.07805v1,Effects of charging and discharging capabilities on trade-offs between model accuracy and computational efficiency in pumped thermal electricity storage,"The increasing need for energy storage solutions to balance variable renewable energy sources has highlighted the potential of Pumped Thermal Electricity Storage (PTES). In this paper, we investigate the trade-offs between model accuracy and computational efficiency in PTES systems. We evaluate a range of PTES models, from physically detailed to simplified variants, focusing on their non-linear charging and discharging capabilities. Our results show that while detailed models provide the most accurate representation of PTES operation by considering mass flow rate (\dot{m}) and state of charge (SoC) dependencies, they come at the cost of increased computational complexity. In contrast, simplified models tend to produce overly optimistic predictions by disregarding capability constraints. Other approximated model variants offer a practical compromise, balancing computational efficiency with acceptable accuracy. In particular, models that disregard \dot{m}-dependency and approximate nonlinear SoC-dependency with a piecewise linear function achieve similar accuracy to more detailed models but with significantly faster computation times. Our findings offer guidance to modelers in selecting the appropriate PTES representation for their investment models.","The rapid growth of variable renewable energy (VRE) sources, such as solar and wind, presents both opportunities and challenges for the decarbonization of energy systems [19, 15, 5]. While these sources are essential for reducing greenhouse gas emissions, their variability introduces reliability and cost concerns for grid operators [45, 37]. Energy storage will play a significant role in balancing supply and demand, enhancing grid stability, and reducing the overall costs associated with integrating large amounts of VRE into power systems [26, 40]. Among the various energy storage options, pumped thermal electricity storage (PTES) is emerging as a particularly promising solution for long-duration energy storage (LDES). PTES (also known as a ‘Carnot battery’, ‘pumped heat electricity storage’, ‘Brayton PTES’, and ‘Joule-Brayton PTES’ in the literature) stores electricity as heat in insulated thermal reservoirs using suitable media such as solid packed beds or liquid storage tanks [54]. During the charging phase, an electrically driven heat pump delivers heat to a hot store, while during the discharging phase, a heat engine converts the stored heat back into electrical energy [29]. The thermodynamic and electromechanical principles underlying PTES technology are well-established and reliable, with numerous demonstration systems currently under development. One example commissioned by Newcastle University is a grid-scale PTES demonstrator with packed beds, a nominal power capacity of 150 \text{kW}_{\text{e}} and an energy storage capacity of 600 \text{kWh}_{\text{e}}, designed for an 8-hour storage cycle [4]. Additionally, Malta Inc. [27] is commercially developing a 100 MW grid-scale PTES system, based on the concept proposed by Laughlin [22], which uses molten salt and coolant reservoirs to support storage cycles ranging 8-200+ hours. PTES is emerging as a competitive alternative to pumped hydro energy storage due to its reduced geographical constraints while still having a long operational life and low cost per kWh [42, 33, 12, 41]. Moreover, PTES offers the advantage of sector coupling, enabling the transfer of surplus energy from VRE sources to residential heating, cooling, and industrial heating sectors [11, 44, 47]. This capability avoids the inefficiencies of converting electricity to a stored energy and then back to electricity for heating or cooling, positioning thermal storage as a cost-effective and efficient solution for large-scale deployment. Accurately modeling PTES systems is essential for determining their value within grid systems [23, 6]. Capacity expansion models (CEMs) optimize the design of electricity grids given cost and performance details of generation, storage, and transmission technologies; emission limits and other policies; and electricity demand time series [39]. CEMs have been used to study the value of PTES [14] and LDES [40]. However, the results of CEM optimizations are contingent on the available technologies being described accurately. Current descriptions of thermal storage in CEMs are very simple, only differentiating technologies based on their cost, leakage rates, and exergetic efficiency. While these factors are important, they do not fully capture the range of operational constraints that can limit thermal storage performance. In addition, current models use the same (limited) operational constraints for all thermal storage technologies. This makes it impossible to evaluate the benefits of a thermal storage technology with higher costs but greater operational flexibility. CEMs have used these simple models of thermal storage thus far because the important additional details are non-linear and most CEMs are linear programs of mixed-integer linear programs. Detailed non-linear numerical models of PTES with packed beds have been developed which take into account detailed heat transfer between the working fluid and bed material [7, 51, 50, 49, 31, 16, 28, 52, 30]. Based on these models, techno-economic analyses of different PTES system variants have been conducted, focusing on optimizing system design, evaluating cost-effectiveness, round-trip efficiency, and operational performance to identify the most suitable configurations for large-scale energy storage [54, 29, 13, 53, 34]. However, these non-linear models have not been incorporated into CEMs as it would make the underlying optimization non-linear and much slower to run or require smaller models. Recent research has shown that CEMs must consider years or decades of data to produce robust grid designs [38], so the preference has been to use simpler technology representations and longer time series. In this paper, we show that it is possible to incorporate non-linear details of PTES operation into linear CEMs and that to not do so misrepresents their value and role in decarbonized grids. There are several additional operational constraints which could be included in a description of PTES. One minor constraint is that PTES systems require startup time to reach operational temperatures (e.g., Malta’s system requires approximately 10 minutes for start-up [43]). Station loads are also necessary to manage the mass flow rate of the working fluids and the operation of storage block segments, introducing added complexity and cost [4]. A major constraint CEMs have yet to consider fully is how the performance of a PTES changes with its state of charge (SoC), in particular its charging and discharging capability. Here, we draw a distinction between capacity and capability, where capability is the instantaneous charging or discharging power the system is capable of while the capacity is the design maximum. In PTES, charging and discharging power depends on the temperature difference between the working fluid and the storage media. During the charging phase, hot working fluid is injected into the top of the tank, initially heating the upper storage media while the bottom remains at its starting temperature. This heat transfer process creates an axial temperature profile, with a thermal front marking the transition. As the thermal front reaches the end of the tank, the temperature difference decreases, reducing the system’s charging capability. During the discharging phase, the cycle reverses: cold working fluid is injected from the bottom, the thermal front moves upward, and the discharging capability decreases as it approaches the other end of the tank. The shape of the thermal front also depends on the mass flow rate of the working fluid, and thus so does corresponding charging or discharging power. This makes the charging and discharging capabilities of PTES a non-linear function of the mass flow rate and SoC. While this complex behavior can be modeled using wave propagation and solved through the finite volume method [51, 49], incorporating such detailed models into dispatch and investment optimization significantly increases computational complexity, leading to longer runtimes and memory usage. Alternative ways to represent heat transfer in PTES have been implemented in techno-economic analyses [25], but Sepulveda et al. [40] and Ghilardi et al. [14] did not include these details in their analysis due to computational limitations. To our knowledge, no CEMs in the literature combine charging and discharging capabilities of PTES with a high temporal resolution and long horizons. In this paper, we developed several increasingly accurate representations of PTES for use in CEMs and examined the trade-offs between their runtime and accuracy. The computational modeling community has addressed these trade-offs and challenges in representations of other generation and storage technologies [3, 10, 21, 9]. For instance, Fälth et al. [8] investigated hydropower models with varying levels of physical and technological detail. By exploring different modeling approaches for charging and discharging capabilities of PTES, we aim to provide insights into the minimum technical detail PTES models must have to give credible results and strategies to balance model accuracy with practical usability for modellers who desire additional detail. This work has the following research aims: 1. To examine how much a commonly simplified PTES model deviates from more physically accurate models that account for mass flow rate and SoC dependencies in charging and discharging capabilities. 2. To explore the trade-offs between accuracy and computational time in different PTES models, with a focus on various approaches for modeling charging and discharging capabilities. 3. To evaluate the scalability of the models when integrated with a larger investment model, utilizing the GenX framework [20, 1]."
https://arxiv.org/html/2411.07686v1,Data-Driven Graph Switching for Cyber-Resilient Control in Microgrids,"Distributed microgrids are conventionally dependent on communication networks to achieve secondary control objectives. This dependence makes them vulnerable to stealth data integrity attacks (DIAs) where adversaries may perform manipulations via infected transmitters and repeaters to jeopardize stability. This paper presents a physics-guided, supervised Artificial Neural Network (ANN)-based framework that identifies communication-level cyberattacks in microgrids by analyzing whether incoming measurements will cause abnormal behavior of the secondary control layer. If abnormalities are detected, an iteration through possible spanning tree graph topologies that can be used to fulfill secondary control objectives is done. Then, a communication network topology that would not create secondary control abnormalities is identified and enforced for maximum stability. By altering the communication graph topology, the framework eliminates the secondary control layer’s dependence on inputs from compromised cyber devices helping it achieve resilience without instability. Several case studies are provided showcasing the framework’s robustness against False Data Injections and repeater-level Man-in-the-Middle attacks. To understand practical feasibility, robustness is also verified against larger microgrid sizes and in the presence of varying noise levels. Our findings indicate that performance can be affected when attempting scalability in the presence of noise. However, the framework operates robustly in low-noise settings.","Microgrids are cyber-physical systems with a hierarchical control framework that involves primary and secondary layers for voltage/frequency control and power-sharing regulations [1]. The microgrid secondary control layer is responsible for set-point tracking and relies on communication devices for nominal operations [2]. This makes the system vulnerable to stealth attacks compromising communication devices and manipulating data flow patterns [3]. Under the attacks’ influence, this layer computes erroneous control signals that propagate further to jeopardize nominal operation [4]. A necessary requirement for convergence of secondary control inputs is to ensure a spanning tree in the cyber (communication graph) topology [5]. If this spanning tree relies on compromised network devices, then it would feed untrustworthy inputs to the secondary controller, forcing it to compute erroneous control signals [6]. Hence, it is essential to ensure that the communication graph topology on which the microgrid secondary control layer is dependent is free from manipulations in the cyber layer [7, 8]. To achieve the objective, this paper presents a physics-guided Artificial Neural Network (ANN) framework that can identify the trustworthiness of the default communication topology by estimating abnormal secondary control outputs that it might create within the microgrid network. In this context, physics-guided means that the rationale behind using the ANN is rooted in the principles of (domain-specific) microgrid control dynamics. The microgrid local parameters are normally synchronous in the steady state as this is an essential objective of cooperative control action. However, [9] has already established that DIAs and other attack vectors like jamming lead to the disruption of cooperative synchronization [10]. This may be reflected as high error outputs from local secondary controllers. Hence, we seek to estimate the total sum of these outputs (via ANN-assisted regression) before the attack propagates to the secondary control layer. If the total sum is estimated to be unconventionally higher than the expected value (where the expected value is determined from microgrid steady-state behaviors during normal operation), a trigger is generated indicating the possible presence of a cyberattack. On the generation of a trigger, the proposed ANN model iterates through possible spanning tree graph topologies (each of which relies on a distinct set of network devices) to identify a topology that can achieve nominal functionality in a trustworthy manner. This topology is then enforced in the microgrid environment isolating and mitigating the cyberattack. We provide several case studies highlighting the proposed method’s resilience against False Data Injection (FDI) [11] and Man-in-the-Middle (MITM) attacks [12]. We also analyze the performance of the proposed framework when scaled up to larger microgrid sizes and in the presence of varying levels of noise."
https://arxiv.org/html/2411.07642v1,Safety Filter Design for Articulated Frame Steering Vehicles In the Presence of Actuator Dynamics Using High-Order Control Barrier Functions,"Articulated Frame Steering (AFS) vehicles are widely used in heavy-duty industries, where they often operate near operators and laborers. Therefore, designing safe controllers for AFS vehicles is essential. In this paper, we develop a Quadratic Program (QP)-based safety filter that ensures feasibility for AFS vehicles with affine actuator dynamics. To achieve this, we first derive the general equations of motion for AFS vehicles, incorporating affine actuator dynamics. We then introduce a novel High-Order Control Barrier Function (HOCBF) candidate with equal relative degrees for both system controls. Finally, we design a Parametric Adaptive HOCBF (PACBF) and an always-feasible, QP-based safety filter. Numerical simulations of AFS vehicle kinematics demonstrate the effectiveness of our approach.","Articulated Frame Steering (AFS) vehicles are widely used in applications where maneuverability is critical, such as off-road environments, construction sites, and other terrains with numerous obstacles and unsafe zones. In these scenarios, the control system must ensure safe navigation, avoiding collisions and other hazards. A key challenge in designing controllers for AFS vehicles is achieving safety while managing the nonlinear dynamics and complex interactions inherent to AFSs. This challenge becomes more complex when we consider the dynamics of hydraulic actuators, which are commonly used in heavy-duty machinery due to their high power-to-weight ratio and efficiency. In large-scale systems, hydraulic actuators introduce significant dynamics, notably input delays, where the hydraulic response lags behind the commanded control input. Such delays increase the risk of safety violations, as delayed actuation can result in collisions with obstacles. To address safety challenges, researchers have increasingly employed Control Barrier Functions (CBFs), which are effective for enforcing safety constraints by ensuring system trajectories remain within safe bounds [1]. For systems with higher relative degrees, CBFs have been extended to High-Order Control Barrier Functions (HOCBFs) [2]. A common approach to implementing HOCBFs is to use them as safety filters over nominal controllers, applying a minimally modifying Quadratic Program (QP) that adjusts potentially unsafe nominal controls to satisfy HOCBF conditions. However, due to actuation limits in mechanical systems like AFS vehicles, this approach can sometimes lead to infeasibility in the QP. Parameter Adaptive HOCBFs (PACBFs) have been introduced to address these infeasibility issues and enhance reliability [3]. Figure 1: Safe goal reaching mission for an AFS vehicle considering actuator dynamics using our PACBF-based safety filter. In this paper, we design a safe controller for AFS vehicles using the PACBF framework. Figure 1 shows the successful application of our developed method on an AFS vehicle in a simulation environment. The main contributions of this work are as follows: • We derive general equations of motion for AFS vehicles, incorporating actuator dynamics alongside the kinematic equations of AFS vehicles. • We propose a novel HOCBF candidate that maintains equal relative degrees with respect to both control inputs of AFS vehicles. • Based on the proposed HOCBF candidate, we design a QP-based safe controller for AFS vehicles, ensuring feasibility through PACBFs. • We validate our method through the implementation of it on an AFS vehicle in a simulated environment. I-A Related Works AFS vehicles have unique kinematic and dynamic properties due to their central articulation. Early research, such as [4], focused on simplified kinematic models to examine the effects of articulation on path-following and stability. Delrobaei et al. [5] designed a Lyapunov function for AFS vehicles, while later studies addressed non-holonomic constraints and the coupling between steering and vehicle dynamics [6]. Despite these efforts, prior works primarily target control without fully addressing the complexities of designing safety controllers for AFS vehicles. Various studies have examined actuator dynamics and their effects on CBF-based safe controllers. Jankovic [7] introduced time delays to inputs for linear systems, while Seiler et al. [8] combined CBFs with integral quadratic constraints (IQCs) to create a robust safety filter. Molnar et al. [9] proposed Environmental Control Barrier Functions (ECBFs) to account for environmental changes during delayed responses, and Zhang et al. [10] developed an extended state observer to anticipate uncertainties and delays within a QP framework. Additional approaches have tackled actuator dynamics and robustness in safe control. For example, control-dependent barrier functions address low-level dynamics [11], Buch et al. [12] designed a robust CBF for uncertain inputs, and Abel et al. [13] presented a state predictor to handle input delays. Despite these contributions, a general solution that provides a QP-based safety controller with feasibility guarantees for AFS vehicles remains unaddressed. The rest of the paper is organized as follows. Theoretical background and materials are given in Section II. The methodology of our solution is outlined in Section III. In Section IV, we give simulation results on sample systems and discuss the results. Finally, section V concludes the paper."
https://arxiv.org/html/2411.07640v1,Reducing Conservativeness of Controlled-Invariant Safe Sets by Introducing a Novel Synthesis of Control Barrier Certificates,"Finding a controlled-invariant safe set for a given system with state and control constraints plays an important role in safety-critical systems. Current methods typically produce conservative solutions. In this paper, we introduce a method to generate controlled-invariant safe sets for nonlinear polynomial control-affine dynamical systems by using the notion of Control Barrier Certificates (CBCs). To this end, we relax CBC conditions into Sum of Squares (SOS) constraints, to be solved by an SOS program. We first assume a controlled-invariant safe set (although small) exists for the system. We then propose a method to iteratively enlarge the safe set. We theoretically prove that our method enlarges the safe set in each iteration. We also demonstrate the efficacy of our method through simulated numerical examples in 2D and 3D for single and multi-input dynamical systems and empirically show that our method produces a larger controlled-invariant safe set in these examples, compared to a state-of-the-art technique using Control Barrier Function (CBF).","Safety-critical control systems are extensively used in various fields, including adaptive cruise control [1], aerospace [2], and robotics [3]. These systems demand rapid and reliable control methods that ensure both safety and optimality. Barrier Certificates (BCs) have been proposed as means to guarantee safety by identifying invariant subsets within a system’s safe states [4]. While BCs have been successfully applied to nonlinear autonomous systems, both deterministic and stochastic, there has been limited work extending these methods to control systems with input constraints. Control Barrier Functions (CBFs) were proposed by Ames et al. [5] extending barrier certificates for control systems. However, in most recent works, CBFs are often used in not rigorous manner. Such heuristically chosen barrier functions may not satisfy the criteria of valid CBFs anymore in the presence of input constraints in run time when solving a Quadratic Programs (QPs) leading to infeasibility mostly caused by neglected input limits [6]. Figure 1: Synthesized safe set for the system of example 1 with our method vs. method proposed in [7]. The figure also shows various random trajectories starting from different points on the safe set boundary, all successfully returning inside while satisfying input constraints. To avoid this limitation, optimization-based approaches have been proposed for synthesizing CBFs considering input constraints. Among them, one of the popular methods is to use Sum-Of-Squares Programs (SOSPs), which has gained significant momentum in CBF synthesis [6]. CBC-based studies impose constraints only at the boundary of the safe set (the controlled-invariant set boundary), while CBFs use class-\kappa functions to extend the constraints to the entire safe set. As expected, results in this method can be overly conservative, depending on the choice or design of the class-\kappa function. Therefore, Control Barrier Certificates (CBCs) design can lead to less conservative SOS constraints, thus larger controlled-invariant safe sets. In this work, we use CBC constraints to synthesize controlled-invariant safe sets. The Summary of our work is shown in Fig. 1. I-A Contributions The main contributions of this work are as follows: • We revisit the concept of barrier certificates and propose a systematic method to synthesize CBCs using SOS programming for polynomial control-affine systems. • We develop a novel algorithm that iteratively enlarges safe sets in the presence of input constraints, leading to larger controlled-invariant safe sets compared to a benchmark method. • We demonstrate the efficacy of our approach on single-input 2D and multiple-input 3D dynamical systems. I-B Related Works The idea of safety certificate synthesis as a convex optimization problem was pioneered by Prajna et al. [4], who defined the BCs for hybrid dynamical systems and was later extended to the control systems with actuator constraints by Wang et al. [8]. Recently, safety certification methods for synthesizing CBFs have focused on the following three approaches: a) Hamilton-Jacobi reachability analysis, b) learning-based approaches, and c) SOSP-based approaches. It is well known that the Hamilton-Jacobi reachability methods can compute the backward reachable sets [9], but the time complexity of these algorithms grows exponentially with respect to the system size. The learning-based methods have been explored in the synthesis of safe controllers [10]. However, this approach involves long training times and cannot guarantee safety in all possible cases. SOSP-based methods are by comparison one of the more practical alternatives [4], providing solutions in polynomial time and also allowing safety certification even with input constraints. While SOSP-based approaches have been successful, the generated safe set with these methods needs to be enlarged iteratively to obtain the largest possible safe sets. Dai et al. recently proposed an SOSP that uses the contrapositive of the CBF derivative condition to synthesize controlled-invariant safe sets [7]. They then maximize the largest inner ellipsoid to enlarge the safe set. However, limiting the enlargement to ellipsoids restricts the method in complex environments, such as those with multiple obstacles. We formulate a safe set synthesis based on the barrier certificate concept presented in [4] and propose a new enlargement method that can work in environments with various obstacle formations. The rest of the paper is organized as follows. Theoretical background and materials are given in Section II. The proposed optimization algorithm for CBC synthesis is outlined in Section III. In Section IV, we give simulation results on sample systems and discuss the results. Finally, section V concludes the paper."
https://arxiv.org/html/2411.07636v1,"Node Reliability: Approximation, Upper Bounds, and Applications to Network Robustness","This paper discusses the reliability of a graph in which the links are perfectly reliable but the nodes may fail with certain probability p. Calculating graph node reliability is an NP-Hard problem. We introduce an efficient and accurate Monte Carlo method and a stochastic approximation for the node reliability polynomial based solely on the degree distribution. We provide the formulas for the node reliability polynomial of both Erdős–Rényi graphs and Random Geometric graphs. The phase transition in the node reliability of Erdős–Rényi graphs such as are also discussed. Additionally, we propose two increasingly accurate upper bounds for the node reliability polynomial solely based on the graph’s degree distributions. The advantages and disadvantages of these two upper bounds are thoroughly compared. Beyond the computation of node reliability polynomials, we also estimate the number of cut sets and present a solution to the reliability-based network enhancement problem.","Reliability research in network science is concerned with the estimation of the probability that the residual network remains operational after the failure of some components [3]. In 1956, Moore and Shannon [2] proposed a probabilistic model for network reliability. Based on the types of component that can fail, network reliability can be classified into two categories: • Network reliability w.r.t. link failures: defined as the probability that the nodes of graph G remain connected if each link is operational with probability p, assuming the nodes of the graph are perfectly reliable [4]. This type of network reliability can be expressed as a so-called reliability polynomial: \text{Rel}_{G}\left(p\right)=\sum_{j=0}^{L}F_{j}(G)\left(1-p\right)^{j}{p_{1}}% ^{L-j}, (1) where F_{j}(G) is the number of sets of j links whose removal leaves G connected, and F_{0}(G)=1. • Network reliability w.r.t. node failures: defined as the probability that the operational nodes of graph G remain connected if each node is operational with probability p, assuming the links of the graph are perfectly reliable [4]. This type of network reliability can be expressed as the node reliability polynomial: \text{nRel}_{G}(p)=\sum_{k=0}^{N}S_{k}(G){p}^{k}(1-p)^{N-k}, (2) where S_{k}(G) denotes the number of induced connected subgraphs with k nodes. Most studies on network reliability focus on link failures. This paper will focus on node failures. The problems of computing the reliability polynomial \text{Rel}_{G}(p) and node reliability polynomial \text{nRel}_{G}(p) are NP-hard [1, 5, 7]. Closed-form analytic expressions for the node reliability polynomial only exist for some specific graph topologies [6]. We give examples in Appendix A. Various Monte Carlo methods give accurate estimations for the node reliability polynomial, but suffer from a high computational complexity [8, 9, 10]. The reliability polynomial is a useful tool in network theory, used to characterize network structures and to guide optimal design. It captures crucial information about a network’s connectivity by encoding all possible cut sets—the sets of links or nodes whose removal would disconnect the network. The reliability polynomial thus serves as a comprehensive measure of a network’s global robustness. Networks with higher values of the reliability polynomial, under the same operational probability p, tend to be more resilient to disconnection, allowing comparisons between different network topologies. In addition to a structural analysis, the reliability polynomial plays a key role in network design [3, 19, 20]. The reliability polynomial can be used to optimize network reliability by identifying critical nodes or links whose addition or removal significantly affects the overall reliability [20]. For instance, adding links can enhance reliability in communication networks by increasing redundancy, whereas removing specific links can effectively contain the spread of diseases[20]. The reliability polynomial finds practical applications in fields such as communication networks, infrastructure systems, and public health [20, 24, 25]. The reliability polynomial helps to optimize network performance and resilience [21]. This paper first introduces a Laplace approximation for the node reliability polynomial in Section II. In Section III, we propose a Monte Carlo method for node reliability polynomials. The proposed Monte Carlo method is inspired by a recent fast approach designed for network reliability polynomials [18]. Additionally, the Monte Carlo method is combined with the Laplace approximation to develop a new hybrid approach, referred to as the Laplace Monte Carlo method. Section IV introduces a stochastic approximation for the node reliability polynomial. The relation between the reliability polynomial \text{Rel}_{G}(p) and the node reliability polynomial \text{nRel}_{G}(p) is also analyzed in Section IV. In Section V, we give the formulas of the node reliability polynomial for the Erdős-Rényi graph and random geometric graph. The intersection of node reliability polynomials for different Erdős-Rényi graphs and the phase transition of the node reliability for Erdős-Rényi graphs are also analyzed. Two different kinds of upper bounds for the node reliability polynomial are given in Section VI. Section VII discusses the practical applications of the network reliability."
https://arxiv.org/html/2411.07570v1,"Constructive RNNs:
An Error-Recurrence Perspective
on Time-Variant Zero Finding Problem Solving Under Uncertainty","When facing time-variant problems in analog computing, the desirable RNN design requires finite-time convergence and robustness with respect to various types of uncertainties, due to the time-variant nature and difficulties in implementation. It is very worthwhile to explore terminal zeroing neural networks, through examining and applying available attracting laws. In this paper, from a control-theoretic point of view, an error recurrence system approach is presented by equipping with uncertainty compensation in the pre-specified error dynamics, capable of enhancing robustness properly. Novel rectifying actions are designed to make finite-time settling so that the convergence speed and the computing accuracy of time-variant computing can be improved. Double-power and power-exponential rectifying actions are respectively formed to construct specific models, while the particular expressions of settling time function for the former are presented, and for the latter the proximate settling-time estimations are given, with which the fixed-time convergence of the corresponding models is in turn established. Moreover, the uncertainty compensation by the signum/smoothing-signum techniques are adopted for finite-duration stabilization. Theoretical results are presented to demonstrate effectiveness (involving fixed-time convergence and robustness) of the proposed computing schemes for the time-variant QP problem solving.","Recurrent neural networks (RNNs) have architecture with feedback loops, offering a tool for online solving problems in science and engineering [1, 2, 3]. As an alternative to analog computing, zeroing neural networks (ZNNs, see [4, 5, 6] and the references therein), are of RNN-like structure aiming to the time-variant problem solving, where with the pre-specified error dynamics, the existence and global stability of solutions of the designed neural networks are guaranteed. Efficient models were particularly designed and implemented for various time-variant problems including matrix inversion and pseudoinverse, linear/nonlinear matrix equations, matrix inequalities, quadratic/nonlinear programming, etc. The theoretical solutions of time-variant problems change with time and the convergence performance is the key to solve them. It is highly desirable to achieve the perfect result, namely, zero-error convergence over the entire time interval. However, theoretical solutions on an initial interval are indeed difficult to be obtained in the presence of initial errors. It is realistic for an RNN in realtime computation that a solution is available after certain initial interval. Fortunately, the way for RNN designs to get out of the difficult situation is to apply the finite-time stability theory. Finite-time convergent ZNN models enable to provide accurate solutions after the settling time. As a supplement to most of existing results guarantee asymptotical stability and exponential stability, the finite-time convergent computing methods have received increased attention, and early works were found in [7, 8, 9]. The signum unit was employed in [7] for switching the neural network structure; the hard limit activation function was used in [8]; and the sign-bi-power one was suggested in [9]. The extensive and comprehensive studies on ZNN models are found in [10, 11, 12, 13, 14], considering the fact that real-time performance is highly demanded for time-variant problems in practice. Very recently, various types of activation functions have been summarized in [15], where the asymptotically/finite-time convergent ZNN models were examined for characterizing the error evolution and its attractiveness. For the time-variant problem solving, one would like to achieve the prescribed convergence performance, which could dramatically improve the computing accuracy. Fixed-time stability is a contemporary concept assuring an upper bound on the settling time function which is independent of initial conditions. The concept of predefined-time stability is so helpful that the settling time is pre-specified and adjustable. Giving an estimation of the bound on the settling-time function of the system undertaken, the system can be re-constructed to realize predefined time stability with predefined settling time proportional to the inverse of the estimation. We refer interested readers to the recent literature [16, 17] and the references therein. Both stability concepts are especially useful to the time-variant computing problem solving, because the exact solution can be obtained definitely after the pre-specified instant. However, the estimations for settling time reported in many related works are conservative, due to the estimate being much larger than the actual one typically for the double-power systems. The closed-form expressions of settling time functions of the typical nonlinear systems, among others, were presented by means of special functions in [18, 19, 20], and the tight bound on settling time was given for exact estimation. In [21], the closed-form settling time function of two-phase systems was obtained, which still admits fixed-time attractors. It is also desirable for neural network designs that achieve the fixed-/predefined-time convergence. The reported ZNNs can realize such computing performance so that theoretical solutions can be achieved in a fixed time [13, 14, 22, 23]. Many existing works focus on constructing the error dynamics and its convergence analysis, especially for speeding up the convergence in the absence of uncertainty. The robustness of neural networks is crucial for analog computing, since external disturbances and noises inevitably exist in the implementation. Varying-parameter ZNNs were proposed in [24], and it was show that by monotonically increasing the parameters, these ZNNs are of super exponential convergence and the residual errors converge to zero even under perturbation situations. In [25], Power-type varying-parameters were introduced in forming the error dynamics to address the robustness issue, by which the computing error caused by possible differentiation and implementation errors can be made arbitrarily small through simply increasing the design parameters. Integral-enhanced computing schemes have been shown to possess robustness and achieve the tolerance of constant disturbances, for solving matrix inversion and constrained nonlinear optimization [26, 27, 28]. By simultaneously considering finite-time convergence and robustness, in [29], PID-type error dynamics was constructed for solving Lyapunov equation in the presence of various kinds of additive noises. In [30, 31, 32], both linear and signum terms of the designed error dynamics were shown to be effective in handling bounded nonvanishing noises. From the above mentioned, a control-theoretic approach is expected to contribute to a sound design rationale for enhancing robustness of ZNN models, not only aiming at specific types of uncertainties. The problem to find a zero of a given time-invariant function was addressed from the point of view of feedback control, in [33], showing how to arrive at a control taxonomy of the methods to solve computing problems. Exactly speaking, the computing problem undertaken can be expressed as the regulation problem in control. The proposed approach offers a unified framework to derive zero finding algorithms (including the Newton-Rapshon algorithm). In addition, a new interpretation of the conjugate gradient algorithm as a proportional-derivative controller was made. In [34], the design method is extended to the case when the Jacobian can be decomposed into a known part and a partially known part. The extension of existing zero finding methods to this setting allows the consideration of functions which have singular Jacobians as well as the underdetermined case. Continuous effort has been made to extend this method for solving time-variant zero finding problems in [35, 36, 37]. An integration term was introduced into the error model, in [35], and a generalized proportional integral-derivative controller was constructed for the problem solving. In [36], the proposed RNN model remedies limitations of the activated functions, through the removal of the convex restriction. In [37], discrete recurrent neural dynamics were constructed to robustly cope with noise, showing how iterative methods for solving time-variant computing problems can be used in a control framework. By building into the constructive error dynamics of ZNNs a certain of flexibility for zero finding problem solving, in this paper, an error recurrence system (ERS) approach is proposed, which is capable of enhancing robustness properly with respect to various uncertainties in implementation. A rectifying action, aiming to improve convergence rate, is involved in the error dynamics and with the chosen input action, the computing scheme can be designed with ease. Double-power and power-exponential rectifying actions are applied, respectively, with which specific forms of ZNN models are constructed. The uncertainty compensation by the signum/smoothing-signum techniques are adopted for achieving finite-duration stabilization, and the constructed models are analyzed for solving QP problems in the presence of uncertainty."
https://arxiv.org/html/2411.07540v1,Lateral String Stability in Autonomous & Connected Vehicle Platoons,"This paper addresses the lateral control of Autonomous and Connected Vehicles (ACVs) in a platoon executing an Emergency Lane Change (ELC) maneuver. These maneuvers are typically triggered by emergency signals from the front or rear of the platoon in response to the need to avoid obstacles or allow other vehicles to pass. The study assumes that ACVs maintain reliable connectivity, enabling each following vehicle to access GPS position traces of both the lead and immediately preceding vehicles in the platoon. We demonstrate that lateral string stability in the ACV platoon can be achieved using communicated information solely from the lead and preceding vehicles. Additionally, we present a lateral control framework for ACVs, which helps track a discretized preview of the trajectory constructed from the communicated data. This framework involves constructing two distinct trajectories based on the preview data from the lead and preceding vehicles, calculating the associated errors and lateral control actions for each, and then integrating these to generate a steering command. Numerical results validate the effectiveness of the proposed lateral control scheme.","I INTRODUCTION Maintaining a desired and safe trajectory is essential for the lateral control of autonomous vehicles. Numerous lateral controllers have been proposed in the literature for lane-keeping, which involves keeping the vehicle within lane boundaries [1, 2]. This study focuses on designing a lateral controller for Autonomous and Connected Vehicles (ACVs) in a platoon during an Emergency Lane Change (ELC), which is also suitable for lane-keeping. ELC maneuvers may be initiated by an obstacle avoidance action from the front or by an approaching emergency vehicle from behind. For such scenarios, the lateral controllers proposed for ACVs in the literature often rely on road infrastructure elements like reference wires, magnets, embedded signals, or lane markers [3, 4, 5]. This dependency presents challenges related to cost and vulnerability to adverse weather conditions such as snow or smog. Moreover, in situations requiring close following distances, like eco-driving, or on roads without lane markings, detecting these markings may not be feasible [6]. Research on vehicle following for lane-free driving environments underscores the difficulties and solutions for operating without traditional road infrastructure [7]. Consequently, there is a need for developing lateral control schemes that operate independently of road infrastructure, leveraging the advantages of vehicle connectivity instead. This study addresses the challenge of controlling the lateral dynamics of an ACV in a platoon with limited preview information from the lead and preceding vehicles. During an ELC maneuver, the lead vehicle performs a lane change to avoid the obstacle, constructs the trajectory in real-time, and communicates this information to its followers. It is assumed that the maneuver is not extreme, as the lead vehicle has the best view or information from infrastructure about the obstacle and does not need to follow any vehicle ahead. The proposed lateral controller typically consists of both feedforward and feedback components as shown in Fig. 1. Figure 1: Structure of the proposed lateral controller. The feedforward controller uses the preview information to construct a trajectory to be tracked, based on which a feedforward steering angle input is computed. If the lateral vehicle dynamic model were known exactly, and the vehicle were following the desired trajectory with no lateral and heading errors, such an input would maintain the vehicle on the trajectory in the absence of disturbances. However, since this ideal scenario is rarely encountered in practice, a feedback controller is also necessary. The task of a feedback controller is to get the vehicle back onto the desired trajectory in the presence of disturbances, model uncertainties, and initial errors in heading and lateral position and their rates. It relies on the cross-track error e_{lat} (the distance between the ACV and the closest point on the target trajectory), heading error \tilde{\theta} (the deviation between the ACV’s heading and the direction of the tangent to the target trajectory at the closest point), and yaw rate error \dot{\tilde{\theta}}. Within this framework, two target trajectories are constructed based on the information communicated from the lead and immediate preceding vehicles, respectively. Connectivity is crucial for executing the proposed lateral control framework, as it enables each ACV in the platoon to access the position information of both the lead ACV and its immediate preceding ACV. I-A Literature Review and Novel Contributions A concise overview of lane-changing maneuvers and associated control methods is provided in previous studies [8, 9]. In contrast to methods that employ vision systems, magnetometers, or guided wire technology [5, 10, 11], the proposed approach exclusively utilizes communicated positional data from both the lead and preceding vehicles, along with GPS/IMU measurements to determine the ego vehicle’s position, heading, yaw rate, and longitudinal velocity (as illustrated in the outputs of the vehicle block in Fig. 1). The proposed lateral controller framework includes both feedforward and feedback components. Such a framework has been previously used in various studies [11, 12, 13, 14, 15]. This work differs from [12] as the following ACVs do not have a pre-planned trajectory to track; instead, trajectories are constructed in real-time based on the limited GPS measurements communicated. Unlike the works in [13, 14], we propose a lateral control scheme for a platoon with more than two ACVs. Additionally, the method we employ to utilize the communicated data from the lead and preceding vehicles for enhanced performance differs from our previous work in [15]. In our prior work [15], we demonstrated lateral string instability using predecessor-only information, even when the lead vehicle was tracking a straight line. In contrast, this study provides a theoretical proof establishing the lateral string stability of a platoon utilizing communicated information from both lead and preceding vehicles while executing a straight-line maneuver. This theoretical proof complements the numerical simulations presented in the dissertation of Liu [16], which are illustrated in this paper to offer comprehensive validation of the proposed control scheme. The novel contributions of this paper are significant to the field of autonomous vehicles and are twofold. Firstly, we demonstrate the lateral string stability of a platoon utilizing communicated information from both lead and preceding vehicles while executing a straight-line maneuver. Motivated by this finding, we propose a lateral control framework designed for platoons with more than two ACVs, expanding its applicability beyond previous works. A key innovation in our framework is the method we employ to utilize communicated data from both the lead and preceding vehicles, enhancing performance and scalability for larger platoons. I-B Organization The structure of this paper is as follows: Section II provides a brief overview of the lateral dynamic model of the vehicle. Section III introduces the proposed lateral control scheme. Section IV presents the proof for the lateral string stability of the ACV platoon using the lead and preceding vehicles information. Section V presents the simulation results. Finally, Section VI concludes the paper with final remarks."
https://arxiv.org/html/2411.07470v1,Two-Layer Attention Optimization for Bimanual Coordination,"Bimanual tasks performed by human agents present unique optimal control considerations compared to cyberphysical agents. These considerations include minimizing attention, distributing attention across two isolated hands, and coordinating the two hands to reach a broader goal. In this work, we propose a two-layer controller that captures these considerations. The upper layer solves an attention distribution problem, while the two lower layer controllers (one per hand) tracks a trajectory using the solution given by the upper layer. We introduce a formulation of the attention controller where attention is a vector that is bound within a hyperbolic feasible region, which is determined by specifications of the task the lower layer controllers. This two-layer controller is used to optimize a single-player game of pong, where the agent must rally the ball between two paddles for as long as possible. We find that adding an attention layer on top of the lower controllers allows the agent to coordinate the left and right hands, which minimizes attention and control effort over the course of the rallying task.","Bimanual tasks are motor coordination problems that require both hands. In previous works, bimanual control problems (such as juggling or devil-sticking [1][2]) exploit the left-right symmetry of both the dynamics and the controller so that a two-handed problem is divided into two instances of one-handed problems. These problems can be formulated as a hybrid control problem to model the discontinuous collision dynamics. This formulation is useful for implementing the control policies in robots. Bimanual tasks are also useful for studying human sensorimotor control policies, which present considerations distinct from cyberphysical systems. For example, most humans have more fine-grained control of their right hand and arm than their left, so the control problems are not symmetric. In addition to motor control, the brain must also strategically send signals to the separate limbs so that a broader coordination task can be achieved. Furthermore, the brain allocates time-varying attention, both towards active sensing and limb actuation, as a limited resource, whereas control policies for robots or cyberphysical systems usually give uniform attention across all sensors, actuators, and time. Optimal control has been used to study human bimanual coordination. As demonstrated in participant experiments [3][4], changing the goals and constraints of bimanual tasks can change the attention allocation strategy between the arms. Furthermore, the authors of [3] were able to correlate distinct attention policies to different LQR gains. In our work, rather than have the attention be implicit in the control policy, we explore strategies that emerge when attention is explicitly quantified. Other works use data from human driving experiments to show how active sensing policies can be described with Inverse Optimal Control (IOC) [5] [6]. This suggests that there is a cost to using sensory data that does not significantly contribute to the success of a task, and the brain is executing some strategy to minimize this cost. While these works represent the attention strategy as a sensor model, attention strategy in our work takes the form of modulating controller parameters. In this work, we show that the allocation of bimanual attention can be formulated as a two-layered optimization problem. The upper layer solution provides a controller for attention, and the lower layer provides a controller for actuation. We apply this layered optimization in a scenario where an agent plays a game of pong with themselves, where their task is to stabilize the ball while minimizing attention. We evaluate the two-layer controller’s ability to converge to a steady state under different coordination policies and handedness parameters, which allows us to observe emergent behavior that comes from tradeoffs between variable attention, task difficulty, control effort, and actuator asymmetry."
https://arxiv.org/html/2411.07419v1,Machine Learning Based Cyber System Restoration for IEC 61850 Based Digital Substations,"Substation Automation Systems (SAS) that adhere to the International Electrotechnical Commission (IEC) 61850 standard have already been widely implemented across various on-site local substations. However, the digitalization of substations, which involves the use of cyber system, inherently increases their vulnerability to cyberattacks. This paper proposes the detection of cyberattacks through an anomaly-based approach utilizing Machine Learning (ML) methods within central control systems of the power system network. Furthermore, when an anomaly is identified, mitigation and restoration strategies employing concurrent Intelligent Electronic Devices (CIEDs) are utilized to ensure robust substation automation system operations. The proposed ML model is trained using Sampled Value (SV) and Generic Object Oriented Substation Event (GOOSE) data from each substation within the entire transmission system. As a result, the trained ML models can classify cyberattacks and normal faults, while the use of CIEDs contributes to cyberattack mitigation, and substation restoration.","The transmission system is essential infrastructure for delivering power with minimal losses. Protecting this transmission network is critical for maintaining the stability, reliability, and resilience of the entire power system, including distribution and generation systems [1]. Protection schemes, such as overcurrent, differential, and distance protection using Intelligent Electronic Devices (IEDs), have been widely adopted across numerous countries and regions. These schemes contribute to stable transmission systems with high accuracy and rapid response times. Those are categorized as rule-based protection schemes, typically executed according to predefined threshold values. However, they have the drawback of being less effective in detecting and responding to cyberattacks targeting digitalized power systems. Therefore, data-driven methods for distinguishing power system events are advantageous for detecting anomalies, leading to active research into anomaly-based approaches that rely on data itself [2]. In addition to detecting cyberattacks, developing strategies to respond to them is a crucial aspect of this topic. A lack of adequate response to malicious attacks can lead to prolonged blackouts, critical equipment damage, and significant economic repercussions. Therefore, strategies for mitigation and restoration against anomalies in substation operations are also essential functions. Recent studies have been conducted on this topic. The study by [2] defines cyberattacks according to the MITRE Adversarial Tactics, Techniques, and Common Knowledge (MITRE ATT&CK) framework and detects anomalies using a machine learning-based approach. ML models are trained utilizing the SV and GOOSE information from the IEC 61850 communication protocol. Methods for securing Routable SV (R-SV) and Routable GOOSE (R-GOOSE) were proposed by [3]. The authors of [4] trained ML models using Root Mean Square (RMS) measurements from each substation within the transmission system. The research by [5] developed the cybersecurity scheme for the local substation. The work by [6] structured mitigation strategies for protection systems by incorporating Battery Energy Storage System (BESS) and Hybrid Energy Storage (HES), which are components within the microgrid. A substation system restoration scheme algorithm based on resilience characteristics was proposed by [7]. These studies either focus solely on evaluating the performance of data-driven based ML models for detecting anomalies or emphasize only on mitigation and restoration strategies when abnormalities occur. However, detecting cyberattacks and subsequently implementing mitigation and recovery strategies are continuous events that must be considered together to safeguard substation automation systems. In addition, the anomaly-based detection methods used in these studies face challenges in accurately identifying specific locations and fault types. This can provide field technicians with precise information to expedite the repair of transmission line failures, serving as a valuable guideline. As a result, implementing these methods across the wider scope of the entire power system presents significant challenges, underscoring the necessity for solutions that are both more extensive and adaptable. Accordingly, in order to overcome the challenges, this study presents an approach to fault and cyberattack detection using machine learning algorithms, combined with mitigation and restoration strategy. The proposed ML based anomaly detection framework enables operators to perform post event studies more efficiently compared to traditional approaches by considering cyber system data and physical system data. Additionally, the performance of the mitigation and restoration strategy was initially validated by using the CIED. The methodology was tested and validated using the real-time power system simulator. The test results showed that the proposed approach effectively distinguished between cyberattacks and conventional power system faults, including identifying their locations. Furthermore, this testbed demonstrated that transitioning from IEDs to CIEDs effectively mitigates cyberattacks and restores functionality within the substation. Key contributions of this paper are as follows: • The proposed method uses machine learning to analyze physical system data (e.g., voltage, angle, and frequency in SV packets) and digital system data (e.g., circuit breaker (CB) trip signals and CB status data in GOOSE packets) from both the affected substation and adjacent substations. This information is sent to the control center system, such as Supervisory Control and Data Acquisition (SCADA). Within SCADA, trained ML models are used to determine whether the current protection action was triggered by a cyberattack or by a normal fault. • Without a mitigation and restoration strategy, detecting a cyberattack alone is insufficient to prevent adverse impacts on the power system. However, the proposed method allows the CIED to rapidly take over the functions of the compromised IEDs when the substation is under attack, thereby preventing damage to the power system. The remaining part of this paper is organized as follows: Section II illustrates a methodology. In Section III, case studies are explained. Finally, this paper is concluded in Section IV."
https://arxiv.org/html/2411.07833v1,Robust Adaptive Safe Robotic Grasping with Tactile Sensing,"Robotic grasping requires safe force interaction to prevent a grasped object from being damaged or slipping out of the hand. In this vein, this paper proposes an integrated framework for grasping with formal safety guarantees based on Control Barrier Functions. We first design contact force and force closure constraints, which are enforced by a safety filter to accomplish safe grasping with finger force control. For sensory feedback, we develop a technique to estimate contact point, force, and torque from tactile sensors at each finger. We verify the framework with various safety filters in a numerical simulation under a two-finger grasping scenario. We then experimentally validate the framework by grasping multiple objects, including fragile lab glassware, in a real robotic setup, showing that safe grasping can be successfully achieved in the real world. We evaluate the performance of each safety filter in the context of safety violation and conservatism, and find that disturbance observer-based control barrier functions provide superior performance for safety guarantees with minimum conservatism. The demonstration video is available at https://youtu.be/Cuj47mkXRdg.","The human hand is capable of adapting to a wide range of complex objects and performing different tasks in daily life robustly [1], since it allows smooth and safe force interaction [2]. Inspired by this, robotic grasping has been investigated across many human-oriented applications such as industrial assembly [3], packing of groceries [4], and household tasks [5]. Despite the significant progress of robotic grasping over the past decades, the grasping research still revolves around determining proper finger postures for grasping from a kinematic perspective [6][7]. At the same time, contact dynamics are often overlooked in existing grasping methods, which are not sufficient for achieving dexterous grasping [8]. This limitation prevents robotic grasping from being applied to more general tasks that require safe behaviors, as they cannot prevent slippage between the hand and object. To mitigate this limitation, some researchers have focused on force-based grasping techniques. In [9], they propose an adaptive force control framework that allows the hand to be compliant by mapping the human hand posture data into the desired force commands. To prevent undesired slippage, [10] proposes a robust force controller for grasping to maintain secure grasping by introducing force feedback. Similarly, [11] demonstrates a low-level impedance-based controller that incorporated task-based search strategies, comparing its effectiveness on a peg-in-hole task using various robotic hands. Furthermore, a method for regulating the grasping force based on tactile sensors is proposed in [12] to increase grasping stability for unknown objects by leveraging data-driven models like deep neural networks or Gaussian mixture models to detect and estimate contact status and force. Others have demonstrated that online adaptive control strategies can account for disturbances and model error [13] using dynamical systems-based [14] approaches for dexterous manipulation. The modeling of contact dynamics has also played a key role in many robust grasping works. In [15], internal and friction forces of a grasped object are determined in order to provide stable finger gaits so that the proposed impedance controller can handle the dynamic changes in object state during reconfiguration of the fingers. Similarly, [16] proposes a hybrid position/force-controller used to generate a finger gaiting sequence while considering dynamic constraints imposed by friction cones, enabling stable control over object pose. Figure 1: Safe robotic grasping for fragile lab glassware. Such force-based grasping methods have primarily focused only on stable grasping, neglecting safety considerations, which are crucial in real world applications. For instance, safety regulations are essential for laboratory automation [17] and space applications including robotic hands [18]. Therefore, ensuring safety in robotic grasping remains an open challenge beyond merely achieving compliant manipulation. To that end, a few works exist, proposing methods such as safe grasping with impact force measurements [19], safety-optimized strategies with visual depth data and false-positive detection for human safety [20], and an integrated framework for predicting safe grasping forces using transformers [21]. Figure 2: Overview of the proposed safe grasping framework. The framework consists of three main components: safety filters, estimation of contact information, and fingertip force controller. Safety filters include CBF, RaCBF, RCBF, and DOBCBF with a disturbance observer. Tactile sensor data from the hand is mapped to the actual force, and the contact force/torque and point on a fingertip are estimated to be used in the controllers. The fingertip force controller is designed to track safe control input to the hand. Nevertheless, to the best of our knowledge, few studies take into account formal safety guarantees (e.g., force regulation and closure to avoid slippage) for force-based grasping control; thus, imbuing grasping algorithms with formal safety guarantees should be investigated for broader use cases such as medical, space, and chemical laboratory applications. Safety guarantees can be efficiently achieved by using reachability analysis [22], set invariance techniques [23], and control barrier functions (CBFs) [24][25][26]. In this paper, we focus on CBF-based controllers to ensure safety guarantees in robotic grasping. CBF-based controllers enforce the forward invariance of a defined safe set and can be efficiently implemented using quadratic programming, enabling their widespread use in many applications [27][28]. The use of CBFs for safe grasping was first proposed by [29]. Their methodology used robust CBFs in order to handle external disturbances, and consequently, achieved safe grasping while avoiding undesired slippage. I-A Contributions The main novelty presented in this paper is the introduction of an integrated robotic safe grasping framework with tactile sensing as shown in Fig. 2. The framework is specifically designed to maintain maximum and minimum safe grasping forces and achieve safe force closure by employing robust and adaptive approaches in the presence of model uncertainty. The contributions of this paper are outlined as follows: • We present a safe grasping framework with formal safety guarantees, based on CBFs schemes as safety filters. The scalability of the framework presented in this paper is demonstrated by applying several safety filters, including CBF [24], robust adaptive CBF [25] (RaCBF), robust CBF (RCBF) [26], and disturbance observer CBF (DOBCBF) [30]. • To this end, we implement force control with an inner position loop for a dexterous robot hand and develop sensor processing techniques to estimate contact force and point from electromagnetic tactile sensors on fingertips. This approach enables us to use contact dynamics model effectively in CBFs-based controller. • The effectiveness of our framework is empirically validated with multiple objects in a scenario where the grasping force must be regulated within safety limits to prevent damage to objects and slippage as well. Additionally, we provide the detailed comparisons between each safety filter in the context of safety violation and conservatism. We use Shadow Robot platform for validations as shown in Fig. 1. • To the best of our knowledge, this work is the first attempt not only to introduce an integrated framework with tactile sensing for grasping with formal safety guarantees, but also to demonstrate it in the real-world. We show significant improvements in our framework compared to [29] with respect to adaptation, conservatism, and grasping force regulation. Our method incorporates adaptation to parametric uncertainty and while estimating external disturbances, making it less conservative than [29]. Additionally, since [29] maintains a strong grasping force to prevent slipping without regulating the force, it may lead to applying excessive force and potentially damaging the grasped object. In this work, constraints including contact force and force closure are simultaneously considered. This paper is organized as follows. Section II defines a problem solved in the paper and Section III provides safety filters and a disturbance observer as preliminaries. Section IV presents the proposed framework. Subsequently, we verify our framework in numerical simulations in Section V and validate it in the real multiple experiments in Section VI. Lastly, we conclude our paper with future works in Section VII."
https://arxiv.org/html/2411.07654v1,Spike Talk in Power Electronic Grids,"Emerging distributed generation demands highly reliable and resilient coordinating control in microgrids. To improve on these aspects, spiking neural network is leveraged, as a grid-edge intelligence tool to establish a talkative infrastructure, Spike Talk, expediting coordination in next-generation microgrids without the need of communication at all. This paper unravels the physics behind Spike Talk from the perspective of its distributed infrastructure, which aims to address the Von Neumann Bottleneck. Relying on inferring information via power flows in tie lines, Spike Talk allows adaptive and flexible control and coordination itself, and features in synaptic plasticity facilitating online and local training functionality. Preliminary case studies are demonstrated with results, while more extensive validations are to be included as future scopes of work.","The energy consumption of data centers has become a major concern in modern society. As the distributed energy resources (DERs) are increasingly promoted, the carbon footprint is also becoming more critical in power grids where large amount of data are involved [1]. Meanwhile, distributed generation is escalating the demand for coordinating control in cyber-physical microgrids to ensure operational reliability. In turn, challenges persist thereupon, involving delays [2] and susceptibility to cyberattacks [3]. It is therefore of much value to study on a decentralized transition of the operation paradigm to address both challenges. Under this scenario, Talkative Power has been accordingly developed, aiming to co-transfer power and information along transmission lines [4]. System resilience is effectively improved, while additional energy consumption on the transmission line is inevitable. besides, as Talkative Power relies on request-respond information exchange protocol, its scalability is limited when multiple agents are involved in information exchange simultaneously. In contrast, we delve into the realm of a publish-subscribe protocol, where information is fetched locally as needed. Navigated by the biologically plausible neuron model [5, 6], spiking neural network (SNN) has emerged with great advantage in energy efficient computation due to its event-driven feature. Beyond the von-Neumann computing architecture activated by real numbers and perceptrons, SNN leverages a leaky-charge framework instead that are triggered by asynchronous spikes. Empowered by SNN, we formalize the Spike Talk tailored for microgrids, harnessing power flow dynamics to infer remote information locally. As spiking neurons and spiking neural networks have the features of synaptic plasticity and spike-timing-dependent plasticity (STDP), the neuromorphic infrastructure also shows potential in online learning and effectively reduces the data and energy requirements for training. Furthermore, the necessity for communication channels is eliminated, thus effectively addressing the resilience challenges in microgrids. This article thus delineates the infrastructure of Spike Talk and explains it from the perspective of its decentralized and online learning features. The remaining parts of this article is organized as follows: Section II introduces the inspiration of neuromorphic infrastructure for power grids from the von Neumann bottleneck. Section III elaborates on the online learning potential of Spike Talk by investigating the training principles. Section IV presents a case study, and Section V concludes the entire article. By discussing its inherent advantages, more promising real-world applications are implied, which should be the future scope of our work."
https://arxiv.org/html/2411.07603v1,\mathscr{H}_{2} Model Reduction for Linear Quantum Systems,"In this paper, an \mathscr{H}_{2} norm-based model reduction method for linear quantum systems is presented, which can obtain a physically realizable model with a reduced order for closely approximating the original system. The model reduction problem is described as an optimization problem, whose objective is taken as an \mathscr{H}_{2} norm of the difference between the transfer function of the original system and that of the reduced one. Different from classical model reduction problems, physical realizability conditions for guaranteeing that the reduced-order system is also a quantum system should be taken as nonlinear constraints in the optimization. To solve the optimization problem with such nonlinear constraints, we employ a matrix inequality approach to transform nonlinear inequality constraints into readily solvable linear matrix inequalities (LMIs) and nonlinear equality constraints, so that the optimization problem can be solved by a lifting variables approach. We emphasize that different from existing work, which only introduces a criterion to evaluate the performance after model reduction, we guide our method to obtain an optimal reduced model with respect to the \mathscr{H}_{2} norm. In addition, the above approach for model reduction is extended to passive linear quantum systems. Finally, examples of active and passive linear quantum systems validate the efficacy of the proposed method.","Linear quantum systems play a crucial role in advancing applications such as quantum computing, quantum communication [1], and quantum sensing [2]. Many applications involve several linear quantum system components, which results in a high-dimensional total system. For example, linear quantum systems are widely used in applications such as cavity opto-mechanical systems [3] and multi-mode quantum harmonic oscillators [4]. These systems usually consist of multiple quantum states whose interactions form complex coupling networks [5, 6]. Furthermore, in the augmented system for non-Markovian quantum systems[7, 8, 9, 10], many linear quantum systems are used for modelling quantum colored noise, which leads to a high dimensional Hilbert space for the augmented system. Hence, it would be difficult to control these systems in real time since the high dimension of the system leads to a heavy computation burden for controllers[13, 14]. In the above contexts, it is necessary to develop model reduction methods which can approximate a high-dimensional system with a lower-dimensional model as well as retain the essential dynamics of the original system [15, 16], thus making the design of control algorithms more feasible and efficient. However, classical model reduction methods cannot be applied directly to linear quantum systems since the matrices in their linear dynamical equations should satisfy physical realizability conditions [11, 12]. These conditions resulting from commutation rules in quantum mechanics [13] guarantee the quantumness of a system, which is essential for outperforming its classical counterparts. Hence, model reduction methods for quantum systems should be developed. In early research, model reduction methods were limited to singular perturbation techniques for passive linear quantum systems [17], which can preserve desired parts by separating fast changing parts from slow changing parts. To develop a more general method, several effective classical techniques have been extended to linear quantum systems including balanced truncation methods and interpolation projection methods [18, 19]. By identifying the energy balance state of a system, the balanced truncation method projects the system into a subspace with higher energy, thereby generating a simplified model that retains its essential dynamics. In contrast to singular perturbation techniques, balanced truncation methods are capable of preserving stability and also provide error bounds, which serve to control approximation errors [18]. Additionally, similar to the interpolation projection methods used in classical model reduction, interpolation projection methods for linear quantum systems can ensure that the input-output responses of the original and reduced-order systems match at multiple selected frequencies. Ref. [19] introduces a tangential interpolation projection method for model reduction of linear quantum systems and establishes \mathscr{H}_{\infty} error bounds for the proposed method, incorporating a heuristic algorithm for the selection of tangential directions. However, this method only guarantees minimal error near the interpolation points and does not achieve accurate approximation across the entire frequency band. Hence, although numerous methods for model reduction exist, most of which can only evaluate the performance of methods after model reduction, they often fail to achieve optimum performance with respect to a specific metric. In this paper, we consider \mathscr{H}_{2} norm as a criterion for model reduction. Compared with existing model reduction methods, we not only use \mathscr{H}_{2} norm as the criterion to evaluate model reduction, but also design the reduced model according to \mathscr{H}_{2} performance. Unlike in classical systems, obtaining an \mathscr{H}_{2} optimal reduced model while ensuring physical realizability in linear quantum systems is a significant challenge. To address this issue, we first reformulate the model reduction problem as an optimization problem and rigorously derive the necessary conditions for a feasible solution. By utilizing the matrix inequality method, we transform the nonlinear constraints introduced by physical realisability into LMIs and nonlinear equality constraints, which allows us to solve the optimization problem using a matrix lifting method. A method for model reduction of passive linear quantum systems is also presented. Finally, the proposed method is applied to active and passive linear quantum systems, illustrating its effectiveness. This paper is organized as follows. Section 2 introduces linear quantum systems and the model reduction problem. Section 3 introduces the \mathscr{H}_{2} model reduction method. In Section 4, we present the model reduction approach for passive linear quantum systems. Examples are given in Section 5 to demonstrate the effectiveness of our method. Finally, conclusions are drawn in Section 6. Notation For a matrix A=[A_{ij}], the symbols A^{T}, A^{\dagger} and \text{tr}(A) represent the transpose, Hermitian conjugate and trace of A. We denote \text{He}(A)=A+A^{\dagger}. Given two operators N_{1} and N_{2}, [N_{1},N_{2}]=N_{1}N_{2}-N_{2}N_{1} is their commutator. Given a complex number a, \bar{a}, \mathfrak{R}\{a\} and \mathfrak{I}\{a\} represent its conjugate, real part and imaginary part, respectively. The signal \# in the matrix represents symmetric elements."
https://arxiv.org/html/2411.07573v1,Robotic Control Optimization Through Kernel Selection in Safe Bayesian Optimization,"Control system optimization has long been a fundamental challenge in robotics. While recent advancements have led to the development of control algorithms that leverage learning-based approaches, such as SafeOpt, to optimize single feedback controllers, scaling these methods to high-dimensional complex systems with multiple controllers remains an open problem. In this paper, we propose a novel learning-based control optimization method, which enhances the additive Gaussian process-based Safe Bayesian Optimization algorithm to efficiently tackle high-dimensional problems through kernel selection. We use PID controller optimization in drones as a representative example and test the method on Safe Control Gym, a benchmark designed for evaluating safe control techniques. We show that the proposed method provides a more efficient and optimal solution for high-dimensional control optimization problems, demonstrating significant improvements over existing techniques.","To address the widely adopted yet time-consuming and often suboptimal process of manual controller tuning, the demand for automatic controller optimization has grown across various fields, including robotics [1, 2], automotive systems [3], and industrial automation [4]. Most previous methods are based on a simplified system model and determine the optimal controller parameters through the system model [5, 6]. Such methods are often prone to suboptimal results due to a less accurate system model and the influence of the noise [7]. With the development of data-driven methods, optimizing controller parameters based on real robotic system motion data has shown notable improvements in control performance, such as methods using Iterative Feedback Tuning (IFT) [8], Variable gain control [9], and Bayesian optimization [10, 11]. Bayesian optimization is often used to optimize black-box objective functions that are expensive to evaluate. It employs a surrogate model, typically a Gaussian Process (GP), to iteratively predict the function’s distribution and guide the search for the optimum using an acquisition function. Since the noise measurement can be modeled as a GP together with the objective function [12], Bayesian optimization is particularly effective for tasks like optimizing PID controller parameters in control systems. However, Bayesian optimization often evaluates unsafe parameters in practical applications [7] since the iterative process does not consider the physical meaning of the evaluated function. For example, unsafe parameter combinations in quadrotor control could lead to collisions with walls or ceilings. Besides, Bayesian optimization uses Gaussian kernels for GP calculations, which is prone to the curse of dimensionality [13]. When the dimension of the problem becomes larger than 3, the number of evaluations required by Bayesian optimization will increase significantly. Hence, controller parameter tuning of high-order complex systems remains a significant challenge, especially when safety and robustness should be guaranteed [14, 15]. Figure 1: Overview of the method. First, the input and output of the system are measured to obtain a set of observations, which serve as the GP priors for calculating kernel selection. Subsequently, Bayesian optimization via additive Gaussian processes uses the most important one or several additive kernels calculated by kernel selection for optimization, iteratively selecting safe new parameters and testing the performance in the system until the optimal control parameters are obtained. Following this line of research, Sui et al. [16] proposed SafeOpt. This first safety-aware Bayesian optimization algorithm ensures the safety of the optimization process by avoiding evaluating parameters whose performance values fall below the pre-defined safe threshold. However, it is less feasible in high dimensions. Kirschner et al. [17] then proposed the LINEBO algorithm to address the challenges of optimizing high-dimensional problems by breaking them into manageable one-dimensional sub-problems. Still, it is hard to apply to control engineering because it usually takes hundreds of evaluations, which may cause severe wear to the system. Wang et al. [13] proposed the high-dimensional safe Bayesian optimization algorithm via additive Gaussian processes, which is more efficient for control optimization. They employed an additive structure to the traditional Gaussian kernels to enhance the information acquisition efficiency, but it brought too much complexity to the calculation. When the problem dimension gets higher than 6, the experimental validation with actual hardware will be hard to implement. Dimensionality reduction based on feature selection is a standard method to reduce the calculation for complex algorithms [18, 19, 20]. In this framework, the kernel-target alignment method [21] measures the similarity between kernels and the objective function, providing a way to select essential kernels based on the degree of agreement with the learning task. While this approach offers theoretical insights and can improve model accuracy, it is computationally intensive and inefficient in high dimensions. To address these limitations, Ding et al. [22] introduce an alternative approach using the Nyström kernel matrix approximation. This method is more computationally efficient and provides theoretical consistency guarantees, making it a better choice for real-time applications. In this paper, we propose to adopt the Nyström approximation method to ensure efficient and effective kernel selection to identify the most important additive kernels based on the GP prior, and then use the selected kernels to optimize high-dimensional control problems safely and efficiently. The process is illustrated in Fig. 1. We validated our algorithm using the general benchmark Safe Control Gym [23], and experimental results demonstrate that our method outperforms existing high-dimensional safe Bayesian optimization algorithms for quadrotor trajectory control."
https://arxiv.org/html/2411.07444v1,Input-Based Ensemble-Learning Method for Dynamic Memory Configuration of Serverless Computing Functions,"In today’s Function-as-a-Service offerings, a programmer is usually responsible for configuring function memory for its successful execution, which allocates proportional function resources such as CPU and network. However, right-sizing the function memory force developers to speculate performance and make ad-hoc configuration decisions. Recent research has highlighted that a function’s input characteristics, such as input size, type and number of inputs, significantly impact its resource demand, run-time performance and costs with fluctuating workloads. This correlation further makes memory configuration a non-trivial task. On that account, an input-aware function memory allocator not only improves developer productivity by completely hiding resource-related decisions but also drives an opportunity to reduce resource wastage and offer a finer-grained cost-optimised pricing scheme. Therefore, we present MemFigLess, a serverless solution that estimates the memory requirement of a serverless function with input-awareness. The framework executes function profiling in an offline stage and trains a multi-output Random Forest Regression model on the collected metrics to invoke input-aware optimal configurations. We evaluate our work with the state-of-the-art approaches on AWS Lambda service to find that MemFigLess is able to capture the input-aware resource relationships and allocate upto 82% less resources and save up to 87% run-time costs.","The serverless computing paradigm is the latest cloud-native development model that enables application execution without the management of underlying resources. Serverless promotes the idea that a developer should be less concerned about the servers or infrastructure and focus more on productivity that adds value to the business. This shift of responsibility means offloading resource management tasks to the cloud service provider (CSP), such as resource allocation, application scaling and software updates. In the serverless landscape [1], Function-as-a-Service (FaaS) emerged as a microservices-inspired, event-driven execution model where function(s) are integrated with additional Backend-as-a-Service (BaaS) offerings like storage, networking and database services, to set-up an application. A serverless function is a stateless code fragment, executed on-demand within lightweight virtual machines (VM), microVMs or containers for short-term duration, and bills its resources as per usage. In 2014, Amazon Web Services (AWS) introduced AWS Lambda [2], [3] as its first FaaS offering, and since then, a range of FaaS services have emerged, including Google Cloud Functions [4], Azure Functions [5], and many open-source implementations such as OpenFaaS [6], Knative [7] and OpenWhisk [8]. In addition to serverless attributes such as on-demand scalability, zero idle-resource costs, and no resource management, FaaS uniquely features scale-to-zero capability where function resources are released after an extended period of inactivity, endorsing a multi-tenant resource-sharing and pay-per-use pricing model. FaaS has increasingly found its relevance in a variety of use cases like video streaming platform [9], multi-media processing [10], CI/CD pipeline [11], AI/ML inference task [12], and Large-Language-Model (LLM) query processing [13]. The operational model of FaaS hides the complex infrastructure management from end users and does not signify the absence of servers. A serverless function still requires resources, including computing, network and memory, for a successful execution. In the current FaaS implementations, a developer is responsible for requesting the right combination of resources to guarantee successful function execution. However, service providers only expose a small set of resource knobs, usually memory 111We refer to FaaS platforms like AWS Lambda that allow developers to provide only memory configuration and allocate CPU, network bandwidth, etc., in a proportional fashion. with proportionally allocated CPU, disk I/O, network bandwidth, etc. [14]. Prior studies [15][16][17] have identified that a higher memory configuration speeds up function execution and has a significant impact on its start-up performance and costs. However, the execution speedup is non-linear and has a diminishing marginal improvement with increasing memory allocations [18]. With limited observability into short-running functions and unaware of function performance, developers usually resort to speculative decisions for memory configuration or make experience-based ad-hoc decisions with an expectation to fulfil service level objectives (SLO) [19]. To validate such developer behaviour, an industry insight [20] reports the ease of controlling function execution duration via memory configuration, while 47% of production-level functions still run with the default memory configuration without exploring the entire configuration space. Additionally, selecting an optimal memory configuration from an exponentially large search space requires a careful understanding of the correlation between function performance and resource requirements. Hence, configuring the function with the right amount of memory that guarantees shorter execution times and lower execution costs is an intricate task. (a) Payload vs Duration matmul function metrics (b) Payload vs Duration linpack function metrics (c) Payload vs Memory Utilisation graph-mst function metrics (d) Payload vs Memory Utilisation matmul function metrics (e) Payload vs Memory Utilisation linpack function metrics (f) Payload vs Memory Utilisation graph-mst function metrics Figure 1: Function Metrics Insight - Payload vs Memory Utilisation vs Billed Duration TABLE I: List of collected function metrics Metric Name Description request_id unique function invocation ID payload function input parameter(s) memory_size amount of memory allocated to function memory_utilisation maximum memory measured as a percentage of the memory allocated to the function memory_used measured memory of the function sandbox billed_duration function execution time rounded to nearest millisecond billed_mb_ms total billed Gb-s, a pricing unit for function cold_start function cold start (true/false) init_duration amount of time spent in the init phase of the execution environment lifecycle function_error any function run-time error Recent research [17][21][22][23] that optimise the function resource allocation process has highlighted a drastic impact of input parameters on its performance. Additionally, a static memory configuration is used for concurrent function invocations while expecting similar performance for distinct function inputs. Therefore, setting a static memory configuration for all function invocations, regardless of their input, leads to a fluctuating performance with varying workload and input arguments. This performance unpredictability demands an input-argument-aware approach in determining the memory configuration for function invocations that balances execution cost and running time while reducing excess resource allocation. This input-based memory configuration has a two-fold effect of providing a more autonomous developer experience and a chance for CSPs to maximise resource utilisation and deliver a finer-grained, cost-effective pricing model for users. Additionally, existing efforts [17][21][22][23] to configure function resources either focus on an average-case function execution to recommend maximum used memory/resources or propose to re-run their solution for specific input parameters to optimise the memory allocation process. This may lead to higher run-time costs and resource wastage and on the other hand, running multiple models for previously unseen input values extends the data collection process as well as increases the model training and tuning complexity. Therefore, a solution is warranted that captures the relationship of input parameters with function resources to precisely model and predict the required memory configuration for successful execution and reducing excess resource allocation. To this end, we present MemFigLess, an end-to-end estimation and function memory allocation framework that makes input-aware memory configuration decisions for a serverless function. MemFigLess takes as an input the function details, such as the representative function input arguments, expected running time and cost SLOs and a range of memory allocations to explore. The framework executes an offline profiling loop to take advantage of a robust tree-based ensemble learning technique, multi-output Random Forest Regression (RFR), which analyses the relationship between input parameters and other function metrics such as execution time, billed cost, and function memory requirement. The RFR model is then exploited in an online fashion to make an optimal selection of memory configuration for individual function invocations. Additionally, the framework provides a feedback loop to re-train the model in a sliding-window manner with a new set of collected metrics to capture the performance variation."
https://arxiv.org/html/2411.07442v1,Learned Slip-Detection-Severity Framework using Tactile Deformation Field Feedback for Robotic Manipulation,"Safely handling objects and avoiding slippage are fundamental challenges in robotic manipulation, yet traditional techniques often oversimplify the issue by treating slippage as a binary occurrence. Our research presents a framework that both identifies slip incidents and measures their severity. We introduce a set of features based on detailed vector field analysis of tactile deformation data captured by the GelSight Mini sensor. Two distinct machine learning models use these features: one focuses on slip detection, and the other evaluates the slip’s severity, which is the slipping velocity of the object against the sensor surface. Our slip detection model achieves an average accuracy of 92%, and the slip severity estimation model exhibits a mean absolute error (MAE) of 0.6 cm/s for unseen objects. To demonstrate the synergistic approach of this framework, we employ both the models in a tactile feedback-guided vertical sliding task. Leveraging the high accuracy of slip detection, we utilize it as the foundational and corrective model and integrate the slip severity estimation into the feedback control loop to address slips without overcompensating. Videos and demonstrations are available at: https://sites.google.com/uw.edu/lsds","I INTRODUCTION Tactile sensing plays a pivotal role in robotic manipulation, offering a rich source of information for understanding and interacting with the environment [1]. Manipulation tasks such as delicate handling of objects [2] and secure grasping [3] under dynamic conditions rely on effective slip detection [4]. Traditional approaches to slip detection have predominantly relied on binary indicators of slip occurrence [5, 6, 7, 8, 9], leveraging tactile data to discern stable grips from unstable ones. However, this binary treatment overlooks the nuanced spectrum of slip dynamics, potentially leading to inadequate control strategies. Furthermore, reliance solely on visual feedback for slip detection and manipulation tasks presents inherent limitations, such as occlusions, varying lighting conditions, and the need for external viewpoints that may not always be feasible in confined environments [10]. Figure 1: Summary of the Slip-Detection-Severity Framework: A robot executes an object handling task, during which tactile features from the GelSight Mini sensor are extracted in real time. These features simultaneously feed into the Slip Detection and Slip Severity models. Upon detecting slip, the feedback controller actively adjusts the gripper to mitigate slip severity. To overcome the identified limitations, this research seeks to answer the primary question of how slip detection methods can be improved to effectively manage the complexities of slip dynamics. Furthermore, it explores how can slip be quantified to aid in the development of feedback control algorithms for more precise slip handling and mitigation. Our contribution centers on the direct detection of slip occurrences and the simultaneous estimation of slip severity, utilizing real-time tactile sensor data. We introduce a framework for learned slip detection and severity assessment, derived from the tactile features of the deformation vector field extracted from GelSight sensors. By identifying essential tactile features that capture the non-linear surface dynamics associated with slip events, we employ machine learning to directly map these features for slip detection and severity assessment. The efficacy of our framework is demonstrated through its integration into a feedback gripper controller as shown in Figure 1, showcasing how the slip-detection-severity feedback enables precise gripper positioning without the need for explicit force control or prior knowledge of the object’s size, geometry, or texture. Background and Related Work: Research in tactile sensing was initiated in the 1970s with the introduction of piezoelectric elements as strain sensors [11], and has expanded to include a diverse array of sensor technologies. These technologies are capable of detecting various object properties such as mass, geometry, texture, slip, and hardness, utilizing piezoelectric, capacitive array, optical, and magnetic sensors [12]. Among these, optical marker-based tactile sensors, including TacTip [13], TouchRoller [14], and GelSight [15][16], represent significant advancements in sensing high-resolution surface features and texture. TacTip sensors employ a camera to monitor the movement of white pins within a membrane upon object contact, mirroring a biomimetic design. TouchRoller, designed as a rolling sensor, acquires geometric information by traversing an object’s surface. This study focuses on the GelSight sensor, which uses a reflective gel-coated elastomer and LED illumination to track marker displacement, enabling accurate measurements of contact deformation using the deformation vector field. Foundational work has been established by demonstrating the use of tactile data for slip detection [17]. Optical methods utilize the eccentricity of the contact surface to measure object deformation [18]. The integration of machine learning has expanded slip detection capabilities, with Support Vector Machines (SVMs) utilized alongside TacTip sensor data [19]. Neural networks have been applied for slip classification [20], and GelSight sensor data have been incorporated to enhance object shape detection [7]. Furthermore, entropy-based methods utilize learning algorithms and shear marker displacement to predict slip likelihood [9, 21]. In this study, we concentrate on extracting tactile features through vector field analysis of the deformation field and employing learned models to detect slip. Research on predicting the relative velocity between grippers and manipulated objects through tactile sensing alone remains limited due to the complexity involved. A method for calculating sliding velocity utilizes a nonlinear observer used with the SUNTouch tactile sensor [22]. However, this technique encounters difficulties with objects that have small curvature radii or high deformability. Another approach, employing linear regression and capacitive-based nib-structure tactile sensors for slip speed prediction [23], presents a promising direction despite limitations in the feature space and low correlation with the regression variable. Recent efforts have leveraged CNNs alongside BioTac sensor measurements to assess the slipping speed of objects [24]. This approach is particularly effective with rigid objects but faces challenges in generalizing to deformable ones. This study aims to establish a direct correlation between slip velocity and tactile features extracted from vector field analysis, using learned models. We demonstrate that extracting features from the optical tactile sensors enables the development of models that effectively generalize across different object types."
https://arxiv.org/html/2411.07405v1,Quality of Control based Resource Dimensioning for Collaborative Edge Robotics,"With the increasing focus on flexible automation, which emphasizes systems capable of adapting to varied tasks and conditions, exploring future deployments of cloud and edge-based network infrastructures in robotic systems becomes crucial. This work, examines how wireless solutions could support the shift from rigid, wired setups toward more adaptive, flexible automation in industrial environments. We provide a quality of control (QoC) based abstraction for robotic workloads, parameterized on loop latency and reliability, and jointly optimize system performance. The setup involves collaborative robots working on distributed tasks, underscoring how wireless communication can enable more dynamic coordination in flexible automation systems. We use our abstraction to optimally maximize the QoC ensuring efficient operation even under varying network conditions. Additionally, our solution allocates the communication resources in time slots, optimizing the balance between communication and control costs. Our simulation results highlight that minimizing the delay in the system may not always ensure the best QoC but can lead to substantial gains in QoC if delays are sometimes relaxed, allowing more packets to be delivered reliably.","Flexible automation and mass customization are key drivers in this era of smart manufacturing and industrial development [1], contributing to advancing the goals of Industry 4.0 and 5.0. They can enable systems to adapt to varying production needs with minimal reconfiguration, making handling diverse and complex tasks easier, and finally paving the way for producing personalized products at scale. With the emergence of flexible automation, revisiting the mostly wired robotic solutions prevalent in the industry is required, as they do not offer mobility, adaptability and restrict collaboration. For such joint tasks, wireless collaborative robotics is crucial [2]. Wireless collaborative robotics necessitate managed systems promising low latency, high reliability, and flexibility, where 5G networks in combination with local edge computing seem attractive [3]. These 5G-based systems offer the possibility to offload different computational tasks to the edge cloud. However, practical implementations of collaborative robotics [4] where communications and compute are provided by 5G and edge computing-based infrastructures, respectively, are not well understood, opening some questions: (a) What are the different information flows in a collaborative robotic setup and how can their looped interactions be mapped to a 5G-based implementation? (b) How would the robotic setup react to uncertainties in terms of delays, reliability, capacity constraints emanating from a 5G setup? (c) Can an acceptable quality-of-control (QoC) [5] tracking the energy and control expenditure, be maintained? and finally, (d) How to develop an abstraction of robotic workloads parameterized by these uncertainties, to jointly optimize system performance? There have been a few works addressing these questions: Some works such as [6], minimize the end-to-end latency for networked robotic applications like automated guided vehicles (AGVs) on a factory floor. In their work, provided a delay requirement for their system, they find an allocation satisfying that requirement with fixed reliability bounds. While such works minimize the end-to-end latency, they lack an understanding of network reliability and how that might affect the robotic system. [7, 8] provide orchestration frameworks for networked collaborative robotics that offload motion level control to the edge. They highlight the importance of minimizing the time for the successful completion of mission-critical tasks and assume packet error rate (PER) and delay thresholds, with [7] also indicating capacity constraints. All these works, however, have a binary value function, where as long as a certain latency, PER or task success rate target is met, the system is considered stable. This calls for a joint understanding of latency and reliability to characterize the performance of the control application with a measurable metric such as QoC. Authors in [5] propose a QoC metric that is sensitive to control loop instabilities, estimating positional errors in tactile-visual control applications. While they individually show how QoC is affected by end-to-end latency, jitter, and packet drops, they don’t provide a coherent abstraction of QoC based on their combined understanding for robotic workloads and still lack in indicating relevant tradeoffs and joint optimization potentials. [9] tackles this by providing such an abstraction based on tradeoffs between latency and reliability for the problem of estimating dynamical systems over communication channels. Using this abstraction they obtain an optimal code-block length for the state estimation problem. Their communication model, however, is based on information/coding theory for finite blocklengths. Overall, we observe a dearth in literature where an abstraction of a collaborative robotic system parameterized with both latency and reliability is used to jointly optimize control applications connected with a 5G network. We aim to fill this gap, assuming a multi-robot setup where they collaborate towards a joint task, having offloaded their motion-level control to the 5G edge, with our contributions: • We derive a QoC-based abstraction for collaborative robotics, considering both network delay and reliability. • We exemplify the applicability of our approach with a consensus-based scenario, by considering a joint robotic system and network optimization. • Our simulation results show that by relaxing the delays in some cases, we save up to 32% in energy consumption as compared to State-of-the-art (SOTA) evaluation schemes. The rest of the paper is structured as follows: In Section II we introduce the system model before we derive QoC for a collaborative robotic system in Section III. We present our optimization framework, evaluation methodology and results in Section IV, while the paper is concluded in Section V."
https://arxiv.org/html/2411.07235v1,"Circulating Currents in Electric Machines:
Positive Impact of The End Windings Length on Losses","Circulating currents occurring in windings of electric machines received rising interest recent years. Circulating currents represent unwanted currents flowing between parallel-connected conductors. This phenomenon is due to various reasons such as asymmetries in the winding and differences in electric potential between parallel-connected conductors. This effect occurs both at no-load and on-load conditions, and always lead to uneven distribution of the current between the parallel conductors, therefore leading to higher losses, as proven in the authors’ previous work. Circulating currents are occurring mainly due to asymmetries and electric potential difference in the active part, meaning that long end windings are advantageous to mitigate the effect of circulating currents. Losses due to circulating currents decrease at a rate proportional to the inverse square of the end windings length. The aim of this paper is to mathematically prove this property and present a case study application in an electric machine.","The phonemenon of circulating currents is defined and reviewed in [1, 2], with an application in electric machines. Circulating currents become a concern in the windings of electric machines especially at high rotational speeds [3, 4, 5, 6, 7]. The same phenomenon is observed in transformers and inductors operating at high frequencies [8, 9, 10, 11, 12, 13]. Circulating currents cause uneven distribution of the total current, flowing in the bundle, between the parallel-connected conductors. This occurs also at no load, meaning that circulating currents are flowing between the parallel conductors, with a total current being zero. This uneven distribution always leads to higher losses in the windings compared to the case of even distribution of current between the conductors. This fundamental property of circulating currents is proved mathematically by authors in [1] with a case application in electric machines. The causes of circulating currents and the drawbacks are also discussed in [1]. The major causes are the differences in the inductance and the electric potential, which can be due to different placements of parallel conductors inside the slots [14, 15, 16, 17, 18, 19]. Circulating currents occur also in other situations in electric machines [20], however, in this work, we focus on this effect occurring in parallel-connected conductors. There exists other phenomena in windings, not covered in this paper, such as the skin effect [2, 21] and the proximity effect [2, 21, 22, 23, 24, 25], causing higher losses. These effects have been previously reviewed by authors in [2], summarizing the main models in the literature, with an application in electric machines. Several solutions have been proposed in the literature to reduce losses due to circulating currents such as transposition [26, 27, 28, 29, 30, 31, 32, 33]. Circulating currents can also be mitigated by increasing the length of the end windings. This will result in higher joules losses, however, it might be beneficial in situations where circulating currents losses are very high. More specifically, losses due to circulating currents decrease at a rate proportional to the inverse square of the end windings length. This property remains valid for conductors with both circular [34, 35, 36] and rectangular shapes [37, 38, 39]. The aim of this paper is to present a proof of this this property in a mathematical framework. The main models, allowing the evaluation of losses due circulating currents in electric machine’s windings are reviewed in [2] and are categorized into three main types: finite element models coupled with circuit analysis [40], analytical models [14, 26, 27], and hybrid models [41], combining results from finite element analysis and analytical formulas. The hybrid model [41] is used in this work to prove the property mentioned above. The basic terms and the preliminary formulas used in this paper are listed in Section 2. The definition of circulating currents [1] is re-presented in Section 3 as well as the model used. Section 4, presents the property mentioned above, initially discussed in previous authors’ work111The property and its proof can be referred to using this paper and the preceding paper [2]: https://ieeexplore.ieee.org/document/10366258. (Appendix A of [2]), but extended in this paper in a detailed and complete version, along with a mathematical proof framework and a case application in electric machines in Section 5."
https://arxiv.org/html/2411.07192v1,Data-Driven Predictive Control of Nonholonomic Robots Based on a Bilinear Koopman Realization: Data Does Not Replace Geometry,"Advances in machine learning and the growing trend towards effortless data generation in real-world systems has led to an increasing interest for data-inferred models and data-based control in robotics. It seems appealing to govern robots solely based on data, bypassing the traditional, more elaborate pipeline of system modeling through first-principles and subsequent controller design. One promising data-driven approach is the Extended Dynamic Mode Decomposition (EDMD) for control-affine systems, a system class which contains many vehicles and machines of immense practical importance including, e.g., typical wheeled mobile robots. EDMD can be highly data-efficient, computationally inexpensive, can deal with nonlinear dynamics as prevalent in robotics and mechanics, and has a sound theoretical foundation rooted in Koopman theory. On this background, this present paper examines how EDMD models can be integrated into predictive controllers for nonholonomic mobile robots. In addition to the conventional kinematic mobile robot, we also cover the complete data-driven control pipeline – from data acquisition to control design – when the robot is not treated in terms of first-order kinematics but in a second-order manner, allowing to account for actuator dynamics. Using only real-world measurement data, it is shown in both simulations and hardware experiments that the surrogate models enable high-precision predictive controllers in the studied cases. However, the findings raise significant concerns about purely data-centric approaches that overlook the underlying geometry of nonholonomic systems, showing that, for nonholonomic systems, some geometric insight seems necessary and cannot be easily compensated for with large amounts of data.","Autonomous mobile robots have become indispensable in many applications – be it as service robots for vacuuming and cleaning, for logistics and delivery, and in security and defence. In many of these applications, there is nowadays a tendency to move toward the application of whole robot fleets, where the individual robots are comparatively simple and mass-produced in an economical manner, e.g., by minimizing the number of moving parts and not relying on high-precision driveline components. This has multiple repercussions. On the one hand, whereas omnidirectional and fully actuated mobile robots exist on the ground and in the air, bringing advantages regarding maneuverability and thereby simplifying control and path planning, economic and simplicity considerations have led to a prevalence of underactuated, nonholonomic mobile robots [1, Ch. 8]. However, precise control of nonholonomic systems presents a challenge to this day [2, 3, 4, 5], in particular for optimal control [6, 7, 8]. Nevertheless, optimal control, such as model predictive control (MPC), has some key advantages making it very suitable for robotics, including straight-forward controller tuning, clear definition of the control goal, and the direct consideration of constraints, which allows robots to operate safely at the border of their operating regime for maximum task-solution performance. This motivates the usage of MPC as this paper’s control method of choice. On the other hand, cost-efficient, mass-produced robots that do not rely on high-precision parts may divert from a nominal, ideal behavior as it may be described by a nominal model. This can motivate to use a data-driven or data-augmented model for an MPC controller instead of a (purely) nominal one [9, 10]. Ideally, few data points are needed to obtain a model useful for model-based closed-loop control, so that newly produced robots can be ”calibrated” quickly and cost efficiently. As indicated before, model predictive control of nonholonomic vehicles is by no means simple if the control task is to ensure asymptotic stabilization of arbitrary setpoints. The latter can be practically useful, e.g., for a measurement robot that needs to reach certain poses with a very high accuracy to perform accurate measurements. It was only quite recently that it was shown that, for this task – even for the arguably simplest nonholonomic system, i.e., the differential-drive mobile robot – the canonically employed quadratic cost function provably does not lead to a functional MPC controller [7]. On the contrary, tailored non-quadratic stage costs [11, 12] or terminal conditions (either using non-quadratic terminal costs or non-convex terminal regions, see, e.g., [13, 14]) yield a guaranteed setpoint stabilization. The latter would be undesirable practically as it can introduce feasibility issues and control failure. Until very recently, it was even unknown how to generally design functioning cost functions for general nonholonomic vehicles, i.e., with higher degrees of nonholonomy and with drift [8, 15]. In [8], it has become clear that tailored, non-norm, mixed-exponents cost functions, which can be inferred from a homogeneous approximation of the nonholonomic system [16, 4], are key, and [15] has extended this to nonholonomic systems with certain kinds of actuation dynamics, i.e., if acceleration is not immediate and, thus, velocity setpoints are, in the model, not reached immediately. The respective stability analysis can be nicely embedded in the framework [16] for general nonlinear systems with null-controllable homogeneous approximations. In the above context, this paper provides several new contributions. Firstly, to the knowledge of the authors, it is the first time that purely data-driven model predictive control of a nonholonomic vehicle is performed and experimentally validated with a theory-conforming mixed-exponents cost function, without using a predefined nominal model. Secondly, as will be seen, we employ EDMD on a Koopman background to infer the model used for MPC. As far as the authors are aware, in this framework, this paper represents the first occasion where a second-order dynamics model of a nonholonomic mobile robot accurate enough for closed-loop MPC control is learned from real-world data. As will be seen, the drift present in this model has some repercussions on data acquisition, model learning, and on control design, which are all solved within this paper. This paper builds upon our previous work [17], in which a first-order kinematic model was considered, approximated, and its performance assessed regarding open-loop predictions only. Compared to the latter, novelties include studying closed-loop control in a model predictive controller with cost tailored to nonholonomic systems’ sub-Riemannian geometry [4] in conjunction with data-inferred models as well as data-inferred models capable of taking into account acceleration-level effects in the first place, including how to obtain and process real-world data for them. The paper is organized as follows. Section 2 introduces the considered system class, model predictive control, and the EDMD approach for control-affine systems. Section 3 then introduces the kinematic and second-order dynamics differential-drive robot, as well as the specialities of controlling such a nonholonomic vehicle by means of a model predictive controller. In Section 4, the data-driven models for the kinematic mobile robot, inferred from experimental measurement data, are used within predictive controllers, both in simulation, to allow large-scale comparative studies under ideal conditions, and in hardware experiments for validating whether the closed-loop controllers work as promised. In Section 5, real-world data is collected for the differential-drive robot with actuator dynamics, and insight is given into post-processing the data and identifying an EDMD-based model that accounts for actuator dynamics. This model is then used within predictive controllers showing the necessity of a cost function taking into account the system’s sub-Riemannian geometry arising from the nonholonomic constraints. Section 6 further points out how less data is needed when utilizing an EDMD-based predictive controller. A brief summary and outlook is given in Section 7. Subsequently, for integers n,m\in\mathbb{Z} with n\leq m, we define \mathbb{Z}_{n:m}\coloneqq\mathbb{Z}\cap[n,m]. Bold variables denote a vector, or in the case of a capital bold letter, a matrix, e.g., \bm{v} and \bm{A}, respectively. Furthermore, a part of a vector is denoted by specifying its component’s indices as a subscript, e.g., \bm{v}_{2:4} containing components 2 to 4 of vector \bm{v} in their original order."
https://arxiv.org/html/2411.07069v1,Two-Stage Stochastic Optimization for Low-Carbon Dispatch in a Combined Energy System,"While wind and solar power contribute to sustainability, their intermittent nature poses challenges when integrated into the grid. To mitigate these issues, renewable energy can be combined with coal-fired power and hydropower sources to stabilize the energy system, with battery storage serving as a backup source to smooth the total output. This study develops a low-carbon dispatch model for a combined energy system using a two-stage stochastic optimization approach. The model incorporates a carbon trading mechanism to regulate emissions and addresses the uncertainty in wind and solar outputs by clustering output curves into typical scenarios to derive a joint distribution. In the initial stage of scheduling, decisions are made regarding the unit commitment for coal-fired power plants. The second stage optimizes the expected operation cost of other energy generation sources. The feasibility of the model is demonstrated by comparing the results of stochastic and deterministic scenarios through simulation. Analysis of different carbon prices further explores the impact of the carbon trading mechanism on the system’s operation cost.","As traditional fossil fuels become depleted and renewable energy experiences unprecedented growth, the electric power system is shifting toward a cleaner, more efficient, and economically sustainable energy framework. In this era of energy transition, high emissions from existing coal-fired power plants (CFPPs) present a significant challenge to achieving the 1.5°C climate goal, while the intermittency and variability of renewable energy sources create obstacles to grid reliability. Relying exclusively on either traditional or renewable energy sources is not a viable long-term solution. Instead, an integrated combined energy system harnesses the complementary strengths of diverse generation sources, offering a more resilient approach. In a combined energy system, the coordinated output from each generation source is key to achieving efficient planning and operation. Traditionally, deterministic optimization methods have been used, where all inputs are assumed to be known and fixed. However, these methods often fail to capture the inherent uncertainty in renewable outputs. To address this issue, advancements in generation unit commitment (UC) and economic dispatch (ED) models under uncertainty have been made. These improvements include robust optimization, distributionally robust optimization, and stochastic optimization [1]. Robust optimization aims to minimize the highest possible cost considering all potential scenarios with uncertain inputs. For example, an adaptive robust optimization model is proposed in the coordinated operation of wind, solar, and battery storage to address the UC problem[2]. Considering the combination of traditional coal power units and wind power, a distributionally robust planning model for UC problems is developed in [3]. Although robust optimization reduces the need for scenario-based modeling, it can lead to higher operational costs due to its conservative nature. Stochastic optimization considers the minimization of the expected cost across all scenarios. In unit commitment and economic dispatch (UCED) problems, it accounts for both the fixed cost of scheduling generating units and the variable cost of dispatching them based on uncertainties. A security-constrained unit commitment stochastic model is proposed to combine wind and traditional power systems, utilizing neural network-based prediction intervals to address wind power forecast uncertainties [4]. A stochastic programming model is proposed for the planning of a multi-source energy system comprising hydropower, thermal power, wind power, and pumped storage systems [5]. A multistage stochastic programming approach is employed to manage microgrids that are operated with variable renewable energy sources and battery storage, taking into account short- and long-term uncertainties[6]. Scenario-based stochastic optimization is widely regarded as a common approach for capturing uncertainty, relying on scenario representation algorithms to model various uncertainty factors. A scenario mapping technology is introduced in a stochastic unit commitment model to compress a large number of renewable energy scenarios [7]. Traditional energy system optimization typically focuses on minimizing operational costs, often neglecting the environmental impact. As the energy transition accelerates, there is an increasing focus on low-carbon power dispatch, which considers the external costs of greenhouse gas (GHG) emissions and the interaction of electricity and carbon markets. A distributed robust optimization model is developed for the wind and thermal power combined system in the carbon market, using carbon emission reductions and cost savings as the key evaluation criteria [8]. A risk-constrained ED model is introduced for electricity-gas systems, incorporating carbon trading prices and integrating carbon capture systems and demand response technologies [9]. In the integrated energy systems (IES), the stochastic scheduling model is often combined with the segmented index-based carbon market model [10] and life cycle assessments [11] to evaluate and limit greenhouse gas emissions. The contributions of this paper can be summarized as follows: (1) a two-stage stochastic programming model is proposed to address the uncertainties in renewable energy sources for UCED problem, and (2) a carbon trading mechanism is integrated to enable low-carbon dispatch. The rest of the paper is organized as follows. Section II introduces the framework of the proposed low-carbon dispatch model and the carbon trading mechanism. Section III establishes the two-stage stochastic optimization model for the combined energy system. In Section IV, a case study and analysis of the impact of various carbon prices are conducted. Conclusions are drawn in Section V."
https://arxiv.org/html/2411.06905v1,Co-Scheduling of Energy and Production in Discrete Manufacturing Considering Decision-Dependent Uncertainties,"Modern discrete manufacturing requires real-time energy and production co-scheduling to reduce business costs. In discrete manufacturing, production lines and equipment are complex and numerous, which introduces significant uncertainty during the production process. Among these uncertainties, decision-dependent uncertainties (DDUs) pose additional challenges in finding optimal production strategies, as the signature or the shape of the uncertainty set cannot be determined before solving the model. However, existing research does not account for decision-dependent uncertainties (DDUs) present in discrete manufacturing; moreover, current algorithms for solving models with DDUs suffer from high computational complexity, making them unsuitable for the real-time control requirements of modern industry. To this end, we proposed an energy-production co-scheduling model for discrete manufacturing, taking into account decision-dependent uncertainties (DDUs). Subsequently, we devised a method for transforming the constraints associated with DDUs into linear form ones. Finally, we designed a novel algorithm based on the column-and-constraint generation (C&CG) algorithm and undertook a theoretical analysis of its performance of convergence and algorithmic complexity. A real-world engine assembly line was used to test our model and algorithm. Simulation results demonstrate that our method significantly reduces production costs and achieves better frequency regulation performance.","I-A Backgroound The discrete manufacturing sector has experienced significant growth in recent years, driven by the globalization of supply chains and the rise of customization [1]. Unlike process manufacturing, such as the chemical industry, discrete manufacturing—like engine assembly—typically involves a greater number of machines and more flexible and interchangeable process sequences. This results in a higher degree of freedom at the scheduling level, allowing for more complex decision-making and adjustments during production [2]. This has led to a growing interest in the co-scheduling problem based on production and energy in discrete manufacturing. However, the inherent uncertainty of the discrete manufacturing industry presents a significant challenge to efficient scheduling [3, 4, 5]. The uncertainties inherent to discrete manufacturing can be classified into two categories: decision-independent uncertainties (DIUs) and decision-dependent uncertainties (DDUs). Modern discrete manufacturing industrial parks typically consist of a production system and an energy system. The production system includes production lines and warehousing facilities, while the energy system comprises distributed energy resources (DER) generation equipment and energy storage systems. In the industrial park, DIUs originate from the intrinsic attributes of the equipment or the production environment, for instance, the quality of the raw materials or the daily power generation of solar panels. In contrast, DDUs are uncertainties related to production decisions. The following are the main types of DDUs in industrial parks: (1) The inclination towards producing main or by-products is related to uncertain order status and future production conditions, and it’s also connected to current production decisions; (2) Product yield is influenced by both inherent equipment uncertainties and the choice of the production line; (3) Power frequency regulation requirements in production objectives are constrained by actual electricity usage decisions and are linked to unknown requirements form the power utility as well. A production and energy co-scheduling model for discrete manufacturing industries can be modeled as a two-stage robust optimization model. In the production process, production profit serves as the primary objective, while energy requirements (such as frequency regulation and minimizing battery degradation) are considered secondary. As a result, this scheduling process is modeled as a two-stage optimization problem. Additionally, due to the presence of uncertainties, including DIUs and DDUs, the model becomes a robust decision-making problem. I-B Related Works Existing research on uncertainties in the scheduling process of discrete manufacturing primarily focuses on decision-independent uncertainties (DIUs). Work [6] concentrates on discrete manufacturing industries with a focus on DIUs of fluctuations in product and customer demand. It proposes a two-dimensional model to assess the ability to cope with these DIUs. Work [7] focuses on the DIU of fluctuating daily production quantities and employs the non-dominated sorting adaptive differential evolution (NSJADE) algorithm to obtain a robust order scheduling solution. Work [8] focuses on the uncertainty of machine failures, which are modeled using an exponential distribution and then handled through Markov chains. The uncertainty addressed in work [9] is the examination of machine yield as a fuzzy variable, which is characterized by a probability-of-failure-working-time equation. Work [10] is concerned with DDU of performance degradation (e.g., equipment failures) within discrete manufacturing. To address these uncertainties, a simultaneous update method combining Adaboost, DNN, and LSTM is proposed. However, some uncertainties within these DIUs, such as equipment defect rates [9, 10]—which are not only related to the inherent uncertainties of the equipment but also to the combination of upstream production equipment—should be modeled as decision-dependent uncertainties (DDUs). Current algorithms for solving two-stage robust optimization models considering DDUs are primarily based on iterative decomposition algorithms or scenario tree-based partitioning algorithms. In work [11], recursive optimization methods and numerical enumeration techniques are employed to solve the multi-stage models with the presence of DDUs. Work [12] investigates the two-stage robust optimization with polyhedral DDUs and proposes an iterative algorithm based on Benders decomposition. The algorithm employs optimality and feasibility cuts to address the coupling between uncertainty and decision-making. Work [13] initially transforms the multi-stage optimization problem containing DDU into its dualized form, subsequently employing the adversarial cutting plane algorithm to resolve the model. Work [14] employs a scenario tree based on model predictive control to transform the optimization problem containing DDUs into a series of subproblems to be solved under different learning scenarios. Work [15] uses an adaptive reliability improvement unit commitment (ARIUC) algorithm to efficiently decompose the DDU-related model into multiple sub-problems for solving. In conclusion, works [11, 12, 13] employ iterative decomposition algorithms, while works [14, 15] utilize partitioning approaches. However, both of these algorithms (iterative decomposition algorithms and scenario tree-based partitioning algorithms) exhibit high computational complexity, making them unsuitable for the real-time and rapid scheduling needs of modern industrial applications. I-C Methodology To sum up, existing work on the co-scheduling of industrial production and energy, considering the presence of uncertainties, presents the following two issues about modeling and solving: 1) Modeling: Existing work does not examine the discrete manufacturing scheduling problem that encompasses DDUs; rather, it focuses exclusively on DIUs. However, some of the DIUs therein are pertinent to decision-making processes and should be modeled as DDUs. 2) Solving: In contrast to previous methodologies for DDU-related two-stage problems that focused solely on solvability, the modern industry requires algorithms that converge with low computational complexity, thereby enabling rapid and real-time control to achieve optimal production results. In light of the aforementioned issues, we proposed a two-stage robust optimization model considering a complete set of three types of DDUs for discrete manufacturing, and a novel algorithm with performance guarantees is also designed. First of all, we constructed a model comprising multiple DDUs. Then, we simplify the constraints associated with the aforementioned three DDUs to linear forms using methods based on ambiguous sets, the imprecise Dirichlet model, and Cantelli’s inequality, respectively. The problem was subsequently reduced to a mixed integer quadratic program (MIQP) problem. Subsequently, we proposed the decision-dependent C&CG (DDCCG) algorithm based on the traditional C&CG algorithm and provided a performance analysis for its convergence and optimality. Finally, we use the Petri net to represent a real-world engine assembly line and test our model and algorithm on this study case. The findings demonstrate that the industrial park reduces costs, enhances the anti-interference capabilities of the production line, and responds to frequency regulation and peak shaving requirements. These results substantiate the superiority of the proposed model and validate the reliability of the underlying algorithm. In summary, the main contributions of this paper include: • This paper presents a two-stage robust optimization model for the discrete manufacturing industry that considers multiple DDUs. In particular, the model incorporates product yield DDU, frequency regulation penalty DDU, and product structure DDU into the energy and production co-scheduling process. In comparison to previous work in this field, this model more accurately reflects the interdependence between decisions and uncertainties in real-world industrial parks. • We presented three paradigms for reducing different DDU-related constraints to a linear form. In particular, the three types of DDUs in discrete manufacturing are mathematically represented as ambiguous sets, univariate distributions, and multivariate distributions, which encompass the majority of real-world DDUs. Therefore, the methods presented in this paper can be extended and applied to other engineering scenarios in the real world. • We proposed the DDCCG algorithm for the decision-dependent two-stage robust optimization model and proved its convergence and optimality theoretically. The original C&CG algorithm has a fast convergence speed but cannot adapt to the decision-dependent model. Our work removes this limitation and can be adapted to the needs of modern industry. The remainder of this paper is structured as follows. Section II introduces the basic form of the optimization problem. Section III presents modeling and linearization techniques for DDUs. Section IV describes the subsequent simplification of the problem and algorithm design. Section V is case analysis. The last section summarizes our work. Figure 1: Schematic diagram of the overall model architecture."
https://arxiv.org/html/2411.06808v1,Modeling and Detection of Critical Slowing Down in Epileptic Dynamics,"Epilepsy is a common neurological disorder characterized by abrupt seizures. Although seizures may appear random, they are often preceded by early warning signs in neural signals, notably, critical slowing down, a phenomenon in which the system’s recovery rate from perturbations declines when it approaches a critical point. Detecting these markers could enable preventive therapies. This paper introduces a multi-stable slow-fast system to capture critical slowing down in epileptic dynamics. We construct regions of attraction for stable states, shedding light on how dynamic bifurcations drive pathological oscillations. We derive the recovery rate after perturbations to formalize critical slowing down. A novel algorithm for detecting precursors to ictal transitions is presented, along with a proof-of-concept event-based feedback control strategy to prevent impending pathological oscillations. Numerical studies are conducted to validate our theoretical findings.","I INTRODUCTION Epilepsy is one of the most common and serious chronic neurological disorders [1], characterized by the sudden onset of harmful seizures. Current interventions—such as medication, surgery, and neuromodulation—offer limited effectiveness due to their empirical nature [2]. This highlights an urgent need for systematic, mechanistic approaches to develop optimized, individualized therapies. Modeling epilepsy dynamics offers a promising approach to understanding individual mechanisms, fostering the development and testing of personalized, optimized therapies. Epilepsy involves two primary brain states: the interictal state, characterized by normal neural activity, and the ictal one, marked by excessively synchronized oscillations [3]. A variety of models use bistability to capture these two states [4], with a stable equilibrium representing normal neural activity and a stable limit cycle representing pathological oscillations. Transitions between these states model seizure onset and offset. Studies suggest that transitions can be triggered by internal disturbances or external stimuli [5], aligning with the dynamics observed in absence seizures [6]. However, simple bistable mechanisms may not fully capture the dynamics of all epilepsy types. In many cases, such as temporal lobe epilepsy, seizures arise from gradual changes at cellular, molecular, and network levels in the brain [7]. Some studies suggest that these transitions are tied to the brain’s gradual drift toward a critical threshold, progressively destabilizing healthy states. Upon crossing the critical point, such normal states transition to pathological ones abruptly [8]. Interestingly, neural signals at the verge of the critical point often exhibit critical slowing down [9], a phenomenon characterized by slower recovery times after perturbations (see Fig. 1). This has been explored as a potential biomarker for seizure prediction [10]. A rigorous understanding of critical slowing down holds significant potential for designing effective schemes to detect early warning signs of seizures and for developing timely preventive interventions. Far from critical transitionNear critical transitionFast recoverySlow recoveryGradual drift Figure 1: Illustration of critical slowing down Related work. Researchers have employed networks of linear-threshold units to study epileptic dynamics, focusing on bifurcations between equilibria and limit cycles [11, 12, 13]. Additionally, several studies have developed network control strategies aimed at preventing the spread of pathological oscillations [14] or restoring normal states [15, 16, 17]. Other approaches use linear fractional-order systems [18, 19] and bilinear systems[20] to model and regulate epilepsy. However, none of these works address the modeling and detection of critical slowing down. Recent work offers methods to detect critical slowing down in network systems [20], but they rely on passive monitoring of the states and work only when states remain close to the equilibrium. Contribution. This paper provides an analytical exploration of critical slowing down in epileptic dynamics with four main contributions. First, we introduce a multi-stable slow-fast system to model epileptic dynamics, where a slow state captures changes in neuronal excitability, leading to dynamic bifurcations that initiate or terminate epileptiform oscillations by shifting between stable states. Second, we construct regions of attraction for these states, offering a theoretical basis for transitions between normal neural activity and pathological oscillations. Third, we derive a convergence rate for any perturbations within these attraction regions, providing formal evidence of critical slowing down. This is different from existing approaches that rely on linearization, which only applies to small, near-equilibrium perturbations [9, 20]. Fourth, leveraging this derived convergence rate, we design an active detection scheme that probes and monitors the system to identify pre-ictal events, setting it apart from traditional methods relying on passive recordings [8, 10]. Additionally, we present a proof-of-concept feedback control strategy that prevents seizure onset by responding to detected precursors, laying a foundation for preventive therapies for epilepsy with minimal intervention. Our theoretical results are demonstrated by numerical studies."
https://arxiv.org/html/2411.06787v1,A System Parametrization for Direct Data-Driven Analysis and Control with Error-in-Variables,"In this paper, we present a new parametrization to perform direct data-driven analysis and controller synthesis for the error-in-variables case. To achieve this, we employ the Sherman-Morrison-Woodbury formula to transform the problem into a linear fractional transformation (LFT) with unknown measurement errors and disturbances as uncertainties. For bounded uncertainties, we apply robust control techniques to derive a guaranteed upper bound on the \mathcal{H}_{2}-norm of the unknown true system. To this end, a single semidefinite program (SDP) needs to be solved, with complexity that is independent of the amount of data. Furthermore, we exploit the signal-to-noise ratio to provide a data-dependent condition, that characterizes whether the proposed parametrization can be employed. The modular formulation allows to extend this framework to controller synthesis with different performance criteria, input-output settings, and various system properties. Finally, we validate the proposed approach through a numerical example.","I INTRODUCTION In recent years, interest in system analysis and controller design based on collected data has been continuously increasing [1, 2, 3]. Indirect data-driven control first identifies a model to employ model-based techniques for controller design and analysis as a second step. In practice, only a finite number of possibly noisy data points are available, making identifying the true system challenging in many cases, even for the linear case [4]. This mismatch between the true and identified model may lead to instabilities or deteriorated performance guarantees when applying the controller to the true system. A possible remedy is offered by direct data-driven control, where end-to-end guarantees can be provided even for a finite number of noisy data samples [5]. Here, available results are often restricted to bounded noise, which includes, e.g., energy bounds and individual noise bounds for each time instance. Many results in the literature build on the data-informativity framework [6, 7], which provides stability and performance guarantees for the true system by verifying a desired property for all systems consistent with the data. Another direct data-driven approach relies on parameterizing the set of consistent system matrices in terms of a right inverse of the stacked data matrices [8, 9]. Most existing works consider only the case of additive process noise [10, 11]. Although additive process noise often appears in linear dynamical systems, it is also possible to approximate nonlinear systems via basis functions and treat the resulting approximation error as an additive disturbance [3]. By deriving a bound on the approximation error, it is possible to provide guarantees for unknown nonlinear systems. When collecting data, the occurring measurement noise introduces yet another error source leading to the error-in-variables problem. Without explicitly addressing the measurement noise, the derived analysis often results in biases. For example, in the linear least squares problem, not considering error-in-variables results in systematically smaller parameters [12]. In system identification, the bias can be eliminated based on set-membership estimation, where the set of consistent systems is identified [13]. Similarly, [14] designs a stabilizing controller using such an identified set. To achieve this, the authors reformulate the problem as a sum-of-squares problem, which can be solved iteratively using SDPs. To design guaranteed stabilizing controllers for sufficiently small measurement noise, an alternative is to introduce a regularization term to a predictive controller based on Hankel matrices [15]. This leads to a trade-off between a simple parametrization and reliance on data. Moreover, [8] treats error-in-variables as additive process noise, which requires additional knowledge of the singular values of the true system. However, this knowledge may not always be available. Recent results extend the data-informativity setting to the case with error-in-variables under the assumption that the process noise and the measurement error satisfy a particular quadratic matrix inequality [16] . In this work, we derive an explicit and exact parametrization of all systems consistent with the data by interpreting the data-driven control problem as linear regression. The main contribution of this paper is to generalize the results of [8, 11] to the errors-in-variables problem. Moreover the proposed parameterization allows for a more flexible description of the noise affecting the data than the data-informativity framework, while still requiring the solution of only a single SDP. Furthermore, we exemplify the use of the derived parametrization to compute an upper bound on the \mathcal{H}_{2}-norm of the true unknown system. Finally, by formulating the parameterization in terms of an LFT, many already existing methods from robust control can be applied to extend this setup for different performance criteria, noise descriptions and controller design adding flexibility."
https://arxiv.org/html/2411.06689v1,Resilient control under denial-of-service and uncertainty:  approach,"In this paper, a new framework for the resilient control of continuous-time linear systems under denial-of-service (DoS) attacks and system uncertainty is presented. Integrating techniques from reinforcement learning and output regulation theory, it is shown that resilient optimal controllers can be learned directly from real-time state and input data collected from the systems subjected to attacks. Sufficient conditions are given under which the closed-loop system remains stable given any upper bound of DoS attack duration. Simulation results are used to demonstrate the efficacy of the proposed learning-based framework for resilient control under DoS attacks and model uncertainty.","As modern control engineering systems become more and more complex and uncertain, it is important to learn controllers from online data that avoids the reliance on the accurate model knowledge. A plausible strategy is to learn the dynamics of control systems first, and then design controllers based on the learned dynamics [1, 31]. The success of this strategy usually requires the discrepancy between learned and actual dynamics to be small enough. The second strategy is to learn control policy directly from online data instead of learning dynamics [23, 15, 22, 14]. Following this strategy, adaptive dynamic programming (ADP) has been used to search for the solution to an optimal decision making problem based on active agent-environment interactions [28, 19, 26, 21, 16, 8, 30]. Its efficiency has been proved especially in situations where the exact system model is difficult or even impossible to be acquired. Apart from the unknown system dynamics, modern control systems with ADP algorithms must also deal with nonvanishing disturbance, dynamic uncertainty and cyber attacks. By the integration of differential game theory and ADP, one can learn the Nash policy to attenuate the effect from nonvanishing disturbances [27, 20]. In order to further reject disturbance while asymptotically tracking some prescribed reference signal, ADP has been leveraged to achieve output regulation in an adaptive optimal sense [6, 2, 13]. However, these methods usually consider static uncertainties in the system based on an assumption that the system order is known exactly. In order to relax this assumption, robust ADP (for short, RADP) has been proposed via a combination of ADP and nonlinear small-gain theory [12]. Under some mild small-gain condition, one can learn a robust optimal control policy via RADP for unknown control systems in the presence of dynamic uncertainties. Nevertheless, most of existing (robust) ADP methods ignore the resiliency of control systems when the control system is subject to malicious attacks. As a superset of ADP, reinforcement learning has been studied to improve the resilience of control systems while preserving the optimality or at least suboptimality; see [11, 10] and references therein. However, most of these studies fail to guarantee the asymptotic stability of the closed-loop system. In our previous work, we have proposed a model-free method to analyze the resiliency and the stability of control systems in closed-loop with controllers learned by ADP [7]. Without knowing exactly the system dynamics, one can provide an estimate on the upper bound of cyberattacks for which the control system remains operational. However, this method of [7] may fail to work whenever the cyberattacks go beyond our estimation. As cyberattacks become increasingly frequent in modern engineering systems, it is urgent to develop a novel learning-based controller design framework that addresses resiliency besides the common issues of stability and optimality. Contributions The contributions of this paper are summarized as follows. First, we develop an original learning-based resilient optimal controller design framework via ADP for a class of continuous-time linear systems affected by disturbances and denial-of-service (DoS) attacks. Interestingly, one can rely on this framework to design a resilient optimal control policy to achieve output regulation (i.e., asymptotic tracking with disturbance rejection) given any upper bound of DoS attack duration. Second, comparing with existing resilient control design approaches [5, 4, 3, 25], the overall control design process under the developed framework does not require the precise knowledge of system dynamics in the state equation. Third, this framework is a data-based generalization of the average dwell-time approach in model-based switched system theory [32, 18]. Structure The remainder of this paper is organized as follows. Section II formulates an optimal output regulation problem with desired convergence rate and presents a model-based solution to this problem. In Section III, we propose a learning-based resilient optimal controller design framework and an algorithm based on the proposed framework. We have also rigorously analyzed the resiliency of the closed-loop system. Section IV contains simulation results on a benchmark adaptive cruise control problem of an autonomous vehicle under DoS attacks by leveraging the proposed algorithm. Section V closes the paper with brief concluding remarks. Notations Throughout this paper, \mathbb{R}_{+} denotes the set of nonnegative real numbers, and \mathbb{Z}_{+} the set of nonnegative integers. \mathbb{C}^{-} indicates the open left-half complex plane. |\cdot| represents the Euclidean norm for vectors and the induced norm for matrices. The symbol \otimes indicates the Kronecker product operator and {\rm vec}(A)=[a_{1}^{T},a_{2}^{T},\ldots,a_{m}^{T}]^{T}, where a_{i}\in\mathbb{R}^{n} are the columns of A\in\mathbb{R}^{n\times m}. When n=m, \sigma(A) is the spectrum of A. For an arbitrary column vector v\in\mathbb{R}^{n}, {\rm vecv}(v)=[v_{1}^{2},v_{1}v_{2},\ldots,v_{1}v_{n},v_{2}^{2},v_{2}v_{3},% \ldots,v_{n-1}v_{n},v_{n}^{2}]^{T}\in\mathbb{R}^{\frac{1}{2}n(n+1)}. {\rm vecs}(P)=[p_{11},2p_{12},\ldots,2p_{1m},p_{22},2p_{23},\ldots, 2p_{m-1,m} ,p_{mm}]^{T}\in\mathbb{R}^{\frac{1}{2}m(m+1)} for a symmetric matrix P\in\mathbb{R}^{m\times m}, and \lambda_{M}(P) (resp. \lambda_{m}(P)) denotes the maximum (resp. minimum) eigenvalue of a real symmetric matrix P. P\succ 0 (resp. P\prec 0) represents that P is positive (resp. negative) definite."
https://arxiv.org/html/2411.06649v1,A Novel Combined Data-Driven Approach for Electricity Theft Detection,"The two-way flow of information and energy is an important feature of the Energy Internet. Data analytics is a powerful tool in the information flow that aims to solve practical problems using data mining techniques. As the problem of electricity thefts via tampering with smart meters continues to increase, the abnormal behaviors of thefts become more diversified and more difficult to detect. Thus, a data analytics method for detecting various types of electricity thefts is required. However, the existing methods either require a labeled dataset or additional system information which is difficult to obtain in reality or have poor detection accuracy. In this paper, we combine two novel data mining techniques to solve the problem. One technique is the Maximum Information Coefficient (MIC), which can find the correlations between the non-technical loss (NTL) and a certain electricity behavior of the consumer. MIC can be used to precisely detect thefts that appear normal in shapes. The other technique is the clustering technique by fast search and find of density peaks (CFSFDP). CFSFDP finds the abnormal users among thousands of load profiles, making it quite suitable for detecting electricity thefts with arbitrary shapes. Next, a framework for combining the advantages of the two techniques is proposed. Numerical experiments on the Irish smart meter dataset are conducted to show the good performance of the combined method.","The Energy Internet, which is proposed as the next step in the evolution of Smart Grid [1], has the important feature of bi-directional energy and information flow. The advanced metering infrastructure (AMI) is the basis of the information flow in the Energy Internet. With the deployment of smart meters, AMI now provides power utilities with massive amounts of electricity consumption data at a higher frequency, thus enabling precise user behavior modeling [2], load forecasting [3], load estimation [4], and demand response [5]. However, making the information flow of Energy Internet secure has proved to be a challenging issue due to the unique characteristics of AMI. Fraudulent users can tamper with the smart meter data using digital tools or cyber attacks. Thus, the form of electricity thefts in Energy Internet is very different from the thefts in the past, which relies mostly on physically bypassing or destructing mechanical meters [6]. Cases of organized energy theft spreading tampering tools and methods against smart meters that caused severe loss of power utilities were reported by the U.S. Federal Bureau of Investigation [7] and Fujian Daily [8] in China. In total, the non-technical loss (NTL) due to consumer fraud in the electrical grid in the U.S. was estimated to be $6 billion/year [9]. Because the traditional detection methods of sending technical staff or Video Surveillance are quite time-consuming and labor-intensive, electricity theft detection methods that take the advantage of Energy Internet’s information flow are urgently needed to solve the problem of the ”Billion-Dollar Bug”. The existing non-hardware electricity theft detection methods can be classified into three categories: artificial intelligence-based (AI-based), state-based, and game theory-based [10]. The AI-based methods use machine learning techniques, such as classification and clustering to analyze the load profiles of consumers to find the abnormal users because the consumption patterns of fraudulent users are believed to differ from those of benign users. Classification methods [11, 12, 13] usually require the labeled dataset to train the classifier, whereas clustering methods [14, 15, 16] are unsupervised and can be applied to an unlabeled dataset. The state-based methods [17, 18] use additional measurements, such as power, voltage, and current in the distribution network to detect electricity thefts. Because fraudulent users are incapable of tampering with the network measurements, so conflicts will arise between the system states and smart meter records. Although high detection accuracy can be achieved, these methods require the network topology and additional meters. The game theory-based methods [19, 20] assume that there is a game between fraudulent users and power utilities and that different distributions of fraudulent users’ and benign users’ consumption can be derived from the game equilibrium. Detection can be conducted according to the difference between the distributions. Because the game theory-based methods focus on theoretical analysis with strong assumptions, they are beyond the scope of this paper. A brief review of the existing state- and AI-based electricity theft detection methods in the literature is presented here. The physical model of a power network indicates that the system variables should satisfy certain mathematical equations, which derives the consistency of the variables. The state-based methods utilize the fact that tampering with smart meter data will certainly create inconsistencies between system variables including power, voltage and current. In [21], a linear regression method is used to estimate the resistance of distribution lines from active power and current measurements; next, the NTL of each line is calculated according to the estimated resistance value to find the electricity theft. A state-estimation-based approach for distribution transformer load estimation is exploited to detect meter tampering in [22]. The variance of measurements and estimated values is analyzed to create a suspect list of customers with metering problems. Neto et al. proposed a probabilistic methodology for NTL estimation in the distribution network [17]. The technical loss sensitivity in relation to the load variation is derived and the probabilistic distributions of total loss and technical loss are calculated. In their methodology, if the two distributions have big differences, then the NTL is indicated. Han et al. proposed a fast NTL fraud detection (FNFD) scheme in [23], where the NTL is calculated from observer meters and the Recursive Least Square (RLS) algorithm is used to find the correlation between smart meter data and the NTL. FNFD can catch proportional electricity thieves who steal energy at a fixed proportion. In [24], a deep-learning-based real-time mechanism for detecting electricity thefts was proposed. In this mechanism, the state vector estimator (SVE) calculates the attack vector and the state vector from real-time measurements and the power system topology, and an identification scheme based on a deep belief network helps the SVE finds the false data injection (FDI). Xiao et al. proposed an algorithm for regional and individual electricity theft detection using random matrix theory (RMT) in [25]. A pattern signal is constructed from real time power and voltage measurements as an indicator for NTL. Most of the state-based methods rely on the real-time acquisition of the system topology and additional physical measurements, which is sometimes unattainable. In almost all occasions, the tampered load profiles differ from the original ones. The AI-based methods attempt to find the abnormal patterns among all load profiles of the consumers. Nizar et al. applied the Extreme Learning Machine (ELM) for electricity theft detection [11]. The ELM-based approach extracts patterns of customer behavior from historical kWh consumption data and detects abnormal behaviors. In [10], a multi-class support vector machine (SVM) was trained to detect whether a new sample of load profiles is normal or malicious. The problem of imbalanced training dataset is addressed and solved by generating a synthetic dataset. In [12], a Wide & Deep Convolutional Neural Networks (CNN) model was developed and applied to analyze the electricity thefts in Smart Grid. In [14], an optimum-path forest (OPF) based unsupervised NTL identification method was proposed and compared with other well-known clustering methods including k-means and Birch. Zanetti et al. proposed a fraud detection system (FDS) based on anomaly detection on the energy consumption reports from smart meters [15]. In their approach, an FDS state machine is designed to judge whether a grid subsystem is in an abnormal state, and unsupervised techniques of k-means, fuzzy c-means (FCM) and self-organized map (SOM) are used to detect FDI. In our previous work [26], the clustering technique by fast search and find of density peaks (CFSFDP) [27] is applied to detect load profiles with abnormal shapes. In [13], various modeling techniques including supervised SVM, decision trees and bayesian networks, unsupervised OPF and real time state estimation are reviewed. Their basic assumptions, methodologies, and simulation results are presented systematically. In fact, the existing methods have some issues that must be addressed further. For AI-based methods, due to the difficulty in building a labeled dataset of electricity thefts, the application of classification methods is limited. Because the clustering methods are unsupervised, tampered load profiles with normal shapes can not be detected, resulting in low detection accuracy. For the state-based methods, the measurement data and system information acquisition are much more difficult to obtain. In real applications, the consumption patterns which are the focus of AI-based methods and the state consistency which is the focus of state-based methods should be both considered and utilized. In this paper, a real and general scene in which an observer meter is installed for every area containing a group of users is considered. The recorded data of the observer meter are the sum of the electricity consumptions of the area during a certain time interval. The data are available to most of the distribution system operators (DSOs) or electricity retailers. We attempt to combine the advantages of AI- and state-based methods to propose a detecting framework that adapts to the least parameters or system information to ensure general application and achieves good accuracy without any labeled training set. In particular, the maximum information coefficient (MIC) [28] is used to detect the association between NTL and the tampered load profiles with minimal additional system information. Next, CFSFDP is applied to catch thieves whose load profiles are more random and arbitrary according to their abnormal density features. We ensemble the two techniques by combining the suspicion ranks to cover most types of electricity thefts. The main contributions of this paper are as follows. 1. Novel Framework: Proposing a complementary combined electricity theft detecting framework which can quantify the suspicion ranks from both the shape-similarity perspective and the magnitude-correlation perspective. 2. New Techniques: Applying advanced and efficient machine learning techniques for abnormal detection. Specifically, MIC is used as a state-based electricity theft detecting method for correlation analysis, which only requires the observer meter data (i.e., the area total electricity consumption data) in addition to the load profiles and has high accuracy in detecting electricity thefts that appear normal in shapes; the unsupervised learning technique CFSFDP, a parameter-free method, is used to detect load profiles with unusual shapes that MIC cannot consider. 3. Comprehensive Experiments: Conducting comprehensive numerical experiments for different types of electricity theft behaviors and comparing our method with various state-off-the-art methods to verify the effectiveness and superiority of the framework. The rest of this paper is organized as follows. Section II describes the applicable scene and gives the basic problem statement. Section III presents a theory of the two techniques and shows the framework of combined electricity theft detection. Numerical experiments are conducted and the evaluation results are shown in Section IV. Finally, Section V draws conclusions. Figure 1: Observer meters for areas and smart meters for customers"
https://arxiv.org/html/2411.06612v1,An exact active sensing strategy for a class of bio-inspired systems,"We consider a general class of translation-invariant systems with a specific category of output nonlinearities motivated by biological sensing. We show that no dynamic output feedback can stabilize this class of systems to an isolated equilibrium point. To overcome this fundamental limitation, we propose a simple control scheme that includes a low-amplitude periodic forcing function akin to so-called “active sensing” in biology, together with nonlinear output feedback. Our analysis shows that this approach leads to the emergence of an exponentially stable limit cycle. These findings offer a provably stable active sensing strategy and may thus help to rationalize the active sensing movements made by animals as they perform certain motor behaviors.","Biological sensory systems often exhibit an attenuated response to constant (DC) stimuli, allowing such biosensors to excel at detecting changes (AC) rather than measuring absolute values [1]. This feature, often referred to as sensory adaptation, poses significant challenges for conventional state estimation and control. For specific examples related to control theory and systems biology, see [2, 3]. To overcome this sensory adaptation, animals appear to use ancillary movements, referred to as active sensing movements, that drive robust responses in their change-detecting sensory systems [4, 5, 6]. Animals use this strategy to enhance sensory information across sensory modalities, e.g., echolocation [7], whisking [8] and other forms of touch [9, 10], electrosense [11, 12, 13], and vision [14, 15]. It is well established that conditions of decreased sensory acuity lead to increased active movements [16, 17, 18, 13, 12, 15, 7, 19, 20], suggesting a closed-loop perceptual process [21, 22]. The ubiquity of active sensing in nature motivated us to explore the mathematical conditions that might necessitate active sensing. One theory is that active sensing is at least in part borne out of the need for nonlinear state estimation [23, 24]. Under this theory, animals use active sensing—that is, the generation of time-varying motor commands that continuously stimulate their sensory receptors—so that the system states can be estimated from sensor measurements. A complementary approach—and one we pursue in this paper—is that active movements do indeed enhance observability, but that full state estimation itself may be unnecessary. In other words, active sensing movements may enable stabilizing output feedback without recourse to state estimation as an intermediate step. In this paper, we examine a class of systems with a nonlinear sensory output that mimics sensory adaptation and perceptual fading in nature [1, 25, 26] resulting in a system whose linearized dynamics is unobservable [27] (Section II). In essence, the usual state-estimate-based control framework that dominates engineering practice in many fields [28] cannot be naïvely applied. More fundamentally, we show that the class of bio-inspired nonlinear models considered here cannot be stabilized around an equilibrium point with any choice of dynamic output feedback (Section III.1). However, with appropriate control inputs, nonlinear observability can persist, allowing us to mimic active sensing behavior observed in animals [21, 23]. Specifically, we present an active-sensing-based output feedback system (Section IV), prove that it stabilizes an arbitrarily small limit cycle (Section V), and numerically characterize the nonlinear system dynamics (Section VI). Figure 1: (A) Weakly electric fish control their position using active sensing to remain within a refuge; x(t) is the fish’s position relative to the refuge. (B) Simplified model."
https://arxiv.org/html/2411.06589v2,"Skipped Adjacency Pulse Width Modulation:
Zero Voltage Switching over Full Duty Cycle Range for Hybrid Flying Capacitor Multi-Level Converters without Dynamic Level Changing","This paper proposes a method to achieve zero voltage switching (ZVS) across the full duty cycle range in hybrid flying capacitor multilevel (FCML) converters, eliminating the need for dynamic level changing and active re-balancing. Utilizing skipped adjacency pulse width modulation (SAPWM), this approach avoids the nearest pole voltage level, thereby increasing volt-seconds within specific duty cycle range. The method uses a modified PWM scheme, which preserves effective pole voltage by changing duty reference and employing digital logic processing. Simulation results verify the proposed method achieving full-range ZVS. This SAPWM technique is compatible with hybrid FCML converters with various levels, offering enhanced efficiency and reduced switching losses.","I Background Hybrid flying capacitor multilevel (FCML) converter as shown in Fig. 1, which effectively reduces inductor volt-seconds and increases effective switching frequency, has gained attention for enhancing power density in power converters [1]. By increasing the switching frequency, power density can be improved, allowing for reduced volumes of energy storage components such as inductors and capacitors [2]. However, higher switching frequencies also lead to increased switching losses, which degrade power efficiency. To mitigate the switching loss, zero voltage switching (ZVS) techniques can be applied [3, 4]. A common approach is the use of variable switching frequencies to achieve ZVS, as widely investigated to date [5, 6, 7]. In FCML topologies, however, certain duty ranges require exceptionally low switching frequencies to achieve ZVS, a limitation that arises due to the extremely low volt-seconds. To address this, recent work has introduced dynamic level changing to increase volt-seconds [8, 9]. The switching frequency for ZVS across the duty cycle range is chosen to prevent excessively low switching frequencies in 4-level and 5-level FCML converters. To avoid extremely low switching frequencies, the switching level is changed according to the duty cycle, allowing the system to achieve an appropriate switching frequency for ZVS. This enables almost full-range ZVS, making suitable for applications requiring variable duty at steady-state, such as AC-DC and DC-AC conversion [10, 11]. Nevertheless, the dynamic level changing approach presents the following limitations: (1) Dynamic level changing inherently requires active re-balancing during level transitions, which increases the control complexity. (2) Active re-balancing during dynamic level changing could not achieve ZVS instantaneously for rapidly and continuously changing AC voltage input/output, due to the settling time required for active balancing. Note: In literature [9], active balancing requires approximately 80 \mus per operation. In grid-tied conversion applications with a 60 Hz line frequency, a total of 12 rebalancing events are required, summing to around 1 ms, 8% of the 13.3 ms per a line voltage cycle. (3) To implement dynamic level changing, more switches are required than the optimize number required to fully utilize the voltage rating of switches effectively [12]. Note: For a dynamic level changing with 4- and 5- levels, the switch voltage rating must be designed to meet the requirements of the 4-level configuration. (4) Additional conduction loss is induced due to the additional conduction path resulting from the increased levels in the FCML. Fig. 1: Circuit diagram of hybrid FCML topology. This study addresses the identified research gap by proposing a new method to achieve full-range ZVS. The approach eliminates the need for dynamic level changing, active re-balancing, and additional switch sets. Instead, it relies solely on modified switching techniques. The implementation is feasible by maintaining the phase-shifted pulse width modulation (PSPWM) carrier of the micro-controller unit (MCU) and adding a simple digital circuit, making the proposed method adaptable to various FCML levels. The following content is organized into three sections: Chapter II introduces conventional ZVS methods using PSPWM and variable switching frequency, highlighting the limitations of PSPWM for ZVS operation. Chapter III presents the proposed PWM technique for achieving ZVS across the full duty range. Chapter IV demonstrates the applicability of the proposed method through simulation results. Chapter V includes future work."
https://arxiv.org/html/2411.06567v1,DERs-Aided Blackstart and Load Restoration Framework for Distribution Systems Considering Synchronization and Frequency Security Constraints,"Extreme weather events have led to long-duration outages in the distribution system (DS), necessitating novel approaches to blackstart and restore the system. Existing blackstart solutions utilize blackstart units to establish multiple microgrids, sequentially energize non-blackstart units, and restore loads. However, these approaches often result in isolated microgrids. In DER-aided blackstart, the continuous operation of these microgrids is uncertain due to the finite energy capacity of commonly used blackstart units, such as battery energy storage (BES)-based grid-forming inverters (GFMIs). To address this issue, this article proposes a holistic blackstart and restoration framework that incorporates synchronization between microgrids and the entire DS with the transmission grid (TG). To support synchronization, we leveraged virtual synchronous generator-based control for GFMIs to estimate their frequency response to load pick-up events using only initial/final quasi-steady-state points. Subsequently, a synchronization switching condition was developed to model synchronizing switches, aligning them seamlessly with a linearized branch flow problem. Finally, we designed a bottom-up blackstart and restoration framework that considers the switching structure of the DS, energizing/synchronizing switches, DERs with grid-following inverters, and BES-based GFMIs with frequency security constraints. The proposed framework is validated in IEEE-123-bus system, considering cases with two and four GFMIs under various TG recovery instants.","Transmission grids (TG) can be vulnerable to extreme weather events, which can result in blackouts in the downstream Distribution System (DS). Traditionally, distribution operators have relied on diesel generators (DGs) to blackstart and restore power during prolonged outages, ensuring a continuous power supply by maintaining a steady fuel source. With the anticipated widespread adoption of renewable energy sources in the DS, there is considerable interest in utilizing distributed energy resources (DERs) such as battery energy storage (BES) and renewables for blackstarting operations. Successfully implementing this technology would significantly reduce DS operators’ reliance on DGs, which are known for their high operating costs. DS utilities subscribe to weather forecasting agencies to predict extreme weather events, which aids in pre-event preparation where blackstart and load restoration plans are essential [1]. Blackstart planning with multiple BES inverters poses several challenges: (a) BES-based grid-forming inverters (GFMIs) must operate within frequency security constraints, (b) their finite energy capacity must be optimally utilized to establish cranking paths for activating renewable-based grid-following inverters (GFLIs) while supplying non-switchable loads, (c) optimizing synchronizing decisions to aggregate the energy and power capacity of BES-based GFMIs for rapid load restoration, and (d) ensuring synchronization with the TG upon availability to maintain continuous operation. Broadly, blackstart and load restoration strategies in the DS can be categorized into two main approaches, studied through: (a) determining the final configuration [2, 3, 4], and (b) sequencing configurations [5, 6, 7, 8, 9, 10, 11, 12, 13, 14]. The former works focus on establishing the final network topology without detailing the step-by-step evolution needed to achieve this configuration, which can complicate implementation. In contrast, the latter approach is more practical as it provides a sequence of feasible configurations to guide operators during the blackstart process. Furthermore, the latter works can be sub-classified into studies that either neglect [5, 6, 7, 8, 9, 10] or incorporate [11, 12, 13, 14] frequency constraints in their restoration models. Figure 1: Example DS illustrating bus-blocks, switches, and blackstart units. The authors in [5, 6] presented a detailed restoration model considering the distribution system’s bus-block structures and connecting switches (as shown in Fig. 1). Furthermore, the authors in [7] enhanced the restoration model of DS accounting the flexibility of behind-the-meters DERs. Another enhancement of restoration model is presented in [8] by considering capabilities and operational limits of different switching devices of DS. In [9], the authors propose variable time step mixed-integer linear programming models to incorporate switches operating in multiple timescale while blackstarting DS. However, the models in [5, 6, 7, 8, 9] are designed for blackstart generators with a very large energy capacity (e.g., diesel or thermal generators) and loads without cold load pick-up (CLPU) effect. Considering blackstart units with finite energy capacity such as BES-based GFMIs will potentially lead to different solution and challenges, which were not sudied in these above literature. The study in [10] incorporates GFMI/GFLI models in DS restoration and provides the percentage of load pick up schedules in each buses; however, this approach is not practical as most DS loads are not dispatchable. Moreover, all these works neglect frequency security constraints, which are crucial for ensuring the stability of microgrids during blackstart operations. A sequential restoration of loads in the DS, forming multiple microgrids without exceeding maximum frequency nadir limits in each restoration sequence, is studied in [11]. This is one of the pioneering works where the frequency nadir limit is estimated without the need to simulate the dynamic model of the DS with blackstart generators. However, a slight oversight of the work is that the microgrid also needs to satisfy the RoCoF and quasi-steady-state (QSS) frequency limits not only the frequency nadir for secure operation [15]. A restoration model incorporating blackstart GFMIs and non-blackstart GFLIs with frequency dynamic constraints is developed in [12]. This model requires assistance from dynamic simulations to verify whether the frequency nadir exceeds secure limits when picking up loads. Although accurate, relying on simulations can be computationally expensive for large-scale restoration planning problems. The authors in [14] establish GFMI and GFLI models with a p-f droop-based relationship to estimate QSS frequency when restoring loads. However, frequency security constraints related to RoCoF and frequency nadir are ignored. All the above works neglect the synchronization of islands with the transmission grid (TG), resulting in multiple islanded microgrids forming at the end of the restoration process. However, the continuous operation of these microgrids beyond the restoration period cannot be guaranteed if the blackstart units are battery energy storage (BES)-based and the non-blackstart units are renewable-based. To address this issue, synchronization must be an integral part of the restoration process, a consideration currently missing in the literature. Additionally, synchronization among islands facilitates resource sharing and enables a faster connection with the TG. The significance of synchronization for a resilient distribution system (DS) is advocated in [13], where the synchronization is conducted at the end of the restoration process. However, their work has not optimized the energizing and synchronizing switching sequences. Hence, this paper proposes a holistic bottom-up blackstart and load restoration planning framework leveraging BES-based GFMIs and renewable GFLIs. The proposed framework initiates blackstart with multiple GFMIs, sequentially expands the boundaries of islanded microgrids while establishing cranking paths to GFLIs, synchronizes microgrids to form larger islands, and finally synchronizes with the TG to complete the restoration process. To incorporate synchronizing decisions in the problem, we first integrate virtual synchronous generator (VSG) control features into the GFMI. By leveraging VSG parameters (such as inertia constant and p-f droop), we establish and validate the GFMI’s frequency response to cold load pick-up (CLPU) in terms of quasi-steady-state (QSS) frequency, Rate-of-Change-of-Frequency (RoCoF), and frequency nadir. This approach requires only the system’s initial and final QSS points for frequency response estimation, making it suitable for optimization problems and eliminating the need for dynamic simulation assistance. Additionally, we model GFMI frequency adjustment to facilitate synchronization. Secondly, we model synchronizing switches, establishing conditions for their operation that align with the branch flow model of the DS. A dynamic radiality constraint is designed to support synchronization by allowing GFMIs to change the status of their root nodes to non-root nodes. Finally, all these models, along with GFLI dynamics, CLPU considerations, energizing switches, and the switching structure of the DS, are integrated with the linearized power flow model to develop a mixed-integer quadratic constrained problem for optimizing blackstart and load restoration. We validated the proposed framework using an unbalanced IEEE-123-bus system, considering cases with two and four GFMIs under various scenarios of TG recovery instants. To this end, the technical contributions of this work can be summarized as: • Model a DERs-aided bottom-up blackstart framework that incorporates the frequency response of VSG-based GFMIs. Validate the GFMI’s estimated frequency response to load pick-up events by using a dynamic model of the DS and GFMIs. • Model synchronizing switches (SSWs) with conditions that align with the branch flow-based restoration model. Optimize both the energizing and synchronizing switches efficiently in the blackstart and restoration process. • Develop dynamic radiality constraints to support synchronization and allow microgrids to expand and integrate their boundaries during blackstart process."
https://arxiv.org/html/2411.06560v2,ElectricityEmissions.jl: A Framework for the Comparison of Carbon Intensity Signals,"An increasing number of individuals, companies and organizations are interested in computing and minimizing the carbon emissions associated with their real-time electricity consumption. To achieve this, they require a carbon signal, i.e. a metric that defines the real-time carbon intensity of their electricity supply. Unfortunately, in a grid with multiple generation sources and multiple consumers, there is no unambiguous way to trace electricity from source to sink. This makes it hard to define an appropriate signal, leading to a raging discussion about how to best quantify the carbon footprint of electricity. This paper seeks to inform the discussion about which carbon signal is better or more suitable for two important use cases, namely carbon-informed load shifting and carbon accounting. We do this by developing a new software package ElectricityEmissions.jl, that computes several established and newly proposed carbon emission metrics for standard electric grid test cases. We also demonstrate how the package can be used to investigate the effects of using these metrics to guide load shifting. Our results affirm previous research, which showed that the choice of carbon emission metric has significant impact on shifting results and associated carbon emission reductions. In addition, we demonstrate the impact of load shifting on both the consumers that perform the shifting and consumers that do not. Disconcertingly, we observe that shifting according to common metrics such as average carbon emissions can reduce the amount of emissions allocated to data center, but cause an increase in the total emissions of the system.","As the impacts on climate change become increasingly visible, many electricity consumers, from private persons to large corporations, show increasing interest in assessing and reducing their carbon footprint. For many consumers, a significant portion of their carbon footprint is tied to their electricity consumption and associated emissions from electricity generation. The source of electricity generation (e.g. solar, wind, hydro, nuclear, natural gas or coal) as well as the temporal availability of renewable power (i.e. daily and seasonal cycles of solar, wind and hydropower) vary widely between locations, leading to differences in carbon intensity of electric power across time and space. This provides an opportunity for consumers to reduce carbon emissions by using power where and when low-carbon electricity is available. However, to actively reduce the carbon emissions of their electricity supply, consumers need real-time information about how carbon emissions of electricity vary. Furthermore, they need a framework for leveraging this information not only for real-time load shifting but also for long-term carbon accounting, such that they can get credit for their efforts to reduce emissions. This is particularly important for companies and industries that are subject to emerging carbon emission regulations and reporting requirements, including e.g. producers of clean hydrogen (Internal Revenue Service, 2023). Quantifying the carbon intensity of electricity consumption at differing times/grid locations, however, is not straightforward. While it is possible obtain information on emissions from the generators and assess how the total carbon footprint varies over time, the physics of the grid do not lend themselves to a clear definition of generation mix (and hence emissions intensity) at individual locations. This leaves significant confusion and open questions regarding what the best metric for carbon emissions, which is the topic of ongoing debate (Sukprasert et al., 2024). This paper develops a software package and an analysis framework to help evaluate the effectiveness of different carbon emission metrics. 1.1. Related work Past research has focused on computing and analyzing different carbon emission metrics for real-time electricity consumption (e.g. (Electricitymaps, [n. d.]; Callaway et al., 2018; WattTime, [n. d.]; Lindberg et al., 2021b; Chen et al., 2023b, c)) or quantified impacts of current standards for long-term carbon accounting on long-term emissions (see (Langer et al., 2023) and references therein). Recently, there has also been increased interest in comparing these metrics for the purposes of carbon accounting and informing load-shifting applications. (Sukprasert et al., 2024; Richardson, 2023; Corradi, 2023) The two main shifting metrics that are currently in use and being debated are ‘Average Carbon Emissions’ (ACE), defined as total system emissions divided by total system load, and ‘Locational Marginal Carbon Emissions’ (LMCE), defined as the emissions factor of the marginal generator(s) (i.e. the generator(s) that would respond if a load changes its consumption by a small amount). In a recent paper, the authors of (Sukprasert et al., 2024) compare marginal and average carbon emissions signals for the purpose of data-center workload scheduling. Their results, based on data from 65 regions throughout the world, found the two signals to be largely negatively-correlated, leading to significant differences in workload scheduling outcomes as well as accounted emissions. Inter-metric variability in shifting-incentive and emissions accounting properties, such as that demonstrated in (Sukprasert et al., 2024), highlight the need for new tools and analysis frameworks to compare different emissions metrics. This is particularly important given that ACE and LMCE both have their drawbacks, which has led to interest in defining new metrics such as those based on ‘carbon flow’ methods (Chen et al., 2023a, b). To inform the debate about the effectiveness and appropriateness of different carbon emission metrics, we develop a software package for comparing the different metrics from a power system perspective. Our comparison framework is based on emulating the market clearing mechanism of an electric grid (represented by solving a DC optimal power flow (OPF)) and computing total carbon emissions for the overall grid as well as various carbon emission metrics for different locations based on the result. This information can then be used for both carbon accounting (i.e. to allocate carbon emissions to different loads) and as an input to simulate real-time load-shifting, where a carbon-sensitive load, such as a data center, changes their consumption in response to the carbon metrics. Finally, it is possible to assess the effectiveness of real-time load shifting by re-running the simulation of the electricity market clearing with the updated electricity consumption pattern. One important benefit of the framework is that it allows us (and other researchers) to evaluate both whether a given metric is effective in guiding real-time load shifting (i.e. if a consumer reduces their use of electricity in an hour with a high emission value and increases it in an hour with low emission value, does this lead to a reduction in overall system emissions?) and whether it possesses desirable properties for carbon accounting (e.g. does the metric guarantee that the total carbon emissions assigned to consumers are equal to the total carbon emissions from generation?). This can help inform the debate around current metrics. However, our second goal is to provide a platform for evaluation of newly proposed metrics. We demonstrate this by proposing a new carbon emission metric, namely the adjusted locational marginal emissions (ALMCE) metric, and assessing its merits relative to existing metrics using our framework. 1.2. Contributions • We provide a qualitative overview of existing carbon emission metrics and their pros and cons, and propose the new ALMCE metric to try to combine benefits of several metrics. • We contribute a new open source Julia package, ”ElectricityEmissions.jl” that enables easy calculation of several existing and emerging carbon emissions metrics, including the newly proposed ALMCE metric. • We demonstrate how the package can support quantitative comparison of different carbon emission metrics, both with and without load shifting, in a case study on the standard RTS-GMLC test system. • This case study is, to the best of our knowledge, the first to assess the impact of load shifting not only on total carbon emissions, but also on the accounted carbon emissions that are allocated to different types of loads."
https://arxiv.org/html/2411.06531v1,"Decentralized Bus Voltage Restoration
for DC Microgrids","Regulating the voltage of the common DC bus, also referred to as the “load bus”, in DC microgrids is crucial for ensuring reliability and maintaining the nominal load voltage, which is essential for protecting sensitive loads from voltage variations. Stability and reliability are thereby enhanced, preventing malfunctions and extending the lifespan of sensitive loads (e.g., electronic devices). Voltage drops are caused by resistances of feeders connecting converters to the common DC bus, resulting in a reduced DC bus voltage compared to the nominal/desired value. Existing techniques to restore this voltage in DC microgrids are mainly centralized and rely on secondary control layers. These layers sense the common DC bus voltage, compare it to the nominal value, and utilize a PI controller to send corrections via communication links to each converter. In this paper, a local and straightforward approach to restoring the bus voltage in DC microgrids is presented, ensuring regulation in a decentralized manner. Voltage drops across resistances of feeders are compensated by an additional control loop feedback within each converter, based on the converter output current and feeder resistance. The proposed approach is verified through simulation and hardware-in-the-loop results, eliminating the need for communication links and hence increasing reliability and reducing cybersecurity threats.","In modern converter-dominated power systems, the integration of emerging distributed generation technologies has become increasingly prevalent, with solar photovoltaic (PV) and wind energy widely adopted. Microgrids, which aggregate various sources and loads into a single dispatchable system, can operate either interconnected with the main grid (AC or DC) or in standalone mode [1, 2]. They are mainly categorized into three types: AC microgrids, DC microgrids, and hybrid microgrids that combine both AC and DC elements. Unlike the complex control requirements of AC microgrids, which include managing frequency, voltage, and reactive power, DC microgrids are preferable in several applications due to their simpler control mechanisms [3, 4, 5]. DC microgrids can be further classified based on control configurations into centralized, distributed, and decentralized architectures. Centralized control is characterized by a single control center responsible for decision-making, which simplifies management but introduces a single point of failure risk and scalability challenges. Distributed control employs a hierarchical structure that combines centralized and decentralized elements, balancing optimization and resilience but requiring complex implementation and robust communication infrastructure. In decentralized control, independent controllers make local decisions, enhancing reliability and scalability, though coordination and integration are more complex. The selection of control architecture is determined by the specific needs and infrastructure of the microgrid [6]. In DC microgrids, the primary control objectives are load power sharing and DC bus voltage regulation. The resistances of the feeders connecting the converters to the common DC bus play a crucial role in achieving these objectives. On the one hand, the load power sharing is typically ensured via a droop control strategy, which is implemented at the converter level and is effective under conditions of identical or matched feeder resistances. However, when feeder resistances are nonidentical or mismatched, additional control solutions must be adopted. These solutions include adaptive droop coefficients and virtual resistances, which are necessary to maintain accurate load sharing [7, 3, 8]. On the other hand, voltage deviation in DC microgrids refers to fluctuations from the desired voltage levels, primarily caused by line resistances due to inherent conductor resistance and high current flows, leading to voltage drops and power losses. Several voltage restoration techniques are employed in the literature [9, 6, 3, 10, 11]. Existing techniques to restore voltage in DC microgrids are commonly centralized and rely on a secondary control layer. This layer senses the DC bus voltage, compares it to the nominal value, and utilizes a PI controller to send corrections via communication links to each converter. However, reliance on communication links makes both centralized and distributed control approaches to be potentially vulnerable to cyber attacks and failures when communication links fail [12]. This paper presents a decentralized method for restoring bus voltage in DC microgrids. The proposed method compensates for voltage drops across each feeder line by implementing an additional control loop feedback within each converter, utilizing only the converter output current and feeder resistance. Unlike centralized methods that require secondary control loops and communication links to restore voltage to the nominal value, the proposed method is local, thereby obviating the necessity for communication links, enhancing reliability, and mitigating cybersecurity risks. The remainder of this paper is organized as follows: Section II provides an overview of traditional centralized islanded DC Microgrids. Section III presents the proposed decentralized methodology for restoring bus voltage in DC microgrids. Section V reports simulation and hardware-in-the-loop verification results. Finally, Section VI concludes the paper."
https://arxiv.org/html/2411.06511v1,"MANUSCRIPT TITLE (UP TO 6 INCHES IN WIDTH AND CENTERED, 14 POINT BOLD FONT, MAJUSCULE)","The abstract should briefly state the purpose of the manuscript, the problem to be addressed, the approach taken, and the nature of results or conclusions that can be expected. It should stand independently and tell enough about the manuscript to permit the reader to decide whether the subject is of specific interest. The abstract shall be typed single space, justified, centered, and with a column width of 4.5 inches. The abstract is not preceded by a heading of “Abstract” and its length may not extend beyond the first page.","The American Astronautical Society (AAS) publishes bound sets of printed conference proceedings for personal, institutional, and library usage. The availability of hardcopy enhances the longevity of your work and elevates the importance of your conference contribution to professionals through archival publication. To preserve the consistency and quality of the proceedings, all authors adhere to the latest version of AAS conference proceedings format. This document is intended to serve as a visual and instructional guide, and as a LaTeX document template, for the AAS conference proceedings format. This template provides the basic font sizes, styles, and margins required by the publisher’s formatting instructions. There are also styles for centered equations, figure and table captions, section and sub-section headings, footnote text, etc. This template provides samples of their usage. To use this document as a template, simply copy and change its contents with your own information while maintaining the required predefined style, rather than starting anew. Since this is not a tutorial on how to use LaTeX refer to LaTeX manuals for more information. Your manuscript should include a paper number, a title, an author listing, an abstract, an introductory section, one or more sections containing the main body of the manuscript, a concluding summary section, and a reference or bibliography section. You may also include a section on notation, an acknowledgements section, and appendices, as illustrated in the sequel. You should not include a leading cover sheet. Author affiliation shall appear on the first page, added as a footnote to the last name of each the author. If a distributional release statement or copyright notice is required by your sponsor, this is added as a footnote to the title of the manuscript, appearing on the first page only. Page numbers should be centered halfway between the lower margin and the bottom edge of the page (i.e., approximately 0.75 inches from the bottom). Copy should be single space with double space between paragraphs, with the first line of each paragraph indented 0.2 inches. The recommended sans-serif font for paper number, title, and author listing is Arial, or, Helvetica. The title font and paper-number font should be the same: 14-point sans-serif, centered, and bold. The author-listing font should be 12-point sans-serif, centered, and bold. The recommended serif font for body text, headings, etc., is Times or Times New Roman at 10-12 point, 11 point preferred. The captions for figures and tables are bold 10-point serif font. The endnote reference text and footnote text is 9-point serif font. The right-hand margin of body text should be justified; if not, it should be fairly even nevertheless. All text and text background shall remain uncolored (black on white). These conventions should be automatically implemented in this LaTeX template when the predefined styles of this template are used. The body text of this template is based on the preferred font size of 11 points. To change this to 12-point size, increase the font size at the top of the LaTeX template by uncommenting the appropriate documentclass[]{} line. For very long manuscripts, a 10-point font may be used to keep the manuscript within the publisher’s limit of twenty (20) physical pages."
https://arxiv.org/html/2411.06482v1,One controller to rule them all,"Imagine having a system to control and only know that it belongs to a certain class of dynamical systems. Would it not be amazing to simply plug in a controller and have it work as intended? With the rise of in-context learning and powerful architectures like Transformers, this might be possible, and we want to show it. In this work, within the model reference framework, we hence propose the first in-context learning-based approach to design a unique contextual controller for an entire class of dynamical systems rather than focusing on just a single instance. Our promising numerical results show the possible advantages of the proposed paradigm, paving the way for a shift from the “one-system-one-controller” control design paradigm to a new “one-class-one-controller” logic.","Regulating a system whose dynamics is partially or totally unknown requires data to either identify a model for the system [Ljung(1999)] and then design a model-based controller or to directly design the controller from data (see [Hou and Wang(2013)] for a discussion on the difference between direct and indirect data-driven control strategies). Nevertheless, existing direct and indirect control approaches generally rely on the assumption that data for control design are gathered from the plant to be regulated. Using the machine learning terminology, this is equivalent to assuming there is no domain shift [Farahani et al.(2021)Farahani, Voghoei, Rasheed, and Arabnia] between the training and the test set, which is nonetheless often unrealistic in practice. Indeed, in most real-world scenarios, data distribution shifts might occur due to several factors, e.g., changes in the environment, in the data acquisition process, or due to intrinsic differences among systems that are nominally the same. From the evidence of this distribution shifts in practice comes the need for tools like transfer learning and meta learning [Li et al.(2018)Li, Yang, Song, and Hospedales, Khoee et al.(2024)Khoee, Yu, and Feldt] that allow one to leverage data from different, yet somehow similar, distributions for learning. Among these approaches, in-context learning (ICL) [Brown et al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei, Dong et al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and Sui] is a paradigm where a (sufficiently) powerful model is pre-trained with data from similar systems or tasks to immediately and accurately undertake a new task in a new system by relying upon a few data from the new instance of the problem without further tuning. This aspect differentiates ICL from other meta-learning approaches that most often require further adaptation [Finn et al.(2017)Finn, Abbeel, and Levine] or fine-tuning [Bansal et al.(2022)Bansal, Alzubi, Wang, Lee, and McCallum], while being particularly appealing for control design as it would allow to undertake the design effort once and then have a running controller for an entire class of systems. In light of this intuition, ICL has recently caught the attention of the control community, with the seminal works in [Forgione et al.(2023)Forgione, Pura, and Piga] and [Busetto et al.(2024)Busetto, Breschi, Forgione, Piga, and Formentin] showing that ICL could be a viable (and advantageous) solution for system identification and state estimation. Both these approaches rely on Transformers, whose attention mechanism [Vaswani(2017)] has already been proven to be a powerful enough architectural choice to carry out ICL tasks in several fields, e.g., natural language processing [Akyürek et al.(2024)Akyürek, Wang, Kim, and Andreas], computer vision [Wang and Zhu(2023)] and robotics [Zhu et al.(2024)Zhu, Cano, Bermudez, and Drozdzal], and whose theoretical viability to tackle state estimation is analyzed in [Goel and Bartlett(2024)]. Following the footsteps of [Forgione et al.(2023)Forgione, Pura, and Piga, Busetto et al.(2024)Busetto, Breschi, Forgione, Piga, and Formentin], we here exploit the ICL paradigm to design a model reference, contextual controller that can regulate all the systems belonging to a given class. Given the challenging task we aim to pursue, we propose a training procedure that relies on curriculum learning to gradually increase the complexity of the learning task over training, given the success stories of its use for Transformer’s training (see, e.g., [Xu et al.(2020)Xu, Zhang, Mao, Wang, Xie, and Zhang, Narvekar et al.(2020)Narvekar, Peng, Leonetti, Sinapov, Taylor, and Stone]). Due to our architectural choices and training strategies, learning such a powerful tool is demanding in terms of time and computational resources. However, training can be conducted offline, resulting in a unique controller ready to be deployed for an entire class of systems. As shown numerically in this paper, our approach (i) allows zero-shot regulation of any system belonging to the class, (ii) with no need for hyperparameters selection apart from the (rather general) structural choices for the controller, leading to (iii) closed-loop performance comparable to the ones of optimal control approaches. Though seminal, this paper represents a first step towards a shift in the control design paradigm, drifting from the practical limitation of the classical “one-system-one-controller” logic, leveraging ICL to design a unique controller to regulate multiple systems. The paper is structured as follows. The contextual control problem is introduced in Section 2 and formulated in Section 3. Details on the contextual controller structure and training are then provided in Section 4. Its effectiveness is then assessed numerically in Section 5, with the paper ending with concluding remarks and directions for future work."
https://arxiv.org/html/2411.06380v1,Stability Analysis of Distributed Estimators for Large-Scale Interconnected Systems: Time-Varying and Time-Invariant Cases,"This paper studies a distributed estimation problem for time-varying/time-invariant large-scale interconnected systems (LISs). A fully distributed estimator is presented by recursively solving a distributed modified Riccati equation (DMRE) with decoupling variables. By partitioning the LIS based on the transition matrix’s block structure, it turns out that the stability of the subsystem is independent of the global LIS if the decoupling variable is selected as the number of out-neighbors. Additionally, it is revealed that any LIS can be equivalently represented by a Markov system. Based on this insight, we show that the stability decoupling above can also be achieved if the decoupling variable equals the number of in-neighbors. Then, the distributed estimator is proved to be stable if the DMRE remains uniformly bounded. When the LIS is considered time-invariant, and by analyzing the spectral radius of a linear operator, it is proved that the DMRE is uniformly bounded if and only if a linear matrix inequality is feasible. Based on the boundedness result, we also show that the distributed estimator converges to a unique steady state for any initial condition. Finally, simulations verify the effectiveness of the proposed methods.","Large-scale interconnected systems (LISs), which consist of a large number of local subsystems distributed and interconnected in a spatial domain, can be frequently found in fields such as robotics [1] and smart grids [2]. To ensure that the fundamental control [3] and detection [4] tasks of LISs can be carried out successfully, so it is essential to design algorithms to accurately acquire system states. Theoretically, by joining all subsystems, the state of the whole system can be estimated by the classical Kalman filtering. However, the centralized method imposes a huge computation, storage and communication burden on the estimator due to the large number of subsystems, sensors, and parameters of LISs. In contrast, the distributed manner does not need to integrate all subsystems, but only requires each subsystem to collect information from its neighbor subsystems, and is therefore more suitable for solving the estimation problem of LISs than the centralized manner. In this century, many distributed estimation methods have been proposed. [5] decomposed sparsely banded LISs into multiple subsystems with overlapping states and measurements and proposed a fusion method to combine overlapping information. Also, for the banded LIS, a moving horizon estimation algorithm was derived in [6] by the Chebyshev approximation method. Moreover, Kalman-type distributed estimators were proposed in [7] and [8] for a class of sequential LISs without loops exist among subsystems. In [9], the sequential condition above was relaxed to allow for the existence of some special loops. Note that the topology of physical characteristics determine LISs and cannot be changed unilaterally. Therefore, the application scenarios of the methods in [5, 6, 7, 8, 9] are limited. In [10], a linear minimum mean-square error distributed estimator was proposed for general LISs without topology constraints. Also for general LISs, [11] developed an optimization-based design scheme for stabilizing feedback systems using free-weighting matrix techniques. However, the methods in [10] and [11] require that the parameters of the global LIS be available to each local subsystem, which imposes a significant computational and storage burden. While a fully distributed algorithm was proposed in [12], its computation burden is still large since an optimization problem needs to be solved in real-time. Moreover, in [13] and [14], the concept of robust positive invariance was utilized to design a distributed estimator. Nevertheless, these two works assumed that the measurement noise is zero, which is quite restrictive. [15] and [16] respectively designed distributed Moving horizon estimators and Kalman filters by assuming that the estimation error covariance matrices among subsystems are zero. Although this assumption facilitates the design, it may lead to inconsistent estimates and poor robustness [17]. A plug-and-play distributed Kalman filter was proposed in [18], which ensures the consistency of estimates by constructing upper bounds. Then, following the work in [18], a more compact estimation method was proposed in [19] using linear matrix inequality (LMI) techniques. It should be emphasized that although there have been a lot of works on the distributed estimation problems for LISs, they have some common limitations: i) Lack of discussion on time-varying LISs. In practice, the coupling and sampling frequency of the LIS are easily changed, which often makes the LIS time-varying [20]. However, only the time-invariant LIS is discussed in [6, 9, 10, 11, 15, 16] and [19]. ii) The topology is not general. The methods in [5, 6, 7, 9, 8] are only suitable for some LISs with specific topologies, which limits their application scenarios. iii) The estimation methods are only semi-distributed. The methods in [7, 10, 11], and [13] are only distributed in terms of communication, while their storage and computation are still centralized, i.e., local estimators therein depend on the global system model. iv) The communication and computation costs are relatively high. As mentioned before, the local estimators in [7, 10, 11], and [13] require the global system model, and thus, their computational effort will be related to the scale of the LIS, which tends to be large. Moreover, the methods in [8, 10, 12] and [19] need to solve optimization problems in real-time, which is also tedious. v) The stability analysis may need to be further refined. Although [5, 6, 7, 9, 8] provide stability analysis, they have strict topology assumptions. A necessary stability condition is derived in [19], but the sufficient condition is not provided. Moreover, the convergence result in [18] is sensitive to initial conditions and requires invertible state transition matrices, which are strict in practical engineering. In light of the preceding discussion, this paper aims to propose a fully distributed estimator for LISs without topology constraints while analyzing its stability. For the time-varying LIS, the main results are summarized as follows: • A fully distributed estimator is formulated by recursively solving a distributed modified Riccati equation (DMRE) with decoupling variables. • By decomposing the LIS according to the block structure of the transition matrix, it is shown that the stability of each subsystem is related only to its neighbors if the decoupling variable is set to the number of its out-neighbors. Additionally, we prove that any LIS is equivalent to a Markov system in terms of the dynamic behavior, providing an explicit expression for the equivalent Markov system. Based on this, we show that the aforementioned stability decoupling can also be achieved if the decoupling variable is chosen as the number of in-neighbors. • Following the conclusion on stability decoupling, we prove that the distributed estimator is stable if the DMRE is uniformly bounded from above. Moreover, it is demonstrated that the uniform detectability or reachability of the local subsystem ensures the stability of the distributed estimator provided that the coupling strengths among subsystems are sufficiently small. Additionally, for the case where the LIS is time-invariant, we further obtain the following results: • The feasibility of a centralized LMI is shown to be both sufficient and necessary for bounding the DMRE through analyzing the spectral radius of a linear operator. Moreover, by seeking a feasible solution to the centralized LMI, an easy-to-verify distributed stability condition is derived. • By investigating the monotonicity of the DMRE, it is demonstrated that the distributed estimator converges to a unique steady state for any initial condition. Then, a steady-state distributed estimator is given based on the convergence result. Notations. {\mathbb{R}}^{r} and {\mathbb{R}}^{r\times s} denote the r dimensional and r\times s dimensional Euclidean spaces, respectively. \mathrm{Diag}\{\cdot\} stands for block diagonal matrix. The notations [\cdots,\cdots,\cdots] and [\cdots;\cdots;\cdots] indicate vertical and horizontal concatenations, respectively. The symmetric terms in a symmetric matrix are denoted by “\star”. I stands for identity matrix. \mathbf{1}_{rs} represents the all-1 matrix in {\mathbb{R}}^{r\times s}. \mathrm{Tr}(\cdot), \mathrm{Det}(\cdot) and, \|\cdot\| represent the trace, determinant and 2-norm of a matrix, respectively. The inner product of two matrices is defined by \langle X,Y\rangle\triangleq\mathrm{Tr}(Y^{\top}X). The spectral radius of a linear operator \mathfrak{T}:\mathbb{S}\to\mathbb{S} on a Hilbert space \mathbb{S} is denoted as \rho(\mathfrak{T}). For X,Y\in{\mathbb{R}}^{r\times r}, X>Y and X\geq Y mean that X-Y is symmetric positive definite and semi-definite, respectively. \sqrt{X} is the square root of a matrix X, where X\geq 0. \odot and \otimes denote Hadamard product and Kronecker product, respectively. \delta is the Kronecker delta function. The power of a set is denoted as |\cdot| and the power of the empty set is 0. \mathrm{p}(\cdot) and \mathrm{E}[\cdot] represent the probability density function and expectation of a random variable. \mathrm{Pr}(\cdot) represents the probability of a random event. The composite of functions is defined as \mathfrak{F}\circ\mathfrak{G}(\cdot)\triangleq\mathfrak{F}\big{(}\mathfrak{G}(% \cdot)\big{)}."
https://arxiv.org/html/2411.06288v1,Smooth Zone Barrier Lyapunov Functions for Nonlinear Constrained Control Systems,"This paper introduces the Smooth Zone Barrier Lyapunov Function (s-ZBLF) for output and full-state constrained nonlinear control systems. Unlike traditional BLF methods, where control effort continuously increases as the state moves toward the constraint boundaries, the s-ZBLF method keeps the control effort nearly zero near the origin, with a more aggressive increase as the system approaches the boundary. However, unlike previous works where control effort was zero within a predefined safe region around the origin, the s-ZBLF overcomes the disadvantage of discontinuous control activation by providing a smooth, gradual increase in control effort as the state nears the constraints. This smooth transition improves continuity in the control response and enhances stability by reducing chattering. Additionally, the s-ZBLF provides the advantage of minimal control effort in regions far from the constraints, reducing energy consumption and actuator wear. Two forms of the s-ZBLF—logarithmic-based and rational-based—are presented. Theoretical analysis guarantees that all system states remain within the defined constraints, ensuring boundedness and stability of the closed-loop system. Simulation results validate the effectiveness of the proposed method in handling constrained nonlinear systems.","In modern control systems, ensuring that state variables remain within predefined bounds is important for maintaining system stability and safety, particularly in safety-critical applications such as autonomous vehicles, robotics, and aerospace systems. These constraints are often required to avoid system failure, maintain safe operation, or ensure compliance with physical or operational limits. To address these constraints, Barrier Lyapunov Functions (BLFs) have emerged as a powerful tool, providing a systematic approach to enforce hard constraints on the system states [1, 2, 3]. To overcome these limitations, the Zone Barrier Lyapunov Function (zBLF) was introduced, offering an improvement over traditional BLFs by introducing the concept of zones [4]. In zBLF, system states are allowed to move freely within predefined safe zones, and control effort is only applied as the states approach the boundary of these zones. This significantly reduces unnecessary control actions, improves energy efficiency, and reduces actuator wear. The zBLF method has shown great promise, particularly in applications where maintaining efficient control is important, as it allows the system to operate without intervention within safe zones. Despite the advantages of zBLF, there are some inherent limitations. Most notably, zBLF suffers from discontinuous control activation near the boundary. As the system states transition from the free-moving zone to the constrained region, the control effort can switch on abruptly, leading to sharp control actions. This lack of fine-grained control near the boundaries can result in chattering, control instability, or degraded system performance, particularly in applications requiring smooth and continuous control. Additionally, zBLF’s reliance on strict boundaries can make it challenging to ensure a smooth and gradual increase in control effort as the system approaches the constraints. To address these challenges, we propose a new approach called the Smooth Zone Barrier Lyapunov Function (s-ZBLF). The s-ZBLF retains the energy-saving advantages of zBLF while addressing the issue of discontinuous control activation. Our approach introduces a smooth transition between the safe zone and the constrained region, providing a gradual increase in control effort as the system state moves closer to the boundary. By making the control effort grow progressively, s-ZBLF ensures continuity in the Lyapunov derivative, which leads to smoother and more stable control behavior. Near the origin, where the error is low, the control effort remains minimal, gradually increasing as the system approaches the constraint boundaries, and applying hard constraints only when the boundary is near. This progression ensures that control actions are smoother, leading to more effective constraint enforcement while maintaining system stability. The key contributions of this paper are as follows: • We propose the s-ZBLF, which introduces smooth transitions in control effort as the system approaches constraint boundaries, addressing the discontinuous control issue present in zBLF. • s-ZBLF maintains minimal control effort when the system state is far from the constraint boundaries, while gradually increasing the control effort as the state moves closer to the constraints, ensuring smooth and stable transitions. • The proposed method is demonstrated through theoretical analysis and simulation results, showing the performance of smooth control, stability, and constraint handling of s-ZBLF."
https://arxiv.org/html/2411.06219v1,RRT* Based Optimal Trajectory Generation with Linear Temporal Logic Specifications under Kinodynamic Constraints,"In this paper, we present a novel RRT*-based strategy for generating kinodynamically feasible paths that satisfy temporal logic specifications. Our approach integrates a robustness metric for Linear Temporal Logics (LTL) with the system’s motion constraints, ensuring that the resulting trajectories are both optimal and executable. We introduce a cost function that recursively computes the robustness of temporal logic specifications while penalizing time and control effort, striking a balance between path feasibility and logical correctness. We validate our approach with simulations and real-world experiments in complex environments, demonstrating its effectiveness in producing robust and practical motion plans. This work represents a significant step towards expanding the applicability of motion planning algorithms to more complex, real-world scenarios.","I INTRODUCTION Recent developments in motion planning have increasingly focused on handling intricate objectives and constraints using advanced formalisms, including temporal logic [1, 2]. Due to their expressive nature and precise semantics, temporal logics, such as Linear Temporal Logic (LTL) [3], Metric Temporal Logic (MTL) [4], and Signal Temporal Logic (STL) [5], have become vital tools in defining desirable behaviors for dynamic systems. The robust theoretical foundations and practical tools available for temporal logic have facilitated its widespread use in motion planning for robotics and control systems. Current approaches for generating trajectories that satisfy temporal logic specifications include symbolic control techniques [6], sampling-based methods [7, 8], graph search techniques [9], and optimization-based methods [10]. However, although symbolic control techniques can address input constraints, they typically rely on state space abstraction, which significantly increases computational complexity, especially as the dimensionality of the state space grows. Sampling-based techniques, for instance, abstract continuous state spaces into graph structures that are then used to search for feasible paths. However, they disregard the system dynamics and as these graphs become more complex, scalability issues arise, rendering these methods less effective for complicated planning problems. On the other hand, mixed-integer linear programming (MILP)-based techniques [11] encode LTL specifications directly into optimization problems, yielding optimal solutions. Despite their precision, these methods are often limited to specific subclasses of LTL formulas and heavily rely on user-defined parameters, which constrains their applicability in more general scenarios. Figure 1: Real-world demonstration of optimal, kinodynamically feasible trajectories, meeting LTL specifications. In recent years, tree-based algorithms have gained attention for incorporating temporal logic preferences into planning, leveraging spatial robustness as a key part of their cost function [12, 13, 14]. Robustness in this context refers to a quantitative measure of how well a trajectory satisfies or violates the given temporal logic specification. It provides a real-valued score that indicates the degree of satisfaction, thus enabling planners to prioritize paths that best meet the desired temporal properties. For example, Karlsson et al. [12] formulated a cost function that balances STL spatial robustness with trajectory duration, while others like Vasile et al. [14] aim to maximize the spatial robustness of STL specifications during motion planning. While these methods enhance the robustness of trajectory planning, they often do not account for system dynamics, state, and input constraints, which is essential for ensuring the feasibility of the generated paths. A trajectory that satisfies temporal logic specifications but fails to adhere to the system’s kinodynamic constraints may not be implementable in real-world scenarios. Kinodynamic RRT [15] addresses this issue by extending traditional Rapidly-exploring Random Trees (RRT) to consider system dynamics, generating trajectories that not only avoid obstacles but also comply with the system’s motion capabilities. In this work, we build on these principles by proposing a novel RRT*-based strategy to generate kinodynamically feasible paths that also satisfy temporal logic specifications. Unlike traditional methods, our approach not only focuses on meeting complex temporal logic requirements, through LTL, but also ensures that the trajectories align with the system’s physical constraints. This integration of temporal logic and kinodynamic considerations results in a more robust and practical motion planning solution, especially suited for dynamic and constraint-heavy environments."
https://arxiv.org/html/2411.06204v1,Why Has Advanced Commercial HVAC Control Not Yet Achieved Its Promise?,"Over the last two decades, research and development efforts have shown that advanced control of heating, ventilation, and air conditioning (HVAC) equipment in commercial buildings can improve energy efficiency, reduce emissions, and turn buildings into active participants in the power grid. Despite these efforts, advanced commercial HVAC control has not yet seen widespread adoption. In this paper, we argue that the research community can help companies deploy advanced HVAC control at speed and scale by reorienting research efforts toward clearly demonstrating the business case for adoption. To support this argument, we draw on findings from the 2023 Intelligent Building Operations Workshop, which brought together researchers, entrepreneurs, and representatives from industry and government to discuss current business offerings, state-of-the-art field demonstrations, barriers to adoption, and future directions.","Advanced control of commercial heating, ventilation and air conditioning (HVAC) equipment has great potential to improve energy efficiency, reduce air pollution and greenhouse gas emissions, and turn buildings into active participants in power grid operations. The authors’ recent review of field demonstrations of model predictive control (MPC) and reinforcement learning control (RLC) – two popular algorithms for advanced commercial HVAC control – studied 56 field tests reported in 36 peer-reviewed publications between 2005 and 2023 [khabbazi]. The review found that both MPC and RLC can be expected to reduce costs and emissions by 15–25% relative to conventional rule-based control. Scaled to the entire commercial building fleet, these savings could substantially reduce energy costs and greenhouse gas emissions. For example, in the United States, where HVAC systems in commercial buildings cause about 7% of all greenhouse gas emissions [epa], 15–25% savings could reduce the country’s greenhouse gas emissions by 1–2%, a comparable amount to all emissions from domestic aviation [faa], and save building operators $13–22 billion per year [cbecs]. Despite its potential, advanced commercial HVAC control has not achieved widespread industry adoption. While major building automation companies have explored MPC, these efforts have largely ended. For example, to the best of our knowledge, Siemens discontinued its commercial HVAC MPC activities around 2016 after its OptiControl I and II projects [sturzenegger2015model], and Johnson Controls dissolved its commercial HVAC MPC team [madison2015johnson, rawlings2018economic] around 2019. A number of start-up companies, several of which are discussed in this paper, offer MPC or RLC products, or related sensing or data integration technologies, but so far have not achieved significant market share. We believe that few companies have adopted advanced commercial HVAC control because its business case has not been clearly demonstrated, is not an attractive offering today, or both. To assess project finance metrics (such as payback period, net present value, or internal rate of return), business decision-makers need to understand costs as well as benefits. Today, the research community understands the real-world benefits of advanced commercial HVAC control fairly well. However, the benefits may not have been convincingly conveyed to decision-makers in business, and more importantly, deployment costs are hardly understood at all. Very few deployment cost estimates can be found in past field studies, and the few available estimates are not encouraging. For example, Sturzenegger et al. found after deploying MPC in five floors of an office building that “the required initial investment is likely too high to justify the deployment in everyday building projects on the basis of operating cost savings alone” [sturzenegger2015model]. Similarly, Blum et al. reported that implementing MPC in two floors of an office building required 10 person-months of engineering labor [blum2022field]. The central argument of this paper is that the research community should reorient its efforts to emphasize characterization and improvement of the business case for advanced commercial HVAC control. Assuming finite resources for research and development, this will entail de-emphasizing the algorithmic refinements that currently dominate the research literature. Instead, this paper argues for a) synthesizing the lessons learned from past field studies; b) conducting more, and more focused, field studies; c) focusing as much on deployment costs as past studies have focused on economic and environmental benefits; and d) studying how the costs and benefits of deploying advanced HVAC control depend on the building characteristics, climate, utility rate structure, and other boundary conditions. We believe that these efforts, if successful, will convince decision-makers in industry to invest in creating advanced HVAC control products and, ultimately, get the technology onto a learning curve similar to those that have radically reduced the costs of wind turbines, solar photovoltaics, and batteries over the last decade [lazard]. This viewpoint paper represents the distilled views of the participants in the 2023 Intelligent Building Operations IBO Workshop, one decade after a similar viewpoint paper in this journal by one of the authors asked whether MPC represented a quantum leap for building controls [henze2013model]. This workshop was the sixth in a series, held alternately at the University of Colorado Boulder and Purdue University, that began in 2011. The 2023 workshop, titled “Bridging the Abyss from Algorithms to Applications,” focused on how researchers, engineers, and entrepreneurs can help move advanced HVAC control technology out of the laboratory and into real buildings at speed and scale. The participants hailed from start-up companies, large incumbent building technology companies, the United States Department of Energy, and from universities and national laboratories in the United States and Europe. The organizers of the workshop – also the authors of this paper – collectively have 90 years of research and development experience in the field of advanced HVAC control. The workshop and this paper grew in part out of our disappointment that the technology that we have worked on for so long still has not seen widespread adoption. This paper is organized as follows. To familiarize the reader with the current state of the advanced commercial HVAC control field, Sections 2 and 3 provide brief samplings of current business offerings and recent state-of-the-art field demonstrations, respectively. Informed by the background information from Sections 2 and 3, Section 4 then discusses challenges and opportunities related to business considerations (Section 4.1), data access and system integration (Section 4.2), mandates for new business thinking (Section 4.3), and truth in advertising (Section 4.4). Section 5 concludes by reflecting critically on the state of the field and recommending new research directions. Detailed summaries of IBO workshop presentations can be found in the appendices."
https://arxiv.org/html/2411.06113v1,Behavior-Aware Efficient Detection of Malicious EVs in V2G Systems,"With the rapid development of electric vehicles (EVs) and vehicle-to-grid (V2G) technology, detecting malicious EV drivers is becoming increasingly important for the reliability and efficiency of smart grids. To address this challenge, machine learning (ML) algorithms are employed to predict user behavior and identify patterns of non-cooperation. However, the ML predictions are often untrusted, which can significantly degrade the performance of existing algorithms. In this paper, we propose a safety-enabled group testing scheme, GTUA, which combines the efficiency of probabilistic group testing with ML predictions and the robustness of combinatorial group testing. We prove that GTUA is O(d)-consistent and O(d\log n)-robust, striking a near-optimal trade-off. Experiments on synthetic data and case studies based on ACN-Data, a real-world EV charging dataset validate the efficacy of GTUA for efficiently detecting malicious users in V2G systems. Our findings contribute to the growing field of algorithms with predictions and provide insights for incorporating distributional ML advice into algorithmic decision-making in energy and transportation-related systems.","Recently, with the rising demand for green technology in modern cities, the electric vehicle (EV) industry is experiencing rapid development in smart cities [1]. Integrating EVs into smart cities presents a significant potential to cut greenhouse gas emissions [2]. One promising approach for this integration is Vehicle-to-Grid (V2G), which allows bidirectional power exchange between EVs and the smart grid [3]. This enables EVs to charge from the smart grid during off-peak hours and supply electricity to the grid during peak periods, further enhancing the flexibility of the power systems, and boosting the implementation of virtual power plant in smart cities. [4, 5]. Furthermore, V2G can provides significant economic benefits for EV owners by lowering ownership costs [6]. Collectively, these benefits position V2G as a highly promising technology for the future. While the V2G system provides significant flexibility to the smart grid, it also demands considerable flexibility from its users [7], and its reliability and efficiency are closely related to users’ behaviors [8]. The issues will arise when V2G users are either malicious or lack flexibility. These users might ignore the committed sojourn times and drive their EVs away during peak hours after getting charged during off-peak hours, preventing the V2G system from accessing the stored electricity. Although installing meters at each charger allows for individual monitoring, the large-scale data generated introduces substantial financial and logistical burdens, and transmitting such data exposes users to privacy risks [9, 10]. Additionally, the cost for a residential electric meter ranges from \$60 to \$250 [11], this approach may not be practical for detecting malicious users in a large-scale power grid due to the high expense of equipping each user with sensors. Thus, a more efficient technique is crucial for reducing data expenses and enhancing privacy while effectively identifying malicious users. 1.1 Previous Work Previous work has explored economic incentive approaches to deter malicious behavior in V2G systems. For instance, Vickrey-Clarke-Groves (VCG) auction-based algorithms have been proposed to encourage the participation of EVs in V2G networks by using penalty payments to promote honest reporting and discourage malicious activity [12, 13]. Despite their advantages, VCG algorithms have notable limitations. They are susceptible to false-name attacks, as highlighted in [14]. Furthermore, the reliance on exact optimization renders them computationally infeasible in complex scenarios, and also, the VCG mechanism often fails to adequately address strategic behaviors that deviate from simple truth-telling [15]. To address these challenges, an effective approach is to use group testing algorithms to directly detect malicious users, offering a more robust alternative to the relatively “soft"" method provided by VCG mechanisms. Group testing algorithms aim to reduce the number of tests by grouping individuals collectively [16, 17]. More recently, group testing algorithms have been applied in various areas of smart grid like fault identification and system reliability assessment. The Adaptive Binary Splitting Inspection (ABSI) algorithm [18] employs binary search techniques to efficiently identify malicious users in large-scale smart grid environments. In 2020, Xia et al. [19] proposed an inspection algorithm based on group testing to detect malicious users. This approach adaptively switches between individual and group testing strategies to achieve higher efficiency. However, the aforementioned group testing algorithms did not consider user history behavior, which is easily obtainable in a smart grid environment. In this paper, our proposed algorithm GTUA fully incorporates user history behavior, thereby improving the efficiency of the testing process. 1.2 Contributions Our main contributions are: (1) We reformulate the detection task as a novel group testing problem that utilizes untrusted machine-learned distributional advice to detect malicious users in a V2G system, as illustrated in Figure 1; (2) Introduce GTUA, a safety-enabled testing method (Algorithm 1) combining combinatorial and probabilistic strategies; (3) We provide theoretical bounds on the expected \kappa-regret (Theorem 1), demonstrating robust utilization of predicted statistics; (4) We establish that our method achieves O(d)-consistency and O(d\log n)-robustness (Corollary 1), where d is the number of malicious drivers and n the total drivers, showing near-optimal trade-offs for group testing algorithms. Figure 1: Illustration of Malicious User Detection in a V2G System (see the model defined in Section 2). A V2G manager is equipped with a sensor to detect malicious EVs from a subset of EVs specified by an algorithm. Machine learning models learned from grid data generate distributional advice \widetilde{p} (denoting the probabilities of behaving maliciously). An algorithm, denoted by GTUA is presented in Section 3 for efficient malicious EV detection."
https://arxiv.org/html/2411.06107v1,A capacity renting framework for shared energy storage considering peer-to-peer energy trading of prosumers with privacy protection,"Shared energy storage systems (ESS) present a promising solution to the temporal imbalance between energy generation from renewable distributed generators (DGs) and the power demands of prosumers. However, as DG penetration rates rise, spatial energy imbalances become increasingly significant, necessitating the integration of peer-to-peer (P2P) energy trading within the shared ESS framework. Two key challenges emerge in this context: the absence of effective mechanisms and the greater difficulty for privacy protection due to increased data communication. This research proposes a capacity renting framework for shared ESS considering P2P energy trading of prosumers. In the proposed framework, prosumers can participate in P2P energy trading and rent capacities from shared ESS. A generalized Nash game is formulated to model the trading process and the competitive interactions among prosumers, and the variational equilibrium of the game is proved to be equivalent to the optimal solution of a quadratic programming (QP) problem. To address the privacy protection concern, the problem is solved using the alternating direction method of multipliers (ADMM) with the Paillier cryptosystem. Finally, numerical simulations demonstrate the impact of P2P energy trading on the shared ESS framework and validate the effectiveness of the proposed privacy-preserving algorithm.","The proliferation of distributed generators (DGs), especially distributed photovoltaics (PVs) and wind turbines (WTs), has changed electricity production and consumption patterns [1]. An increasing number of consumers have been converted into prosumers with the installation of DGs [2]. Energy storage systems (ESS) are considered promising solution to mitigate the temporal imbalances between the intermittent generation of DGs and the power demand of prosumers, while also enhancing the economic viability of DGs [3]. However, many prosumers face difficulties in shouldering the high costs and space requirements associated with individual ESS deployment [4]. The concept of shared ESS, which involves centralized energy storage serving multiple prosumers, is receiving attention in numerous countries as it can effectively tackle these challenges with scale effect [5]. Currently, energy transaction and capacity allocation are two main ways of energy storage sharing [6]. In [7], the energy transaction framework is employed to enable users to share ESS with VCG mechanism. However, the energy transaction framework cannot directly reflect the prosumers’ demand for ESS. The capacity allocation method allows consumers to rent part of the shared ESS for a designated period [8], better reflecting their need for regulation capacity. In [9], a cooperative game-based approach is applied to allocate shared battery and thermal ESS capacities across various integrated energy systems, with the Nash bargaining method determining the leasing price for capacity. In [10], an energy capacity trading and operation game is proposed to allocate the ESS capacity based on the prosumers’ bids. In [6], prosumers rent storage and power capacities separately, further enhancing the flexibility and efficiency of shared energy storage utilization. Collectively, these studies demonstrate that shared ESS can reduce the operational costs for both prosumers and society. Privacy protection is another important concern in the structure of shared ESS due to the interaction among different stakeholders. Shared ESS can earn more revenue through price discrimination while reducing prosumers’ profits if shared ESS can obtain more information from prosumers [11]. In [12], a centralized solution is used to address capacity allocation for community ESS, which requires users’ private information, raising the risk of privacy disclosure. To mitigate such risks, many studies have employed distributed algorithms to avoid the transmission of sensitive parameters between agents. In [10], the NI-function type method is applied to find the variational equilibrium of the proposed game for capacity allocation. In [13], a hybrid distributed optimization method based on intelligent heuristic algorithms and mathematical programming is proposed to solve the problem of coordinated dispatching of shared ESS, microgrids, and distributed networks. In [14], algorithm of alternative direction multiplier method (ADMM) is applied for benefit allocation among shared ESS and users, where only iterative variable information is exchanged between agents, reducing the risk of privacy leaks. However, even in such cases, private data can sometimes be inferred from iterative data [15]. In [16], an example is provided to show how private information is inferred through the iterative data of the dual decomposition based method. As the penetration of distributed generators (DGs) among prosumers increases, spatial energy imbalances have become a significant challenge. Peer-to-peer (P2P) energy trading has emerged as a cost-effective solution to address this issue [17]. In [18], a P2P energy sharing mechanism based on a generalized Nash game model is proposed to increase users’ flexibility. In [19], the prosumers are aggregated into several regions, and P2P energy transactions are executed among the aggregators. To enhance economic efficiency, it becomes essential to integrate P2P energy trading into the shared ESS framework, thereby addressing both temporal and spatial imbalances in energy generation and demand. This integration leads to more complex interactions. Both energy and capacity are traded, and prosumers are coupled deeply. Conventional distributed algorithms like ADMM are insufficient in this context due to the complex interdependencies [20], necessitating a new structure to support and describe these interactions. In the new structure, more information transmission raises more concerns about privacy protection. Distributed optimization alone is no longer sufficient, and techniques such as differential privacy [21] and homomorphic encryption [22] are required. Among them, homomorphic encryption provides greater security and privacy with no artificial noise [23]. In [24], Zhang et al. modify traditional ADMM to incorporate homomorphic encryption for privacy protection. However, this approach is limited in applicability when optimization problems include constraints. Considering the shortcomings of the existing literature, this research aims to propose a demand-side market mechanism that integrates P2P energy trading into the capacity sharing framework of shared ESS. The market structure and rules are outlined, and the market participants (prosumers and shared ESS) are modeled. The trading process is formulated as a generalized Nash game among prosumers, which is then transformed into a quadratic programming (QP) problem conditions to solve the equilibrium. To address privacy concerns, a distributed solution algorithm using ADMM with the Paillier cryptosystem is applied. Finally, numerical simulations demonstrate the effectiveness and validity of the proposed mechanism. The main contributions of the research can be summarized below: 1) A capacity renting framework of shared ESS considering P2P energy trading of prosumers is proposed. In this framework, prosumers can rent capacity from shared ESS and trade energy with other prosumers. A model based on a generalized Nash game is developed to describe the trading process and competition among prosumers. The existence of the variational equilibrium for this game is demonstrated, and the variational equilibrium is proved to be equivalent to the optimal solution of a QP problem. To the best of our knowledge, such existence and equivalence guarantees have not been provided by existing literature. By applying this demand-side mechanism, prosumers’ operational costs are reduced. 2) The ADMM algorithm with the Paillier cryptosystem is proposed to solve the equilibrium of the generalized Nash game. The game is transformed into a two-block coupled problem, and the ADMM algorithm with the Paillier cryptosystem is employed to quickly reach equilibrium while safeguarding the privacy of each prosumer. The proposed algorithm can be applied on optimization problems including constraints. Through homomorphic encryption, private information remains secure, preventing agents from inferring any private information from communicated information."
https://arxiv.org/html/2411.05999v1,"Cyber-Physical Security of Vehicles:
Zero Dynamics Attacks Against Vehicle’s Lateral Dynamics","Modern vehicles have evolved from mechanical systems to complex and connected ones controlled by numerous digital computers interconnected through internal networks. While this development has improved their efficiency and safety, it also brings new potential risks, particularly cyber-attacks. Several studies have explored the security of vehicle dynamics against such threats. Among these dynamics, the vehicle’s lateral dynamics are crucial for maintaining stability and control during turns and maneuvers, making them a key focus of research. However, only a few recent studies have specifically investigated the security of lateral dynamics. This paper explores the potential for zero dynamics attacks on the vehicle’s lateral dynamics, where the attacker can remain undetected by leaving no trace on the system’s outputs. Three scenarios are studied: when the output includes yaw rate, lateral acceleration, and their combination. These two critical measurements of a vehicle’s lateral motion are accessible through the inertial measurement units (IMU) in every vehicle. For each scenario, the impact of zero dynamics attacks on system performance is analyzed and illustrated through simulations. Finally, the paper provides recommendations for securing vehicles’ lateral dynamics against such attacks.","I-A Motivation In recent years, modern vehicles become complex systems that contain hundreds of electronic control units, actuators and sensors communicating with each other through internal networks. While this advancement enables significant functionalities and efficiencies, it also makes the vehicle vulnerable to security weaknesses and opens the door for cyber-attacks. The attacker can gain access to the vehicle’s internal network, eavesdrop on the messages, and compromise sensor data and control input signals transmitted within it [5, 17]. Several cyber-physical attacks against vehicles have occurred in real-world scenarios. For example, an attacker remotely crashed a Jeep from 10 miles away, and another attacker took remote control of a Tesla from 12 miles away [19]. As vulnerabilities to cyber-attacks against vehicles threaten human safety, addressing the vehicle’s security becomes a critical concern and fundamental problem that requires dedicated research efforts from both academic and industrial domains. I-B Literature review The research on Cyber-Physical Systems (CPS) security focuses on designing and defending systems against one of the three main types of cyber attacks: denial of service attacks (DoS), replay attacks, and false data injection (FDI) attacks [8]. In DoS attacks, the attacker blocks the transmission of sensor measurements and control input signals. In replay attacks, the attacker records sensor measurements and replays them later instead of the actual measurements. In FDI attacks, the attacker injects false data into the true sensor measurements and control inputs. Some researchs have already studied securing vehicles against DoS attacks [1], and reply attacks [2]. In [3, 16, 30], the FDI attacks against connected vehicles are studied, where the attacks occur on the transmitted signals between vehicles, rather than within the individual vehicle. In [15, 26, 29], FDI and spoofing attacks against vehicle sensors attacks are studied. Fundamental vehicle dynamics are the lateral dynamics, which describe the vehicle’s lateral movement. These dynamics have been widely studied in academic and industrial domains [12, 27]. Lateral dynamics involve the lateral velocity and yaw rate dynamics, with two inputs: the steering angle, which is an input by the driver, and the yaw moment, which can be generated by independent in-wheel motors. The yaw moment plays a vital role in enhancing vehicle stability and controllability. The yaw rate can be directly measured by IMU sensors, but lateral velocity lacks direct measurement. Instead, it is estimated based on the dynamic model, inputs and other sensors’ measurements, such as yaw rate measurements [18, 22], lateral acceleration measurements [9], or a combination of both [6, 7]. Due to the importance of lateral dynamics, extensive research has focused on control methods [4, 13, 32, 33], as well as on estimation and observation techniques for the lateral model [9, 10, 18, 22]. On the other hand, only few and recent works have focused on the security of lateral model. Attacks that modify sensor signals to cause damage in the lateral control have been proposed in [11]. In [23], security measures are proposed to protect against attackers who aim to infer the values of lateral controller gains. In [20, 21], the lateral model is compromised by attacking the braking system and continuously varying the longitudinal slip of the wheels. One class of FDI attacks that target system inputs is the zero dynamics attacks, where the attacker exploits the invariant zeros of the system to perform attacks leaving no trace on the system’s outputs, making these attacks undetectable [24, 25]. It is proven in [28] that the zero dynamics attacks are disruptive, i.e. puts the system on high risk, if the attacks excite unstable invariant zeros of the system. I-C Contributions To the best of the authors’ knowledge, zero dynamics attacks have not been studied for vehicle lateral dynamics. Moreover, studies on zero dynamics attacks offer theoretical insights but rarely provide comprehensive, real-world examples. In the current work, we study and analyze the invariant zeros of the vehicle’s lateral model, and show how the attacker can exploit these zero dynamics to perform undetectable attacks, leaving no trace to the system’s outputs, namely lateral acceleration and yaw rate. We study three cases of output, when the output consists only of yaw rate measurements, when it consists only of lateral acceleration measurements, and when it consists of a combination of both. Additionally, we exploit the relationship between the system’s invariant zeros and its strong observability and detectability properties to analyze these characteristics in the lateral dynamics model. Our motivation for this work is not to create zero dynamics attacks but to evaluate vehicle security against them and improve protection measures. The main contributions of this work are: 1. Attacks Design: Study the existence of invariant zeros of the vehicle’s lateral dynamics, design zero dynamics attacks and explore their potential to be disruptive. 2. Attacks Prevention: Suggest measures to protect the vehicle’s lateral dynamics against zero dynamics attacks. 3. Observability Under Attacks: Investigate the strong observability and detectability properties of the lateral dynamics model by examining its invariant zeros. The remainder of this paper is as follows: Section II provides preliminaries on the vehicle lateral model, invariant zeros, zero dynamics attacks, and the problem statement. Section III studies the existence of invariant zeros, designs and analyzes the zero dynamics attacks for the three cases of output. Section IV provides some simulations to illustrate the findings. Finally, Section V concludes the paper."
https://arxiv.org/html/2411.05975v1,Adaptive Tracking Control with Binary-Valued Output Observations,"This paper considers real-time control and learning problems for finite-dimensional linear systems under binary-valued and randomly disturbed output observations. This has long been regarded as an open problem because the exact values of the traditional regression vectors used in the construction of adaptive algorithms are unavailable, as one only has binary-valued output information. To overcome this difficulty, we consider the adaptive estimation problem of the corresponding infinite-impulse-response (IIR) dynamical systems, and apply the double array martingale theory that has not been previously used in adaptive control. This enables us to establish global convergence results for both the adaptive prediction regret and the parameter estimation error, without resorting to such stringent data conditions as persistent excitation and bounded system signals that have been used in almost all existing related literature. Based on this, an adaptive control law will be designed that can effectively combine adaptive learning and feedback control. Finally, we are able to show that the closed-loop adaptive control system is optimal in the sense that the long-run average tracking error is minimized almost surely for any given bounded reference signals. To the best of the authors’ knowledge, this appears to be the first adaptive control result for general linear systems with general binary sensors and arbitrarily given bounded reference signals.","Binary output observation data, which refers to system output observations that can only take the values 0 or 1, is prevalent in practical systems. One example comes from the signal detection problem in wireless communication [1, 2], where signals are limited to 1-bit via a low-precision analog-to-digital converter, due to constraints in energy consumption and hardware complexity. Another example is the binary classification problem in machine learning [3, 4], where labels provide only discrete class information, rather than precise or continuous values. Furthermore, binary sensors have also been widely used in engineering applications, such as gas content sensors (CO2, H2, etc.) in the gas and oil industry [5], and shift-by-wire switch sensors in automotive applications [6, 7]. Given their importance, extensive theoretical research has been devoted to estimation and control problems under such binary or quantized observations. Various parameter identification methods have been proposed for estimating unknown parameters using binary/quantized observation information, including the empirical measure approaches [7], maximum likelihood algorithms [8], set-membership methods [9], stochastic approximation-type algorithms [11, 10], and stochastic Newton algorithms [12, 14]. Meanwhile, for the control problems under binary/quantized observations, numerous studies devoted to problems such as stabilization with quantized feedback observations (see, e.g.,[15, 16]) and consensus under quantized communications [17, 18], under the assumption that the model parameters are known a prior. However, in many practical control scenarios, the models of systems are uncertain, one may require simultaneous parameter learning and real-time control. This basic adaptive control problem has rarely been explored in the existing literature, because it turns out that such a problem is highly non-trivial in both controller design and theoretical analysis due to the non-availability of the exact output observations. This is the primary motivation for us to initiate an investigation, by introducing new methods and establishing new results for the adaptive control of a basic class of finite-dimensional uncertain linear systems under binary output observations. For that purpose, one needs to build on some significant milestones of the existing adaptive control theory. In fact, over the past half a century, much progress has been made in the area of adaptive control. A natural approach in the design of adaptive control is the certainty equivalence principle, which involves two steps: (1) designing an online estimation algorithm using the observation data to estimate the unknown parameters, and (2) using these online estimated parameters to design the adaptive controller in place of the unknown true parameters. It is well-known that the nonlinear and complex nature of such closed-loop adaptive systems makes the corresponding theoretical analysis quite challenging and thus is regarded as a central issue in adaptive control. Take the basic adaptive control problem of linear stochastic systems as an example, motivated by the need to establish a rigorous theory for the least-squares(LS)-based self-turning regulators proposed by Åström and Wittenmark [19], a great deal of effort had been devoted to the convergence study of LS with stochastic feedback signals resulting from stochastic adaptive control ([20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]). Among the many significant contributions in this direction, we mention that Lai and Wei [23] established the asymptotic analysis of LS under the weakest possible excitation condition on the system stochastic signals, by using stochastic Lyapunov function methods and martingale convergence theorems. The convergence rates of either adaptive tracking control [26] or adaptive LQG control [31] were established by resorting to certain kinds of external excitation signals for open-loop stable systems. The complete convergence theories of both the adaptive tracking control and adaptive LQG control were established later in Guo and Chen [28, 29], Guo [32], and [33]. These turn out to be useful foundations for the study of adaptive control under binary-valued observations in the current paper. The aforementioned work all focuses on adaptive control problems under traditional continuous-valued output observations, which cannot be directly applied to the current case of binary-valued output observations. In fact, even the adaptive parameter estimation theory under binary-valued output observations has not been fully established for general linear systems, let alone the investigation of the related adaptive control problems. This is primarily due to the following two reasons. Firstly, the regression vectors traditionally used for the design of adaptive learning algorithms are not available in the current scenario due to the availability of binary-valued observation information only. Consequently, almost all theoretical results on parameter estimation under binary output observations are limited to finite-impulse-response (FIR) systems with no poles in the linear models. Secondly, due to the nonlinearity of the current binary observation function, almost all existing adaptive estimation theories require stringent data conditions, such as independent and stationary conditions, deterministic persistent excitation (PE) conditions, and boundedness signal conditions, which are difficult or even impossible to verify for the closed-loop control system signals. For these reasons, only a few works have been devoted to adaptive control theory under quantized/binary-valued observations. Guo et al. [34] considered the adaptive tracking problem for FIR systems with a scalar unknown parameter. Zhao et al. [35] further investigated adaptive tracking of FIR systems under binary-valued observations for periodic tracking signals with full-rank conditions. In these two scenarios, it is possible to verify the PE and boundedness conditions required for parameter identification. However, such verification is challenging for infinite-impulse-response (IIR) systems and general reference signals. Furthermore, Zhao et al.[36] studied the adaptive regulation problem by using a stochastic approximation-based control algorithm, where the tracking signal is a fixed setpoint and the binary observation thresholds are specifically designed and depend on the given fixed setpoint. In summary, the adaptive control theory for general linear systems with both general binary sensors and time-varying reference signals remains a challenging research problem. Partly motivated by the above open problem, we have established an adaptive estimation theory under binary-valued output observations and non-PE data conditions in our recent works [12, 13], but these results cannot be used to directly solve the abovementioned basic adaptive control problems, mainly due to the following two reasons: 1) the regression vector contains non-available exact output signals, and 2) the boundedness assumption of the input signals imposed in [12] is hard to verify for closed-loop control systems. To sidestep the difficulty of using the traditional regression vectors, we use the transfer function to transform the current system description into an IIR system description. Then, to establish the adaptive control theory of the current IIR system, we introduce the double array martingale theory developed by one of the authors in [38] for the adaptive estimation theory of the ARX(\infty) model, thereby establishing a convergence theory for parameter estimation without the need of PE and boundedness system signal conditions. Furthermore, in order to verify that the closed-loop system signals satisfy the required unbounded growth condition for adaptive estimation, we design an adaptive control by using a switching control technique depending on a certain level of signal excitation. All the above will make it possible for us to establish the stability and optimality of the closed-loop adaptive tracking control systems under binary-valued output observations. To the best of the authors’ knowledge, this appears to be the first such result for adaptive control under binary output observations for general tracking signals. The remainder of this paper is organized as follows. Section 2 formulates the problem and states the basic assumptions. Section 3 gives the main results of this paper, including the proposed estimation algorithms, the adaptive control laws, and the main theorems. Section 4 provides the proofs of the main results along with some key lemmas. Numerical examples are provided in Section 5. Finally, we conclude the paper with some remarks. Notations By \|\cdot\|, we denote the Euclidean norm of vectors or matrices. The maximum and minimum eigenvalues of a matrix M are denoted by \lambda_{max}\left\{M\right\} and \lambda_{min}\left\{M\right\} respectively. Besides, by det(M) we mean the determinant of the matrix M. Moreover, \left\{\mathcal{F}_{k},k\geq 0\right\} is the sequence of \sigma-algebra together with that of conditional mathematical expectation operator \mathbb{E}[\cdot\mid\mathcal{F}_{k}]."
https://arxiv.org/html/2411.05870v1,An Adaptive Online Smoother with Closed-Form Solutions and Information-Theoretic Lag Selection for Conditional Gaussian Nonlinear Systems,"Data assimilation (DA) combines partial observations with a dynamical model to improve state estimation. Filter-based DA uses only past and present data and is the prerequisite for real-time forecasts. Smoother-based DA exploits both past and future observations. It aims to fill in missing data, provide more accurate estimations, and develop high-quality datasets. However, the standard smoothing procedure requires using all historical state estimations, which is storage-demanding, especially for high-dimensional systems. This paper develops an adaptive-lag online smoother for a large class of complex dynamical systems with strong nonlinear and non-Gaussian features, which has important applications to many real-world problems. The adaptive lag allows the DA to utilize only observations within a nearby window, significantly reducing computational storage. Online lag adjustment is essential for tackling turbulent systems, where temporal autocorrelation varies significantly over time due to intermittency, extreme events, and nonlinearity. Based on the uncertainty reduction in the estimated state, an information criterion is developed to systematically determine the adaptive lag. Notably, the mathematical structure of these systems facilitates the use of closed analytic formulae to calculate the online smoother and the adaptive lag, avoiding empirical tunings as in ensemble-based DA methods. The adaptive online smoother is applied to studying three important scientific problems. First, it helps detect online causal relationships between state variables. Second, its advantage of computational storage is illustrated via Lagrangian DA, a high-dimensional nonlinear problem. Finally, the adaptive smoother advances online parameter estimation with partial observations, emphasizing the role of the observed extreme events in accelerating convergence.","Complex turbulent nonlinear dynamical systems (CTNDSs) have broad applications across various fields [1, 2, 3, 4]. These systems are characterized by their high dimensionality and multiscale structures, with strong nonlinear interactions occurring between state variables at different spatiotemporal scales. Extreme events, intermittency, and non-Gaussian probability density functions (PDFs) are commonly observed in these systems [5, 6, 7]. State estimation is essential for parameter estimation, prediction, optimal control, and generating complete datasets [8, 9, 10]. However, the turbulent nature of dynamics can amplify small errors in model structure, spatiotemporal solutions, or initial conditions when relying solely on forecasts. Data assimilation (DA), which integrates observations with system dynamics, is widely used to improve state estimation [11, 12, 13, 14, 15]. Given the inevitable uncertainty in state estimation, especially for the unobserved variables in a CTNDS, probabilistic state approaches via Bayesian inference are natural choices. The model provides a prior distribution, while observations inform the likelihood. They are combined to form the posterior distribution for state estimation. DA can be classified into two categories based on when observational data is incorporated. Filtering uses observations only up to the current time. Serving as the initialization, filter-based state estimation is the prerequisite for real-time forecasts. In contrast, smoothing [15, 16, 17, 18, 19] leverages data from the entire observation period, including future data, which makes it highly effective for optimal state estimation in offline data postprocessing. This helps to fill missing values, minimize bias, and create complete datasets [20]. With the extra information from future observations, smoothing often produces more accurate and less uncertain state estimates than filtering. When the system dynamics and observational mappings are linear, with additive Gaussian noise, the corresponding filtering and smoothing methods are the Kalman filter and the Rauch-Tung-Striebel (RTS) smoother, respectively [16, 21, 22], where the posterior distribution can be computed using closed-form analytical solutions. Due to the intrinsic nonlinear dynamics and non-Gaussian statistics of CTNDSs, analytic solutions for DA are rarely available. As a result, various numerical and approximate methods have been developed, including the ensemble Kalman filter/smoother and the particle (or sequential Monte Carlo) filter/smoother [14, 15, 23, 24, 25, 26, 27]. These methods are widely used but often face tremendous computational costs, especially in high-dimensional systems [28], which limits the number of particles or ensemble members, potentially causing biases and numerical instabilities [29, 30, 31]. Empirical tuning techniques, such as noise inflation, localization, and resampling, are widely used in practice to mitigate these issues [23, 32, 33, 34]. However, these ad hoc tuning methods are usually quite challenging to implement systematically. Closed-form analytic solutions for DA are thus highly desirable, as they improve computational efficiency, stability, and accuracy, especially in capturing non-Gaussian features, including intermittency and extreme events. They also facilitate theoretical analysis of error and uncertainty in state estimation. Instead of refining DA schemes directly, computational challenges in state estimation can be addressed by developing approximate models that yield analytic solutions for the posterior distribution. While linear approximations allow for standard methods like the Kalman filter or RTS smoother, linearizing a strongly nonlinear system often leads to biases and instabilities. An alternative is a recently developed class of nonlinear systems that includes many turbulent models in geophysics, fluids, engineering, and neuroscience [35, 36, 37, 38, 39]. Despite their nonlinear dynamics and non-Gaussian statistics, the conditional distributions of unobserved state variables given observations are Gaussian (which is precisely the posterior distribution in the DA context), leading to the term conditional Gaussian nonlinear systems (CGNSs). The CGNS framework allows the use of closed analytic formulae for solving these conditional distributions, helping develop efficient algorithms for filtering, smoothing, and sampling without the ad hoc tuning often needed in ensemble-based DA methods. It also facilitates rigorous analysis of these methods. Additionally, CGNSs have been utilized as surrogate models in various applications, including DA, prediction, preconditioning, and machine learning [39, 40, 41, 42]. The standard smoother-based state estimation procedure involves executing a forward pass for filtering across the entire observational period, followed by a backward pass for smoothing [16, 17]. However, the standard offline smoother requires storing the filter solution for the entire duration before initiating the backward pass, which requires substantial computational storage, particularly in high-dimensional systems. Due to the wide application of smoother-based state estimation, it is of practical importance to develop a computationally efficient and accurate algorithm that significantly reduces storage. This paper presents a forward-in-time online smoother algorithm with adaptive lags for the CGNS framework, eliminating the need for a full backward pass. The online smoother sequentially updates the current state as new observations become available. By doing so, it effectively addresses the computational storage issue. While online schemes exist for the RTS smoother and ensemble-based methods, the CGNS online smoother has several unique advantages. First, despite the intrinsic nonlinearity of the underlying dynamics, closed analytic formulae are available to compute the nonlinear online smoother. These analytic formulae provide precise and accurate solutions, which avoid numerical and sampling errors as in ensemble-based methods. Second, due to the turbulent nature of the system, observations influence the estimated state only within a short time window, which enhances computational efficiency and reduces storage needs. Third, different from fixed-lag smoothers [17, 27, 43, 44], the lag in the CGNS smoother is adaptively determined. Online lag adjustment is essential for studying turbulent systems, where temporal autocorrelation varies significantly over time due to intermittency, extreme events, and nonlinearity. A fixed lag usually either overuses storage (if the lag is overestimated) or introduces a large bias (if the lag is underestimated). In contrast, an adaptive lag optimizes the use of data and computational storage. Finally, the adaptive lag is systematically determined using an information criterion based on the uncertainty reduction in the posterior distribution [38, 45]. It emphasizes the importance of the posterior uncertainty and differs from some of the existing adaptive lag selection criteria that rely solely on the posterior mean [46]. As closed analytic formulae are available for posterior distributions, the information gain can be computed efficiently and accurately. The adaptive online smoother for the CGNS framework is applied to studying three important scientific problems. First, the online update of the smoother estimate allows for quantification of the improvement in state estimation by incorporating future information. It facilitates revealing causal dependence between state variables. A nonlinear dyad model with strong non-Gaussian features is utilized for such a study. Second, the CGNS framework is applied to Lagrangian data assimilation (DA), which is a high-dimensional nonlinear problem that has a significant storage requirement [42, 47, 48, 49]. The online smoother allows for the estimation of the unobserved flow states based on Lagrangian tracers. The study highlights the role of the online smoother in reducing computational storage. Finally, the online smoother facilitates developing an online parameter estimation algorithm with partial observations. It helps reveal the role of the observed intermittent extreme events in advancing parameter estimation. The remainder of this paper is organized as follows. Section 2 introduces the CGNS modeling framework, including the equations for the offline optimal nonlinear filter and smoother state estimation. Section 3 presents the adaptive online smoother. In Section 4, the application of the adaptive online smoother to the three key problems is demonstrated. Section 5 includes the conclusion. The appendices contain detailed analysis and proofs."
https://arxiv.org/html/2411.05867v1,Modeling Nonlinear Oscillator Networks Using Physics-Informed Hybrid Reservoir Computing,"Surrogate modeling of non-linear oscillator networks remains challenging due to discrepancies between simplified analytical models and real-world complexity. To bridge this gap, we investigate hybrid reservoir computing, combining reservoir computing with “expert” analytical models. Simulating the absence of an exact model, we first test the surrogate models with parameter errors in their expert model. Second, we assess their performance when their expert model lacks key non-linear coupling terms present in an extended ground-truth model. We focus on short-term forecasting across diverse dynamical regimes, evaluating the use of these surrogates for control applications. We show that hybrid reservoir computers generally outperform standard reservoir computers and exhibit greater robustness to parameter tuning. Notably, unlike standard reservoir computers, the performance of the hybrid does not degrade when crossing an observed spectral radius threshold. Furthermore, there is good performance for dynamical regimes not accessible to the expert model, demonstrating the contribution of the reservoir.","Networks of oscillators appear widely across engineering, and in both the physical and biological sciences. When the networks are non-linear their dynamical behavior can be complex, displaying synchronization, chaos, and traveling waves [1, 2, 3]. Creating analytical or data-driven models that can predict their dynamics is important in applications, for example in producing surrogate models. Downstream applications for a surrogate model of non-linear oscillator networks (NLONs) include smart electrical grid optimization [4, 5, 6], biological computing [7], synthetic biology [8, 9] and the diagnosis and treatment of neurological disorders such as epilepsy [10] and Parkinson’s disease [11, 12, 13, 14]. These surrogate models have two broad applications: parameter inference and control. Parameter inference can help identify critical states and associated parameter values within a system, ideally also exposing underlying mechanistic processes. Control using surrogate models aims to exploit system knowledge to improve control performance. Methods such as model predictive control [15] and model-based reinforcement learning [16] are examples. In this paper, we focus on hybrid reservoir computing (RC), a specific form of physics-informed machine learning (PIML). In particular, with a view to probing its viability for control applications, we investigate how well hybrid RC performs surrogate modeling of NLONs. Real NLONs are often high dimensional, partially observable, noisy, and involve complex intra-network interactions. As such, it can be extremely difficult to create accurate surrogate models. This challenging task may be approached in several ways. The classical approach is direct physics-based modeling, where mechanisms are pre-ordained and parameters fit to data. Machine learning (ML), or data-driven modeling, is an alternative approach which uses fully parameterized models and with parameters updated using learning algorithms. These two contrasting methods confer distinct benefits and drawbacks. Physics-based models are physically accurate within the bounds of the assumptions made in their construction. Similarly, within the domain of their training data, data-driven models perform well, however physical accuracy is not enforced. Failure when predicting out-of-domain is therefore common to the two approaches. Out-of-domain failure is guaranteed for physics-based models as there is no scope for adapting their structure or parameters. ML models however, can be updated online using new data to adapt to new situations, although this is in itself a challenging problem [17]. Physics-based models are inherently interpretable: each term generally has some understood physical meaning, or represents some physical laws or constraints. On the other hand, ML discards this prior knowledge in favor of complete parameterisation from observed data. The correspondence between the parameters and the physical system can be difficult to discern. When data-driven models have many parameters, they require large amounts of data. Physics-based models tend to have few parameters that need fitting to data, and therefore generally require less. Both methods can be computationally expensive as physics-based models rely on complex numerical schemes, and data-driven models often need to use extensive training algorithms for parameter updates. PIML is a recently-formulated approach for surrogate modeling and prediction applications [18]. It combines both physics-based and data-driven methods, in an attempt to make use of the best features of each approach. Its goal is to obtain physically constrained, robust, and interpretable models that capture both expert knowledge of dynamical processes, and the information that can be extracted from data obtained from sensing and recording devices. PIML models promise to be more data efficient as they do not require all of the dynamics to be learned from scratch, and they may also facilitate adaptivity through the use of machine learning. PIML-based control for NLONs may thus result in more robust, efficient and accurate controllers that are adaptable and generalisable. For PIML-based modeling of dynamical systems, it is natural to consider ML components with a time component or sequential nature. For instance, recurrent neural networks (RNNs) and their variants (LSTMs [19], GRUs [20]) are a common choice [21, 22, 23]. However, RC [24, 25] is a particularly promising alternative, due to its small size and simple training procedure. The simplicity of an RC may also offer a unique benefit for PIML parameter inference; while a large RNN can learn, or over learn, a complete model of the data without any input from the physics-based component, an RC has a limited capacity which may force the model to use the physics-based component, making it more interpretable. The small number of parameters used by an RC may also further enhance the low-data requirement conferred by the use of system knowledge. RCs are a restricted form of recurrent neural network (RNN), where learning only takes place in an external readout layer. Sequential data is passed into the reservoir via a fixed, random input weight matrix. An update rule then acts as a discrete non-linear map upon the internal state stored within the activations of the reservoir nodes. The result is a high-dimensional non-linear filter of the incoming data with a fading memory of past states. Scaling the weights of the input matrix and internal connectivity controls the extent to which past-state information is maintained in the hidden state and how much influence is exerted by the input data. To compute an output from the reservoir’s internal state, an output weight matrix is trained, often using regularized linear regression. The readout can be trained to perform classification of the reservoir state, and thus the input sequence, or regression to predict numerical features. The readout layer may also be trained to perform n-step-ahead prediction. When configured to predict the next step in a sequence, the reservoir may be run autoregressively with its output fed back in as the next input instance and thus used for time-series forecasting. This is the format we are considering here: using RCs for the prediction of dynamical system trajectories. The main advantage of RCs over more complex RNNs is their ease of initialization and simplicity of training. Good time-series forecasting performance can be achieved using only linear regression, even when chaotic dynamics are present [26], and issues such as the vanishing gradient problem are avoided. Since they require only general non-linear high-dimensional filtering of inputs and a fading memory of past inputs, RCs can also be constructed in a wide range of physical substrates [27, 28]. Recently, a PIML variant of an echo state network RC was proposed[29]. In hybrid RC (Fig. 1), the prediction from a standard RC is augmented by a single-step integration of an expert ordinary differential equation (ODE) model of the system being predicted. The next step prediction of the ODE model is passed into the reservoir alongside the current state. It is also passed around the reservoir to be considered by the output weight matrix on its own merit, during training and inference. The output weight matrix, still the only trained component, thus aims to combine the augmented reservoir state with the ODE model prediction to most accurately predict the next state. This approach allows the reservoir to compensate for errors in the expert ODE model, and has been shown to result in superior performance when compared to models that use only one of the two components, that is, a standard RC or an expert ODE model. CurrentExpertReservoirLinearPrediction Figure 1: A cartoon diagram of the hybrid RC. The current observed state acts as input for both the expert model and the reservoir; the reservoir also receives input from the expert model. The outputs from both the reservoir and expert model form the input to the linear regression layer, whose output maps to a prediction. The only tunable parameters in the model are in the regression layer. In particular, the hybrid RC was used to predict the dynamics of the Lorenz and Kuramoto-Sivashinsky systems when incorporating a model of each system with parameter error. With the correct model structure, the hybrid RC was shown to perform well, better than either a standard RC or ODE model in isolation. The hybrid RC also maintained good performance when, under particular parameter settings, the standard RC and ODE model performed poorly [29]. To investigate the potential of hybrid RCs for the novel example of NLON prediction and control — where the ground-truth dynamics is more complex, or the expert model is further from the true system than in previous work[29] — we evaluated their performance on two tasks: parameter error and residual physics. • Parameter error, the first, simpler, task tests how well a hybrid RC predicts the trajectory of a network of standard Kuramoto oscillators, when the parameters in the hybrid RC model do not correctly match the parameters of the ground truth model. This follows the previous evaluation but with the Kuramoto oscillator network replacing the Lorenz system [29]. The test is run across a range of hyperparameters to assess performance robustness to tuning, and across three qualitatively different dynamical regimes. • Residual physics, our second task, is more challenging. We measure the short term prediction performance of the hybrid RC when the hybrid RC uses a simpler model than the ground truth. This is intended to mimic real-world examples where the complex interactions of an oscillating system are unknown. In these examples a simpler, approximate model, is often used, but even small non-linear terms can quickly make predictions inaccurate. Our interest is in whether the reservoir component of the hybrid RC can compensate for the over-simplification of the model. In our particular implementation, the residual physics is an additional higher harmonic in the coupling term for the ground truth Kuramoto-like system: we give the hybrid RC the standard Kuramoto model without this addition. This bi-harmonic Kuramoto model [30] produces behaviors not accessible to the original Kuramoto model. For example, when clustering of the oscillators around a phase occurs in the standard Kuramoto model, there is only one cluster; with an extra harmonic term this need not be true. The residual physics task aims to replicate realistic control scenarios with incomplete knowledge of the system structure. This is an interesting challenge for the hybrid RC since exact knowledge of the ground truth non-linearities has previously been identified as being crucial for Kuramoto oscillator network attractor reconstruction when using the Next Generation Reservoir Computer [31, 32]. In a similar fashion to the parameter error task, we run this test across a range of hyper parameters, with four qualitatively different dynamical regimes. We then use this to inform a demonstrative grid-search optimization process simulating the development of a surrogate model for control applications."
https://arxiv.org/html/2411.05842v1,Efficient and Robust Freeway Traffic Speed Estimation under Oblique Grid using Vehicle Trajectory Data,"Accurately estimating spatiotemporal traffic states on freeways is a significant challenge due to limited sensor deployment and potential data corruption. In this study, we propose an efficient and robust low-rank model for precise spatiotemporal traffic speed state estimation (TSE) using low-penetration vehicle trajectory data. Leveraging traffic wave priors, an oblique grid-based matrix is first designed to transform the inherent dependencies of spatiotemporal traffic states into the algebraic low-rankness of a matrix. Then, with the enhanced traffic state low-rankness in the oblique matrix, a low-rank matrix completion method is tailored to explicitly capture spatiotemporal traffic propagation characteristics and precisely reconstruct traffic states. In addition, an anomaly-tolerant module based on a sparse matrix is developed to accommodate corrupted data input and thereby improve the TSE model robustness. Notably, driven by the understanding of traffic waves, the computational complexity of the proposed efficient method is only correlated with the problem size itself, not with dataset size and hyperparameter selection prevalent in existing studies. Extensive experiments demonstrate the effectiveness, robustness, and efficiency of the proposed model. The performance of the proposed method achieves up to a 12\% improvement in Root Mean Squared Error (RMSE) in the TSE scenarios and an 18\% improvement in RMSE in the robust TSE scenarios, and it runs more than 20 times faster than the state-of-the-art (SOTA) methods.","Figure 1: Visualization of constructing a traffic state matrix (TSM). Traffic states exhibit high correlations along the direction of backward traffic waves. Conventional rectangular grid-based modeling in (a) is less desirable to effectively capture such correlations, as it simply vertically and horizontally divides the spatiotemporal region (e.g., cells A and B). In this study, we adopted the oblique grid-based modeling in (b), strategically positioning traffic state observations along the traffic wave direction into the same matrix column (e.g., cells C and D). This approach adeptly transforms the correlation of traffic states into the algebraic low-rankness of the matrix, therefore ensuring a low-rank representation method to proficiently capture the spatiotemporal correlations inherent in traffic states. I-A Motivation Precise and complete traffic states (e.g., 5-sec traffic speed) provide reliable support for freeway proactive traffic control and management, especially in current and future connected and automated vehicular environments, e.g., connected and automated vehicle (CAV) cruise control, eco-driving, and dynamic routing planning [1, 2, 3]. In practice, field traffic state measurements are often limited and noisy [4, 5, 6]. Fixed detectors are costly and often sparsely installed along the road, resulting in limited spatial coverage. Mobile sensors, benefiting from the advancements of connected vehicle (CV) technologies, provide more extensive spatial coverage. However, they suffer from sparsity in the temporal domain [7] due to the low penetration rate in the current mixed conventional and connected environment. Reconstructing accurate traffic states on the freeway from the sparse and corrupted observations is still a challenging task that needs to be addressed in current applications of Intelligent Transportation Systems (ITSs). I-B State-of-the-Art (SOTA) Initially, researchers carefully abstracted physical traffic flow characteristics and utilized traffic flow models including the first-order model like the well-known Lighthill-Whitham-Richards (LWR) to estimate traffic states [8, 9, 10, 11, 12, 13], employing various data assimilation techniques. To more accurately capture complex traffic phenomena, higher-order models such as the Payne-Whitham (PW) models [14, 15], Aw-Rascle-Zhang (ARZ) models [4, 16], and METANET models [17, 18, 19] have also been explored in TSE. An alternative approach to TSE assumes that the average speed of regular vehicles equals that of CVs [20, 21, 22, 23, 24]. This speed-uniformity assumption simplifies TSE by using a data-driven conservation equation model with Kalman filters [18]. Recent overviews of freeway TSE highlight these developments [4, 17]. Benefiting from domain knowledge, these methods are physically interpretable and require a small amount of data. Despite the simplicity, model-based methods can be constrained by the capacity of the traffic flow models and assumptions made in the data assimilation process [5]. Moreover, model-based methods usually require time-consuming and labor-intensive parameter calibration processes. With the rapid progress in computation ability and wide availability of multi-source data, data-driven methods have flourished in TSE. The main approach of this category is to exploit the spatiotemporal dependencies from traffic data using various learning frameworks, such as adaptive smoothing kernel [25, 26, 5, 27], Gaussian process [28, 29], deep learning [7, 30, 31, 32, 33, 34, 35], low-rank matrix/tensor completion [36, 6, 37], etc. The most prevalent modeling approach is discretizing the spatiotemporal domain into a spatiotemporal grid/matrix/diagram as shown in Fig. 1(a). Then, fixed or mobile data are aggregated and transformed into partial observations of the grid. The grid-based TSE modeling has become a popular framework due to its easy implementation and convenience in capturing high-dimensional spatiotemporal traffic flow dependencies [30, 6]. By decomposing the spatiotemporal domains into small unified grids, Rempe et al. [30] developed a convolutional neural network (CNN) to learn and reconstruct the spatiotemporal traffic speeds within these grids. Thodi et al. [7] further incorporated kinematic wave priors into CNN by designing anisotropic kernels to capture directional traffic propagation characteristics. In addition, graph neural networks [32, 33] and generative adversarial networks [34, 35] are also applied. However, these deep learning-based methods may require massive and high-quality training data. It is worth noting that obtaining a suitable training dataset may not always be feasible in practice [29]. Although the training data can be collected from traffic simulations [7], the simulated dataset may not accurately represent road segments in the real world, depending on the quality of calibrations. To mitigate the reliance on complete training data, physics-informed deep learning approaches assisted by physical models have conducted successful trials in TSE [38, 39, 40, 41, 42, 43, 44]. However, under conditions of sparse data, the performance of the physics-informed deep learning method may be sensitive to the trade-off between model-driven and data-driven components, making reliable training greatly challenging. Alternatively, low-rank matrix/tensor completion, a data-efficient grid-based data-driven approach, has emerged to deal with limited data scenarios and achieved promising results in the TSE domain using only sparse observations [45, 46, 36, 6, 37]. Based on the spatiotemporal grid/matrix, the basic idea of this approach is to recover the spatiotemporal traffic state by representing spatiotemporal traffic dynamic dependencies with algebraic low-rankness. For example, Wang et al. [6] transformed the traffic state matrix into a fourth-order Hankel tensor and applied low-rank matrix completion on the unfolded matrix to recover spatiotemporal traffic speeds using limited vehicle trajectory data. Nie et al. [37] organized spatiotemporal traffic speeds into a tensor and implemented spatiotemporal traffic speeds kriging by graph-embedded tensor completion. However, these pure data-driven low-rank representation methods may degrade under extremely sparse data environments (e.g. 3\% or less vehicle trajectories). Focusing on online applications, there are streaming-data-driven methods that only use streaming data (e.g., real-time data) [47, 4, 48, 49]. These methods rely less on prior knowledge, thereby demonstrating high robustness to uncertain phenomena and unpredictable incidents. In addition to conventional fixed and mobile sensor data, various types of interesting streaming data are also utilized in this category, including extended floating car data (xFCD) that can measure space and time headway [47, 49], and unmanned aerial vehicle (UAV) data that can provide fast and accurate traffic state observations at any desired locations in multiple travel directions [50, 51, 52, 53]. However, a large amount of streaming data is usually required for streaming-data-driven methods to provide accurate state estimations. I-C Research Challenges and Contributions Despite the fact that grid-based data-driven methods have achieved high precision in previous literature, researchers continuously contribute to this branch by tackling the following three major challenges: C1: consistency with backward wave propagation. Previous research has highlighted the advantages of modeling spatiotemporal traffic characteristics along the direction of backward waves, which propagate obliquely [54, 55]. However, most Traffic State Estimation (TSE) methods typically use an orthogonal grid-based approach as shown in Fig. 1(a), leading to inconsistencies with the actual propagation of non-orthogonal backward traffic waves. As a result, these inconsistencies cause inhomogeneous traffic states within certain grids, e.g., cells A and B in Fig. 1(a), potentially introducing biased entries for the TSE and diminishing its accuracy [7, 29, 56]. Furthermore, under extremely sparse data environments, constructing the TSM with orthogonal grids may lead to the entire column-missing problem, which may weaken the performance of pure data-driven models depending on column-wise algebra similarity [6, 57]. Recognizing the limitations of orthogonal grids, He et al. [58] proposed oblique grids for better alignment with traffic wave propagation, enhancing the segment-level travel time estimation accuracy. For the spatiotemporal grid-level estimation (the focus of this study), they utilized a simple neighborhood-based imputation method, which becomes less effective when significant data is missing. Additionally, their approach was limited by relatively low estimation resolutions. C2: robustness to corrupted input data. The TSE model can be degraded when encountering unfavorable conditions such as noisy or corrupted measurement, emphasizing the robustness requirements against data noise and corruption. The previous works mainly focused on the former and enhanced their model robustness by characterizing the uncertainty caused by stochastic disturbances in TSE [28, 29]. However, random data corruption that does not follow Gaussian distribution can also be problematic. Though data pre-processing methods are usually effective in removing these corrupted observations, they might inadvertently filter out genuine observations that are crucial for accurate traffic state estimation, depending on hyper-parameter selection, e.g., filtering threshold. To ensure that all potentially valuable information is utilized for accurate state estimation, a reliable model that is robust to corrupted raw data without destroying its integrity is desirable for TSE. C3: computational complexity. The computational complexity of exiting grid-based data-driven methods is not only related to the problem size (i.e., temporal and spatial length of reconstructed area) but also positively correlated with other variables, such as the number of observations [25, 5] and model hyperparameters [6], bringing overwhelming computational costs for TSE. For large-scale TSE applications with significant problem sizes, it is practically essential to develop an efficient model with no additional scenario-dependent or parameter-induced computational complexity. The existing studies have attempted to handle one or two of the above challenges. In this study, we propose a tailored matrix completion approach that simultaneously tackles all these three issues. To address the C1, we integrate traffic wave priors into a customized low-rank matrix completion model based on the oblique grid-modeling approach by He et al. [58]. The differences between their studies and our work are as follows. First, given oblique grids, instead of exploiting the enhanced traffic state homogeneity only, we further leverage the enhanced algebraic low-rankness inherent in the traffic state matrix, significantly improving TSE accuracy, especially under severe data scarcity conditions. Second, He et al. [58] utilized a simple interpolation-based imputation to estimate traffic states with low resolutions ranging from 150m/90s to 50m/30s, while our study proposes a tailored low-rank approach capable of estimating high-resolution states at 3m/5s, addressing greater challenges with an 88\% rate of empty cells compared to 21\% in the prior work. (2) To tackle the C2, we design an anomaly-tolerance module to accommodate potentially corrupted traffic state observations. Specifically, we assume the ubiquitous data corruptions are randomly and sparsely distributed, and treat the corrupted data detection as a sparse matrix completion problem. (3) To respond to the C3, we employ a simple and efficient matrix completion, in which the per-iteration computational complexity is only related to the temporal and spatial length of the TSE reconstructed area. The contributions of this paper are summarized as follows: 1. A traffic wave-inspired low-rank model is tailored for traffic state estimation, in which an oblique grid-based matrix is designed to enhance the low-rank nature within the traffic states and thereby helps to proficiently capture spatiotemporal traffic state dependencies. 2. An anomaly-tolerant module is developed to accommodate corrupted data input in robust traffic state estimation, without requiring additional data pre-processing procedures. 3. Theoretical computational complexity analysis and empirical running time evidence prove the efficiency of the proposed method. Numerous experiment results also demonstrate its superior estimation accuracy and robustness. The remainder of this paper is organized as follows. Section II gives some basic notations and defines the traffic speed estimation problem. Section III formulates the proposed model and derives the associated solving algorithm. Section IV implements experiments on a real-world traffic dataset and presents the results. Section V presents further discussions. Finally, Section VI concludes this paper and provides future research directions. Figure 2: Illustration of the proposed method. An oblique grid-based traffic state matrix is constructed (subsection III-A) using incomplete and corrupted traffic state observations, and then a low-rank and sparse matrix completion model (subsection III-B) is applied to recover the complete low-rank spatiotemporal traffic state and to simultaneously detect potential sparse corrupted/anomaly data."
https://arxiv.org/html/2411.05835v1,Improved Convolution-Based Analysis for Worst-Case Probability Response Time of CAN,"Controller Area Networks (CANs) are widely adopted in real-time automotive control and are increasingly standard in factory automation. Considering their critical application in safety-critical systems, The error rate of the system must be accurately predicted and guaranteed. Through simulation, it is possible to obtain a low-precision overview of the system’s behavior. However, for low-probability events, the required number of samples in simulation increases rapidly, making it difficult to conduct a sufficient number of simulations in practical applications, and the statistical results may deviate from the actual outcomes. Therefore, a formal analysis is needed to evaluate the error rate of the system. This paper improves the worst-case probability response time analysis by using convolution-based busy-window and backlog techniques under the error retransmission protocol of CANs. Empirical analysis shows that the proposed method improves upon existing methods in terms of accuracy and efficiency.","Probabilistic real-time analysis has become an important method for analyzing variable task execution times in real-time systems. Unlike traditional deterministic analysis, which assumes a single worst-case execution time for each task, probabilistic analysis leverages probability distributions to model the execution time of tasks. Typically, the worst-case execution time of tasks has a very low probability of occurring, making it difficult to accurately describe the actual system when analyzing based on worst-case execution time. Probabilistic real-time analysis provides a more accurate representation of task behavior by considering the complex software and hardware interactions prevalent in modern real-time systems. A significant advantage of probabilistic analysis is its shift from guaranteeing absolute certainty in meeting timing requirements to evaluating the likelihood of meeting those requirements while allowing for the possibility of not meeting the requirements to some extent. By quantifying the probabilities of meeting deadlines, system designers can make informed decisions based on predefined limits [11]. Many real-time applications, including critical systems like active steering in automotive contexts, operate within safety-critical environments where adherence to stringent safety standards is paramount. For example, the automotive standard ISO-26262 [10] specifies specific failure rates for each Automotive Safety Integrity Level (ASIL), providing a framework for decision-making in the automotive industry. In the context of Controller Area Networks (CAN), which are extensively used in automotive and industrial applications, the consideration of errors and their implications is particularly crucial. Systems utilizing CAN protocols often incorporate fault-tolerance mechanisms to ensure reliability. However, these mechanisms introduce additional overhead in response to errors, such as increased signaling and recovery processes like Automatic Repeat Requests (ARQ), which initiate retransmissions. This overhead is essential for maintaining system integrity and meeting real-time constraints in the presence of unpredictable error events typical in CAN-based environments. In this paper, we analyze the probabilistic guarantees on worst-case response times for the CAN protocol with error retransmission mechanisms and propose an improved reliable convolution-based algorithm to calculate the upper bound of the probability that response times exceed a limit. The contributions of this paper are described as follows: 1. We construct a probability CAN model with a probabilistic error retransmission mechanism. 2. We propose a novel convolution algorithm based on busy-window and backlog to analyze the probabilistic worst-case response time for frames in CAN. Our algorithm provides an efficient convolution process and explicitly specifies the stopping conditions. 3. Through extensive empirical evaluation encompassing a wide range of parameter settings, we demonstrate that our new technique significantly outperforms state-of-theart methods in terms of analysis precision and speed. The remainder of this paper is structured as follows: After summarizing related work in Section 2, Section 3 presents the system model, including the CAN model and the error retransmission model, and introduces relevant probabilistic operation. Section 4 4introduces our response time analysis method and convolution algorithm. Section 5 5 conducts comparative experiments. Finally, we conclude the paper and propose future work in Section 6."
https://arxiv.org/html/2411.07179v1,"Joint Age-State Belief is All You Need: 
Minimizing AoII via Pull-Based Remote Estimation","Age of incorrect information (AoII) is a recently proposed freshness and mismatch metric that penalizes an incorrect estimation along with its duration. Therefore, keeping track of AoII requires the knowledge of both the source and estimation processes. In this paper, we consider a time-slotted pull-based remote estimation system under a sampling rate constraint where the information source is a general discrete-time Markov chain (DTMC) process. Moreover, packet transmission times from the source to the monitor are non-zero which disallows the monitor to have perfect information on the actual AoII process at any time. Hence, for this pull-based system, we propose the monitor to maintain a sufficient statistic called belief which stands for the joint distribution of the age and source processes to be obtained from the history of all observations. Using belief, we first propose a maximum a posteriori (MAP) estimator to be used at the monitor as opposed to existing martingale estimators in the literature. Second, we obtain the optimality equations from the belief-MDP (Markov decision process) formulation. Finally, we propose two belief-dependent policies one of which is based on deep reinforcement learning, and the other one is a threshold-based policy based on the instantaneous expected AoII.","Age of information (AoI) metric has recently been proposed to capture information freshness in remote estimation problems [1]. The AoI metric quantifies information freshness by a monitor which keeps track of how long ago the latest received information packet in the system had been generated. However, it is argued in [2] that AoI may fall short of capturing freshness in certain estimation problems since it does not consider the dynamics of the sampled process since even though the latest received packet may have been generated a long time ago, it is possible that the source may not have changed since then, and therefore, the packet can still be fresh. Similarly, a recently received packet may contain stale information if the source has already changed its state after the packet was generated. Stemming from this drawback of AoI, [2] proposes an alternative freshness metric, namely age of incorrect information (AoII) that penalizes the mismatch between the source and its estimation over time, and regardless of when it is sampled, it defines the estimation as fresh if it is the same as the source. Another interesting feature of AoII in contrast to AoI is that the monitor is not required to get a new sample to bring the age down to zero since the mismatch condition between the source and the monitor may as well be brought to end with a transition of the source to the estimated value at the monitor. In this paper, we consider the following AoII minimization problem in which an information source observes a DTMC process, and a remote monitor estimates the process from the updates received by the monitor from the source. We consider a pull-based scheme such that the source transmits its current state whenever a pull request arrives at the source, and the transmission is completed in the next time slot. The monitor updates its estimation by considering the source dynamics with the MAP estimator. Notice that the monitor does not have full information on AoII including the very same time slot the most recent update is received. Additionally, we consider a sampling rate constraint on the monitor that limits the average number of pull requests it can send. Therefore, we aim to find an AoII-minimizing policy at the monitor with the timely generation of pull requests based on partial observations. Generally, AoII is investigated for symmetric Markov chains, and a single threshold policy is proposed to minimize the average AoII [2, 3, 4]. Additionally, in these works, the latest received information is used, termed as the martingale estimator [5], since it would be optimum only if the source were a martingale. On the other hand, in our previous works [6, 7, 8], we have shown that if the source process is a general asymmetric Markov chain, a simple threshold policy would not be guaranteed to perform optimally. More specifically, in [7, 8] it is shown that the optimum transmission policy should take into account all the estimation, source, and age values. Similarly, it was shown in [6] that the monitor can reduce the average AoII value if the pull request rates are allowed to be dependent on the latest received information. However, in that work, we have considered a preemption mechanism at the transmitter which aborts the transmission of the current information packet if the source process changes before the packet is received. Similarly, the pull-based AoII minimization problem in [4] considers an immediate transmission that allows the monitor to keep track of AoII. On the other hand, in the system model here, the decision maker (the monitor) has no direct information on the source process and hence the current AoII. In this paper, a sufficient statistic corresponding to their joint probability distribution under the MAP estimation rule is derived. Therefore, this paper motivates to obtain policies based on this distribution. The closest MDP formulation to our problem is the partially observable Markov decision process (POMDP) formulation that considers states which are not observable directly, but their probability distributions can be obtained with observations using partial information [9]; this distribution is called belief. The main assumption of this formulation is that an MDP can be defined from the unobserved states, and the expected reward of the system can be calculated from this distribution. Because of the curse of dimensionality of the underlying POMDP formulation, exact solutions [10, 11] for POMDPs are not tractable for problems with large state and action spaces. An approximate solution is the so-called myopic policy which selects the action that minimizes the expected reward by ignoring its effects on future rewards [12, 13]. In [12], it is shown that a myopic policy is optimum for POMDPs under certain conditions. Similarly, the Whittle index approach can be adapted for POMDPs to obtain an index policy [14, 15]. Because of the dependency of the estimator on the belief, it is no longer straightforward to use the POMDP formulation for the problem of interest. However, we can formulate our problem as a belief-MDP such that the belief in unobserved states is viewed as a fully-observable continuous-valued state of the belief-MDP, and its equivalence to a POMDP is shown in [11]. In some cases [16, 15], the belief-MDP can further be converted to an MDP with observable and finite states, and optimum policies can be obtained accordingly. Deep reinforcement learning (DRL) has recently gained popularity for solving MDPs using the exploration-exploitation trade-off [17]. Indeed, the value function for a POMDP can be approximated from the belief using DRL [18], or from the observation [19], and the action that minimizes the value function is applied as a sub-optimal policy. POMDP formulation has been used to minimize AoI in several works [15, 20, 16, 21, 22, 23, 24]. In [15, 20, 24, 22], system models involving multiple sensors and a single monitor have been studied for different scenarios. In these works, the monitor is only aware of the AoI of the sensor that successfully transmits at that time slot and estimates the AoI of other sensors based on its previous observations. In [15], [24] and [22], sub-optimal policies are obtained via the index policy, the myopic policy, and the particle filter, respectively. On the other hand, the authors of [22] convert the POMDP into a fully-observable discrete space MDP problem, and obtain an optimum policy. A similar system model has been studied in [25]. In that work, an entropy-based metric, namely uncertainty of information, is minimized by employing an index policy. Additionally, the AoI minimization problems in [21, 23, 16] consider failure status, channel availability, and the battery level as unobservable states, respectively. Under the assumption that each update includes a timestamp of the generation time, the monitor can access the correct AoI value for the time slot the update is received. On the other hand, when the delay on the channel is considered, the source process may change before the update arrives, thus no updates guarantee resetting AoII and the monitor never has the correct AoII value including the time slot an update arrives. Additionally, AoII metric depends on the dynamics of the source process, and it is upper bounded even if there is no sampling. These aspects make AoII problems different and more challenging from AoI-based formulations. The contribution of this paper can be summarized as follows: i) We propose a MAP estimator to be used at the monitor in place of the simple martingale estimator, for which the monitor updates its estimation with the MAP rule. ii) We derive a sufficient statistic, namely belief, that corresponds to the joint distribution of AoII and source state, for general asymmetric Markov chains. iii) We propose two belief-dependent policies, one of which is based on DRL, and we compare these two policies against two baseline belief-agnostic policies."
https://arxiv.org/html/2411.07087v2,OCMDP: Observation-Constrained Markov Decision Process,"In many practical applications, decision-making processes must balance the costs of acquiring information with the benefits it provides. Traditional control systems often assume full observability, an unrealistic assumption when observations are expensive. We tackle the challenge of simultaneously learning observation and control strategies in such cost-sensitive environments by introducing the Observation-Constrained Markov Decision Process (OCMDP), where the policy influences the observability of the true state. To manage the complexity arising from the combined observation and control actions, we develop an iterative, model-free deep reinforcement learning algorithm that separates the sensing and control components of the policy. This decomposition enables efficient learning in the expanded action space by focusing on when and what to observe, as well as determining optimal control actions, without requiring knowledge of the environment’s dynamics. We validate our approach on a simulated diagnostic task and a realistic healthcare environment using HeartPole. Given both scenarios, the experimental results demonstrate that our model achieves a substantial reduction in observation costs on average, significantly outperforming baseline methods by a notable margin in efficiency.","In traditional control systems, it is often assumed that all necessary information is readily available, which is seldom the case in practical scenarios [1, 29, 28, 27, 26]. The need to actively decide which observations to make adds a layer of complexity to the decision-making process, as it requires balancing the benefits of additional information against the costs of acquiring it [10, 12]. Additionally, tasks in virtual environments, such as those running on simulators, often disregard observation costs because the optimization goals—such as maximizing rewards or achieving specific objectives—are not inherently aligned with the expenses involved in acquiring observations [19, 9]. This disconnect allows for the assumption of complete knowledge of the environment, but ignoring observation costs in such models makes reinforcement learning applications diverge from real-world practice. In many real-world applications, particularly in healthcare, decision-making processes must account for the costs associated with actively obtaining observations. Medical assessments, diagnostic tests, and patient monitoring not only require financial resources but also demand significant time from healthcare professionals and patients alike. This inherent cost associated with information gathering necessitates strategies that judiciously balance the need for information with the resources available [3, 32]. Medical assessments can be regarded as sequential decision-making problems, where treatments are administered based on the patient’s current health states. These health states are inferred from observations, including physical examinations and clinical metrics. The challenge lies in making optimal decisions with limited and costly observations, which is critical for both patient outcomes and resource management [13, 21, 4]. Some examples of active observation actions within the full space are presented in Figure 1. Figure 1: Active observations within full state space This paper addresses the challenge of simultaneously learning an observation strategy and a control strategy in environments where observations are costly. In such settings, there is a fundamental trade-off between the cost of acquiring observations and the benefit they provide in making informed control decisions. We consider the simplest case of observation action design, where each observation action is a binary decision: whether to acquire a specific observation or not. This formulation leads to a total action space of 2^{|\mathcal{O}|}\times|\mathcal{A}_{\text{control}}|, where |\mathcal{O}| is the number of possible observations to make and |\mathcal{A}_{\text{control}}| is the number of control actions. This exponential growth in the action space introduces the curse of dimensionality, making the learning process significantly more complex than when learning a control policy alone. To tackle this challenge, we propose an iterative, model-free deep reinforcement learning approach that decomposes the sensing and control policies. By separating these two aspects, the learning algorithm can more efficiently navigate the enlarged action space. The model focuses on learning when and what to observe, alongside determining the optimal control actions, without requiring a complete model of the environment’s dynamics [23, 11]. We demonstrate the effectiveness of our method on two fronts of medical practice: a simulated Diagnostic Chain task designed to capture the essential elements of the problem, and a realistic healthcare environment using HeartPole [17]. These experiments showcase the model’s ability to make cost-effective observation decisions while achieving desirable control outcomes."
https://arxiv.org/html/2411.06887v1,Symmetrizable systems,Transforming an asymmetric system into a symmetric system makes it possible to exploit the simplifying properties of symmetry in control problems. We define and characterize the family of symmetrizable systems. These systems can be transformed into symmetric systems by a linear transformation of their inputs and outputs. We show that a Khatri-Rao rank needs to be satisfied for a system to be symmetrizable. We conclude that linear systems are generically neither symmetric nor symmetrizable.,"Symmetric systems, also known as reciprocal systems in the literature, are control systems that exhibit input-output symmetry in their transfer function matrices G(s)\in\mathbb{C}^{m\times m} as follows \Sigma_{e}G^{\top}(s)=G(s)\Sigma_{e}, (1) where \Sigma_{e} is a constant diagonal matrix whose diagonal elements are either 1 or -1. Relation (1) implies that a system is symmetric, if and only if, either its transfer function is a symmetric matrix or it becomes one by multiplying one or more of its columns by -1. Symmetric systems are found in diverse application areas, such as electrical circuits, chemical reactors, mechanical systems, and power networks [mohammadpour2010efficient, pates2022passive]. These systems have properties that simplify many control problems. For example, certain H_{2} and H_{\infty} optimal control problems have known analytic solutions when applied to symmetric systems [pates2022passive, pates2019optimal], the optimal linear-quadratic regulator of symmetric systems can be obtained by iterative learning control with no bias [taghavian2024optimal], and estimated by a single trajectory of the system [drummond2024learning]. In this paper, we extend the family of symmetric systems, by introducing symmetrizable systems. These systems may not be symmetric, but after a linear transformation of their inputs and outputs as H(s)=K^{-1}G(s)K, (2) they become symmetric, where K\in\mathbb{R}^{m\times m} is a constant matrix, called the symmetrizing gain. This definition extends the simplifying properties of symmetric systems to asymmetric systems. Namely, one can first transform the asymmetric system G(s) to a symmetric system H(s) as (2), solve a control problem for H(s), and convert the result back for G(s). For example, consider the static output-feedback controller u_{s}(t)=K_{s}\,y_{s}(t) designed for the symmetric system H(s) with input u_{s} and output y_{s}. This control law is equivalent to applying the following static output feedback to G(s): u(t)=KK_{s}K^{-1}y(t), (3) where u and y are the input and output of the asymmetric system G(s), respectively. We study the transformation (2) and provide the necessary and sufficient conditions for a given system G(s) to admit such a transformation. Further, we propose a method to find a symmetrizing gain K when one exists. The preliminaries are given in Section II and a background on symmetric systems is provided in Section III. Symmetrizable systems are introduced in Section IV and further discussed in Section V. We give some examples to demonstrate the results in Section VI and provide the concluding remarks in Section VII."
https://arxiv.org/html/2411.06870v1,AI-Native Multi-Access Future Networks - The REASON Architecture,"The development of the sixth generation of communication networks (6G) has been gaining momentum over the past years, with a target of being introduced by 2030. Several initiatives worldwide are developing innovative solutions and setting the direction for the key features of these networks. Some common emerging themes are the tight integration of AI, the convergence of multiple access technologies and sustainable operation, aiming to meet stringent performance and societal requirements. To that end, we are introducing REASON - Realising Enabling Architectures and Solutions for Open Networks. The REASON project aims to address technical challenges in future network deployments, such as E2E service orchestration, sustainability, security and trust management, and policy management, utilising AI-native principles, considering multiple access technologies and cloud-native solutions.This paper presents REASON’s architecture and the identified requirements for future networks. The architecture is meticulously designed for modularity, interoperability, scalability, simplified troubleshooting, flexibility, and enhanced security, taking into consideration current and future standardisation efforts, and the ease of implementation and training. It is structured into four horizontal layers: Physical Infrastructure, Network Service, Knowledge, and End-User Application, complemented by two vertical layers: Management and Orchestration, and E2E Security. This layered approach ensures a robust, adaptable framework to support the diverse and evolving requirements of 6G networks, fostering innovation and facilitating seamless integration of advanced technologies.","The forthcoming generation of communication networks, colloquially referred to as Sixth Generation (6G), represents a transformative leap beyond the existing paradigms of mobile communications [1]. This leap is essential to meet burgeoning demands for individual users and vertical industries, covering both indoor and outdoor environments and spanning multiple sectors and applications like automotive, manufacturing, public safety, eHealth, immersive media, and more [2]. These applications shift away from conventional Key Performance Indicators (KPIs), like faster data rates, reduced latency, or broader coverage, and move towards more complex classes of enablers for advanced services [3]. For example, the Fifth Generation (5G) Infrastructure Association (5G IA) [4] provides a comprehensive list discussing “integrated sensing and communications”, “cognition and connected intelligence”, “trustworthy and sustainable infrastructures”, and more. Similarly, other initiatives, like Hexa-X [5], discuss enablers like “network of networks”, “global service coverage”, “extreme experiences”, etc. A comprehensive comparison of the different visions can be found at [6]. Overall, all visions converge to a future network that architects a seamlessly interconnected world, where the integration of digital, physical, and human systems unfolds new dimensions of experience, efficiency, and societal transformation. TABLE I: List of Acronyms. Acronym Description 5G 5th Generation 5GPPP 5G Infrastructure Public Private Partnership 6G 6th Generation AI Artificial Intelligence AIaaS AI-as-a-Service AT Access Technology CIA Confidentiality, Integrity, and Availability CI/CD Continuous Integration / Continuous Delivery CNF Container Network Function CPaaS Communications-Platform-as-a-Service DAO Distributed Autonomous Organisation DLT Distributed Ledger Technology DT Digital Twin E2E End-to-End eMBB enhanced Mobile Broadband FDRL Federated Deep Reinforcement Learning FL Federated Learning FSO Free Space Optical GEO Geostationary Orbit IaC Infrastructure-as-Code IMT International Mobile Telecommunications IoT Internet of Things IT Information Technology ITU-T ITU Telecommunication KPI Key Performance Indicators KVI Key Value Indicators LEO Low Earth Orbit LiFi Light Fidelity MANO Management and Orchestration mATRIC Multi-access Technology Real-Time Intelligent Controller MEO Medium Earth Orbit mMTC massive Machine Type Communication MLOps Machine Learning Operations NaC Network-as-code NF Network Function NFV Network Function Virtualisation NGMN Next Generation Mobile Networks Alliance NIO Network Intelligence Orchestration NPN Non-Public Network NRE Network Resource Elasticity NTN Non-Terrestrial Network OPEX Operational Cost O-RAN Open Radio Access Network OT Operational Technology QoS Quality of Service QoE Quality of Experience RAN Radio Access Network RAT Radio Access Technologies REASON Realising Enabling Architectures and Solutions for Open Networks RINA Recursive InterNetwork Architecture RF Radio Frequency SBA Service-based Architecture SDG Sustainable Development Goals SLA Service-Level Agreement SFC Service Function Chain UPF User Plane Function URLLC Ultra-Reliable and Low-Latency Communication VIM Virtual Infrastructure Manager VM Virtual Machine VNF Virtual Network Function XAI Explainable AI However, shifting from traditional KPIs to a more complex set of enablers is only part of the wider paradigm shift shown in 6G ecosystems. We witness a transition from “govern by performance” to “govern by value” [3]. In other words, what is needed is not an indication of performance but an indication of value for the developed new ecosystems. This is how the term Key Value Indicators (KVIs) was born [7]. 6G is expected to not only bring innovative solutions for the above complex problems but also address other societal challenges, like environmental sustainability, ethical implications of technological advancements, and more [8]. A comprehensive, collaborative, and multifaceted approach is imperative to ensure progress that benefits all sectors of society and positively contributes to the global community. The architecture design is one of the most important aspects of each new generation of a communications network system. It is considered its foundation and describes what and how services are provided and how they are integrated. Based on that, this paper presents Realising Enabling Architectures and Solutions for Open Networks (REASON) project111REASON Project: https://reason-open-networks.ac.uk/ and its envisaged architecture. REASON aims to address the above technical and non-technical challenges and deliver an End-to-End (E2E) reference Open Network blueprint, considering all segments and functions of a future network. REASON will pursue to influence the future technology roadmap, making openness, interoperability and Artificial Intelligence (AI)-native capabilities the default standards in network architectures and systems. Reference architectures are usually driven by three factors [9], i.e., the “new scenarios and requirements to be delivered”, the “new technologies that need to be integrated”, and the “inspiration from existing systems”. This is the scope of this paper. Analysing the challenges identified and several use cases defined, we will provide a comprehensive set of requirements to drive future 6G networks. These requirements will later be used to define a novel reference architecture that enables principles like interoperability, agility, sustainability, resilience, and security, all key to future Industrial, Entertainment and Smart Cities applications [10]. Our use case proposition is also expected to demonstrate not only the capabilities of the REASON architecture but also to motivate future scenarios with strong market demand and commercial opportunities. As an expected outcome, the REASON architecture aims to enable multi-technology access network integration to meet the emerging 6G KPIs. Moreover, it will promote network densification to ensure support of the described 6G use cases. The architecture will be AI-native, enabling service and network optimisations in an E2E fashion. Cognitive tools and task-based networking are considered integral parts of the architecture, and we will discuss how they are incorporated into our system. Finally, the aspects of energy-awareness, E2E monitoring, seamless edge-cloud computing continuum, openness, interoperability and security will drive many of our architectural decisions, and we explain how the proposed design addresses them. This paper is structured as follows. Sec. II compares 5G and expected 6G capabilities and comprehensively compares REASON with other future networking projects. The technical and societal challenges identified in future 6G networks and some expected usage scenarios are presented in Sec. III. Sec. IV provides insight into the use cases identified within REASON. Building upon the challenges and use cases, Sec. V presents all the requirements that should be addressed from future network architecture and implementation. Following the requirements, the envisaged REASON architecture is detailed in Sec. VI. Finally, Sec. VIII provides our final remarks and some future directions within the REASON project. A table summarising all the acronyms can be found in Table I."
https://arxiv.org/html/2411.06782v1,QuadWBG: Generalizable Quadrupedal Whole-Body Grasping,"Legged robots with advanced manipulation capabilities have the potential to significantly improve household duties and urban maintenance. Despite considerable progress in developing robust locomotion and precise manipulation methods, seamlessly integrating these into cohesive whole-body control for real-world applications remains challenging. In this paper, we present a modular framework for robust and generalizable whole-body loco-manipulation controller based on a single arm-mounted camera. By using reinforcement learning (RL), we enable a robust low-level policy for command execution over 5 dimensions (5D) and a grasp-aware high-level policy guided by a novel metric, Generalized Oriented Reachability Map (GORM). The proposed system achieves state-of-the-art one-time grasping accuracy of 89% in real world, including challenging tasks such as grasping transparent objects. Through extensive simulations and real-world experiments, we demonstrate that our system can effectively manage a large workspace, from floor level to above body height, and perform diverse whole-body loco-manipulation tasks. See our robot at work: quadwbg.github.io.","I INTRODUCTION Quadrupedal loco-manipulation, which integrates legged locomotion with robotic arm manipulation, has emerged as a key research area due to its broad potential applications, including household assistance, urban maintenance, disaster relief, and autonomous field operations [1, 2, 3, 4]. Recent advancements in reinforcement learning (RL) have enabled the development of end-to-end policies for whole-body locomotion and manipulation [5, 6, 7, 8, 9], allowing robots to perform tasks that require seamless coordination of movement and object interaction. While end-to-end RL has substantially improved locomotion skills [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22], loco-manipulation remains highly challenging due to the increased action dimensionality and complex physical interactions involved. These challenges often result in loco-manipulation policies with mediocre accuracy and limited generalizability [5, 7], especially when grasping objects of different shapes, sizes, and materials, thereby restricting their effectiveness in real-world applications. To enhance both the performance and generalizability of whole-body grasping systems, we draw inspiration from the success of various grasp detection techniques [23, 24, 25, 26, 27, 28, 29]. These methods demonstrate robust performance in detecting grasp poses for diverse, unseen objects in cluttered environments, including challenging materials like transparent or specular surfaces. By integrating grasp pose detection with motion planning, these approaches consistently achieve impressive accuracy, typically exceeding a 90% grasping success rate across arbitrary object configurations in table-top settings, as illustrated in 1. Figure 1: Real-world examples of whole-body grasping objects from various locations. Top row: Grasping transparent and specular objects in cluttered environments. Bottom row: Grasping of objects at various heights. This inspires us to take the best of both worlds via integrating legged locomotion with grasp detection to achieve high-performance and highly generalizable loco-manipulation. However, this combination is highly nontrivial. Directly applying grasp detection results for arm motion planning in legged robots is insufficient, as it ignores the coordination required between the robot’s body and arm movements. To address these challenges, we introduce QuadWBG (Generalizable Quadrupedal Whole-Body Grasping), a modular system consisting of four key modules: perception, planning, locomotion, and manipulation (see Figure 2). The perception module integrates object segmentation and grasp detection, handling object tracking and grasp pose prediction. The planning and locomotion modules function as high-level and low-level controllers, respectively, guiding the robot to approach the grasp pose. Finally, the manipulation module leverages motion planning to move the arm and execute the grasp while maintaining the body stationary. At the core of this system is a key innovation: the Generalized Oriented Reachability Map (GORM). GORM acts as a metric for evaluating the reachability of the base pose relative to the target pose across six degrees of freedom. It efficiently guides the planning module during training by calculating the optimal base pose for grasping tasks. GORM also captures the robot’s overall reachability from various positions and orientations, enabling the policy to select base poses that optimize both arm reachability and robot stability. This ensures precise grasping while maintaining balance and locomotion stability. Our framework offers additional advantages. By incorporating highly robust grasp detection modules trained on large-scale real and synthetic data, we eliminate the sim-to-real gap commonly encountered in end-to-end visual policy learning for manipulation. Furthermore, our framework provides clearer insights into system performance and allows for targeted optimization of each module. Through extensive simulations and real-world experiments, we demonstrate that our system achieves state-of-the-art performance in both grasping accuracy and handling a wide range of objects. The system consistently delivers robust whole-body control across a variety of tasks. Notably, it achieves a remarkable one-time grasping accuracy of 89% in real-world scenarios, even excelling in challenging tasks such as grasping transparent objects. These results underscore the system’s high precision and adaptability in complex environments. Figure 2: Pipeline of our system. First, we train a teacher-student 5D low-level policy in simulation (A). The perception module (B) then continuously tracks the object, and generates grasping pose guiding manipulation module (C). This pose is also utilized by the planning module (D) to command the locomotion policy."
https://arxiv.org/html/2411.06751v1,DP and QP Based Decision-making and Planning for Autonomous Vehicle,"Autonomous driving technology is rapidly evolving and becoming a pivotal element of modern automation systems. Effective decision-making and planning are essential to ensuring autonomous vehicles operate safely and efficiently in complex environments. This paper introduces a decision-making and planning framework for autonomous vehicles, leveraging dynamic programming (DP) for global path planning and quadratic programming (QP) for local trajectory optimization. The proposed approach utilizes S-T graphs to achieve both dynamic and static obstacle avoidance. A comprehensive vehicle dynamics model supports the control system, enabling precise path tracking and obstacle handling. Simulation studies are conducted to evaluate the system’s performance in a variety of scenarios, including global path planning, static obstacle avoidance, and dynamic obstacle avoidance involving pedestrian interactions. The results confirm the effectiveness and robustness of the proposed decision-making and planning algorithms in navigating complex environments, demonstrating the feasibility of this approach for autonomous driving applications.","I-A Motivation and Problem Statement In recent years, autonomous driving technology has made significant advancements, becoming a cornerstone of modern transportation automation. Unlike drones or smart robots, autonomous vehicles are responsible for performing various ground transportation tasks while maintaining passenger comfort and vehicle stability. To ensure the safety and comfort of passengers, it is crucial that autonomous vehicles maintain stability in all driving conditions. Therefore, a reliable decision-making and planning system is required to navigate effectively in challenging scenarios, such as avoiding dynamically appearing obstacles, handling sharp turns, and ensuring pedestrian safety on roadways. Additionally, unstable driving and extra mileage can lead to increased tire wear and passenger discomfort, emphasizing the need for planning algorithms that can generate efficient, stable, and collision-free paths. Before executing decision-making and planning algorithms, it is essential to establish a comprehensive understanding of the driving environment using global perception techniques, such as those described in [1] [2]. These technologies enable autonomous vehicles to perceive and interpret their surroundings in real-time, accurately identifying obstacles and their positions, which is crucial for making informed decisions. Once the perception layer is in place, the focus shifts to designing effective path-planning and control strategies that ensure the vehicle’s safety and stability in dynamic environments. Planning algorithms for autonomous vehicles are responsible for generating safe and optimized routes that avoid potential collisions while remaining stable. In this context, dynamic programming (DP) and quadratic programming (QP) algorithms are essential, as they enable decision-making that balances optimal path selection with vehicle stability [3]. The S-T graph representation provides a visual framework for trajectory planning, allowing the vehicle to adapt to both static and dynamic obstacles. This paper presents a decision-making and planning framework for autonomous vehicles, integrating DP and QP algorithms to achieve safe and stable navigation. Global planning is handled through dynamic programming to generate optimal routes, while local trajectory optimization is performed using quadratic programming to ensure smooth and collision-free paths. The S-T graph approach is employed to facilitate real-time obstacle avoidance and path adjustments based on the perceived environment. The main contributions of this work are summarized as follows: • Decision-making and Planning Framework: This paper proposes an integrated decision-making and planning framework based on DP for global planning and QP for local trajectory optimization. The combined approach allows the autonomous vehicle to navigate complex environments safely and efficiently. • Dynamic and Static Obstacle Avoidance: The proposed framework leverages an S-T graph to represent the vehicle’s planned path and speed profile in real time, enabling effective avoidance of both dynamic and static obstacles. • Enhanced Navigation Capabilities: By incorporating DP and QP algorithms, this work enhances the vehicle’s ability to generate optimized routes and trajectories, improving overall navigation performance and operational safety. I-B Related Works Various controllers have been developed to achieve effective control of autonomous vehicles. Stanford University’s Stanley autonomous vehicle utilized a Proportional-Integral (PI) controller, which was designed based on a linearized model of the vehicle’s dynamics relative to the path [4]. A Radial Basis Function Neural Network (RBFNN)-based adaptive PID controller was proposed for longitudinal control to achieve precise speed tracking [5]. Moreover, fuzzy logic controllers were integrated with traditional PID controllers to enable parameter adaptation, improving control accuracy [6]. However, these methods often face challenges in achieving precise longitudinal control, as throttle and brake are separate components, and a single PID controller may not be efficient or accurate enough in coordinating both systems. For lateral control, alternative techniques such as sliding mode control (SMC) have been employed to counter parameter uncertainties and external disturbances, allowing better trajectory control [7]. Additionally, fuzzy PID controllers focusing on new self-adaptive fuzzy PID designs based on nonlinear multi-input multi-output (MIMO) structures have been explored [8] [9]. However, PID control remains a simple feedback mechanism with manually tuned parameters, making it difficult to achieve real-time and precise responses [10]. In contrast, Linear Quadratic Regulator (LQR) control is designed using a state-space model of the system, enabling it to leverage the system’s dynamic characteristics more effectively [11]. For decision-making and planning algorithms, commonly employed methods include the A* search algorithm and Dijkstra’s algorithm [12]. A typical decision-making and planning module can be divided into three levels: route planning, behavioral decision-making, and motion planning. Zhao et al. identified route planning as the process of generating a global path by combining map information after receiving a specified travel destination, which serves as a reference for subsequent detailed path planning [13]. The behavioral decision-making layer makes decisions based on perception data and, through motion planning, ultimately generates a trajectory that satisfies specific constraints. This paper adopts a combination of DP and QP, which is well-suited for multi-stage decision problems and efficiently handles scenarios with large state spaces and complex state transitions [14]."
https://arxiv.org/html/2411.06556v1,"EO-GRAPE and EO-DRLPE: Open and Closed Loop Approaches
for Energy Efficient Quantum Optimal Control","This research investigates the possibility of using quantum optimal control techniques to co-optimize the energetic cost and the process fidelity of a quantum unitary gate. The energetic cost is theoretically defined, and thereby, the gradient of the energetic cost for pulse engineering is derived. We empirically demonstrate the Pareto optimality in the trade-off between process fidelity and energetic cost. Thereafter, two novel numerical quantum optimal control approaches are proposed: (i) energy-optimized gradient ascent pulse engineering (EO-GRAPE) as an open-loop gradient-based method, and (ii) energy-optimized deep reinforcement learning for pulse engineering (EO-DRLPE) as a closed-loop method. The performance of both methods is probed in the presence of increasing noise. We find that the EO-GRAPE method performs better than the EO-DRLPE methods with and without a warm start for most experimental settings. Additionally, for one qubit unitary gate, we illustrate the correlation between the Bloch sphere path length and the energetic cost.","The field of quantum computing (QC) is undergoing rapid development in both theoretical and practical aspects from both academic and industrial stakeholders. At its foundation, QC involves orchestrating quantum mechanical properties for information processing [1, 2] via quantum algorithms, thereby promising a significant reduction in computational resources [3] for specific applications over their classical counterparts. Exemplary use cases include simulating quantum mechanical systems [4] towards novel discovery in material sciences and breaking cryptographic protocols [5] ubiquitous for secure transactions over the internet. The required orchestration of quantum information towards QC is physically implemented by engineering quantum processing units (QPU), which are currently prototyped using a myriad of technologies like superconducting circuits, trapped ions, photonics, electron spin, etc. A significant challenge in the QC field [6, 7, 8] is to manufacture better quality and scalable QPU against the fragility of quantum information from environmental noises and operational imperfections. Akin to classical computers, the interface between the QC applications and the processor is organized into translation layers, called the quantum computation stack [9, 10], as shown in figure 1. From top to bottom, first, the application is formulated as a quantum algorithm and expressed in a quantum programming language. Then, a quantum compiler decomposes and optimizes the high-level code into native operations supported by the target QPU. Thereafter, the quantum microarchitecture schedules and issues the low-level instructions in real-time. These instructions (like initialization, unitary gates, and measurements) further need to be translated to corresponding analog pulses that optimally control the accessible degrees of freedom of the quantum system. These electromagnetic analog signals perform the necessary transformation for synthesizing quantum unitary gates on specific addressable qubits while mitigating the undesirable effects of noise. Eventually, these hardware-aware signals implement the desired logical operation dictated by the hardware-agnostic quantum algorithm on the target QPU. Figure 1: Overview of the system design of a quantum accelerator with classical control and various software modules required for research and development is shown on the left. The different abstraction layers for full-stack quantum computing are shown on the right. This research pertains to the optimal control layer highlighted with a dotted outline. The QC stack, as discussed above, provides abstraction layers to organize the research and development. Thus, operations, including a computationally universal set of gates, initialization, and measurements, are provided as primitives for the microarchitecture layer. In the ideal scenario, the pulse-level implementation details of these operations are predetermined in the control layer during the characterization and calibration of the QPU prior to deployment. However, in the current noisy intermediate-scale quantum (NISQ) computing era, this abstraction [11] is preventing further optimization of the QPU performance. Pulse-level control, when exposed to higher stack layers, allows advanced operating procedures that can mitigate the effect of noise dynamics and fine-tune a more extensive set of unitary gates with respect to other dependencies (e.g., cross-talk). While there are many advantages to pulse control, such as a high degree of flexibility and higher speed of gate execution, there are also certain downsides to using pulse control, such as calibration requirements and overhead and scalability bottlenecks. Nevertheless, advancing research in this direction is not only imperative for current QC roadmaps to extract maximal performance but also allows a principled theoretical understanding of the limits of quantum information processing using classical control algorithms and electronics. Current pulse control approaches focus primarily on optimizing the fidelity score. Motivated by the theoretical possibility and operational need, this research concerns a novel approach for pulse-level unitary gate synthesis, that co-optimizes the fidelity along with the pulse energy. Within the allied field of quantum thermodynamics, there is a growing interest in the possibility of achieving quantum advantage through the perspective of energy efficiency [12] instead of computational resources like runtime and memory needs. This is imperative for at least two specific reasons. Firstly, one of the earliest motivations of quantum computation stems from reversible computation [13] with minimal energetic cost instead of a computational complexity advantage. Secondly, quantum processors within a quantum accelerator stack are benchmarked in performance against state-of-the-art classical high-performance computing (HPC) systems. In such HPC systems, optimizing and benchmarking the energetic cost [14] is motivated by both operational costs and environmental sustainability directives. While this direction is gaining traction, specific research in the energetic cost of a quantum computational process is both sparse and theoretical at present. For example, [15] proves an inequality bounding the change of Shannon information encoded in the logical quantum states by quantifying the energetic cost of Hamiltonian gate operations. Subsequently, [16] showed that optimal control problems can be solved within the powerful framework on quantum speed limits and derive state-independent lower bounds on energetic cost. Recent work in quantum optimal control (QOC) theory has primarily focused on developing control to carry out quantum processes with the highest fidelity possible. These processes include quantum processes such as state initialization, quantum measurements, and implementing quantum unitary gates. However, in the context of the growing impetus in achieving quantum advantage through energy efficiency [17], it seems crucial to investigate the energy efficiency of quantum operations, in particular, unitary quantum gates within a pragmatic quantum compiler framework. To this effect, we address three main research questions in this article: \mathcal{RQ}_{1}: How can we estimate the energetic cost of synthesizing a quantum unitary gate? \mathcal{RQ}_{2}: What is the relation between the fidelity of unitary synthesis and the energetic cost associated with the pulse? \mathcal{RQ}_{3}: How can fidelity and the energetic cost be co-optimized within existing quantum optimal control strategies? The core contributions of this research are summarized below: 1. A theoretical formulation of the energetic cost of implementing a quantum unitary gate using discrete control pulses and the gradient of the energetic cost with respect to the control parameters that are required to optimize the cost. 2. Development of a modified version of the gradient ascent pulse engineering (GRAPE) algorithm to co-optimize a quantum unitary gate’s fidelity and energetic cost. The proposed novel energy-optimized algorithm is called EO-GRAPE. 3. Identification and analysis of the trade-off between the fidelity and energetic cost of implementing a quantum unitary gate. 4. Development of a deep reinforcement learning (DRL) agent able to learn and generate energy-optimized control pulses for a universal set of quantum unitary gates. The proposed novel energy-optimized method is called EO-DRLPE. 5. Benchmarks to evaluate the performance of the EO-GRAPE algorithm and EO-DRLPE method with increasing noise levels. 6. Evaluation of the optimality of the geodesic path cost of the two methods for synthesizing 1-qubit unitary gates. The rest of the article is organized as follows. Section 2 provides background information on pulse engineering for unitary synthesis. Thereafter, a non-exhaustive survey and classification of quantum optimal control techniques is provided. The energetic cost of a quantum operation is introduced. In section 3, the GRAPE algorithm is introduced. Thereafter, we present our novel EO-GRAPE algorithm that co-optimizes the energy and process fidelity based on the derived gradient. The trade-off between these factors is explored. Section 4 introduces the alternative closed-loop approach, EO-DRLPE and its consecutive performance analysis. In section 5, we provide an analysis of the correlation between the energetic cost and Bloch sphere path length for single qubit unitary synthesis. Section 6 concludes the article and provides suggestions for future research directions."
https://arxiv.org/html/2411.06542v2,Is Linear Feedback on Smoothed Dynamics Sufficient for Stabilizing Contact-Rich Plans?,"Designing planners and controllers for contact-rich manipulation is extremely challenging as contact violates the smoothness conditions that many gradient-based controller synthesis tools assume. Contact smoothing approximates a non-smooth system with a smooth one, allowing one to use these synthesis tools more effectively. However, applying classical control synthesis methods to smoothed contact dynamics remains relatively under-explored. This paper analyzes the efficacy of linear controller synthesis using differential simulators based on contact smoothing. We introduce natural baselines for leveraging contact smoothing to compute (a) open-loop plans robust to uncertain conditions and/or dynamics, and (b) feedback gains to stabilize around open-loop plans. Using robotic bimanual whole-body manipulation as a testbed, we perform extensive empirical experiments on over 300 trajectories and analyze why LQR seems insufficient for stabilizing contact-rich plans. The video summarizing this paper and hardware experiments is found here.","Dexterous manipulation is full of contact-rich interactions, enabling various tasks through complex frictional interactions [1]. Historically, the non-smooth nature of contact has precluded a range of planning and control methods that rely on gradients of the dynamics. Recent advances have utilized contact smoothing — where non-smooth dynamics are replaced by a continuously differentiable proxy — to great effect as surrogate dynamics models for planning through contact [2, 3, 4, 5]. One may hope, then, that smoothing enables the use gradient-based control. (a) LQR. (b) Open-loop controller. Figure 1: These figures show snapshots of hardware experiments using LQR and open-loop controllers under perturbations to initial conditions of cylinder. The thick and thin lines represent the desired frame at the terminal time step and the current frame of the cylinder, respectively. While LQR outperforms open-loop in this example, a more comprehensive evaluation shows that LQR generally performs poorly. This work suggests that the above hope may face significant obstacles. We (1) introduce LQR control for contact-manipulation via contact smoothing. Furthermore, we (2) present and analyze robust trajectory optimization, hoping that the generated trajectories are robust to the model errors accumulated by using a surrogate dynamics model for control, and thus more amenable to LQR. Then, we (3) extensively evalute the performance of these methods, both in simulation and in hardware, on a bimanual whole-body manipulation as shown in Fig. 1. In short, we find: Despite its efficacy in planning through contact, dynamical smoothing alone is unsatisfactory as a means to obtaining linear control policies. Finally, we (4) identify the key factors leading to the inadequacies of linear control; namely, the unilaterality of contact, and the tendency of controllers to “push and pull” unless the dynamics are only very-slightly smoothed."
https://arxiv.org/html/2411.06382v1,Hardware-in-the-Loop for Characterization of Embedded State Estimation for Flying Microrobots,"Autonomous flapping-wing micro-aerial vehicles (FWMAV) have a host of potential applications such as environmental monitoring, artificial pollination, and search and rescue operations. One of the challenges for achieving these applications is the implementation of an onboard sensor suite due to the small size and limited payload capacity of FWMAVs. The current solution for accurate state estimation is the use of offboard motion capture cameras, thus restricting vehicle operation to a special flight arena. In addition, the small payload capacity and highly non-linear oscillating dynamics of FWMAVs makes state estimation using onboard sensors challenging due to limited compute power and sensor noise. In this paper, we develop a novel hardware-in-the-loop (HWIL) testing pipeline that recreates flight trajectories of the Harvard RoboBee, a 100mg FWMAV. We apply this testing pipeline to evaluate a potential suite of sensors for robust altitude and attitude estimation by implementing and characterizing a Complimentary Extended Kalman Filter. The HWIL system includes a mechanical noise generator, such that both trajectories and oscillatinos can be emulated and evaluated. Our onboard sensing package works towards the future goal of enabling fully autonomous control for micro-aerial vehicles.","Autonomous flapping-wing micro-aerial vehicles (FWMAV) possess the unique combination of small size and high maneuverability, opening a wide range of potential real-world applications. With wingspans as small as 2.5 centimeters Wood (2008), FWMAVs have the potential of assisting in tasks such as artificial pollination, environmental monitoring, or search and rescue in small, difficult to reach areas. To accomplish such applications, fully autonomous flight of FWMAVs (which includes robust control and state estimation using an onboard electronics package) must be achieved. With the small scale of FWMAVs comes challenging dynamics that makes state estimation difficult. As vehicle size diminishes, the vehicle’s dynamics scale accordingly. For example, rotational acceleration rate scales as l^{-1} , resulting in the ability to perform rapid attitude changes, similar to saccades observed in flying insects and birds Kumar and Michael (2012); Dickinson (2005); Wissa (2022). The vehicle dynamics are inherently unstable, requiring active control to perform corrective maneuvers which produces low frequency oscillation modes in FWMAV flights McLean (2003). Furthermore, the induced oscillations from flapping wings generates additional disturbances to the observed sensor data. As a result, state estimators must carefully consider the vehicle’s dynamics and characteristic modes to provide accurate estimations. While solutions have been derived for similar flight dynamics, an additional challenge for the design of any state estimation algorithm is accounting for the power, weight, and compute restrictions of these vehicles Aurecianus et al. (2018). Figure 1: (a) The Harvard RoboBee, the target FWMAV, photographed with the IMU and ToF components utilized in the hardware experiments. (b) A RoboBee flight trajectory that the robot arm reproduces in (c), while carrying the IMU and ToF sensors to collect data. (d) The sensor package, mounted to the distal end of the arm, which contains the sensor components shown in (a) for state estimation. The first demonstration of an insect-scale microrobot to carry its own weight solved the problem of flapping-wing propulsion at the micro-scale by driving the wings with piezoelectric bimorph actuators Wood (2008). Due to the poor efficiency of motors at smaller scales (due to surface area to volume ratio scaling, resulting in a greater dominance of friction Trimmer (1989)), alternate propulsion techniques for insect-scale FWMAV have been developed, including dielectric elastomer actuators and piezoelectric bending actuators Chen et al. (2022); Ozaki et al. (2021). This paper considers the Harvard RoboBee, an 81~{}\mathrm{mg} FWMAV shown in Figure 1 (a). The Harvard RoboBee leverages piezoelectric bimorph actuators to generate a thrust-to-weight ratio greater than unity, sufficient to power agile flight with some overhead available for a small payload (40~{}\mathrm{mg}). However, the vehicle’s minimal payload, along with fast vehicle dynamics, has historically resulted in several tasks being performed off-board including computation and sensing. Specifically, current autonomous flights for the RoboBee rely on an array of external motion capture cameras to provide the necessary state estimates required for feedback control McGill et al. (2022). Research in control autonomy for larger systems has demonstrated onboard localization using a wide array of sensors, such as cameras and scanning laser range finders Bi et al. (2018) Hu et al. (2022). However, the considered vehicles are one to two order magnitudes larger than the RoboBee, which makes several of the considered sensors infeasible as they do not satisfy the RoboBee’s limited weight and power constraints. Nonetheless, biology offers evidence of a plethora of unique sensory mechanisms that enable birds and insects to maintain stable flight orientations Sane et al. (2007); Croon et al. (2022); De Croon et al. (2022). Thus, past research has similarly demonstrated the possibility of onboard sensor feedback for RoboBee control. Fuller et. al. considered several options for enabling stable orientation control for the RoboBee, using an onboard gyroscope and a bio-inspired ocelli (horizon detection sensor) Fuller et al. (2014, 2013). Helbling et. al. demonstrated onboard integration of a Time-of-Flight sensor for altitude estimation for the RoboBee Helbling et al. (2017). These prior research efforts informed the survey and selection of sensor components as shown in Figure 1 (a). However, the mechanical complexity of the RoboBee makes the integration and evaluation of potential sensing packages and algorithms challenging. As a result, the scale, manufacturing complexity, and limitations of the RoboBee prevented the prototyping of advanced estimation algorithms. This challenge along with computational limitations led to the use of relatively simple state estimation algorithms that relied on techniques like gyroscopic integration which produced significant drift up to 6 degrees in orientation estimation after only a 2 second flight experiment Fuller et al. (2014). Figure 2: Block diagram illustrating the overall pipeline for the evaluation of state estimation algorithms. Based on RoboBee flight trajectory data, which is captured with a suite of Vicon motion capture cameras, a candidate onboard sensor package is evaluated by reproducing the same flight trajectories in hardware (or simulation). Subsequently, the sensor data is fed into the state estimation algorithm which utilizes a complementary filter and EKF to estimate the orientation and altitude of the vehicle. These challenges motivated the development of a lightweight sensing suite, which integrates several of the components listed above, and a novel state estimation algorithm based on an Extended Kalman Filter by Talwekar et al. This algorithm simultaneously estimates pitch, altitude, and translational velocity Talwekar et al. (2022) for extended durations (20 seconds). To evaluate the proposed sensing package while avoiding the constraints imposed by insect-scale FWMAVs, Talwekar used an offboard experimental setup, in which he waved the sensing package by hand inside a motion capture arena to collect ground truth data for evaluation. Although this testing pipeline allowed for rapid evaluation and iteration, the dynamics of actual FWMAV flights are complex and introduce several sources of noise that were not reflected in their evaluation procedure. Although the majority of the previous sensing research for insect-scale FWMAVs do not explore more advanced state estimation algorithms, researchers have investigated novel estimators for vehicles with similar noisy dynamics. Mahony et. al derived a fundamental filtering architecture that applies to rotation matrices and similarly Madgwick et al. proposed a gradient-descent complementary filter for attitude estimation for quad-rotors Mahony et al. (2008); Madgwick et al. (2011). In particular, researchers have explored algorithms for oscillating environments using filter architectures such as Cascaded Complementary Filters (CCF) and Complementary Kalman Filters Chiella et al. (2019); C and Jain (2016). While these papers consider a similar challenge (i.e., state estimation for aerial robots), the performance of each algorithm is validated in a simulated environment, with minimal, if any, experiments on hardware. Given the notable hardware constraints, however, performing thorough hardware experiments to validate accuracy of potential state estimation algorithms and sensors under FWMAV flight dynamics is important for motivating the substantial efforts needed to bring the compute and sensors onboard. However, given the hardware complexity in the manufacturing of vehicles like Harvard’s RoboBee, evaluate state estimation algorithms in hardware settings is quite challenging. Thus, given insect-scale FWMAV’s limited accessibility and complex fabrication process, research has also been invested into developing methods of approximating RoboBee dynamics and controls for hardware-in-the-loop testing of more complicated software solutions that can only be solved with experimentation. Chen, et. al presented a method of simulating FWMAV dynamics on quadrotors to create a more reproducible experimental setup for software control/state estimation solutions Chen et al. (2017). While this example successfully reproduces RoboBee dynamics by mapping the control inputs of a FWMAV to quadrotor inputs, they do not simulate the body oscillation dynamics that makes state estimation for FWMAVs particularly challenging. Effective evaluation of potential state estimation algorithms and sensors requires a more complete reproduction of FWMAV flight dynamics. Specifically, we seek to incorporate a richer suite of dynamics, including noise from various oscillation modes, in addition to reproducing gross trajectories in order to subject sensors and algorithms to more realistic flight scenarios. Consequently, in this paper, we not only present a minimal sensor suite and a simplified state estimation algorithm for FWMAVs, but also design a hardware-in-the-loop pipeline to characterize estimation performance. This pipeline recreates FWMAV flight experiments (Figure 1 (b)) using a UR5e (Figure 1 (c)) and adds realistic noise observed in flight experiments. This setup allows for rapid evaluation of algorithm accuracy on the considered hardware sensor components (Figure 1 (d)), achieving more accurate state estimation results than previous efforts. 1.1 Our Contributions In this paper, we propose a comprehensive state estimation algorithm and sensor suite that satisfies the RoboBee’s hardware constraints. Although we do not deploy the suite onboard the robot, we constrain hardware choices to those with opportunities for miniaturization. Specifically, we limit sensor selection to those that would fit within the approximately 50~{}\mathrm{mg} mass budget. The sensor suite incorporates an off-the-shelf nine-axis inertial measurement unit (IMU) with an accelerometer, magnetometer, and gyroscope (InvenSense ICM20948) and a time-of-flight (ToF) sensor (VL6180), shown in Figure 1 (a), that provides sensor measurements for a Complementary Extended Kalman Filter (CEKF), based on RoboBee dynamics. Additionally, we develop a testing pipeline using both simulation and hardware experiments, leveraging RoboBee flight experiments (as outlined in Figure 2) to validate that the accuracy of the algorithm does not degrade with the presence of body oscillatory modes. The Supplemental Video provides an overview of replayed trajectories utilized in this work as well as the mechanical noise generator designed to induce body oscillatory dynamics on the sensor suite. We observe that under RoboBee flight dynamics, the CEKF achieves state estimates with less than 1^{\circ} RMSE error in orientation and less than 2~{}\mathrm{mm} error in altitude with 16-bit fixed-point precision. Additionally, we approximate that a cycle of the proposed CEKF will require less than 11 microseconds on an onboard microcontroller for computations, which satisfies real-time latency requirements for the proposed state estimation algorithms. The rest of the paper is organized as follows. In Section 2 we derive the Complementary Extended Kalman Filter architecture for state estimation and evaluate the performance on simulated sensor data generated from open-loop RoboBee flight experiments. In Section 3 we select sensors that meet the SWaP constraints of the vehicle and characterize their performance. Section 4 presents our hardware-in-the-loop evaluation approach and the performance of the sensor suite and algorithm with RoboBee closed-loop dynamics. Finally, given the real-time latency requirement of state feedback for controllers, section 5 evaluates the required floating point operations (FLOPS) and the rate at which CEKF accuracy degrades for less precise floating point bit representations."
https://arxiv.org/html/2411.06136v1,Decentralized Semantic Communication and Cooperative Tracking Control for a UAV Swarm over Wireless MIMO Fading Channels,"This paper investigates the semantic communication and cooperative tracking control for an UAV swarm comprising a leader UAV and a group of follower UAVs, all interconnected via unreliable wireless multiple-input-multiple-output (MIMO) channels. Initially, we develop a dynamic model for the UAV swarm that accounts for both the internal interactions among the cooperative follower UAVs and the imperfections inherent in the MIMO channels that interlink the leader and follower UAVs. Building on this model, we incorporate the power costs of the UAVs and formulate the communication and cooperative tracking control challenge as a drift-plus-penalty optimization problem. We then derive a closed-form optimal solution that maintains a decentralized semantic architecture, dynamically adjusting to the tracking error costs and local channel conditions within the swarm. Employing Lyapunov drift analysis, we establish closed-form sufficient conditions for the stabilization of the UAV swarm’s tracking performance. Numerical results demonstrate the significant enhancements in our proposed scheme over various state-of-the-art methods.","Cooperative tracking control for unmanned aerial vehicle (UAVs) swarms has garnered substantial interest across both the industrial and academic realms, owing to its broad applications in fields such as surveillance and agricultural monitoring [1, 2]. A typical UAV swarm, comprising a group of follower UAV agents and a leader controller UAV, is depicted in Fig. 1. The leader UAV monitors the real-time states of the follower UAVs—including their position, speed, and angular velocity—and intermittently generates tracking control signals that are transmitted to the follower UAVs via an unreliable wireless network. Upon reception of these control directives, the follower UAVs adjust their states to conform to predetermined target profiles. The wireless network connecting the follower UAVs and the leader UAV is susceptible to a myriad of impairments, such as signal fading and channel noise, which has the potential to markedly degrade the tracking control efficacy of the UAV swarm. Figure 1: Typical architecture of a UAV swarm over the wireless network. Tracking controller design for UAV swarms over wireless network entails considerable challenges. Firstly, most existing research on UAV control assumes static communication channels within UAV swarms. For instance, UAV tracking control laws that leverage the pole placement technique and the proportional-integral-derivative (PID)-based approach have been developed in [3] and [4, 5, 6], respectively. However, these methods are heuristic and lack optimality. To mitigate reliance on trial-and-error methods, a linear optimal tracking control law utilizing Riccati equations to derive optimal control gains for UAV swarms was introduced in [7, 8]. It is important to note that all the aforementioned works assume static channels in UAV swarms. Brute-force applications of these control laws to time-varying fading channels can severely impair tracking performance. While several studies, such as [9], consider random wireless channels in UAV swarms, they oversimplify the wireless network to packet dropout channels with finite channel state information (CSI) realizations. Extending these models to accommodate more general wireless fading channels remains a significant challenge. Secondly, many existing works on controller design for UAV swarms overlooked the power costs at UAVs, focusing primarily on tracking stability performance. To address power efficiency, recent studies have integrated communication and tracking controller design for UAV swarms. For instance, in [10], a periodic communication and control policy was considered, where UAVs communicate updated tracking control signals at regular intervals. However, such an approach fail to account for the real-time states of UAVs. When the discrepancy between the target and real-time UAV states are significant, a long activation period can lead to tracking instability. Conversely, short activation periods, when the differences are minimal, may result in unnecessary power consumption. Recent advancements in semantic communications, as explored in [11, 12, 13, 14], focus on transmitting crucial application-specific information. In the context of tracking control for UAV swarms, this involves prioritizing control commands that significantly influence tracking stability. For instance, state-dependent semantic control, as proposed in [15, 16, 17, 18], triggers transmissions among UAVs based on significant tracking errors. However, in scenarios involving wireless channels, it is critical to also embrace the channel state in both communication and controller design. Consequently, semantic communication for UAV swarms over complex wireless communication environments requires further investigation. Motivated by these issues, we investigate the decentralized semantic communication and cooperative tracking control for UAV swarms over the wireless MIMO fading channels. The key contributions are summarized as follows. • Semantic Communication for UAV Swarms over MIMO Fading Channels. We introduce an efficient communication strategy by taking aware both the power cost and tracking stability of UAV swarms. The solution has an semantic structure that adapts to the real-time tracking error costs and the local channel conditions. • Semantic Cooperative Tracking Control for UAV Swarms over MIMO Fading Channels. To effectively control the UAV swarm under random wireless network, the tracking control strategy for UAVs must adapt to the UAV state and the instantaneous CSI. Traditional approaches involve solving complex systems of coupled Bellman equations in an augmented state space [19]. Thus, we introduce a low-complexity control algorithm that minimizes Lyapunov drift across the multi-agent UAV swarm under general wireless MIMO fading channels. Our control policy is semantically structured, relying solely on the UAV states and local CSIs in the swarm. • Closed-form Tracking Control Performance Analysis. Analyzing the tracking stability performance of UAV swarms is challenging due to state-dependent communication and control solutions, as well as the time-varying nature of the wireless channels among the UAVs. In this article, we utilize the Lyapunov analysis method to establish a closed-form sufficient condition for tracking stability in UAV swarms operating over MIMO fading channels."
https://arxiv.org/html/2411.06042v1,Personalized Hierarchical Split Federated Learning in Wireless Networks,"Extreme resource constraints make large-scale machine learning (ML) with distributed clients challenging in wireless networks. On the one hand, large-scale ML requires massive information exchange between clients and server(s). On the other hand, these clients have limited battery and computation powers that are often dedicated to operational computations. \Acsfl is emerging as a potential solution to mitigate these challenges, by splitting the ML model into client-side and server-side model blocks, where only the client-side block is trained on the client device. However, practical applications require personalized models that are suitable for the client’s personal task. Motivated by this, we propose a personalized hierarchical split federated learning (PHSFL) algorithm that is specially designed to achieve better personalization performance. More specially, owing to the fact that regardless of the severity of the statistical data distributions across the clients, many of the features have similar attributes, we only train the body part of the federated learning (FL) model while keeping the (randomly initialized) classifier frozen during the training phase. We first perform extensive theoretical analysis to understand the impact of model splitting and hierarchical model aggregations on the global model. Once the global model is trained, we fine-tune each client classifier to obtain the personalized models. Our empirical findings suggest that while the globally trained model with the untrained classifier performs quite similarly to other existing solutions, the fine-tuned models show significantly improved personalized performance.","I Introduction Given the massive number of wireless devices that are packed with onboard computation chips, we are one step closer to a connected world. While these devices perform many onboard computations, they usually have limited computational and storage resources that can be dedicated to training ML models. Conversely, cloud computing for ML models raises severe privacy questions. Among various distributed learning approaches, FL [1] is widely popular as it lets the devices keep their data private. These distributed learning algorithms are not confined to theory anymore; we have seen their practical usage in many real-world applications, such as the Google keyboard (Gboard) [2]. \Ac fl, however, has its own challenges [3], which are mostly the results of diverse system configurations, commonly known as system heterogeneity, and statistical data distributions of the client devices. On top of these common issues, varying wireless network conditions also largely affect the FL training process when the devices are wireless: the model has to be exchanged using the wireless channel between the devices and server(s). While such difficulties are often addressed jointly by optimizing the networking and computational resources (e.g., see [4, 5] and the references therein), traditional FL may not be applied directly in many practical resource-constrained applications if the end devices need to train the entire model [6]. \Ac sl [7] brings a potential solution to the limited-resource constraints problem by dividing the model into two parts: (a) a much smaller front-end part and (b) a bulky back-end part. The front-end part --- also called the client-side model --- is trained on the user device, while the bulky back-end part --- also called the server-side model --- is trained on the server. \Acsl thus can enable training extremely bulky models at the wireless network edge, which typically incurs significant computational and communication overheads in traditional FL (e.g., see [8] and the references therein). For example, as large foundation models [9] --- that can have billions of trainable model parameters --- are becoming a part of our day-to-day life and are also envisioned to be an integral part of wireless networks [10], split learning (SL) can facilitate training these large models at the wireless edge. While SL can be integrated into the FL framework [11], it still has several key challenges, particularly when the clients’ data distributions are highly non-IID (independent and identically distributed). While FL typically seeks a single global model that can be used for all users, the performance reduces drastically under severe non-IID data distributions. This is due to the fact that general FL algorithms, like the widely popular federated averaging (FedAvg) [1], may inherently push the global model toward the local model weights of a client who has more training samples: these samples, however, may not be statistically significant. A such-trained global model underperforms in other clients’ test datasets, raising severe concerns about this ‘‘one model fits all"" approach. To empirically illustrate this, we implemented a simple hierarchical split federated learning (HSFL) algorithm with 100 clients and 4 edge servers that follows the general architecture of [12] and performs 5 local epochs 3 edge rounds and 100 global rounds. The test performances, as shown in Fig. 1, show that the globally trained model yields very different test accuracies in different clients’ test datasets. Figure 1: Globally trained model’s performance on CIFAR10: 65.36\%, 83.93\% and 33.33\% mean, maximum and minimum test accuracy, respectively, across 100 clients, when data samples are distributed following \mathrm{Dir}(\boldsymbol{\alpha}=\mathbf{0.1}) [4] I-A State of the Art Many recent works extended the idea of SL [7] into variants of FL [1] algorithms [11, 13, 12, 14, 15, 16, 17]. Xu et al. proposed a split federated learning (SFL) algorithm leveraging a single server with distributed clients [11]. In particular, the authors assumed that the client-side model is aggregated only after finishing the local rounds, while the server can aggregate the server-side models in each local training round. Liu et al. proposed hybrid SFL, where a part of the clients train their entire models locally, while others collaborate with their serving base station (BS) to train their respective model following SL [13]. Ao et al. proposed SFL assuming the BS selects a subset of the clients to participate in model training [16]. In particular, [16] jointly optimizes client scheduling, power allocation, and cut layer selection to minimize a weighted utility function that strikes a balance between the training time and energy overheads. Khan et al. proposed an HSFL algorithm leveraging hierarchical federated learning (HFL) and SL [12]. In particular, the authors aimed at optimizing the latency and energy cost associated with HSFL. Liao et al. proposed a similar HSFL algorithm that partitioned clients into different clusters [14]. The authors leveraged grouping the clients into appropriate clusters to tame the system heterogeneity. However, statistical data heterogeneity is well known to cause client drifts [18]. Lin et al. proposed a parallel SL algorithm that jointly optimized the cut layer splitting strategy, radio allocation, and power control to minimize per-round training latency [17]. As opposed to SFL, parallel SL did not aggregate client-side models. On the personalized SFL side, Han et al. proposed weighted aggregation of the local and global models during the local model synchronization phase [19]. Similar ideas were also explored in [20], where the authors mixed partial global model parameters with the local model parameters during the local model synchronization. However, [20] did not explore SL. Chen et al. proposed a 3-stage U-shape split learning algorithm [21]. More specifically, the authors divided the model into front, middle and back parts, where the clients retained the front and back parts, while the server had the bulky middle part. Such U-shape architecture incurs additional communication burden compared to [19]. I-B Research Gaps and Our Contributions The existing studies [11, 13, 12, 14, 15, 16, 17] considered SFL, HSFL and parallel SL extensively without addressing the need for personalization. While [19, 21] addressed joint personalization and split FL, these works were based on a traditional single server with distributed clients case. Therefore, weighted aggregation in multi-tier/hierarchical networks may lose personalization ability if the learning rate is not significantly low. Moreover, training the entire model is proven to have poor personalization capability [22]: even though the model was split into client-side and server-side parts, all model blocks on both sides of the models were updated in [19, 21]. Motivated by the above facts, we propose a PHSFL algorithm that integrates HFL and SL. The designed algorithm lets distributed clients train only the body part of the ML model to learn feature representations while keeping the output layer (e.g., the classifier) frozen during the training process inspired by the fact that globally trained model works great for generalization, while performs poorly for personalization. Besides, we perform extensive theoretical analysis to find the theoretical bound of the average global gradient norm of the global loss function. Our simulation results suggest that while the global trained model performs similarly to HSFL in generalization, with the similarly fine-tuned models for both cases, our proposed solution achieves significantly better personalization performance."
https://arxiv.org/html/2411.05933v1,"A Passivity Analysis for Nonlinear Consensus
on Balanced Digraphs","This work deals with the output consensus problem for multiagent systems over balanced digraphs by passivity analysis. As the standard diffusive coupling structure only models the undirected interconnection, we propose a general approach capable of processing directed coupling and performing passivity analysis. To mitigate the complexity arising from the nonlinearity and directed interconnections, we reformulate the output consensus problem as a convergence analysis on a submanifold. We provide passivity analysis and establish a sufficient condition based on passivity for achieving output agreement in multi-agent systems over balanced digraphs. The results are supported by a numerical example.","Multi-agent systems (MASs) have received extensive attention in both industrial practice and theoretical research, ranging from smart grids, distributed sensing and transportation networks to control, robotics and computer science [1, 2, 3, 4]. From a control perspective, one of the most fundamental tasks in this field is the consensus problem [5], the objective of which is to control the dynamics of each agent so that they reach an agreement on some state or trajectory. Consensus analysis requires understanding the fundamental interplay between agent dynamics, information exchange structures, and interaction protocols in networked systems [6, 7]. Diffusively-coupled networks provide a canonical architecture for studying these relationships [8]. Their inherent structure, composed of symmetric and feedback interconnections, makes passivity theory a natural tool for its analysis [9]. Arcak’s seminal work [10] leveraged passivity to characterize network convergence behavior. This approach was later extended into a comprehensive passivity-based cooperative control framework for single-input single-output (SISO) systems [6] and subsequently for multi-input multi-output (MIMO) systems [7]. This framework revealed a profound connection between system trajectories and dual network optimization problems [11]. While the passivity framework has proved very powerful, it relies heavily on the symmetric feedback interconnection of the incidence matrix in the diffusively-coupled networks. This structural requirement limits the framework’s applicability to systems with undirected interconnections. Replacing one of the incidence matrices in the structure (detailed in Section II) enables the representation of directed graph topologies but sacrifices the diffusive coupling property due to the loss of symmetry. Moreover, given passive edge controllers, the feedback path in the loop may not preserve passivity for the entire interconnection. These challenges leave the passivity analysis for MASs on digraphs blank. This motivates the development of a more general approach for analyzing network systems with directed information exchanging topologies, which can leverage the advantages of passivity theory to solve consensus problems. On the other hand, since the diffusive coupling networks and passivity enable the separate analysis of system dynamics and the underlying graphs, we can categorize the consensus problems based on the linearity of the system dynamics and the directionality of graphs. The problem can be classified, in order of increasing complexity, as linear dynamics over undirected graphs (e.g., the standard linear consensus protocol), nonlinear dynamics over undirected graphs (e.g., [6]), linear dynamics over digraphs (e.g., the linear consensus protocol for digraphs), and nonlinear dynamics over digraphs (e.g., [12, 13]). Also, there are two types of consensus behaviors: average consensus and regular consensus. A system achieves (regular) consensus when the states of all agents converge to the same value, while average consensus requires the converged state to equal the mean of the initial conditions. When applying linear consensus protocols, systems over connected undirected graphs achieve average consensus, whereas systems over digraphs containing a rooted out-branching only achieve regular consensus. However, if the considered digraph is also balanced, the system achieves average consensus. This observation suggests that, in the linear case, balanced digraphs may occupy an intermediate position between directed and undirected topologies, motivating our investigation into balanced digraphs in this paper. This paper focuses on the hardest consensus problem within the above taxonomy: nonlinear dynamics over digraphs. The works [12, 13] were related to this topic and developed a passivation approach, but they only considered the case where the controllers are linear static maps and didn’t provide a general analysis method for network systems with directed coupling. Montenbruck et al. [14] regarded the agreement space as a submanifold and developed powerful analytical tools to establish connections between passivity properties and stabilization around a submanifold, yielding explicit controller synthesis methods. However, they only solved the stabilization problem, which cannot guarantee convergence to the submanifold. Moreover, this analysis considered all controllers as a single entity without examining the passivity of each individual agent or controller. Consequently, it did not allow for an in-depth investigation of the interplay among the dynamics of the controllers, the agents, and the underlying digraphs within the MASs. In this paper, we conduct the passivity analysis for MASs interconnected via balanced digraphs and investigate the relationship between passivity and output consensus behavior in the systems. Our contributions are as follows. We begin by discussing the difference between the diffusively coupled network and its variant for digraphs. Our analysis uncovers a potential loss of passivity in the feedback path of the variant structure for general digraphs, even under the fundamental linear consensus protocol. With these insights, we provide a general approach to handling the directed coupling, enabling passivity analysis for the considered network systems. Then, we transform the output agreement problem to examine the convergence to a submanifold. We derive passivity conditions for agents and controllers that guarantee output agreement in network systems governed by balanced digraphs. The remainder of the paper is organized as follows. Section II introduces some preliminaries, including digraphs, diffusively-coupled networks, and tools to analyze the convergence to submanifolds. Section III proposes a general approach for analyzing directed coupling and reformulates the output agreement problem. Section IV provides the passivity analysis for the diffusively-coupled network with balanced digraphs. Numerical examples and concluding remarks are given in Sections V and VI. Notations The notation \mathbb{1}_{n} (\mathbb{0}_{n}) denotes the n-dimensional vector of all ones (zeros), and I_{n} represents the n\times n identity matrix, where the subscript n may be omitted when the dimension is clear from the context. For a set A, its cardinality is denoted by |A|. For a linear transformation T:\ X\to Y, we denote the kernel of T by \ker(T). We denote the orthogonal complement of a subspace U by U^{\perp}, and the orthogonal projection of some x\in\mathbb{R}^{n} onto U by \operatorname{Proj}_{U}(x). For a smoothly embedded submanifold M, the notation d(x,M) denotes the infimal Euclidean distance from all the points in M to x. Fundamental notions from algebraic graph theory are also used in this paper. A directed graph \mathcal{G}=(\mathbb{V},\mathbb{E}) comprises of a finite vertex set \mathbb{V} and an edge set \mathbb{E}\subset\mathbb{V}\times\mathbb{V}. The incidence matrix E\in\mathbb{R}^{|\mathbb{V}|\times|\mathbb{E}|} is defined as follows. [E]_{ik}:=1 if i is the head of edge e_{k}=(i,k), [E]_{ik}:=-1 if i is the tail of edge e_{k} and [E]_{ik}:=0 otherwise. The incidence matrix can be represented as the sum of the out-incidence matrix B_{o} and the in-incidence matrix B_{i} [15], i.e., E=B_{o}+B_{i}. The elements of B_{o} and B_{i} are defined as: [B_{o}]_{ik}:=1 if i is the head of edge e_{k}=(i,k) and [B_{o}]_{ik}:=0 otherwise; [B_{i}]_{ik}:=-1 if i is the tail of edge e_{k} and [B_{o}]_{ik}:=0 otherwise. The graph Laplacian of undirected graphs is defined as L=EE^{\top}. Similarly, for digraphs, we can define the in-Laplacian matrix L_{i}=B_{i}E^{\top} and out-Laplacian matrix L_{o}=B_{o}E^{\top}."
https://arxiv.org/html/2411.05904v1,Autonomous Industrial Control using an Agentic Framework with Large Language Models,"As chemical plants evolve towards full autonomy, the need for effective fault handling and control in dynamic, unpredictable environments becomes increasingly critical. This paper proposes an innovative approach to industrial automation, introducing validation and reprompting architectures utilizing large language model (LLM)-based autonomous control agents. The proposed agentic system—comprising of operator, validator, and reprompter agents—enables autonomous management of control tasks, adapting to unforeseen disturbances without human intervention. By utilizing validation and reprompting architectures, the framework allows agents to recover from errors and continuously improve decision-making in real-time industrial scenarios. We hypothesize that this mechanism will enhance performance and reliability across a variety of LLMs, offering a path toward fully autonomous systems capable of handling unexpected challenges, paving the way for robust, adaptive control in complex industrial environments. To demonstrate the concept’s effectiveness, we created a simple case study involving a temperature control experiment embedded on a microcontroller device, validating the proposed approach.","Chemical plants are moving towards autonomous operations. Especially for routine operations that follow well-defined procedures, autonomous operation is considered technically feasible with currently available technologies (Borghesan et al. (2022)). However, a significant challenge in developing autonomous control systems is the need to account for long-tail events, which are rare, unpredictable occurrences that fall outside of the scope of typical operational scenarios. In industrial contexts, these long-tail events can range from unexpected equipment failures to highly unusual process disturbances. Traditional automation approaches struggle to handle such events, as they rely heavily on predefined rules and algorithms, rendering them overly rigid and poorly adapted to situations that deviate from expected patterns. Solutions leveraging machine learning models have made some progress in handling known unknowns such as known disturbances or possible plant-model mismatch but they tend to fail in handling anomalies. This is primarily because these models are trained on majority-class data, as anomaly data is scarce or available in too few samples. As a result, these solutions struggle to detect and react to anomalies in real-time, particularly in scenarios involving unknown unknowns—unforeseen disturbances that the system was not designed to handle. Currently, human operators play a key role in managing the type of unknown unknowns discussed previously. Leveraging their reasoning abilities and domain knowledge human operators can dynamically assess a situation and adjust their actions based on real-time feedback. The overarching goal of this work is to bridge these reasoning and knowledge use abilities to autonomous systems using generative machine learning models as intelligent control agents. We particularly focus on the use of Large Language Models (LLMs) for this purpose. LLMs, with their extensive knowledge bases and reasoning capabilities, represent a promising avenue for developing intelligent control agents capable of autonomously analyzing incoming data, diagnosing anomalies, and making informed control decisions in a zero-shot manner- making inferences and offering solutions to scenarios they have not explicitly encountered in training (Pantelides et al., 2024). The challenge is transitioning to a fully automated system that can evaluate responses and adjust actions independently. To address this, we propose a reprompting architecture that empowers LLMs to function as autonomous control agents. This architecture enables agents to validate their actions against a digital twin, implementing them in the physical system if they pass validation; if not, the agent is prompted to revise its approach. This iterative process significantly enhances decision-making capabilities and improves system performance in real-time."
