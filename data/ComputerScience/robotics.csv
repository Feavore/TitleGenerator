URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10446v1,VeriGraph: Scene Graphs for Execution Verifiable Robot Planning,"Recent advancements in vision-language models (VLMs) offer potential for robot task planning, but challenges remain due to VLMs’ tendency to generate incorrect action sequences. To address these limitations, we propose VeriGraph, a novel framework that integrates VLMs for robotic planning while verifying action feasibility. VeriGraph employs scene graphs as an intermediate representation, capturing key objects and spatial relationships to improve plan verification and refinement. The system generates a scene graph from input images and uses it to iteratively check and correct action sequences generated by an LLM-based task planner, ensuring constraints are respected and actions are executable. Our approach significantly enhances task completion rates across diverse manipulation scenarios, outperforming baseline methods by 58\% for language-based tasks and 30\% for image-based tasks.","For robots to be able to solve complex manipulation problems in the real world, they need to understand the physical world around them, including object locations and relationships between objects in the scene. Humans intuitively understand spatial relationships between objects in the world and can use this understanding to develop efficient and executable plans to complete tasks. Consider the example of organizing a cluttered room. Humans can quickly understand which objects are out of place based on their understanding of how objects are supposed to relate to each other. For example, it seems intuitive that a book should be on a shelf, not on a cup. Robots struggle to perceive the world around them the way humans do. Additionally, physical constraints in the real world restrict the order in which actions can be executed. For example, if a glass cup is on a book, which is on a desk, the robot must pick up the cup first and place it on the desk before picking up the book. Because of these constraints, robots need to understand the relationships between objects in the scene. If the robot does not understand that the cup is on the book, it might not factor that into its planning and may try to pick up the book first, which could result in the cup falling and breaking. Figure 1: VeriGraph is able to utilize an initial scene image and a reference image which may or may not be from the same setting. Using the two images, our approach generates the corresponding scene graphs. Using a VLM as the planner along with execution-verifiability, we generate and execute a plan using the robot. Recent advances in large language models (LLMs) and vision-language models (VLMs) have opened up new possibilities for robot task planning [1, 2]. These models demonstrate impressive reasoning capabilities and world knowledge. Prior work [3, 4, 5] used LLMs to generate Planning Domain Definition Language (PDDL), which can be used by classical planners to create a task plan. While the results have been promising, PDDL is inherently restrictive and does not generalize well [6, 7]. Other lines of work use VLMs to generate high-level task plans directly using images [8, 9, 10]. While the results are remarkable, relying on raw pixel data for complex manipulation can be suboptimal. Pixel-level information is often noisy and may contain extraneous details irrelevant to the high-level planning task. To address the scene representation issue, we propose VeriGraph, a novel approach that utilizes scene graphs as an intermediate representation for robot task planning. Scene graphs have proven particularly valuable in robotic task planning [11, 12, 13, 14]. Their structured nature enables the abstraction of object-level details into symbolic graphs, making them robust to noise while retaining essential information about object interactions. For example, a scene graph might encode that a ""cup is on the table"" or a ""spoon is inside the cup,"" providing a framework for reasoning about actions such as moving the spoon or rearranging the objects in the scene. This abstraction is especially important for tasks where the physical appearance of individual objects is not important. One such task is using a reference scene to arrange objects in another scene to look like the reference scene. Because of the scene graph representation, VeriGraph can solve this task significantly better than methods that rely on raw pixel data. Despite their powerful nature, VLMs are prone to failures in planning, often requiring multiple iterations of prompting to achieve the correct result [15]. Approaches such as [10] attempt to solve this problem by inserting a human directly into the loop. However, this is time-consuming and requires constant human supervision. To address the plan verification and correcting problem we add an iterative planning component to VeriGraph. The structured nature of scene graphs allows VeriGraph to represent each action in the plan as graph operations. For example, moving an object from one location to another can be represented with an edge manipulation operation. This representation allows VeriGraph to quickly check for constraint violations and iterate with the task planner to generate valid action sequences. This setup, shown in Figure 2, allows for more accurate and robust planning. For some tasks, it might be easier to specify the goal state via language instructions, while for some tasks a reference image is sufficient. To be able to support both types of task specification, VeriGraph supports flexible goal specification, allowing manipulation tasks to be defined through either target scene images or natural language instructions. The system can generate goal scene graphs from these inputs, providing a unified planning framework across different task specifications. Note that for reference images, VeriGraph does not require the exact same scene, only contextually similar scene (e.g., refer to Figure 1). This versatility makes VeriGraph applicable to various real-world scenarios where goals may be communicated in different formats. We show that VeriGraph beats existing methods that rely on raw pixels as input to the task planner while being execution-verifiable. Our main contributions are as follows: • We present a modular and fast approach that uses scene graphs to enhance planning with LLMs, improving the understanding of spatial relationships and constraints. • We propose an iterative planning and verification mechanism that uses scene graphs to represent and verify action sequences, enhancing the system’s ability to identify and correct constraint violations without human intervention. • We utilize VLMs to generate goal scene graphs based on a reference image or language instruction to create a unified goal specification method."
https://arxiv.org/html/2411.10336v1,BMP: Bridging the Gap between B-Spline and Movement Primitives,"This work introduces B-spline Movement Primitives (BMPs), a new Movement Primitive (MP) variant that leverages B-splines for motion representation. B-splines are a well-known concept in motion planning due to their ability to generate complex, smooth trajectories with only a few control points while satisfying boundary conditions, i.e., passing through a specified desired position with desired velocity. However, current usages of B-splines tend to ignore the higher-order statistics in trajectory distributions, which limits their usage in imitation learning (IL) and reinforcement learning (RL), where modeling trajectory distribution is essential. In contrast, MPs are commonly used in IL and RL for their capacity to capture trajectory likelihoods and correlations. However, MPs are constrained by their abilities to satisfy boundary conditions and usually need extra terms in learning objectives to satisfy velocity constraints. By reformulating B-splines as MPs, represented through basis functions and weight parameters, BMPs combine the strengths of both approaches, allowing B-splines to capture higher-order statistics while retaining their ability to satisfy boundary conditions. Empirical results in IL and RL demonstrate that BMPs broaden the applicability of B-splines in robot learning and offer greater expressiveness compared to existing MP variants.","Movement Primitives (MPs)[1, 2, 3, 4] are a well-established concept in robot learning, offering a compact parameterization of movements that can be efficiently reused, adapted and combined to synthesize complex behavior. They can be categorized into dynamic system-based MPs and probabilistic-based MPs [4]. Dynamic system-based MPs, model movements as dynamic systems with an emphasis on ensuring global stability. The first of this kind is Dynamic Movement Primitives (DMP) [1]. DMP employs a second-order dynamic system that guarantees trajectory convergence to a goal attractor. While DMPs are effective for point-to-point tasks, they are less suited for more general motion tasks that require passing through several via points. Additionally, DMPs lack probabilistic modeling capabilities due to their dynamic system formulation, limiting their applicability in tasks that require trajectory distribution modeling, such as imitation learning (IL) and reinforcement learning (RL). On the other hand, probabilistic-based MPs model movements directly as trajectory distributions, allowing for flexibility in maintaining and adapting these distributions. A key example is Probabilistic Movement Primitives (ProMPs)[2], which represent trajectories as a linear combination of basis functions with weights. ProMPs use a linear Gaussian model, enabling probabilistic inference to handle complex interactions and adapt trajectories based on context. While ProMPs excel in modeling uncertainty and adapting to new situations, they struggle with satisfying boundary conditions like initial states, particularly in reinforcement learning tasks where robots are reset to different initial states at the beginning of each episode. In the realm of trajectory planning, B-splines are widely applied due to their minimal parameter requirements and local support of basis functions, which benefit optimization processes. In addition, B-splines can satisfy any boundary conditions, i.e., passing through specified positions with desired velocity, making them ideal for planning and replanning smooth trajectories. However, current usages of B-splines focusing on model single trajectory, ignoring the trajectory distributions [5, 6, 7, 8], limiting their usages in IL and RL where trajectories likelihood and temporal correlations are desired [4, 9]. In this paper, we propose BMPs, a novel approach that integrates B-splines as probabilistic-based Movement Primitives. This unified framework is suitable for both imitation learning (IL) and episodic reinforcement learning (ERL) [9, 10, 11, 12], retaining the strengths of MPs—such as probabilistic representation—while also ensuring the satisfaction of boundary conditions. Through three experiments, we demonstrate the effectiveness of BMPs in both IL and ERL settings, showcasing their broader applicability and expressiveness in robot learning."
https://arxiv.org/html/2411.10291v1,Moving Forward: A Review of Autonomous Driving Software and Hardware Systems,"With their potential to significantly reduce traffic accidents, enhance road safety, optimize traffic flow, and decrease congestion, autonomous driving systems are a major focus of research and development in recent years. Beyond these immediate benefits, they offer long-term advantages in promoting sustainable transportation by reducing emissions and fuel consumption. Achieving a high level of autonomy across diverse conditions requires a comprehensive understanding of the environment. This is accomplished by processing data from sensors such as cameras, radars, and LiDARs through a software stack that relies heavily on machine learning algorithms. These ML models demand significant computational resources and involve large-scale data movement, presenting challenges for hardware to execute them efficiently and at high speed.In this survey, we first outline and highlight the key components of self-driving systems, covering input sensors, commonly used datasets, simulation platforms, and the software architecture. We then explore the underlying hardware platforms that support the execution of these software systems.By presenting a comprehensive view of autonomous driving systems and their increasing demands, particularly for higher levels of autonomy, we analyze the performance and efficiency of scaled-up off-the-shelf GPU/CPU-based systems, emphasizing the challenges within the computational components. Through examples showcasing the diverse computational and memory requirements in the software stack, we demonstrate how more specialized hardware and processing closer to memory can enable more efficient execution with lower latency. Finally, based on current trends and future demands, we conclude by speculating what a future hardware platform for autonomous driving might look like.","Autonomous driving systems have gained significant attention because of their potential to: (1) enhance road safety by reducing traffic accidents caused by human error; (2) improve traffic efficiency and productivity by optimizing traffic flow; and (3) minimize environmental impact by optimizing driving patterns, which in turn lowers fuel consumption and promotes sustainability [1]. The Society of Automotive Engineers (SAE) has developed a taxonomy for the different levels of autonomous driving [2]. There are six levels in this taxonomy, ranging from No Automation (level 0) up to Full Automation (level 5). Currently, available vehicles are certified as Partial or Conditional Automation (level 2 or 3, respectively). However, recent announcements and developments are positioning experimental and near-future products in the High Driving Automation category (level 4). Towards this goal, recent advances in computer vision and deep learning (DL) models, as well as more efficient hardware, have enabled the evolution of autonomous driving systems. These innovations have improved the accuracy and effectiveness of decision-making in complex scenarios, such as urban environments, offering improvements over traditional rule-based systems [3]. Figure 1: Autonomous driving system modules. A high-level simplified view of an autonomous driving system is depicted in Figure 1. This system can be represented as a collection of four basic modules: Input, Output, Software, and Hardware. The Input module consists of different types of sensors (e.g. cameras) that are used to reconstruct the surrounding environment of the vehicle, as well as to provide its current location. The Output module includes actuators, such as steering, accelerator, and brakes, that given a certain decision, are able to influence the vehicle’s movement. The decisions are determined by the Software module. The core of this module is a complex DL model or set of DL models which take as input the information from the sensors, process it according to the trained parameters and produce decisions that are then sent to the actuators. Different autonomous driving DL models have been proposed. Yurtsever et al. [4] showed that DL autonomous driving systems typically can be divided into two categories: (1) generic modular systems and (2) end-to-end systems. initially, generic modular systems aimed to decompose the complex autonomous driving problem into smaller easier to tackle sub-problems, using a different specialized DL model for each of those problems. In contrast, end-to-end systems were proposed as a way to more closely model the way that humans approach the driving problem as a whole with a single complex model. This approach is preferred by most current solutions. The last module is the Hardware which is composed of computer systems (e.g. CPU- or GPU-based system) capable of providing the necessary computational power to execute the above-mentioned DL model within the tight requirements for this application. The combination of all these modules forms a complete solution for autonomous driving. Several such solutions have been proposed in the past. Boss [5] is the solution that won the 2007 DARPA Urban Challenge. IARA [6] is the first Brazilian self-driving car solution. Waymo [7] is a self-driving car solution developed by Google. Other relevant solutions are Apollo [8] from Baidu and the solution offered by Tesla [9]. As mentioned before, the ultimate goal for autonomous driving is to progress from the existing solutions, which are at levels 2 to 4, towards achieving novel solutions for Full Automation at level 5. Several challenges need to be addressed in order to reach this goal. To achieve level 5, there is a need for better environmental information, requiring more input sensors of different types and higher accuracy [10]. This immediately results in more complex and larger DL models capable of handling all this input information. In addition, these models will also become more complex as they are required to handle more corner cases. All of this will require a more powerful and at the same time more energy-efficient hardware platform to satisfy all the needs. The goal of this work is to present the challenges and directions that designers should consider for the development of the hardware accelerators needed to support level 5 autonomous driving. To set he stage for this, we present an extensive review of the state-of-the-art in terms of the most relevant DL models and hardware systems for autonomous driving, as well as emerging technologies for hardware accelerators."
https://arxiv.org/html/2411.10195v1,BEV-ODOM: Reducing Scale Drift in Monocular Visual Odometry with BEV Representation,"Monocular visual odometry (MVO) is vital in autonomous navigation and robotics, providing a cost-effective and flexible motion tracking solution, but the inherent scale ambiguity in monocular setups often leads to cumulative errors over time. In this paper, we present BEV-ODOM, a novel MVO framework leveraging the Bird’s Eye View (BEV) Representation to address scale drift. Unlike existing approaches, BEV-ODOM integrates a depth-based perspective-view (PV) to BEV encoder, a correlation feature extraction neck, and a CNN-MLP-based decoder, enabling it to estimate motion across three degrees of freedom without the need for depth supervision or complex optimization techniques. Our framework reduces scale drift in long-term sequences and achieves accurate motion estimation across various datasets, including NCLT, Oxford, and KITTI. The results indicate that BEV-ODOM outperforms current MVO methods, demonstrating reduced scale drift and higher accuracy.","Monocular visual odometry (MVO) has been of interest for years due to its cost-effectiveness, serving as a notable solution in robotics and autonomous driving. It acts as an affordable and easily deployable supplement to navigation aids like GPS and inertial navigation systems. Despite its advantages, MVO’s widespread adoption is limited by a key challenge: scale ambiguity. Due to the lack of general depth information, monocular systems typically estimate motion on a relative scale. Traditional MVO methods, such as feature-based methods [1, 2], semi-direct methods [3], and direct methods [4], establish their scale during initialization, using it as a global reference. This approach closely links scale estimation with initial motion, making tracking performance highly sensitive to startup movement speed. Moreover, these methods heavily rely on the initial scale setting, resulting in a severe scale drift issue over time. Learning-based MVO methods utilize the powerful fitting capabilities of machine learning to model the prior distributions in training data. [5, 6] use Convolutional Neural Networks (CNNs) to automatically extract features from images and regress poses based on temporal modeling methods. Additionally, methods such as [7, 8, 9, 10, 11, 12, 13] combine the interpretability of traditional methods with the strong data-fitting abilities of deep learning. These methods incorporate deep learning into steps like absolute scale recovery and feature point selection to achieve absolute scale and enhance matching robustness. To achieve high depth estimation accuracy, these methods often introduce depth supervision or optical flow supervision as additional supervision, which brings additional costs. Figure 1: Comparison of MVO approaches: traditional methods lack consistent scaling; learning-based methods require additional supervision. In contrast, our method achieves low scale drift using only pose supervision with BEV representation. In recent years, with the advancement of BEV transformation techniques and the excellent performance of BEV representation in 3D detection and scene segmentation, some methods have begun to utilize BEV representation for visual odometry implementation, such as [14, 15, 16]. The motivation for using BEV representation lies in leveraging the common ground plane assumption in autonomous driving to simplify the six degrees of freedom (6-DoF) odometry estimation problem. However, these methods have not moved beyond the framework of other 3D tasks under BEV representation; they require scene segmentation first and then use the segmentation results to estimate the pose. The use of side-task supervision raises questions about whether the inherent scale properties come from the BEV representation itself or the supervision. Furthermore, these methods lead to high label acquisition costs and do not fully capitalize on the direct information provided by the BEV Representation. To solve this problem, we propose BEV-ODOM, a novel approach to MVO utilizing a BEV representation. Our framework is structured around a depth-based perspective-view to Bird’s-Eye-View encoder, a correlation feature extraction neck to assess the similarity between two BEVs with different shifts, and a decoder that integrates CNNs and Multi-Layer Perceptrons (MLPs) for estimating motion across three degrees of freedom (3-DoF). Different from existing learning-based MVO methods, our approach eschews complex procedures such as bundle adjustment, pose graph optimization, and side-task supervision. Different from other visual odometry methods that rely on BEV representation, our approach does not rely on segmentation results under BEV maps or occupancy maps for pose estimation, nor does it require additional supervision. This simplification not only enhances the efficiency of our method but also avoids the impact of inaccurate segmentation results on MVO and reduces data collection costs. By fully utilizing the consistent scale properties and the precise, detailed feature extraction capabilities of BEV representation, our method demonstrates excellent scale consistency and achieves state-of-the-art (SOTA) performance on challenging datasets under a 3-DoF evaluation. Because the NCLT and Oxford datasets exhibit minimal changes in z-axis translation, pitch, and roll, our method’s performance remains equally excellent under a 6-DoF evaluation. The contributions of our works are as follows: • We propose a novel MVO framework utilizing BEV representation, effectively addressing scale drift and achieving better accuracy. • Our method simplifies the learning-based MVO pipeline from BEV representation, eliminating the need for supervision from side tasks including depth estimation, segmentation and occupancy map generation, improving its efficiency and robustness. • Our method achieves SOTA performance among current MVO methods on challenging datasets."
https://arxiv.org/html/2411.10171v1,"Imagine-2-Drive: 
High-Fidelity World Modeling in CARLA for Autonomous Vehicles","In autonomous driving with image based state space, accurate prediction of future events and modeling diverse behavioral modes are essential for safety and effective decision-making. World model-based Reinforcement Learning (WMRL) approaches offers a promising solution by simulating future states from current state and actions. However, utility of world models is often limited by typical RL policies being limited to deterministic or single gaussian distribution. By failing to capture the full spectrum of possible actions, reduces their adaptability in complex, dynamic environments. In this work, we introduce Imagine-2-Drive, a framework that consists of two components, VISTAPlan, a high-fidelity world model for accurate future prediction and Diffusion Policy Actor (DPA), a diffusion based policy to model multi-modal behaviors for trajectory prediction. We use VISTAPlan to simulate and evaluate trajectories from DPA and use Denoising Diffusion Policy Optimization (DDPO) to train DPA to maximize the cumulative sum of rewards over the trajectories. We analyze the benefits of each component and the framework as a whole in CARLA with standard driving metrics. As a consequence of our twin novelties- VISTAPlan and DPA, we significantly outperform the state of the art (SOTA) world models on standard driving metrics by 15% and 20% on Route Completion and Success Rate respectively.Project website: https://anantagrg.github.io/Imagine-2-Drive.github.io/Keywords: Diffusion Policy, World Model based Reinforcement Learning","I INTRODUCTION Autonomous driving systems must operate safely and effectively in complex, dynamic environments, where accurate prediction of future events and diverse behavioral modeling are critical for informed decision-making. The ability to foresee potential obstacles, navigate uncertain traffic conditions, and make proactive adjustments to driving strategies relies on robust future prediction capabilities. World model-based Reinforcement Learning approaches [pan2022isodreamisolatingleveragingnoncontrollable, li2024think2driveefficientreinforcementlearning] have emerged as a promising solution to this challenge by simulating future states based on current observations and actions. These models enable autonomous vehicles (AVs) to internally “imagine” possible future scenarios, facilitating more efficient exploration and reducing the risks and costs associated with real-world interactions. However, the utility of these world models is often limited by the nature of traditional RL policies. Most RL policies are constrained to deterministic outputs or single gaussian distributions, which fail to capture the full range of possible behaviors. This undermines the adaptability of the world models and their ability to handle the complexity and variability found in driving environments. The significant advantages of world models also highlights the importance of learning an accurate world model. Current WMRL [hafner2020dreamcontrollearningbehaviors, hafner2022masteringataridiscreteworld, hafner2024masteringdiversedomainsworld, pan2022isodreamisolatingleveragingnoncontrollable, deng2021dreamerproreconstructionfreemodelbasedreinforcement] approaches, model the environment dynamics in latent space using a Recurrent Neural Network (RNN) based network. A common limitation of these approaches is their reliance on single-step transition models, where errors accumulate over multi-step planning, causing planned states to drift from the on-policy distribution. Prior works [wang2023drivedreamerrealworlddrivenworldmodels, ding2024diffusionworldmodelfuture, gao2024vistageneralizabledrivingworld] leverage video diffusion [blattmann2023stablevideodiffusionscaling] based approaches to predict the future states in a single pass, with VISTA [gao2024vistageneralizabledrivingworld] being the most versatile and accurate. To overcome these limitations, we present Imagine-2-Drive , a novel framework that incorporates two key innovations: VISTAPlan, a high-fidelity world model designed for precise future prediction, and DPA, a diffusion-based policy actor that models diverse behavioral modes for trajectory planning. VISTAPlan extends upon the VISTA’s prediction capabilities by incorporating additional modules to predict reward and discount factors. These enhancements enable VISTAPlan to function as a comprehensive world model, facilitating more effective planning and decision-making. Similar to [janner2022planningdiffusionflexiblebehavior, saha2023edmpensembleofcostsguideddiffusionmotion, ze20243ddiffusionpolicygeneralizable, jiang2023motiondiffusercontrollablemultiagentmotion, yang2024diffusionesgradientfreeplanningdiffusion], DPA utilizes a diffusion based model to predict the trajectory (a sequence of actions) in a single pass. Being a generative model, it allows DPA to model multiple behavioral modes. By incorporating the diverse behavior patterns inherent to diffusion policies, our framework can explore a broader range of behaviors, thereby improving its overall performance and robustness. DPA is trained with DDPO [black2024trainingdiffusionmodelsreinforcement] using VISTAPlan to simulate and evaluate trajectories to maximize the cumulative reward over the trajectories We validate our framework in the autonomous driving domain in CARLA [dosovitskiy2017carlaopenurbandriving] simulator, demonstrating its superiority over existing methods. A comprehensive evaluation across diverse driving scenarios highlights the contributions of each component, and how they effectively complement one another to enhance the overall performance. To summarize, our key contributions are: 1. We propose VISTAPlan, a high fidelity world model based reinforcement learning framework for autonomous driving, with image as the sole input modality. VISTAPlan extends VISTA by incorporating additional reward and discount factor heads enabling it for effective planning and decision-making. 2. DPA, a novel diffusion based policy actor which can model multiple behavioral modes, thereby enabling it to explore a broader range of behaviors. It is trained using DDPO to maximize the cumulative sum of rewards over trajectories. 3. Thorough analysis of our approach and baselines in complex CARLA driving scenarios over standard driving metrics to understand the effectiveness of our framework and validating each component."
https://arxiv.org/html/2411.10170v1,"Better Safe Than Sorry: Enhancing Arbitration Graphs for
Safe and Robust Autonomous Decision-Making","This paper introduces an extension to the arbitration graph framework designed to enhance the safety and robustness of autonomous systems in complex, dynamic environments. Building on the flexibility and scalability of arbitration graphs, the proposed method incorporates a verification step and structured fallback layers in the decision-making process. This ensures that only verified and safe commands are executed while enabling graceful degradation in the presence of unexpected faults or bugs. The approach is demonstrated using a Pac-Man simulation and further validated in the context of autonomous driving, where it shows significant reductions in accident risk and improvements in overall system safety. The bottom-up design of arbitration graphs allows for an incremental integration of new behavior components. The extension presented in this work enables the integration of experimental or immature behavior components while maintaining system safety by clearly and precisely defining the conditions under which behaviors are considered safe. The proposed method is implemented as a ready to use header-only C++ library, published under the MIT License. Together with the Pac-Man demo, it is available at github.com/KIT-MRT/arbitration_graphs.","I-A Motivation Behavior planning and decision-making are crucial for robots to operate autonomously in dynamic environments, ensuring to achieve their goals while adapting to changes and uncertainties. Key to reliable operation in fields like mobile, industrial, or service robotics is ensuring safety and robustness in these processes. Arbitration graphs, hierarchical behavior models, manage complex decision-making by allowing integration of diverse methods while ensuring scalability, maintainability, and transparency. However, real-world complexities challenge the safety and robustness of such systems. This paper aims to enhance arbitration graph safety and robustness by identifying and handling erroneous or unsafe behavior commands at runtime. Figure 1: The Pac-Man simulation used to demonstrate the presented extension to the arbitration graph framework. Leveraging the framework’s flexibility and scalability, we incorporate a verification step and fallback layers to ensure robust and safe decision-making. I-B State of the art Behavioral decision-making includes both monothematic methods and generic architectures. End-to-end machine learning approaches learn the entire process from sensor data to commands, requiring extensive data and computational power. Due to their highly integrated nature, it is challenging to interpret or influence the resulting behavior directly. Traditional architectures like Finite State Machines (FSMs) allow situational planning but scale poorly with complexity. Behavior-based methods, derived from Brooks’ subsumption architecture, evolved into Behavior Trees (BTs), among others. Popularized by their use in gaming, they are also applied in robotics. These provide a hierarchical decision making structure, offering modularity and responsiveness but becoming cumbersome with extensive conditions. Arbitration graphs, combining subsumption and object-oriented programming, enhance reusability and system clarity through modularity and functional decomposition. Used in robotic soccer [lauerCognitiveConceptsAutonomous2010] and automated driving [orzechowskiDecisionMakingAutomatedVehicles2020], these graphs employ behavior components to interpret situations and plan actions, while arbitrators select the most suitable behaviors. I-C Contributions Verification Logic We extend the arbitration process to ensure that only verified behaviors are executed. Fallback Logic We introduce fallback options for cases where behavior commands fail verification. Application We validate the safety concept in autonomous driving simulations, demonstrating reduced accident risk and improved safety."
https://arxiv.org/html/2411.10164v1,"Evaluating Text-to-Image Diffusion Models 
for Texturing Synthetic Data","Building generic robotic manipulation systems often requires large amounts of real-world data, which can be dificult to collect. Synthetic data generation offers a promising alternative, but limiting the sim-to-real gap requires significant engineering efforts. To reduce this engineering effort, we investigate the use of pretrained text-to-image diffusion models for texturing synthetic images and compare this approach with using random textures, a common domain randomization technique in synthetic data generation. We focus on generating object-centric representations, such as keypoints and segmentation masks, which are important for robotic manipulation and require precise annotations. We evaluate the efficacy of the texturing methods by training models on the synthetic data and measuring their performance on real-world datasets for three object categories: shoes, T-shirts, and mugs. Surprisingly, we find that texturing using a diffusion model performs on par with random textures, despite generating seemingly more realistic images. Our results suggest that, for now, using diffusion models for texturing does not benefit synthetic data generation for robotics.The code, data and trained models are available at https://github.com/tlpss/diffusing-synthetic-data.git.","Building generic robotic manipulation systems often requires learning-based methods to deal with the diversity in environments and objects. The performance of these learned models largely depends on the amount of data available to train them. Real-world (robot) data collection is time-consuming [1, 2, 3]. Therefore, the amount of data is often a bottleneck to the creation of generic robotic manipulation systems. Pretrained foundation models can reduce this need for task-specific data, but they cannot yet cover all use cases [4]. A parallel approach to overcome this data bottleneck is to train the robot system on synthetically generated data instead of real-world data. The main difficulty with synthetic data is to make sure that models trained on this synthetic data transfer well to the real-world, i.e., to limit the sim-to-real performance gap [5]. In practice, this often requires significant amounts of manual engineering for 3D asset generation, scene composition and texturing.[6, 7, 8, 9, 10]. In this work we focus on texturing, which can be summarized as creating the appearance of 3D objects and scenes by specifying the optical properties (color being the most important one) of each part of an object. Recently, researchers have sought to (partially) outsource this work to neural networks, for example, by generating synthetic images using text-to-image diffusion models [11, 12, 13]. In this work, we further investigate the use of text-to-image diffusion models to texture RGB images of a 3D scene and compare this method against using random textures. We also explore various design choices and pipelines for diffusion-based texturing. To generate the synthetic images, we first create a 3D scene and obtain the annotations for that scene. We then texture the scene by either adding random textures to all elements or by using a diffusion model to generate the textures. The process is illustrated in Figure 1. We focus on pixel-level representations that are often used in robotic manipulation: keypoints and segmentation masks. These representations require precise annotations and therefore we first create an explicit 3D scene instead of directly generating images from text prompts using a text-to-image diffusion model: From the 3D scene, we can extract pixel-perfect annotations. In addition, we need to ensure that the diffusion model does not alter the semantics of the scene during texturing as this would invalidate the annotations. For example, the diffusion model cannot alter the shape of the object or change its pose in the image. To accomplish this, we use a Controlnet [14] to condition on both a depth image of the scene and a prompt, as in [12, 15]. We evaluated the efficacy of the data generation methods by measuring the downstream performance of models trained on the data. We generated data for static scenes of 3 different object categories lying on a table: shoes, T-shirts and mugs. The models were evaluated on real-world test datasets using common metrics for each representation: mean average precision (mAP) for segmentation and average keypoint distance (AKD) for keypoint detection. Surprisingly, we found that texturing using a diffusion model performs similarly to using random textures. In a series of additional experiments, we observed that both methods exhibit limited scaling behavior and that using LLM-generated prompts resulted in the best performance for diffusion-based texturing. To summarize, our contributions are as follows: • We are the first to use text-to-image models to generate synthetic data for keypoint detection, an important task for robotic manipulation systems which requires fine-grained annotations. • We extensively compare diffusion-based synthetic data texturing against using random textures and find they perform similarly on both keypoint detection and semantic segmentation tasks. • We provide insight into the use of diffusion models for synthetic data generation by analyzing the scaling behavior of both methods and evaluating a number of design choices. Figure 1: Left: In this work, we compare text-to-image diffusion models against random textures for texturing 3D scenes in a synthetic data generation pipeline. Right: We evaluate the efficacy of the synthetic data on real-world data for both keypoint detection and segmentation."
https://arxiv.org/html/2411.10148v1,Multi-UAV Search and Rescue in Wilderness Using Smart Agent-Based Probability Models,"The application of Multiple Unmanned Aerial Vehicles (Multi-UAV) in Wilderness Search and Rescue (WiSAR) significantly enhances mission success due to their rapid coverage of search areas from high altitudes and their adaptability to complex terrains. This capability is particularly crucial because time is a critical factor in searching for a lost person in the wilderness; as time passes, survival rates decrease and the search area expands. The probability of success in such searches can be further improved if UAVs leverage terrain features to predict the lost person’s position. In this paper, we aim to enhance search missions by proposing a smart agent-based probability model that combines Monte Carlo simulations with an agent strategy list, mimicking the behavior of a lost person in the wildness areas. Furthermore, we develop a distributed Multi-UAV receding horizon search strategy with dynamic partitioning, utilizing the generated probability density model as prior information to prioritize locations where the lost person is most likely to be found. Simulated search experiments across different terrains have been conducted to validate the search efficiency of the proposed methods compared to other benchmark methods.","With the rapid advancement of electronic technologies, including more powerful and affordable onboard computers, improved battery life, advanced sensors, and better communication systems, Unmanned Aerial Vehicles (UAVs) have been widely deployed in various applications such as search and rescue operations [1, 2], agricultural monitoring [3, 4], and parcel delivery [5, 6]. In particular, Multi-UAV systems equipped with advanced sensors and communication equipment have gained significant attention. The cooperative operation with Multi-UAV can substantially improve mission success rates and work efficiency. Among various applications, Wilderness Search and Rescue (WiSAR) is particularly well-suited to the solutions provided by Multi-UAV systems. For instance, in 2019 in Queensland, Search and Rescue (SAR) authorities assisted 1,648 people, utilizing 8,733 police person-hours and over 34,000 volunteer hours in SAR [7]. Similarly, the National Missing and Unidentified Persons System in the USA reports that over 600,000 individuals go missing each year, while 70,000 missing persons are reported annually in the UK [8]. Implementing Multi-UAV systems could significantly reduce both the costs and time associated with these missions compared to using human or canine field teams [9]. In WiSAR operations, locating the target quickly is crucial due to the time-sensitive nature of search and rescue efforts. Research indicates that the survival rate of lost hikers and young children aged 4-6 years drops rapidly after the first 24 hours, owing to injuries, exposure, exhaustion, and dehydration [10]. Long search duration result in a larger area to cover, reducing the likelihood of finding the target. Therefore, minimizing search time not only limits the search area but also maximizes the chances of a successful rescue. Another significant challenge of WiSAR is operating in rugged terrains such as mountains, forests, and streams, which complicate tracking moving targets. Additionally, weather conditions, as well as the physiological and psychological state of the person, can influence their decisions in wilderness [11]. These factors collectively make it difficult to predict such target-specific probability distribution. Researchers have studied historical search and rescue cases to understand lost individuals’ behavior. For instance, [12] provides a comprehensive analysis of over 50,000 SAR incidents, detailing factors like recovery locations, survival rates, elevation changes, and dispersion angles. This data helps estimate where a lost person might be. Additionally, regional case studies, such as those in [7], [13], and [14], offer valuable insights for SAR teams before deployment. Historical datasets have led to various mathematical models for predicting a lost person’s location. The Euclidean distance ring model, introduced in 1998 [15], uses concentric rings at 25%, 50%, and 75% distances to estimate the probability of the target being within each range. However, it overlooks elevation changes and obstacles. In contrast, the Target Iso-probability Curves from [16] incorporate terrain and obstacles, providing a more realistic probabilistic representation. Similarly, a Bayesian approach in [17] updates prior beliefs about the person’s movement based on terrain features using a discretized hexagonal grid map. More recent work [18] models complex behaviors by assigning probabilities to actions like seeking higher ground or resting, aiming to better simulate the target’s movement patterns. Despite these advancements, existing methods still neglect how interactions with specific environmental features, such as streams or barriers, impact the probability distribution, which could offer SAR teams more precise location insights. Multi-UAV search strategies have been extensively researched over the past decades. When dealing with a probability distribution of a lost person over time, the challenge for a Multi-UAV search and rescue team differs from the typical coverage planning problem, which focuses on maximizing area coverage, as discussed in [19] and [20]. Instead, the emphasis is on searching guided by a probability map. While centralized control approaches, supported by a ground control station, have been employed in studies such as [21] and [22], there has been a growing preference for distributed systems due to their fault tolerance, scalability, and adaptability. Notable examples include the distributed real-time search path planning methods using distributed model predictive control introduced in [23] and [24], the multi-target search framework proposed in [25], and the moving target search method discussed in [26]. However, many of these methods are based on discretized cell-based systems, which can be limited in resolution and may miss finer details of probability distributions. In distributed systems, continuous communication is often prioritized to keep all UAVs connected, as seen in studies like [27], [23], and [28], focus on keeping all UAVs connected throughout operations, which can ensure stable communication but may limit movement and reduce search efficiency. An alternative is the recurrent connectivity method [29], where UAVs communicate only when valuable information is found. However, this approach is designed for exploration and data collection without using a probability map. In WiSAR, where search areas expand 82 km^{2} every one hour[30], commonly used LoRa devices in wilderness settings have a communication range typically limited to 10-20 km [31]. Thus, maintaining constant connectivity may restrict UAV mobility and efficiency, necessitating a balance between connectivity and search coverage. In multi-agent search operations, a common strategy is for each UAV to move towards high-probability locations while maintaining a safe distance from other UAVs, as presented in [23]. This approach works effectively when the probability areas are convex. However, in complex, non-convex probability landscapes, UAVs may converge on the same local high-probability areas, leading to insufficient exploration of unexplored regions. Some task area assignment methods have been well-investigated for WiSAR to ensure coverage of whole searching area, such as the layered search and rescue algorithm proposed in [32], which assigns UAVs to each probability layer. Since Iso-probability curves was proposed for predicting a lost person’s position by Macwan et al. [16], many researchers have explored specific search strategies utilizing the features of Iso curves. For example, the methods discussed in [11] and further in [33] extend a 3D Iso-probability curve. Similarly, an angular motion control method proposed in [34] and [35] achieves smooth transitions between curves, also incorporating the concept of a time-related confidence area into the search process. However, traditional task area assignment methods often fall short due to their static nature, where each UAV is assigned a specific zone without the flexibility to adapt as the mission evolves. This rigidity can lead to inefficiencies, particularly in complex environments like WiSAR, where search conditions are highly dynamic. Figure 1: Illustration of a distributed multi-UAV system in WiSAR. UAVs communicate with adjacent UAVs within their communication range. One of the predicted trajectories of the lost person are shown in green line. Figure 2: System architecture of the proposed framework. To address above limitations in existing researches, this paper proposes a novel multi-UAV cooperative search strategy designed for a continuous environment, incorporating a topology-based dynamic task area assignment. Additionally, the search is informed by prior information provided through a dynamic probability density map, which is generated using a smart agent-based model. An illustration of the proposed framework work in wildness can be seen in Fig. 1. The main contributions of this paper are summarized as follows: • We introduce a smart agent-based probability distribution model, developed using the Monte Carlo method, which integrates terrain features and the lost person’s behavior profile. This model simulates decision-making in wilderness settings, such as seeking high-altitude areas for better visibility and following known routes. Additionally, a guideline map is created to ensure that simulated trajectories align with natural paths a person might take, considering obstacles like rivers, streams, and impassable areas. • We establish an optimization-based search strategy that operates in continuous space using a dynamic probability distribution map. A receding horizon mechanism is incorporated into the search process to reduce the likelihood of a single UAV getting stuck in local minima. • A proximity control function is implemented as a soft constraint in the distributed multi-UAV search system. It ensures each UAV maintains an appropriate distance from others, facilitating reliable communication and avoiding collisions in expansive wilderness areas where constant connectivity is challenging. This balance maintains essential communication in the distributed system while minimizing the impact on search coverage. • We propose a dynamic partitioning method inspired by Voronoi diagrams for WiSAR missions. This approach assigns each UAV to a specific search area, adapting dynamically as the mission progresses. The method ensures that UAVs do not converge on the same high probability areas and encourages exploration of unsearched regions to maximize coverage. The organization of this paper is as follows: Section II introduces the smart agent-based probability distribution model, covering the guideline strategy, lost person behavior strategy, and the generation of the dynamic probability map. Section III then presents the formulation of the proposed multi-UAV strategy with dynamic task assignment. Finally, Section IV details the simulation results, comparing them with two other benchmark methods, followed by a discussion and conclusions in Section V."
https://arxiv.org/html/2411.10049v1,"SPLIT: SE(3)-diffusion via Local Geometry-based Score Prediction 
for 3D Scene-to-Pose-Set Matching Problems","To enable versatile robot manipulation, robots must detect task-relevant poses for different purposes from raw scenes. Currently, many perception algorithms are designed for specific purposes, which limits the flexibility of the perception module. We present a general problem formulation called 3D scene-to-pose-set matching, which directly matches the corresponding poses from the scene without relying on task-specific heuristics. To address this, we introduce SPLIT, an SE(3)-diffusion model for generating pose samples from a scene. The model’s efficiency comes from predicting scores based on local geometry with respect to the sample pose. Moreover, leveraging the conditioned generation capability of diffusion models, we demonstrate that SPLIT can generate the multi-purpose poses, required to complete both the mug reorientation and hanging manipulation within a single model.","Detecting task-relevant poses from a raw scene input is a crucial ability for robots to operate autonomously in unstructured environments. For instance, object pose estimation [1, 2] focuses on predicting the canonical pose of the target object. In a similar context, object descriptor detection [3, 4] aims to predict object descriptors that annotate object-specific locations in the scene, such as identifying the handle of a mug. Grasp detection algorithms[5, 6], on the other hand, identify graspable poses, regardless of the object’s shape or category. These pose detection problems have been actively studied for decades in the fields of robotics and computer vision. Traditionally, each perception algorithm has relied on problem-specific heuristics designed for the particular detection problem at hand. However, as robotic manipulation becomes more complex, multiple perception capabilities are often required simultaneously. For instance, even a simple task like object reorientation (shown in Fig. 1) requires the robot to determine both how to grasp the object (i.e., grasp detection) and in which pose the object should be placed (i.e., object descriptor detection). As robots are required to perform more complex tasks, relying on single-purpose perception algorithms may limit their adaptability and flexibility. In this paper, we aim to develop a generalizable framework capable of handling a wide range of pose detection problems without the need for problem-specific adjustments. Figure 1: Multiple pose descriptors and grasp detections are required to execute the mug reorientation and hanging task. The robot must determine a stable pose for upright placement, the handle direction for hanging, and grasp candidates for picking. The figure illustrates the subgoal configurations and corresponding poses needed to solve the task: (a) pick-reorient, (b) pick-hang. To this end, we introduce a new problem called 3D scene-to-pose-set matching that can encompass various pose detection problems in a single formulation. The formulation assumes that specific geometric features in the scene locally determine the corresponding poses around them, which we call spatial locality. For instance, in grasp detection, stable grasp poses are primarily related to the contact geometry. Similarly, detecting an object descriptor for tool use can be determined by the shape of the tool’s operational part. This approach offers a more flexible framework, as it does not rely on prior knowledge of specific problems. Instead, it induces the perception module to learn the relationship between geometric features and the corresponding pose sets. A recent study on SE(3)-diffusion models [7] provided valuable insights for our 3D scene-to-pose-set matching problem by formulating pose detection as a geometry-based pose generation problem. To generate target poses, the geometry and a pose sample are encoded into a feature vector using an MLP, and the model estimates score functions based on the concatenated featureugh iterative score-based updates, called forward diffusion, the pose samples converge to the desired poses. However, we observed that this approach struggles when complex geometry such as scene input is considered. This limitation arises from the model’s lack of spatial reasoning. When geometry and the pose query are compressed into a single latent vector, the model fails to capture local geometric details and cannot leverage inductive biases like SE(3)-equivariance. This issue was also discussed in 3D reconstruction tasks [8, 9]. The goal of this paper is to develop an SE(3)-diffusion model that generates SE(3) poses relevant to a given scene, aiming to address various pose detection tasks using a single framework. We call our model SPLIT: Score Prediction with Local Geometry Information for Rigid Transformation Diffusion. Instead of encoding global geometry and a sample pose, SPLIT encodes the local geometric context with respect to the sample pose. This enhances model efficiency, as 1) predicting poses with a localized view of the geometry is simpler than using the full geometry and 2) the same local geometry encoder can be applied to all SE(3) pose samples in the scene, naturally exploiting symmetry. To summarize, the contributions of this paper are as follows. First, we present a new problem formulation called 3D scene-to-pose-set matching to tackle a broad class of pose detection tasks within a unified framework (Sec. IV-A). We identify the concept of spatial locality as a key insight for solving this problem effectively. Second, as a solution, we propose SPLIT, a novel SE(3)-diffusion model (Sec. IV-B), which utilizes spatial locality in its architecture. Using SPLIT, we demonstrate that the mug reorientation and hanging tasks (Fig. 1), which require solving multiple pose detection problems, can be handled with a single model."
https://arxiv.org/html/2411.10038v1,"Remote Life Support Robot Interface System for Global Task Planning 
and Local Action Expansion Using Foundation Models","Robot systems capable of executing tasks based on language instructions have been actively researched. It is challenging to convey uncertain information that can only be determined on-site with a single language instruction to the robot. In this study, we propose a system that includes ambiguous parts as template variables in language instructions to communicate the information to be collected and the options to be presented to the robot for predictable uncertain events. This study implements prompt generation for each robot action function based on template variables to collect information, and a feedback system for presenting and selecting options based on template variables for user-to-robot communication. The effectiveness of the proposed system was demonstrated through its application to real-life support tasks performed by the robot.","Robot systems for life support have been researched for a long time [1]. In the future, it will be important for people to be able to entrust high-degree-of-freedom life support robots with tasks at remote locations to improve their quality of life. People should have more free time while the robot is performing tasks, and the time people need to intervene in the robot’s operation should be as short as possible. Accurately telling tasks to robots has long been a challenge. However, recent advances in Large Language Models (LLMs) are revolutionizing this process, enabling robots to interpret instructions communicated in natural language and convert them into sequences of action functions. Yet, a system that relies solely on natural language cannot effectively handle tasks that require the robot to query the user based on local information. Fig. 1 shows an example of tasks requiring local information collection and user queries and our proposed solution in this research. The user requests the robot to buy food at a food shop and fetch drinks from a refrigerator. However, it is difficult to convey complete information about the tasks beforehand because the user does not know what food or drinks are available on site. When the robot arrives at the food shop or refrigerator, it needs to collect the data necessary for task continuation and find out what the user wants and where to deliver it. In this research, we propose a novel concept of generating robot task scripts with template variables. This approach not only resolves geometric uncertainties but also presents the user with options based on on-site information, and executes the generated script’s template variables. The user communicates with the robot in natural language through a text chat, including task template variables, and the robot then generates a task script composed of its action functions. The template variables are described in the task script, and the robot sets up recognizers or presents options to the user to expand these variables. In this research, we have used tap operations on smartphones and operations with AR glasses to ensure intuitive user operation. Figure 1: Problem solved in this research (top) and the proposed system (bottom). The user requests the robot to buy food and bring drinks, but the robot does not know what to look for on-site, what the user wants, or where to deliver the items. In this research, we propose using natural language instructions with template variables surrounded by @ to specify the information that needs to be collected and the queries that need to be made when the user instructs the robot. When executing action functions containing template variables, the robot sets up recognizers (Vision Language Model prompts) according to the content of the template variables, presents options to the user’s operation device, and continues task actions by receiving feedback from these options and the operation UI."
https://arxiv.org/html/2411.10016v1,"‘What did the Robot do in my Absence?’ 
Video Foundation Models to Enhance Intermittent Supervision","This paper investigates the application of Video Foundation Models (ViFMs) for generating robot data summaries to enhance intermittent human supervision of robot teams. We propose a novel framework that produces both generic and query-driven summaries of long-duration robot vision data in three modalities: storyboards, short videos, and text. Through a user study involving 30 participants, we evaluate the efficacy of these summary methods in allowing operators to accurately retrieve the observations and actions that occurred while the robot was operating without supervision over an extended duration (\qty40min). Our findings reveal that query-driven summaries significantly improve retrieval accuracy compared to generic summaries or raw data, albeit with increased task duration. Storyboards are found to be the most effective presentation modality, especially for object-related queries. This work represents, to our knowledge, the first zero-shot application of ViFMs for generating multi-modal robot-to-human communication in intermittent supervision contexts, demonstrating both the promise and limitations of these models in human-robot interaction (HRI) scenarios.","I INTRODUCTION Advancements in artificial intelligence (AI) and robotics are leading to increased deployment of robots in challenging environments. While current robots, operating in challenging environments are frequently teleoperated and require close human supervision, there is a drive towards increased robotic autonomy, enabling the robot to work for longer periods, such as for hours or even days, with no supervision. When an autonomous robot encounters exceptions or challenges in real-world deployment, it is common for the robot to request intervention from a human supervisor, shifting from full autonomy to partial autonomy or teleoperation, i.e. intermittent supervision. To facilitate effective intermittent supervision, a robot needs to inform its human operator ‘what has been happening’ while it has been operating autonomously, which can extend over long durations. Current approaches often involve requiring the human operator to remain attentive throughout the deployment [1] or manually examining raw data collected by the robot, such as video recordings from onboard cameras or robot status logs, which is time-consuming, incurs high cognitive load, and requires expert knowledge to interpret [2]. This limits an expert operator’s efficiency in supervising multiple robots and makes it challenging for novice operators to provide intermittent supervision. Therefore, we are motivated to investigate how data collected during autonomous robot operation can be distilled to facilitate efficient human supervision. Recent advancements in Large Language Models (LLMs) for robotics have paved the way for a more natural human-robot interaction. However, most existing research focuses on mapping natural language commands to a robot’s plan and actions [3], or for a robot to generate conversational responses in social interactions [4]. Using LLMs to generate robot-to-human communication to aid human comprehension of robot status and intervention in functional tasks remains largely unexplored, with only a single study [5] exploring the use of a custom-trained LLM for generic and query-driven robot action summarisation. Figure 1: Diagram of the proposed robot summary generation system. The system generates generic or query-driven summaries of long egocentric robot videos in the form of storyboards, short videos, or text to help a user review the robots’ autonomous history. We propose to employ Foundation Models to summarise robot data and improve intermittent supervision in HRI, as shown in Figure 1. In particular, we propose to use pre-trained Video Foundation Models (ViFMs) to summarise long (\qty40min) egocentric robot videos to assist operators to rapidly identify object occurrences and temporal events in a multi-robot search-and-rescue scenarios [1]. There are two main challenges in adopting ViFMs for generating robot summaries for humans: 1. Summaries generated by ViFMs may not capture relevant information for a specific task. Thus, a user may need to explicitly query the model for targeted answers. 2. The generated summary may be inaccurate. Thus, the modality in which the summary is presented may influence a user’s ability to verify the summary and calibrate their trust towards the information provided. To investigate ViFMs for robot summary generation, we develop a framework that provides a user with either generic or query-driven summaries of robot videos in the following output modalities: a ‘storyboard’ of key images (static summarisation), a short summary ‘video’ (dynamic summarisation), and a language-based ‘text’ summary. We evaluate the proposed framework in a user study (n = 30) to compare participants’ task performance and subjective experience using these different summary generation methods. To the best of our knowledge, this is the first work which incorporates ViFMs in a zero-shot setting to generate summary communication in multiple natural modalities from a robot to a human for intermittent supervision. Our work illustrates the benefits and limitations of such models, as well as the transferability of ViFMs trained with general data for a specific HRI task scenario."
https://arxiv.org/html/2411.09975v1,"Express Yourself: Enabling large-scale
public events involving
multi-human-swarm interaction for
social applications with MOSAIX","Robot swarms have the potential to help groups of people with social tasks, given their ability to scale to large numbers of robots and users. Developing multi-human-swarm interaction is therefore crucial to support multiple people interacting with the swarm simultaneously - which is an area that is scarcely researched, unlike single-human, single-robot or single-human, multi-robot interaction. Moreover, most robots are still confined to laboratory settings. In this paper, we present our work with MOSAIX, a swarm of robot Tiles, that facilitated ideation at a science museum. 63 robots were used as a swarm of smart sticky notes, collecting input from the public and aggregating it based on themes, providing an evolving visualization tool that engaged visitors and fostered their participation. Our contribution lies in creating a large-scale (63 robots and 294 attendees) public event, with a completely decentralized swarm system in real-life settings. We also discuss learnings we obtained that might help future researchers create multi-human-swarm interaction with the public.","I INTRODUCTION Humans, for centuries, have developed many ways to express themselves to one another to achieve human-human interaction. Such methods include the spoken word, paintings, poetry, acting, and other media. Yet robots could be used as a novel, exciting medium by which humans can express themselves to and through the robots. In group interactions, robot swarms in particular can be helpful, given the swarm’s ability to scale to many robots and many users as needed. Social group tasks that involve humans expressing themselves, and which robots could help facilitate, can include group decision-making, brainstorming, collective art-making and education - activities where humans present and discuss their ideas, opinions, and thoughts [1, 2]. As such, multi-human-swarm interaction needs to be developed in order to support and facilitate these tasks. While single-human, single-robot interaction in the social robotics field is being extensively studied [3], multi-human, multi-robot interaction, (specifically multi-human-swarm interaction), has been scarcely researched. Moreover, swarm robotics research is still mostly confined to laboratory settings and has not been widely tested in real-life [4]. This includes human-swarm interaction, which has been greatly concerned with single-human-swarm interaction so far in laboratory settings or simulation. Furthermore, controlling swarm systems in general is challenging due to their distributed nature [5]. Yet achieving such control, without compromising their distributed nature, could lead to useful real-world systems that are robust and scalable to many users and robots. To this end, we created MOSAIX [1], a versatile swarm robotic system that can be used in various social applications by groups of people. MOSAIX is made of 4-inch touchscreen-on-wheels robots, called Tiles. Each Tile has proximity sensors and a camera. We previously used MOSAIX with the public to help users express their opinions, create collective art and for education [1]. In this paper, we present our latest work with MOSAIX in collaboration with We The Curious, a science museum in Bristol, UK. The museum sought an innovative way to engage visitors while collecting their insight on how the city could improve residents’ health and well-being. Those insights could serve as ideas for future exhibits for the museum. MOSAIX was a perfect fit, as it can both collect input and visually represent this input to the public, thereby increasing engagement and fostering meaningful participation. We used the Tiles of MOSAIX as smart sticky notes, enabling users to enter their ideas. The robots then autonomously aggregated based on different themes, creating a dynamic and evolving idea pool to engage visitors throughout the day. We successfully deployed 63 Tiles in the museum over 3 days, collecting around 315 ideas from approximately 294 attendees. This application of MOSAIX could be utilized in the future by teams, for example in corporations, to help with the ideation process of brainstorming. Our contribution, lies in developing the first -to our knowledge- completely decentralized, large-scale (63 robots), multi-human-swarm interaction with members of the public in the real world. We further present lessons learned from our accumulated experience from this public event, as well as previous ones presented in [1]. These insights are intended to assist future researchers in developing smooth large-scale public events with swarm robots."
https://arxiv.org/html/2411.09953v1,Brain-inspired Action Generation with Spiking Transformer Diffusion Policy Model,"Spiking Neural Networks (SNNs) has the ability to extract spatio-temporal features due to their spiking sequence. While previous research has primarily foucus on the classification of image and reinforcement learning. In our paper, we put forward novel diffusion policy model based on Spiking Transformer Neural Networks and Denoising Diffusion Probabilistic Model (DDPM): Spiking Transformer Modulate Diffusion Policy Model (STMDP), a new brain-inspired model for generating robot action trajectories. In order to improve the performance of this model, we develop a novel decoder module: Spiking Modulate Decoder (SMD), which replaces the traditional Decoder module within the Transformer architecture. Additionally, we explored the substitution of DDPM with Denoising Diffusion Implicit Models (DDIM) in our framework. We conducted experiments across four robotic manipulation tasks and performed ablation studies on the modulate block. Our model consistently outperforms existing Transformer-based diffusion policy method. Especially in Can task, we achieved an improvement of 8\%. The proposed STMDP method integrates SNNs, dffusion model and Transformer architecture, which offers new perspectives and promising directions for exploration in brain-inspired robotics.","Spiking Neural Networks transmit information through spike sequences, which encode both temporal and spatial information. This grants SNNs spatio-temporal feature extraction capabilities [1]. The most common models include the Integrate-and-Fire (IF) neuron, the Leaky Integrate-and-Fire (LIF) neuron [2], and the Hodgkin-Huxley (HH) neuron [3]. Since SNNs are modeled in a more biologically plausible manner, their training processes differ slightly from those of artificial neural networks (ANNs). Behavior Cloning (BC), a branch of imitation learning, directly learns a policy by mapping states to actions based on expert demonstration trajectories [4]. But, it suffers from compounding errors, resulting in poor generalization and suboptimal performance in practical scenarios. Action Chuncking with Transformers (ACT) [5] employs a Variational Autoencoder (VAE) for imitation learning, generating action trajectories, while diffusion policy [6] uses diffusion models for the same purpose. Due to the current transformer-based spiking neural network models performing poorly in generating action trajectories. We propose the Spiking Transformer Modulate Diffusion Policy Model and the Spiking Modulate Decoder to enhance the accuracy of robotic manipulation tasks. In this work, we build upon the diffusion policy method [6] and integrate it with SNNs to tackle the aforementioned challenges. Diffusion policies offer strong generative capabilities, enabling them to learn diverse patterns from data, while SNNs’ spatio-temporal feature extraction ensures more coherent action trajectories. We propose a novel diffusion policy model based on Spiking Transformer Neural Networks, along with a novel decoder module. Our key contributions are as follows: \bullet We present the Spiking Transformer Modulate Diffusion Policy Model , and its core module of this model: the Spiking Modulate Decoder module. \bullet Additionally, We implemented diffusion policies utilizing a spiking Transformer architecture, and develop the Spiking Diffusion Transformer (SDIT), which is based on Diffusion with Transformer (DIT) framework. \bullet We conducted experiments on four robotic manipulation tasks, demonstrating that our proposed STMDP model significantly enhanced performance. Specifically, in the Can task, our model outperformed the current best Transformer-based Diffusion Policy model by 8%."
https://arxiv.org/html/2411.09942v1,ALPHA-\alpha and Bi-ACT Are All You Need: Importance of Position and Force Information/Control for Imitation Learning of Unimanual and Bimanual Robotic Manipulation with Low-Cost System,"Autonomous manipulation in everyday tasks requires flexible action generation to handle complex, diverse real-world environments, such as objects with varying hardness and softness. Imitation Learning (IL) enables robots to learn complex tasks from expert demonstrations. However, a lot of existing methods rely on position/unilateral control, leaving challenges in tasks that require force information/control, like carefully grasping fragile or varying-hardness objects. As the need for diverse controls increases, there are demand for low-cost bimanual robots that consider various motor inputs. To address these challenges, we introduce Bilateral Control-Based Imitation Learning via Action Chunking with Transformers(Bi-ACT) and”A” ”L”ow-cost ”P”hysical ”Ha”rdware Considering Diverse Motor Control Modes for Research in Everyday Bimanual Robotic Manipulation (ALPHA-\alpha). Bi-ACT leverages bilateral control to utilize both position and force information, enhancing the robot’s adaptability to object characteristics such as hardness, shape, and weight. The concept of ALPHA-\alpha is affordability, ease of use, repairability, ease of assembly, and diverse control modes (position, velocity, torque), allowing researchers/developers to freely build control systems using ALPHA-\alpha. In our experiments, we conducted a detailed analysis of Bi-ACT in unimanual manipulation tasks, confirming its superior performance and adaptability compared to Bi-ACT without force control. Based on these results, we applied Bi-ACT to bimanual manipulation tasks using ALPHA-\alpha. Experimental results demonstrated high success rates in coordinated bimanual operations across multiple tasks, verifying the effectiveness of our approach in complex real-world scenarios. The effectiveness of the Bi-ACT and ALPHA-\alpha can be seen through comprehensive real-world experiments. Video available at: https://mertcookimg.github.io/alpha-biact/","Autonomous manipulation in real-world tasks remains a considerable challenge due to varied environments that necessitate the generation of adaptive motions. Recent advancements in robotic manipulation have been driven by Imitation Learning (IL), which enables robots to learn complex tasks from expert demonstrations[1, 2, 3, 4]. These approaches have been instrumental in enhancing the adaptability and proficiency of robots in handling complex manipulation tasks. Critical to IL’s success is effective data collection[5, 6], often facilitated by teleoperated systems such as virtual reality headsets with hand-tracking mechanisms[7], smartphones[8], keyboard inputs[9], UMI[10], and leader-follower systems[11, 12]. In particular, systems like ALOHA[12, 13, 14] and Mobile ALOHA[15], being low-cost, facilitate a broader participation of researchers and developers in building and experimenting with these systems, thereby significantly advancing the field of research. Furthermore, the availability of an affordable system for data collection is essential for robotics research, such as the development of robotic foundational models. ALOHA[12, 13, 14], and Mobile ALOHA[15] have shown significant advancements in data collection methods by using leader-follower systems that collect robot joint angles along with image data by unilateral control. Unilateral control is a method in which control commands are issued in one direction from the operator to the robot, with no sensory feedback loop to adjust actions based on interaction with the environment. However, the lack of force information and feedback control presents a limitation, making it difficult to grasp object characteristics such as hardness, shape, and weight. Therefore there is growing interest in data collection through bilateral control systems that can process both position and force information/control without the use of force/tactile sensors[1, 16]. In the field of IL, model selection is crucial for robots to accurately understand and replicate complex behaviors. Traditional methods, such as RNN, Long Short-Term Memory (LSTM)[17] and Transformer[18], have been used to process time-series data but suffer from a ’compounding error’ issue, where small mistakes in action prediction lead to larger errors over time[19]. Action Chunking with Transformers (ACT)[12], trained with data collected via ALOHA, has overcome these challenges by predicting multiple future steps, thereby reducing error accumulation. This approach also addresses issues like pauses during demonstrations, which are difficult to model using Markovian single-step policies[20]. Addressing current limitations in IL, Action Chunking with Transformers(ACT) method learned from data on robot joint angles and images, without incorporating force information and control[12, 15]. In contrast, bilateral control-based imitation learning[16] utilizes both position and force information/control, but most methods use LSTM as model[17], limiting complex task performance. In addition, there are methods using Transformer encoder by Kobayashi[21] and innovative approaches like Mamba[22] by Tsuji; however, they are all evaluated using only the robot’s joint information and do not handle both image and joint information like ACT[12, 15]. To bridge these gaps, Buamanee and Kobayashi proposed Bilateral Control-Based Imitation Learning via Action Chunking with Transformers (Bi-ACT)[1]. In Bi-ACT[1], the effectiveness of Bi-ACT was demonstrated through a ”Pick-and-Place” task involving objects of varying hardness, size, shape consistency, and weight distribution using an unimanual robot. However, detailed analysis and validation were limited to simple tasks with an unimanual robot and adaptation to bimanual robots had not been implemented. Therefore, this paper contributes by providing a more in-depth analysis of unimanual tasks with Bi-ACT and applying Bi-ACT to bimanual tasks by proposed ALPHA-\alpha” which is ”A” ”L”ow-cost ”P”hysical ”Ha”rdware Considering Diverse Motor Control Modes for Research in Everyday Bimanual Robotic Manipulation, as shown in Fig. ALPHA-\alpha and Bi-ACT Are All You Need: Importance of Position and Force Information/Control for Imitation Learning of Unimanual and Bimanual Robotic Manipulation with Low-Cost System. Our contributions are as follows: • Detailed Analysis of Bi-ACT in Unimanual Tasks: We conducted an in-depth analysis of Bi-ACT in unimanual manipulation tasks.Experimental results confirmed that Bi-ACT exhibits excellent performance and adaptability when using position and force information/control, especially in manipulating objects of different hardness, size, shape, and weight distribution. • Design and Implementation of ALPHA-\alpha: We developed ALPHA-\alpha, a low-cost physical hardware considering diverse motor control modes for research in everyday bimanual robotic manipulation. ALPHA-\alpha considers various motor control modes such as joint position, velocity, and torque, enabling researchers to create and utilize diverse control systems. • Application to Bimanual Tasks Using ALPHA-\alpha and Bi-ACT: We extended the application of Bi-ACT to bimanual manipulation tasks using the ALPHA-\alpha via bilateral control. The combination demonstrated high success rates in coordinated bimanual operations across multiple tasks, validating the effectiveness of our approach in complex, real-world scenarios. Our research extends from the evaluation of only unimanual arm robots in the paper[1] to provide detailed insight into the performance of Bi-ACT, introducing ALPHA-\alpha and applying Bi-ACT to bimanual tasks and ALPHA-\alpha. These contributions demonstrate the importance of position and force information/control in the field of robotic manipulation and the realization of a wide variety of tasks with inexpensive ALPHA-\alpha. These suggest that more researchers can enter the field of bimanual robotics."
https://arxiv.org/html/2411.09935v1,Whole-Body Impedance Coordinative Control of Wheel-Legged Robot on Uncertain Terrain,"This article propose a whole-body impedance coordinative control framework for a wheel-legged humanoid robot to achieve adaptability on complex terrains while maintaining robot upper body stability. The framework contains a bi-level control strategy. The outer level is a variable damping impedance controller, which optimizes the damping parameters to ensure the stability of the upper body while holding an object. The inner level employs Whole-Body Control (WBC) optimization that integrates real-time terrain estimation based on wheel-foot position and force data. It generates motor torques while accounting for dynamic constraints, joint limits, friction cones, real-time terrain updates, and a model-free friction compensation strategy. The proposed whole-body coordinative control method has been tested on a recently developed quadruped humanoid robot. The results demonstrate that the proposed algorithm effectively controls the robot, maintaining upper body stability to successfully complete a water-carrying task while adapting to varying terrains.","Humanoid robots serve as a crucial platform for research in embodied intelligence, combining dexterous manipulation, agile locomotion, and artificial intelligence, and they will play an increasingly important role in people’s daily lives. Robots needs to adapt to uncertain terrain conditions, including varied ground materials like cobblestone and uphill or downhill slopes, while also maintaining passive stability in upper body tasks. This passive stability allows the robot to remain steady by passively responding to dynamic external forces, crucially enabling it to handle tasks involving delicate balance and object stability. Achieving stability under unpredictable conditions is essential for robot applications in real-world environments, where the terrain and external forces may vary significantly. This adaptability is particularly critical for robots designed to navigate mixed terrain while carrying objects that require careful stability, such as trays with liquids or delicate equipment. At present, most of the bipedal or two wheel humanoid robots need to design complex control algorithms to maintain their own balance, and their load-bearing capacity is limited. Therefore, it is necessary to redefine the robot’s mechanical structure in the human-centered environment. These research directions are not systematic enough at present, so it is necessary to conduct research on these issues. Figure 1: The newly developed wheel-legged robot, X-Man, with its 80 kg mass, introduces significant inertial challenges. Despite this, our work enables it to achieve upper-limb passive stability and demonstrate adaptability across varied uncertain terrains while maintaining balance. In this paper, we present a novel whole-body control framework for our self-developed robot, X-Man. The main contributions are summarized as follows: • The impedance coordinative controller is proposed to achieve passive stability and terrain adaption. • A novel whole-body control of a humanoid robot is proposed, including whole-body dynamics optimization, terrain frame estimation and model-free compensation. • Experiment validation on the X-man, demonstrated the capability of whole body motion control, complex terrain adaptability. The remainder of this paper is organized as follows: Section II reviews related work. Section III details the robot system. The impedance coordinative controller and WBC controller are presented in Section IV, V, respectively. Section VI provides experiments. Section VII concludes the paper."
https://arxiv.org/html/2411.09929v1,"Autonomous Robotic Pepper Harvesting:
Imitation Learning in Unstructured Agricultural Environments","Automating tasks in outdoor agricultural fields poses significant challenges due to environmental variability, unstructured terrain, and diverse crop characteristics. We present a robotic system for autonomous pepper harvesting designed to operate in these unprotected, complex settings. Utilizing a custom handheld shear-gripper, we collected 300 demonstrations to train a visuomotor policy, enabling the system to adapt to varying field conditions and crop diversity. We achieved a success rate of 28.95% with a cycle time of 31.71 seconds, comparable to existing systems tested under more controlled conditions like greenhouses. Our system demonstrates the feasibility and effectiveness of leveraging imitation learning for automated harvesting in unstructured agricultural environments. This work aims to advance scalable, automated robotic solutions for agriculture in natural settings.","In light of rising food security concerns and a rapidly growing global population, the agricultural industry faces unprecedented pressure to enhance productivity and sustainability. Automation, particularly through robotics, has emerged as a critical pathway to meet these demands, helping to address labor shortages, imprecise environment monitoring, and inconsistent crop handling [1]. Traditional farming, characterized by labor-intensive processes and variable efficiency, is being augmented by advanced technologies including robotics and artificial intelligence, offering potential solutions that can increase yield and promote sustainability in food production. Among these advancements, the focus in robotic manipulation has shifted from traditional control algorithms to adaptive, learning-based approaches such as imitation learning. Imitation learning enables control policies to capture delicate behaviors observed in human demonstrations, which is particularly advantageous in agricultural tasks that demand complex physical interactions, such as harvesting and pruning. Traditional control algorithms often struggle to adapt to the natural variability in agricultural products and field conditions, while imitation learning enables robots to facilitate the precise manipulation needed in real world agricultural scenarios, substantially diminishing reliance on explicit programming [2]. (a)(b)(c) Figure 1: (a) Robotic automation in agriculture faces unique domain-specific challenges, including variable plant morphology, unpredictable lighting, and unstructured field conditions. (b) Pepper harvesting demonstrations are collected with a custom handheld shear-gripper device to train a visuomotor policy via imitation learning. (c) The trained policy enables autonomous robotic pepper harvesting in an outdoor field setting. In this work, we address the challenges of deploying imitation learning-based robotic systems in outdoor agricultural environments (Fig. 1(a)), which are characterized by diverse plant morphologies, inconsistent lighting, and unstructured dynamic field conditions. We build upon an adaptable data collection framework [3], employing a customized handheld device outfitted with sensors to capture high-quality data of pepper harvesting demonstrations (Fig. 1(b)). By leveraging this approach, we demonstrate the effectiveness of a diffusion policy [4] trained on 300 real-world harvesting demonstrations. Our contributions are as follows: • The first comprehensive study on robotic pepper harvesting in unprotected outdoor field settings, with a thorough evaluation and discussion of system performance. • The application of an imitation learning approach on agricultural manipulation that generalizes across environmental and crop variability in complex, unstructured field conditions, addressing unique domain-specific challenges. • A publicly available dataset comprising 300 demonstrations of pepper harvesting in natural conditions, contributing valuable real-world data to the field. Through these contributions, our work aims to advance the adaptability and effectiveness of robotic systems in agriculture, highlighting the potential for imitation learning to support sustainable solutions in challenging field environments."
https://arxiv.org/html/2411.09904v1,Self-Supervised Learning of Grasping Arbitrary Objects On-the-Move,"Mobile grasping enhances manipulation efficiency by utilizing robots’ mobility. This study aims to enable a commercial off-the-shelf robot for mobile grasping, requiring precise timing and pose adjustments. Self-supervised learning can develop a generalizable policy to adjust the robot’s velocity and determine grasp position and orientation based on the target object’s shape and pose. Due to mobile grasping’s complexity, action primitivization and step-by-step learning are crucial to avoid data sparsity in learning from trial and error. This study simplifies mobile grasping into two grasp action primitives and a moving action primitive, which can be operated with limited degrees of freedom for the manipulator. This study introduces three fully convolutional neural network (FCN) models to predict static grasp primitive, dynamic grasp primitive, and residual moving velocity error from visual inputs. A two-stage grasp learning approach facilitates seamless FCN model learning. The ablation study demonstrated that the proposed method achieved the highest grasping accuracy and pick-and-place efficiency. Furthermore, randomizing object shapes and environments in the simulation effectively achieved generalizable mobile grasping.","To improve the efficiency of tidying the home environment using a mobile manipulator, grasping on-the-move (mobile grasping) is a promising approach. Fig. 1 illustrates mobile grasping by a manipulator in both simulations (a) and the real world (b). Existing robotic mobile grasping methods for tidy-up tasks require stopping in front of each object to grasp them and are limited applicability to diverse objects [1, 2, 3, 4]. While many studies have successfully generated mobile grasping motions that account for uncertainties caused by the manipulator’s movement [5, 6, 7], further efforts are needed to handle objects of various shapes. Towards a practical application, this study aims to enable a commercial off-the-shelf robot to perform mobile grasping, which requires fine-tuning the timing and pose of the grasp. A key feature of mobile grasping is that both the kinematics and dynamics of the moving hand affect the success rate. Thus, grasp plans for stationary objects cannot be directly applied. It is crucial to minimize movement errors that depend on the moving velocity and to execute grasping at the appropriate time. As it is challenging to manually teach these skills for grasping various shapes on the move, a framework enabling robots to self-acquire these abilities is necessary. Our approach utilizes the self-supervised learning method, which has previously shown high success rates in several manipulation tasks [8, 9]. (a) Training in simulations (b) Execution in the real world Figure 1: Scenes of mobile grasping for different objects in simulations and the real world. Figure 2: Overview of the mobile grasping action generator. The models of the red boxes are first trained for the stationary object grasping action policy. These are then frozen, and the other models of the blue boxes are trained to learn the moving and dynamic grasping action policies. Fig. 2 illustrates the proposed mobile grasping action generator, comprising three modules utilizing fully convolutional networks (FCNs) tailored to specific abilities. The self-supervised learning framework allows models to train through trial and error using robot-generated correct labels. To address the inefficiency of mobile manipulators learning grasping from scratch, we propose a dynamic grasping module leveraging a static grasping module pretrained for stationary objects. This yields a two-stage learning process where static grasping serves as the foundation for acquiring mobile grasping skills. Both grasping modules assume a parallel two-finger gripper approaches the target vertically to pick it up. These modules estimate pixel-wise grasping success probabilities, generalizing to mobile grasping of various shapes. To mitigate grasp errors caused by discrepancies between actual and desired moving velocities, we introduce an FCN model that predicts residual velocity, adjusting commanded velocity to enhance grasping success. Essentially, this study demonstrates the efficacy of the proposed models in predicting dynamic grasp success probability based on static grasp predictions and residual velocity. Each model specializes in its role, facilitating efficient learning. During training, the moving velocity and target object shape are varied randomly to develop a model adaptable to diverse velocities and shapes. The major contributions of this work are twofold: 1. We propose a self-supervised learning framework employing a two-stage grasp learning method to integrate three action primitive-specific FCN models generalized by randomization techniques, achieving object-agnostic mobile grasping ability. 2. The method is widely applicable to practical applications, using only a commercial off-the-shelf manipulator with limited degrees of freedom (DoFs) and RGB-D sensor, which facilitates setup and maintenance. Our mobile grasping system empirically show higher success rate and efficiency through thorough simulations and real-world tests. Our experiments involved mobile grasping at different velocities for target objects of various shapes, demonstrating the effectiveness of the proposed method by comparing grasping success rates, learning performance, and tidy-up task completion times against methods excluding each module, in both simulations and real-world scenarios."
https://arxiv.org/html/2411.09892v1,Deep learning robotics using self-supervised spatial differentiation drive autonomous contact-based semiconductor characterization,"Integrating autonomous contact-based robotic characterization into self-driving laboratories can enhance measurement quality, reliability, and throughput. While deep learning models support robust autonomy, current methods lack pixel-precision positioning and require extensive labeled data. To overcome these challenges, we propose a self-supervised convolutional neural network with a spatially differentiable loss function, incorporating shape priors to refine predictions of optimal robot contact poses for semiconductor characterization. This network improves valid pose generation by 20.0%, relative to existing models. We demonstrate our network’s performance by driving a 4-degree-of-freedom robot to characterize photoconductivity at 3,025 predicted poses across a gradient of perovskite compositions, achieving throughputs over 125 measurements per hour. Spatially mapping photoconductivity onto each drop-casted film reveals regions of inhomogeneity. With this self-supervised deep learning-driven robotic system, we enable high-precision and reliable automation of contact-based characterization techniques at high throughputs, thereby allowing the measurement of previously inaccessible yet important semiconductor properties for self-driving laboratories.","Results Workflow for autonomous robotic control using vision and CNNs Figure 1: Autonomous robotic process workflow for contact-based characterization of semiconductors. a, Synthesized and annealed drop-casted semiconductor films are fed into the autonomous process, consisting of computer vision segmentation of the semiconductor films, prediction of optimal robot poses using a spatially differentiable CNN (SDCNN), distance-minimizing path planning, and then robotic control with subsequent measurement. b, Photoconductivity and surface profilometry (smoothed) use-cases of the robotic contact-based characterization process. c, Average positional and rotational accuracy across 3,500 robot pose predictions with two standard deviation error bars for our proposed SDCNN method, compared against seven other models. Through optimal pose prediction from computer vision inputs, we autonomously drive a 4DOF robot with a four-point probe end effector to characterize the photoconductive properties of semiconductors spatially. Figure 1a illustrates this autonomous control pipeline. Firstly, semiconductor films are drop-cast offline from the autonomous feedback loop using an OpenTrons volumetric pipetter [32]. Then, an on-board camera takes an image of the semiconductors, \Phi_{\textrm{cam}}, which is then rectified from the camera reference frame to the robot reference frame using a series of calibration matrices, K: \Phi_{\mathrm{robot}}=K_{\mathrm{img\rightarrow robot}}\left(K_{\mathrm{cam% \rightarrow img}}\Phi_{\mathrm{cam}}\right)\;, (1) where K_{\mathrm{cam\rightarrow img}} rectifies \Phi_{\textrm{cam}} from the camera to the image reference frame and then K_{\mathrm{img\rightarrow robot}} rectifies \Phi_{\textrm{img}} from the image to the robot reference frame. Once, in the same coordinate frame as the robot, the Fast Segment Anything Model (FastSAM) [33] is used to quickly find the edges of each drop-casted film, creating image segments, I. Next, optimal poses are predicted directly onto the image segments using the proposed SDCNN model, an 8-layer CNN with customized spatially differentiable loss function. The optimality of each pose is determined using I as a prior in image space, which can be back-propagated into the network as a loss due to spatial differentiability. A stochastic Dijkstra’s planner [34] is used to find a travel route that minimizes the total distance to all predicted poses across all drop-casted films. Finally, the robot is controlled to each SDCNN-predicted pose to characterize the properties of the semiconductor at that location in space. The average positional and rotational accuracies of predicted poses by the SDCNN are shown to be higher than predictions coming from CNNs using existing loss functions (Fig. 1c), with improvements between 1.5% and 8.9%. Designing the SDCNN to predict poses with high positional accuracy and rotational uniqueness enables comprehensive spatial mapping of important measured semiconductor properties in the fewest number of poses. The spatial mapping capabilities of this autonomous workflow can be generalized to function with different contact-based end effectors. Figure 1b highlights two characterization use cases: photoconductivity and surface profilometry. Photoconductivity is measured at each predicted pose by taking the difference between illuminated and dark current-voltage curves (Fig. 1b, left). The blue-shaded regions highlight the spread of the photoconductive properties across the area of a drop-casted methylammonium lead iodide (MAPbI3) perovskite film for a set of k=3 poses, predicted by the SDCNN to maximize the unique spatial area measured. These results indicate that photoconductivity is generally uniform across the area of this particular film. Conversely, the thickness of the film varies largely by tens of micrometers, measured using surface profilometry at the same spatially predicted contact poses (Fig. 1b, right). Utilizing different end effectors with the same driving SDCNN model, shows the ability to precisely it becomes possible to spatially resolve important properties of semiconductor materials in an autonomous fashion. Spatial differentiability for optimal robot pose prediction Figure 2: Creating a spatially differentiable loss function for image-based pose prediction. a, Image segment pixels (I_{X},I_{Y}) are passed through a Gaussian filter to maintain the differentiability of the edges. Predicted pose pixels, (\mathrm{Pose}_{X},\mathrm{Pose}_{Y}), from the SDCNN are superimposed onto the differentiable segment pixels, (I^{\prime}_{X},I^{\prime}_{Y}), using another Gaussian filter and sigmoid function, S(\cdot), to perform direct computation and back-propagation in image space. b, SDCNN architecture and differential predicted poses, (\mathrm{Pose}^{\prime}_{X},\mathrm{Pose}^{\prime}_{Y}), composed onto the image segment pixels, (I^{\prime}_{X},I^{\prime}_{Y}). The SDCNN is trained to predict poses onto the regions of the films with high spatial reward (orange region). Spatial differentiation enables our loss function to be differentiable across the pixel domain of an input, in turn, allowing for image-based computations to be back-propagated through the neurons of an SDCNN. Here, we use spatial differentiability to convert unsupervised learning into self-supervised learning by creating a pixel-based loss function that accepts shape priors as inputs to refine a prediction. We aim to have the SDCNN predict a set of k valid robot poses that will maximize the number of pixels making unique contact with the spatial area of each film. The image segments, I, and predicted poses, \{\mathrm{Pose}_{i}\}_{i=1}^{k}, are the shape priors to the loss function. This method is valid for convex shapes in general and does not assume a perfect circle (Fig. S-5). These shapes are first passed through Gaussian filters to ensure they are smooth and differentiable before being passed to the loss function, generating, I^{\prime} and \{\mathrm{Pose}^{\prime}_{i}\}_{i=1}^{k} (Fig. 2a). A predicted pose is a valid contact if all its pixels fall within the non-zero regions of the segment, otherwise, the pose is invalid. Pixels with zero values are considered the background. The SDCNN is trained to predict poses onto the pixel region of I^{\prime} with high spatial reward towards its center (Fig. 2b), tuned by changing the standard deviation of the Gaussian filter, \sigma (Fig. 2a). The spatially differentiable loss function consists of a tunable weighted sum of two optimization objectives: (1) maximization of the intersection of each pose with non-zero segment pixels and (2) maximization of the pose rotational variance to encourage pose uniqueness: \begin{split}\mathrm{loss}\left(I^{\prime},\{\mathrm{Pose}^{\prime}_{i}\}_{i=1% }^{k}\right)=&-\underbrace{\omega_{1}\sum_{i=1}^{k}\left(I^{\prime}_{XY}\circ% \{\mathrm{Pose}^{\prime}_{XY_{i}}\}_{i=1}^{k}\right)}_{\text{Maximize % intersection with segment}}-\underbrace{\omega_{2}\operatorname{Var}\left(\{% \mathrm{Pose}^{\prime}_{\theta_{i}}\}_{i=1}^{k}\right)\vphantom{\sum_{i=1}^{k}% }}_{\text{Maximize angular variance}}\\[4.30554pt] &\qquad\qquad\text{subject to}\quad\mathrm{Pose}^{\prime}_{i}\cap\mathrm{Pose}% ^{\prime}_{j}=\emptyset,\quad\forall\,i\neq j,\end{split} (2) where \omega_{1} and \omega_{2} are weights set to \omega_{1}=\omega_{2} and I^{\prime}_{XY}\circ\mathrm{Pose}^{\prime}_{XY} is the composition of the differentiable segment pixels onto the differentiable pose pixels. \mathrm{Pose}^{\prime}_{XY_{i}} and \mathrm{Pose}^{\prime}_{\theta_{i}} are the XY-pixel coordinates and the yaw-rotation angles, respectively, for i\in\{1,..,k\} unique differentiable poses. The objectives are negated to form a loss and are subject to the constraint of reducing the number of overlapping pixels between any two unique predicted poses. Figure 3: Comparison of performance across different CNN loss functions on the robot pose prediction task. a, Loss minimization procedure in image space using our spatially differentiable CNN (SDCNN). b, Predicted poses onto a subset of 9 experimentally synthesized semiconductor films for k=3 poses per film. Eight models are compared: ours (SDCNN), recent loss functions from literature designed for robust and spatial tasks (Reverse Huber [30], Wing [29], and Barron [31]), and conventional loss functions (mean squared error (MSE), mean absolute error (MAE), Huber, and Poisson negative log-likelihood). N-number of valid contact poses that fully intersect with the drop-casted film are shown in white, and invalid poses are shown in red. The photocurrent curve at each valid pose is measured experimentally using the robotic system. c, Left: poses output by the same stochastic process used for generating the ground truth poses to train the conventional and robust models. Middle: success rate of generating k=3 valid poses for each film in the full set of 35 experimentally synthesized semiconductors across 100 unique trials. Right: computational performance of CNN inference time on the pose prediction task for the full set of films across 100 unique trials, run on an NVIDIA Tesla V100 GPU. Figure 3a illustrates this loss minimization procedure in image space. Before training, the network has a high loss since it has not learned how to use the shape priors, resulting in placing poses randomly. After training, the network learns to place poses in high-reward regions of shape priors. Figure 3b compares the performance on the valid pose prediction task of our SDCNN against 7 other CNN models that use existing loss functions. Corresponding photoconductivity curves are experimentally measured on the perovskite film using the 4DOF robotic system for each valid predicted pose. The CNN models for comparison use either recent loss functions from literature designed for robust and spatial tasks [29, 30, 31] or conventional loss functions. Aside from the loss function, all models have equivalent network architectures and training parameters. However, since the models using existing loss functions are supervised, a labeled set of poses for each input segment is generated using a stochastic process (Fig. 3c, left). The models using existing loss functions tend to cluster predicted poses tightly together, often resulting in close or partial overlaps. In contrast, our approach encourages predictions to be more spread apart, effectively utilizing the full segment prior, while still being within the segment boundaries. Furthermore, in addition to tight clustering, as shown in Fig. 3b, the existing models also have lower positional accuracy (Fig. 1c), resulting in fewer valid poses being successfully generated for this subset of 9 perovskite films, compared to our spatially differentiable model. In Fig. 3c (middle), we expand this analysis from 9 films to 35 films and perform 100 replicate trials of valid pose generation. Across these 100 trials, we demonstrate that our SDCNN model achieves improvements of 20.0% and 16.2% in the median success rate of generating valid poses over robust loss functions and conventional loss functions, respectively. Given these performance improvements, the inference time of our loss function is negligible compared to the hardware response time, increasing by only 2.4 ns, relative to the slowest tested loss function in our evaluation, Reverse Huber (Fig. 3c, right). Hence, utilizing spatial differentiability in a neural network for pose prediction tasks improves the reliability of autonomous robotics by successfully generating valid predictions without sacrificing compute speed, while also reducing the data labeling burden through self-supervision. Autonomous characterization of perovskite photoconductivity Figure 4: Photoconductivity of perovskite semiconductors characterized by an autonomous robotic platform controlled by a spatially differentiable CNN. a, Photoconductance, G_{\mathrm{ph}}, measured at 3,025 unique poses, predicted by the spatially differentiable CNN (SDCNN), across a gradient of drop-casted methylammonium lead bromide (MAPbBr3) to methylammonium lead iodide (MAPbI3) mixed-halide perovskite semiconductor films, MAPb(Br1-xIx)3. b, All measured photocurrent, I_{\mathrm{ph}}, curves with colormap corresponding to G_{\mathrm{ph}} (slope of I_{\mathrm{ph}}). Measured I_{\mathrm{ph}} curves for the following perovskite film compositions: c, MAPb(Br0.94I0.06)3, d, MAPb(Br0.47I0.53)3, and e, MAPb(Br0.03I0.97)3. f, Top: images of synthesized MAPb(Br1-xIx)3 films. Bottom: spatially mapped G_{\mathrm{ph}} with Gaussian interpolation, collected using the autonomous robotic system. g, Perovskite films with non-uniform spatially-resolved G_{\mathrm{ph}}. Semiconductor self-driving laboratories are capable of rapidly synthesizing materials that must be quickly screened for photoactivity to identify compositions yielding functional semiconductors for applications like solar cells [35, 36, 37] and light-emitting diodes (LEDs) [38, 39]. This need motivates the development of our coupled SDCNN and 4DOF contact-based robotic probe to characterize semiconductor photoconductivity autonomously at high-throughputs using an illuminating four-point probe end effector. The measured semiconductors during the campaign are a gradient of drop-casted methylammonium lead bromide (MAPbBr3) to methylammonium lead iodide (MAPbI3) mixed-halide perovskite films, MAPb(Br1-xIx)3. Throughout the duration of a 24-hour campaign, 3,025 unique poses are predicted and measured by the SDCNN-controlled robotic system. Resulting in a characterization throughput of over 125 measurements per hour. At each SDCNN-predicted pose, the photocurrent, I_{\mathrm{ph}}, is measured as a function of voltage, V, by taking the difference between the illuminated and dark current-voltage curves. Then, photoconductance, G_{\mathrm{ph}}, can be characterized at each pose by computing the slope of each photocurrent-voltage curve: G_{\mathrm{ph}}=\frac{I_{\mathrm{ph}}}{V}=\frac{I_{\mathrm{light}}-I_{\mathrm{% dark}}}{V}\;. (3) Figure 4a shows all characterized values of G_{\mathrm{ph}} across the gradient of drop-casted perovskites, measured at 3,025 uniquely predicted contact poses by the SDCNN. The distribution of G_{\mathrm{ph}} along the y-axis for each composition illustrates the spatial variance of photoconductance across the area of each film with median values shown as black diamonds. An increasing trend in G_{\mathrm{ph}} is observed (dashed curve in Fig. 4a) as the composition shifts from MAPbBr3 to MAPbI3 under broad-band white light illumination. This is consistent with the decreasing bandgaps of the corresponding perovskite compositions from 2.3 to 1.6 eV [40], assuming similar thicknesses between samples. Figure 4b displays the full experimentally measured photocurrent-voltage curves, measured at each predicted pose. The spatially characterized I_{\mathrm{ph}} curves are highlighted for three perovskite films within the MAPb(Br1-xIx)3 gradient: bromine-rich (Fig. 4c), mixed (Fig. 4d), and iodine-rich (Fig. 4e). Data collected across several spatially distinct contact points for a given film allows us to generate a spatial map of experimental G_{\mathrm{ph}} values using a simple Gaussian interpolation (Fig. 4f, bottom). Detailed spatial mapping is critical for identifying defects or non-uniformities in material synthesis, which can significantly affect device performance. For example, based on the trend observed in Fig. 4a, we expect iodine-rich films to have higher G_{\mathrm{ph}} values across the area of the film area with only some expected spatial variation. However, in certain instances (Fig. 4g), we observe regions of very low G_{\mathrm{ph}} values, likely induced by early degradation or pinhole defects in the film. Hence, with highly resolved spatial data, which can now be collected in an autonomous and high-throughput fashion for key properties like photoconductance, the root causes of critical defects and non-uniformities can be better understood faster. In turn, the advancement of these autonomous spatial mapping capabilities supports the improvement of material and device quality within the semiconductor development pipeline."
https://arxiv.org/html/2411.09887v1,Preparation of Papers for IEEE Sponsored Conferences & Symposia*,"This electronic document is a ÒliveÓ template. The various components of your paper [title, text, heads, etc.] are already defined on the style sheet, as illustrated by the portions given in this document.","I INTRODUCTION This template provides authors with most of the formatting specifications needed for preparing electronic versions of their papers. All standard paper components have been specified for three reasons: (1) ease of use when formatting individual papers, (2) automatic compliance to electronic requirements that facilitate the concurrent or later production of electronic products, and (3) conformity of style throughout a conference proceedings. Margins, column widths, line spacing, and type styles are built-in; examples of the type styles are provided throughout this document and are identified in italic type, within parentheses, following the example. Some components, such as multi-leveled equations, graphics, and tables are not prescribed, although the various table text styles are provided. The formatter will need to create these components, incorporating the applicable criteria that follow."
https://arxiv.org/html/2411.09870v1,Impact-Aware Control using Time-Invariant Reference Spreading,"With the goal of increasing the speed and efficiency in robotic manipulation, a control approach is presented that aims to utilize intentional simultaneous impacts to its advantage. This approach exploits the concept of the time-invariant reference spreading framework, in which partly-overlapping ante- and post-impact reference vector fields are used. These vector fields are coupled via an impact model in proximity of the expected impact area, minimizing the otherwise large impact-induced velocity errors and control efforts. We show how a nonsmooth physics engine can be used to construct this impact model for complex scenarios, which warrants applicability to a large range of possible impact states without requiring contact stiffness and damping parameters. In addition, a novel interim-impact control mode provides robustness in the execution against the inevitable lack of exact impact simultaneity and the corresponding unreliable velocity error during the time when contact is only partially established. This interim mode uses a position feedback signal that is derived from the ante-impact velocity reference to promote contact completion, and smoothly transitions into the post-impact mode. An experimental validation of time-invariant reference spreading control is presented for the first time through a set of 600 robotic hit-and-push and dual-arm grabbing experiments.","Exploiting impacts in robotics has been a topic of interest for many years, enabling complex robotic locomotion tasks such as running [1, 2] and jumping [3]. Recently, research interest has also started to increase towards robotic impact-aware manipulation tasks such as hitting [4], catching [5] or fast grabbing [6, 7, 8] of objects. One application domain that can benefit from this research is the field of material handling in logistics processes. Fast and reliable automated solutions with a small footprint are sought in this domain, but at the moment only human operators are capable of performing these tasks, which are often injury-prone and not fully ergonomic. Within this context, impact-aware dual-arm manipulation could provide a solution for fast material handling of heavy objects, potentially allowing for human-like swift pick-and-place operations without the need of creating a custom end effector for different types of handled products [9, 10]. Utilization of impacts in robotic manipulation in a human-like fashion can decrease cycle times, but requires addressing multiple challenges. Firstly, it needs to be ensured that hardware does not get damaged due to potentially large peak forces on the robots induced by impacts [7, 11]. But secondly, even when impacts stay within safe limits, the rapid velocity transitions that result from the impact can result in a peak in the error between the desired and actual velocity of the robot under traditional tracking control as a result of the inevitable mismatch between the actual and the predicted impact times [12, 13]. This, in turn, results in undesired peaks in the actuator commands, potentially inducing vibrations, controller destabilization, hardware damage and increased energy consumption. Figure 1: Depiction of the dual-arm robotic setup used for the experimental validation of the control approach presented in this work. This work focuses on the extension of a framework for robotic manipulation under nominally simultaneous impacts [14] suitable for single- and dual-arm manipulation. In a nominally simultaneous impact, contact between the environment and the robot or robots in all relevant contact areas is ideally established simultaneously with nonzero velocity. In the development of the proposed control strategy, impacts are modeled as instantaneous events, following theory of nonsmooth mechanics [15, 16]. It is, however, inevitable that impacting surfaces are not perfectly aligned at the time of impact, or that one robot establishes contact just before the other robot (in dual-arm manipulation), for example, due to uncertainties in the environment. As a result, a series of unplanned intermediate impacts and a corresponding unpredictable series of velocity jumps will typically occur, where it is generally not possible to estimate the contact state. As confirmed by simulations and experiments, this implies that velocity feedback control cannot be reliably used during an impact sequence [17, 18]. The control approach proposed in this work combines the ability to handle simultaneous impacts with a time-invariant framework for generating references, and is called time-invariant reference spreading. In this work, we present for the first time an application of this time-invariant reference spreading framework on a real-world robotic setup, as well as a thorough experimental validation. In the remainder of this section, we will present the related works in more detail, followed by the precise research contribution and an outline of this work. I-A Related works In recent years, a handful of dedicated control methods have been developed to tackle the execution of motions that include impacts while avoiding unwanted spikes in the control inputs. As an example, [12, 19, 20] defines the tracking error through a distance function that includes knowledge on the predicted velocity jump, removing the velocity tracking error peak caused by a mismatch between the planned and actual impact timing. Similarly, [21] performs tracking control of a bouncing ball using an additional mirrored reference to address a possible impact timing mismatch, in this case assuming a fully elastic impact. An explicit transition phase is defined in [22] to perform tracking control during contact transitions with chattering caused by impacts. Alternatively, [18] projects the velocity tracking error onto a subspace that is invariant to the impact event, temporarily removing velocity feedback for the control objectives affected by the impact, but retaining full control over all remaining control objectives. The removal of unwanted input spikes caused by impacts has also been addressed by the framework of reference spreading (RS) [14, 23, 17]. RS, as introduced in [24] and expanded in [25, 26, 27, 28] and references therein for single impact scenarios, is a hybrid tracking control approach that deals with impacts by defining ante- and post-impact references as a function of time that are coupled by an impact map at the nominal impact time, and overlap about this time. It is ensured that the reference corresponds with the actual contact state of the robot by switching the reference based on impact detection, which avoids error peaking and related spikes in the control inputs. In [14], a strategy to deal with simultaneous impacts is proposed, defining an interim-impact mode that uses only feedforward signals as long as contact is only partially established. This framework is extended in [23] by also using position feedback during the interim mode to pursue persistent contact establishment without relying on velocity feedback. While [14] and [23] only included a numerical validation based on simulations, an experimental validation of RS for simultaneous impacts is provided in [17] using a dual-arm robotic setup targeted to perform a quick grabbing maneuver. Furthermore, the approaches in [23, 17] are cast into the quadratic programming (QP) robot control framework [29, 30], similar to other impact-aware QP control approaches like [31, 32]. This allows to include constraints ensuring for example collision avoidance and adherence to joint limits away from impacts, which are essential in real applications. While [14, 23, 18, 22] use traditional time-based references, [33] has introduced a time-invariant version of RS, which has been extended in [34] for dual-arm manipulation, and will be further extended in this work as highlighted in Section I-B. As opposed to time-based tracking, the time-invariant nature of this approach enjoys the positive features of path following [35], maneuver regulation [36] and dynamical systems [37] approaches, preventing the systems to unnecessarily accelerate or decelerate in the presence of disturbances and deviations caused by temporary collision avoidance or conflicting tasks. The approach also suits a large range of initial conditions without requiring replanning of the reference due to the absence of a predefined path that is to be followed as with traditional tracking control. The references in [33, 34] are prescribed by desired ante- and post-impact vector fields, which overlap in position around the surface where the impact is expected to occur, to make sure a reference corresponding to the contact state is also at hand when the impact occurs away from this expected surface. At this surface, the ante- and post-impact velocity fields are coupled by an impact map [16, 15]. In [33, 34], the interim mode control developed in [23] is extended to the case of time-invariant reference vector fields, constructing a position feedback signal through time integration of the ante-impact velocity reference. To promote simultaneity between the impacts for dual-arm manipulation when using with time-invariant references, [34] also introduces a synchronization approach between the two arms, resulting in a near-simultaneous impact regardless of the initial condition of the robots. Recent approaches, like [4] and [6], focusing on hitting and dual-arm grabbing of objects, respectively, also employ such a time-invariant approach in the context of impact-aware manipulation and originate from the well-known framework of dynamical systems-based robot control [37, 38]. These approaches, however, focus on motion generation and not on control challenges such as the removal of impact-induced input peaks, and are thus complementary to the present work. While a numerical validation of time-invariant RS using simulations is provided in [33, 34], no experimental validation has yet been performed. As mentioned before, both time-based and time-invariant RS rely on an impact map to ensure that the velocity jump embedded in the ante- and post-impact references matches that of the real system. This impact map can be determined in different ways. For systems involving only a few degrees of freedom and a few contact points, an analytical impact map can be determined, which forms a closed-form expression to determine the post-impact velocity as a function of the ante-impact configuration and velocity. This method has been used in [24, 14, 27, 28, 23] while developing and validating the RS approach. However, such an approach is not scalable to complex and realistic scenarios that could emerge in real applications. Alternatively, [17] extracts the impact map from physical experiments. This removes the need of computing the impact map, but is also not scalable to situations where impacts are expected to happen with previously unhandled objects or in different configurations, potentially unknown a priori. An approach to extract the impact map from a set of simulations was proposed in [34], using it for a time-invariant RS approach and validating the approach through a different set of simulations. The potential of using a physics engine based on nonsmooth mechanics to compute a rigid impact map has been confirmed in [39] by validating this computed impact map against a set of physical experiments. The validation in [39] showed only a 3.55% average mismatch between the predicted velocity jump and the experimentally identified velocity jump for a set of dual-arm grabbing and hit-and-push motions, which is deemed sufficiently low to be reliably applied within the RS framework. The experimental validation of time-invariant RS presented in this work will therefore use the simulation-based approach developed in [39] to compute the rigid impact map. I-B Contribution The main contribution of this paper is the formulation and experimental validation of a control framework to perform robotic motions that contain nominally simultaneous impacts, such as dual-arm grabbing. Our aim is to remove control input peaks otherwise caused by impact-induced velocity jumps, while accurately tracking the time-invariant velocity vector fields that prescribe the motions. This paper builds on preliminary work on the framework of time-invariant RS in [33, 34] and is a direct extension of [34]. Time-invariant RS uses overlapping ante- and post-impact velocity reference fields that are coupled via the impact dynamics, together with a suitable control structure with an ante-impact, interim and post-impact control mode. This work shows for the first time an experimental validation of time-invariant RS, demonstrating our approach using a robotic dual 7 degree of freedom (DOF) arm setup for a hit-and-push and a dual-arm grabbing use cases. While [33] and [34] did include a numerical validation using simulations on a planar use cases, no experimental validation was performed in these works. We also demonstrate for the first time that we can reliably use an impact map obtained through a nonsmooth physics engine in this time-invariant RS approach to couple ante- and post-impact velocity reference fields, building on the work of [39] that experimentally validated the use of nonsmooth simulations to build an impact map. The approach of [34] did present a similar simulation-based approach using Matlab simulations to evaluate the impact map, which lacked the scalability of simulations using an established physics engine, while the smooth contact model that was used requires knowledge on contact stiffness and damping parameters that are difficult to obtain. Lastly, the control approach from [33, 34] is modified to be effective in a real-life setting, in particular through the definition of a novel interim-impact mode, which is active when contact is only partially established. The interim-impact mode defined in [33, 34] uses knowledge about the moment when the impact sequence is finished, which is not available in an experimental setting. This work therefore defines a novel interim mode that builds on ideas of [33, 34] by generating a position feedback signal based on the velocity field references, but is also effective in an experimental setting. I-C Outline The outline of this paper is as follows. In Section II, we provide equations of motion of the dual-arm robot system used to demonstrate the proposed control approach. Section III presents the approach to formulate the ante- and post-impact reference velocity fields, coupled via an impact map through simulations with a nonsmooth physics engine. Section IV then describes the control framework consisting of the ante-impact, interim, and post-impact modes, used to track these references. In Section V, we show an experimental validation of the proposed framework and a comparison with three baseline approaches for a hit-and-push and a dual-arm grabbing use case, before drawing conclusions in Section VI."
https://arxiv.org/html/2411.09810v1,"Robustness Assessment of Static Structures
for Efficient Object Handling","This work establishes a solution to the problem of assessing the robustness of multi-object assemblies to external forces. Our physically-grounded approach handles arbitrary static structures made from rigid objects of any shape and mass distribution without relying on heuristics or approximations. The result is a method that provides a foundation for autonomous robot decision-making when interacting with objects in frictional contact. Our strategy decouples slipping from toppling, enabling independent assessments of these two phenomena, with a shared robustness representation being key to combining the results into an accurate robustness assessment. Our algorithms can be used by motion planners to produce efficient assembly transportation plans, and by object placement planners to select poses that improve the strength of an assembly. Compared to prior work, our approach is more generally applicable than commonly used heuristics and more efficient than dynamics simulations.","Assessing the capacity of multi-object assemblies to withstand external forces is crucial for enabling robots to make informed decisions in real-world environments, where autonomy hinges on accurate and reliable world models. Mobile robots efficiently traversing cluttered environments [1, 2], robotic manipulators planning grasps [3, 4, 5] or placing objects [6, 7, 8], autonomous construction robots [9, 10], and robots in many other contexts all need to answer the question: What forces can objects withstand before becoming unstable? Answering this question accurately enables a robot to safely interact with its surroundings and make informed decisions when handling and manipulating objects. This work proposes a novel method for evaluating the robustness of rigid object assemblies, that is, their capacity to withstand external forces, as pictured in Fig. 1. Specifically, we address the problem of determining how external forces impact the robustness of an assembly of rigid bodies in frictional contact and static equilibrium. We propose and evaluate algorithms that, given the known but arbitrary poses, inertial parameters, and friction coefficients of bodies in an assembly, assess the assembly’s robustness and determine how applied forces can improve its stability. We believe that our solution to the aforementioned problem can provide a foundation for autonomous decision-making when interacting with objects in frictional contact. To highlight potential applications of our method, we present two key use cases illustrated in Fig. 2: object placement planning and efficient object transportation. In the first application, we show that an existing stable object placement planner [11] can produce stronger placements when using our proposed method. In the second application, we demonstrate how our method can be used to determine the acceleration that a mobile manipulator should not exceed when transporting an assembly of objects. Figure 1: Using our method, a robot can assess the robustness of objects in complex scenes (lighter colors indicates higher robustness), allowing motion planners to avoid areas close to insecure objects (e.g. the wine bottle) and object placement planners to identify stable spots (e.g. the bottom of the book). Figure 2: Two key applications of our method: object placement planning [11] (left) and efficient transportation [1] (right). Due to the interplay of objects in contact, assessing the robustness of an assembly is complex and depends on the shapes, inertial parameters, frictional properties, and poses of all objects. Although heuristic methods can suffice in select applications [12, 13], a more general approach is needed to enable robots to handle diverse objects in a wide variety of scenarios. Previous efforts have resorted to computationally expensive methods, like dynamics simulations, to assess the robustness of assemblies [14, 15, 16]. In contrast, this work proposes algorithms that offer a compromise between heuristic methods and dynamics simulations, producing an accurate solution that can be used on any rigid object assembly. In section II, we highlight prior work on friction modeling, contact force resolution, and assembly stability assessment. The optimization problem used to compute contact forces is detailed in section IV, and our proposed method for assessing the robustness of an assembly is presented in section V. In section VI, we highlight two key applications of our method: robust placement planning and efficient object transportation. Simulation experiments are presented in section VII to demonstrate the effectiveness of the algorithms proposed in this work when combined with the placement planner from [11]. Finally, an outlook on the assumptions, advantages, and limitations of our work is provided in section VIII."
https://arxiv.org/html/2411.10316v1,M3TR: Generalist HD Map Construction with Variable Map Priors,"Autonomous vehicles require road information for their operation, usually in form of HD maps. Since offline maps eventually become outdated or may only be partially available, online HD map construction methods have been proposed to infer map information from live sensor data. A key issue remains how to exploit such partial or outdated map information as a prior. We introduce M3TR (Multi-Masking Map Transformer), a generalist approach for HD map construction both with and without map priors. We address shortcomings in ground truth generation for Argoverse 2 and nuScenes and propose the first realistic scenarios with semantically diverse map priors. Examining various query designs, we use an improved method for integrating prior map elements into a HD map construction model, increasing performance by +4.3 mAP. Finally, we show that training across all prior scenarios yields a single Generalist model, whose performance is on par with previous Expert models that can handle only one specific type of map prior. M3TR thus is the first model capable of leveraging variable map priors, making it suitable for real-world deployment. https://github.com/immel-f/m3tr","In order to drive safely, autonomous vehicles need to understand the geometry and topology of the roads as well as the traffic rules that apply to them. Current systems employ detailed semantic high-definition (HD) maps that provide this rich knowledge, but are primarily created using offline SLAM approaches. However, mapping the entire world in advance and maintaining HD maps to account for changes in the road layout does not scale. Therefore, recent advances in computer vision aim to perceive HD map information with onboard sensors [11, 17, 13, 14, 5, 28]. This task of online vectorized HD map construction uses sensor data, e.g. from cameras or LiDAR sensors, to detect vectorized map elements (lane markings, road borders, etc.) with their semantic meaning. Compared to offline HD planning maps however, the output of online HD map construction models still lacks a large amount of information. Since HD maps only become outdated gradually, in addition to live sensor data, oftentimes some kind of vectorized prior map information is available and still up-to-date. This prior can range from navigation maps on a road level to parts of an HD map or even complete and recent HD maps. Existing work that incorporates prior information falls short for three main reasons: Detection transformer queries are used to provide vectorized priors to the model [22], but the option space for query design is left underexplored. Furthermore, previous approaches focus on specialized models that can only exploit a single type of map prior that is assumed to be known in advance – an unrealistic expectation for real-world deployment. Finally, current approaches lack a task definition and evaluation metric that both uses meaningful prior scenarios and can differentiate map elements with a prior and those that need to be perceived online. To address these points, we propose M3TR, a generalist HD map construction method with variable map priors. Figure 1: Overview of the model architecture of M3TR and the investigated point query encoder designs. For our evaluated task of HD map completion, we mask out instances from the ground truth map \mathcal{M_{\mathrm{GT}}} to create a map prior \mathcal{M_{\mathrm{P}}}. Using \mathcal{M_{\mathrm{P}}}, we try to reconstruct \mathcal{M_{\mathrm{GT}}}. The map prior instances are supplied to the model as queries, influenced by the shown point query encoder and the detection query set design which is further illustrated in Fig. 5. Contributions Our contributions can be summarized as follows: • We combine several recent label improvements and introduce semantically richer labels to move the HD map construction task closer towards real HD planning maps. • We systematically investigate the HD map completion task, i.e. map construction with prior map information. We introduce meaningful prior scenarios and quantify the importance of different prior element categories by evaluating their performance impact on those perceived online. • We use synthetic map masking as efficient augmentation for HD map construction with priors. • We propose a novel query design to incorporate map priors on a point query and query set level that considerably improves detection performance on the Argoverse 2 and nuScenes datasets by up to +4.3 mAP. • We introduce a novel training regime which results in a single model for all map prior scenarios. This Generalist model achieves performance on par with specialized models without needing to know which kind of map information is available."
https://arxiv.org/html/2411.10203v1,Learning Generalizable 3D Manipulation With 10 Demonstrations,"Learning robust and generalizable manipulation skills from demonstrations remains a key challenge in robotics, with broad applications in industrial automation and service robotics. While recent imitation learning methods have achieved impressive results, they often require large amounts of demonstration data and struggle to generalize across different spatial variants. In this work, we present a novel framework that learns manipulation skills from as few as 10 demonstrations, yet still generalizes to spatial variants such as different initial object positions and camera viewpoints. Our framework consists of two key modules: Semantic Guided Perception (SGP), which constructs task-focused, spatially aware 3D point cloud representations from RGB-D inputs; and Spatial Generalized Decision (SGD), an efficient diffusion-based decision-making module that generates actions via denoising. To effectively learn generalization ability from limited data, we introduce a critical spatially equivariant training strategy that captures the spatial knowledge embedded in expert demonstrations. We validate our framework through extensive experiments on both simulation benchmarks and real-world robotic systems. Our method demonstrates a 60–70% improvement in success rates over state-of-the-art approaches on a series of challenging tasks, even with substantial variations in object poses and camera viewpoints. This work shows significant potential for advancing efficient, generalizable manipulation skill learning in real-world applications.111 https://github.com/renyu2016/Generalized-3D-Manipulation","I INTRODUCTION Learning robust and generalizable manipulation skills from demonstrations[1, 2, 3, 4] is a longstanding goal in robotics research, with broad applications in various aspects of human life, such as industrial robot assembly [5, 6] and service robots performing housework [7, 3]. Recently, several imitation learning methods[8, 9, 10] have shown remarkable performance in learning manipulation skills. However, these methods still suffer from the need for large amounts of demonstration data and exhibit limited generalization ability. Recent approaches combining 3D representations with Diffusion Policy [11, 10, 12] have shown potential performance in learning manipulation tasks from limited demonstrations, generalizing to different visual appearances, instance geometries, and camera viewpoints. However, these methods tend to overfit specific training trajectories rather than capturing the spatial relationships[13, 14, 15, 16, 17] needed for generalization. This limitation leads to poor performance in tasks with varying initial object and target positions, or when the starting pose of a manipulated object significantly deviates from the scenes contained in training data, i.e., these methods lack the 3D generalization ability. Figure 1: In a), we report the average success rates of the two methods on a series of challenging tasks. In b), we show the average success rates of the two methods under different camera viewpoint settings. In c), we progressively expand the random initialization region where the manipulated object is located at the start of the tasks. Compared to the state-of-the-art 3D manipulation learning method DP3, our framework demonstrates significant improvements in success rates and generalization ability. To efficiently learn manipulation skills from a few demonstrations while ensuring 3D generalization, we propose a framework that leverages the spatial information in demonstrations to achieve 3D-generalized manipulation skill learning. Our framework consists of two key components: Semantic Guided Perception (SGP) and Spatial Generalized Decesion (SGD). SGP constructs a 3D representation from RGB-D image pairs that is both task-focused and spatially aware. SGD is an efficient decision-making module based on diffusion policy[18, 19, 20, 21, 22]. To enable SGD to generalize across different 3D spatial variations, we introduce a spatially equivariant training strategy that fully explores the spatial knowledge embedded in expert trajectories. To evaluate our framework, we conducted extensive experiments on both simulation benchmarks and real-world hardware system. Using the same task settings as current state-of-the-art methods, our approach achieved a 60-70% improvement in success rates on a series of challenging simulation tasks with only 10 demonstrations. Additionally, we designed comprehensive experiments to demonstrate the strong generalization ability of our framework in handling spatial variations and viewpoint changes. Our contributions are summarized as follows: • We propose a framework that learns 3D generalized manipulation skills with only 10 demonstrations. Our framework generalizes to varying object initial poses and camera viewpoints. • We develop an easy to use yet highly effective training strategy for manipulation policies, enabling the exploration of spatial knowledge embedded in demonstration trajectories. This training strategy is easily integrated with any frameworks that use point cloud as input for decision policy learning. • We validate our framework through extensive simulation and real-world experiments, achieving over a 60% improvement on a series of challenging tasks compared to state-of-the-art methods, demonstrating its strong effectiveness."
https://arxiv.org/html/2411.10176v1,Let people fail! Exploring the influence of explainable virtual and robotic agents in learning-by-doing tasks,"Collaborative decision-making with artificial intelligence (AI) agents presents opportunities and challenges. While human-AI performance often surpasses that of individuals, the impact of such technology on human behavior remains insufficiently understood, primarily when AI agents can provide justifiable explanations for their suggestions. This study compares the effects of classic vs. partner-aware explanations on human behavior and performance during a learning-by-doing task. Three participant groups were involved: one interacting with a computer, another with a humanoid robot, and a third one without assistance. Results indicated that partner-aware explanations influenced participants differently based on the type of artificial agents involved. With the computer, participants enhanced their task completion times. At the same time, those interacting with the humanoid robot were more inclined to follow its suggestions, although they did not reduce their timing. Interestingly, participants autonomously performing the learning-by-doing task demonstrated superior knowledge acquisition than those assisted by explainable AI (XAI). These findings raise profound questions and have significant implications for automated tutoring and human-AI collaboration.","The maturation of artificial intelligence (AI) techniques has facilitated their extensive utilization across various domains. The integration and refinement of explainable AI (XAI) methods have further empowered non-expert users to incorporate AI models into decision-making settings Waldman and Martin (2022). The resultant dynamics of human-AI collaboration have become a focal point of interest for the human-computer interaction (HCI) community and society at large Buçinca et al. (2021). While human-AI collaboration in decision-making has predominantly been addressed in HCI in recent years, with individuals interacting with artificial agents or receiving suggestions and explanations from recommendation systems Malhi et al. (2020); Lai et al. (2021), the study of the human-robot collaboration received the scientific community’s attention since the dawn of the human-robot interaction (HRI) research field. However, recent years have witnessed implementing and testing explainable techniques with robots in collaborative contexts Anjomshoae et al. (2019); Wallkötter et al. (2021). Differently from the HCI context, the HRI one provides richer interaction modalities, offering a more diverse range of opportunities for personalizing XAI and the modality of the explanations delivery Matarese et al. (2021). An emerging challenge addressed by both the HCI and HRI communities is examining how AI technologies influence human behavior in the context of human-AI collaboration Green and Chen (2019). Multidisciplinary efforts have investigated the impact of AI suggestions on human decision-making, exploring implications related to human cognitive biases Bertrand et al. (2022). Moreover, the introduction of XAI techniques has a dual effect, enabling non-expert users to benefit from such powerful technology while also raising concerns about over-reliance on AI models and the reinforcement of negative human heuristics, such as automation bias Vered et al. (2023). This work investigates the impact of interacting with virtual and robotic explainable agents on people’s behavior and performance during a learning-by-doing task Anzai and Simon (1979); Schank et al. (2013). In our experiments, participants had to learn an unknown task with the assistance of an explainable artificial agent, specifically a virtual talking agent and a social humanoid robot. Additionally, a separate group performed the task autonomously without assistance. During the experiments, we employed an assessment task to directly and quantitatively measure the utility of the human-agent explanatory interactions, building on prior work111M. Matarese, F. Rea, K. Rohlfing, A. Sciutti. How informative is your XAI? Assessing the Quality of Explanations through Information Power (under review).. We aimed to compare the effect of different explanation strategies and explainable agents on participants’ behavior, focusing on their final knowledge of the task. The subsequent sections are organized as follows. Section 2 reviews related works, categorizing them into three parts: human-AI collaboration, explanations in human-AI decision-making, and explanations evaluation. Section 3 outlines the methods employed in the user study, presenting the peculiar methodologies and the technology used during the experiments. Section 4 details the results of the user study, showing comparisons between the experimental conditions. These results are extensively discussed in Section 5, with reference to the existing literature. Finally, Section 6 summarizes our work, highlighting its limitations."
https://arxiv.org/html/2411.09971v1,"Explanation for Trajectory Planning using
Multi-modal Large Language Model
for Autonomous Driving","End-to-end style autonomous driving models have been developed recently. These models lack interpretability of decision-making process from perception to control of the ego vehicle, resulting in anxiety for passengers. To alleviate it, it is effective to build a model which outputs captions describing future behaviors of the ego vehicle and their reason. However, the existing approaches generate reasoning text that inadequately reflects the future plans of the ego vehicle, because they train models to output captions using momentary control signals as inputs. In this study, we propose a reasoning model that takes future planning trajectories of the ego vehicle as inputs to solve this limitation with the dataset newly collected.","Figure 1: Pipeline of our proposed method. In order to improve accountability of an ego-vehicle action, trajectory planning information is embedded as a trajectory image and combined to a camera image in a Image-Trajectory Encoder. Research on autonomous driving has been actively conducted since 2010s after the Darpa Grand Challenge [1]. The advancement of deep learning has significantly improved performance of machine learning tasks such as object detection and semantic segmentation, and has contributed to autonomous driving research. A typical architecture of a autonomous driving system is a modular system, which consists of separate components such as localization, perception, prediction, planning, and control [22]. On the other hand, an end-to-end approach[3], which integrates all components and performs from inputs of sensor data to outputs of control signals consistently, has been emerged recently[21]. It has expected to avoid cumulative errors occurred in each component of the modular system, but it is difficult to know the process of decision making in the approach. Passengers feel anxiety when they cannot understand what the ego vehicle recognizes and why it takes the action. Recent researches have tried to add a large language model (LLM) such as GPT-4[11] and generate captions describing behaviors of the ego vehicle and reasons for them in order to solve the above problem[5, 21, 6, 16, 13, 10, 14]. However, the existing methods have a limitation that they can describe only current or past actions. In this paper, we propose a method, in which a visual image and future driving plans (trajectory planning information) are combined and their fused features are used to generate accurate captions of actions which the ego vehicle will take and reasons for them. To this end, we collect and build a new dataset which includes both trajectory planning information and its captions which the existing datasets do not have, because where the ego vehicle should pay attention depends on the trajectory planning information. Let us consider the scenario in which a vehicle ahead of the ego vehicle is stopping. If the ego vehicle wants to stop in front of the vehicle ahead, it will confirm the space between them. Conversely, if the intention of the ego vehicle is to proceed by avoiding the front vehicle to the left and right sides of the road, it should pay attention to the surrounding situations, including the presence of pedestrians and vehicles in adjacent lanes. The main contributions of this paper are summarized as follows: • We present a new approach to the spatial fusion of visual information and trajectory planning information using cross attention. • We demonstrate improvement of generated captions that describe and justify future behaviors of the ego vehicle, using the fused features and a BLIP-2[7] based vision-language model. • We compile and annotate a new dataset consisting of videos, trajectory planning information, and captions for them."
https://arxiv.org/html/2411.09658v1,"Motion Before Action: Diffusing Object Motion 
as Manipulation Condition","Inferring object motion representations from observations enhances the performance of robotic manipulation tasks. This paper introduces a new paradigm for robot imitation learning that generates action sequences by reasoning about object motion from visual observations. We propose MBA, a novel module that employs two cascaded diffusion processes for object motion generation and robot action generation under object motion guidance. MBA first predicts the future pose sequence of the object based on observations, then uses this sequence as a condition to guide robot action generation. Designed as a plug-and-play component, MBA can be flexibly integrated into existing robotic manipulation policies with diffusion action heads. Extensive experiments in both simulated and real-world environments demonstrate that our approach substantially improves the performance of existing policies across a wide range of manipulation tasks.","Physiological research shows that humans process complex object motion information in their environment to support effective action execution [Human]. This motion analysis enables an understanding of object dynamics, which in turn guides human actions such as reaching, grasping, and maneuvering around obstacles. In contrast to the biological approach, most existing robot policies [ACT, octo, DP, RISE, cage, DP3] are predominantly guided by observation, employing feature encoder and adopting generative approaches to predict actions. While effective, it often results in an overreliance on environmental cues, with the model focusing on memorizing observation features rather than reasoning about object motion patterns, as humans do. Consequently, when encountering extensive pose shifts in real-world objects or actions, many policies often struggle to generalize effectively [octo, openvla], which can limit their practical performance. To address these challenges and improve execution capabilities, we aim to equip the robot with human-like reasoning skills by inferring future object motion from observations, and then predicting future actions under the object motion guidance. By achieving these objectives, the robot can derive intrinsic information (such as poses and motions) from the scene, allowing the policy to map observations to actions in a way that aligns more closely with human reasoning, i.e., reasoning about object motion rather than simply memorizing actions [jannerreasoning]. In this work, we introduce Motion Before Action (referred to as MBA), a novel module that equips existing robotic manipulation policies with such reasoning skills. We observe that representing the object motion using 6D pose aligns with the robot’s end-effector pose in the same format, making the actions compatible with this representation. Consequently, they exhibit mathematical consistency. During task execution, at each time step, the robot pose, object pose, and action are in proximity within the same space, with their kinematic relationships following a learnable pattern, demonstrating physical consistency [xu2021end]. Based on these two distributional consistencies, we hypothesize the learnability of object pose representation in diffusion models [DDPM, DDIM]: 1) diffusion models have been shown to effectively reconstruct actions [DP], so they should similarly excel at reconstructing analogous object poses; 2) just as the robot’s pose can serve as a condition [octo] for generating actions via diffusion models, so does the object pose. In this paper, we make the following key contributions: • We propose a novel imitation learning paradigm for robotic manipulation that allows robots to extract object pose sequences from observations and use them to aid in action prediction, thereby enhancing the robustness and kinematic consistency of the policy’s observation-to-action mapping. • We introduce MBA, a flexible auxiliary module that can be easily integrated into existing policies with diffusion heads, rather than functioning as an independent policy. • We conduct comparative experiments with MBA on three 2D and 3D robotic manipulation policies with diffusion action heads [DP, DP3, RISE], demonstrating substantial performance improvements across various tasks. These tasks, comprising 57 tasks from 3 simulation benchmarks and 3 real-world tasks, involve articulated object manipulation, soft and rigid body manipulation, tool use, non-tool use, and diverse action patterns. Results show that MBA consistently enhances the performance of such policies in both simulated and real-world environments."
https://arxiv.org/html/2411.09643v1,"Modular Fault Diagnosis Framework
for Complex Autonomous Driving Systems","Fault diagnosis is crucial for complex autonomous mobile systems, especially for modern-day autonomous driving (AD). Different actors, numerous use cases, and complex heterogeneous components motivate a fault diagnosis of the system and overall system integrity. AD systems are composed of many heterogeneous components, each with different functionality and possibly using a different algorithm (e.g., rule-based vs. AI components). In addition, these components are subject to the vehicle’s driving state and are highly dependent. This paper, therefore, faces this problem by presenting the concept of a modular fault diagnosis framework for AD systems. The concept suggests modular state monitoring and diagnosis elements, together with a state- and dependency-aware aggregation method. Our proposed classification scheme allows for the categorization of the fault diagnosis modules. The concept is implemented on AD shuttle buses and evaluated to demonstrate its capabilities.","I INTRODUCTION AD systems comprise heterogeneous software parts with different objectives, making development, testing, admission, and operation, possibly with over-the-air updates, even more challenging. In all these phases of an AD vehicle lifecycle, insights into the system are vital. Developers must identify software deficiencies, authorities might require an interface to the system’s internals, and operators, regardless of whether they are private users or public transport companies, must intervene quickly on the system’s defects. In addition, their expertise ranges from AD experts to average consumers without technical experience. This demonstrates the need for a versatile interface to the AD system’s internals. Data is transformed and enriched in different consecutive parts of the system. For example, the AD system pre-processes sensor data, extracts relevant features, and, based on this, creates a solving strategy for a specific task. Consequently, the various parts of the system are highly dependent on each other, leading also to error propagation. A fault diagnosis framework must consider these dependencies when providing insights into the system’s internals, e.g. by hiding irrelevant information from the user for clarity. Moreover, such information can lead to fatal decisions. Figure 1: The modular diagnostic framework schematized here contributes to a system-wide diagnosis by considering the diagnostic states of the submodules (e.g. ""3 of 3 Okay"") of each component (green boxes, e.g. controller area network or ""CAN"") and their dependencies (orange arrows). The framework includes the current driving state of the AD vehicle, enabling the exclusion of irrelevant components from the diagnosis at a given time. Furthermore, the number of possible use cases will rise with the future application of AD vehicles. AD is not only present in research anymore but is deployed in public spaces. The driving task itself is increasingly moving into the background. This shift from a sole driving task implies the system is in different states, like driving or waiting at the bus stop. Again, such conditions may influence the system’s ability to build an adequate understanding of its internals. Safety and robustness are still paramount objectives of an AD vehicle. A fault diagnosis that supports these objectives provides an understandable overview of its functionality while considering the system’s dependencies and states (Fig.1). Thus, we propose a modular fault diagnosis system for AD systems, which we describe in the following. After discussing the relevant literature in Sec. II, we briefly introduce the terminology used throughout this paper in Sec. III. Then, we present our concept for a modular fault diagnosis system for AD in Sec. IV. We implemented this concept on our shuttles in Sec. V, on which we also conducted an evaluation (see Sec. VI). Finally, we conclude our work in Sec. VII."
https://arxiv.org/html/2411.09627v1,"One-Shot Manipulation Strategy Learning
by Making Contact Analogies","We present a novel approach, magic (manipulation analogies for generalizable intelligent contacts), for one-shot learning of manipulation strategies with fast and extensive generalization to novel objects. By leveraging a reference action trajectory, magic effectively identifies similar contact points and sequences of actions on novel objects to replicate a demonstrated strategy, such as using different hooks to retrieve distant objects of different shapes and sizes. Our method is based on a two-stage contact-point matching process that combines global shape matching using pretrained neural features with local curvature analysis to ensure precise and physically plausible contact points. We experiment with three tasks including scooping, hanging, and hooking objects. magic demonstrates superior performance over existing methods, achieving significant improvements in runtime speed and generalization to different object categories. Website: https://magic-2024.github.io/.","A hallmark of human intelligence is flexible tool use: humans can quickly acquire new manipulation “strategies” from just a handful of demonstrations and apply these strategies across various scenarios, including generalization to novel objects of unseen categories. For example, as illustrated in Fig. 1, even from a single demonstration of using a hook to reach distant objects or putting hangers on a rod, we can generalize to different object positions, sizes, and diverse categories, such as hangers and mugs. Traditionally, two main approaches have been widely studied to build machines that can flexibly use tools: model-based and analytic approaches which take novel scenarios and goals and use built-in physical models to compute plans [1, 2, 3, 4], and policy learning, which leverages various types of priors (e.g., object-based and part-based models) and pretrained neural features for generalization [5, 6, 7, 8]. However, both approaches have their limitations. Model-based planning generalizes well given accurate object and physical models. However, it is slow and usually does not benefit from learning. Policy learning approaches, on the other hand, are very efficient at performance time but usually exhibit limited generalization to novel objects and scenarios, particularly when the shape of the novel objects differs significantly from objects seen during training, such as generalizing from hangers to mugs. In this paper, we present a novel approach, magic (manipulation analogies for generalizable intelligent contacts), for one-shot manipulation strategy learning. Shown in Fig. 1, given a single reference action trajectory (e.g., using a hook to reach for a distant object) and a novel scenario (e.g., with different tools and different objects), the goal of the algorithm is to generate a sequence of robot actions that apply a “similar” strategy to the test objects specified by users: in this example, having the target object moving along a certain direction for a given distance. magic extends two critical insights into a broad class of manipulation strategies. First, many strategies such as hooking, hanging, hammering, pushing, reaching [9, 10], stacking, pouring [11, 12, 13], and cutting [13] can be characterized by a sequence of contact waypoints (i.e., the order in which contacts between objects and robot bodies are made); second, these contacts are characterized by forceful affordances between object pairs: a specific pair of contact points on two objects would enable the application of forces along certain directions. However, searching for contact points that would enable the specific affordance is generally challenging due to complex constraints on reachability, collision avoidance, and motion stability. Figure 1: We introduce magic (Manipulation Analogies for Generalizable Intelligent Contacts), a pipeline that is capable of learning manipulation strategies from single demonstrations and applying them to novel objects. magic tackles these challenges by combining data-driven and analytic approaches to generate contact waypoints in novel scenarios. In particular, it first extracts the sequence of contacts among objects in the reference trajectory and then proposes (pairs of) contact points that have similar global and local shape properties as the contact points in the reference, which can be used as guidance for motion planning or motion retargeting. Finally, it utilizes a physical simulator to discard trajectories that fail to achieve the goal due to collisions, unstable physical contact, or violations of joint and torque limits. Our key innovation lies in a novel global-to-local matching algorithm to find functional correspondences between the target objects and reference objects. Intuitively, a “good” contact point would satisfy both a global and a local matching property. First, the points on two objects should be on similar parts of the global shape (e.g., in the hook-using example, we need a contact point on the tool that is at the end of a long rod). Second, the hooking contact point should have a matched local curvature with the target object being hooked so that we can execute the actions stably. Therefore, we propose to use a pretrained visual feature-based correspondence matching to resolve the global matching property. This enables us to quickly search over different parts of the objects but the resulting contact point is usually not precise. Next, we use a local curvature-based matching algorithm to find the best contact point within a local region of the previously proposed contact point, which gives us precise and physically plausible (e.g., collision-free and physically stable) contacts. Overall, magic tackles the problem of one-shot manipulation strategy learning by making analogies in contact waypoints. We validate the effectiveness of our approach on three challenging tasks: scooping a ball against a concave arc with a spoon, hanging a mug onto a mug tree, and using tools to hook objects of varying sizes. Compared to global shape-matching algorithms, our framework achieves significant improvements when the reference objects are from different categories than the test objects. Compared to local shape-matching and simulation-based approaches, our framework is orders of magnitude faster — for most test objects, we need to run simulations on fewer than three candidate contact points to find a solution. Finally, compared to pretrained feature-matching-based approaches, our method finds more precise and physically plausible solutions."
https://arxiv.org/html/2411.09623v1,Vision-based Manipulation of Transparent Plastic Bags in Industrial Setups,"This paper addresses the challenges of vision-based manipulation for autonomous cutting and unpacking of transparent plastic bags in industrial setups, aligning with the Industry 4.0 paradigm. Industry 4.0, driven by data, connectivity, analytics, and robotics, promises enhanced accessibility and sustainability throughout the value chain. The integration of autonomous systems, including collaborative robots (cobots), into industrial processes is pivotal for efficiency and safety. The proposed solution employs advanced Machine Learning algorithms, particularly Convolutional Neural Networks (CNNs), to identify transparent plastic bags under varying lighting and background conditions. Tracking algorithms and depth sensing technologies are utilized for 3D spatial awareness during pick and placement. The system addresses challenges in grasping and manipulation, considering optimal points, compliance control with vacuum gripping technology, and real-time automation for safe interaction in dynamic environments. The system’s successful testing and validation in the lab with the FRANKA robot arm, showcases its potential for widespread industrial applications, while demonstrating effectiveness in automating the unpacking and cutting of transparent plastic bags for an 8-stack bulk-loader based on specific requirements and rigorous testing.","I INTRODUCTION Industry 4.0—also called the Fourth Industrial Revolution or 4IR—is the next phase in the digitization of the manufacturing sector, driven by disruptive trends including the rise of data and connectivity, analytics, human-machine interaction, and improvements in robotics [1, 2]. This could make products and services more easily accessible and transmissible for businesses, consumers, and stakeholders all along the value chain [3]. Preliminary data indicate that successfully scaling 4IR technology makes supply chains more efficient and sustainable [4], creates a safer and more productive environment for the employees, reduces occupational accidents and factory waste, and has countless other benefits. Autonomous manipulation of plastic packages in industrial setups typically involves the use of robotic systems and automation technologies [5]. These systems are designed to handle, move, and manipulate plastic packages in a variety of industrial processes, such as packaging, recycling and sorting, food processing, and quality control [6, 7]. Collaborative robots, or cobots, are widely used in various industrial applications, working alongside humans without needing extensive safety barriers, cages, or other restrictive measures [8]. These robots use different sensors to identify their environment, recognise objects and are programmed for better accessibility, flexibility and repeatability. Example cases can be found in the textile industry as described in [9], where the authors proposed a dual arm collaborative system for textile material identification. By imitating human behavior, in this work the robots use actions such as pulling and twisting to identify and learn more about textile properties. In recent years, the recycling and waste management industry has begun to use vision-based robotic systems for the classification and accurate sorting of waste materials [10]. Indicative examples can be found in different recycling industries for the management of construction waste [11, 12], recyclable materials [13, 14] or electronic parts [15, 16]. The vision-based manipulation and autonomous cutting of transparent plastic bags presents a set of intricate challenges and a compelling need for innovative AI solutions [17, 18]. The inherent transparency of the bags poses difficulties in accurate detection due to the reflection and refraction of light, demanding sophisticated computer vision algorithms for reliable identification [19]. The deformable nature of plastic bags adds complexity to the grasping and manipulation process, necessitating advanced robotic control strategies to handle their variability [20]. Additionally, autonomous cutting requires well-considered mechanical design and precise vision-guided tools to discern optimal cutting points while avoiding unintended damage. Ensuring the safety and efficiency of these systems in real-time, dynamic environments further amplifies the challenge. The pressing need for such technologies arises from the increasing demand for automated waste management, recycling, and packaging processes, where vision-based systems can enhance efficiency, reduce human intervention, and contribute to sustainable practices by facilitating the effective processing of transparent plastic bags [21]. In this work, through the use of advanced Machine Learning algorithms, based on Convolutional Neural Networks (CNNs), the system can identify transparent plastic bags within its visual field, taking into account variations in lighting and background. Once the bags are detected, the system utilizes tracking algorithms to follow the pick and placement of the bags, and, integrate depth sensing technologies for 3D spatial awareness. The next steps involve developing algorithms for robotic grasping and manipulation, accounting for the challenges posed by the deformable and transparent nature of plastic bags. This includes considerations for optimal grasping points, compliance control using vacuum gripping technology, and real-time automation and processing to ensure effective and safe interaction with the bags in dynamic environments. The rest of the paper is organized as follows. Section II describes the mechanical design of the proposed system. Section III presents the object detection and manipulation approach based on deep-learning and Section IV presents the autonomous cutting mechanism and the automation process. The testing of the pilot proof-of-concept prototype is presented in Section V. Finally, the last section discusses the obtained results and highlights directions for future work."
https://arxiv.org/html/2411.09603v1,"ARTICLE PUBLISHED IN INTERNATIONAL JOURNAL OF COMPUTER INTEGRATED MANUFACTURING, TAYLOR & FRANCIS,","The polishing of luxury leather shoes is a delicate, labor-intensive process traditionally performed by skilled craftsmen. Footwear companies aim to automate parts of this process to enhance quality, productivity, and operator well-being, but the unique nature of luxury shoe production presents challenges. This paper introduces a solution involving a collaborative robotic cell to assist in shoe polishing. A collaborative robotic manipulator, equipped with a specialized tool and governed by force control, executes the polishing tasks. Key factors such as trajectory design, applied force, polishing speed, and polish amount were analyzed. Polishing trajectories are designed using CAM software and transferred to the robot’s control system. Human operators design the process, supervise the robot, and perform final finishing, ensuring their expertise is integral to achieving quality. Extensive testing on various shoe models showed significant improvements in quality and reliability, leading to successful implementation on an industrial production line.","Nowadays, many manufacturing companies, in order to be more competitive in the global market, need to automate their process to achieve better results on product quality and productivity. Not only large companies, but also small ones are moving toward process automation, due to the need to improve mental and physical well-being of the workers. Human-centred manufacturing is crucial to increase flexibility, agility and competitiveness (Lu et al., 2022). Humans are no longer meant to do tedious and strenuous work, but to add high-value content to final products by exploiting their cognitive abilities. The relationship between humans and machines has radically changed with the transformation of Industry 5.0: machines are required to perceive and adapt to workers’ needs, not vice versa. Production is changing in all companies in different sectors, even in those that have production characteristics totally opposed to an automated process. For example, in footwear luxury industries the processes are typically manually performed by expert artisans due to high quality, precision and comfort requirements of products and to the wide variability and customisation of production. All these features are in contrast to full automation, and only craft work can bring profitability to the company. However, the need for automation is strong to ensure high productivity but also the well-being of the workers. A central process in the production of shoes is polishing. In the luxury sector this process is carried out at present by skilled craftsmen that are able to ensure the best quality of the product and the right shine and color to the leather. The variability of the polishing process is very wide, depending on different factors such as leather color, leather type, shoe model and so on. Unfortunately this process is also one of the most tiring because it requires a long time (about 20-23 minutes per shoe) with tedious and repetitive movements. Automation of this process would reduce the physical effort of the operator, who could simply supervise the operation and control the quality of polishing, improving it if the automated process did not meet the requirements. A collaborative robot can be used for this purpose since it can work alongside and cooperating with the operator. In this type of industry there is a reluctance to completely replace the manual process, because it is the human experience that gives the product the highest level of quality in terms of aesthetic beauty, tactile feel, and comfort. Therefore, automation involving human cooperation and supervision is the only one possible to combine the two requirements of ensuring a high-quality product and high human well-being. In Figure 1 is illustrated the research problem with the context and the methodology applied to fill the gap. Figure 1: Research problem and applied methods. 1.1 Related Works Several studies can be found in literature on the automation of the footwear industry. Initial solutions for the automation of different shoe production processes, such as shoe lasting machine, shoe gluing and shoe finishing, were presented in Nemec and Žlajpah (2006). In Nemec and Zlajpah (2008) a shoe grinding robotic cell was developed using the robotic kinematic redundancy due to the circular shape of the grinding disc. Also roughing and cementing are important processes that can be automated (Jatta et al., 2004; Hu et al., 2007; Pedrocchi et al., 2015). In Choi, Hwang, and You (2008) a new robot with a deburring tool is studied and implemented for shoe buffing and roughing. Kim et al. (2022) developed a robotic work cell with an industrial robot for upper roughing and cementing and for the sole cementing with a system for path tool planning. The problem of innovation and automation in footwear industries is well known also in the European Commission where different projects were founded in the past. The ROBOFOOT consortium was founded in 2010 with the aim of promoting the introduction of robotics in the European Footwear Manufacturing Industry (Maurtua, Ibarguren, and Tellaeche, 2012). The consortium remarks the importance of the coexistence of manual operations with automated ones. In the European IDEA-FOOT project a new method for the shoe integrated design and production was introduced: most of the production parameters were derived from the shoe 3D CAD model and an innovative automated production plant in which manipulators and automated machines are fully integrated was developed (Cocuzza, Fornasiero, and Debei, 2013). Some studies have been also conducted on the shoe gluing: in Castelli et al. (2020) a robotic cell for glue deposition on shoe uppers based on an extrusion system for the deposition of molten material originally in the form of a filament was analyzed and proposed. Two solutions were tested: in the first, the extruder is fixed while the robot moves the upper under the extruder; in the second, on the contrary, the extruder is fixed to the robot and moves over the upper to deposit glue. The force exerted by the robot on the extruder is a crucial parameter to have a correct glue deposition; for this reason a load cell was mounted on the robot flange. Some robotics application for footwear gluing were developed using computer vision to plan the tool path (Pagano, Russo, and Savino, 2020; Lee, Kao, and Wang, 2018). The vision system enables the location and reconstruction of the shape of objects to be glued; the detected information is then used to plan the trajectory of the robot. An important aspect concerns also with the material handling for the shoe production: Oliver et al. (2021) developed a novel sole grasping method, compatible with soles of different shapes, sizes, and materials. The method computes antipodal grasping points from point cloud acquired by RGB-D cameras or laser, without requiring a previous recognition of sole. In Gracia et al. (2017); Méndez et al. (2020) a shoe packaging robotic system is presented: the robot moves the shoes from the conveyor to the box and at the same time is able to detect human interaction and the dynamic environment, modifying its behavior accordingly. A monitoring of robotics cell for footwear industries according to principles of Industry 4.0 is done in Román-Ibáñez, Jimeno-Morenilla, and Pujol-López (2018). Regarding the automation with a robotic cell of the shoe polishing cell some works have been done in literature. In Nemec and Žlajpah (2008), in particular, a computer aided design (CAD) approach for automatic generation, optimisation and validation of motion trajectories was integrated into a robotic cell for shoe polishing. In this scenario, the shoe is attached to the robot end-effector and is then moved against a fixed rotating brush without controlling the force executed. Borrell et al. (2023) presented an innovative approach for automatic shoe patina growing in the footwear industry using a collaborative robot and a dedicated polishing tool. In this case, the trajectory is taught by manually moving the robot with the tool along the surface of the shoe, following the desired path. 1.2 Aim of the Research The objective of the presented work is to design and develop a collaborative robotic cell for luxury shoe polishing. To the best of the authors’ knowledge, there is currently no robotic system described in the literature that can perform a complete polishing process on the entire shoe with results comparable in quality to those achieved by manual polishing. This highlights a research gap between the existing literature and the requirements of this particular application. Briefly, this gap can be outlined in the following aspects: a) control of the amount of polish applied, b) maintaining a constant force applied by the robot, c) ensuring that the tool remains perpendicular to the shoe surface, d) precise control over the types of trajectories performed and e) management of the tool speed during polishing. These features are essential to achieve high-quality results in luxury leather footwear, as will be explained in detail below. In the literature, only one study has explored the use of a collaborative robot for polishing luxury leather shoes (Borrell et al., 2023). This paper will provide a detailed comparison between that study and the proposed approach, emphasizing key differences and novel contributions. At present, the process is done completely by hand in two stages: a thicker and more viscous polish is used in the first stage, while the second stage is for finishing, to give the right shine and shade to the shoe. The first step in the process is the one to be automated since it takes the most time and is the most tiring for the operator. Then the operator will perform the final finish with a smoother polish. In addition, the cycle time required by the company must be met. To achieve a good finish, polishing of the entire shoe should be done by performing different types of trajectories for each area of the shoe. These should start and end from different points, so as to leave no polish marks and achieve good homogenization. Other key aspects are the number of trajectories performed and the contact force between tool and shoe. The latter, in particular, should be as much constant as possible; for this reason a force active control (Tian et al., 2016) can be implemented exploiting the load cell at the wrist of the collaborative robot. Studies can be found on polishing complex surfaces using robots, for example in Wang, Dailami, and Matthews (2019). Different types of polishing paths, varying for models and sizes, have been drawn by CAD software working on the 3D model of the shoe. Such paths were then converted into scripts executable by the robot with both a tool speed control in the tangent plane and a force control on the normal direction. The Universal Robot’s UR5e manipulator have been used to implement the process with the integration of a polishing tool and a polish dispenser designed by the authors for the specific application. The entire process was tested on numerous shoes of different sizes and validated by the company’s chemical laboratory, which carefully evaluated the final quality of the processed shoe.The cell is now operational in the production line; the operator merely supervises the first polishing step, monitoring the robot cycle and composing polishing recipes by determining how many and which trajectories to perform on each area of the shoe. The choice of a collaborative robot cell is motivated by the operator’s need to approach the shoe at any time to monitor the process and intervene if necessary. This allows the polishing parameters to be adjusted in real time and any imperfections to be corrected manually without interrupting the robotic operation. Indeed, it is important that the automatic process is not interrupted because a change in the planned trajectories would cause an uneven polish deposit with often irreversible aesthetic defects. As a consequence, the robot must operate in power and force limiting mode (according to ISO/TS 15066:2016) and the polishing tool must be designed to be safe in the event of a collision with the operator, thus fulfilling the risk analysis for a collaborative application. In summary, the rest of the paper has been organized as follows: critical aspects to be taken into account in the automation of the shoe polishing process are described in Section 2; in Section 3, the methodology used in robotic polishing is explained; the results obtained from the tests are presented in Section 4; finally, a discussion of the main aspects of the work and concluding remarks are given in Section 5 and Section 6, respectively."
https://arxiv.org/html/2411.09524v1,"FlowNav: Learning Efficient Navigation Policies 
via Conditional Flow Matching","Effective robot navigation in dynamic environments is a challenging task that depends on generating precise control actions at high frequencies. Recent advancements have framed navigation as a goal-conditioned control problem. Current state-of-the-art methods for goal-based navigation, such as diffusion policies, either generate sub-goal images or robot control actions to guide robots. However, despite their high accuracy, these methods incur substantial computational costs, which limits their practicality for real-time applications. Recently, Conditional Flow Matching (CFM) framework has emerged as a more efficient and robust generalization of diffusion. In this work, we explore using CFM to learn action policies that help the robot navigate its environment. Our results demonstrate that CFM can generate highly accurate robot actions. CFM not only matches the accuracy of diffusion policies but also significantly improves runtime performance. This makes it particularly advantageous for real-time robot navigation, where swift, reliable action generation is vital for collision avoidance and smooth operation. By leveraging CFM, we provide a pathway to more scalable, responsive robot navigation systems capable of handling the demands of dynamic and unpredictable environments.","Mobile robots need fast action control outputs especially when dealing with dynamic obstacles and for improved utility. While diffusion-based policies [1] for navigation [2] are effective for generating multi-modal data they struggle when actions need to be quick. Recent work in training Continuous Normalizing Flows (CNF) [3] using flow matching [4, 5, 6, 7] has proven to be quite successful in other modalities and provides much faster inference times [7]. In our work, we show that using the CFM framework instead of a diffusion-based policy for navigation improves the inference times significantly (8x) while providing the same level of accuracy. Navigation for mobile robots is an important and well-studied problem that requires a robot to move from it’s current state to a goal state while avoiding obstacles and following a close-to-optimal path. Robots first perceive the environment to find areas of interest as well as to map and localize within their environment [8]. Then, based on optimizing some predetermined objective, the robot plans a path to its target destination on the map and acts optimally to move along the planned path. This technique has demonstrated success in various challenging scenarios [9] and has been deployed in numerous applications. Despite these successes, mobile robots haven’t been as widely deployed as one would have thought because of issues related to reliability, cost, safety, etc. Recent work [2, 10, 11] tries to solve this problem by training models across different embodiments and environments with large amounts of data. The aim of such an approach is to replicate the successes seen in different modalities such as language and be able to train a visual navigation foundation model [11]. Further work [2] tries to improve on this by using diffusion policies [1] for action generation. Diffusion policy is a great choice because it provides the ability to train multimodal action distributions and provides stable training[7]. Diffusion models have proven to be effective across multiple modalities [7, 12, 13, 14, 15] for data generation. They approximate a stochastic differential equation that transforms the Gaussian distribution into the desired distribution. Recent work [4, 5, 6, 7] has shown that CNF trained with a flow matching framework leads to high-quality samples and the source distribution can be arbitrary and does not necessarily have to be Gaussian. CNF can be trained with a regression over the drift of the ODE and provides a stable objective similar to diffusion. CFM can use straighter flows which can be integrated in fewer steps to get the output. In the rest of the paper, we first introduce related work and talk about classical and learning-based navigation. Next, we give some background on Denoising Diffusion Probabilistic Models (DDPM) [12] and how it is used in diffusion policy [1] and NoMaD [2]. We then present the CFM framework and explain how we use it for training our navigation policy. Next, we explain our experimental evaluations and their results and show that FlowNav is as effective as the state of the art while being considerably faster."
https://arxiv.org/html/2411.09493v1,Strategic Sacrifice: Self-Organized Robot Swarm Localization for Inspection Productivity,"Robot swarms offer significant potential for inspecting diverse infrastructure, ranging from bridges to space stations. However, effective inspection requires accurate robot localization, which demands substantial computational resources and limits productivity. Inspired by biological systems, we introduce a novel cooperative localization mechanism that minimizes collective computation expenditure through self-organized sacrifice. Here, a few agents bear the computational burden of localization; through local interactions, they improve the inspection productivity of the swarm. Our approach adaptively maximizes inspection productivity for unconstrained trajectories in dynamic interaction and environmental settings. We demonstrate the optimality and robustness using mean-field analytical models, multi-agent simulations, and hardware experiments with metal climbing robots inspecting a 3D cylinder.","Imagine a future where small robotic teams roam our infrastructure—bridges, pipelines, buildings, and satellites—detecting problems promptly, such as leaks and cracks (Figure 1(b)) [1, 2, 3, 4]. Teams of robots offer many advantages for inspection, including high parallelization, resilience to failure, and potentially low unit cost. However, for effective inspection, robots need to know where they are, a problem known as localization. In most indoor and remote locations, external localization mechanisms like GPS are unavailable or unreliable [5]. Robots must autonomously perform localization, overcoming difficulties such as sensor noise, limited landmarks, and slippage. While the literature primarily addresses the localization problem from a single-robot perspective, with approaches such as SLAM achieving good accuracy [6], these methods are computationally intensive. Computation is a finite and valuable resource; if most of an agent’s computation is used for localization, little remains for productive inspection tasks like finding cracks, measuring vibration, and monitoring rust accumulation. We explore if collaboration among robots can simplify this localization problem, allocating more computation for inspection. We draw inspiration from nature, where individuals sacrifice for the group’s benefit. In anti-predator vigilance, some members watch for predators, allowing others to eat safely [7, 8]. Army ants form bridges out of their bodies, facilitating cargo transport across gaps and cracks [9, 10]. The vigilant animals forego eating, and the bridge ants do not carry cargo; instead, by sacrificing individual productivity, they enhance group success. What is especially interesting about these biological cases is that the fraction of sacrificers is self-organized and adapts to environmental demands. We introduce a novel cooperative localization mechanism for robot swarms that leverages this idea of self-organized sacrifice for the group’s benefit. While previous studies have used cooperation to enhance localization accuracy and enable localization in unknown environments [11, 12, 13], these methods often increase computational requirements, constrain robot trajectories, and lack adaptability to changing environments. In our approach, individuals become dedicated localizers or inspectors, with this distribution self-organized based on local interactions. We demonstrate that this decentralized mechanism optimizes collective productivity in dynamic conditions, validated through theoretical models, numerical simulations, and hardware experiments. By deriving mean-field models (Sec 4), we prove that sacrificing agents as dedicated localizers improves swarm productivity. We also show that the swarm can reconfigure based on local interactions to always maximize productivity. Using agent-based numerical simulations (Sec 5), we demonstrate that group productivity increases further through smarter collaboration. Hardware experiments (Sec 6) conducted with a swarm of 10 metal climbing robots inspecting a 3D metal cylinder (Figure 1) demonstrate the effectiveness of this approach in navigating complex physical environments. Moreover, these experiments show that the emergent behavior optimizes productivity amidst dynamically changing interactions. Figure 1: Rovables [14] (a) on a 3D metal cylinder, (b) on a piping system, and (c) in the palm of a human hand. (d) Rovables with markers for Vicon Motion Capture."
https://arxiv.org/html/2411.09451v1,DiffRoad: Realistic and Diverse Road Scenario Generation for Autonomous Vehicle Testing,"Generating realistic and diverse road scenarios is essential for autonomous vehicle testing and validation. Nevertheless, owing to the complexity and variability of real-world road environments, creating authentic and varied scenarios for intelligent driving testing is challenging. In this paper, we propose DiffRoad, a novel diffusion model designed to produce controllable and high-fidelity 3D road scenarios. DiffRoad leverages the generative capabilities of diffusion models to synthesize road layouts from white noise through an inverse denoising process, preserving real-world spatial features. To enhance the quality of generated scenarios, we design the Road-UNet architecture, optimizing the balance between backbone and skip connections for high-realism scenario generation. Furthermore, we introduce a road scenario evaluation module that screens adequate and reasonable scenarios for intelligent driving testing using two critical metrics: road continuity and road reasonableness. Experimental results on multiple real-world datasets demonstrate DiffRoad’s ability to generate realistic and smooth road structures while maintaining the original distribution. Additionally, the generated scenarios can be fully automated into the OpenDRIVE format, facilitating generalized autonomous vehicle simulation testing. DiffRoad provides a rich and diverse scenario library for large-scale autonomous vehicle testing and offers valuable insights for future infrastructure designs that are better suited for autonomous vehicles.","I INTRODUCTION Testing and validating autonomous vehicles (AVs) necessitate a substantial number of diverse scenarios [1]. Road scenarios are the foundation for autonomous driving tests [2]. However, there is a significant lack of comprehensive road scenario libraries suitable for digital twin-based simulation testing [3]. The complexity and variability of real-world road scenarios critically affect the safety and overall performance of AVs. One critical bottleneck in AV simulation testing is the limited availability of diverse road scenarios for simulation purposes, with existing scenarios being predominantly homogeneous. Therefore, there is a pressing need for an effective road scenario generation method that can produce realistic and diverse road scenarios to enhance AV testing. In addition, the rapid advancement of AV technology is ushering in a transformative era in the transportation sector [4]. Unlike human-driven vehicles, AVs operate in fundamentally different ways [5]. However, the existing road infrastructure, primarily developed for human drivers, may not fully capitalize on the benefits provided by this emerging technology [6]. To ensure seamless integration and optimal performance, road infrastructure needs to adapt accordingly. There is an urgent need for a comprehensive library of road scenarios tailored for digital twin-based AV testing. Such a library is essential for guiding the future design of road infrastructure, ensuring it meets the unique requirements of AVs and maximizes their potential benefits. To achieve high-precision intelligent driving simulation tests, several road scenario formats have been proposed, including OpenDrive [7], Lanelet2 [8], and CommonRoad [9]. Among these, OpenDrive is widely used by most well-known simulators, such as Intel’s CARLA [10], Virtual Test Drive (VTD) [11], Apollo [12], PreScan [13], and SUMO [14]. Despite its widespread use, there is little work on generating realistic and diverse road scenarios. Current methods for constructing test road scenarios rely primarily on aerial imagery [15], sensor data reconstruction [16], or manual creation using commercial tools such as MATLAB RoadRunner [17]. Besides, the CommonRoad Scenario Designer is designed in [18] to to convert OpenStreetMap (OSM) [19] maps maps to CommonRoad format. This scenario designer only enables scenario format conversion and cannot generate more scenarios for AV testing. A model-driven approach is proposed in [20] to generate interchanges based on the topology graph. However, this method can only generate specific types of interchanges scenarios. In addition, all existing methods are limited to either converting road scene formats or generating a single type of road scene from existing data, and they fall short of generating large-scale, realistic, and diverse road scenes. Therefore, an efficient method for generating a comprehensive library of realistic and diverse road scenarios for AV testing has become a critical challenge. TABLE I: Comparison of existing road scenario generation methods. Method 3D Scene Lane Level OpenDrive Simulation Test Intersection PUDO Roundabout Flyover Scenes StreetGAN [21] - - - - \checkmark - - - \ast Conditional GANs [22] - - - \checkmark \checkmark - - - \ast CruzWay [23] - \checkmark \checkmark \checkmark \checkmark - - - \ast CommonRoad [18] - \checkmark \checkmark \checkmark \checkmark - \checkmark - 39,799 JunctionArt [24] - \checkmark \checkmark \checkmark \checkmark - - - \ast Predefine [25] - \checkmark \checkmark \checkmark - - \checkmark - \ast FLYOVER [20] \checkmark \checkmark \checkmark \checkmark - - - \checkmark 1443 GFlowNets [26] - \checkmark - \checkmark - - \checkmark - \ast Autoencoder and GAN [27] - - - - \checkmark - - - \ast HDMapGen [28] - - - - \checkmark - - - \ast RoBus [29] - - \checkmark \checkmark \checkmark - - - 72,400 DriveSceneGen [30] - \checkmark - \checkmark \checkmark - - - \ast DiffRoad \checkmark \checkmark \checkmark \checkmark \checkmark \checkmark \checkmark \checkmark Infinite To address the challenges in road scenario generation for AV testing, we introduce DiffRoad, a diffusion model-based framework that can effectively capture the spatial distribution of road structures and generate high-quality road scenarios. The core idea behind this approach is to perturb the road distribution with noise through a forward diffusion process and then recover the original distribution from Gaussian noise via a learning-based denoising process. This approach results in a highly flexible road scenario generation model. To achieve controllable road scenario generation, we introduce an innovative road attribute embedding module. This module encodes road attribute information through a wide and deep network, enabling controllable, high-precision road scenario generation. Besides, to further improve the realism and usability of the generated road scenarios, we develop a scene evaluation and screening model. Finally, the evaluated scenarios are automatically converted into a generic OpenDRIVE format, facilitating digital twin-based AV testing. In summary, our main contributions are as follows: • We introduce DiffRoad, the first work to utilize the diffusion model for 3D road scenario generation, pioneering the creation of a diverse lane-level road scenario library. DiffRoad effectively harnesses the diffusion model to capture the distribution of real-world road structures, thereby enabling the generation of realistic and varied road scenarios with high efficiency and transferability. • To capture the spatial distribution of road structures and achieve controllable, high-fidelity road scenario generation, we design a novel denoising network architecture called Road-UNet. Road-UNet integrates the FreeU architecture to optimize the balance between the backbone and skip connections, significantly enhancing the quality of generated road scenarios. Furthermore, we introduce a smoothness regularization mechanism to improve the continuity and realism of the generated roads, ensuring smoother transitions and more realistic road layouts. • We propose a scene-level scoring function to select the generated road scenarios that are more realistic and sensible. To enhance the generalizability of the generated road scenarios, we automatically convert them into the OpenDRIVE format, making them compatible with mainstream simulation test platforms. • The effectiveness of DiffRoad is validated using three road datasets that we collected. The results demonstrate that our method can generate high-fidelity road scenarios while preserving essential statistical properties. Figure 1: The architecture of DiffRoad. The overarching framework of DiffRoad primarily comprises the road structure data generation model based on the enhanced conditional diffusion model, alongside the scene-level evaluation and filtering module. DiffRoad incorporates the road attention mechanism and Road MultiFreeU-Net (Road-UNet) module, iteratively refining the noise estimation based on the road attribute information to synthesize realistic and diverse road scenarios."
https://arxiv.org/html/2411.09441v1,A ROS 2-based Navigation and Simulation Stack for the Robotino,"The Robotino, developed by Festo Didactic, serves as a versatile platform in education and research for mobile robotics tasks. However, there currently is no ROS 2 integration for the Robotino available. In this paper we describe our work on a Webots simulation environment for a Robotino platform extended by LIght Detection And Ranging (LIDAR) sensors. A ROS 2 integration and a pre-configured setup for localization and navigation using existing ROS packages from the Nav2 suite is provided. We validate our setup by comparing simulations with real-world experiments conducted by three Robotinos in a logistics environment in our lab. Additionally, we tested the setup using a ROS 2 hardware driver for the Robotino developed by team GRIPS of the RoboCup Logistics League. The results demonstrate the feasibility of using ROS 2 and Nav2 for navigation tasks on the Robotino platform showing great consistency between simulation and real-world performance.","The Robotino is a mobile robot platform for science and education developed and distributed by Festo Didactic, featuring a holonomic drive, close-range infrared sensors and an Inertial Measurement Unit (IMU). It can be used for developing navigation and mapping methods [6, 3] , it is being used in applications such as office mail delivery [19], or in production logistics scenarios such as the RoboCup Logistics League (RCLL) [15]. The latest Robotino platform is using Ubuntu 18.04 and 20.04 as base OS. C++ and REST APIs are provided as well as graphical programming support and Robot Operating System (ROS) 1 nodes. However, ROS 1 [18] is nearing its end of life in 2025 and its successor, ROS 2 [12], offers more advanced features, including an extensive framework for navigation named as Nav2 [13]. In order to make use of this framework, several components need to be configured according to the characteristics of the robot at hand, including components for planning, path following, localization and sensing. Figure 1: ROS components overview In this paper, we present a ROS 2 integration for Robotino navigation, seamlessly bridging simulation and real-world deployment for rapid prototyping and testing of navigation algorithms, by leveraging the Webots simulation framework [22, 14]. This approach accelerates development, reduces costs, and enables reliable performance comparisons in common environments. Figure 1 depicts an overview of the presented Robotino ROS 2 integration. In order to obtain native support for the latest ROS 2 LTS version “Humble”, we deployed Ubuntu 22.04 and installed the core drivers from older debian packages. The installed packages are: rec-rpc and robotino-dev for interprocess communication; robotino-api2 that offers a C++ interface to the hardware; and robotino-daemons that provides the services (rpcd, controld3 and gyrod) to start and stop the driver. We further extend the Robotino 4 with two SICK TiM571 LIDAR sensors using 3D printed mounts1113D models can be found at https://github.com/carologistics/hardware/tree/master/cad/robotino/stlas shown in Figure 2. The core component of the system is the robotino driver that models the drive kinematics and odometry of the Robotino (see Section 3.1). It translates linear and angular velocity commands, which are published over a cmd_vel topic given either by an input device like a joystick or by the Nav2 stack, into corresponding motor velocities. The Robotino driver not only controls drive kinematics and odometry but also interfaces with built-in sensors like the gyroscope, infrared sensors and bumpers. It publishes sensor data over corresponding topics and provides the static transforms for each sensor relative to the base_link. Furthermore, it provides joint_state data vital for odometry calculations and localization. Similarly, the external LIDARs need to provide their data in ROS, which we also account for in our simulation setup. For the real LIDARs official packages provided by SICK are used to interface the data to ROS. The rest of the paper is as follows. We present our implementation of a driver for the Webots simulation framework [22, 14] in Section 3. While the Robotino platform has no official ROS 2 driver yet, a ROS 2 driver (using the RobotinoApi2222https://wiki.openrobotino.org/index.php?title=API2 and developed by team GRIPS from the RCLL [5]) is used to interface with the hardware for our real world experiments. The rest of the system is set up to seamlessly process data from both, simulated environments and real-world trials without a distinction of the data source. The data from both individual LIDARs is accumulated to a common scan topic while an Extended Kalman Filter [9, 7] is used to fuse the odometry and IMU data for robust localization of the robot. These steps provide the input for the Nav2 stack, which is used for localization and navigation as described in Section 4. To demonstrate our results, we compare the runs of three Robotinos on a field of the RCLL in simulation with the same runs conducted in the real world in Section 5. Then we conclude. Figure 2: LIDAR setup in simulation and on the real robot"
https://arxiv.org/html/2411.09436v1,"Robot Tasks with Fuzzy Time Requirements
from Natural Language Instructions","Natural language allows robot programming to be accessible to everyone. However, the inherent fuzziness in natural language poses challenges for inflexible, traditional robot systems. We focus on instructions with fuzzy time requirements (e.g., “start in a few minutes”). Building on previous robotics research, we introduce fuzzy skills. These define an execution by the robot with so-called satisfaction functions representing vague execution time requirements. Such functions express a user’s satisfaction over potential starting times for skill execution. When the robot handles multiple fuzzy skills, the satisfaction function provides a temporal tolerance window for execution, thus, enabling optimal scheduling based on satisfaction. We generalized such functions based on individual user expectations with a user study. The participants rated their satisfaction with an instruction’s execution at various times. Our investigations reveal that trapezoidal functions best approximate the users’ satisfaction. Additionally, the results suggest that users are more lenient if the execution is specified further into the future.","Automating household tasks and production in small and medium enterprises is attracting considerable attention in robotics (e.g., [1, 2]). Robot programming is still mostly relegated to specialized robotics experts, resulting in high costs and slow adaptation to new situations [2]. One response to this is to increase the accessibility of robot programming [3, 4], for example, with natural language [5]. Natural language contains inherent fuzziness that reduces the user’s cognitive load. However, this contrasts with the rigid parameters (like concrete execution times) demanded by traditional robot systems [6]. For example, the instruction “Prepare some food in about ten minutes!” (Fig. 1) contains fuzziness regarding parameters (“some food”) and execution time (“about ten minutes”). To handle fuzzy parameters, fuzzy-logic [7] is commonly used to deduce exact parameters (e.g., weight in grams) for the operating robot system [8, 9]. For time-dependent parameters, fuzzy-logic is extended to temporal fuzzy logic [10]. However, previous research focused hardly on the user’s perception of fuzziness regarding execution time. For instance in Fig. 1, there is no immediate loss if the robot prepares the food five minutes earlier or later. Nevertheless, the user may be dissatisfied if the operation is not performed around the specified time, which depends on the instruction and the context. In this example, execution after 15 minutes leads to higher user satisfaction than after 20 minutes. Figure 1: Natural language instructions are inherently fuzzy, requiring an interpretation within the context of the instruction and instructor. Here, the user’s satisfaction varies over time based on the start of the task execution. This paper investigates instructions with fuzzy time requirements. An instruction describes a fuzzy skill that encodes the manipulation of an object by the robot (in continuation of [11, 12]). If the user instructs only one fuzzy skill, it can be executed at maximum satisfaction by default. However, suppose the user instructs several fuzzy skills combined into one superordinate plan, the fuzzy task. In that case, the task execution may require compromises, i.e., the robot must perform some skills at a suboptimal time. For example, if the user issues another command that should also start in ten minutes (Fig. 1), rigid robot systems lack the knowledge of which operation to prefer. The satisfaction function provides this required knowledge – enabling the scheduling to maximize overall satisfaction. Precisely identifying the expectations of an individual user before execution is challenging. In addition to the given instruction itself, other aspects could also influence satisfaction, e.g., the context of the scenario, the user’s previous experience, or the expected abilities of the actor. Another challenge is that several users with (partly) divergent expectations may instruct the robot system. One response to this could be the creation of user profiles. However, this does not account for frequent user changes or public scenarios with previously unknown users. Hence, we aim to provide general statements about deriving the satisfaction functions from the instructions and context. We present two central contributions: (i) We formalize fuzzy tasks, their inference from language, and the scheduling (Section III-A and Section III-B). (ii) We deduce an overall satisfaction function from individual user satisfaction (Section III-C). On this basis, we examine fuzzy time requirements regarding their modeling, the difference between human and robot actors, and the influence of time until the required execution starts (Section IV). For this, we exploit subjective satisfaction data gathered with an online user study."
https://arxiv.org/html/2411.09360v1,D4W: Dependable Data-Driven Dynamics for Wheeled Robots,"Wheeled robots have gained significant attention due to their wide range of applications in manufacturing, logistics, and service industries. However, due to the difficulty of building a highly accurate dynamics model for wheeled robots, developing and testing control algorithms for them remains challenging and time-consuming, requiring extensive physical experimentation. To address this problem, we propose D4W, i.e., Dependable Data-Driven Dynamics for Wheeled Robots, a simulation framework incorporating data-driven methods to accelerate the development and evaluation of algorithms for wheeled robots. The key contribution of D4W is a solution that utilizes real-world sensor data to learn accurate models of robot dynamics. The learned dynamics can capture complex robot behaviors and interactions with the environment throughout simulations, surpassing the limitations of analytical methods, which only work in simplified scenarios. Experimental results show that D4W achieves the best simulation accuracy compared to traditional approaches, allowing for rapid iteration of wheel robot algorithms with less or no need for fine-tuning in reality. We further verify the usability and practicality of the proposed framework through integration with existing simulators and controllers.","Wheeled mobile robots (WMR) play a crucial role in various domains, from industrial and agricultural automation to public services, due to their versatility and flexibility in dynamic environments (Siciliano et al., 2008). Designed to navigate independently around the work facilities, wheeled robots utilize proprioceptive sensory data and baseline maps to perform path planning and collision avoidance. Although such robots have a simpler configuration space than limbed robots, developing effective control and navigation algorithms for them is still challenging, requiring accurate modeling of robot dynamics. Traditional approaches often rely on analytical models, where a physical simulator computes a robot’s trajectory under given commands in a virtual environment (Todorov et al., 2012; Coumans and Bai, 2016; Makoviychuk et al., 2021). The simulator typically has access to known properties of the robot, such as its mass, inertia, and geometry from the design schematics, predefined controller models, and parameters (Degrave et al., 2019), which can be manually adjusted to match reality. While analytical methods are theoretically accurate, they may not capture the intricate dynamics of real-world systems, such as skids and slides. As a result, the performance of algorithms evaluated on these models may be sub-optimal in practice, leading to reduced efficiency and increased safety risks. Eventually, manual alignment with real-world dynamics is required to guarantee the robot’s usability, which becomes a bottleneck in the algorithm development. A sufficiently accurate dynamics model of wheeled robots is required to address the challenges above. To this end, we propose a framework named D4W (Dependable Data-Driven Dynamics for Wheeled Robots), designed to extract the underlying complex non-linear relationships from real-world observations by combining physics-based simulation with data-driven methods. Specifically, D4W automates the data-gathering procedure that builds the dataset in an efficient and unattended manner. It makes the robot record its states while carrying out a standard sequence, allowing it to traverse the reachable areas without triggering collisions. To improve the explainability and generalizability of the learned dynamics, we perform egocentric transformations on model inputs in each simulation step of D4W. This guarantees translational and rotational symmetry in space and translational symmetry in time. Furthermore, D4W provides interoperability with existing simulator and controller implementations, enabling a seamless transition to data-driven dynamics while retaining well-defined functionalities such as rendering (Murthy et al., 2020; Mittal et al., 2023). In summary, the contributions of this paper lie in the following: • We propose D4W, a generalized framework for learning an accurate dynamics model by minimizing the difference between simulated trajectories and the observed ones while satisfying necessary kinematic invariants, which sets up a routine to gather real-world robot motion and sensory data automatically and efficiently. • To our knowledge, this is the first work realizing an interoperable dynamics simulator augmented with neural networks and trained on real-world observations in the field of wheeled mobile robots. • The parameterized dynamics model trained by D4W achieves the best simulation accuracy compared with existing analytical simulators."
https://arxiv.org/html/2411.09299v1,Hearing the Robot’s Mind: Sonification for Explicit Feedback in Human-Robot Interaction,"Social robots are required not only to understand human intentions but also to effectively communicate their intentions or own internal states to users. This study explores the use of sonification to provide explicit auditory feedback, enhancing mutual understanding in HRI. We introduce a novel sonification approach that conveys the robot’s internal state, linked to its perception of nearby individuals and their interaction intentions. The approach is evaluated through a two-fold user study: an online video-based survey with 26 participants and live experiments with 10 participants. Results indicate that while sonification improves the robot’s expressivity and communication effectiveness, the design of the auditory feedback needs refinement to enhance user experience. Participants found the auditory cues useful but described the sounds as uninteresting and unpleasant. These findings underscore the importance of carefully designed auditory feedback in developing more effective and engaging Human-Robot Interaction (HRI) systems.","Many novel applications of robotics envision close interaction with humans in everyday life settings, both in private [8] and public spaces such as hospitals [11], hotels [27] or museums [14]. For effective interactions in such scenarios, robots must be capable of operating gently, and responding to different human needs and behaviors; in practice, they need emotional [22] or social intelligence [29, 18]. This new class of social robots is rapidly growing, setting new technological and research challenges. In fact, in such challenging scenarios, robots have to accurately detect and interpret human intentions and behaviors. At the same time, the robot’s awareness of the human’s intention must be made clear to the users, to promote smooth and efficient interaction. Consider a robot tasked with offering chocolate treats to the visitors of a public building, as illustrated in Fig. 1. Figure 1: A robot offering chocolate treats to a visitor of a public building uses audio cues to inform nearby people of its internal state. To ensure the positive involvement of all the users, even those skeptical or shy, the robot must be capable of exteroceptive perception, detecting nearby individuals, predicting their intentions, and promptly reacting to those who show interest. During this process, it is also desirable that robots express their internal state and beliefs about the world to the users using non-verbal communication channels; which may, in some cases, “even say some things with greater facility and efficiency than with words”[17]. Technically, detecting users who enter the robot’s social space is feasible with current state-of-the-art sensors. Building on these technologies, our previous work [1, 4] developed a perception pipeline to discern users’ intentions to interact with the robot. These self-supervised learning methods utilize body pose and motion cues to predict the likelihood of future interactions [1]. Integrating facial features and a mutual gaze detector (specifically designed for HRI applications [3]) further enhances prediction performance [4]. Once user intention is predicted, the robot can use such information to trigger appropriate robot behaviors. This approach has been thoroughly tested in both controlled and uncontrolled scenarios, demonstrating the robot’s ability to perceive the user intention [5] – a form of indirect, nonverbal communication from the user to the robot. In this work, we aim to establish a two-way communication stream by exploring how sonification techniques [26] can facilitate the flow of information from the robot to the users and examine how users perceive these sounds. In assessing these assumptions, we propose the following contributions. Firstly, we present a sonification approach to convey the robot’s internal state, linked to its perception of the surroundings, and more specifically to the estimated probability of interaction of nearby individuals. Secondly, we evaluate how the sonification of the robot’s internal state is perceived by people, through an ad-hoc questionnaire. The remainder of the paper is structured as follows. Section 2 reviews the state-of-the-art and Sec. 3 details the experimental setup used to deploy our robot. The results are presented in Sec. 4, whereas final discussions and conclusions are in Sec. 5."
https://arxiv.org/html/2411.09294v1,"Learning Hand State Estimation
for a Light Exoskeleton","We propose a machine learning-based estimator of the hand state for rehabilitation purposes, using light exoskeletons. These devices are easy to use and useful for delivering domestic and frequent therapies. We build a supervised approach using information from the muscular activity of the forearm and the motion of the exoskeleton to reconstruct the hand’s opening degree and compliance level. Such information can be used to evaluate the therapy progress and develop adaptive control behaviors. Our approach is validated with a real light exoskeleton. The experiments demonstrate good predictive performance of our approach when trained on data coming from a single user and tested on the same user, even across different sessions. This generalization capability makes our system promising for practical use in real rehabilitation.","Stroke is one of the main causes of disability [6], resulting in severe hand functionalities limitation [13], or long-lasting hand impairments [14]. Given the importance of manual operations in everyday life, rehabilitation procedures are given a crucial role [24, 10]. Cutting-edge technology, like Virtual Reality (VR) [9, 16] and robotics [2], can assist standard rehabilitation to achieve better outcomes. In this context, it is expected that Artificial Intelligence (AI) can bring great benefits, especially in solving perception challenges that are otherwise difficult to tackle with standard devices. Indeed, Machine Learning (ML), and AI in general, have been proven to be effective in tackling complex perception tasks in robotics, e.g. for complex localization problems [22, 23] or human-robot interaction purposes [1, 4]. AI finds application also in the domain of medical robotics [30]. Hand exoskeletons are very useful in rehabilitation (see e.g. [11, 27]). Equipping them with an onboard and light perception module would enable even better therapy outcomes for several reasons. First, a perception system that does not require complex infrastructure or additional heavy devices confers versatility and lightness on the exoskeleton. These aspects are particularly important for favoring ease of use, and domestic and frequent utilization, a key factor for a successful therapy [12, 7]. Secondly, an online and robust perception of the patients’ state would allow adaptive closed-loop exoskeleton control with a positive impact on the therapy, as tuning the therapy to the patient’s state positively affects motor learning [31, 21]. Finally, and very importantly, a perception system would permit the measure of the patients’ sensorimotor deficit, crucial for delivering optimal rehabilitation [15]. At the moment, the therapy verification is manually performed by clinicians and suffer from low reliability and standardization, often affected by the therapist’s perception of the patient’s abilities [8]. Robotics could offer accurate and objective assessments of function and impairment [19, 15]. Figure 1: Light exoskeletons are promising and powerful tools for effective rehabilitation therapies. The challenge addressed in this work is to provide this kind of device, having little sensory equipment, with advanced perception systems to online measure the patient’s state and the therapy progress. For example, robotic devices are deployed to evaluate proprioception and haptic perception [21], and finger Range of Motion (RoM) [26, 29]. Fingers’ RoM are predictors of rehabilitation outcomes [20] and could be measured using hand trackers. These systems need external infrastructures, be it cameras [18] or other specialized hardware, such as gloves [3]. However, vision-based approaches have limited tracking area, as they are constrained by the camera field of view, and are susceptible to external disturbances such as light changing and occlusion. While gloves are robust in that sense, they may not be suitable to be used together with exoskeletons. Also, an under-actuated exoskeleton has been proposed to track the user’s fingers RoM [26], but it can not perform rehabilitation, since it only provides force feedback at the fingertips level. Ultimately, it is desirable to measure the hands RoM, for further evaluation of the therapy effectiveness, with no heavy infrastructure and complex wearing procedure. A dry surface Electromyography (EMG) sensor is a compelling alternative, as it is easy to wear and its measure of muscle activation can serve the monitoring of neuromuscular pathologies [5]. Light hand exoskeletons (as the one shown in Fig. 1) offer interesting possibilities for rehabilitation as they are: easy to wear and simple to use for both patient and therapist; flexible and compliant so that they easily adapt to the needs of stroke patients (who might have stiffness in the hand); relatively inexpensive, which favors their large usage. On the other side, their mechanism is simple and lacks rich sensory equipment. Thus, building an advanced perception system able to measure the patient’s state and the therapy progress requires tackling technological and research challenges. This work proposes to provide light hand exoskeletons with the perception of the patient’s hand state to improve the current rehabilitation setups and unlock possibilities in this domain. Our perception module (Sec. 2) is built on the measurement of the forearm muscle activity, obtained with EMG sensors, and the exoskeleton motion. A supervised approach learns the actual behavior of the hand wearing the exoskeleton. The experimental setup, the data collection procedure, and other implementation details are described in Sec. 3. Results are presented in Sec. 4, whereas Sec. 5 concludes the paper with final remarks."
https://arxiv.org/html/2411.09241v1,"BlueME: Robust Underwater Robot-to-Robot Communication 
Using Compact Magnetoelectric Antennas","We present the design, development, and experimental validation of BlueME, a compact magnetoelectric (ME) antenna array system for underwater robot-to-robot communication. BlueME employs ME antennas operating at their natural mechanical resonance frequency to efficiently transmit and receive very-low-frequency (VLF) electromagnetic signals underwater. To evaluate its performance, we deployed BlueME on an autonomous surface vehicle (ASV) and a remotely operated vehicle (ROV) in open-water field trials. Our tests demonstrate that BlueME maintains reliable signal transmission at distances beyond 200 meters while consuming only 1 watt of power. Field trials show that the system operates effectively in challenging underwater conditions such as turbidity, obstacles, and multipath interference– that generally affect acoustics and optics. Our analysis also examines the impact of complete submersion on system performance and identify key deployment considerations. This work represents the first practical underwater deployment of ME antennas outside the laboratory, and implements the largest VLF ME array system to date. BlueME demonstrates significant potential for marine robotics and automation in multi-robot cooperative systems and remote sensor networks.","Underwater robot-to-robot communication is essential for ensuring coordinated operation of autonomous underwater vehicles (AUVs) in a wide range of applications including environmental monitoring, search and rescue, and scientific expeditions [1, 2]. In subsea environments where human intervention is limited, AUVs rely on seamless communication to share information, synchronize tasks, and respond to dynamic changes in real time [3, 4] in multi-robot missions or when communicating with surface vessels [5, 6]. This capability is particularly crucial in surveying or mapping of deep-sea areas [4], navigating overhead structures [7], and collecting samples from areas that are inaccessible to human scuba divers [6]. Figure 1: The proposed BlueME system includes a novel ME antenna design; we use a 3\times5 array of these antennas to enable real-time communications between underwater robots and wireless sensor nodes. Traditional electromagnetic and radio frequency (RF) signals are severely attenuated underwater [1, 8], limiting their effective communication range to only a few meters. While propagation is acceptable with very low frequencies (VLFs) [9], it typically comes at the cost of high transmission power requirements and impractically large antenna sizes [10, 11]. In seawater, electromagnetic signal absorption ranges from 1 to 10 dB/meter depending on conductivity [12]. This severely limits the viability of RF communications for marine robotics applications [13]. Acoustic communication has been the most commonly used modality for underwater robot communications, offering omnidirectional transmission and acceptable signal attenuation over longer distances [14, 15]. Low-frequency long-range systems are used in ocean basin communication [16], while high-frequency short-range systems are preferred for robot positioning and data transfer [17, 18]. Acoustic communications suffer from Doppler effects, phase and magnitude fluctuations, and multi-path interference [19, 20, 21], compromising reliability and efficiency. Acoustic signals can be disruptive to marine ecosystems as well, limiting their long-term deployments. Optical communication systems (e.g., visible light, blue-green lasers) offer higher data rates [22, 23, 24], but are limited by line-of-sight requirements. They are susceptible to biofouling [25], a process where microorganisms and biological buildup degrade optical sensors and signal quality over time [26]. Magnetoelectric (ME) antennas offer a promising solution to overcome the limitations of acoustic and optical communication in underwater environments [27, 9, 28, 29]. By leveraging the magnetoelectric effect, these antennas efficiently transmit and receive very-low-frequency (VLF) electromagnetic signals with a more compact design than traditional electrically small antennas (ESAs). This efficiency, combined with their resilience to multi-path interference, non-line-of-sight requirements, and Doppler effects, makes ME antennas well-suited for space- and power-constrained underwater applications. In this letter, we present the design, development, and validation of BlueME, a novel magnetoelectric (ME) antenna system for real-time data transmission between underwater robots. As shown in Fig. 1, we use a compact array of ME antennas integrated into pressure-compensated enclosures for real-time data transmission and retrieval between robots and sensors deployed underwater. While previous ME-based underwater communication efforts have primarily focused on basic connectivity tests in controlled water-tank environments [27, 30, 31, 9], we validate BlueME in a natural lake, demonstrating its potential as a deployable communication system for practical marine robotics applications. In particular, we deploy the proposed system on ASVs (Autonomous Surface Vehicles) and underwater ROVs (Remotely Operated Vehicles) for comprehensive performance evaluation and feasibility analysis. Our field experimental results demonstrate that the BlueME system offers a more robust, long-range, scalable, and efficient alternative to the traditional acoustic and optical communication modalities. We also analyze the antenna characteristics, underlying theory of operations, and relevant engineering constructs for practical deployments of ME antenna arrays underwater. Overall, we make the following contributions in this work: 1. We design, simulate, and fabricate an ME antenna array system (BlueME) for real-time underwater communication between mobile robots and/or sensor nodes. This is the first ME antenna array system tested in practical environments beyond controlled water-tank setups. 2. We present a seamless integration of BlueME into pressure-compensated enclosures to ensure reliable underwater operations. The enclosed system is compact and portable for use on low-power embedded devices. 3. We validate the system through simulation as well as open-water experiments, demonstrating its effectiveness for underwater communication. The system performs reliably when fully submerged and is unaffected by turbidity, line-of-sight obstacles, and shallow-water interference, which limit acoustic and optical systems. 4. Our field deployments reveal that we can achieve high-fidelity communications between mobile robots for up to 200 meters with a power footprint of only 1 watt. These capabilities demonstrate its potential for robust and low-latency communication underwater."
https://arxiv.org/html/2411.09198v1,Risk-aware MPPI for Stochastic Hybrid Systems,"Path Planning for stochastic hybrid systems presents a unique challenge of predicting distributions of future states subject to a state-dependent dynamics switching function. In this work, we propose a variant of Model Predictive Path Integral Control (MPPI) to plan kinodynamic paths for such systems. Monte Carlo may be inaccurate when few samples are chosen to predict future states under state-dependent disturbances. We employ recently proposed Unscented Transform-based methods to capture stochasticity in the states as well as the state-dependent switching surfaces. This is in contrast to previous works that perform switching based only on the mean of predicted states. We focus our motion planning application on the navigation of a mobile robot in the presence of dynamically moving agents whose responses are based on sensor-constrained attention zones. We evaluate our framework on a simulated mobile robot and show faster convergence to a goal without collisions when the robot exploits the hybrid human dynamics versus when it does not.","Hybrid dynamical systems [1, 2], in which the dynamics function changes in response to an event, appear in several robotic applications. Examples include walking legged robots [3], contact-rich quadrotor navigation [4], and mobile robot navigation on different terrains [5]. In practice, due to disturbances or a priori unknown dynamics of the robot or the environment, these systems are often modeled as stochastic hybrid systems [6]. While there is extensive research on path planning and control for hybrid dynamical systems [7, 8], we focus here on model predictive approaches to address these challenges. Our motivation for studying these systems arises from navigation challenges, and particularly two interrelated, but often independently addressed, challenges. First, the motion of agents, whether human or non-human, is challenging due to noisy observations of past states and the often unpredictable behavior of these agents (e.g., sudden changes in direction). Planners must account for this inherent uncertainty, which may be captured by modeling agent motion as a stochastic hybrid system. Second, agents may abruptly alter their behavior upon perceiving new entities in their environment, including robots, obstacles, or other agents. In this work, we model agents’ dynamic modes as hybrid dynamical systems, where each mode represents varying levels of cooperation or responsiveness to a robot. Mode switching is triggered, for example, when the robot enters an agent’s attention zone, such as a human’s field of view or an aquatic or aerial agent’s sensory range. The integration of estimators into path planning systems is crucial for navigating environments that involve diverse agents. Previous works [9, 10] have proposed estimators that provide deterministic predictions, typically trained on datasets specific to constrained contexts like pedestrian traffic in mall corridors. More recent research has shifted towards developing adaptable behavioral models for estimation [11, 12, 13] or inferring deviations from expected motion based on observations [14]. These approaches consider factors such as social attention, which prioritizes nearby agents [10], or context-specific parameters like environmental constraints and sensory limitations [11]. In this work, we focus on modeling cooperative behaviors where agent interactions are influenced by sensory zones, such as a field of view or sensory range. These factors play a critical role in enhancing situational awareness and guiding responses to dynamic environmental changes [13]. Since exact analytical expressions for propagating probability distributions through nonlinear functions are generally not available, several approximate methods have been developed to solve stochastic MPC. These methods include Monte Carlo simulations [15], successive Gaussian approximation [16], and the Unscented Transform [17]. In the domain of hybrid systems, MPC problems are typically formulated as mixed-integer optimizations [5, 18], which can become computationally prohibitive, particularly when the system dynamics are nonlinear. Since solving the hybrid MPC exactly may not be feasible for real-time implementation, we adopt Model Predictive Path Integral (MPPI) Control [19], a sampling-based, gradient-free method for solving unconstrained MPC problems in real-time. MPPI typically converts hard constraints into soft constraints by incorporating them into the objective function. Variants of MPPI have been developed to address uncertainty in the initial state [20] and disturbances in system dynamics [21, 22]. However, [20] focuses on deterministic dynamics, and [21, 22] address only state-independent noise. To our knowledge, applying MPPI to systems with state-dependent disturbances remains a challenge due to the lack of analytical methods for propagating future states under such disturbances. In this work, we leverage recent advances in uncertainty propagation techniques[17] for stochastic MPC within the MPPI framework. The problem of switching dynamics under uncertain states is similar to work on belief tree planning for motion planning under estimator uncertainty [23]. These works incorporate state estimation uncertainty in the planning module to probabilistically bound the safety violation metric as the robot navigates with an onboard localization module. The correction step of the Extended Kalman filter (EKF) that is used for state estimation is triggered only when an obstacle is within the sensing region of the robot. However, [23] implements the switch between these different modes based only on the mean of the state estimate, thereby making switching a deterministic phenomenon despite the state estimates being stochastic. In this work, we propose to use a particle-based uncertainty propagation scheme based on Unscented Transform [17] in MPPI. Our contributions are twofold: First, by utilizing weighted particles, we map the stochasticity of states to the stochasticity of mode switching, moving beyond deterministic switching based only on predicted state means. Second, we extend MPPI to systems with arbitrary state-dependent disturbances. Both of these contributions enable us to define a risk-aware framework for path planning with respect to static and dynamic obstacles for a hybrid dynamical system. Our experimental results demonstrate improved cost and safety performance by accounting for the hybrid nature of non-ego agent dynamics, compared to planning approaches that neglect cooperative behaviors or assume constant velocity models [24, 25]. I-A Related Work The work most similar to ours in terms of solution approach is [26], wherein different terrains induce different dynamics for a mobile robot. An unknown traction parameter in the dynamics is modeled with a terrain-dependent probability distribution, thereby making this also an instance of stochastic hybrid dynamics. They propose a Monte Carlo (MC) scheme motivated by [27] to evaluate the cost of executing a control input sequence. From an application standpoint, [26] applied MPPI when the uncertainty in dynamics is dependent on static terrain features whereas uncertainty in our case stems from active interaction between dynamically moving ego and non-ego agents. However, the main difference is that the method presented in [26] uses Monte Carlo whereas we use UT to propagate state distributions across nonlinear functions. Whereas MC can require a prohibitively large number of samples [28], for similar performance in accuracy UT is shown to be more computationally efficient in contrast [29, 30, 17]."
https://arxiv.org/html/2411.09052v1,ClevrSkills: Compositional Language and Visual Reasoning in Robotics,"Robotics tasks are highly compositional by nature. For example, to perform a high-level task like cleaning the table a robot must employ low-level capabilities of moving the effectors to the objects on the table, pick them up and then move them off the table one-by-one, while re-evaluating the consequently dynamic scenario in the process. Given that large vision language models (VLMs) have shown progress on many tasks that require high level, human-like reasoning, we ask the question: if the models are taught the requisite low-level capabilities, can they compose them in novel ways to achieve interesting high-level tasks like cleaning the table without having to be explicitly taught so? To this end, we present ClevrSkills - a benchmark suite for compositional reasoning in robotics. ClevrSkills is an environment suite developed on top of the ManiSkill2 [16] simulator and an accompanying dataset. The dataset contains trajectories generated on a range of robotics tasks with language and visual annotations as well as multi-modal prompts as task specification. The suite includes a curriculum of tasks with three levels of compositional understanding, starting with simple tasks requiring basic motor skills. We benchmark multiple different VLM baselines on ClevrSkills and show that even after being pre-trained on large numbers of tasks, these models fail on compositional reasoning in robotics tasks.","Compositional generalization is a hallmark feature of human intelligence. Unlike any other animals, humans can receive instructions in natural language and successfully perform previously unseen tasks with minimal to no task-specific learning or adaptation. Modeling this capability has been a long-standing aspiration in AI, dating back at least to Winograd’s influential SHRDLU system [45] developed more than half a century ago. The architectural underpinnings that enable these capabilities in humans have remained an inspiration as well as puzzle until this day [22, 37]. A potential steppingstone towards replicating this ability in AI systems is the recent progress in language modeling, based on large models pre-trained using next-token-prediction. These models have shown encouraging compositional reasoning behaviors in response to language-based prompts – an ability that was confined initially to text-based tasks, but that since has been extended to multi-modal, and most recently also to robotics tasks. Compositional reasoning based on language has evolved hand-in-hand with the introduction of benchmark tasks and challenges. For language-based reasoning tasks, these include, for example, the bAbI AI challenge [44], GSM8k [7], and many others. Multi-modal tasks include the popular CLEVR challenge [21] and its descendants (eg., [47, 2]), and various intuitive physics datasets (eg., [25, 46, 15]). Common to these challenges is that they require a model to reason about a scene or situation. Despite their reliance on some degree of “common sense”, these existing challenges do not require any type of actions, behaviors or planning. As such, they are confined to evaluating compositionality in a purely abstract setting, even in the case where the input data is multi-modal. In this work, we propose an environment and corresponding suite of tasks, which instead allow us to study compositional generalization in a highly controlled, but complex robotics context. Our benchmark is based on dexterous manipulation tasks, such as pick, place, throw, touch and push within the ManiSkill2 simulation environment [16], and it evaluates the ability to generalize to complex tasks based on these low-level capabilities. Our benchmark allows us to assess a model’s capability to perform compositional generalization with respect to the creation and execution of step-by-step execution plans. However, unlike existing benchmarks, such as Vima [20], our benchmark includes not just the higher-level planning but also the low-level execution layers for a wide variety of end-to-end robotics tasks. This allows us to assess not just a model’s ability to perform abstract planning in isolation but a model’s ability to plan-and-execute within a closed loop. Our contributions in detail are as follows: • We introduce the ClevrSkills111Data and code are available at https://www.qualcomm.com/developer/software/clevrskills-dataset and https://github.com/Qualcomm-AI-research/ClevrSkills environment suite, consisting of 33 different tasks spread across 3 different levels which can be used to benchmark compositional reasoning in robotics models. • We introduce an accompanying dataset of 330k ground truth trajectories generated by scripted oracle policies which use motion planning to achieve the tasks that can be used for imitation learning. The dataset also contains many types of annotation, including language, action classes, bounding boxes for objects, visibility annotations, key-steps, rewards (for offline RL), camera parameters and more. • We benchmark SOTA open-source vision language models and show that they tend to fail on tasks requiring compositional understanding."
https://arxiv.org/html/2411.09022v1,DART-LLM: Dependency-Aware Multi-Robot Task Decomposition and Execution using Large Language Models,"Large Language Models (LLMs) have demonstrated significant reasoning capabilities in robotic systems. However, their deployment in multi-robot systems remains fragmented and struggles to handle complex task dependencies and parallel execution. This study introduces the DART-LLM (Dependency-Aware Multi-Robot Task Decomposition and Execution using Large Language Models) system, designed to address these challenges. DART-LLM utilizes LLMs to parse natural language instructions, decomposing them into multiple subtasks with dependencies to establish complex task sequences, thereby enhancing efficient coordination and parallel execution in multi-robot systems. The system includes the QA LLM module, Breakdown Function modules, Actuation module, and a Vision-Language Model (VLM)-based object detection module, enabling task decomposition and execution from natural language instructions to robotic actions. Experimental results demonstrate that DART-LLM excels in handling long-horizon tasks and collaborative tasks with complex dependencies. Even when using smaller models like Llama 3.1 8B, the system achieves good performance, highlighting DART-LLM’s robustness in terms of model size. Please refer to the project website https://wyd0817.github.io/project-dart-llm/ for videos and code.","Large Language Models (LLMs) have demonstrated significant reasoning capabilities that can be applied to robot systems. Chen et al. introduced the Decision Transformer, a reinforcement learning method that redefines decision-making processes as sequence modeling problems [1]. Reed et al. developed Gato, a generalist artificial intelligence model capable of learning and adapting across multiple tasks and modalities [2]. Ahn et al.’s FLAN-SayCan and PaLM-SayCan leverage the capabilities of large pre-trained language models (LLMs) to interpret and execute tasks based on natural language instructions, enabling robots to understand and perform tasks driven by complex natural language commands [3]. Huang et al.’s Inner Monologue method integrates language models into planning and reasoning processes, enhancing robots’ decision-making capabilities [4]. Brohan et al. proposed RT-1, a model for real-time control and decision-making in complex environments, applying transformer architecture directly to robotics technology to ensure unprecedented precision and adaptability in understanding and navigating the physical world [5]. Brohan et al. further introduced the RT-2 model, which integrates vision, language, and action capabilities, enabling robots to autonomously learn and control actual actions from web knowledge [6]. Driess et al.’s Palm-E model enhances the perception and action capabilities of language models, offering a new way for artificial intelligence systems to interact more naturally with the external world through enhanced multimodal understanding [7]. Liang et al. launched Code as Policies, enabling large language models (LLMs) to convert natural language commands into robot strategy code with minimal prompts [8]. Huang et al. introduced VoxPoser, which extracts language-conditioned affordances and constraints from LLMs and applies them to the perceptual space through VLMs. VoxPoser uses a code interface without additional training for any component [9]. Multi-robot systems have shown immense potential across various scenarios, such as complex construction sites and disaster response [10], due to their high flexibility and efficiency. The key advantage of these systems lies in their ability to perform tasks in parallel and collaborate to solve problems that are difficult for a single robot to handle independently. Zhao et al. proposed the RoCo framework, utilizing LLMs for high-level communication and low-level path planning [11]. Kannan et al. developed the SMART-LLM framework, specifically targeting task planning issues in multi-robot systems [12]. The comparison of the above-mentioned studies is shown in Table I. Table I. indicates that although existing research has significantly enhanced robots’ understanding of natural language and flexibility in task execution, driven by cutting-edge technologies like LLMs, there remain fragmented deployments of multi-robot systems. Moreover, these studies still face limitations in handling complex task dependencies and parallel execution. They utilize linear methods for task decomposition, making it challenging to effectively manage the interdependencies among subtasks generated from tasks with complex dependencies, which is particularly critical in multi-robot systems. The key advantage of multi-robot systems lies in their ability to collaboratively solve problems that are difficult for a single robot to handle independently. However, achieving this collaboration requires an effective mechanism that ensures logical sequencing of interdependent subtasks among multiple robots, while enabling parallel execution of independent tasks. To address these issues, we propose the DART-LLM (Dependency-Aware Multi-Robot Task Decomposition and Execution using Large Language Models) system. The main contributions are as follows: 1. Integrated System Framework: An integrated system framework named DART-LLM, comprising multiple modules, is proposed. The framework includes the QA LLM module, Breakdown Function module, Actuation module, and a Vision-Language Model (VLM)-based object detection module. These modules perform Instruction Parsing and Task Decomposition, Parsing of Decomposed Tasks, Grounding in Embodiments, and Updating the Object Map Database to accomplish task decomposition and execution from natural language instructions to robotic actions. 2. Dependency-Aware Task Decomposition: A dependency-aware task decomposition method is proposed, which breaks down complex tasks into multiple interdependent subtasks. By accounting for dependencies, the DART-LLM allows execute dependent subtasks sequentially while running independent tasks in parallel, thereby enhancing logical capabilities and improving the efficiency of complex tasks. TABLE I: Comparison of Related Work in Multi-Robot Systems and Large Language Models (LLMs) Related Work Multi-Robot System Robot Mobility Real-time Capability Dependency-Aware Chen et al., Decision Transformer [1] ✓ ✓ Reed et al., Gato [2] ✓ ✓ Ahn et al., FLAN-SayCan, PaLM-SayCan [3] ✓ ✓ Huang et al., Inner Monologue [4] ✓ ✓ Brohan et al., RT-1 [5] ✓ ✓ Brohan et al., RT-2 [6] ✓ ✓ Driess et al., Palm-E [7] ✓ ✓ Liang et al., Code as Policies [8] ✓ Huang et al., VoxPoser [9] ✓ Zhao et al., RoCo [11] ✓ ✓ Kannan et al., SMART-LLM [12]"
https://arxiv.org/html/2411.09020v1,"Predictive Visuo-Tactile Interactive Perception Framework 
for Object Properties Inference","Interactive exploration of the unknown physical properties of objects such as stiffness, mass, center of mass, friction coefficient, and shape is crucial for autonomous robotic systems operating continuously in unstructured environments. Precise identification of these properties is essential to manipulate objects in a stable and controlled way, and is also required to anticipate the outcomes of (prehensile or non-prehensile) manipulation actions such as pushing, pulling, lifting, etc. Our study focuses on autonomously inferring the physical properties of a diverse set of various homogeneous, heterogeneous, and articulated objects utilizing a robotic system equipped with vision and tactile sensors. We propose a novel predictive perception framework for identifying object properties of the diverse objects by leveraging versatile exploratory actions: non-prehensile pushing and prehensile pulling. As part of the framework, we propose a novel active shape perception to seamlessly initiate exploration. Our innovative dual differentiable filtering with Graph Neural Networks learns the object-robot interaction and performs consistent inference of indirectly observable time-invariant object properties. In addition, we formulate a N-step information gain approach to actively select the most informative actions for efficient learning and inference. Extensive real-robot experiments with planar objects show that our predictive perception framework results in better performance than the state-of-the-art baseline, and demonstrate our framework in three major applications for i) object tracking, ii) goal-driven task, and iii) change in environment detection.","Figure 1: Overview of the proposed framework for visuo-tactile based interactive perception framework for active object exploration with three main components. 1. Uses visual information to actively estimate the shape of diverse objects based on superquadrics. 2. Actively selects the most informative action affordance for interaction. 3. Utilizes dual differentiable filtering for the estimation of objects’ properties using visual and tactile information. To increase the autonomy of the robotic system involved in various object manipulation tasks, it is essential that the robot perceive the physical properties of the object. However, estimating properties such as mass, the center of mass, and surface friction is challenging, as they are not directly observable in static environments and are salient only under specific object-robot interactions [1]. Exploring previously unseen objects poses a challenge for current visual or tactile perception frameworks and necessitates the use of simple and robust interaction strategies. In this study, we introduce a novel predictive perception framework for inferring the properties of objects of various rigid objects such as with homogeneous, heterogeneous, articulated properties and using vision and tactile sensing. Previously, researchers have relied predominantly on vision or tactile methods to estimate the physical properties of objects. Tactile sensing offers a rich and diverse set of information about the object and allows the inference of multiple object properties. However, it requires precise information and prior knowledge [2, 3]. On the other hand, the range of properties observable through vision is limited [4]. Nevertheless, it can provide a global overview of the shape, and movement, and guide autonomous exploration. Recent works by Murali et al. [5] and Lee et al. [6] have shown how a visuo-tactile-based approach can significantly improve the performance of robotic systems by addressing challenges such as pose estimation and contact-rich manipulation tasks. We aim to integrate complementary vision and tactile sensing to improve the reliability of robotic systems. In such interactive visuo-tactile perception, purposeful physical interactions or explorations are made to improve object perception [7, 8, 3, 9, 10, 11]. Taking inspiration from an infant playful exploration [12, 13, 14, 15] of pulling and pushing objects on the floor, our study focuses on two types of simple and natural exploratory actions: non-prehensile pushing and prehensile pulling to explore the diverse set of objects. Pushing an object for a robotic system is a more straightforward task, particularly when dealing with large and heavy objects or when there is no prior knowledge about the object. In contrast, form-enclosure grasping [16] (prehensile) and pulling an object is a more stable approach compared to lifting and manipulating it, as factors such as the object’s geometry and grasp stability come into consideration [13, 17, 18]. Furthermore, for more sophisticated exploratory actions, complex robotic hardware would be required along with an intricate control mechanism. Nevertheless, exploring the properties of objects through non-prehensile push or prehensile pull poses a difficult challenge. This is due to the complexity of the dynamics of interaction between the object and the robot. Furthermore, the parameters are interrelated and there are significant uncertainties in both the contacts and the surface irregularities. To address such a challenging problem, we draw inspiration from neuroscience research, where such exploration behavior is inherent in humans [19]. A key working principle of human perception acknowledged since Helmholtz is that individuals actively predict perceptions relying on an internal model of their environment. Based on this prediction and immediate observation, humans make inferences online about their environment with associated uncertainties [20]. In this study, we designed such a predictive perception framework for a robotic system to learn and infer the properties of objects through interactive actions. The key aspect of our approach consists of encoding the object-robot interactions as Probabilistic Markov Models and learning the interaction model capable of predicting the visuo-tactile observation. This, compared with actual observation, will be used to estimate physical parameters such as mass, center of mass, and relative friction, since the physical properties of the objects cannot be observed directly. Bayesian filtering techniques coupled with the learnable model (differentiable filter [21]) are used as the foundation of the predictive perception framework. We generalize and improve our previously proposed dual differentiable filter [22] that can be used to infer the time-varying object motion and time-invariant parameters consistently with pertinent uncertainty for diverse rigid objects with homogeneous, heterogeneous, articulated properties. Furthermore, we leverage learnable noise models with the differentiable filter to detect changes in the learned model, which is not sufficiently researched in such interactive exploration setting. A critical aspect of such differentiable filters is the learned models. Given the diverse range of objects targeted, particularly with the inclusion of articulated objects, adopting a robust inductive bias inspired by physics becomes imperative in model selection. In this work, we leverage graph neural networks (GNNs) for this purpose, to encapsulate the intricate dynamics of interactions and elucidate how shape, pose, and physical properties contribute to visual and tactile observations. We propose a novel graph representation and propagation of graphs to model the interaction between a robot and an object. This representation focuses on the causal relationship of the interaction forces as the central element. It captures how the robot acts on the object (cause) while accounting for the environment in this interaction. This choice proves advantageous both in generalizing the model to be used in all the different cases of object-robot interaction presented in this work and also in capturing complex interactions sufficiently to account for tactile observations accurately (effect of object’s movement and properties on the robot). Furthermore, to improve data efficiency and inference time, it is essential for robotic systems to actively explore by strategically selecting the next-best exploratory actions. Previous works have shown that active object exploration outperforms a uniform and random strategy to reduce the uncertainty about objects while tackling different problems such as object recognition [23] and pose estimation [24]. In this study, we further evaluated our previously proposed non-greedy N-step Information Gain formulation [22] for active exploratory action selection and provide a more comprehensive analysis of this metric in various experimental scenarios, validating its effectiveness. In addition, to ensure robust and seamless object exploration, we utilize superquadrics for explicit shape representation and estimation, employing a Bayesian inference scheme [25]. This approach is advantageous because it requires no prior knowledge of shapes or primitives and effectively handles significant visual noise. Furthermore, compared to our previous approach in [22], this approach effectively handles all the different object types. We prefer a low-dimensional shape representation [26], which is particularly beneficial for exploring novel objects [27], over traditional methods that produce high-dimensional point clouds or meshes necessitating complex post-processing. Additionally, we introduce a novel viewpoint selection method to improve the efficiency of shape estimation in a real robotic scenario where noisy and partial views of objects are unavoidable. The overview of our proposed interactive perception framework for active object exploration and inference of physical properties is illustrated in Fig. 1. 00footnotetext: For supplementary materials, please visit https://www.robotact.de/predictive-vistac"
https://arxiv.org/html/2411.08999v1,Learning-Based Control Barrier Function with Provably Safe Guarantees: Reducing Conservatism with Heading-Aware Safety Margin,"We propose a learning-based Control Barrier Function to reduce conservatism in collision avoidance of car-like robots. Traditional CBFs often use Euclidean distance between robots’ centers as safety margin, neglecting headings and simplifying geometries to circles. While this ensures smooth, differentiable safety functions required by CBFs, it can be overly conservative in tight environments. To address this limitation, we design a heading-aware safety margin that accounts for the robots’ orientations, enabling a less conservative and more accurate estimation of safe regions. Since the function computing this safety margin is non-differentiable, we approximate it with a neural network to ensure differentiability and facilitate integration with CBFs. We describe how we achieve bounded learning error and incorporate the upper bound into the CBF to provide formal safety guarantees through forward invariance. We show that our CBF is a high-order CBF with relative degree two for a system with two robots whose dynamics are modeled by the nonlinear kinematic bicycle model. Experimental results in overtaking and bypassing scenarios reveal a 33.5\text{\,}\mathrm{\char 37\relax}\text{/} reduction in conservatism compared to traditional methods, while maintaining safety.Code: github.com/bassamlab/sigmarl","Control Barrier Functions are critical tools for ensuring safety in control systems, particularly for autonomous robots in dynamic environments. They provide a formal way to enforce safety constraints by rendering a designated safe set forward invariant [1]. In the context of car-like robots, such as Connected and Automated Vehicles, safety is critical. These robots must navigate complex environments and avoid collisions with other agents, while achieving their intended goals efficiently. Motion planning for car-like robots involves generating trajectories that are not only feasible concerning the robot’s dynamics but also safe w.r.t. the environment and other agents. Traditional Control Barrier Functions often simplify robots’ geometries to facilitate the computation of Control Barrier Function conditions or simplify safety estimation. A common simplification is to model the robots as circles, which allows for straightforward distance computation but ignores the robots’ actual shapes and orientations. While this simplifies the safety analysis and ensures the smoothness and differentiability required by CBFs, it can lead to overly conservative behaviors, especially in confined or densely populated environments. To address these limitations, we propose a learning-based Control Barrier Function that incorporates a heading-aware safety margin, inspired by Separating Axis Theorem [2] and Minimum Translation Vector [3], which we term Minimum Translation Vector-based safety margin. By accounting for car-like robots’ orientations and actual geometries, our method provides a more accurate estimation of safe regions. This approach reduces conservatism in collision avoidance, allowing them to navigate more efficiently without compromising safety. I-A Related Work Collision avoidance for car-like robots is a well-studied problem. While optimization-based approaches such as model predictive control are widely used [4, 5, 6, 7], their can be computationally intensive. ently, CBFs have gained attention for their forward invariance and formal safety guarantees. Traditional CBFs often simplify robot geometries as circles and define safety margins based on Euclidean distances between centers, which we refer to as the Center-to-Center-based safety margin. Work [8] uses Control Barrier Functions to ensure safety distance to a so-called avoidable set, which defines safe boundaries around round-shaped moving obstacles. Work [9] introduced safety barrier certificates for collision-free multi-robot systems using Control Barrier Functions, where each robot is modeled as a circle to simplify collision avoidance constraints. Study [10] extended CBFs to systems with high relative degrees, maintaining the circular approximation for robots. Further advancements include integrating learning into CBF frameworks with Center-to-Center-based safety margin. Work [11] uses off-the-center disks to avoid the conservatism in the Euclidean distance-based safety margins, where the deviation direction of the disks depends on the direction of the obstacles w.r.t. the lane center of the ego robot. In the domain of Connected and Automated Vehicles, studies [12] and [13] employed Center-to-Center-based safety margins within multi-agent Reinforcement Learning frameworks to ensure safety of the learned policies. They introduce another term to the longitudinal distance between Connected and Automated Vehicles to consider lane-changing behavior. Other similar works using Center-to-Center-based safety margin are [14, 15, 16]. While the circle approximation simplifies computations and ensures differentiability, it does not accurately capture the actual shape and orientation of car-like robots. This discrepancy can lead to overly conservative behaviors, limiting the robots’ ability to navigate efficiently in complex environments. To improve upon the circle approximation, some researchers have modeled robots or obstacles as ellipses (or ellipses in case of 3D space), which better represent their elongated shapes, despite that their distances cannot be easily computed. Work [17] proposes a conservative distance estimate between ellipsoids, which is shown to be an eigenvalue problem. Study [18] derives a closed-form expression that represents a distance metric of two ellipsoids in 3D space. In [19], robots and obstacles are represented by sets of ellipsoids and a point-world transformation is proposed to transform these ellipsoids to points, simplifying collision avoidance through customized navigation functions in the point world. The proposed transformation has been successfully applied in many other works such as [20]. Furthermore, some works use a mixture of circles and ellipses for shape approximation. This can happen by either approximating the ego robot with a circle and its surrounding robots with ellipses [21, 22] or conversely [23]. Note that only [18, 22] combine Control Barrier Functions. These approaches reduce conservatism compared to the pure circle-based approximation but still cannot fully capture the actual shape of car-like robots. I-B Paper Contributions The main contributions of this work are threefold: 1. We propose a non-differentiable, heading-aware safety margin based on Minimum Translation Vector that considers the headings and geometries of car-like robots, offering a less conservative and more accurate estimation of safe regions for collision avoidance. We train a differentiable neural network to learn it with estimable upper bound on approximation errors. 2. We establish a theorem providing our learning- and Minimum Translation Vector-based safety margin as a high-order Control Barrier Function with relative degree two for a system with two robots modeled by the nonlinear kinematic bicycle model. 3. We validate the theoretical findings through numerical simulations in overtaking and bypassing scenarios involving two car-like robots, demonstrating reduced conservatism compared to traditional Center-to-Center-based approach. Our work appears to be the first work in using Minimum Translation Vector-based safety margin to compute safety distance in Control Barrier Function. I-C Notation A variable x is annotated with a superscript x^{i} if it belongs to robot i. A relative state includes two letters in its superscript to indicate direction, e.g., x^{ji} denotes the relative x-position of robot j w.r.t. robot i. If the relative state is expressed in robot i’s ego perspective rather than in the global coordinate system, an underline is used, e.g., x^{j\underline{i}}. Vectors, such as state vector \bm{x} and control input vector \bm{u}, are bolded, and the dot product of two vectors \bm{a} and \bm{b} is denoted by \bm{a}\cdot\bm{b}. Time arguments of time-variant variables are omitted throughout the paper for simplicity. I-D Paper Structure Section II introduces preliminaries required for this work. Section III proposes our Minimum Translation Vector-based safety margin and its integration with Control Barrier Functions. Section IV discusses experimental results and limitations of our work. Section V draws conclusions and outlines future research directions."
https://arxiv.org/html/2411.09160v1,"Rationality based Innate-Values-driven 
Reinforcement Learning","Innate values describe agents’ intrinsic motivations, which reflect their inherent interests and preferences to pursue goals and drive them to develop diverse skills satisfying their various needs. The essence of reinforcement learning (RL) is learning from interaction based on reward-driven behaviors, much like natural agents. It is an excellent model to describe the innate-values-driven (IV) behaviors of AI agents. Especially developing the awareness of the AI agent through balancing internal and external utilities based on its needs in different tasks is a crucial problem for individuals learning to support AI agents integrating human society with safety and harmony in the long term. This paper proposes a hierarchical compound intrinsic value reinforcement learning model – innate-values-driven reinforcement learning termed IVRL to describe the complex behaviors of AI agents’ interaction. We formulated the IVRL model and proposed two IVRL models: DQN and A2C. By comparing them with benchmark algorithms such as DQN, DDQN, A2C, and PPO in the Role-Playing Game (RPG) reinforcement learning test platform VIZDoom, we demonstrated that rationally organizing various individual needs can effectively achieve better performance.","In natural systems, motivation is concerned explicitly with the activities of creatures that reflect the pursuit of a particular goal and form a meaningful unit of behavior in this function heckhausen2018motivation . Furthermore, intrinsic motivations describe incentives relating to an activity itself, and these incentives residing in pursuing an activity are intrinsic. Intrinsic motivations deriving from an activity may be driven primarily by interest or activity-specific incentives, depending on whether the object of an activity or its performance provides the main incentive schiefele1996motivation . They also fall in the category of cognitive motivation theories, which include theories of the mind that tend to be abstracted from the biological system of the behaving organism merrick2013novelty . However, when we analyze natural agents, such as humans, they are usually combined motivation entities. They have biological motivations, including physiological, safety, and existence needs; social motivation, such as love and esteem needs; and cognitive motivation, like self-actualization or relatedness and growth needs merrick2009motivated . The combined motivation theories include Maslow’s Hierarchy of Needs maslow1958dynamic and Alderfer’s Existence Relatedness Growth (ERG) theory alderfer1972existence . Fig. 2 and 2 illustrate the general innate values (intrinsic motivations) model and various models with three-level needs of different amounts, respectively. Many researchers regard motivated behavior as behavior that involves the assessment of the consequences of behavior through learned expectations, which makes motivation theories tend to be intimately linked to theories of learning and decision-making baldassarre2013intrinsically . In particular, intrinsic motivation leads organisms to engage in exploration, play, strategies, and skills driven by expected rewards. The computational theory of reinforcement learning (RL) addresses how predictive values can be learned and used to direct behavior, making RL naturally relevant to studying motivation. Figure 1: The illustration of the proposed innate-values-driven reinforcement learning (IVRL) model. Figure 2: The illustration of innate values models with three-level needs of different amounts. S: Small; M: Medium; L: Large In artificial intelligence, researchers propose various abstract computational structures to form the fundamental units of cognition and motivations, such as states, goals, actions, and strategies. For intrinsic motivation modeling, the approaches can be generally classified into three categories: prediction-based schmidhuber1991curious ; schmidhuber2010formal , novelty-based marsland2000real ; merrick2009motivated , and competence-based barto2004intrinsically ; schembri2007evolution . Furthermore, the concept of intrinsic motivation was introduced in machine learning and robotics to develop artificial systems learning diverse skills autonomously yang2024bayesian . The idea is that intelligent machines and robots could autonomously acquire skills and knowledge under the guidance of intrinsic motivations and later exploit such knowledge and skills to accomplish tasks more efficiently and faster than if they had to acquire them from scratch baldassarre2013intrinsically . In other words, by investigating intrinsically motivated learning systems, we would clearly improve the utility and autonomy of intelligent artificial systems in dynamic, complex, and dangerous environments yang2022game ; yang2023hierarchical . Specifically, compared with the traditional RL model, intrinsically motivated RL refines it by dividing the environment into an external environment and an internal environment, which clearly generates all reward signals within the organism111Here, the organism represents all the components of the internal environment in the AI agent. baldassarre2013intrinsically . However, although the extrinsic reward signals are triggered by the objects and events of the external environment, and activities of the internal environment cause the intrinsic reward signals, it is hard to determine the complexity and variability of the intrinsic rewards (innate values) generating mechanism. To address those gaps, we introduce the innate-values-driven reinforcement learning (IVRL) model to describe the complex behaviors in AI agents’ interactions by integrating with combined motivation theories. We formalize the idea and propose two IVRL models based on classic DQN and A2C algorithms. Then, we compare them with benchmark RL algorithms such as DQN mnih2015human , DDQN wang2016dueling , A2C mnih2016asynchronous , and PPO schulman2017proximal in the RPG RL test platform VIZDoom Kempka2016ViZDoom ; Wydmuch2019ViZdoom . The results demonstrate that the IVRL model can achieve convergence and adapt efficiently to complex and challenging tasks."
https://arxiv.org/html/2411.09153v1,VidMan: Exploiting Implicit Dynamics from Video Diffusion Model for Effective Robot Manipulation,"Recent advancements utilizing large-scale video data for learning video generation models demonstrate significant potential in understanding complex physical dynamics. It suggests the feasibility of leveraging diverse robot trajectory data to develop a unified, dynamics-aware model to enhance robot manipulation. However, given the relatively small amount of available robot data, directly fitting data without considering the relationship between visual observations and actions could lead to suboptimal data utilization. To this end, we propose VidMan (Video Diffusion for Robot Manipulation), a novel framework that employs a two-stage training mechanism inspired by dual-process theory from neuroscience to enhance stability and improve data utilization efficiency. Specifically, in the first stage, VidMan is pre-trained on the Open X-Embodiment dataset (OXE) for predicting future visual trajectories in a video denoising diffusion manner, enabling the model to develop a long horizontal awareness of the environment’s dynamics. In the second stage, a flexible yet effective layer-wise self-attention adapter is introduced to transform VidMan into an efficient inverse dynamics model that predicts action modulated by the implicit dynamics knowledge via parameter sharing. Our VidMan framework outperforms state-of-the-art baseline model GR-1 on the CALVIN benchmark, achieving a 11.7% relative improvement, and demonstrates over 9% precision gains on the OXE small-scale dataset. These results provide compelling evidence that world models can significantly enhance the precision of robot action prediction. Codes and models will be public.","In the rapidly advancing field of robotics, accurately predicting and executing precise actions based on sensory inputs is crucial. While traditional approaches [1, 2, 3, 4, 5] for robot manipulation often rely on labor-intensive hand-engineered features and models prone to errors, data-driven methods [6, 7, 8] offer promising solutions. However, the challenge lies in the difficulty and cost of acquiring high-quality robotic data. Recent advancements [9, 10, 11, 12], particularly those utilizing large-scale online video data for learning a video generator, demonstrate significant potential in comprehending complex physical dynamics of the real world. These models, trained on diverse datasets [13, 8], possess a nuanced understanding of the world, suggesting the feasibility of amalgamating and leveraging varied robot visual trajectory data [14, 15, 16] to develop a unified dynamics-aware model for enhanced robot manipulation. Yet, achieving this unification poses challenges; merely fitting data without considering the relationship between visual observations and actions could lead to suboptimal utilization of the data. Hence, there is a pressing need to develop efficient training mechanism and model architecture that can effectively leverage existing cross-robot and cross-scene data to enhance action prediction accuracy. As shown in Fig. 1, to optimize the utilization of diverse robot data [8], we draw upon insights from neuroscience’s dual process theory [17, 18, 19], which unveils the complex mechanisms of information processing and decision-making in the human brain. This theory distinguishes between two cognitive processes: System 1, responsible for rapid, intuitive responses based on immediate sensory inputs, and System 2, which involves slower, long-horizon planning grounded in abstract concepts and understanding of world dynamics [20]. Inspired by these insights, we adopt a two-stage paradigm for robot learning, exemplified by our innovative framework, terms as VidMan (Video diffusion for robot Manipulation). VidMan leverages the power of the video diffusion generation method Open-Sora [21] for robot imitation learning, tapping into the awareness of long-horizon dynamics inherent in video diffusion models to achieve more nuanced and dynamics-modulated robot action prediction. By learning different facets of data at distinct stages, VidMan acquires an inductive bias of inverse dynamics of robot control, wherein actions are the outcomes of state sequences, significantly enhancing the method’s generalization performance, especially under scenarios with limited data. Figure 1: VidMan’s two-stage training paradigm mirrors dual process theory: its first stage (like System 2) pre-trains on understanding environment dynamics through video diffusion, forming a foundation for accurate action prediction, while its second stage (like System 1) was adapted from the first stage to leverage the learned dynamics knowledge for rapid, low-level action inference. Specifically, our VidMan employs a two-stage training mechanism, akin to the principles of dual process theory, to enhance stability and significantly improve data utilization: 1) In the first stage, namely the Dynamics-aware Visionary Stage, VidMan undergoes pre-training on the Open X-Embodiment [8] dataset (OXE) using a video diffusion model to predict future trajectories based on historical observations and language instructions. This stage involves the robot learning the dynamics of state transitions from data and accurately perceiving the current environmental state, enabling the model to develop a deep understanding of the environment’s dynamics, forming a robust foundation for accurate action prediction; 2) In the second stage, dubbed the Dynamics-modulated Action Stage, VidMan incorporates a flexible yet powerful layer-wise self-attention adapter [22] to seamlessly integrate the pre-trained knowledge from the first stage into action prediction. Through shared neural architecture and parameters with the dynamics-aware visionary stage, this phase transforms VidMan into an implicit inverse dynamics model that infers dynamics-modulated actions without explicitly generating future visual trajectories, rendering it suitable for real-world robot control scenarios. The performance of VidMan has been evaluated against SOTA baselines on the CALVIN [14] benchmark, where it achieves a 11.7% relative improvement. In addition, VidMan has shown notable effectiveness on the OXE small-scale dataset, achieving over 9% precision gains. This improvement is particularly evident when the data from the target robot is small, underscoring the effective data utilization of our method. Extensive ablation studies have been conducted to analyze the effects of various design decisions within our method. These experimental results suggest that VidMan represents a meaningful advancement in robotics, providing a valuable tool for developing more capable and responsive robotic systems."
https://arxiv.org/html/2411.09145v2,"UniHOI: Learning Fast, Dense and Generalizable 4D Reconstruction for Egocentric Hand Object Interaction Videos","Egocentric Hand Object Interaction (HOI) videos provide valuable insights into human interactions with the physical world, attracting growing interest from the computer vision and robotics communities. A key task in fully understanding the geometry and dynamics of HOI scenes is dense pointclouds sequence reconstruction. However, the inherent motion of both hands and the camera makes this challenging. Current methods often rely on time-consuming test-time optimization, making them impractical for reconstructing internet-scale videos. To address this, we introduce UniHOI, a model that unifies the estimation of all variables necessary for dense 4D reconstruction, including camera intrinsic, camera poses, and video depth, for egocentric HOI scene in a fast feed-forward manner. We end-to-end optimize all these variables to improve their consistency in 3D space. Furthermore, our model could be trained solely on large-scale monocular video datasets, overcoming the limitation of scarce labeled HOI data. We evaluate UniHOI with both in-domain and zero-shot generalization setting, surpassing all baselines in pointclouds sequence reconstruction and long-term 3D scene flow recovery. UniHOI is the first approach to offer fast, dense, and generalizable monocular egocentric HOI scene reconstruction in the presence of motion. Code and trained model will be released in the future.","Egocentric Hand Object Interaction (HOI) videos capture a vast amount of knowledge about human interaction with the physical world, particularly in tool usage. Due to this rich source of interaction knowledge, egocentric human videos have gained increasing interest from both the research community (e.g., computer vision [91, 51] and robotics [44, 87]) and industry (e.g., virtual reality [47]). Figure 1: We propose UniHOI, a generalizable model that unifies camera intrinsic, procrustes-alignment confidence maps (for camera poses estimation), camera poses, and video depth estimation for fast and dense 4D reconstruction of egocentric HOI scene. To better understand the geometry and dynamics in these activities, a crucial task is the dense 4D reconstruction from internet-scale egocentric HOI video datasets [15, 21]. Here, dense 4D reconstruction is represented as a pointclouds sequence which captures the 3D position of every pixel in each frame within a global coordinate system [77, 90], preserving maximal geometry details. This task requires: (1) fast processing speeds to support large-scale reconstruction, (2) dense per-pixel reconstruction, (3) strong generalization to adapt to unseen HOI scenes, and (4) the ability to handle dynamic motions in HOI videos. However, current methods do not meet these demands. Traditional methods [57, 70], such as Structure-from-Motion (SfM) or SLAM system combined with dense depth estimation [49, 86], face significant challenges in reconstructing dynamic scenes. These multi-step approaches also suffer from accumulated errors and inconsistencies between modules [77, 74]. Recent methods have focused on time-consuming test-time optimization techniques [89, 29, 32, 40], like Gaussian Splatting [26], but their speed is impractical for reconstructing large-scale egocentric datasets [21, 38]. To address these challenges, we aim to explore fast, dense and generalizable reconstruction of highly dynamic egocentric HOI scenes. Our key insight is to simultaneously predict all variables necessary for dense 4D reconstruction with a unified model (Figure 1), including camera intrinsic, camera poses and video depth [64, 37]. This paradigm enables fast, streaming feed-forward inference, achieving speeds 30x faster than the current test-time optimization methods [68, 75, 88]. We optimize the prediction of all variables together by directly supervising the reconstructed pointclouds sequence in 3D space. This end-to-end optimization enhances scale and geometry consistency across all variables, mitigating the accumulated errors and inconsistencies in previous multi-step methods, resulting in improved reconstruction outcomes [77]. One challenge in training HOI scene reconstruction model is the limited availability of datasets with pointclouds sequence labels [31, 76]. To overcome this, we train our model purely on large-scale monocular video datasets by leveraging priors from foundation vision models. Specifically, we (1) regularize the pointclouds sequence with pre-computed per-frame monocular scene reconstruction results [49]; (2) ensure inter-frame consistency via 3D flow [83] and tracking [24] with a designed scale-agnostic loss. We transform camera pose estimation into a dense procrustes-alignment confidence map prediction problem [63, 64] and further (3) regularize the confidence map with predicted HOI masks from off-the-shelf model [91]. By training on large-scale, unlabeled mixed datasets [15, 20, 38, 31, 34], our model achieves zero-shot generalization ability. We evaluate our model on both in-domain and zero-shot unseen scenes. UniHOI successfully recovers the 3D structure of scenes and the motion of dynamic parts, even in challenging synthetic surgery HOI videos [76]. We also perform a quantitative comparison of our model with other methods that have near-linear time complexity (to the number of frames), focusing on two fundamental 4D tasks: dense pointclouds sequence reconstruction [64, 77] and long-term 3D scene flow recovery [30, 87, 3]. UniHOI demonstrates strong performance on evaluation metrics and outperforms all other baseline methods. In conclusion, our main contributions are as follows: • We propose UniHOI, a model that unifies camera intrinsic, camera poses and video depth estimation for fast, dense and generalizable 4D reconstruction of egocentric HOI scene. • We train our model solely on large-scale, unlabeled monocular HOI datasets, which helps alleviate the issue of labeled data scarcity. • UniHOI demonstrates promising performance in both in-domain and zero-shot unseen scenes, surpassing all baselines in pointclouds sequence reconstruction and long-term 3D scene flow recovery tasks. Figure 2: The overview of our UniHOI framework. The model first simultaneously predicts camera intrinsic, video depth, and procrustes-alignment confidence maps (for camera pose estimation). Camera poses are then calculated by aligning unprojected pointclouds from different frames with predicted confidence maps. The final dense pointclouds sequence reconstruction is assembled using all the predicted variables. We train our model purely on unlabeled HOI video datasets, leveraging foundation vision priors from pretrained EgoHOS [91], UniDepth [49], GMFlow [83], and CoTracker [24] as training supervision."
https://arxiv.org/html/2411.09110v1,Information-Optimal Multi-Spacecraft Positioning for Interstellar Object Exploration,"Interstellar objects (ISOs), astronomical objects not gravitationally bound to the sun, could present valuable opportunities to advance our understanding of the universe’s formation and composition. In response to the unpredictable nature of their discoveries that inherently come with large and rapidly changing uncertainty in their state, this paper proposes a novel multi-spacecraft framework for locally maximizing information to be gained through ISO encounters with formal probabilistic guarantees. Given some approximated control and estimation policies for fully autonomous spacecraft operations, we first construct an ellipsoid around its terminal position, where the ISO would be located with a finite probability. The large state uncertainty of the ISO is formally handled here through the hierarchical property in stochastically contracting nonlinear systems. We then propose a method to find the terminal positions of the multiple spacecraft optimally distributed around the ellipsoid, which locally maximizes the information we can get from all the points of interest (POIs). This utilizes a probabilistic information cost function that accounts for spacecraft positions, camera specifications, and ISO position uncertainty, where the information is defined as visual data collected by cameras. Numerical simulations demonstrate the efficacy of this approach using synthetic ISO candidates generated from quasi-realistic empirical populations. Our method allows each spacecraft to optimally select its terminal state and determine the ideal number of POIs to investigate, potentially enhancing the ability to study these rare and fleeting interstellar visitors while minimizing resource utilization.","Interstellar objects (ISOs), astronomical objects traveling through space without any attachment to any system, present a unique opportunity to study various aspects of the universe, such as the formation and composition of other star systems, the origin of the universe, and possibly other forces at play in the expanses of space [1]. For example, one of the two ISOs observed to date [2, 3], named 1I/‘Oumuamua, visited us from the rough direction of the constellation Lyra exhibiting some unusual physical characteristics (a highly elongated shape, a lack of typical cometary volatiles, and a deviation from Keplerian trajectories [4, 5]), which might provide some clues into how our solar system and neighboring exoplanetary star systems are formed. Exploring these interstellar visitors in situ with dedicated spacecraft would offer an alternative means to acquire firsthand knowledge about interstellar space, which goes beyond what is obtained through remote observations via telescopes. Due to the hyperbolic nature of their orbit, ISOs only pass through the solar system once in their lifetime with inherently large and rapidly changing uncertainty in their state, often at high inclination and relative velocities [6]. This poses significant challenges in designing a guidance, navigation, and control (GNC) strategy for the ISO encounter, requiring fast response autonomous operations even with the limited computational capacity of spacecraft. These would involve some levels of approximations in the GNC policies for the sake of their onboard execution, which then introduces additional difficulties in formally quantifying the probability of a successful encounter. Furthermore, theoretical connections between this probability and our mission objective of maximizing the scientific return through the encounter (which we characterize by the amount of visual data collected by spacecraft cameras) still remain ambiguous. 1.1 Contributions This paper proposes a novel multi-spacecraft framework for locally maximizing the amount of information obtained through ISO encounters, with some probabilistic guarantees. Building on our previous work [7], we first derive a formal upper bound on the failure probability of the ISO encounter for hierarchical Itô stochastic nonlinear system, which models our spacecraft relative dynamics in the local-vertical local-horizontal (LVLH) frame [8, pp. 710-712] centered on the ISO with the large uncertainty in its state measurement. In order to achieve fast response autonomous operations even with the limited onboard computational capacity of the spacecraft, potential approximation errors in their control and estimation policies are explicitly considered when deriving the bound. The failure probability then enables the construction of an ellipsoid around the terminal position of the chief spacecraft, in which the ISO would be located at the terminal time with a finite probability. We further propose an approach to finding the terminal positions of the deputy spacecraft with respect to the chief spacecraft, optimally distributed around the ellipsoid to maximize the information we get from all the points of interest (POIs) in it. In particular, we utilize an information cost function characterized by the distance between the center of the ellipsoid and each spacecraft in the swarm and the visual coverage of the POIs, accounting for spacecraft positions and attitudes, camera specifications, and ISO position uncertainty, where the information here is defined as imaging data collected by spacecraft onboard cameras. Since this optimization only requires an upper bound for the ISO state uncertainty history over time, which we could obtain as in our previous work [6] with [9, 10], the optimal relative positions of the deputy spacecraft can be pre-computed offline, freeing onboard computational resources for some other highly autonomous tasks. Numerical simulations are performed to demonstrate the efficacy of our method using a quasi-realistic empirical population of ISOs [11, 9] with the empirical history of the navigation uncertainties [9, 10, 6, 7], where the optimization problems are solved using Nelder-Mead. Our first simulation examines the probability that a single spacecraft would be able to view the ISO, assuming an uncertainty ellipsoid with equal x, y, and z radii. As is expected from the aforementioned probability bound, the probability that the spacecraft views the ISO decreases as the ISO state uncertainty increases. Our second simulation empirically determines the optimal number of spacecraft to view as many POIs on the sphere as possible, where the size of the uncertainty sphere remains constant. It is demonstrated that a multi-spacecraft system both observes more POIs and results in a lower information cost than a singular spacecraft, however, an excess of spacecraft increases the overlap between the visual coverage of the system. 1.2 Related Work Previous studies have investigated the potential of ISO flyby missions for a single spacecraft, proposing viable methods for observing ISOs and other high-speed celestial objects [6, 7]. For example, it is rigorously shown that a control policy designed using the spectrally normalized deep neural network (SN-DNN) guarantees local, finite-time exponential boundedness of the expected spacecraft delivery error, assuming that the bounds for the estimation errors are known. These approaches would allow at least to encounter ISOs even under their large state uncertainty. However, the entirety of the ISO cannot be observed through a flyby as matching the spacecraft velocity with the ISOs’ is unrealistic with current propulsion technologies [9], making such missions inefficient for gathering valuable scientific data. Several numerical and theoretical methods have been proposed to circumvent these difficulties through the lens of information-based optimization. Previous works in this field with a swarm of spacecraft can be broken into two categories: gaining information from a body with a determined orbit, such as satellites or other spacecraft, and gaining information from an object with an undetermined orbit, such as celestial bodies. In [12], a passive observation technique is proposed for celestial bodies with known trajectories, whereas for the case of ISOs, an active approach should be taken to sufficiently capture information to address the large state uncertainty. Further, the algorithm utilized in the study only maximizes the amount of the celestial body covered by the field of view (FOV) of the spacecraft in the swarm. The overlap between spacecraft in the swarm must be minimized to make a more robust algorithm. This would prevent resources from being wasted by spacecraft redundantly examining the same area of the ISO. The collision avoidance techniques introduced in [13] utilize a minimum scalar distance that a spacecraft must be from each other. However, this constraint ignores any angular or directional constraints. With the highly uncertain trajectory of the ISO, it is crucial to minimize the amount of overlap in information gained by any two spacecraft. This consideration is crucial when optimizing information from bodies with controlled trajectories, such as during on-orbit inspection or servicing of satellites [14], [15]. These studies utilize an information cost to create a fuel and energy-efficient orbit around a target satellite. However, the information cost presented in these studies does not account for the spacecraft’s orientation or field of view. Instead, they determine a baseline distance for the spacecraft to start at and then algorithmically determine an orbit for the spacecraft to take. In the case of this study, orbiting the ISO would be infeasible again due to the ISO’s high relative velocity and the uncertainty of its dynamics. Some other studies focus on optimizing information from celestial bodies with uncontrolled trajectories; however, only comets and asteroids have been examined so far [16, 17]. In these studies, either the spacecraft that have to gain information are stationary, or the object they are observing is stationary and the spacecraft performs a flyby inspection of the celestial body. With an ISO, the spacecraft has to assume a formation in order to maximize the information gained, since the ISO’s state is uncertain and it never remains stationary. This paper proposes one solution to the issues listed above through the proactive deployment of multiple spacecraft, extending the ideas of information gain-based optimization. Hierarchical stochastic contraction also enables explicitly considering the state uncertainty of the ISO, having the spacecraft cooperate to gather visual data on all perspectives of the ISO locally optimally in one flyby mission."
https://arxiv.org/html/2411.09062v1,Multimodal Object Detection using Depth and Image Data for Manufacturing Parts,"Manufacturing requires reliable object detection methods for precise picking and handling of diverse types of manufacturing parts and components. Traditional object detection methods utilize either only 2D images from cameras or 3D data from lidars or similar 3D sensors. However, each of these sensors have weaknesses and limitations. Cameras do not have depth perception and 3D sensors typically do not carry color information. These weaknesses can undermine the reliability and robustness of industrial manufacturing systems. To address these challenges, this work proposes a multi-sensor system combining an red-green-blue (RGB) camera and a 3D point cloud sensor. The two sensors are calibrated for precise alignment of the multimodal data captured from the two hardware devices. A novel multimodal object detection method is developed to process both RGB and depth data. This object detector is based on the Faster R-CNN baseline that was originally designed to process only camera images. The results show that the multimodal model significantly outperforms the depth-only and RGB-only baselines on established object detection metrics. More specifically, the multimodal model improves mAP by 13% and raises Mean Precision by 11.8% in comparison to the RGB-only baseline. Compared to the depth-only baseline, it improves mAP by 78% and raises Mean Precision by 57%. Hence, this method facilitates more reliable and robust object detection in service to smart manufacturing applications.","Рис. 1: Overview of the RGBD-Man multimodal object detection framework. Employing Artificial Intelligence (AI) for automation of manufacturing has resulted in increased efficiency, precision, and flexibility and created a pardigm shift in the design of manufacturing systems. AI has been successfully applied to a vast array of manufacturing tasks in the industry [1, 2]. While AI-based methods have improved the manufacturing process, there are still challenges in ensuring that the AI-based black box systems continue to be reliable and robust. The most fundamental layer underlying all smart manufacturing systems is object detection, which allows the system to identify the type and position of the objects that it needs to handle. Object detection is a established computer vision problem, which involves identifying and categorizing specific objects of interest within a larger image by placing a bounding box around each detected object [3]. An effective automation system also requires proper sensor design to provide adequate coverage over the environment and allow the system to properly observe the scene and the objects [4]. The past decades have seen great advancement in sensor hardware. Camera resolutions have increased and they have become more affordable at the same time. Similarly, consumer applications have facilitated mass manufacturing of 3D sensors like lidars and stereo cameras, which are great sensors for smart manufacturing. Despite these improvements, sensors have inherent limitations rooted in their physics. For example, a single image captured by camera does not carry depth information. 3D sensors capture point cloud data which addresses this issue, but these sensors are typically low resolution and do not provide color information. An effective and reliable automation system requires selecting the right sensors and an object detection system that can effectively ingest the data provided by these sensors. Prior work has shown the limitations of object detection systems that rely solely on cameras [5, 6, 7, 8]. Image distortions like blur and noise can significantly lower the detection accuracy [5]. Although cameras provide color information, there are environments where there is low contrast between objects and the background, and as a result detection accuracy may suffer [9] and cameras may not be enough for handling these environments. Moreover, camera-based object detection systems are sensitive to illumination and can be fragile if there are changes to the lighting conditions. It has also been shown that illumination can negatively affect [6] camera-based object detection systems, because they struggle to generalize to operate under lighting conditions different from what they have experienced during training. Similarly, the performance of these systems diminishes in scenarios where objects vary significantly in size or when they blend indistinguishable with the background in camera’s view [7]. While 3D sensors are less sensitive to lighting, they pose their own set of challenges when used in industrial environments [10]. As an example, object detection systems using 3D sensors often struggle when the scene contains densely arranged objects [11]. In addition, presence of objects with similar shapes and sizes, or objects with repetitive patterns cause difficulties for systems relying on point clouds for object detection [12]. Another challenge with using point cloud data is the extra complexity of modeling 3D information and the slower speed of processing 3D information. This is a significant obstacle, particularly for manufacturing environments which require real-time object detection and therefore cannot afford too much complexity and computational overhead [13, 14]. These conditions emphasize the need for advanced detection techniques that can take advantage of 3D sensors for improved performance while keeping the system simple and computationally efficient. Since different sensors can be complementary and cover each other’s blind spots and weaknesses, it makes sense to create object detectors which can leverage the strengths of multiple sensors to improve accuracy and dependability rather than relying on a single sensor. Multimodal object detection [15, 16], which utilizes data from multiple sensor modalities has the potential to address the above-mentioned limitations of single-sensor systems. Integrating information from multiple sensors can be achieved via the sensor fusion process [17], which can produce more accurate and more reliable data for the object detection model. Different types of sensor fusion have been studied [18] in the past. Early fusion methods merge sensor data at the input stage. Intermediate fusion methods combine features derived from different sensors at an intermediate layer of a model. Lastly, late fusion approaches aggregate decisions proposed from different sensors at the final stage in the model. All types of sensor fusion can contribute to overcoming the limitations of single-sensor systems. Prior work has explored multimodal object detection. One notable approach [19] added a depth branch to the Faster-RCNN architecture to process depth in parallel with the RGB data. The depth branch created feature maps from depth inputs and these feature maps were concatenated with feature maps generated from RGB images. The authors showed that using depth allowed the model to succeed in some scenarios where color information alone could not distinguish objects from their backgrounds [19]. Similarly, Zhu et al. [20] added a depth processing branch to the Faster R-CNN framework which enabled their model to handle both RGB and depth images for enhanced object detection. Their method employed two distinct CNNs to extract features from RGB and depth images independently. They also used depth information to delineate object edges more clearly and distinguish them from their backgrounds. Following feature extraction, the features were aligned and merged using a feature fusion layer, which implemented sum fusion, max fusion, and concatenation fusion strategies. Garbouge et al. [21] presented an approach that integrated RGB and depth data at an early stage by stacking them into a four-channel input for a CNN inspired by the AlexNet architecture for a classification task and showed that employing depth information improves object classification metrics. However, none of the existing methods have explored efficient four-channel RGB+D inputs in the context of object detection tasks. The processing of RGB and depth data through a single shared backbone offers several advantages. Firstly, when RGB and depth data are stacked at the input level, the model is able to efficiently extract features that merge information from both modalities, for example color and texture from RGB alongside spatial and structural details from depth. This integrated approach enables the network to learn more meaningful features early in the feature extraction process. By processing both data types simultaneously, the network can better generalize to unseen data as it learns to recognize and interpret patterns across both modalities. Additionally, employing a single backbone for processing reduces the computational overhead significantly. This computational efficiency comes from managing only one set of weights during back-propagation and inference. This approach creates a leaner model which is faster and less costly to run in a manufacturing plant. Lastly, the lower architectural complexity and lack of complex fusion mechanisms simplifies the development and tuning process for training and deploying the final model in the manufacturing environment. This work introduces a novel method that employs early sensor fusion, where data from RGB images and 3D point clouds are integrated at the data level before being used by the detection algorithm. This method utilizes the combined strengths of both modalities for a more comprehensive representation of the scene that enhances object detection capabilities. Early fusion facilitates the extraction of comprehensive features that embody both the visual and spatial attributes of objects, which addresses the challenges posed by occlusions, variable sizes, and complex background information more effectively than late fusion or single-sensor methods. Hence, this work presents a novel model tailored for processing RGB-D images for object detection tasks in manufacturing. The rest of the paper is organized as follows. Section 2 discusses the methodology of the multimodal object detection framework. Section 3 outlines experiments which compare this approach to both RBG-only and Depth-only object detection of a NIST manufacturing task board. Section 4 discusses the experiment results, and Section 5 discusses the conclusions and suggestions for future work."
https://arxiv.org/html/2411.08851v1,Experience-based Subproblem Planning for Multi-Robot Motion Planning,"Multi-robot systems enhance efficiency and productivity across various applications, from manufacturing to surveillance. While single-robot motion planning has improved by using databases of prior solutions, extending this approach to multi-robot motion planning (MRMP) presents challenges due to the increased complexity and diversity of tasks and configurations. Recent discrete methods have attempted to address this by focusing on relevant lower-dimensional subproblems, but they are inadequate for complex scenarios like those involving manipulator robots. To overcome this, we propose a novel approach that constructs and utilizes databases of solutions for smaller sub-problems. By focusing on interactions between fewer robots, our method reduces the need for exhaustive database growth, allowing for efficient handling of more complex MRMP scenarios. We validate our approach with experiments involving both mobile and manipulator robots, demonstrating significant improvements over existing methods in scalability and planning efficiency. Our contributions include a rapidly constructed database for low-dimensional MRMP problems, a framework for applying these solutions to larger problems, and experimental validation with up to 32 mobile and 16 manipulator robots.","Multi-robot systems are important in daily life and in various industrial applications. From manufacturing and warehouse management to surveillance and delivery services, multi-robot systems significantly enhance efficiency and productivity. In these environments, the interactions between robots during mobile or manipulation tasks are often repetitive and exhibit common patterns, suggesting that there is significant potential to learn from and reuse previous experiences. Experience-based planning leverages stored solutions from past scenarios to improve efficiency in new planning tasks by reusing relevant knowledge. To date, most of the research on experience-based motion planning has focused on single robot problems, where the planning space is much simpler and more manageable to learn. In this paper, we propose an innovative experience-based planning approach for large and complex MRMP problems. An analysis from our previous work, ARC [1], indicates that in MRMP problems, even with large numbers of robots, most conflicts require the coordination of only a few robots. Thus, instead of attempting to capture a comprehensive set of experiences for an entire team, we focus on the most relevant interactions among smaller groups of robots. These interactions are modeled as lower-dimensional subproblems, which are stored in a compact database. Since these subproblems involve only a few robots, learning their planning spaces requires fewer experiences, resulting in a database that is both smaller and quicker to compute. This approach enables us to efficiently solve these subproblems and, in turn, tackle larger and more complex scenarios involving many robots. (a) (b) (c) Figure 1: A simplified overview of our method: a) Detect a conflict between two robots’ paths. b) Define a local subproblem around the conflict. c) Retrieve the best solution from the database and solve the subproblem. Experience-based planning has proven effective for single-robot motion planning by using databases to store and retrieve meaningful experiences (paths) for solving new problems [2, 3]. While these methods have been adapted to multi-robot scenarios in restricted cases, such as grid-like Multi-Robot Pathfinding (MAPF) [4, 5, 6], they primarily address complexity by focusing on lower-dimensional subproblems that involve only the robots in conflict. However, the grid-based, discrete approach of these methods limits their application to high-dimensional, continuous environments like those required for manipulator robots, where flexible and adaptable high-dimensional solutions are essential. Our work addresses the primary limitation of experience-based planning in MRMP: the exponential growth of the solution database with increasing problem size and complexity. By focusing on lower-dimensional subproblems, we can capture a substantial number of robot interactions and store them efficiently in a compact database. We leverage our previous work [1] to identify and create these subproblems within the larger MRMP context. As a result, we can effectively use a smaller, more manageable database to solve these subproblems, allowing us to tackle larger and more complex scenarios. Additionally, most conflicts require coordination of only a few robots [1], making solutions for larger groups rarely necessary. This motivates the use of databases for smaller sets of robots. In this work, we present a novel, feasible, and cost-effective method to build usable databases by leveraging experience-based planning to solve complex MRMP problems with many robots, including both mobile and manipulator robots. Our method involves constructing and utilizing a database of precomputed solutions for smaller subproblems, such as those involving only 2-4 robots. We validate our method across various scenarios, including both mobile and manipulator robots arranged in different configurations, with and without obstacles. Our approach does more with less by utilizing a database that is smaller and up to 342 times faster to construct while still providing notable improvements in planning efficiency and scalability compared to state-of-the-art sampling-based and experience-based methods that require much larger databases. Our contributions are as follows: • An efficient, fast-constructing database for low-dimensionality MRMP problems. • A framework to use this low-dimensionality database effectively to leverage experience-based planning in solving large MRMP problems. • Experimental validation of this proposed method by solving complex problems with up to 32 mobile and 16 manipulator robots by using a smaller database that is up 342 times faster to construct."
https://arxiv.org/html/2411.08835v1,Goal-oriented Semantic Communication for Robot Arm Reconstruction in Digital Twin: Feature and Temporal Selections,"As one of the most promising technologies in industry, the Digital Twin (DT) facilitates real-time monitoring and predictive analysis for real-world systems by precisely reconstructing virtual replicas of physical entities. However, this reconstruction faces unprecedented challenges due to the ever-increasing communication overhead, especially for digital robot arm reconstruction. To this end, we propose a novel goal-oriented semantic communication (GSC) framework to extract the GSC information for the robot arm reconstruction task in the DT, with the aim of minimising the communication load under the strict and relaxed reconstruction error constraints. Unlike the traditional reconstruction framework that periodically transmits a reconstruction message for real-time DT reconstruction, our framework implements a feature selection (FS) algorithm to extract the semantic information from the reconstruction message, and a deep reinforcement learning-based temporal selection algorithm to selectively transmit the semantic information over time. We validate our proposed GSC framework through both Pybullet simulations and lab experiments based on the Franka Research 3 robot arm. For a range of distinct robotic tasks, simulation results show that our framework can reduce the communication load by at least 59.5% under strict reconstruction error constraints and 80% under relaxed reconstruction error constraints, compared with traditional communication framework. Also, experimental results confirm the effectiveness of our framework, where the communication load is reduced by 53% in strict constraint case and 74% in relaxed constraint case. The demo is available at: https://youtu.be/2OdeHKxcgnk.","As an emerging paradigm in industry, the Digital Twin (DT) is envisioned to enhance operational safety and reliability through integrating the physical world and digital world [1]. By reconstructing high-fidelity digital models of physical entities, DTs can comprehensively replicate real-world systems, thus enabling functionalities such as fault diagnosis [2], predictive maintenance [3], and optimal decision making [4]. However, the real-time reconstruction of digital models normally requires intensive communication resource, posing significant challenges to existing wireless networks [5]. This challenge becomes even more severe for digital robot arm reconstruction due to its high operating frequency and complex dynamics. It has been shown that a single control message can take up more than 100 bytes [6], and the throughput requirement for reconstructing an industrial-grade dual robot arm operated at 1ms scale in the DT can reach 138,500 bytes per second [7], not to mention its extension to large-scale industrial scenarios with thousands of robot arms operating simultaneously. Meanwhile, observations from existing testbed indicate that the frequent control message transmissions without considering the importance of data can lead to data congestion due to the buffer overflow at the receiver end [8]. Therefore, there is an urgent need to reduce the communication load for robot arm reconstruction in DTs. To tackle this, existing works mainly focused on designing the goal-oriented/task-oriented framework for DT reconstruction, with the aim of reducing communication cost while maintaining the reconstruction error in acceptable levels. In the context of robot arm DT, reconstruction error is commonly characterised by effectiveness-level performance metrics such as the mean square error (MSE) [9][10] and Euclidean Distance [11]. In [9], the authors implemented movement prediction algorithms at the DT side to compensate for packet loss and mitigate the Root-MSE (RMSE) between the digital robot arm trajectory and the ground truth. This method was further exploited to develop a communication and prediction co-design framework [10], where a deep reinforcement learning (DRL) algorithm jointly optimises the real-to-simulation updating frequency and the prediction window, to minimise the communication load subject to the MSE constraint on trajectory difference. Extending from [10], the authors in [11] defined the Euclidean distance between the positions and orientations of the physical robot and its DT as the effectiveness metric. They further reduced the communication load by introducing expert knowledge to the DRL algorithm and refining the reward design. The above task-oriented frameworks have shown improvements in real-time DT reconstruction, but there still exists two research gaps. First, the trajectory difference cannot comprehensively describe the DT reconstruction quality since the robot dynamics (e.g., velocity difference) might introduce deviations in the reconstruction process, this motivates us to refine the effectiveness-level design in this work. Second, these frameworks tend to transmit all generated messages without considering the significance and usefulness of their contents, i.e., the semantic-level problem is unsolved. To tackle the potential network congestion and increased latency caused by excessive message transmission in goal-oriented frameworks [12], the concept of semantic communication has been proposed[13]. By extracting and transmitting the semantic information behind the original bits, it can significantly reduce the length or transmission frequency of original messages. Prior works have applied semantic communication to a wide range of traditional data types, such as text [14], speech [15], image [16], and video [17]. Meanwhile, semantic communication has also been leveraged to filter and compress the control message or sensor data in reconstruction tasks [18, 19, 20, 21, 22]. The Age of Information (AoI) [18] and its variants (e.g., Age of Incorrect Information [19] and ultra-low AoI [20]) have been commonly used as the semantic metrics to evaluate the importance of the time-critical sensor data. They in essence characterise the data freshness, and the freshest data are typically assigned the highest priority. In [21], the authors quantified the semantic value of the transmitted messages by combining the AoI with the similarity between adjacent control messages to improve the communication efficiency for the unmanned aerial vehicle (UAV) control task. The authors in [22] were among the first to study semantic communication for DT robot arm reconstruction, where the authors optimised the communication by discarding the useless information based on the robot arm’s current motion. However, they only considered wired connection and their approach might cause the DT to lag behind the real-world. The aforementioned works [14, 15, 16, 17, 18, 19, 20, 21, 22] have explored the benefits of semantic communication extensively, but inevitably share a common limitation. That is, they ignored that the semantic value of messages is not only dependent on their context, but also closely coupled with the specific communication goal. To take the advantages of both the goal-oriented framework and semantic communication, an integrated goal-oriented semantic communication (GSC) framework was proposed in [23]. It jointly considers the semantic-level information and effectiveness-level performance metrics for multi-modal data in various tasks. This framework was further implemented in the context of UAV trajectory control [24]. The authors utilised a joint function of AoI and Value of information to identify the most important control and command data, with the GSC goal of minimising the trajectory MSE. Another GSC framework extending from [23] was proposed for the point cloud-based avatar reconstruction in the Metaverse [25], where only the critical nodes of the avatar skeleton graph are transmitted to minimise bandwidth usage. It can be seen that the GSC framework has been developed for various scenarios, but its application for robot arm reconstruction in DT has never been studied yet, where the communication efficiency needs further improvement, and both the effectiveness-level metrics for the reconstruction task as well as the semantic-level information remain unknown. Motivated by this, we propose a novel GSC framework for robot arm reconstruction in DT. Compared with the existing frameworks [9, 10, 11, 22], we not only analyse the specific contents of reconstruction messages to identify the most critical GSC information for transmission, but also perform temporal selection to transmit only the most important messages at critical moments without degrading the reconstruction quality. Additionally, we incorporate the velocity difference between the physical and digital robots into the effectiveness-level metrics, to ensure the DT and the physical world share the same dynamics during the reconstruction process. Our main contributions are summarised as follows: • We consider a real-time DT reconstruction task for a physical robot arm that performs three different tasks, including pick-and-place, pick-and-toss and push-and-pull tasks. The physical robot arm transmits reconstruction messages to the DT, with the goal to accurately replicate its dynamics, states, and real-time motions with reduced communication cost. • At the effectiveness-level, we jointly consider the impact of robot dynamics and kinematics, and define the goal-oriented performance metrics as the reconstruction error, which includes the joint angle error and joint velocity error of the robot arm. At the semantic-level, we reveal that the significance of different message contents (i.e., different features111We refer to features as different components of the reconstruction message in this work, not the general concept of features in machine learning.) changes based on robot’s current movement, with certain contents lacking semantic information at specific times as they do not affect the reconstruction accuracy. Meanwhile, we capture the temporal features of the reconstruction messages and show that dropping redundant or less useful messages barely affect real-time DT reconstruction. • We formulate the GSC goal as minimising the communication load under the DT reconstruction error constraints. To achieve this, we propose a GSC reconstruction framework that incorporates the effectiveness-level and semantic-level designs. Specifically, we develop a Feature Selection (FS) algorithm that can divide the robotic task into several phases (e.g., grasp and release), and only transmit message contents that contain GSC information for the current phase. Building upon this, a Proportional-Integral-Derivative-based Primal-dual Deep Q-Network (PPDQN) algorithm is designed to evaluate the importance of the reconstruction message at current time slot, and discard the redundant or less useful ones. • Our proposed framework is validated via both Pybullet simulations and experiments using the Franka Research 3 robot arm. For the three different robotic tasks, our proposed GSC framework can reduce the communication load by at least 59.5% under strict reconstruction error constraints and 80% under relaxed reconstruction error constraints in the simulations. Experimental results further validate our framework, as the communication load is reduced by 53% and 74% in the strict error constraints case and relaxed error constraint case, respectively. The rest of this paper is organised as follows: Section II presents the system model and the formulated problem. Section III and Section IV introduce the traditional DT reconstruction framework and our proposed GSC reconstruction framework. In Section V, the results of the simulations and physical experiments are presented to verify our proposed framework. Finally, we conclude our work in Section VI."
https://arxiv.org/html/2411.08832v1,Offline Adaptation of Quadruped Locomotion using Diffusion Models,"We present a diffusion-based approach to quadrupedal locomotion that simultaneously addresses the limitations of learning and interpolating between multiple skills (modes) and of offline adapting to new locomotion behaviours after training. This is the first framework to apply classifier-free guided diffusion to quadruped locomotion and demonstrate its efficacy by extracting goal-conditioned behaviour from an originally unlabelled dataset. We show that these capabilities are compatible with a multi-skill policy and can be applied with little modification and minimal compute overhead, i.e., running entirely on the robot’s onboard CPU. We verify the validity of our approach with hardware experiments on the ANYmal quadruped platform.","Quadruped robots’ ability to traverse complex terrain while carrying useful payloads makes them excellent choices for applications in manufacturing, construction and search & rescue. The state of the art for quadruped locomotion is maturing rapidly and both learning-based and gradient based methods are prevalent in research and industry. Learning-based methods such as reinforcement learning (RL) have produced impressive results [1, 2, 3, 4, 5, 6], but still suffer from a number of limitations. Due to the limited expressive capacity of neural network policies, using multiple skills in previous works required a hierarchical approach with a difficult multi-step training pipeline [5]. Since the methods are trained online, the resultant policies often can’t be adapted to new behaviours without being retrained from scratch. Our approach attempts to propose a solution to each of these two limitations in particular. To alleviate these issues, an alternative learning-based paradigm, imitation learning, can be used. This involves reproducing a set of reference motions, which popular approaches achieve with an adversarial training loop [7]. In recent years, Diffusion models [8, 9] have outperformed prior methods such as [10] and are able to approximate multi-modal distributions at a high fidelity. By approximating the score function [11] of a distribution, instead of directly learning the densities, and iteratively applying Langevin dynamics [12] to generate samples, they are able to accurately represent highly multi-modal distributions, incorporate flexible conditioning [13], and also scale well with dataset and model size. This multi-modality allows us to circumvent the need for hierarchical planning, as previous works have shown diffusion policies are capable of learning multiple skills in a single model [9]. Figure 1: Our diffusion-based approach possesses two capabilities: interpolation between distinct skills or operation modes (walk to a low crawl) and offline adaption after training. We leverage diffusion models to approximate multi-modal distributions to a high fidelity and also show that using classifier-free guidance can achieve offline adaption after training to produce novel locomotion behaviours. The guidance is used to generate trajectories that maximise the return of new reward functions unseen during data collection. Though large datasets are prevalent and can fuel imitation learning, there are limitations. There is no guarantee that the dataset contains trajectories which maximise a desired test time reward. For example, our locomotion dataset may contain a broad range of motions where the robot is moving with different gaits and speeds, but at test time we require specific behaviours. If data is labelled it is easy to extract this desired behaviour, but this is often not the case making this problem challenging. Therefore, we would like to tune our diffusion model to stitch together locomotion trajectories which maximise the return of reward functions for certain types of motion. To tackle the challenge of offline adaptation, we propose using classifier-free guidance (CFG) [14] to optimise a diffusion-model trajectories after training. This is achieved by treating the expected return as a probability function [13, 15] and using an Offline RL [16] training paradigm. In particular, we label the dataset with a velocity tracking reward and use CFG to guide our trajectories to maximise it, thus adapting our model to generate the desired behaviour. In this paper we present a diffusion-based approach to quadruped locomotion, capable of switching between discrete skills and adapting to new reward functions offline. Our contributions are as follows: 1. We present the first application of a stochastic differential equation-based diffusion framework to quadruped locomotion and use it to train a policy capable of interpolating between discrete skills. 2. We apply classifier-free guidance to perform Offline RL and adapt our policy to new goal-conditioned behaviour. 3. We deploy onto real hardware and demonstrate the benefits of fast sampling with a model that can run entirely on the robot’s onboard CPU. We evaluate our approach on a set of locomotion tasks both in simulation and on hardware using an ANYbotics ANYmal quadruped robot [17]."
https://arxiv.org/html/2411.08727v1,Voxeland: Probabilistic Instance-Aware Semantic Mapping with Evidence-based Uncertainty Quantification*,"Robots in human-centered environments require accurate scene understanding to perform high-level tasks effectively. This understanding can be achieved through instance-aware semantic mapping, which involves reconstructing elements at the level of individual instances. Neural networks, the de facto solution for scene understanding, still face limitations such as overconfident incorrect predictions with out-of-distribution objects or generating inaccurate masks. Placing excessive reliance on these predictions makes the reconstruction susceptible to errors, reducing the robustness of the resulting maps and hampering robot operation. In this work, we propose Voxeland, a probabilistic framework for incrementally building instance-aware semantic maps. Inspired by the Theory of Evidence, Voxeland treats neural network predictions as subjective opinions regarding map instances at both geometric and semantic levels. These opinions are aggregated over time to form evidences, which are formalized through a probabilistic model. This enables us to quantify uncertainty in the reconstruction process, facilitating the identification of map areas requiring improvement (e.g. reobservation or reclassification). As one strategy to exploit this, we incorporate a Large Vision-Language Model (LVLM) to perform semantic level disambiguation for instances with high uncertainty. Results from the standard benchmarking on the publicly available SceneNN dataset demonstrate that Voxeland outperforms state-of-the-art methods, highlighting the benefits of incorporating and leveraging both instance- and semantic-level uncertainties to enhance reconstruction robustness. This is further validated through qualitative experiments conducted on the real-world ScanNet dataset.","I INTRODUCTION For mobile robots to effectively operate in complex, human-centered environments such as factories, healthcare facilities, or homes, achieving accurate scene understanding is a critical prerequisite. For example, stocking specific items on designated shelves in a medical supply room requires the robot to discern between different items, understand the layout of shelves, and perceive spatial relationships within its surroundings. This understanding is codified into a semantic knowledge base linked to the geometric model of the environment (i.e. a semantic map) [1], which enables the robot to reason about the location of items, recognize them, and interact by grasping or relocating as needed, and communicate with humans regarding these items. Among the different methods for building semantic maps, instance-aware approaches [2, 3, 4, 5] address this problem by annotating elements in a reconstructed 3D scene with semantic information such as object categories, functionalities, or physical characteristics. These elements are mapped at the level of individual instances, permitting to distinguish between multiple occurrences of the same object type within a scene. Traditionally, this task involves processing a sequence of RGB-D images with known poses, as outlined in previous works [2, 5]. Figure 1: Reconstruction at the instance and semantic levels of the sequence 096 from the SceneNN [6] dataset produced by the proposed method. While each different color in the instance map refers to a different instance, the colors in the semantic map represent the specific category indicated in the top-right legend. Additionally, both generated uncertainty maps are shown, being the blue areas those with minimum uncertainty, while red areas are those with the highest uncertainty. Initially, each incoming RGB image is processed using an instance semantic segmentation network that generates object instance predictions, each one labeled with a category and a confidence score. Subsequently, this information is projected to the 3D space using the corresponding depth image and is integrated with previous observations through semantic fusion strategies. Neural networks (NNs) have become the de facto technique for instance semantic segmentation because of their notable perception capabilities. However, when facing out-of-distribution objects (i.e. , objects with categories not present in the training data), their performance suffers considerable degradation, leading to erroneous predictions that are often supported by high confidence scores [7]. Additionally, the predicted masks frequently exhibit inaccuracies, either including parts of the background or over-segmenting objects [5]. Placing excessive trust on the network’s predictions makes the semantic map highly prone to these errors, leading to inconsistent representations. Thus, it becomes necessary to incorporate a systematic method for uncertainty quantification into the mapping process. As is common in many robotic tasks [8, 9], incorporating uncertainty into the semantic mapping process appears to be a logical and necessary approach. This uncertainty can be defined across the multiple network’s predictions for the same instance at i) the geometric level, to accurately delineate instance boundaries, and ii) at the semantic level, to differentiate between highly-confident classified objects and those requiring disambiguation. It must be stressed that state-of-the-art approaches often neglect this uncertainty by adopting simplistic fusion strategies, such as label counting or summing confidence scores [2, 5]. These methods typically assign the highest-scored object category to each instance after integrating all the observations, ignoring other possible categories regardless of how close their scores are to the maximum. In this work, we propose Voxeland, a probabilistic framework for the generation of instance-aware semantic maps, coping with uncertainty from the perspective of Evidential Learning [10] and rooting our formulation in the Dempster–Shafer Theory of Evidence (DST) [11]. Particularly, we interpret each neural network prediction –both the mask and the categorization– as a subjective opinion, which is accumulated over time to form evidence. Our approach handles two types of evidence: i) at geometric level, where voxels are employed as the representation primitive, and opinions are aggregated to update the belief regarding which object instance a voxel belongs to, and ii) at the semantic level, in which each object instance in the map is updated with the incoming opinions about its object category. The integration of these potentially conflicting pieces of evidence into a probabilistic framework is the main contribution of this work, enabling a quantification of both sources of uncertainty, and enhancing the reliability of the resulting semantic maps. As illustrated in Figure 1, leveraging uncertainty at the geometric level enables us to identify voxels where it is not clear to which instance they belong after multiple conflicting observations. Typically, these conflicts arise from inaccuracies in the 2D prediction masks, or noise in the 3D projection. Similarly, leveraging uncertainty at the semantic level allows distinguishing which instances may need to be reclassified as being out-of-distribution, or that have received too few observations to be confidently classified. As a way to exploit this, instances that require a disambiguation decision to discern their final category are evaluated with a Large Vision-Language Model (LVLM), which is provided with multiple observations of the object, the current evidence and its reconstructed geometry. To implement Voxeland, we build on top of Bonxai [12], an open-source library to efficiently store and manipulate volumetric data based on voxel-hashing, which we extend to provide a complete probabilistic framework for instance-aware semantic mapping. Additionally, we rely on probabilistic 3D occupancy mapping with binary Bayes filtering [13], enabling our method to adapt to dynamic scenes. To evaluate our proposal, a set of experiments carried out in multiple sequences from two real-world datasets, SceneNN [6] and ScanNet [14], demonstrates that our approach outperforms state-of-the-art competitors in instance-aware semantic mapping. Figure 1 illustrates the reconstruction of a scene from SceneNN, as well as the inferred uncertainty maps. In summary, our work provides the following contributions: • Voxeland, a complete probabilistic framework for instance-aware semantic mapping rooted on concepts from Theory of Evidence. This allows for the quantification of uncertainty in the predictions integrated into the semantic map. • Generation of uncertainty maps at both geometric and semantic levels, providing valuable insights for identifying areas of the map that require refinement. • An extensive evaluation on the real-world SceneNN dataset, on which main state-of-the-art approaches are evaluated, obtaining an average improvement of 5.6\% in instance-level segmentation accuracy. Additionally, the performance of our proposal is further validated with a set of qualitative experiments with the ScanNet dataset. • A C++ open-source extension to Bonxai that implements the proposed approach, publicly available as a ROS2 package at: https://github.com/MAPIRlab/Voxeland."
https://arxiv.org/html/2411.08661v1,Energy Optimal Traversal Between Hover Waypoints for Lift+Cruise Electric Powered Aircraft,"Advanced Air Mobility aircraft require energy efficient flight plans to be economically viable. This paper defines minimum energy direct trajectories between waypoints for Lift+Cruise electric Vertical Take-Off and Landing (eVTOL) aircraft. Energy consumption is optimized over accelerated and cruise flight profiles with consideration of mode transitions. Because eVTOL operations start and end in hover for vertical take-off and landing, hover waypoints are utilized. Energy consumption is modeled as a function of airspeed for each flight mode, providing the basis to prove energy optimality for multi-mode traversal. Wind magnitude and direction dictate feasibility of straight-line traversal because Lift+Cruise aircraft point into the relative wind direction while hovering but also have a maximum heading rate constraint. Energy and power use for an experimentally validated QuadPlane small eVTOL aircraft are characterized with respect to airspeed and acceleration in all flight modes. Optimal QuadPlane traversals are presented. Constraints on acceleration and wind are derived for straight-line QuadPlane traversal. Results show an optimal QuadPlane 500m traversal between hover waypoints saves 71\% energy compared to pure vertical flight traversal for a representative case study with a direct 4m/s crosswind. Energy optimal eVTOL direct trajectory definition with transitions to and from hover is novel to this work. Future work should model three-dimensional flight and wind as well as optimize maneuver primitives when required.","Advanced Air Mobility (AAM) [1, 2] promises fast and convenient transportation of goods and people with increasingly autonomous [3] electric vertical take-off and landing (eVTOL) vehicles. AAM missions include on-demand air mobility [4, 5] for passengers and heavy cargo as well as small Uncrewed Aircraft System (sUAS) operations such as mapping [6], surveillance [7], inspection [8], and package delivery [9]. Several AAM vehicle designs have been proposed [10] and are broadly categorized [11, 12, 13, 14] as Multirotor, Side-by-Side Helicopter, Lift+Cruise and Tilt-wing. Lift+Cruise eVTOL offers energy efficient cruise plus short-term hover capability. AAM eVTOL aircraft have three modes of operation - vertical (lift), forward (cruise) and transition flight. An AAM flight plan requires taking off vertically, then transitioning to forward flight mode while continuing acceleration to cruise speed. Near the destination, the eVTOL aircraft decelerates back to vertical flight mode to prepare for a vertiport landing. eVTOL aircraft, e.g., sUAS, may need to hover at specific waypoints for missions such as surveillance and inspection, resulting in multiple trajectory segments that require transitions from vertical to forward flight mode and vice versa. Early vertical take-off and landing (VTOL) aircraft such as the V-22 Osprey [15] required the flight crew to manually trigger flight mode transitions, but the transition sequence itself was fully automated to avoid aerodynamic instabilities inherent in the twin engine V-22 tilt-rotor design [16]. VTOL aircraft aerodynamic modeling and control design has since advanced by combining fixed-wing aircraft [17, 18] and multicopters [19] enabling autonomous transition control schemes as shown in [20, 21]. Autonomous transitions are necessary for ensuring safe and reliable operations but require accurate aircraft models for designing robust transition control schemes. Aerodynamic data is essential for developing aircraft models, however despite the release of several designs [10], experimentally validated aerodynamic data for eVTOL aircraft is not easily accessible with our small Uncrewed Aircraft System (sUAS) QuadPlane as an exception [22, 23]. Trajectory planning for fixed wing and multicopter sUAS has been done in zero wind [24, 25], yet some sUAS have operating airspeeds comparable to typical wind speeds. Energy-efficient trajectory generation in a wind field for a fixed-wing sUAS [26, 27] as well as for multicopters [28] has also been studied, but not for a transitioning eVTOL. Authors in [29] study energy efficiency of stop-n-go trajectories between hover waypoints using a quadcopter. Because AAM aircraft hover consumes far more energy than wing-lift-supported cruise flight, Lift+Cruise eVTOL aircraft will typically transition to cruise flight whenever possible. Optimal control approaches are applied in [21] using NASA’s Lift+Cruise aircraft concept [12], yet pure cruise mode is not utilized as vertical motors are spinning even at cruise airspeeds. This paper examines how Lift-Cruise eVTOL energy savings can be further improved with optimal flight mode switching between lift, cruise and transition flight modes. This paper presents a novel analysis of accelerated flight mode switching to derive energy optimal traversal between hover waypoints for Lift+Cruise aircraft in steady wind. To derive an energy optimal traversal solution, we first formally define all vehicle specific parameters affecting flight mode selection and straight-line acceleration, cruise and deceleration profiles. Axioms characterize energy consumption as a function of airspeed for Lift+Cruise aircraft across all three flight modes consistent with experimental data in [22]. Energy optimality is proven in zero wind for our multi-mode traversal solution over the set of feasible acceleration, deceleration and cruise airspeed values. Straight-line traversal is desired as the minimum distance solution, but in wind Lift+Cruise aircraft hover stably by pointing into the relative wind. Because relative wind direction rapidly changes with airspeed over accelerated flight, the feasibility of straight-line traversal depends on wind magnitude, direction, aircraft acceleration and heading rate limit. When straight-line traversal is infeasible, the use of maneuver primitives [30] is prescribed. An example cubic spline maneuver primitive is described over course angle around each hover waypoint to assure satisfaction of the aircraft heading rate constraint. An eVTOL sUAS QuadPlane [20] platform case study is explored due to its publicly available aerodynamic data [22, 23]. Note that QuadPlane [20] mode nomenclature is adopted in this work, i.e. vertical (lift) is ""Quad"" mode, forward (cruise) is ""Plane"" mode, and transitions occur in ""Hybrid"" mode. The primary contribution of this paper is definition of energy optimal traversal between adjacent hover waypoints for Lift+Cruise aircraft. Previous work has derived energy optimal trajectories with constant speed [27, 28] and accelerated flight [26, 21] but assumes operations in only a single flight mode. Our second contribution is definition of a feasibility constraint for straight-line traversal in a given steady wind magnitude and direction subject to aircraft acceleration and heading rate constraints. The final contribution of this paper is specification of power, energy, and energy optimal traversal profiles for experimentally validated [22] QuadPlane sUAS [20]. Explicit constraints on wind magnitude and direction partition cases for straight-line traversal versus maneuver primitive traversal in steady wind. Below, Section 2 defines coordinate frames and relevant aircraft state and wind parameters. Section 3 defines a straight-line traversal data structure and its specification to minimize energy cost. Next, Section 4 describes traversal in steady wind, including an algorithm to determine straight-line traversal feasibility and an example maneuver primitive to apply when necessary. Section 5 describes energy optimal traversal for the QuadPlane use case followed by concluding remarks in Section 6."
https://arxiv.org/html/2411.08622v1,"Precision-Focused Reinforcement Learning Model for
Robotic Object Pushing","Non-prehensile manipulation, such as pushing objects to a desired target position, is an important skill for robots to assist humans in everyday situations. However, the task is challenging due to the large variety of objects with different and sometimes unknown physical properties, such as shape, size, mass, and friction. This can lead to the object overshooting its target position, requiring fast corrective movements of the robot around the object, especially in cases where objects need to be precisely pushed. In this paper, we improve the state-of-the-art by introducing a new memory-based vision-proprioception RL model to push objects more precisely to target positions using fewer corrective movements.","Humans intuitively interact with objects in everyday situations, often without explicitly planning or thinking about how objects will behave. Non-prehensile object manipulation is an important skill for robots that are designed to assist humans. This work focuses on object pushing, a sub class of robotic manipulation that is crucial e.g. for service robots working in a kitchen, but it can also be beneficial for industrial applications. Consider a robot that has to grasp a cup that is placed on a shelf behind other objects. A simple strategy to reach the desired cup is to push the other items aside. Pushing can also be considered as an alternative to pick-and-place, e.g. if objects are too heavy or too large to be grasped. In addition, fragile objects that are likely to be damaged when grasped can be pushed. However, object pushing is demanding for robots due to the large variety of objects that all behave differently depending on their shape, size, mass, and friction. The task becomes even more challenging, considering that not all physical properties, such as mass or friction, are directly observable. These unknown properties can lead to the object overshooting its target position, requiring fast corrective movements of the robot around the object that are particularly difficult to model explicitly, as they require decisions about when a correction is necessary, how to adapt the pushing direction, and how to plan the corresponding movement. Corrections are even more challenging if the approach should generalize to objects with varying physical parameters. The difficulty of modeling such corrective movements is also evident in other recent work. \citeauthorcong_self-adapting_2020 [cong_self-adapting_2020] report that their model for object pushing based on a recurrent neural network (RNN) and model predictive control (MPC) cannot properly switch pushing sides, i.e. the model is not able to perform corrective movements. Additionally, the authors also train a RL agent as a model-free baseline. In contrast to the data-driven model, the RL agent learns to properly perform corrective movements. These results show that it is reasonable to further investigate RL in the context of object pushing. \citeauthorcong_reinforcement_2022 [cong_reinforcement_2022] therefore proposed a vision-proprioception model for planar object pushing, which is a state-of-the-art RL model that uses latent representations to encode the characteristics of an object. This vision-proprioception model is the starting point of this work. We investigate the following problem: a robot has to precisely push objects with varying physical parameters from starting positions to goal positions, both of which are randomly chosen. The main contribution of this work is the adaption of the vision-proprioception model to push objects more precisely to target positions by using fewer corrective movements around an object. Originally, the task was considered successful if the distance between the center of the object and the goal is smaller than 5\,cm [cong_reinforcement_2022], which is a threshold that is commonly used in the literature [plappert_multi-goal_2018, gallouedec_panda-gym_2021, xu_cocoi_2021]. However, this is a comparatively large tolerance, particularly if pushing is considered as an alternative for pick-and-place, which is often used to precisely reposition objects. Therefore, we decrease the threshold to 1\,cm. The vision-proprioception model is adapted by providing the agent with the complete episode history of observations, using a gated recurrent unit (GRU) [cho_learning_2014] as a feature extractor and improving the sampling of object parameters during training. Fig. 1 provides an overview of our approach. Figure 1: Schematic Overview. We adapt a state-of-the-art RL model to push objects more precisely to target positions by improving the sampling of object parameters and adding a gated recurrent unit to provide the agent with a memory. Figure 2: Model Architecture. We concatenate the Cartesian (x,y) EE position with the latent object and goal states generated by an encoder trained prior to the RL agent. The observations of the entire episode are stored in a memory buffer to be processed by a GRU-layer. The hidden state of the most recent time step is used as the feature vector for actor and critic MLPs."
https://arxiv.org/html/2411.08605v1,Lo-MARVE: A Low Cost Autonomous Underwater Vehicle for Marine Exploration,"This paper presents Low-cost Marine Autonomous Robotic Vehicle Explorer (Lo-MARVE), a novel autonomous underwater vehicle (AUV) designed to provide a low cost solution for underwater exploration and environmental monitoring in shallow water environments. Lo-MARVE offers a cost-effective alternative to existing AUVs, featuring a modular design, low-cost sensors, and wireless communication capabilities. The total cost of Lo-MARVE is approximately EUR 500. Lo-MARVE is developed using the Raspberry Pi 4B microprocessor, with control software written in Python. The proposed AUV was validated through field testing outside of a laboratory setting, in the freshwater environment of the River Corrib in Galway, Ireland. This demonstrates its ability to navigate autonomously, collect data, and communicate effectively outside of a controlled laboratory setting. The successful deployment of Lo-MARVE in a real-world environment validates its proof of concept.","††footnotetext: Proc. of the 12th International Conference on Control, Mechatronics and Automation (ICCMA 2024), London, UK, November 11-13, 2024, https://www.iccma.org/. 2024. Autonomous Underwater Vehicles (AUVs) have emerged as indispensable tools for exploring the vast and largely unexplored depths of the oceans. Their ability to operate independently, navigate complex underwater environments, and collect valuable data has revolutionized fields such as marine science, oceanography, and resource exploration. AUVs are equipped with advanced sensors, communication systems, and propulsion mechanisms, enabling them to perform tasks that would be impractical or dangerous for human divers. The development of robust and reliable AUV technologies is crucial for addressing pressing global challenges, including climate change, marine conservation, and sustainable resource management. Advanced AUVs, such as Vityaz-D, have reached depths of over 10km [Alekseev et al.(2021)]. These highly capable AUVs offer advanced capabilities and can reach significant depths. The main limitation of these advanced AUVs is that they can be prohibitively expensive. Purchasing such AUVs presents a barrier to researchers with modest budgets who wish to conduct research on AUVs. This provides the motivation for research into low cost AUVs. There have been many low cost AUVs published in the literature. For example LoCo was proposed in 2020 [Edge et al.([n. d.])]. Studies such as this demonstrate that it is possible to develop capable AUVs for lower cost. The proposed AUV aims to provide a more accessible and affordable option for researchers and organizations seeking to conduct underwater exploration and data collection. The main advantage of the proposed AUV is its low cost when compared to existing low cost AUVs. The contributions of this paper are: • The design and development of a very low cost AUV for marine robotics research. • To deploy the proposed AUV in the field to demonstrate it’s functionality. The rest of the paper is structured as follows. Section 2 will give an overview of the research in the literature in low cost AUV development. Section 3 will outline the design of the proposed ultra low cost AUV. Section 4 will then outline the experimental procedure for evaluating the AUV. The results of AUV testing are presented in Section 5. Finally, Section 6 will draw conclusions from this research."
https://arxiv.org/html/2411.08566v1,Grammarization-Based Grasping with Deep Multi-Autoencoder Latent Space Exploration by Reinforcement Learning Agent,"Grasping by a robot in unstructured environments is deemed a critical challenge because of the requirement for effective adaptation to a wide variation in object geometries, material properties, and other environmental factors. In this paper, we propose a novel framework for robotic grasping based on the idea of compressing high-dimensional target and gripper features in a common latent space using a set of autoencoders. Our approach simplifies grasping by using three autoencoders dedicated to the target, the gripper, and a third one that fuses their latent representations. This allows the RL agent to achieve higher learning rates at the initial stages of exploration of a new environment, as well as at non-zero shot grasp attempts. The agent explores the latent space of the third autoencoder for better quality grasp without explicit reconstruction of objects. By implementing the PoWER algorithm into the RL training process, updates on the agent’s policy will be made through the perturbation in the reward-weighted latent space. The successful exploration efficiently constrains both position and pose integrity for feasible executions of grasps. We evaluate our system on a diverse set of objects, demonstrating the high success rate in grasping with minimum computational overhead. We found that approach enhances the adaptation of the RL agent by more than 35 % in simulation experiments.","Robotic grasping in unstructured and dynamic settings is still one of the significant challenges in robotics. Generalization and adaptation to new objects, environments, and tasks remain limited with a robot, even with recent progress in Reinforcement Learning and Deep Learning. In particular, grasping tasks are constituted by a multitude of factors, including object density distribution, mass, surface properties, environmental conditions, and more — many of which are either not explored or ignored in current methods [1]. This results in lower adaptability and often suboptimal performance when robots encounter new or unexpected situations. All these methods are often fully dependable on visual or haptic sensor-based inputs only, which cannot capture the complexity of real-world grasping scenarios [2]. Moreover, most current grasping approaches ignore the dependency of task success on various environmental factors like humidity, temperature, lighting, and others. For instance, changes in friction between a gripper and an object will influence the force required for a stable grasp [5]. Similarly, many other environmental factors affect the grasping algorithms’ grasp quality and adaptation performance. Most well-performing strategies in a controlled environment fail when applied in dynamic and unpredictable environments [6]. A limitation of the current RL-based grasping approaches is that they are unable to work adaptively or efficiently in new environments or with new objects for which they were not trained. Most current approaches require a large amount of training data or a great amount of time to fine-tune policies on novel tasks, preventing them from being employed in applications that profit most strongly from real-time performance in changing environments [8]. Further, this is complicated by high-dimensionality observation and action spaces for robotic manipulation tasks, making the learning process both slow and sample-inefficient [4]. In order to curb these challenges, there have been propositions for latent space representations; however, most do not encompass some critical physical properties of an object, such as mass, center of mass, or surface friction, which are fundamental for accomplishing a precise and stable grasp [9]. I-A Grammarization of Grasping Components Grammarization is defined as the process of abstracting and encoding the physical properties and behaviors of multiple objects simultaneously into a set of computable metrics that, while preserving their essential information for the purpose that grammarization was performed. Mathematically, the grammarization process can be formalized as follows: Given a feature vector f\in\mathbb{R}^{n} representing an object’s features and characteristics (extracted by any of the existing feature identification methodologies) such as mass, center of mass, geometrical features, and more, the grammarization function g:\mathbb{R}^{n}\to\mathbb{R}^{m} is defined such that: \boldsymbol{g}(\boldsymbol{f})=\left(\theta_{1}(f_{1},f_{2},...,f_{n}),\theta_% {m}(f_{1},f_{2},...,f_{n})\right)\in\mathbb{R}^{m},m\leq n where \theta_{i} represents a specific mapping (grammar rule) with which the same features f_{i}\in\mathbb{R} are correlated in a different way. This process is surjective (but not not necessarily injective), meaning that every element in the lower-dimensional space corresponds to at least one element in the higher-dimensional space, ensuring the copmression of the critical properties of the object, and deliberately allowing different feature representations \boldsymbol{f}\in\mathbb{R}^{n} and \boldsymbol{f^{\prime}}\in\mathbb{R}^{n} to lead to similar or even the same grammarized representations (\theta_{i}(f_{1},f_{2},...,f_{n})\approx\theta^{\prime}_{i}(f_{1},f_{2},...,f_% {n})). I-B Main Contributions The main contributions of our work can be summarized as follows: • Grammarization of Gripper and Target Object: We compress the high-dimensional properties of the gripper and target object into a common latent space using separate autoencoders, capturing both geometrical and physical parameters, providing a more comprehensive understanding of the manipulation scenario. • Reinforcement Learning in Latent Space: We introduce a reinforcement learning framework where the agent operates in the compressed latent space of the gripper-target correlation, enabling faster learning and adaptation by exploring a lower-dimensional yet highly informative space [7]. • Environmental and Physical Integration: Our framework integrates physical parameters (e.g., mass, friction, moments of inertia) and environmental factors (e.g., humidity, temperature, solar irradiation), broadening its applicability to dynamic real-world scenarios [5, 6]."
https://arxiv.org/html/2411.08533v1,ACROSS: A Deformation-Based Cross-Modal Representation for Robotic Tactile Perception,"Tactile perception is essential for human interaction with the environment and is becoming increasingly crucial in robotics. Tactile sensors like the BioTac mimic human fingertips and provide detailed interaction data. Despite its utility in applications like slip detection and object identification, this sensor is now deprecated, making many existing valuable datasets obsolete. However, recreating similar datasets with newer sensor technologies is both tedious and time-consuming. Therefore, it is crucial to adapt these existing datasets for use with new setups and modalities. In response, we introduce ACROSS, a novel framework for translating data between tactile sensors by exploiting sensor deformation information. We demonstrate the approach by translating BioTac signals into the DIGIT sensor. Our framework consists of first converting the input signals into 3D deformation meshes. We then transition from the 3D deformation mesh of one sensor to the mesh of another, and finally convert the generated 3D deformation mesh into the corresponding output space. We demonstrate our approach to the most challenging problem of going from a low-dimensional tactile representation to a high-dimensional one. In particular, we transfer the tactile signals of a BioTac sensor to DIGIT tactile images. Our approach enables the continued use of valuable datasets and the exchange of data between groups with different setups.","Tactile feedback is gaining significant attention in robotics[1, 2]. Tactile sensors leverage various information modalities, come in diverse shapes and sizes, and are implemented in a wide range of technologies. This diversity makes the exchange of data and trained models challenging. Moreover, as sensor technology improves, datasets become obsolete. For instance, BioTac by SynTouch was a high-end tactile sensor, designed like a human fingertip. It has an elastomer covering a rigid core filled with an incompressible conductive fluid. The sensor outputs voltage readings from 19 internal electrodes, capturing changes in the fluid. These readings are processed as time-series signal data [3, 4, 5]. The BioTac has been proven useful in various applications such as detecting object slips and the direction of slips [6, 7] or identifying objects [8]. However, this sensor is now deprecated. Consequently, many influential existing datasets, such as the BioTac SP direction of slip dataset [7], the BioTac SP grasp stability dataset [6] or the BioTac 2P grasp stability dataset [9], are now obsolete. These datasets capture sensor outputs, specifically BioTac signals, recorded while the sensors are mounted on robotic hands that grasp various objects under different conditions, with the stability of the grasps being evaluated. Despite their obsolescence, such datasets remain important, as grasp stability and slip detection continue to be an active field of research [10]. Furthermore, designing and collecting similar datasets is a time-consuming and complex task. It requires careful consideration of various factors, such as the choice of sensors and their resolution, the data collection methods, and the labeling process, among other requirements. Hence, there is a need to convert existing useful datasets into formats compatible with newer sensor modalities, even if they involve different robotic or sensor configurations. This allows researchers to leverage intrinsic information still relevant to specific tasks, while also saving time and resources by avoiding the need to collect entirely new datasets. To this end, we propose ACROSS, a versatile approach for transferring tactile data between sensors of varying resolutions, including low-to-high, high-to-high, and high-to-low resolution transfers [11]. We demonstrate the effectiveness of ACROSS by converting low-resolution tactile (time series) data from a BioTac sensor into a high-resolution vision-based DIGIT sensor [12]. Our method enables the utilization of existing datasets gathered with outdated sensors, avoiding the tedious process of gathering data from scratch. Moreover, it facilitates a way to transition between two intrinsically distinct tactile sensor modalities, e.g., signal data to visual representations. Additionally, we provide an openly available dataset comprising over 155K unique 3D mesh deformation pairs from interactions involving BioTac and DIGIT sensors. This dataset includes various types of indenters, the force exerted on each sensor, and rendered images of the scenes. The source code, dataset, and neural network checkpoints can be found on our website: https://wzaielamri.github.io/publication/across."
https://arxiv.org/html/2411.08499v1,Learning Robust Grasping Strategy Through Tactile Sensing and Adaption Skill,"Robust grasping represents an essential task in robotics, necessitating tactile feedback and reactive grasping adjustments for robust grasping of objects. Previous research has extensively combined tactile sensing with grasping, primarily relying on rule-based approaches, frequently neglecting post-grasping difficulties such as external disruptions or inherent uncertainties of the object’s physics and geometry. To address these limitations, this paper introduces an human-demonstration-based adaptive grasping policy base on tactile, which aims to achieve robust gripping while resisting disturbances to maintain grasp stability. Our trained model generalizes to daily objects with seven different sizes, shapes, and textures. Experimental results demonstrate that our method performs well in dynamic and force interaction tasks and exhibits excellent generalization ability.","I INTRODUCTION The grasping operation is a fundamental action commonly performed by robots [1]. Due to its versatility, it finds application in various fields such as warehousing [2], food processing [3], rescue operations [4], aerospace [5], and agriculture [6]. During the grasping process, factors such as the shape, state, material of the object being grasped, as well as the external environment, need to be considered. For example, when a robot holds a power drill to perform assembly tasks in a workshop, the high-frequency vibrations of the drill could cause it to slip out of the gripper. Therefore, exploring adaptive adjustment strategies for robotic grasping operations is crucial, ensuring robustness and stability even when faced with numerous external disturbances. Robots generally lack prior knowledge about objects, making the stability of the grasping process susceptible to issues such as sliding vibrations or changes in mass [7]. For example, when a robot uses one arm to grasp a cup while the other fills it with a beverage, changes in the object’s mass or weight exceeding the estimated value can alter the grasping configuration, thus requiring a stable immediate response. Figure 1: Human demonstration and Flexible tactile sensor. A human controls the gripper manually to change the parameter \theta as shown in the figure, with with adding water as an external disturbance. This figure details the mechanical structure and tactile visualization of the tactile sensor. To perceive and understand these external disturbances during the grasping process, tactile sensors are indispensable. Previous research has extensively explored the role of tactile perception in robust grasping and has proposed various methods to enhance the robustness and stability [8], [9], [10]. Specifically, these studies utilizing tactile feedback mainly focus on uncertainties originating from the object’s geometric model [11], position [12], and external disturbances [13]. These uncertainties can directly affect the relative configuration between the robotic hand and the object, thus directly impacting the grasping stability. For example, an object-level impedance control strategy was introduced in [14], proposing a robust method to handle grasping uncertainties. Even when a person applies arbitrary pulling actions on the object being grasped, the Allegro hand can continuously adjust its state through tactile feedback to maintain a stable grasp. The initial grasp in this method is pre-defined rather than learned, which may reduce its robustness in complex situations. In [15], a method of simulating tactile data near the grasp position to evaluate and correct the grasp of new objects is proposed. However, since only a single corrective action is performed, the robotic hand may accidentally knock over the object, preventing re-establishment of contact and thus failing to find additional adaptive measures, which leaves potential for grasping failure. In [16], a method for detecting rotational displacement of objects caused by non-center-of-mass grasping is proposed. Researchers, utilizing the visual-tactile information provided by GelSight, integrate the rotational detection feedback into a closed-loop re-grasping framework to robots. A haptic event-based grasping algorithm is proposed in [17], which uses predefined events as state transition triggers, inspired by human grasping behavior (hold, place, unload). This means that each step of the system is predefined, and transitions between states are triggered by specific haptic events, which limits its adaptability in complex and dynamic environments. To sum up, these previous studies have largely focused on predefined and logic-based controllers and have concentrated more on single adjustment actions rather than continuous actions. To this end, in this paper we present an human-demonstration-based adaptive grasping policy for resisting any external disturbances (human interface, vibration, deformation) and validated on an electric gripper. Specifically, first, we collect stable grasp data by having humans manually control the gripper. Next, for the adaptive strategy, we design an initial grasp generator, a stability estimator, and an adaptive grasp strategy. To begin with, an initial grasp action is generated by the grasp generator. Essentially, the stability estimator functions as a probabilistic classifier, where an output below a given threshold indicates an unstable grasp. Once the grasp adapter proves the current grasp to be unstable, it adjusts using parameters learned from skilled demonstrations. Finally, we train the adaptive strategy using a neural network based on self-attention mechanisms. Further, we conduct grasp stability tests on seven different objects. Experimental results show that the strategy successfully handles arbitrary dragging by humans and disturbances such as adding water to containers, demonstrating the method’s potential in robotic grasping. Briefly, our contributions can be summarized as follows: 1) A grasp generator and a stability estimator for the adaptive grasping process is presented. 2) A real-time dynamic adjustment strategy for robots based on human demonstrations is proposed. The reminder of this paper is organized as follows: The modeling and analysis of the proposed adaptive grasping policy are shown in Section II. The experimental results are presented in Section III to demonstrate the effectiveness of this method, followed by a conclusion in Section IV, with a discussion on the limits of the current design."
https://arxiv.org/html/2411.08474v1,"A Cost-effective, Stand-alone, and Real-time TinyML-Based Gait Diagnosis Unit Aimed at Lower-limb Robotic Prostheses and Exoskeletons","Robotic prostheses and exoskeletons can do wonders compared to their non-robotic counterpart. However, in a cost-soaring world where 1 in every 10 patients has access to normal medical prostheses, access to advanced ones is, unfortunately, extremely limited especially due to their high cost, a significant portion of which is contributed to by the diagnosis and controlling units. However, affordability is often not a major concern for developing such devices as with cost reduction, performance is also found to be deducted due to the cost vs. performance trade-off. Considering the gravity of such circumstances, the goal of this research was to propose an affordable wearable real-time gait diagnosis unit (GDU) aimed at robotic prostheses and exoskeletons. As a proof of concept, it has also developed the GDU prototype which leveraged TinyML to run two parallel quantized int8 models into an ESP32 NodeMCU development board (7.30 USD) to effectively classify five gait scenarios (idle, walk, run, hopping, and skip) and generate an anomaly score based on acceleration data received from two attached IMUs. The developed wearable gait diagnosis stand-alone unit could be fitted to any prosthesis or exoskeleton and could effectively classify the gait scenarios with an overall accuracy of 92% and provide anomaly scores within 95-96 ms with only 3 seconds of gait data in real-time.","From birth defects to unexpected accidents, since the dawn of the human race, people have been consistently exposed to the unforgiving consequences of losing their limbs leaving them incapable of standard human mobility. However, the untamable human nature to overcome imposed limitations has prompted them to fight back against the unfavourable consequences of losing limbs with various types of prostheses dating back to early Egyptian civilization in 950 B.C. [1]. Prostheses, as widely considered, are artificial limbs dedicated to mimicking or compensating the functionalities of the lost limbs to some partial extent. Robotic prostheses or powered prostheses are an upgrade to the traditional mechanical prostheses that harness the capabilities of modern electro-mechanical components and intelligent control systems to provide more accurate gaits [2]. The concept of exoskeletons, on the other hand, originated with the goal of augmenting the pre-existing mobility state, either unimpaired or impaired. These devices are primarily dedicated to rehabilitation purposes such as allowing patients to walk after losing locomotion due to spinal cord injuries, stroke, or other causes, which do not necessarily require the absence of limbs, unlike prostheses. Modern robotic exoskeletons utilize mechanical or electro-mechanical components such as motors, hydraulic actuators, levers, and so on [3]. For a long time, the developments for these devices were only limited to mechanical advancements such as material improvements, joint improvements, and weight-minimization which, albeit made them comfortable and easier to use, could not drastically increase the fidelity of these devices compared to the natural locomotion of the lost limb or human-body in general. The incorporation of modern electro-mechanical components transformed the landscape of medical research, ushering in a new era of robotic rehabilitation with improved prostheses and exoskeletons. The advent of robotic prostheses and exoskeletons in the domain of medical science has led to a much-needed paradigm shift in terms of the capability of these devices. However, despite the promises and the huge potential robotic prostheses and exoskeletons offer compared to their traditional counterpart, some hindrances such as the cost and complexity are rather amplified, making them less accessible [3]. Figure 1: Disparity in accessibility to prostheses and assistive devices: Demand vs. Access and the major catalyst. Summarized in figure 1, a 2017 global estimate by the World Health Organization (WHO) stated that up to 40 million people were reported to have been in need of medical prostheses or assistive devices [4, 5] whereas, only 1 in every 10 people could access them due to their high cost and other impediments [4]. Although there is no strong numerical data dedicated to robotic prostheses and exoskeletons expressing such disparity, it is, for certain, much more severe as they are significantly costlier compared to their non-robotic counterpart. A major reason behind such high cost is the cost of the processing units leveraged for diagnosing the user gaits for any anomalies, and also for controlling the device. However, as evident in figure 2, although the cost is the primary impediment to making robotic prostheses accessible, there is an obvious lack of research focus on developing affordable robotic prostheses compared to the whole research domain dedicated to robotic prostheses hinted by the number of yearly publication indexed in the esteemed Google Scholar database. A common reason behind such a tendency is the trade-off between cost and performance as to alleviate the cost, less sophisticated equipment is to be used which is not adequate in a sensitive application such as medical prostheses where precision and performance are primary priorities. Considering the gravity of such issues in hand, this paper intended to delve deep into a comprehensive literature review exploring the recent advancements in the field of wearable gait diagnosis systems which could be potentially applied to robotic prostheses and exoskeletons. It was found that the gait diagnosis units (GDUs) leveraged were not only expensive but also lacked stand-alone capabilities in general. To address the research gap, this study proposed a wearable cost-effective (7.30 USD) stand-alone GDU based on TinyML. The quantized int8 Artificial Neural Network (ANN) model could successfully classify five types of gait scenarios -Idle, Walk, Run, Hopping, and Skip with an overall classification accuracy of 92%. Additionally, the device ran a K-means clustering model in parallel that generated an anomaly score for any unexpected movement scenarios. As a proof of concept, the wearable GDU was integrated with a lower-limb exoskeleton prototype where it provided inference within 95-96 ms after taking just 3 seconds of gait data. Figure 2: Disparity in research focus: Robotic Prostheses vs. Affordable Robotic Prostheses based on the yearly number of publications. Next, Section 2 goes through a literature review of related works providing a glimpse of the state-of-the-art and the research gaps. The following section describes the methodology including the design of the GDU, data collection, processing, the parallel model architecture, and model deployment. Section 4 delineates the results in terms of model performance, average anomaly score for various age groups, and latency calculation followed by a discussion highlighting the strengths and limitations of this research. Finally, the study is concluded in Section 6 and future plans are briefed."
https://arxiv.org/html/2411.08447v1,Learning Dynamic Cognitive Map with Autonomous Navigation,"Inspired by animal navigation strategies, we introduce a novel computational model to navigate and map a space rooted in biologically inspired principles. Animals exhibit extraordinary navigation prowess, harnessing memory, imagination, and strategic decision-making to traverse complex and aliased environments adeptly. Our model aims to replicate these capabilities by incorporating a dynamically expanding cognitive map over predicted poses within an Active Inference framework, enhancing our agent’s generative model plasticity to novelty and environmental changes. Through structure learning and active inference navigation, our model demonstrates efficient exploration and exploitation, dynamically expanding its model capacity in response to anticipated novel un-visited locations and updating the map given new evidence contradicting previous beliefs. Comparative analyses in mini-grid environments with the Clone-Structured Cognitive Graph model (CSCG), which shares similar objectives, highlight our model’s ability to rapidly learn environmental structures within a single episode, with minimal navigation overlap. Our model achieves this without prior knowledge of observation and world dimensions, underscoring its robustness and efficacy in navigating intricate environments.autonomous navigation; active inference; cognitive map; structure learning; dynamic mapping; knowledge learning","Humans effortlessly discern their position in space, plan their next move, and rapidly grasp the layout of their surroundings (1, 2) when faced with ambiguous sensory input (3). Replicating these abilities in autonomous artificial agents is a significant challenge, requiring robust sensory systems, efficient memory management, and sophisticated decision-making algorithms. Unlike humans, artificial agents lack inherent cognitive abilities and adaptive learning mechanisms, particularly when confronted with aliased observations, where sensory inputs are ambiguous or misleading (4). To replicate human navigational abilities, an agent must capture the dynamic spatial layout of the environment, localise itself and predict the consequences of its actions. Most attempts to achieve this combine those fundamental elements in SLAM algorithms (Simultaneous Localisation and Mapping), often based on Euclidian maps (5, 6). However, these methods require substantial memory as the world expands. Other strategies involve deep learning models, which depend on large datasets and struggle to adapt to unexpected events not encountered during training (7). A more efficient alternative lies in cognitive graphs or maps and learning a mental representation of the world from partial observations (8, 9), creating a symbolic structure of the environment (10, 11). Cognitive graphs, by definition, represent a ”mental understanding of an environment” derived from contextual cues like spatial relationships (12). Alongside this structure, the ability to imagine the outcomes of actions enables more reliable navigation decisions based on preferences (13, 14). Our approach integrates those biological capabilities into a unified model. Using visual observations and proprioception (15), we construct a cognitive map through a generative model, enabling navigation with an Active Inference (AIF) framework. This model links states by incorporating observations and positions through transitions, as illustrated in Figure 1b), showing the processed observation of the agent and c) presenting the resulting cognitive graph. Figure 1: a) From a full 3 by 3 rooms mini-grid environment (16, 17) to b) rooms observation layout as perceived by the agent and the path it has taken between rooms - composed of a line from black to white-, c) shows the agent final internal topological graph (cognitive graph) linking all the locations between them. Using Bayesian inference, the model predicts future states and positions, growing its cognitive map by forming prior beliefs about un-visited locations. As new observations are made, the agent updates its internal model, dynamically refining its representation of the environment (11). This continual adjustment allows the agent to effectively navigate complex environments by anticipating and learning from uncharted areas (18). Our internal positioning system draws inspiration from the neural positioning system found in rodents and primates, aiding in self-localisation and providing an intrinsic metric for measuring distance and relative direction between locations (15, 3, 19). To achieve goal-directed navigation and exploration, we employ AIF to model the agent’s intrinsic behaviour in a biologically plausible way. Unlike methods relying on pre-training for specific environments, our approach introduces a navigation and dynamic cognitive map growing based on the Free Energy (FE) principle. This map is inspired by mechanisms observed in animals, such as border cells for obstacle detection (20) and the visual cortex for visual perception. It continuously expands by predicting new observations and adapting dynamically to changing environmental structures. This work aims to develop an autonomous agent that can determine where it is, decide where to navigate, and learn the structure of complex, unknown environments without prior training, mimicking the adaptability and spatial awareness observed in biological organisms. Traditional exploration approaches and deep learning models struggle in dynamic settings, requiring extensive memory and pre-collected datasets to predict future settings, or they face difficulties in adapting to untrained situations. The challenge is to design a model that allows agents to autonomously build, update, and expand an internal map based on current sensory data and past beliefs, efficiently managing ambiguous observations (such as aliased states) and responding flexibly to unexpected environmental changes. Our contribution to this problem encompasses several key aspects: • Proposing a novel dynamic cognitive mapping approach that allows agents to predict and extend their internal map over imagined trajectories, enabling anticipatory navigation and rapid adaptation to new environments. • Developing a navigation model that operates without pre-training or prior exposure, allowing the agent to successfully explore and make decisions in unfamiliar environments. • Proposing a flexible navigation behaviour fully explicit by relying upon the AIF framework. • Outperforming in environmental learning and decision-making efficiency the Clone-Structured Cognitive Graph (CSCG) model (21), a prominent model for cognitive map representation (21). • Showcasing robust adaptability, where the model responds seamlessly to dynamic environmental changes, replicating rat maze-like scenarios, thus emphasising its practical application in flexible, real-world navigation tasks. • Incorporating biologically inspired processes like border cells and visual cortex perception, our agent’s navigation strategy is theoretically grounded and scalable to more realistic settings."
https://arxiv.org/html/2411.08433v1,3D Multi-Object Tracking with Semi-Supervised GRU-Kalman Filter,"3D Multi-Object Tracking (MOT), a fundamental component of environmental perception, is essential for intelligent systems like autonomous driving and robotic sensing. Although Tracking-by-Detection frameworks have demonstrated excellent performance in recent years, their application in real-world scenarios faces significant challenges. Object movement in complex environments is often highly nonlinear, while existing methods typically rely on linear approximations of motion. Furthermore, system noise is frequently modeled as a Gaussian distribution, which fails to capture the true complexity of the noise dynamics. These oversimplified modeling assumptions can lead to significant reductions in tracking precision. To address this, we propose a GRU-based MOT method, which introduces a learnable Kalman filter into the motion module. This approach is able to learn object motion characteristics through data-driven learning, thereby avoiding the need for manual model design and model error. At the same time, to avoid abnormal supervision caused by the wrong association between annotations and trajectories, we design a semi-supervised learning strategy to accelerate the convergence speed and improve the robustness of the model. Evaluation experiment on the nuScenes and Argoverse2 datasets demonstrates that our system exhibits superior performance and significant potential compared to traditional TBD methods.The code is available at https://github.com/xiang-1208/GRUTrack.","I INTRODUCTION Multi-Object Tracking (MOT)[1] is a crucial research topic within the field of computer vision and serves as a foundational technology in numerous intelligent applications, such as autonomous driving, traffic flow analysis, security surveillance, robotics, and action recognition. At present, with the increasing performance of Multi-Object Detection (MOD)[2, 3, 4, 5], Multi-Object Tracking (MOT) methods based on the “Tracking-by-Detection” (TBD) [1, 6, 7] have demonstrated superior accuracy and robustness. These TBD methods follow the motion process of the tracked object, contrasting with “Joint Detection and Tracking” (JDT) approaches [8, 2, 9] which do not perform as well. In general, TBD methods update the state of tracked objects incrementally by constructing motion model and employing recursive Bayesian filter estimator. However, due to the varied motion characteristics of different objects within the scene, a single state space (SS) and estimator parameter cannot well match the different motion characteristics between various categories, which reduces the consistency between the motion state update and the actual, leading to false matching and inaccurate state update. Some approaches[7, 10, 11] take note of this and design motion parameters or association strategies for each class to be more relevant to the different characteristics of different classes. However, these methods fail to fundamentally solve the problem of multi-category differences. On the one hand, with the continuous addition or refinement of categories, it is not only tedious to design motion models for each category, but also too dependent on the designer’s experience. On the other hand, although it is possible to capture different types of motion characteristics by continuously refining the categories, most methods still use model-based state filters, such as Kalman Filter (KF)[12] and Extended Kalman Filter (EKF)[13, 14]. The effectiveness of these models depends on the accuracy of the state model and the validity of the motion hypothesis. In the actual MOT, the latent state of the system is nonlinear and complex, and it is even difficult to be accurately described as a tractable state model. In this case, model-based state estimators typically simplify the motion dynamics by linearizing the process and assuming the system noise follows a Gaussian distribution. This assumption does not match the actual situation, and these modeling inaccuracies often bring the loss of tracking precision to the system. To this end, we propose a partially learnable MOT method by introducing a Gated Recurrent Unit (GRU)-based Kalman filter into the motion module of TBD. This method can replace the traditional manual model design in a data-driven manner, thus eliminating the need to design a unique SS and estimator for each class. Specifically, we use multiple GRUs to simulate the loops in Recursive Bayesian Filtering. The model automatically learns the noise distribution, state transition matrix and observation matrix. It avoids the mismatch of noise modeling and the loss of precision caused by linearizing the state transition and observation function. This is doable in theory because neural network-based state estimation has been shown to capture the motion characteristics of complex processes[15, 16], which is also applicable to state transitions in MOT. Figure 1: The pipeline of our proposed method at frame n. T^{D}_{n} is the trajectories updated by associating upper observations D_{n} and using the motion module. Our design focuses on two parts, one is GRU-Kalman Filter: it uses three GRUs to simulate the Kalman filtering process. The second is Semi-Supervised learning, which uses dataset annotations and pseudo-labels generated by a parallel Kalman filter for joint training. However, it is not feasible to directly use the learnable Kalman Filter in MOT. On the one hand, due to the partial annotation of the dataset, the amount of trainable data is small, which is easy to overfitting. On the other hand, since the annotations and trajectories are associated by a hand-designed association strategy, the errors of association will introduce error supervision into the system. Therefore, we propose to parallel a Kalman filter during the training process to generate pseudo-labels for those unlabeled data for semi-supervised training. The experimental results prove the effectiveness of our strategy. Specifically, our contributions are as follows: • We propose a data-driven MOT method by using a GRU-based motion module to avoid the precision loss of traditional methods for noise mismatch modeling and motion process linearization. • We design a pseudo-label-based semi-supervised method, which greatly expands the amount of available training data and label robustness, so that the system can converge in fewer training cycles. • We evaluate our method on the nuScenes[17] and Argoverse2[18] datasets. Our system demonstrates performance comparable to traditional MOT systems and strong level of generalization, while obviating the need for manually designing a model for each object category."
https://arxiv.org/html/2411.08400v1,BAMAX: Backtrack Assisted Multi-Agent Exploration using Reinforcement Learning,"Autonomous robots collaboratively exploring an unknown environment is still an open problem. The problem has its roots in coordination among non-stationary agents, each with only a partial view of information. The problem is compounded when the multiple robots must completely explore the environment. In this paper, we introduce Backtrack Assisted Multi-Agent Exploration using Reinforcement Learning (BAMAX), a method for collaborative exploration in multi-agent systems which attempts to explore an entire virtual environment. As in the name, BAMAX leverages backtrack assistance to enhance the performance of agents in exploration tasks. To evaluate BAMAX against traditional approaches, we present the results of experiments conducted across multiple hexagonal shaped grids sizes, ranging from 10x10 to 60x60. The results demonstrate that BAMAX outperforms other methods in terms of faster coverage and less backtracking across these environments.","Autonomous exploration by robots in unknown environments has diverse applications such as search and rescue, environmental monitoring, and disaster management, and is still an open challenge [7]. Moreover, individual robots often struggle with limitations in coverage, efficiency, reliability, resiliency, and adaptability when operating in complex and dynamic environments. To overcome these challenges, multi-agent collaborative systems have gained attention [19]. By leveraging collective knowledge and coordinating actions, these multiple agents can explore the environment more effectively, leading to improved coverage, robustness, and information exchange [17, 3]. However, collaborative strategies may encounter challenges such as navigating local extrema or overcoming dead ends [14]. In this paper we present an approach called Backtrack Assisted Multi-Agent Exploration using Reinforcement Learning (BAMAX, for short). Multi-agent reinforcement learning is chosen as the foundation for our approach due to its capability to enable agents to learn optimal behaviors through interactions with the environment, leveraging rewards and penalties to enhance decision-making abilities. Our method allows multiple robots to autonomously explore and construct maps within hexagonal mazes, incorporating the crucial capability to backtrack to previously known open positions when encountering obstacles. This paper has two main contributions as follows. The first one is the guarantee of full exploration. Our approach leverages the collective abilities of multiple robots to facilitate efficient navigation, overcome walls, and achieve complete coverage of the entire grid. The second contribution is an ability to scale to multiple sizes of hexagonal grids. In the rest of the paper, we will discuss how these contributions are made using the BAMAX method. The section 2 provides an overview of the related works, covering both traditional methods and the application of intelligent agent navigation with Deep Reinforcement Learning (DRL). In section 3, we present our approach, detailing the creation of the environment that addresses our specific problem statement, and the setup of the Reinforcement Learning (RL) framework. Additionally, this section delves into the specifics of our proposed algorithm and the underlying network architecture. The section 4 focuses on the experimentation conducted to evaluate the performance of our method in comparison to other traditional approaches. Thereafter, the section discusses the results obtained from our experiments, offering analysis and comparison of our method with alternative approaches. Finally, the section 5 concludes the paper."
https://arxiv.org/html/2411.08389v1,Integrative Wrapping System for a Dual-Arm Humanoid Robot,"Flexible object manipulation of paper and cloth is a major research challenge in robot manipulation. Although there have been efforts to develop hardware that enables specific actions and to realize a single action of paper folding using sim-to-real and learning, there have been few proposals for humanoid robots and systems that enable continuous, multi-step actions of flexible materials. Wrapping an object with paper and tape is more complex and diverse than traditional manipulation research due to the increased number of objects that need to be handled, as well as the three-dimensionality of the operation. In this research, necessary information is organized and coded based on the characteristics of each object handled in wrapping. We also generalize the hardware configuration, manipulation method, and recognition system that enable humanoid wrapping operations. The system will include manipulation with admittance control focusing on paper tension and state evaluation using point clouds to handle three-dimensional flexible objects. Finally, wrapping objects with different shapes is experimented with to show the generality and effectiveness of the proposed system.","I INTRODUCTION Paper, cloth, and string are indispensable materials in daily life and industry, and the development of robots that can handle these flexible objects is essential for the realization of assistance in our daily life and automation by robots. On the other hand, the manipulation of flexible objects by robots is complex and has been studied from various perspectives. One approach is the development of hardware that enables specific manipulations in origami and cloth folding [1, 2, 3]. Another approach is automating motion generation and improving motion by learning [4, 5, 6]. The problems of these studies include applicability to general-purpose hardware systems, generalization of the behavior, and application to the continuous motion of flexible objects. In response to these issues, this study addresses a system for flexible object manipulation using a versatile dual-armed humanoid robot. Among the flexible object manipulations, the target is a wrapping operation that requires more objects to be manipulated, consists of multiple motion elements such as turning paper or cloth and fixing paper with tape, and must be performed continuously according to a procedure. This research aims to achieve a sequence of wrapping actions such as turning the paper, covering the object, and applying tape to fix the paper. To achieve this, we construct an integrative system that includes symbolization of the target object, organizing hardware requirements, and generalization of recognition and manipulation actions. Figure 1: Wrapping with a dual-armed humanoid robot. The wrapping operation deals with three things: the object to be wrapped, the object that wraps the object, and the object that seals the object. In this paper, these three are represented as Target, Wrapper, and Seal, respectively."
https://arxiv.org/html/2411.08373v1,DG-SLAM: Robust Dynamic Gaussian Splatting SLAM with Hybrid Pose Optimization,"Achieving robust and precise pose estimation in dynamic scenes is a significant research challenge in Visual Simultaneous Localization and Mapping (SLAM). Recent advancements integrating Gaussian Splatting into SLAM systems have proven effective in creating high-quality renderings using explicit 3D Gaussian models, significantly improving environmental reconstruction fidelity. However, these approaches depend on a static environment assumption and face challenges in dynamic environments due to inconsistent observations of geometry and photometry. To address this problem, we propose DG-SLAM, the first robust dynamic visual SLAM system grounded in 3D Gaussians, which provides precise camera pose estimation alongside high-fidelity reconstructions. Specifically, we propose effective strategies, including motion mask generation, adaptive Gaussian point management, and a hybrid camera tracking algorithm to improve the accuracy and robustness of pose estimation. Extensive experiments demonstrate that DG-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and novel-view synthesis in dynamic scenes, outperforming existing methods meanwhile preserving real-time rendering ability.","Visual simultaneous localization and mapping (SLAM), the task of reconstructing a 3D map within an unknown environment while simultaneously estimating the camera pose, is recognized as a critical component to achieving autonomous navigation in novel 3D environments for mobile robots [1]. It has been widely used in various forms in fields such as robotics, autonomous driving, and augmented/virtual reality (AR/VR). However, the majority of previous research [2; 3; 4; 5; 6; 7; 8; 9; 10] typically relies on the assumption of static environments, limiting the practical applicability of this technology in daily life. Consequently, how to achieve accurate and robust pose estimation in dynamic scenes remains an urgent problem to be addressed for mobile robots. Recently, many researchers [6; 7; 8; 9; 10] have endeavored to replace the conventional explicit representations used in visual SLAM, such as Signed Distance Function (SDF), voxel grids [11], meshes [12], and surfel clouds [13], with the neural radiance field (NeRF) [14] approach for reconstructing the neural implicit map. This novel map representation is more continuous, efficient, and able to be optimized with differentiable rendering, which has the potential to benefit applications like navigation and reconstruction. However, these methods exhibit two primary issues: the scene’s bounds are required to be predefined to initialize the neural voxel grid; and the implicit representation proves challenging for information fusion and editing. To address these problems, recent works like GS-SLAM [15], SplaTam [16], and Gaussian splatting SLAM [17] leverage the 3D-GS [18] to explicit represent the scene’s map. This explicit geometric representation is also smooth, continuous, and differentiable. Moreover, a substantial array of Gaussians can be rendered with high efficiency through splatting rasterization techniques, achieving up to 300 frames per second (FPS) at a resolution of 1080p. However, all these above-mentioned neural SLAM methods do not perform well in dynamic scenes. The robustness of these systems significantly decreases, even leading to tracking failures, when dynamic objects appear in the environment. To tackle these problems, we propose a novel 3D Gaussian-based visual SLAM that can reliably track camera motion in dynamic indoor environments. Due to the capability of 3D-GS to accomplish high-quality rendering in real-time, the SLAM system more readily converges to a global optimum during pose optimization, thereby achieving better and more stable pose optimization results. A cornerstone of our approach to achieving robust pose estimation lies in the innovative motion mask generation algorithm. This algorithm filters out sampled pixels situated within invalid zones, thereby refining the estimation process. In addition to the constraint of depth residual, we employ a spatio-temporal consistency strategy within an observation window to generate depth warp masks. By incrementally fusing the depth warp mask and semantic mask, the motion mask will become more precise to reflect the true motion state of objects. To improve the accuracy and stability of pose estimation, we leverage DROID-SLAM [19] odometry (DROID-VO) to provide an initial pose estimate and devise a coarse-to-fine optimization algorithm built upon the initially estimated camera pose. This aims to minimize the disparity between pose estimation and the reconstructed map, employing photorealistic alignment optimization through Gaussian Splatting. Moreover, this hybrid pose optimization approach effectively ensures the accuracy and quality of the generated depth warp mask, thereby facilitating better performance in the next camera tracking stage. To obtain high-quality rendering results, we propose a novel adaptive Gaussian point addition and pruning method to keep the geometry clean and enable accurate and robust camera tracking. Capitalizing on the factor graph structure inherent in DROID-SLAM, our system is capable of executing dense Bundle Adjustment (DBA) upon completion of tracking to eliminate accumulated errors. In summary, our contributions are summarized as follows: (i) To the best of our knowledge, this is the first robust dynamic Gaussian splatting SLAM with hybrid pose optimization, capable of achieving both real-time rendering and high-fidelity reconstruction performance. (ii) To mitigate the impact of dynamic objects during pose estimation, we propose an advanced motion mask generation strategy that integrates spatio-temporal consistent depth masks with semantic priors, thereby significantly enhancing the precision of motion object segmentation. (iii) We design a hybrid camera tracking strategy utilizing the coarse-to-fine pose optimization algorithm to improve the consistency and accuracy between the estimated pose and the reconstructed map. (iv) To better manage and expand the Gaussian map, we propose an adaptive Gaussian point addition and pruning strategy. It ensures geometric integrity and facilitates accurate camera tracking. (v) Extensive evaluations on two challenging dynamic datasets and one common static dataset demonstrate the state-of-the-art performance of our proposed SLAM system, particularly in real-world scenarios."
https://arxiv.org/html/2411.08323v1,Efficient Trajectory Generation in 3D Environments with Multi-Level Map Construction,"We propose a robust and efficient framework to generate global trajectories for ground robots in complex 3D environments. The proposed method takes point cloud as input and efficiently constructs a multi-level map using triangular patches as the basic elements. A kinematic path search is adopted on the patches, where motion primitives on different patches combine to form the global min-time cost initial trajectory. We use a same-level expansion method to locate the nearest obstacle for each trajectory waypoint and construct an objective function with curvature, smoothness and obstacle terms for optimization. We evaluate the method on several complex 3D point cloud maps. Compared to existing methods, our method demonstrates higher robustness to point cloud noise, enabling the generation of high quality trajectory while maintaining high computational efficiency. Our code will be publicly available at https://github.com/ck-tian/MLMC-planner.","Ground robots are widely used in exploration and rescue missions within complex 3D environments. Evaluating the multi-level and uneven structures of these environments and generating smooth, collision-free trajectories with kinematic feasibility are crucial for enhancing the autonomy of robots. A comprehensive map that accurately describes the environment is a prerequisite for effective trajectory generation. Some methods utilize raw point clouds data directly [1], [2] which provide highly detailed representations of the environments. However, these methods often become computationally intensive due to the complexity in data processing. Voxel maps divide the space into a series of volumetric units known as voxels to represent objects, which are widely used in UAV navigation in 3D environments [3], [4]. However, voxel maps are discrete, making them challenging to accurately represent continuous terrain features such as slopes. Elevation map [5] represents environments as 2D height maps, which makes it difficult to represent multi-level structures. Mesh-based [6, 7, 8] maps can accurately represent complex structures, but often have high computational complexity and large storage overhead. Trajectory generations using mesh-based maps also have high computational complexity in trajectory smoothing and obstacle detecting. Figure 1: The result of map construction and trajectory generation using the point cloud library (PCL) [22] to visualize. The triangular patches divided into traversable and untraversable are the basic elements of the map. The red dots represent the waypoints of primitives which are generated based on patches. The yellow line represents the inital trajectory consisting of primitives located on traversable patches and the blue line represents the final trajectory after optimization. Kinematic search methods [9], [10] can ensure trajectories smoothness and kinematic feasibility by using motion primitives as graph edges and allow the incorporation of various optimal objectives such as energy and time consumption. Therefore, they are widely adopted in the nonholonomic and multimodal locomotion robots [11]. However, generating motion primitives and considering additional constraints are computationally intensive in 3D space. We propose a robust and highly efficient framework for generating trajectories in complex 3D environments, which includes map construction and trajectory generation. The map construction method uses raw point cloud as input and builds a map using regularly arranged triangular patches, referred to as patches for concision hereafter, as the basic elements. The map construction method is highly computationally efficient and can accurately represents the multi-level and uneven structures. The patches are regularly arranged and share vertices, forming a continuous map. This feature enables efficient indexing, allowing for rapid localization of the robot within the patch and the nearest obstacle at the same level. It significantly enhances computational efficiency of initial trajectory generation and optimization. We adopt a kinematic path search method to generate motion primitives on the patches. In initial trajectory generation, a method which maintains the smooth of trajectory is proposed for transitioning between different patches. A two-stage optimization is performed to get the final trajectory satisfying ground constraints. Our same-level expansion method guides the trajectory away from obstacles at the current level by excluding the influence from different levels. Our contributions can be summarized as follows: Figure 2: (a) and (b) represent the process of multi-level map construction. {\rm{(}}{{\bf{x}}_{{\rm{wf}}}},{{\bf{y}}_{{\rm{wf}}}},{{\bf{z}}_{{\rm{wf}}}}{% \rm{)}} represents the gravity-aligned world frame. {\rm{(}}{{\bf{x}}_{{\rm{pf}}}},{{\bf{y}}_{{\rm{pf}}}},{{\bf{z}}_{{\rm{pf}}}}{% \rm{)}} represents the patch frame. Red dots represent the point cloud after clustering. {{\bf{S}}_{{\rm{(}}m{\rm{,}}n{\rm{,}}i{\rm{)}}}} is the i-th slice within {\bf{mapcel}}{{\bf{l}}_{{\rm{(}}m{\rm{,}}n{\rm{)}}}}. \bf{meshcell} stores the patches whose projections on the {{\bf{x}}_{{\rm{wf}}}}-{{\bf{y}}_{{\rm{wf}}}} plane fall within it. In (c), surface_{\rm{1}} is connected to surface_{\rm{2}} with wall. The surface_{\rm{3}} is an overhang and the height variation from surface_{\rm{1}} to it is not continuous. 1)Efficient Map Construction: We propose an efficient map construction method that uses point cloud as input to construct multi-level map, which can accurately represent the multi-level and uneven structures in complex 3D environment while mitigating the impact of point cloud noise. 2)Kinematic Path Searching: We adopt a kinematic path searching method on triangular patches to generate motion primitives whose transitions between different patches are smooth. 3)Trajectory Optimization: We propose a same-level expansion method to locate the nearest obstacle of the trajectory waypoint at the same level."
https://arxiv.org/html/2411.08281v1,When to Localize?: A POMDP Approach,"Robots often localize to lower navigational errors and facilitate downstream, high-level tasks. However, a robot may want to selectively localize when localization is costly (such as with resource-constrained robots) or inefficient (for example, submersibles that need to surface), especially when navigating in environments with variable numbers of hazards such as obstacles and shipping lanes. In this study, we propose a method that helps a robot determine “when to localize” to 1) minimize such actions and 2) not exceed the probability of failure (such as surfacing within high-traffic shipping lanes). We formulate our method as a Constrained Partially Observable Markov Decision Process and use the Cost-Constrained POMCP solver to plan the robot’s actions. The solver simulates failure probabilities to decide if a robot moves to its goal or localizes to prevent failure. We performed numerical experiments with multiple baselines.","Self-localization is crucial in robotics, as it seeds situational awareness and enables a robot to perform downstream tasks such as object manipulation and navigation. In many real-world tasks, an autonomous robot perceives its environment (including self-localization), plans, acts, and then repeats this cycle continuously. However, a robot may seldom want to localize when localization is costly. Let us consider examples where an autonomous underwater vehicle (AUV) navigates through high-traffic shipping lanes [1] to deliver supplies, search for critical items such as black boxes from crashed aircraft, or assess post-tsunami underwater damage (Figure 1). Typically, AUVs localize accurately by surfacing to collect GPS data because underwater dead-reckoning drifts over time, and sonar-based localization is infeasible in featureless environments. However, surfacing risks collisions with ships and increases operating time. We explore scenarios where continuous localization may be unnecessary or not required, challenging the traditional perceive-plan-act paradigm. Instead, can a robot plan and act for extended durations, localizing only when necessary? Specifically, we aim to address the question: when should a robot localize to avoid exceeding the probability of failing a task? We argue that this question is non-trivial. For example, what is the appropriate state uncertainty threshold to trigger localization in a given scenario? Furthermore, should the AUV localize preemptively before unsafe regions? If so, how far in advance (when)? We propose a move-localize behavior planner based on a constrained Partially Observable Markov Decision-making Process (POMDP) to address such questions. A constrained POMDP allows us to decouple failure probabilities and rewards and reduce ad-hoc reward tuning. Our formulation proposes the probability of failure as a cost constraint with a threshold and creates a reward strategy that penalizes localization. Figure 1: An underwater vehicle follows a path to perform a task (like searching for an aircraft’s black box) and must choose when to localize, which requires surfacing and poses a collision risk in shipping lanes. Continuous localization (bottom path) is inefficient yet safe. Selective localization (top path) is more efficient and desirable since the vehicle searches underwater longer. But deciding when to localize to avoid hazards and stay on the path is more challenging."
https://arxiv.org/html/2411.08261v1,Control of Biohybrid Actuators using NeuroEvolution,"In medical-related tasks, soft robots can perform better than conventional robots because of their compliant building materials and the movements they are able perform. However, designing soft robot controllers is not an easy task, due to the non-linear properties of their materials. Since human expertise to design such controllers is yet not sufficiently effective, a formal design process is needed. The present research proposes neuroevolution-based algorithms as the core mechanism to automatically generate controllers for biohybrid actuators that can be used on future medical devices, such as a catheter that will deliver drugs. The controllers generated by methodologies based on Neuroevolution of Augmenting Topologies (NEAT) and Hypercube-based NEAT (HyperNEAT) are compared against the ones generated by a standard genetic algorithm (SGA). In specific, the metrics considered are the maximum displacement in upward bending movement and the robustness to control different biohybrid actuator morphologies without redesigning the control strategy. Results indicate that the neuroevolution-based algorithms produce better suited controllers than the SGA. In particular, NEAT designed the best controllers, achieving up to 25% higher displacement when compared with SGA-produced specialised controllers trained over a single morphology and 23% when compared with general purpose controllers trained over a set of morphologies.","Soft robotics is a sub-field of robotics that studies machines built with flexible and ductile materials, such as silicone rubbers [1]. This type of robots have demonstrated better performance in specific tasks related to healthcare [2], due to their morphology and behaviour that are heavily inspired by living organisms. Despite their promising applicability, soft robots face significant challenges, i.e., defining an adequate morphology design. Under a traditional approach of robot designing, considerable time and material resources are utilised since numerous alternative prototypes are tested physically [3]. On the other hand, under a soft robots approach, designing process is more complex due to the materials’ flexibility and mechanical properties being non-linear and difficult to characterise [4]. When a suitable design is found, the next step consists of designing an appropriate controller for the soft robot, a process that can be considered as intricate as finding a suitable morphology. The required strategies to control soft robots have two main considerations: (i) the soft materials constituting the robot can deform at every point, resulting in infinite degrees of freedom, including bending, extension, contraction, and torsion, and (ii) soft materials present non-linear and time-dependent properties. These aspects make modeling of soft robot behaviour and movement a difficult task [5]. A methodology that can assist the design of controllers for soft robots which is worth investigating is Neuroevolution (NE). NE focuses on evolving the topology and weights of artificial neural networks (ANNs) employing a genetic algorithm (GA) methodology. Arguably, the most efficient NE algorithm has proved to be Neuroevolution of Augmenting Topologies (NEAT) [6]. Furthermore, under the rationale that natural structures are composed of shape repetition and patters, an extension of NEAT was developed, namely Hypercube-based Neuroevolution of Augmenting Topologies (HyperNEAT) [7]. HyperNEAT evolves a singular type of ANNs named Compositional Pattern-Producing Networks (CPPNs), whose difference to traditional ANNs focuses around the use of periodic functions (e.g., sine and square wave), in order to generate patterns, like symmetry and repetition that help evolve more interesting topologies [8]. The fundamental objective of the presented research is assessing the suitability of NEAT and HyperNEAT as design engines for controllers of biohybrid actuators (BHAs), a particular type of soft robots’ components that are built utilising biological material, such as tissues or cells. Specifically, we analyse the capabilities of the NEAT and HyperNEAT to design controllers whose objective is to induce an upward bending movement to a given BHA. That BHA may be embodied into a catheter for targeted drug delivery to areas of the human body that are difficult to reach."
https://arxiv.org/html/2411.08253v1,Open-World Task and Motion Planning via Vision-Language Model Inferred Constraints,"Foundation models trained on internet-scale data, such as Vision-Language Models (VLMs), excel at performing tasks involving common sense, such as visual question answering. Despite their impressive capabilities, these models cannot currently be directly applied to challenging robot manipulation problems that require complex and precise continuous reasoning. Task and Motion Planning (TAMP) systems can control high-dimensional continuous systems over long horizons through combining traditional primitive robot operations. However, these systems require detailed model of how the robot can impact its environment, preventing them from directly interpreting and addressing novel human objectives, for example, an arbitrary natural language goal. We propose deploying VLMs within TAMP systems by having them generate discrete and continuous language-parameterized constraints that enable TAMP to reason about open-world concepts. Specifically, we propose algorithms for VLM partial planning that constrain a TAMP system’s discrete temporal search and VLM continuous constraints interpretation to augment the traditional manipulation constraints that TAMP systems seek to satisfy. We demonstrate our approach on two robot embodiments, including a real world robot, across several manipulation tasks, where the desired objectives are conveyed solely through language.","The advent of foundation models trained on internet-scale data has led to unprecedented progress on traditionally-hard tasks in vision and natural language. Current Large Language Models (LLMs) and Vision-Language Models (VLMs) are able to complete text from partial specifications, answer questions about images, and even solve challenging word problems that require reasoning and common sense [1, 2, 3]. This impressive performance has inspired several systems that attempt to use existing pretrained models in robotics [4, 5, 6, 7]. Such systems exhibit impressive flexibility: unlike classical robotics approaches, they are able to accomplish novel goals specified by natural language or images. However, these models are not trained to output continuous values (e.g. joint angles, grasps, placements), which are critical for interacting with the physical world. Figure 1: OWL-TAMP uses VLMs to augment the capabilities of a TAMP system by 1) grounding an input goal description into TAMP’s vocabulary through instantiating goal conditions and a partial plan, and 2) interpreting the semantics of constraints involving open-world concepts through lazily writing constraint tests. In contrast, classical Task and Motion Planning (TAMP) systems are capable of solving complex and long-horizon tasks ranging from setting a dining table to 3D printing complex structures [8, 9, 10]. These systems leverage models of the robot and its environment to explicitly reason about both discrete and continuous values in robotics problems. While such systems are powerful on the set of problems they have been designed for, they do not transfer to novel problems for which their models are unspecified. Enabling a TAMP system to solve novel problems often requires manually augmenting the underlying model with additional concepts, which is tedious and not scalable when operating in unstructured human environments. We are interested in combining the complementary benefits of foundation-models and TAMP to tackle long-horizon manipulation tasks that are open world, namely where the vocabulary of objectives is unbounded. Specifically, we assume tasks are specified in natural language or images, which may involve concepts that an underlying TAMP system does not have built-in. As an example, a TAMP system that is capable of accomplishing pick-and-place tasks expects goals in the form of logical expressions involving predicates like On(apple, plate). Consider the goal statement in Figure 1: ‘‘Put the apple where the green block is’’. This goal cannot be expressed in terms of On, and thus there would be no way a TAMP system could solve it. A pure VLM system would also struggle with this task since it must not only predict that the green block needs to be moved out of the way before moving the apple, but also the full robot motion that realizes this. Our key insight is that we can integrate the discrete-continuous planning of TAMP systems and common sense reasoning of VLMs through the contract of constraints. In particular, VLMs are capable of mapping open world expressions to code to express discrete constraints over action sequences (e.g. that an apple must be cooked before it can be served), and continuous constraints over important decision variables (e.g. the poses of an apple). These constraints can be readily integrated with existing constraints (e.g. avoiding collisions, respecting kinematics) within off-the-shelf TAMP systems. Thus, the overall system is able to return solutions that not only respect constraints derived from the open world goal, but also are physically feasible on robot hardware. We propose OWL-TAMP (Open-World Language-based TAMP), an approach that integrates open world concepts via constraint generation into a TAMP system with traditional robotics operations and constraints. Our key contributions towards this framework are: (1) a method for generating constraints on action sequences to specify partial plans; (2) a method for generating constraints on continuous variables within a TAMP problem; and (3) a method for iteratively reprompting the VLM to refine constraints generated by (2). We test our framework on a variety of open world tasks in simulation and find that it is able to solve open world tasks with a higher success rate than several ablations, including a pure VLM or pure TAMP system respectively. We also demonstrate that our approach enables a real-world robot to solve complex, long-horizon manipulation tasks specified through language directly from sensor input."
https://arxiv.org/html/2411.08231v1,Enhanced Monocular Visual Odometry with AR Poses and Integrated INS-GPS for Robust Localization in Urban Environments,"This paper introduces a cost-effective localization system combining monocular visual odometry (VO), augmented reality (AR) poses, and integrated INS-GPS data. We address monocular VO’s scale factor issues using AR poses and enhance accuracy with INS and GPS data, filtered through an Extended Kalman Filter (EKF). Our approach, tested using manually annotated trajectories from Google Street View, achieves an RMSE of 1.529 meters over a 1 km track. Future work will focus on real-time mobile implementation and further integration of visual-inertial odometry for robust localization. This method offers lane-level accuracy with minimal hardware, making advanced navigation more accessible.","In recent years, many cars have released with SAE Level 2 autonomous driving features. The price tag for a car with these new features is still out of reach of many people, including those who may need them most. One of the major contributors to the price of these vehicles is the cost of their sensors. Currently, a full suite of sensors for a level 2 autonomous vehicle can cost anywhere from $8,000-10,000[1]. As we begin to progress towards higher levels of autonomy, the suite of sensors needed will only increase in complexity, and in turn, price. Additionally, it has been shown that traditionally GPS is not accurate enough to be used as a localization method for autonomous driving and this inaccuracy only increases exponentially in an urban environment. With our approach we aim to create a level of accuracy that is comparable to that of much more expensive sensors at a fraction of the cost. In this paper we propose a way to exploit this ‘urban jungle’ and improve greatly upon the accuracy of a traditional GPS using sensors found on a single smartphone. Our method is based around using Google Street View’s panoramic images and comparing them to those being taken on board with our monocular smartphone camera. We chose to use Street View because it contains data from a large portion of the world and this data gives us both a high resolution image with accurate associated location metadata. Additionally, anyone around the world with an internet connection can access Street View. We also log measurements from the IMU and Apple’s ARKit Framework simultaneously with the images recorded from our smartphone. Our approach involves extracting features from our recorded frames, as well as from the panoramas taken from Google Street View. Once we have these features, we calculate the relative pose between our frame and the surrounding panoramas. We then use the poses in tandem with the coordinates of each panorama provided in the metadata to triangulate our estimated position. Additionally, we aim to use the IMU measurements alongside our image data to compute our Visual-Inertial Odometry module and combine it with our estimated location in order to reduce noise."
https://arxiv.org/html/2411.08144v1,Visual Tracking with Intermittent Visibility: Switched Control Design and Implementation,"This paper addresses the problem of visual target tracking in scenarios where a pursuer may experience intermittent loss of visibility of the target. The design of a Switched Visual Tracker (SVT) is presented which aims to meet the competing requirements of maintaining both proximity and visibility. SVT alternates between a visual tracking mode for following the target, and a recovery mode for regaining visual contact when the target falls out of sight. We establish the stability of SVT by extending the average dwell time theorem from switched systems theory, which may be of independent interest. Our implementation of SVT on an Agilicious drone [1] illustrates its effectiveness on tracking various target trajectories: it reduces the average tracking error by up to 45% and significantly improves visibility duration compared to a baseline algorithm. The results show that our approach effectively handles intermittent vision loss, offering enhanced robustness and adaptability for real-world autonomous missions. Additionally, we demonstrate how the stability analysis provides valuable guidance for selecting parameters, such as tracking speed and recovery distance, to optimize the SVT ’s performance.","The visual tracking task requires a pursuer to follow a moving target using only camera as a sensor. This capability is relevant for search and rescue, delivery, spacecraft docking, in-air-refuelling, navigation, and other autonomous missions. Visual tracking, also called visual pursuit, has been studied by robotics and aerospace researchers [2, 3, 4, 5] (see, other related works in Section V). A popular approach is for the pursuer’s controller to minimize the tracking error defined on image space. These methods are effective if the target is always visible, but cannot recover once it is lost from the pursuer’s camera view. The target may be lost because of motion blur, occlusions, or simply because it moves out of the pursuer’s camera frame. The latter is more likely as the pursuer nears the target and the camera’s viewable field becomes narrower. Thus, the two goals of maintaining visibility and gaining proximity can be conflicting. To tackle this challenge, we propose the design of a Switched Visual Tracker (SVT)—a mode-switching controller [6] that tracks the target when it is visible, in what we call a visual tracking mode, and maneuvers to regain visual contact when the target is lost, in a recovery mode. The design of SVT includes the logic for switching between these two modes. A switching controller for landing a drone on a moving vehicle was presented in [7]. That work, like ours, handles loss of visual contact of the moving target and showed interesting empirical results. To our knowledge, ours is the first to provide a stability analysis and connect the stability criteria with the control design parameter. There is extensive research on stability analysis of switched systems [6, 8, 9, 10]. Hespanha and Morse’s average dwell time theorem [8] gives a stability criterion in terms of the rate of energy (or Lyapunov function) decay in the individual modes (\lambda), the energy gains across the mode switches, and the rate of mode switches. To accommodate the analysis of SVT with recovery, we generalize this theorem to Theorem 1 which allows the system to be temporarily unstable, which leads to an additive (c) and multiplicative (\mu) increase in the Lyapunov functions. We show that, given a sufficiently long average dwell time in the stable modes, the system can still achieve asymptotic stability with respect to a set of states. For SVT this implies guaranteed tracking performance. We compare the effectiveness of SVT implemented on the Agilicious drone [1], with a baseline visual tracking controller. SVT improves the average tracking error by 45% and also significantly improves the fraction of time the target is visible. Further, we observe Theorem 1 can be used to guide the choice of various SVT parameters for improving the system’s performance with respect to tracking and visibility requirements. For example, according to Theorem 1, a higher tracking speed (v_{max}) increases the Lyapunov exponent \lambda, which reduces tracking by allowing smaller dwell time. We observe from experiments that increasing v_{max} from 0.1m/s to 1.0m/s indeed improves the average tracking error from 1.3m to 0.5m. A smaller recovery distance (d_{max}) reduces \mu and c (characterizing the unstable recovery). By reducing the recovery distance from 2.7m to 2.1m, experiments show the average tracking error improves 0.74m to 0.56m while target visibility improves from from 82.8 to 86.4%. In summary, our contribution are as follows: 1. The design of a Switched Visual Tracker (SVT) for the visual tracking problem, which effectively handles intermittent loss of target visibility. 2. A rigorous stability analysis of the SVT based on a modest extension of an existing switched system stability result. This provides guarantees on the tracking performance and guidelines for choosing parameters in the implementation of SVT. 3. An implemention of SVT on an Agilicous-based pursuer drone and comprehensive experimental evaluations with different target trajectories and design parameters."
https://arxiv.org/html/2411.08136v1,Simultaneous Locomotion Mode Classification and Continuous Gait Phase Estimation for Transtibial Prostheses,"Recognizing and identifying human locomotion is a critical step to ensuring fluent control of wearable robots, such as transtibial prostheses. In particular, classifying the intended locomotion mode and estimating the gait phase are key. In this work, a novel, interpretable, and computationally efficient algorithm is presented for simultaneously predicting locomotion mode and gait phase. Using able-bodied (AB) and transtibial prosthesis (PR) data, seven locomotion modes are tested including slow, medium, and fast level walking (0.6, 0.8, and 1.0 m/s), ramp ascent/descent (5 degrees), and stair ascent/descent (20 cm height). Overall classification accuracy was 99.1\% and 99.3\% for the AB and PR conditions, respectively. The average gait phase error across all data was less than 4\%. Exploiting the structure of the data, computational efficiency reached 2.91 \mus per time step. The time complexity of this algorithm scales as O(N\cdot M) with the number of locomotion modes M and samples per gait cycle N. This efficiency and high accuracy could accommodate a much larger set of locomotion modes (\sim 700 on Open-Source Leg Prosthesis) to handle the wide range of activities pursued by individuals during daily living.","I INTRODUCTION Assistive devices like lower-limb prostheses and exoskeletons have the potential to enhance the quality of life for a wide range of individuals. A key challenge in realizing the full potential of these devices is to ensure that the control is consistent and fluent with the user’s intent [1]. These human-centered robots depend on accurate recognition and interpretation of the user’s intended movements to ensure a positive user experience and to maximize the assistive benefit [2]. Recognizing and analyzing human gait can also be used clinically to identify healthy or pathological gait [3, 4, 5, 6] or to assess rehabilitation progress [7, 8]. Therefore, identifying highly accurate ways to classify and monitor human locomotion is key to the advancement of rehabilitation robotics and assistive devices. Figure 1: The accuracy and number of locomotion modes present in representative locomotion mode classification literature. Studies in red present classification only and studies in blue classify the locomotion mode and estimate the gait phase. Figure 2: Experimental data was collected from 7 different locomotion modes: slow/medium/fast walking, ramp ascent/descent, and stair ascent/descent), with 2 different conditions: able-bodied (AB) and with the Open-Source Leg (OSL) transtibial prostheses (PR). Herein, the recognition and identification of human locomotion consists of two primary goals: classifying the intended locomotion mode (such as level walking, ramp walking, or stair climbing) and monitoring the progression of gait within each stride. Traditionally, for control of prostheses and exoskeletons, these goals are pursued consecutively in a hierarchical control framework. The highest level determines the user’s activity intent, the middle level determines gait phase and translates that into device commands, and the lowest level implements those commands [9]. High-level locomotion mode classification has primarily paired various wearable sensors with machine learning (ML) algorithms [10, 11]. Inertial measurement units (IMUs) are frequently employed in these frameworks as they can be accessible and relatively easily worn [12, 13]. ML approaches such as linear discriminant analysis (LDA, [13, 14, 15]), support vector machines (SVMs, [16]), Gaussian mixture models (GMMs, [17, 18]), dynamic Bayesian networks (DBNs, [19, 20]), convolutional neural networks (CNNs, [21, 22, 23, 24]), and others have been used to distinguish between standard locomotion modes. Once high-level classification is complete, the appropriate mid-level controller can be selected. To synchronize the mid-level commands with the user’s gait, however, the gait stride progression must be identified. The gait phase can be defined as the progression from one heel strike to a consecutive same-side heel strike. Finite-state machines (FSMs) monitor gait phase via discrete phases, such as early stance, late stance, and swing [25, 26, 27]. Gait cycle progression can also be continuously estimated by monitoring a phase variable. Phase variables are biomechanic markers that progress monotonically with gait phase, such as the foot center of pressure [28, 29] (during stance only), thigh kinematics [30, 31], and tibia kinematics [32, 33, 34]. Gait phase has most commonly been measured during level-ground walking, but the wide range of achievable activities in the real world demands expanding these techniques to other activities [35]. Most approaches focus on either locomotion mode classification or on gait phase estimation [36], while some have pursued both (​​[37, 38, 39, 40, 41], blue in Fig. 1). While these approaches demonstrate that IMUs on the leg segments can be an effective sensing strategy, two primary limitations persist. The first is that very few locomotion modes are often used for classification, and these modes are often substantially different from one another, such as standing and ascending stairs. While this simplification leads to higher classification accuracy, using a sparse set of locomotion modes is not representative of the nearly infinite task landscape of human locomotion. Furthermore, misclassifications that occur between dissimilar locomotion modes would lead to control actions that are significantly different from expected, potentially leading to falls [42]. The second perceived limitation of many ML strategies for classification and gait phase estimation is that the resulting predictions themselves lack transparency. While the model outputs may offer predictions, ML approaches typically do not provide insight into how or why those predictions were made. This insight could be critical information when analyzing misclassifications or when assessing user progress in a rehabilitation setting. Thus, in this study, we present a novel and interpretable approach to simultaneously identify the locomotion mode and gait phase, and implement it with able-bodied data and with a robotic transtibial prosthesis in a bypass configuration. Figure 2 shows these two conditions, along with the locomotion modes selected for identification, which are level treadmill walking (at 0.6, 0.8, and 1.0 m/s), fixed ramp ascent and descent (5-degree slope), and stair ascent and descent (20 cm stair height). This approach is computationally efficient (2.91 \mus per time step), which allows for a large number of reference locomotion modes with small differences from one another to be predicted with high accuracy (7 modes, >99\%). The gait phase estimation (<4\% average error) is done continuously to allow smooth continuous control of wearable robotics. The contributions herein include 1) the development of a novel, interpretable, computationally efficient algorithm to simultaneously predict locomotion mode and gait phase, 2) the assessment of this algorithm on healthy able-bodied gait and asymmetrical gait from a transtibial prosthesis, and 3) the demonstration of high steady-state classification accuracy and phase estimation while considering a large set of locomotion modes compared to the literature (see Fig. 1)."
https://arxiv.org/html/2411.08070v1,Multi-Objective Algorithms for Learning Open-Ended Robotic Problems,"Quadrupedal locomotion is a complex, open-ended problem vital to expanding autonomous vehicle reach. Traditional reinforcement learning approaches often fall short due to training instability and sample inefficiency. We propose a novel method leveraging multi-objective evolutionary algorithms as an automatic curriculum learning mechanism, which we named Multi-Objective Learning (MOL). Our approach significantly enhances the learning process by projecting velocity commands into an objective space and optimizing for both performance and diversity. Tested within the MuJoCo physics simulator, our method demonstrates superior stability and adaptability compared to baseline approaches. As such, it achieved 19% and 44% fewer errors against our best baseline algorithm in difficult scenarios based on a uniform and tailored evaluation respectively. This work introduces a robust framework for training quadrupedal robots, promising significant advancements in robotic locomotion and open-ended robotic problems.","Improving autonomous vehicle mobility and reach with quadrupedal locomotion traditionally relied on classic approaches, such as optimal control and planning, necessitated intricate modelling of robotic dynamics and demanded significant computational time [4]. Avoiding these shortcomings, Reinforcement Learning (RL) has become a state-of-the-art approach [9]. However, it is still vulnerable to generating unpredictable behaviours, has issues with reproducibility, training instability, difficulties in balancing exploration and exploitation, and sample inefficiency [2]. Moreover, as animals do when striving for natural locomotion in the real world, we encounter an open-ended problem due to the limitless ways to achieve locomotion. In quadruped locomotion, no single walking gait allows movement in all directions. Thus, a locomotion controller must learn diverse behaviours for various locomotion tasks, with no specific behaviour being universally superior. Hence, open-endness makes quadrupedal robotic locomotion particularly challenging, even for RL. Humans also encounter such challenges. Consequently, curriculum learning has emerged as a pivotal mechanism in human education, aiding the acquisition of novel tasks. However, the manual creation of curricula can be time-consuming. Automatic Curriculum Learning (ACL) approaches partially mitigate this problem. While current ACL approaches to open-ended robotic tasks demonstrated novelty and improvement [10], we argue that they may be computationally demanding, complex, brittle, and lack a conjoint directed effort toward higher performance and diversity. Figure 1: Render of the simulated 12 degrees of freedom quadrupedal robot used in the MuJuCo environment. This work addresses the challenge of developing a quadrupedal locomotion controller that achieves desired walking velocities. We conditioned the controller on the x and y linear velocities and z angular velocity, with each combination of velocities regarded as a task, which we refer to as a velocity command. Given the problem’s complexity, the achievable range of commands, and the numerous optimal walking gaits, we consider this an open-ended problem. We propose to look at quadrupedal locomotion as a Multi-Objective (MO) problem where trade-offs are inevitable. We thus suggest projecting commands into an objective space and using the achieved performance for each command as a magnitude within this space. A MO algorithm can then select commands that enhance performance and diversity, thereby improving the controller’s learning. Our approach, that we named Multi-Objective Learning (MOL), is tested on a quadrupedal robot with 12 degrees of freedom simulated in the MuJoCo physics simulator [16]. A render of the robot is shown in Figure 1. While this design was not modeled after an actual robot, we believe a realistically design robot in MuJoCo could learn with our approach and transfer to a real-world robot, such as in [6]. Additionally, in [9], they showed the effectiveness of a teacher-student approach in simulation to enable the use of the learned controller on a real-world robot. This work presents three key contributions in the domain of ACL for quadrupedal locomotion: • Introduction of the novel MOL approach to adapt MO evolutionary algorithms for ACL in a continuous task space. • Generation of a diverse set of high-performing commands for quadrupedal locomotion, maintaining or enhancing performance across the entire command space. • Enhancement of the stability of learning quadrupedal locomotion in complex scenarios within the realistic MuJoCo physics simulator."
https://arxiv.org/html/2411.08060v1,"Online Collision Risk Estimation via Monocular
Depth-Aware Object Detectors and Fuzzy Inference","This paper presents a monitoring framework that infers the level of autonomous vehicle (AV) collision risk based on its object detector’s performance using only monocular camera images. Essentially, the framework takes two sets of predictions produced by different algorithms and associates their inconsistencies with the collision risk via fuzzy inference. The first set of predictions is obtained through retrieving safety-critical 2.5D objects from a depth map, and the second set comes from the AV’s 3D object detector. We experimentally validate that, based on Intersection-over-Union (IoU) and a depth discrepancy measure, the inconsistencies between the two sets of predictions strongly correlate to the safety-related error of the 3D object detector against ground truths. This correlation allows us to construct a fuzzy inference system and map the inconsistency measures to an existing collision risk indicator. In particular, we apply various knowledge- and data-driven techniques and find using particle swarm optimization that learns general fuzzy rules gives the best mapping result. Lastly, we validate our monitor’s capability to produce relevant risk estimates with the large-scale nuScenes dataset and show it can safeguard an AV in closed-loop simulations.","Over the past decade, autonomous vehicles (AVs) have attained great development and can be seen on public roads nowadays. However, it is still possible to hear AV accidents, especially in corner cases such as severe weather conditions or the emergence of rare objects [1]. To ensure the safety of AVs and allow their wider deployment, it is important to have run-time monitors that can identify such performance-hindering situations. Correspondingly, regulations and industrial standards such as EU AI Act [2] and ISO 21448 [3] also demand the inclusion of monitoring mechanisms in safety-critical autonomous systems. In the literature, in fact, one can find various run-time monitoring techniques. Focusing on planning and control, many work derive collision risks based on ego and traffic information, e.g., driving path deviation or time-to-collision to other agents [4]. These approaches, nonetheless, often assume perfect perception, which is generally not the case. In this work, we relax such an assumption and attempt to identify hazards as early as possible in an AV software stack, such that the controller can trigger a safety maneuver in time. Holding a similar mindset, several studies have suggested to monitor the object detection function. For instance, some propose algorithms that check the spatial or temporal consistency of the set of detected objects [5, 6]. Despite effective, one crucial limitation of the existing work is the relevance between the identified anomalies and the actual safety risk of the AV. For example, these monitors may raise a warning for an object that is far from the AV, which actually poses a low risk. Likewise, they only examine the set of detected objects and ignore potential misses (i.e., false negatives of the object detector), which are more likely risk-relevant. Figure 1: The overall monitoring framework using alignment measures between two sets of predictions to infer ego vehicle’s collision risks. The dashed lines mark an offline optimization process using a previously presented risk-correlating metric, USC [7]. Hence, in this work, we aim to find safety-related errors of the object detector and use them to characterize an AV’s collision risks. Fig. 1 depicts the overall framework. In particular, our previous work presented a design-time safety-focused metric, called uncompromising spatial constraints (USC), which highly correlates with AV collision rates [7]. This work’s objective, therefore, is to reproduce it and generate a relevant risk level during operation. To achieve this, there are two challenges: • The first and main challenge lies in the lack of ground-truth labels at run time. To overcome it, we ask the critical question whether employing a separate object retrieval pipeline and measuring the inconsistencies from the original object detector’s predictions can serve as a proxy to the ground truths. Specifically, considering cost factors and the recent breakthrough in monocular depth estimation, we employ the state-of-the-art ZoeDepth [8] and implement image processing techniques to retrieve safety-critical objects. Our key insight, thereby, is an experimental validation that confirms the inconsistencies between the two sets of predictions, measured by Intersection-over-Union (IoU) and depth discrepancy, are closely linked to safety-related errors in the 3D object detector when compared to ground-truth data. • With the confirmed correlation, the second challenge is how to associate them with the desired risk quantifier, USC. Our solution is to use fuzzy logic, which can flexibly model non-linear functions while tolerating potential imprecision [9]. Concretely, we build a fuzzy inference system (FIS) using three distinct knowledge- and data-driven approaches and finally find the one adopting the global particle swarm optimization algorithm gives the best result within a separate testing dataset. To demonstrate the efficacy of our framework, we run it with a state-of-the-art 3D object detector, PGD [10], across the large-scale nuScenes dataset [11] in six conditions, including nominal scenes, night scenes, scenes with rare objects, and scenes augmented with rain, snow, or Gaussian noises. In addition, we implement our monitor on a baseline AV in simulation and showcase that it can indeed infer a useful risk indicator that helps protect the AV. Altogether, our work can be seen as a novel attempt to derive collision risk estimates from the object detection function using a single data source, i.e., images from one monocular camera. Moreover, as we shall show in later sections, the framework offers good interpretability and adaptability, allowing developers to continuously analyze and improve the monitor."
https://arxiv.org/html/2411.08637v1,"Robot See, Robot Do: Imitation Reward for Noisy Financial Environments","The sequential nature of decision-making in financial asset trading aligns naturally with the reinforcement learning (RL) framework, making RL a common approach in this domain. However, the low signal-to-noise ratio in financial markets results in noisy estimates of environment components, including the reward function, which hinders effective policy learning by RL agents. Given the critical importance of reward function design in RL problems, this paper introduces a novel and more robust reward function by leveraging imitation learning, where a trend labeling algorithm acts as an expert. We integrate imitation (expert’s) feedback with reinforcement (agent’s) feedback in a model-free RL algorithm, effectively embedding the imitation learning problem within the RL paradigm to handle the stochasticity of reward signals. Empirical results demonstrate that this novel approach improves financial performance metrics compared to traditional benchmarks and RL agents trained solely using reinforcement feedback.","Financial asset trading is well-suited to the reinforcement learning (RL) framework, where an agent learns through trial and error by interacting with its environment and receiving feedback on its actions [1]. As financial markets are notorious for their non-stationarity and low signal-to-noise ratio [2], they present a unique challenge for applying RL to learn trading strategies. A crucial aspect of the RL setup is designing the reward function, which provides the agent with necessary feedback on its performance [3]. Consequently, managing noisy samples from the underlying reward function of the financial environment presents a significant problem that is often overlooked [4]. In general, the reward function in RL represents the utility function that quantifies the agent’s preferences for different outcomes. Designing the reward function is challenging because it needs to be efficient and easy to optimize, as well as convey the desired task [5]. In financial applications, the reward function is usually designed in terms of the agent’s profit and the risk taken to realize that profit. However, the high degree of stochasticity in noisy market environments often leads to a high degree of stochasticity in the training process and policy learning. Most research has focused on dealing with this noise by improving state and policy representations [6, 7, 8]. However, less attention has been paid to addressing noise specifically within the reward signal itself [9], despite the fact that profit-based reward signals are noisy as it is often difficult to distinguish whether the gains or losses are genuinely driven by underlying price movements or are just random fluctuations around the actual price. In [10], the authors introduce a reward function that combines the agent’s profit with a hindsight bonus to incentivize long-term trading decisions. This approach mitigates the noise associated with short-term price fluctuations in the agent’s profit and demonstrates the agent’s robust performance. In [8], an imitation learning technique is employed where the expert takes a long position at the lowest price and a short position at the highest price within a given day. These expert actions are incorporated into the agent’s policy learning to reduce the inefficient exploration phase. By combining the agent’s reward (profit) with the expert’s actions, the authors reduce the exploration phase and show a more robust performance. Previous research on stochasticity and corruption of the reward signal has shown that without simplifying assumptions, RL agents cannot be expected to avoid this problem [11]. Therefore, providing agents with different data sources and feedback is often the safest option [9]. We build on these findings by using a specific trend labeling algorithm that introduces our agent with expert feedback and provides a more robust source of information compared to the previously mentioned methods. There are a variety of methods that deal with the stochasticity of reward signals, including those that use human feedback [12], reward signal estimators [4, 13], distributional reward estimators [14], and other data-driven methods such as imitation learning (IL) [15], where the agent learns to imitate the expert’s behavior based on demonstrations. Our work is inspired by methods that reduce IL to an RL problem without explicitly learning the reward function, but inferring the reward signal from the expert’s demonstrations [16, 17, 18]. This simple approach rewards the agent for matching the expert’s action in a given state and punishes it otherwise. This encourages the agent to align itself with the expert’s demonstrations over a long horizon [16]. The key contribution of this paper is the introduction of a novel reward function that leverages IL and uses a trend labeling algorithm as an expert. This means that the agent learns not only from its own experience (reinforcement feedback) but also by mimicking the expert’s actions (imitation feedback). By combining both sources of information in a model-free RL framework, the agent gains a more robust understanding of the market. We empirically validate our approach on intraday futures data and show significant improvements in risk-adjusted performance metrics compared to benchmarks consisting of traditional methods and RL agents trained solely using reinforcement feedback."
https://arxiv.org/html/2411.08634v1,On the Application of Model Predictive Control to a Weighted Coverage Path Planning Problem,"This paper considers the application of Model Predictive Control (MPC) to a weighted coverage path planning (WCPP) problem. The problem appears in a wide range of practical applications, such as search and rescue (SAR) missions. The basic setup is that one (or multiple) agents can move around a given search space and collect rewards from a given spatial distribution. Unlike an artificial potential field, each reward can only be collected once. In contrast to a Traveling Salesman Problem (TSP), the agent moves in a continuous space. Moreover, he is not obliged to cover all locations and/or may return to previously visited locations. The WCPP problem is tackled by a new Model Predictive Control (MPC) formulation with so-called Coverage Constraints (CCs). It is shown that the solution becomes more effective if the solver is initialized with a TSP-based heuristic. With and without this initialization, the proposed MPC approach clearly outperforms a naive MPC formulation, as demonstrated in a small simulation study.","Many path planning applications in robotics try to find an optimal path while maximizing some sort of reward. The reward is commonly related to the proximity to given reference value, guiding the system via an artificial potential field (AFP), or keeping it away from unsafe areas via barrier functions. Model Predictive Control (MPC) casts this problem into a numerical optimization program, with an objective function and constraints. Over the past years, it has become a standard approach for path planning, due to its natural handling of general reward functions, system dynamics, and input and state constraints. Coverage Path Planning (CPP) aims for the system to cover an entire area of the state space, or as large a part of it as possible. Problem instances appear in many robotic applications, such autonomous lawn mowers, vacuum cleaners, agricultural robots, and arial / underwater drones used for inspection or surveillance. From the perspective of common MPC-based path planning, this means a uniform objective function. However, for CPP, the objective function changes dynamically in the sense that the reward at an already visited position, and some radius around it, drops to zero. Efficient motion patterns or policies are commonly used for CPP, including a boustrophedon (snake-like) path or straight driving with random reflection angles when hitting boundaries [1]. The main principle is, clearly, to minimize any overlaps in the track of the robot. This paper considers the extended problem of Weighted Coverage Path Planning (WCPP). The main difference to the CPP is that the objective function is not uniform, but weighted by a coverage priority. Problem instances of this also appear frequently, e.g., in search and rescue (SAR) or surveillance missions, where the goal is to find or detect a target whose probability of presence on a map is not uniformly distributed. Correspondingly, the motivational example for this paper is an unmanned aerial vehicle (UAV) with the task of finding a missing person in a given area as fast as possible. The coverage priority is indicated by a probability map, which serves as the reward function and represents a belief distribution integrating all available information at the current time, e.g., from sensor measurements, the observation of eye witnesses, or human behavioral models in the given map [2]. For practical applications, a probabilistic model could be used to dynamically update the belief distribution based on all available information, and thus guide the planning of the mission [3]. I-A Existing Literature CPP is about finding a path that covers all specified points or an entire area or region of interest while avoiding obstacles [4]. A comprehensive overview of CPP algorithms for robotic applications is given in [5]. The article covers both classical and heuristic-based algorithms. These two categories include a whole variety of basic approaches, such as AFPs, greedy search and graph search algorithms, and bio-inspired approaches, such as genetic algorithms or ant colony optimization. A key point in the discussion in [5] is that the resulting paths should be as smooth as possible. Namely, the avoidance of sharp turns in the path prevent premature wear of the robot’s components and it increases the efficiency, especially in UAV applications, where it advantageous for the average speed of the UAV to fly straight ahead or in smooth curves. In this spirit, the approach in [6] proposes a path search algorithm using ant colony optimization based on the Lin-Kernighan heuristic, followed by a smoothing step using a customized approach based on Fourier series. The resulting dynamically smooth trajectory is then fed to the UAV, which is operated by an MPC-based controller. Numerical optimization has become increasingly popular for path planning over the recent years, due to the availability of more powerful hardware and increasingly efficient solvers. MPC, in particular, has been successfully employed for collision-free path planning for autonomous road vehicles [7] and for UAVs [8]. MPC has been used in combination with AFPs [9, 10], and also for CPP with obstacles using a mixed-integer linear programming (MILP) formulation [11]. The area is covered by a uniform grid, where each cell defines a single way point. The way points are subsequently represented as discrete decision variables within the optimization problem, where the objective is to cover the maximum number of (equally weighted) way points. Related problems to the CPP include the Traveling Salesman Problem (TSP) and its variants, most notably the Orienteering Problem (OP) [12]. In contrast to the TSP, where all vertices must be visited and the goal is to minimize the traveled distance, the OP is concerned with maximizing the total reward collected within a limited time frame, without necessarily visiting all the vertices [13]. Both of these problems are known to be NP-hard [14]. Yet it is desirable, for many applications, to extend them further by including the dynamics of an agent in the problem formulation [15]. Recently, a mathematical framework to tackle this problem using techniques from optimal control has been proposed [15, 16]. In addition to the dynamics of an agent, this formulation also accounts for the movement of the sensors, i.e., in this case, a camera. However, the approach relies on nonsmooth calculus and considers only discrete regions of interest. I-B Contributions This paper proposes a new MPC approach for the WCPP problem, using coverage constraints (CCs). The proposed MPC formulation works with any agent moving governed by a nonlinear dynamic model and moving in a continuous space with obstacles. The reward function is arbitrary, but in contrast to an AFP, each reward can only be collected once. This is enforced by the use of quadratic constraints. In contrast to the TSP, the agent is not obliged (and may in fact be far from able to) cover all locations within the given prediction horizon. Furthermore, the paper shows that the solution becomes more effective if the MPC solver is initialized with a TSP-based heuristic. The heuristic is based on a set of key points, which are derived from a Gaussian mixture model that is used to approximate the reward function."
https://arxiv.org/html/2411.08579v1,NavAgent: Multi-scale Urban Street View Fusion For UAV Embodied Vision-and-Language Navigation,"Vision-and-Language Navigation (VLN), as a widely discussed research direction in embodied intelligence, aims to enable embodied agents to navigate in complicated visual environments through natural language commands. Most existing VLN methods focus on indoor ground robot scenarios. However, when applied to UAV VLN in outdoor urban scenes, it faces two significant challenges. First, urban scenes contain numerous objects, which makes it challenging to match fine-grained landmarks in images with complex textual descriptions of these landmarks. Second, overall environmental information encompasses multiple modal dimensions, and the diversity of representations significantly increases the complexity of the encoding process. To address these challenges, we propose NavAgent, the first urban UAV embodied navigation model driven by a large Vision-Language Model. NavAgent undertakes navigation tasks by synthesizing multi-scale environmental information, including topological maps (global), panoramas (medium), and fine-grained landmarks (local). Specifically, we utilize GLIP to build a visual recognizer for landmark capable of identifying and linguisticizing fine-grained landmarks. Subsequently, we develop dynamically growing scene topology map that integrate environmental information and employ Graph Convolutional Networks to encode global environmental data. In addition, to train the visual recognizer for landmark, we develop NavAgent-Landmark2K, the first fine-grained landmark dataset for real urban street scenes. In experiments conducted on the Touchdown and Map2seq datasets, NavAgent outperforms strong baseline models. The code and dataset will be released to the community to facilitate the exploration and development of outdoor VLN.","UAV Vision-and-Language Navigation (VLN) is a specialization of embodied intelligence for navigation applications in the aerial domains[1, 2, 3, 4, 5, 6, 7]. It aims to explore how to enable UAV embodied agents to navigate in unknown urban environments based on natural language commands and environmental observations. This approach has a wide range of applications across various fields, including inspection and monitoring, search and rescue, and low-altitude logistics[8, 9, 10]. However, existing research on VLN primarily focuses on indoor scenes[11, 12, 13, 14, 15]. In contrast, the outdoor urban environments targeted by UAV VLN tasks involve a much larger spatial scale, greater complexity, and sparser landmarks, making them more challenging[16, 17, 18, 19, 20]. Recent studies have demonstrated that large Vision-Language Models, which are capable of processing multimodal inputs, exhibit strong generalization abilities and outstanding performance in embodied tasks[21, 22, 23, 24, 25, 26, 27, 28]. Given this, our objective is to develop the first embodied large Vision-Language Model specifically designed for UAV VLN tasks, enabling UAV agents to navigate autonomously in urban street scenes. However, there are two primary challenges in this process. (1) Difficulty in Matching Fine-Grained Landmarks in Panoramic Observation Images. When the agent is positioned at any observation point, it perceives the surrounding environment through a panoramic image captured at that location. The landmarks that need to be recognized are typically fine-grained targets located on both sides of the road, which comprise less than 5\% of the pixels in the panoramic image. Furthermore, the texts associated with these landmarks are often not simple nouns but rather complex phrases that include multiple modifiers, such as “a green mailbox” or “two red garbage cans”. As a result, ordinary image encoders struggle to accurately match these intricate details. (2) Difficulty in Encoding Overall Environmental Information in the Decision-Making Process. The environments in which the agents operate are complex, requiring the integration of various dimensions of overall environmental information. This includes visual data (e.g., observation images), semantic information (e.g., landmark categories and locations), and geographic data (e.g., environmental map). Not only do these data types have different representations, but they also exhibit a high degree of heterogeneity in both space and time, which complicates the encoding process. Furthermore, the dynamic nature of the environmental information necessitates real-time updates as the agent moves, significantly increasing the challenges associated with coding. To address the aforementioned challenges, we propose a multi-scale environment fusion-enhanced VLN model called NavAgent. As illustrated in Figure 1, this model integrates the global environmental topology map, the panoramic view of the current observation position, and local fine-grained landmark data. This integration facilitates accurate and stable VLN for UAV agents, specifically: Figure 1: Schematic diagram of the VLN model augmented by multi-scale environment fusion, with the environment topology map containing the overall information of the environment in the yellow box, the observation image of the agent at this point in the red box, the fine-grained landmarks extracted from the observation image in the green box, and the navigation text in the black box . Prior research has demonstrated the effectiveness of GLIP in fine-grained target recognition and matching tasks within a general-purpose domain[23]. We modify the structure of GLIP to develop the visual recognizer for landmark. The visual recognizer facilitates fine-grained matching between the images of the environment observed by the agent during navigation and the text of landmarks, enabling precise identification of landmarks present in the observation images. To train the visual recognizer for landmark, we first frame the landmarks within the street images selected from Google Street View[29]. We then use BLIP2[21] to generate descriptions for the images of the landmarks, ultimately creating a dataset with 2,000 landmark annotations. This dataset, named NavAgent-Landmark2K, is the first landmark recognition dataset designed for outdoor VLN. Comparative experiments have shown that the visual recognizer for landmark fine-tuned with this dataset improves accuracy by 9.5\% compared to the GLIP in recognizing landmark images within the context of outdoor VLN. Further, we develop a dynamically evolving scene topology map to integrate environmental information and design the topology map encoder to capture global environmental features. Specifically, we record navigable positions in the urban scene as nodes, initially capturing each node’s position and the orientation relationships between nodes. We then explore the current node and its contiguous nodes, combining them into a cohesive scene topology map. To integrate environmental information, we employ an image encoder to extract visual features from the current observation image. We then utilize a cross-attention mechanism to incorporate these visual features into the scene topology map, facilitating the fusion of multimodal information. After the UAV agent moves, the scene topology map retains historical information and updates the nodes to integrate new environmental data, effectively encoding dynamic and complex environments. In summary, our main contributions are: (1) We propose NavAgent, the first urban UAV embodied navigation model driven by a large Vision-Language Model, enabling autonomous navigation of agent in urban environments through the fusion of multi-scale environmental information. (2) We design and train the visual recognizer for landmark that recognizes fine-grained landmarks by calculating similarity scores by matching region features extracted from observed images with text features extracted from landmark descriptions. Experimental results indicate that the visual recognizer of landmark enhances accuracy by 9.5% in the fine-grained landmark recognition task when compared to the GLIP. (3) We construct a dynamically growing scene topology map and employ the topology map encoder to encode individual nodes and their spatial relationships, thereby enhancing the planning ability of agent for long-distance navigation. (4) We develop the first fine-grained landmark dataset for real urban street scenes, named NavAgent-Landmark2K. This dataset comprises 2,000 image-text pairs, where the images represent fine-grained landmarks occupying approximately 5\% of the pixel area, and the accompanying text consists of landmark phrases that include multiple modifiers. (5) In the experiments conducted on the Touchdown and Map2seq datasets, our proposed NavAgent outperforms the powerful baseline models, achieving improvements of 4.6\% and 2.2\% compared to VELMA on the development sets of two datasets."
https://arxiv.org/html/2411.08395v1,MambaXCTrack: Mamba-based Tracker with SSM Cross-correlation and Motion Prompt for Ultrasound Needle Tracking,"Ultrasound (US)-guided needle insertion is widely employed in percutaneous interventions. However, providing feedback on the needle tip position via US image presents challenges due to noise, artifacts, and the thin imaging plane of US, which degrades needle features and leads to intermittent tip visibility. In this paper, a Mamba-based US needle tracker MambaXCTrack utilizing structured state space models cross-correlation (SSMX-Corr) and implicit motion prompt is proposed, which is the first application of Mamba in US needle tracking. The SSMX-Corr enhances cross-correlation by long-range modeling and global searching of distant semantic features between template and search maps, benefiting the tracking under noise and artifacts by implicitly learning potential distant semantic cues. By combining with cross-map interleaved scan (CIS), local pixel-wise interaction with positional inductive bias can also be introduced to SSMX-Corr. The implicit low-level motion descriptor is proposed as a non-visual prompt to enhance tracking robustness, addressing the intermittent tip visibility problem. Extensive experiments on a dataset with motorized needle insertion in both phantom and tissue samples demonstrate that the proposed tracker outperforms other state-of-the-art trackers while ablation studies further highlight the effectiveness of each proposed tracking module.","I INTRODUCTION In various percutaneous intervention procedures, ultrasound (US)-guided needle insertion is commonly adopted in minimally invasive interventions, such as tissue biopsy, tumor ablation, regional anesthesia [1], etc. As a non-invasive, portable, safe, and cost-effective imaging modality [2], US provides real-time intraoperative imaging of the needle and tissue, thereby minimizing the risk of accidental injury to vessels or critical organs. Despite these advantages, US imaging has inherent limitations, including low resolution and susceptibility to noise and artifacts [1], which can obscure or distort the needle tip position. Furthermore, under the narrow US imaging plane [2], small structures, such as the needle tip, can disappear intermittently when they are obscured by anatomical structures or not co-planar with the US imaging plane. These challenges underscore the necessity for robust and accurate needle tracking to ensure successful insertion procedures under challenging environments. Prior to the widespread adoption of learning-based needle trackers, traditional methods, such as the statistical filter [3] and Gabor filter [4], achieved somewhat satisfactory performance but were hindered by complex workflows and sensitivity to hyper-parameters, failing to address challenging environmental factors in US imaging. A method based on discriminative correlation filter (DCF) has been proposed in [5], but correlation filter-based methods can be susceptible to background distraction and image distortion [6]. Later, DCF [5] is integrated with an optical tracking system for higher accuracy [7], but it is constrained by the cumbersome deployment of the optical tracking system. Recently, deep learning methods based on convolutional neural network (CNN) and transformer [8] have gained popularity in tracking tasks [9, 10, 11, 12], including US needle tracking. Mwikirize et. al. proposed a US needle tracker with a two-step structure based on a fully convolutional network and region-based CNN [13]. A paradigm utilizing digital subtraction is proposed in [14], which augments tip features to enhance visibility prior to tracking. However, these two methods have non-end-to-end structures that require multiple steps to localize the needle tip, potentially affecting robustness and accuracy. Since the needle tip is often obscured or distorted under artifacts and noise, some other methods perform needle shaft segmentation before localizing the needle tip [15, 16]. Although segmentation mask of the needle shaft can provide useful cues on the axial position of needle tip, accumulative error and discrepancy can be accidentally introduced by this two-stage workflow, and again constrained by the disadvantages of non-end-to-end methods. Additional segmentation mask data is also required to train these segmentation models, leading to additional obstacles for the model deployment. As an effective structure, cross-correlation (X-Corr) has been widely adopted in end-to-end learning-based trackers. It measures the similarity between a reference template and a search region by performing convolution with sliding windows. The target location is then obtained from the induced similarity score. Following this diagram, many trackers based on X-Corr have been proposed [10, 11, 17, 18]. However, the existing convolutional X-Corr has a limited modeling range constrained by the kernel size. It cannot learn distant semantic features (e.g. needle shaft) which are important for US needle tracking, since local information can unpredictably become unreliable due to degradation by noise and artifacts. In addition to the challenging environment with noise and artifacts, the intermittently visible needle tip poses another problem that hinders accurate needle tracking. This issue can be caused by deviation of US imaging plane or obstruction by anatomical structures [19]. Mwikirize et. al. proposed a single-shot needle tracker by integrating historical frames to enhance needle tip features, addressing scenarios where the tip is imperceptible or the shaft is invisible [20]. Although it integrates historical information, its feature pre-enhancement can unexpectedly shift the original latent features, causing potential inherent information loss. Integrating historical needle motion into visual tracking presents another approach, since motion information can serve as an effective non-visual prompt to prevent tracking failure when the needle tip is invisible. A motion prediction module is integrated with a visual tracker in [6], yet it is constrained by its explicit motion prediction that poses challenges on generalizability when encountering motion from unseen domains. Mamba [21, 22] has recently drawn considerable attention and is being applied in real-world tracking tasks [23, 24, 25]. Based on the structured state space models (SSMs) [26], Mamba has a computationally efficient long-range lossless modeling capability with its selective scan mechanism. This capability allows Mamba-based trackers by leveraging long-range information, such as aggregating a video-level template set [24] or integrating historical frames [23]. It should be noted that a transformer-based tracker can hardly achieve long-range modeling under similar model size and complexity since the transformer has quadratic time complexity and memory requirement. It also requires positional encoding that may not effectively capture lossless long-range dependencies compared to Mamba which is designed specifically for such tasks. Thus Mamba usually outperforms transformer-based methods under similar model size and complexity [21]. Since no Mamba-based tracker has yet been proposed for US needle tracking, developing an effective Mamba-based needle tracker under US imaging remains an open research area. To address the aforementioned challenges of noise, artifacts, and intermittent visibility in US needle tracking, in this paper, a Mamba-based US needle tracker MambaXCTrack utilizing SSM cross-correlation (SSMX-Corr) and implicit motion prompt is proposed. To the best of our knowledge, it is the first time a Mamba-based tracker has been adopted in US needle tracking. It is also the first time that cross-correlation is implemented with SSM. Leveraging the long-range modeling capability of SSMs, SSMX-Corr enables global search and long-range modeling of distant semantic features. When the tip feature is degraded by noise and artifacts, SSMX-Corr avoids tracking failure by implicitly learning distant semantic feature that potentially comes from visual cues like needle shaft, rather than by explicitly performing segmentation on the needle shaft like existing methods [15, 16]. By further integrating the proposed cross-map interleaved scan (CIS), SSMX-Corr enjoys global search without losing local pixel-wise interaction between search and template maps to keep positional inductive bias, while existing convolutional X-Corr models [9, 10] only consider local modeling. To address the intermittent visibility of the needle tip, the implicit low-level motion descriptor is introduced as a non-visual prompt in addition to visual features that can unpredictably become unreliable. Different from [6] that trains an external motion predictor to explicitly predict the future motion, the proposed workflow preprocesses motion to obtain the low-level motion descriptor, which is then implicitly integrated with visual features. This implicit low-level motion integration introduces image-agnostic raw motion and ensures an end-to-end network to enhance training stability and tracking robustness. Extensive evaluations on a dataset of motorized needle insertions in both phantom and animal tissue demonstrate MambaXCTrack’s superior performance compared to state-of-the-art methods. The main contributions are fourfold: • SSMX-Corr improves existing cross-correlation with SSM by globally searching and long-range modeling distant semantic features, thus learning potential distant visual cues. This represents the first effort to adopt Mamba effectively for US needle tracking. • CIS is proposed to provide SSMX-Corr with local pixel-wise interaction and positional inductive bias in addition to global search to enhance overall tracking performance. • Implicit low-level motion descriptor is adopted to provide a non-visual prompt, thus leveraging motion information to address the challenge of intermittent needle visibility to achieve robust and consistent tracking. • The proposed tracker achieves SOTA performance on both phantom and tissue experiments. Further ablation studies show the effectiveness of the proposed modules."
https://arxiv.org/html/2411.08279v1,MBA-SLAM: Motion Blur Aware Dense Visual SLAM with Radiance Fields Representation,"Emerging 3D scene representations, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have demonstrated their effectiveness in Simultaneous Localization and Mapping (SLAM) for photo-realistic rendering, particularly when using high-quality video sequences as input. However, existing methods struggle with motion-blurred frames, which are common in real-world scenarios like low-light or long-exposure conditions. This often results in a significant reduction in both camera localization accuracy and map reconstruction quality. To address this challenge, we propose a dense visual SLAM pipeline (i.e. MBA-SLAM) to handle severe motion-blurred inputs. Our approach integrates an efficient motion blur-aware tracker with either neural radiance fields or Gaussian Splatting based mapper. By accurately modeling the physical image formation process of motion-blurred images, our method simultaneously learns 3D scene representation and estimates the cameras’ local trajectory during exposure time, enabling proactive compensation for motion blur caused by camera movement. In our experiments, we demonstrate that MBA-SLAM surpasses previous state-of-the-art methods in both camera localization and map reconstruction, showcasing superior performance across a range of datasets, including synthetic and real datasets featuring sharp images as well as those affected by motion blur, highlighting the versatility and robustness of our approach. Code: https://github.com/WU-CVGL/MBA-SLAM.","Simultaneous Localization and Mapping (SLAM) is a fundamental problem in 3D vision with broad applications, including autonomous driving, robotic navigation, and virtual reality. While traditional sparse SLAM methods [1, 2] use sparse point clouds for map reconstruction, recent learning-based dense SLAM systems [3, 4, 5, 6] focus on generating dense maps, which are essential for downstream applications. Due to their ability to enable photo-realistic 3D scene representations, Neural Radiance Fields (NeRF) [7] and 3D Gaussian Splatting (3DGS) [8] have been explored in conjunction with SLAM systems [9, 10, 11, 12, 13, 14, 15, 16, 17], demonstrating significant improvements in map representation and high-fidelity surface reconstruction. However, existing methods heavily rely on high-quality, sharp RGB-D inputs, which poses challenges when dealing with motion-blurred frames, often encountered in low-light or long-exposure conditions. Such conditions can significantly degrade the localization and mapping performance of these methods. The difficulties that motion-blurred images present to dense visual SLAM systems stem from two primary factors: 1) inaccurate pose estimation during tracking: current photo-realistic dense visual SLAM algorithms depend on sharp images to estimate camera poses by maximizing photometric consistency. However, motion-blurred images, commonly occurring in real-world scenarios, violate this assumption, making it difficult to accurately recover poses from blurred frames. These inaccurately tracked poses, in turn, affect the mapping process, leading to inconsistent multi-view geometry. 2) inconsistent multi-view geometry in mapping: the mismatched features between multi-view blurry images introduce erroneous 3D geometry information, resulting in poor 3D map reconstruction. This will degrade map reconstruction quality, which subsequently affects the tracking process. Combined these two factors, existing dense virtual SLAM systems would usually exhibit performance degradation when handling motion-blurred images. To address these challenges, we introduce MBA-SLAM, a photo-realistic dense RGB-D SLAM pipeline designed to handle motion-blurred inputs effectively. Our approach integrates the physical motion blur imaging process into both the tracking and mapping stages. Specifically, we employ a continuous motion model within the \mathbf{SE}(3) space to characterize the camera motion trajectory within exposure time. Given the typically short exposure duration, the trajectory of each motion-blurred image is represented by its initial and final poses at the start and end of the exposure time respectively. During tracking, we firstly render a reference sharp image corresponding to the latest keyframe, from our learned 3D scene representation. The rendered image can then be reblurred to match the current captured blurry image based on the predicted motion trajectory from previous optimization iteration. We enforce the photo-metric consistency between the tracked blurry image and the reblurred image to further refine the camera motion trajectory within exposure time. In the mapping stage, we jointly optimize the trajectories of a set of sparsely selected frames (i.e. keyframes) and the 3D scene representation by minimizing the photo-metric consistency loss. Two commonly used scene representations are explored in our implementation, i.e. implicit neural radiance fields [12] and explicit 3D Gaussian Splatting [8]. Both representations exhibit different advantages and disadvantages. In particular, NeRF-based implementation is able to achieve higher frame rates (FPS) but exhibits lower rendering quality than 3D-GS based implementation. In contrary, 3D-GS based implementation delivers better rendering quality at the expense of lower FPS. We present both implementations to satisfy the requirements of different usage scenarios. We evaluate the performance of MBA-SLAM thoroughly by using both the sharp and blurry datasets, against prior state-of-the-art methods. In particularly, we conducted evaluations with both a public synthetic blurry dataset [18] and a self-captured blurry dataset. The real dataset is collected with a RealSense RGB-D camera under low-lighting conditions. To further evaluate the performance of MBA-SLAM on sharp images, we exploit the commonly used public datasets from Replica [19], ScanNet [20] and TUM RGB-D [21]. The experimental results demonstrate that MBA-SLAM not only delivers more robust performance with blurry images, but also has superior performance with sharp images, than prior state-of-the-art methods. In summary, our contributions are as follows: • We present a novel photometric bundle adjustment formulation specifically designed for motion blurred images, establishing an RGB-D 3DGS/NeRF-based SLAM pipeline that demonstrates robustness against motion blur. • Our SLAM pipeline is enhanced by integrating a motion blur-aware tracker, resulting in improved tracking accuracy, which in turn leads to superior mapping performance. • We illustrate how this formulation enables the acquisition of precise camera trajectories and high-fidelity 3D scene maps from motion-blurred inputs. • Our experimental results demonstrate the superior tracking and mapping performance of MBA-SLAM across various datasets, outperforming previous state-of-the-art NeRF-based and 3DGS-based SLAM methods, including both synthetic and real motion blurred datasets. • Our method also performs well and surpasses previous state-of-the-art dense visual SLAM pipelines on commonly used standard datasets with sharp images. MBA-SLAM is based on three preliminary seminar papers of the authors, i.e., MBA-VO [18], BAD-NeRF [22], and BAD-Gaussians [23], which have been accepted by ICCV 2021 (oral), CVPR 2023, and ECCV 2024, respectively. In this paper, we extend these works in several significant ways: 1) we integrate them into a comprehensive SLAM pipeline, by exploiting the motion blur aware tracker from MBA-VO [18] and the motion blur aware bundle adjustment algorithm from either BAD-NeRF [22] or BAD-Gaussians [23]; 2) we replace the vanilla NeRF representation of BAD-NeRF [22] with a more efficient tri-plane based representation, significantly improving the training efficiency by a factor of 100 times; 3) all the experimental evaluations are newly conducted to thoroughly verify the effectiveness of the pipeline, against prior state-of-the-art methods."
https://arxiv.org/html/2411.08163v1,Emergent functional dynamics of link-bots,"Synthetic active collectives, composed of many nonliving individuals capable of cooperative changes in group shape and dynamics, hold promise for practical applications and for the elucidation of guiding principles of natural collectives. However, the design of collective robotic systems that operate effectively without intelligence or complex control at either the individual or group level is challenging. We investigate how simple steric interaction constraints between active individuals produce a versatile active system with promising functionality. Here we introduce the link-bot: a V-shape-based, single-stranded chain composed of active bots whose dynamics are defined by its geometric link constraints, allowing it to possess scale- and processing-free programmable collective behaviors. A variety of emergent properties arise from this dynamic system, including locomotion, navigation, transportation, and competitive or cooperative interactions. Through the control of a few link parameters, link-bots show rich usefulness by performing a variety of divergent tasks, including traversing or obstructing narrow spaces, passing by or enclosing objects, and propelling loads in both forward and backward directions. The reconfigurable nature of the link-bot suggests that our approach may significantly contribute to the development of programmable soft robotic systems with minimal information and materials at any scale.","I Link-bot structure Individual bots are 3D printed, consisting of a cylindrical body (diameter, d=1.5 cm) on seven circumferentially equidistant legs, pictured in Fig. 1A(i). The legs are tilted, allowing the bot to self-propel in a preferred direction when placed on a vibrating surface. A circular flat arena of diameter 45 cm is vertically vibrated at a frequency of \approx 80 Hz and an amplitude of 70 \upmum, causing a single bot to move at an average speed of 8 \mathrm{cm}/\mathrm{s}. The arena vibration properties are kept constant in all experiments. Fig. 1A shows an example trajectory (ii) and speed profile (iii) of a single bot moving freely for 20 s. The corresponding plot of the translational mean squared displacement with respect to time lag is given in Fig. 1A(iv), which shows ballistic motion (\sim t^{2}) over short time scales and diffusive motion (\sim t) over large time scales, typical of active Brownian motion. Bringing multiple bristle-bots into a collective provides rich and interesting behavior [3, 29]. Previous work on bristle-bots that are connected to form an active chain focuses on elasto-active systems [20, 30, 31] and the mechanical coupling of connected active chains [32, 31]. In this work, we focus on systems of bots that are connected by rigid links with rotational constraints: link-bots. The link-bot is created by connecting N bots with N-1 links in a V-shaped arrangement, inspired by the formations observed in troops and migrating birds [33]. An example where N=7 is pictured in Fig. 1B. Each bot has a cuboidal crest on its top surface, which allows it to fit into the ribbon-shaped notches on both ends of the links. These links serve to maintain constant interbot distances between neighbors, transmit the motion of each bot to its neighbors and constrain each bot’s rotation. Links are characterized by three parameters: length between notches L, notch angle \theta, and spread angle \alpha. The center bot and its neighbors are connected using two center links (pictured in green in Fig. 1B(i)), while all other bots are connected with side links (pictured in blue). The center and side links always have the same length L=1.6 cm, although their angles may differ and will be reported using subscripts c and s for the center and side, respectively. To produce a V-shaped arrangement, the links on one side of the center bot are reversed in relation to the links on the other side. This symmetry effectively suppresses undesired random deformations, such as crumpling and curling, which are often observed in active filaments [34]. Two notable features of the link-bot in comparison to previous connected bristle-bot systems are the broken asymmetry introduced by the V-shape and the threshold constraints imposed by the link notch angles. These features allow for a rich variety of collective behaviors to emerge from its characteristic active chain dynamics to provide a multi-functional soft robotic system."
https://arxiv.org/html/2411.07933v1,Prediction of Acoustic Communication Performance for AUVs using Gaussian Process Classification,"Cooperating autonomous underwater vehicles (AUVs) often rely on acoustic communication to coordinate their actions effectively. However, the reliability of underwater acoustic communication decreases as the communication range between vehicles increases. Consequently, teams of cooperating AUVs typically make conservative assumptions about the maximum range at which they can communicate reliably. To address this limitation, we propose a novel approach that involves learning a map representing the probability of successful communication based on the locations of the transmitting and receiving vehicles. This probabilistic communication map accounts for factors such as the range between vehicles, environmental noise, and multi-path effects at a given location. In pursuit of this goal, we investigate the application of Gaussian process binary classification to generate the desired communication map. We specialize existing results to this specific binary classification problem and explore methods to incorporate uncertainty in vehicle location into the mapping process. Furthermore, we compare the prediction performance of the probability communication map generated using binary classification with that of a signal-to-noise ratio (SNR) communication map generated using Gaussian process regression. Our approach is experimentally validated using communication and navigation data collected during trials with a pair of Virginia Tech 690 AUVs.","Teams of cooperating autonomous underwater vehicles (AUVs) typically use acoustic communication to coordinate their actions. Underwater acoustic communication is characterized by high bit-error rates due to numerous practical challenges, including multi-path, environmental noise, etc [1, 2]. This poses a challenge for teams of AUVs that must plan future communication events in order to coordinate future actions. One solution is to assume that communication is deterministic but range-limited, see [3], [4] among many others. When applied to the unreliable underwater acoustic communication channel, however, the assumption that communication events are almost always successful may hold only if the communication range is conservatively small, which limits its use in real-world applications. In contrast, we seek methods of predicting acoustic communication performance that are less conservative and that represent its fundamentally stochastic behavior. Rather than assuming that communication is always successful so long as the range is conservatively short, we seek methods that predict a high probability of communication success in some locations and at certain ranges, and a lower probability of success at other locations and at longer ranges. Predicting acoustic communication performance is a topic that has received limited attention in the literature. The authors in [2, 5, 6, 7, 8] employ Gaussian processes to predict received signal strength (RSS) for evaluating communication performance. In our prior work [9, 10], we modeled the signal-to-noise ratio (SNR) of successful acoustic communication events using a Gaussian process (GP). However, we note that RSS or SNR do not explicitly address the probability of one vehicle successfully communicating with another vehicle. Indeed, one needs to select a threshold above which the value SNR or RSS corresponds to a successful communication event. On the other hand, in this paper, we seek to explicitly address the probability of successful communication. We intuitively model the probability of successful communication with the corresponding successful and unsuccessful communication events using GP classification, and we evaluate the benefit of this intuition by directly comparing against models generated from SNR using GP regression. To the best of our knowledge, this is the first study that seeks to model acoustic communication performance with a GP classifier. Our approach is data-driven. A team of AUVs use occurrences of successful and unsuccessful communication events to compute a map of communication performance. The map is a Gaussian process binary classification of communication success that is used to predict the probability of communication success between an AUV at one location and an AUV at another location. At the beginning of a mission, the AUVs may assume a simple range-dependent prior. As data on communication performance is acquired during the mission, the communication performance map evolves, and the AUVs generate less conservative and more accurate communication performance predictions when planning their actions. Several Virginia Tech 690 AUVs, which were used for our experiments, are shown in Figure 1 Figure 1: Virginia Tech 690 AUVs In work where RSS or SNR is modelled using GP regression, unsuccessful communication events are not explicitly addressed. Any communication event that results in an SNR or RSS value reported by the acoustic modem can be used for GP regression. In contrast, modeling communication success directly using GP classification requires that unsuccessful communication events are explicitly accounted for. The main challenge of accounting for unsuccessful communication events is that the location of a transmitting node is uncertain. That is, if a receiving node anticipates the arrival of a data packet from the transmitting node, but it does not arrive, then the receiving node has an uncertain estimate of the location of the transmitting node, which grows with each consecutive unsuccessful communication event. Hence, in this work, we also evaluate the benefit of accounting for input location uncertainty in the communication events when computing a GP classification. We also evaluate the case where location uncertainty is initially unknown but is learned from the data. Gaussian process classification is used to solve remote sensing image classification for large data streams [11] and perform collision checking in the robot motion planning [12], among many other examples. In these specific studies, the location of the data is often assumed to be known. On the contrary, Gaussian process classification with uncertain inputs is rarely addressed in the literature [13]. There are several methods that address the case of classification with uncertain inputs such as those proposed by [14], [15] and [16]. However, these approaches do not employ Gaussian process classification since we believe one of the primary challenges with Gaussian process classification is the analytical inconvenience on its logit likelihood function. Our proposed sparse classification methods are built upon the previous work on multi-class Gaussian process classification with noisy inputs (NIMGPC) in [13], which accounts for multi-class classification using Gaussian processes along with noisy uncertain transmitter locations, and upon [17] which addresses sparse Gaussian process classification in the variational distribution setting. NIMGPC admits a sparse Gaussian process, which is used for large amounts of data. In our case, a sparse GP is useful because of the very low bandwidth acoustic communication channel available underwater. Sharing a small number of inducing points for the sparse GP may be possible, while sharing data of the full GP would be prohibitively difficult in real-time [18]. The main contributions of this work are • We provide a novel data-driven approach to predict the probability of communication of success suitable for underwater acoustic communication even in the case where the location of the transmitting node is uncertain, as in the case of AUVs. Moreover, our approach explicitly accounts for unsuccessful communication events and the uncertainty in transmitter location that arises when a communication event is unsuccessful. • We show empirically that GP classification outperforms GP regression on communication data acquired using the Virginia Tech 690 AUVs. • We show that accounting for the uncertainty in the location of the transmitter vehicle further improves the results for AUV communication performance over general sparse Gaussian process classification. This work does not seek to specifically address decentralized GP classification for modelling acoustic communication performance. Rather, we seek to authoritatively evaluate GP classification performance for teams of underwater robots. Follow-on work will explicitly address the additional challenges of decentralized classification among a team of underwater robots. The remainder of this paper is as follows. In Section II, we give a brief review of Gaussian process, sparse Gaussian processes and Gaussian process binary classification. In Section III, we address the problem formulation for noisy input sparse Gaussian process binary classification and derive the evidence lower bound of neural network method modified from the NIMGPC framework to address transmitter location uncertainty. In Section IV, we use communication data acquired from experiments with Virginia Tech 690 AUVs to compare prediction performance between Gaussian process regression methods based on SNR measurements and proposed Gaussian process classification methods."
https://arxiv.org/html/2411.07890v1,Minimally Invasive Flexible Needle Manipulation Based on Finite Element Simulation and Cross Entropy Method,"We present a novel approach for minimally invasive flexible needle manipulations by pairing a real-time finite element simulator with the cross-entropy method. Additionally, we demonstrate how a kinematic-driven bang-bang controller can complement the control framework for better tracking performance. We show how electromagnetic (EM) tracking can be readily incorporated into the framework to provide controller feedback. Tissue phantom experiment with EM tracking shows the average targeting error is 0.16\pm 0.29mm.","Percutaneous needle interventions capture a broad class of minimally invasive diagnosis and treatment procedures, such as biopsy [1, 2, 3], brachytherapy [4, 5], and spinal injection [6, 7, 8]. Depending on the clinical procedure, a range of needles with different gauges, stiffness levels, and tip geometries is available. These inherent needle characteristics play a crucial role in determining how the needle moves through soft biological tissues; additionally, surgeons also employ various techniques, such as rotating or bending the needle, to adjust the position of the needle tip in situ during insertion. Knowledge of intricate needle-tissue interactions, as well as reliable trajectory planning and tracking algorithms are crucial to ensuring a minimally invasive insertion with high placement accuracy. A large amount of research conducted on needle insertion falls under steering of bevel-tip needles, where it is assumed that the needle follows its tip exactly and steering is achieved only via axial rotation [9]. Such assumption implies a low relative bending stiffness of the needle compared to its surrounding soft tissues, and inputs that actively change the shape of the needle cannot be taken into account. Needle manipulation solves another class of problem where the needle is modeled as a continuum, and inputs that actively changes the shape of the needle are also considered, such as base manipulation and lateral displacement using robotic devices [10, 11, 12, 13, 14, 15, 16]. Stemming from the field of solid mechanics and finite element methods, these models can capture a wide variety of input types and can benefit particularly to sensory feedback along the needle, such as medical imaging [11], optical tracking [13], and fiber Bragg grating (FBG) optical sensors [15]. This paper demonstrates a flexible needle planning and control strategy for continuum needle models where the insertion dynamics is not readily captured by a kinematic model. More specifically, a finite element (FE) simulation is needed to capture complex nonlinear tissue behaviors and generic control actions along the length of the needle. Since the needle will be discretized into discrete elements, the complete state of the needle, and the simulation environment in general, could involve hundreds of variables, and planning for a minimally invasive insertion and closed-loop control of the flexible needle becomes a challenging problem. Previous works in this domain focus primarily on resolved-rate control, which relies on inverting a numerical input-output Jacobian matrix obtained either via Broyden’s update law or simulating small input disturbances [10, 13, 17, 18, 15, 16]. Yet obtaining such invertible mapping can be challenging, since it requires the system to have equal number of input and output variables, and can generate potentially dangerous needle manipulations that will cause tissue tearing [16]. Others investigate in the case of bevel-tip needles how to combine lateral actuation with axial rotation for the sole purpose of keeping the needle on a straight line [12], and they identified the phenomenon that lateral actuation has diminishing effect as the needle is embedded deeper into soft tissues, yet it remains to be seen how the two actions can be combined to reach arbitrary targets. In this paper, we show how the cross entropy (CE) method can be readily coupled with a FE simulation of needle insertion for both path planning and closed-loop control for percutaneous interventions. In particular, we demonstrate in the case of a bevel-tip needle, how needle manipulation can be used together with steering to achieve a minimally invasive insertion, and how positional feedback can be seamlessly integrated with the current framework. The rest of the paper is as follows. Section II introduces our realtime FE simulator. Section III explains how the CE method can be used effectively to design both a trajectory planner and a model predictive controller, and how kinematic needle models can be incorporated to improve controller performance. Section IV explains our choice of positional feedback to the controller framework and method of sensor characterization. Sections V and VI describe our experiments and summarize our findings."
https://arxiv.org/html/2411.07848v1,"NL-SLAM for OC-VLN: 
Natural Language Grounded SLAM for Object-Centric VLN","Landmark-based navigation (e.g., go to the wooden desk) and relative positional navigation (e.g., move 5 meters forward) are distinct navigation challenges solved very differently in existing robotics navigation methodology. We present a new dataset, OC-VLN, in order to distinctly evaluate grounding object-centric natural language navigation instructions in a method for performing landmark-based navigation. We also propose Natural Language grounded SLAM (NL-SLAM), a method to ground natural language instructions to robot observations and poses. We actively perform NL-SLAM in order to follow object-centric natural language navigation instructions. Our methods leverage pre-trained vision and language foundation models and require no task-specific training. We construct two strong baselines from state-of-the-art methods on related tasks, Object Goal Navigation and Vision Language Navigation, and we show that our approach, NL-SLAM, outperforms these baselines across all our metrics of success on OC-VLN. Finally, we successfully demonstrate the effectiveness of NL-SLAM for performing navigation instruction following in the real world on a Boston Dynamics Spot robot. The OC-VLN dataset, code, and videos are available at sonia-raychaudhuri.github.io/nlslam.","Humans perform visual navigation by using landmarks to determine where they are in the world. Landmarks are distinctive sites in a scene such as objects or topological features (e.g., a particular tree or building). While we can understand relative movement from our current location (e.g., move 5 meters forward), grounding this spatial understanding in landmarks is required to understand our placement in larger regions (e.g., now I am by the front door) [1, 2, 3, 4]. To follow navigation instructions, humans associate landmarks described in natural language with real landmarks observed in the environment to simultaneously understand where they are in the instruction and in the world. Roboticists studying mapping and navigation have developed methods for creating landmark maps from visual observations and using those maps for autonomous navigation [5, 6, 7, 8, 9, 10, 11]. In fact, work in this area has matured far enough to move beyond the research community. Landmark-based mapping and navigation systems are widely used in autonomous robot operation in many industry applications, including home robot vacuum cleaners [12, 13] and drones for surveying and inspection [14]. With recent advances in natural language understanding due to large language and vision-language models [15, 16, 17, 18, 19, 20], the next frontier is to determine how to accurately align natural language navigation instructions with observation-based autonomous decision making capabilities in these mature robotic systems. To support research in this direction, researchers have developed benchmarks for evaluating an autonomous agent’s ability to follow natural language navigation instructions in simulation, specifically R2R [21], RxR [22], and VLN-CE [23]. These benchmarks consist of datasets of natural language instructions and associated paths through scanned Matterport 3D scenes from inside residential homes [24]. These datasets contain two distinct navigation challenges. The language instructions contain both object-centric references to landmarks and relative positional references to the current agent position. For example, ‘go to the blue chair, then turn left and stop at the wooden table’ versus ‘move forward and turn left and stop in the hallway’ respectively. In this work, we investigate the capability of object-centric instruction following alone. There is substantial value in disentangling these two challenges. This value can be understood from both a cognitive science and robotics perspective. Neuroscience researchers have found that distinct regions of the brain enable people to perform landmark-based spatial navigation using observed scene objects [1]. While successful artificial intelligence algorithms are frequently not a direct mimic of human neural function, roboticists also have a long history of managing landmark-based navigation [25] and relative positional navigation [26, 27, 28] with clearly distinct methodological approaches. This distinction in both biological systems and autonomous navigation algorithms motivates us to consider that successful language grounding for these different types of navigation may also be achieved with distinct methodologies. Contributions. In this work, we present a vision-language navigation dataset of exclusively object-centric navigation instruction following commands (OC-VLN). We also propose natural language grounded SLAM (NL-SLAM) for grounding natural language navigation instructions to robot observations and poses. We present a novel navigation policy to actively perform NL-SLAM in order to follow natural language instructions on OC-VLN. Our novel methods, leveraging pre-trained vision and language foundation models, require no task-specific training. Finally, we demonstrate the viability of NL-SLAM to perform real-world navigation instruction following on a Boston Dynamics Spot robot."
https://arxiv.org/html/2411.07833v1,Robust Adaptive Safe Robotic Grasping with Tactile Sensing,"Robotic grasping requires safe force interaction to prevent a grasped object from being damaged or slipping out of the hand. In this vein, this paper proposes an integrated framework for grasping with formal safety guarantees based on Control Barrier Functions. We first design contact force and force closure constraints, which are enforced by a safety filter to accomplish safe grasping with finger force control. For sensory feedback, we develop a technique to estimate contact point, force, and torque from tactile sensors at each finger. We verify the framework with various safety filters in a numerical simulation under a two-finger grasping scenario. We then experimentally validate the framework by grasping multiple objects, including fragile lab glassware, in a real robotic setup, showing that safe grasping can be successfully achieved in the real world. We evaluate the performance of each safety filter in the context of safety violation and conservatism, and find that disturbance observer-based control barrier functions provide superior performance for safety guarantees with minimum conservatism. The demonstration video is available at https://youtu.be/Cuj47mkXRdg.","The human hand is capable of adapting to a wide range of complex objects and performing different tasks in daily life robustly [1], since it allows smooth and safe force interaction [2]. Inspired by this, robotic grasping has been investigated across many human-oriented applications such as industrial assembly [3], packing of groceries [4], and household tasks [5]. Despite the significant progress of robotic grasping over the past decades, the grasping research still revolves around determining proper finger postures for grasping from a kinematic perspective [6][7]. At the same time, contact dynamics are often overlooked in existing grasping methods, which are not sufficient for achieving dexterous grasping [8]. This limitation prevents robotic grasping from being applied to more general tasks that require safe behaviors, as they cannot prevent slippage between the hand and object. To mitigate this limitation, some researchers have focused on force-based grasping techniques. In [9], they propose an adaptive force control framework that allows the hand to be compliant by mapping the human hand posture data into the desired force commands. To prevent undesired slippage, [10] proposes a robust force controller for grasping to maintain secure grasping by introducing force feedback. Similarly, [11] demonstrates a low-level impedance-based controller that incorporated task-based search strategies, comparing its effectiveness on a peg-in-hole task using various robotic hands. Furthermore, a method for regulating the grasping force based on tactile sensors is proposed in [12] to increase grasping stability for unknown objects by leveraging data-driven models like deep neural networks or Gaussian mixture models to detect and estimate contact status and force. Others have demonstrated that online adaptive control strategies can account for disturbances and model error [13] using dynamical systems-based [14] approaches for dexterous manipulation. The modeling of contact dynamics has also played a key role in many robust grasping works. In [15], internal and friction forces of a grasped object are determined in order to provide stable finger gaits so that the proposed impedance controller can handle the dynamic changes in object state during reconfiguration of the fingers. Similarly, [16] proposes a hybrid position/force-controller used to generate a finger gaiting sequence while considering dynamic constraints imposed by friction cones, enabling stable control over object pose. Figure 1: Safe robotic grasping for fragile lab glassware. Such force-based grasping methods have primarily focused only on stable grasping, neglecting safety considerations, which are crucial in real world applications. For instance, safety regulations are essential for laboratory automation [17] and space applications including robotic hands [18]. Therefore, ensuring safety in robotic grasping remains an open challenge beyond merely achieving compliant manipulation. To that end, a few works exist, proposing methods such as safe grasping with impact force measurements [19], safety-optimized strategies with visual depth data and false-positive detection for human safety [20], and an integrated framework for predicting safe grasping forces using transformers [21]. Figure 2: Overview of the proposed safe grasping framework. The framework consists of three main components: safety filters, estimation of contact information, and fingertip force controller. Safety filters include CBF, RaCBF, RCBF, and DOBCBF with a disturbance observer. Tactile sensor data from the hand is mapped to the actual force, and the contact force/torque and point on a fingertip are estimated to be used in the controllers. The fingertip force controller is designed to track safe control input to the hand. Nevertheless, to the best of our knowledge, few studies take into account formal safety guarantees (e.g., force regulation and closure to avoid slippage) for force-based grasping control; thus, imbuing grasping algorithms with formal safety guarantees should be investigated for broader use cases such as medical, space, and chemical laboratory applications. Safety guarantees can be efficiently achieved by using reachability analysis [22], set invariance techniques [23], and control barrier functions (CBFs) [24][25][26]. In this paper, we focus on CBF-based controllers to ensure safety guarantees in robotic grasping. CBF-based controllers enforce the forward invariance of a defined safe set and can be efficiently implemented using quadratic programming, enabling their widespread use in many applications [27][28]. The use of CBFs for safe grasping was first proposed by [29]. Their methodology used robust CBFs in order to handle external disturbances, and consequently, achieved safe grasping while avoiding undesired slippage. I-A Contributions The main novelty presented in this paper is the introduction of an integrated robotic safe grasping framework with tactile sensing as shown in Fig. 2. The framework is specifically designed to maintain maximum and minimum safe grasping forces and achieve safe force closure by employing robust and adaptive approaches in the presence of model uncertainty. The contributions of this paper are outlined as follows: • We present a safe grasping framework with formal safety guarantees, based on CBFs schemes as safety filters. The scalability of the framework presented in this paper is demonstrated by applying several safety filters, including CBF [24], robust adaptive CBF [25] (RaCBF), robust CBF (RCBF) [26], and disturbance observer CBF (DOBCBF) [30]. • To this end, we implement force control with an inner position loop for a dexterous robot hand and develop sensor processing techniques to estimate contact force and point from electromagnetic tactile sensors on fingertips. This approach enables us to use contact dynamics model effectively in CBFs-based controller. • The effectiveness of our framework is empirically validated with multiple objects in a scenario where the grasping force must be regulated within safety limits to prevent damage to objects and slippage as well. Additionally, we provide the detailed comparisons between each safety filter in the context of safety violation and conservatism. We use Shadow Robot platform for validations as shown in Fig. 1. • To the best of our knowledge, this work is the first attempt not only to introduce an integrated framework with tactile sensing for grasping with formal safety guarantees, but also to demonstrate it in the real-world. We show significant improvements in our framework compared to [29] with respect to adaptation, conservatism, and grasping force regulation. Our method incorporates adaptation to parametric uncertainty and while estimating external disturbances, making it less conservative than [29]. Additionally, since [29] maintains a strong grasping force to prevent slipping without regulating the force, it may lead to applying excessive force and potentially damaging the grasped object. In this work, constraints including contact force and force closure are simultaneously considered. This paper is organized as follows. Section II defines a problem solved in the paper and Section III provides safety filters and a disturbance observer as preliminaries. Section IV presents the proposed framework. Subsequently, we verify our framework in numerical simulations in Section V and validate it in the real multiple experiments in Section VI. Lastly, we conclude our paper with future works in Section VII."
https://arxiv.org/html/2411.07815v1,Reliable-loc: Robust sequential LiDAR global localization in large-scale street scenes based on verifiable cues,"Wearable laser scanning (WLS) system has the advantages of flexibility and portability. It can be used for determining the user’s path within a prior map, which is a huge demand for applications in pedestrian navigation, collaborative mapping, augmented reality, and emergency rescue. However, existing LiDAR-based global localization methods suffer from insufficient robustness, especially in complex large-scale outdoor scenes with insufficient features and incomplete coverage of the prior map. To address such challenges, we propose LiDAR-based reliable global localization (Reliable-loc) exploiting the verifiable cues in the sequential LiDAR data. First, we propose a Monte Carlo Localization (MCL) based on spatially verifiable cues, utilizing the rich information embedded in local features to adjust the particles’ weights hence avoiding the particles converging to erroneous regions. Second, we propose a localization status monitoring mechanism guided by the sequential pose uncertainties and adaptively switching the localization mode using the temporal verifiable cues to avoid the crash of the localization system. To validate the proposed Reliable-loc, comprehensive experiments have been conducted on a large-scale heterogeneous point cloud dataset consisting of high-precision vehicle-mounted mobile laser scanning (MLS) point clouds and helmet-mounted WLS point clouds, which cover various street scenes with a length of over 20km. The experimental results indicate that Reliable-loc exhibits high robustness, accuracy, and efficiency in large-scale, complex street scenes, with a position accuracy of 1.66m, yaw accuracy of 3.09 degrees, and achieves real-time performance. For the code and detailed experimental results, please refer to https://github.com/zouxianghong/Reliable-loc.","Wearable laser scanning (WLS) systems have the advantages of flexibility and portability by integrating LiDAR (Light Detection And Ranging), IMU (inertial measurement unit), and other sensors in a portable device (Li et al., 2023, 2024). It can be used for finding the user’s path, i.e. global localization, which is a huge demand for pedestrian navigation (Baglietto et al., 2011), collaborative localization and mapping (Yuan et al., 2017; Kachurka et al., 2021), augmented reality (Chi et al., 2022), counter-terrorism and emergency rescue (bin Shamsudin et al., 2017). As LiDAR is not sensitive to changes in lighting, works in different weather conditions, and has high measurement accuracy (Goran, 2010; Sharif, 2021), LiDAR-based global localization is practical and can work in GNSS-denied complex environments such as urban canyons, indoors, and underground (Wang et al., 2021). LiDAR-based global localization has been long studied. According to whether sequential LiDAR data is used, LiDAR-based global localization can be divided into two categories: single-shot and sequential global localization. The former usually achieves localization based on place recognition using a single LiDAR frame and is not suitable for large-scale repetitive scenes (Du et al., 2020; Komorowski, 2021; Cattaneo et al., 2022; Zou et al., 2023). The latter usually realizes localization by utilizing place recognition with techniques like MCL and sequential matching, which is more applicable to industry applications in large-scale outdoor scenes (Liu et al., 2019a; Yin et al., 2019b; Chen et al., 2020; Ma et al., 2022). Hence, this paper focuses on the sequential global localization for the wearable device. In real applications, the WLS-based localization system needs to be operated in various scenes, posing many challenges to the reliability of the global localization algorithm. First, the localization system faces challenges from feature-insufficient environments. Take the typical urban street scene in southern China as an example: the road is wide and full of viaducts, and many road sections are only flanked by street trees and flat facades, lacking sufficient features (Li et al., 2023; Lin et al., 2023). Second, the prior map may not fully cover the area, and blank data holes exist. For example, the prior point cloud map is usually collected along roads by the vehicle-mounted MLS systems and can not fully cover roadside areas due to occlusion and accessibility (Serna and Marcotegui, 2013; Mi et al., 2021). In recent years, some LiDAR-based sequential global localization methods have integrated place recognition techniques into MCL and are applicable to large-scale outdoor scenes (Yin et al., 2023a). However, existing methods still have limitations facing the above challenges. On the one hand, most of them solely rely on the global feature (the aggregation of local features) extracted from local maps to construct the observation model in MCL. In feature-insufficient scenes where global features lack descriptive adequacy, particles in MCL are prone to converging towards erroneous regions, ultimately resulting in the localization system crashing. While many place recognition methods extract local features simultaneously to aggregate the global feature, the rich information embedded in local features is ignored by the downstream MCL task. On the other hand, existing methods only rely on point cloud registration for localization once MCL converges, thus leading to localization failure due to continuous unreliable pose estimation in the scenes with insufficient features and incomplete map coverage. To tackle the challenges of feature insufficiency and incomplete coverage of prior maps, we propose Reliable-loc, a reliable sequential global localization method based on verifiable cues from both spatial and temporal aspects. Reliable-loc enhances MCL by exploiting the rich information embedded in local features from place recognition (the spatial cues) and improving localization robustness by monitoring the sequential localization status (the temporal cues). The main contributions of this paper are as follows: 1. A novel MCL incorporating spatially verifiable cues is proposed to adjust particle weights using the rich information embedded in local features. It improves localization robustness in feature-insufficient scenes by avoiding particles converging to erroneous regions. 2. A localization status monitoring mechanism guided by the temporal sequential pose uncertainties is proposed thus the localization mode can be adaptively switched according to the temporal verifiable cues. It improves localization robustness in scenes with insufficient features and incomplete map coverage by exploiting the exploratory capability of particles in MCL. The remainder of the paper is organized as follows. Section 2 reviews the related works on LiDAR-based sequential global localization. Section 3 presents the important preliminary. Section 4 elaborates on the proposed approach. Section 5 presents the datasets and quantitative evaluation. Section 6 presents the ablation studies and discusses the deficiencies and future work. The conclusion is outlined in Section 7."
https://arxiv.org/html/2411.07719v1,": A Flexible Generative Perception Error Model 
for Probing Self-Driving Planners","To handle the complexities of real-world traffic, learning planners for self-driving from data is a promising direction. While recent approaches have shown great progress, they typically assume a setting in which the ground-truth world state is available as input. However, when deployed, planning needs to be robust to the long-tail of errors incurred by a noisy perception system, which is often neglected in evaluation. To address this, previous work has proposed drawing adversarial samples from a perception error model (PEM) mimicking the noise characteristics of a target object detector. However, these methods use simple PEMs that fail to accurately capture all failure modes of detection. In this paper, we present Emperror, a novel transformer-based generative PEM, apply it to stress-test an imitation learning (IL)-based planner and show that it imitates modern detectors more faithfully than previous work. Furthermore, it is able to produce realistic noisy inputs that increase the planner’s collision rate by up to 85\text{\,}\mathrm{\char 37\relax}, demonstrating its utility as a valuable tool for a more complete evaluation of self-driving planners.","After years of progress, autonomous driving systems are able to handle increasingly complex situations [1]. This is enabled, in part, by solving several aspects of driving with learned modules, such as perception [2, 3, 4] and motion forecasting [5, 6]. Recently, there has been increased interest in managing the complexity of human behavior in traffic by also learning the planning task [7, 8, 9], which has been accelerated through the emergence of motion forecasting- and planning-centric benchmarks and datasets [10, 11]. Most of this work assumes a simplified setting where the ground-truth world state is available as input and focuses on accuracy of the planned trajectory with respect to human driving. As a result, robustness to the residual risk of errors in the perception system, which is ultimately just an imperfect model operating on incomplete observations of the world, remains underexplored. Figure 1: Method Overview. We propose Emperror, a generative model that imitates a given detector by modeling the distribution of its perception errors conditioned on a ground-truth state and BEV map as scene context. Adversarial optimization in the model’s latent space can then produce challenging yet plausible proxy detections from that distribution which stress-test the robustness of a given planner, e.g. by inducing collisions. In this work, we aim to illuminate the susceptibility of learned planning, which is often brittle in the face of o.o.d inputs [12], to these errors. Recent seminal studies [13, 14] have approached this problem from the lens of adversarial attacks. To this end, these methods first construct a perception error model (PEM) [15, 16], which allows sampling multiple noisy estimates imitating a target 3D object detector given a ground-truth scene representation as context. Then, by leveraging the PEM as a proxy of the detector, challenging samples that stress-test the target planner can be found by employing an adversarial search strategy. While promising, these works consider simple, synthetic scenes that do not capture the complexities of real-world data. Moreover, they employ simple PEMs that phrase the error modeling task as an isolated, per-object perturbation of the ground-truth state, rather than jointly reasoning over the entire scene context. Hence, they cannot faithfully model the intricacies of the error patterns exhibited by modern 3D object detectors, such as duplicate detections resulting in false-positives and correlations in errors for groups of objects. Motivated by this, we propose Emperror, a novel generative empirical error model based on the transformer architecture, that can more faithfully capture the error characteristics of a target detector. Our key idea is to leverage the attention mechanism and a flexible set of latent queries to model the full range of failure modes, including false-positives, in a scene-consistent manner. Furthermore, Emperror provides a prior over different error patterns for a given scene context, enabling us to draw adversarial, yet plausible samples to probe the robustness of a target planner. Building on these advantages, we design a framework to probe the robustness of learned planners to noisy perception inputs, which is visualized in Fig. 1. We then apply the proposed framework to an imitation learning (IL)-based planner, modeling three different modern camera-based 3D object detectors, and show learned planning is indeed vulnerable to plausible noise from the long-tail of perception errors. We believe Emperror can serve as a valuable tool for data-driven evaluation of self-driving planners. Contributions: (1) We propose Emperror, a novel transformer-based, generative PEM for probing planning, that can more faithfully imitate modern object detectors than previous work. (2) We integrate Emperror into a framework for probing the robustness of a planner to noise in its perception system. (3) We show that our proposed PEM can imitate modern 3D object detectors and enables the generation of challenging yet plausible errors that can be used to evaluate planning in a data-driven manner."
https://arxiv.org/html/2411.07699v1,": Accurate, Robust adar-nertial dometry 
with on-Iterative Estimation*","Precise localization and mapping are critical for achieving autonomous navigation in self-driving vehicles. However, ego-motion estimation still faces significant challenges, particularly when GNSS failures occur or under extreme weather conditions (e.g., fog, rain, and snow). In recent years, scanning radar has emerged as an effective solution due to its strong penetration capabilities. Nevertheless, scanning radar data inherently contains high levels of noise, necessitating hundreds to thousands of iterations of optimization to estimate a reliable transformation from the noisy data. Such iterative solving is time-consuming, unstable, and prone to failure. To address these challenges, we propose an accurate and robust Radar-Inertial Odometry system, RINO, which employs a non-iterative solving approach. Our method decouples rotation and translation estimation and applies an adaptive voting scheme for 2D rotation estimation, enhancing efficiency while ensuring consistent solving time. Additionally, the approach implements a loosely coupled system between the scanning radar and an inertial measurement unit (IMU), leveraging Error-State Kalman Filtering (ESKF). Notably, we successfully estimated the uncertainty of the pose estimation from the scanning radar, incorporating this into the filter’s Maximum A Posteriori estimation, a consideration that has been previously overlooked. Validation on publicly available datasets demonstrates that RINO outperforms state-of-the-art methods and baselines in both accuracy and robustness. Our code is available at https://github.com/yangsc4063/rino.","I INTRODUCTION Autonomous vehicles rely on precise localization to ensure safe and efficient navigation. The Global Navigation Satellite System (GNSS) is commonly employed and highly effective; however, its performance can degrade in urban environments due to obstructions such as trees and tall buildings. Consequently, there is increasing interest in leveraging onboard sensors for ego-motion estimation, either to augment or fully substitute GNSS-based systems, particularly during GNSS outages. Over the past two decades, numerous odometry and SLAM methods have been developed based on camera [1, 2, 3, 4, 5, 6] and LiDAR [7, 8, 9, 10, 11] all of which have demonstrated excellent performance. However, these methods are often limited by adverse weather conditions, such as fog, rain, or snow. In contrast, millimeter-wave radar, due to its longer operating wavelengths, can perform robustly under such conditions, emerging as a promising alternative. Unfortunately, scanning radar data is inherently noisy and prone to significant distortion, which presents challenges for the design of effective odometry or SLAM methods. Given these challenges, an increasing number of researchers are exploring solutions in radar-based odometry [12]. Existing methods for radar odometry based on scanning radar can be broadly categorized into sparse and dense techniques. Dense methods directly process the tensor-format data acquired by scanning radar, whereas sparse methods extract key points from this data before performing registration. In terms of computational complexity and accuracy, sparse methods tend to offer advantages. Most sparse methods are derived from local point cloud registration using the Iterative Closest Point (ICP) algorithm [13], an iterative approach that alternates between nearest point matching and registration residuals to align two scans of point clouds. Although iterative methods can effectively mitigate the influence of outliers over successive iterations and approximate the optimal estimation, they also have inherent drawbacks: high computational cost, instability, and susceptibility to local minima, which may lead to failure. Figure 1: Trajectories obtained by the proposed radar-inertial odometry (RIO) system using the KAIST02 sequence of the Mulran dataset. The raw radar data and IMU data of the KAIST02 sequence are given in the format of ROS messages using the File player provided by the Mulran dataset and can run robustly in real time after being processed by RINO. The trajectory of the second lap almost overlaps with that of the first lap, demonstrating the accuracy of RINO. To address the aforementioned challenges, we propose a radar-inertial odometry (RIO) system, RINO, that eliminates the need for iterative solving of rotation and translation between two scans. Building upon the ORORA framework [14] as a baseline, RINO adopts its workflow for the scanning radar odometry (RO) branch while retaining its method for modeling the anisotropic uncertainty of keypoints extracted from raw radar data. In contrast to ORORA, the scanning radar branch of RINO improves keypoint extraction, implements motion distortion compensation, and avoids iterative solving by employing an adaptive voting technique for rotation and translation estimation. Moreover, we were unexpectedly able to leverage the adaptive voting method to quantify the uncertainty of pose estimation. Consequently, we introduce an inertial measurement unit (IMU), resulting in a loosely coupled odometry system based on error-state Kalman filtering (ESKF), which successfully integrates the pose estimation uncertainty from the radar branch into the filter’s Maximum A Posteriori estimation. For the first time, we incorporate the current state estimation quality of each sensor into the loosely coupled system, rather than modularly fusing their outputs. Given that the IMU is also adaptable to adverse weather conditions, we believe the entire system is capable of handling such challenges as well. The main contributions of this paper are as follows: • A non-iterative method for rotation and translation estimation suitable for high-noise and significantly distorted scanning radar data. This approach not only reduces computation time but also enhances the robustness of the estimation process. • A loosely coupled ESKF system, where RINO enables the online computation of pose estimation uncertainty from the scanning radar, which is adaptively fused with the IMU. This enhances the environmental adaptability of the system. • Extensive experiments on datasets demonstrate that RINO exhibits strong performance and adaptability across various scenarios and adverse weather conditions. Real-vehicle testing further validates the practicality of RINO."
https://arxiv.org/html/2411.07644v1,Human Arm Pose Estimation with a Shoulder-worn Force-Myography Device for Human-Robot Interaction,"Accurate human pose estimation is essential for effective Human-Robot Interaction (HRI). By observing a user’s arm movements, robots can respond appropriately, whether it’s providing assistance or avoiding collisions. While visual perception offers potential for human pose estimation, it can be hindered by factors like poor lighting or occlusions. Additionally, wearable inertial sensors, though useful, require frequent calibration as they do not provide absolute position information. Force-myography (FMG) is an alternative approach where muscle perturbations are externally measured. It has been used to observe finger movements, but its application to full arm state estimation is unexplored. In this letter, we investigate the use of a wearable FMG device that can observe the state of the human arm for real-time applications of HRI. We propose a Transformer-based model to map FMG measurements from the shoulder of the user to the physical pose of the arm. The model is also shown to be transferable to other users with limited decline in accuracy. Through real-world experiments with a robotic arm, we demonstrate collision avoidance without relying on visual perception.","Human-Robot Interaction (HRI) within a shared workspace involves either coexistence or collaborative interaction. In coexistence scenarios, humans and robots work independently on distinct tasks while avoiding each other [1]. In collaborative interaction, they interact dynamically to accomplish a shared goal [2]. Both scenarios require the robot to accurately understand and anticipate the human’s intentions and arm movements in real-time. Applications can include medical procedures [3], rehabilitation [4], factory assistance [5] and domestic robotics [6]. In these scenarios, the robot’s ability to observe human motion and respond is crucial for effective interaction. We consider the problem of estimating the current pose of the entire human arm. Estimation models are usually based on either visual perception or wearable devices. The most notable approach for the former is the use of human pose estimation models (i.e., Skeleton models [7, 8]). Such a model locates key points (e.g., head, shoulders, elbows, wrists, hips and knees) on a human body within an image or video, that can be used to reconstruct a person’s pose in 2D or 3D space. Hence, a camera can observe the user, estimate arm poses in real-time, and proactively initiate actions to avoid collisions or plan task completion [9]. However, the sole reliance on continuous visual feedback can hinder task performance in scenarios with visual uncertainty, such as poor lighting, occlusions, long distance and multiple users in the scene. Additionally, visual sensing demands substantial data and computational resources [10], potentially limiting its practicality in certain applications. Figure 1: A user is working in a share workspace with a robotic arm. A wearable Force-Myography (FMG) device is used to estimate the pose of the human arm in real time. In this example, when the user reaches to pick up a tool, the robot halts its motion to avoid interference and potential collisions. Figure 2: The wearable FMG device includes a back harness and an upper arm band, with a total of 32 FSR sensors. Reflective markers are fixed on the shoulder, elbow and wrist for data collection. To cope with the vision limitations, wearable technology has been proposed with sensing modalities such as Electro-Myography (EMG) [11] and Ultrasound [12]. In general, estimating arm poses solely through wearable sensing can enhance the robustness of HRI systems, particularly in challenging environments with limited or occluded visual information. Later, potential fusing of wearable sensor data with visual inputs can achieve more accurate and reliable pose estimation. The prominent approach is the use of body-worn Inertial Measurement Units (IMU) [13]. Typically, one or two IMU sensors are placed on the user’s arm and used to estimate arm pose [14, 15, 16]. IMUs typically combine accelerometers, gyroscopes and magnetometers, and can be used to estimate kinematic data such as velocity, position and orientation. These estimations are often jeopardized by noise and drift. Kalman filtering and machine learning techniques are commonly employed to address these challenges and improve the accuracy of the estimated motion [17]. However, IMU-based estimations require frequent calibration as they provide relative, rather than absolute, measurements [18]. With similar calibration requirements, IMUs were also used to predict the future target of the users arm for seamless HRI [19, 20]. Force-Myography (FMG) is a non-invasive technique for measuring muscle activity [21]. FMG involves placing simple force sensors on the skin to monitor muscular contractions and relaxations. Compared to EMG, FMG is known for its ease of acquisition, high accuracy and robustness to positioning variations [22, 23]. FMG offers a better signal-to-noise ratio and anti-interference ability, providing more stable signals with resistance to interference such as sweating and electromagnetic disturbances [24]. In addition, FMG technology utilizes simple force-sensitive resistors, resulting in a low-cost device. These advantages have led to its successful application in rehabilitation studies [25], tele-operation [26], prosthetics [27, 28], and classification of hand gestures [29] and held objects [30, 31]. In all of these applications, FMG sensing was conducted on the forearm of the user and, thus, usually provides only information regarding the state of the hand and fingers. To the best of the authors’ knowledge, FMG sensing has not been previously applied to model the complete state of the human arm, and specifically by measuring perturbations of shoulder muscles. In this letter, we explore the feasibility of utilizing FMG for comprehensive modeling of the human arm, with the goal of natural HRI. In a novel approach, FMG sensors are strategically placed on the upper back, shoulder and arm, enabling the acquisition of data that can be used to infer arm pose. This is the first application of FMG on shoulder muscles and for modeling the state of the entire human arm. To estimate the instantaneous pose, we propose a novel approach utilizing the Transformer architecture [32], leveraging its ability to process temporal sequential data effectively. We then demonstrate its use in an HRI scenario where a robot must perform its own task without interfering or colliding with the human user. Unlike the relative positioning of IMUs, FMG offers absolute positioning with respect to the human torso without the need for constant calibration. Hence, it can be integrated in the future to the clothing of users, enabling seamless sensing and data collection in various applications. This work pioneers the use of FMG for full-arm pose estimation, enabling robust human-robot interaction. Key contributions include: • Novel FMG Application: We introduce a novel approach to FMG, positioning sensors on the shoulder to capture arm pose information. • Transformer-based Model: We propose a Transformer-based model to effectively map FMG signals to accurate arm pose estimates. • Cross-User Generalization: We demonstrate the model’s ability to generalize to new users, even with varying body dimensions. • Real-world Validation: The model’s effectiveness is validated in real-world HRI scenarios with a collaborative robot arm. While our focus is on robotic arm collaboration, the proposed approach can also be adapted for applications involving prosthetic hands, drones, teleoperation, and virtual reality."
https://arxiv.org/html/2411.07612v1,A Simple and Effective Multi-agent Joint Prediction Method for Autonomous Driving,"Predicting future motions of road participants is an important task for driving autonomously. Most existing models excel at predicting the marginal trajectory of a single agent, but predicting joint trajectories for multiple agents that are consistent within a scene remains a challenge. Previous research has often focused on marginal predictions, but the importance of joint predictions has become increasingly apparent. Joint prediction aims to generate trajectories that are consistent across the entire scene. Our research builds upon the SIMPL baseline to explore methods for generating scene-consistent trajectories. We tested our algorithm on the Argoverse 2 dataset, and experimental results demonstrate that our approach can generate scene-consistent trajectories. Compared to the SIMPL baseline, our method significantly reduces the collision rate of joint trajectories within the scene.","Accurate prediction of the motion of surrounding traffic participants is crucial for autonomous vehicles. Providing precise and timely predictions of intentions and trajectories is essential for downstream decision-making and planning modules, as it significantly enhances safety and the rationality of planned trajectories. Recent advances in deep learning have demonstrated great success in predicting accurate trajectories by learning from real-world driving examples[1]. However, many existing trajectory prediction models focus on generating marginal prediction samples for individual agents’ future trajectories. How to effectively combine the different modal trajectories predicted for each agent remains a challenging problem. The reason is that the number of possible trajectory combinations grows exponentially as the number of traffic participants in the scene increases. This exponential growth is unacceptable for autonomous vehicles, which require quick responses. Meanwhile, marginal prediction overlooks the interactions between the predicted future trajectories of different agents, leading to potential collisions or overlaps in the predicted trajectories, which fails to adequately simulate the results of trajectory prediction in real-world scenarios. Figure 1: Visualization of Predicted Trajectories for Agents in Scenarios. Our method can generate joint predictions for all agents in the scene simultaneously. The ego vehicle is shown in red, while other vehicles are displayed in gray. The predicted trajectories are visualized using gradient colors. In many trajectory prediction models, the main goal is primarily on marginal prediction for a single target agent. This prediction typically accounts for both the agent’s intentions and possible trajectories, offering multiple trajectory modes along with their associated probabilities[1]-[3]. This methodology has driven significant progress, resulting in the development of numerous high-performing models that have achieved notable success in both benchmark datasets and real-world applications. However, this approach presents challenges when applied to joint prediction scenarios involving multiple agents. Specifically, when predicting the trajectories for two or more agents, selecting the highest probability paths for each agent independently often fails to capture the realistic dynamics of their interactions. The resulting trajectories may overlap or even collide, which indicates a failure to model the true interactions between agents in a shared environment. A straightforward approach to generating joint future predictions involves considering the exponential number of combinations derived from the marginal predictions of individual agents. However, many of these combinations are inconsistent, particularly when agents have overlapping trajectories. Moreover, this method leads to an exponential increase in prediction complexity as the number of agents in a scenario grows[4]. A more natural approach is to allow the network to implicitly learn contextual features of trajectory prediction and to employ a scene-consistent method to generate joint trajectory predictions that are coherent across different scenarios[5]. To address the issues of limited joint prediction methods and relative computational complexity mentioned above, our motivation is to propose a simple, scene-consistent joint prediction approach. In our research, we diverged from approaches that rely on recombining Factored Marginal Trajectories like M2I[4] or Conditional Prediction like SceneTransformer[6]. Instead, we employed an implicit scene learning method, designing a scene-level trajectory loss function. We utilized a Winner-Takes-All(WTA) backpropagation strategy to generate a set of scene predictions that are closest to the true scene trajectories. This approach enables the network to implicitly learn the distribution of scene-level trajectories. Our method is capable of simultaneously generating coherent joint predictions for all agents concerning future trajectories. This represents a crucial step toward the joint optimization of Prediction and Planning in future research. The visualization results of our method are presented in Fig. 1."
https://arxiv.org/html/2411.07590v2,Multiple Non-cooperative Targets Encirclement by Relative Distance based Positioning and Neural Anti-Synchronization Control,"From prehistoric encirclement for hunting to GPS orbiting the earth for positioning, target encirclement has numerous real world applications. However, encircling multiple non-cooperative targets in GPS-denied environments remains challenging. In this work, multiple targets encirclement by using a minimum of two tasking agents, is considered where the relative distance measurements between the agents and the targets can be obtained by using onboard sensors. Based on the measurements, the center of all the targets is estimated directly by a fuzzy wavelet neural network (FWNN) and the least squares fit method. Then, a new distributed anti-synchronization controller (DASC) is designed so that the two tasking agents are able to encircle all targets while staying opposite to each other. In particular, the radius of the desired encirclement trajectory can be dynamically determined to avoid potential collisions between the two agents and all targets. Based on the Lyapunov stability analysis method, the convergence proofs of the neural network prediction error, the target-center position estimation error, and the controller error are addressed respectively. Finally, both numerical simulations and UAV flight experiments are conducted to demonstrate the validity of the encirclement algorithms. The flight tests recorded video and other simulation results can be found in https://youtu.be/B8uTorBNrl4.","In recent years, target tracking and monitoring algorithms have been widely used in maritime, aerospace, environmental surveys, convoy escorts, and security patrol scenarios [1, 2, 3]. The encirclement control, as a type of tracking and monitoring strategy, requires the mobile tasking agents to be able to circumnavigate the target while tracking [4, 5, 6]. Recently, many methods have been proposed along on this research direction [7, 8, 9, 10, 11]. For example, in [8], an estimator-based enclosing controller was proposed to drive an autonomous surface vehicle to surround and monitor a single target affected by random ocean currents. In addition to a single agent encircling a single target, multiple agents have also been used to encircle a single target by designing a corresponding consensus algorithm. In [9], a dynamic control law was proposed that can enable multiple vehicles to move around a target along an ideal trajectory while the target is moving at a time-varying speed. In [11], a relative position-based controller was proposed to enable a group of moving agents to surround a single moving target. Compared with single target encirclement, the multiple target encirclement problem is much more complex. In recent works, there are two main ways to realize the multiple target encirclement. One is to let the tasking agents encircle each target separately, that is, to transform the problem of multiple targets encirclement into a single target encirclement [4]. The other is to make all tasking agents surround all the targets with a desired trajectory [12]. The desired trajectory of the encirclement task needs to be further designed carefully such that the agents do not collide with any of the targets[13, 14]. In [13], the radius of the trajectory was adjusted directly according to the measured distances between the agents and targets. In [14], the position of each target was obtained separately, and then the center and radius of trajectory needed by the agent to circumnavigate all targets are designed according to the obtained position. Most of the mentioned target encirclement algorithms are based on the assumption that the targets are cooperative. For cooperative targets, the agents can directly obtain the feedback information from the target, such as position, displacement, etc [15, 16]. However, in real cases, the targets are non-cooperative, that is, the real-time states and motion models of the targets are all unknown. In some existing results, it is assumed that the non-cooperative target is stationary, or moving at a low or constant speed, thus ensuring that the model of the target can be obtained [17, 18, 19]. On the basis of this assumption, the distance and angle based localization algorithms have been proposed to solve the positioning problem of non-cooperative targets [20, 21]. For example, in [21], a mobile robot was assumed to have a known linear velocity, and a backstepping-based controller was designed to control the direction of the robot so as to achieve the encirclement of the unknown stationary target. However, for applications in complex environments, the non-cooperative target may be moving freely. In this case, the positioning of the targets still remains challenging. In addition, for the encirclement of multiple non-cooperative targets, positioning each target separately may lead to longer execution time and inaccuracy of the whole encirclement algorithm due to the accumulation of errors. Generally, in the encirclement task, a single tasking agent can only observe parts of the states of the targets and cannot achieve the necessary coverage to the targets. Hence, for the monitoring task, multiple tasking agents are required to encircle and monitor the targets. However, other problems may occur such as the complexity of control algorithm, collision avoidance issues, and the implementation cost. To solve above issues, in this paper, we are dedicated to making a trade off, that is to adopt a minimum of two tasking agents to encircle freely moving multiple targets while achieving maximum sensor coverage to the targets. Figure 1: Multiple unknown and non-cooperative targets encircling by using only two tasking agents. The research highlight of our work is shown in Fig. 1, where multiple targets are successfully encircled by using only two mobile tasking agents. Moreover, we also assume that the GPS signal is not available in the tasking environments. All targets we consider are completely unknown and non-cooperative, that is, the two agents cannot directly obtain the states of all targets. Given the challenges in practical applications, such as energy constraints, it is assumed that each agent can only carry a distance measurement sensor, such as vision sensor, attenna array, etc. Based on the relative distance information measurements among the tasking agents and the targets, an estimator is designed such that the target-center position can be estimated in real-time. Furthermore, based on the estimated position information, a distributed anti-synchronization controller (DASC) is designed so that the two tasking agents can symmetrically circumnavigate all the targets by dynamically adjusting the radius of the movement trajectories. The main contributions of this work are highlighted as follows. 1. Different from the existing work related to targets encirclement [6, 22], the main focus in this work is to study the non-cooperative targets encirclement problem, that is, all state information about the targets cannot be obtained directly and the model of target is unknown. By measuring only the relative distance using the onboard ranging sensors, the target-center position can be estimated accurately based on a fuzzy wavelet neural network (FWNN) and the least squares fit method. 2. In contrast to the existing works [14, 23], we estimate the center position of the targets directly without knowing the real-time position of all targets, which avoids the accumulation of errors caused by estimating the real-time position of each target. In addition, the desired trajectory of the two tasking agents encircling all targets can be quickly obtained by direct estimation of the center position, which implies that the direct estimation of the center position can improve the speed of the whole encirclement algorithm. 3. Different from the existing encirclement controllers, based on anti-synchronization (AS) theory and the position estimator [24, 25], our designed the encirclement controller can not only be beneficial for the controller and estimator to act simultaneously to ensure the global stability of the system, but also enables the two agents to circumnavigate all targets symmetrically to ensure maximum sensor coverage of all targets. Notations: \boldsymbol{x}\in\mathrm{R}^{n\times m} is the {n\times m} real matrix. I_{n\times n} and 1_{n} used to denote the n dimensional identity matrix and identity vector, respectively. P^{T} is the transpose of the matrix P. The eigenvalue of a square matrix is represented as the symbol \lambda. \|\cdot\| is the L2 norm, and |\cdot| represents the absolute value. \mathcal{R}\big{\{}\cdot\big{\}} represents the rounding function. <\Upsilon(r(k),k),\boldsymbol{p}_{iC}(k)> represents the included angle between the vectors \Upsilon(r(k),k) and \boldsymbol{p}_{iC}(k)."
https://arxiv.org/html/2411.07573v1,Robotic Control Optimization Through Kernel Selection in Safe Bayesian Optimization,"Control system optimization has long been a fundamental challenge in robotics. While recent advancements have led to the development of control algorithms that leverage learning-based approaches, such as SafeOpt, to optimize single feedback controllers, scaling these methods to high-dimensional complex systems with multiple controllers remains an open problem. In this paper, we propose a novel learning-based control optimization method, which enhances the additive Gaussian process-based Safe Bayesian Optimization algorithm to efficiently tackle high-dimensional problems through kernel selection. We use PID controller optimization in drones as a representative example and test the method on Safe Control Gym, a benchmark designed for evaluating safe control techniques. We show that the proposed method provides a more efficient and optimal solution for high-dimensional control optimization problems, demonstrating significant improvements over existing techniques.","To address the widely adopted yet time-consuming and often suboptimal process of manual controller tuning, the demand for automatic controller optimization has grown across various fields, including robotics [1, 2], automotive systems [3], and industrial automation [4]. Most previous methods are based on a simplified system model and determine the optimal controller parameters through the system model [5, 6]. Such methods are often prone to suboptimal results due to a less accurate system model and the influence of the noise [7]. With the development of data-driven methods, optimizing controller parameters based on real robotic system motion data has shown notable improvements in control performance, such as methods using Iterative Feedback Tuning (IFT) [8], Variable gain control [9], and Bayesian optimization [10, 11]. Bayesian optimization is often used to optimize black-box objective functions that are expensive to evaluate. It employs a surrogate model, typically a Gaussian Process (GP), to iteratively predict the function’s distribution and guide the search for the optimum using an acquisition function. Since the noise measurement can be modeled as a GP together with the objective function [12], Bayesian optimization is particularly effective for tasks like optimizing PID controller parameters in control systems. However, Bayesian optimization often evaluates unsafe parameters in practical applications [7] since the iterative process does not consider the physical meaning of the evaluated function. For example, unsafe parameter combinations in quadrotor control could lead to collisions with walls or ceilings. Besides, Bayesian optimization uses Gaussian kernels for GP calculations, which is prone to the curse of dimensionality [13]. When the dimension of the problem becomes larger than 3, the number of evaluations required by Bayesian optimization will increase significantly. Hence, controller parameter tuning of high-order complex systems remains a significant challenge, especially when safety and robustness should be guaranteed [14, 15]. Figure 1: Overview of the method. First, the input and output of the system are measured to obtain a set of observations, which serve as the GP priors for calculating kernel selection. Subsequently, Bayesian optimization via additive Gaussian processes uses the most important one or several additive kernels calculated by kernel selection for optimization, iteratively selecting safe new parameters and testing the performance in the system until the optimal control parameters are obtained. Following this line of research, Sui et al. [16] proposed SafeOpt. This first safety-aware Bayesian optimization algorithm ensures the safety of the optimization process by avoiding evaluating parameters whose performance values fall below the pre-defined safe threshold. However, it is less feasible in high dimensions. Kirschner et al. [17] then proposed the LINEBO algorithm to address the challenges of optimizing high-dimensional problems by breaking them into manageable one-dimensional sub-problems. Still, it is hard to apply to control engineering because it usually takes hundreds of evaluations, which may cause severe wear to the system. Wang et al. [13] proposed the high-dimensional safe Bayesian optimization algorithm via additive Gaussian processes, which is more efficient for control optimization. They employed an additive structure to the traditional Gaussian kernels to enhance the information acquisition efficiency, but it brought too much complexity to the calculation. When the problem dimension gets higher than 6, the experimental validation with actual hardware will be hard to implement. Dimensionality reduction based on feature selection is a standard method to reduce the calculation for complex algorithms [18, 19, 20]. In this framework, the kernel-target alignment method [21] measures the similarity between kernels and the objective function, providing a way to select essential kernels based on the degree of agreement with the learning task. While this approach offers theoretical insights and can improve model accuracy, it is computationally intensive and inefficient in high dimensions. To address these limitations, Ding et al. [22] introduce an alternative approach using the Nyström kernel matrix approximation. This method is more computationally efficient and provides theoretical consistency guarantees, making it a better choice for real-time applications. In this paper, we propose to adopt the Nyström approximation method to ensure efficient and effective kernel selection to identify the most important additive kernels based on the GP prior, and then use the selected kernels to optimize high-dimensional control problems safely and efficiently. The process is illustrated in Fig. 1. We validated our algorithm using the general benchmark Safe Control Gym [23], and experimental results demonstrate that our method outperforms existing high-dimensional safe Bayesian optimization algorithms for quadrotor trajectory control."
https://arxiv.org/html/2411.07551v1,SP-VIO: Robust and Efficient Filter-Based Visual Inertial Odometry with State Transformation Model and Pose-Only Visual Description,"Due to the advantages of high computational efficiency and small memory requirements, filter-based visual inertial odometry (VIO) has a good application prospect in miniaturized and payload-constrained embedded systems. However, the filter-based method has the problem of insufficient accuracy. To this end, we propose the State transformation and Pose-only VIO (SP-VIO) by rebuilding the state and measurement models, and considering further visual deprived conditions. In detail, we first proposed a system model based on the double state transformation extended Kalman filter (DST-EKF), which has been proven to have better observability and consistency than the models based on extended Kalman filter (EKF) and state transformation extended Kalman filter (ST-EKF). Secondly, to reduce the influence of linearization error caused by inaccurate 3D reconstruction, we adopt the Pose-only (PO) theory to decouple the measurement model from 3D features. Moreover, to deal with visual deprived conditions, we propose a double state transformation Rauch-Tung-Striebel (DST-RTS) backtracking method to optimize motion trajectories during visual interruption.Experiments on public (EuRoC, Tum-VI, KITTI) and personal datasets show that SP-VIO has better accuracy and efficiency than state-of-the-art (SOTA) VIO algorithms, and has better robustness under visual deprived conditions.","Visual inertial odometry (VIO) technology is widely used in various mobile robots for autonomous navigation. Although different VIO algorithms have been developed in the past decade, roboticists still expect algorithms with lower computation costs, higher accuracy, and better robustness. The mainstream VIO algorithms can be broadly divided into two categories: optimization-based [1, 2, 3, 4, 5] and filter-based methods [6, 7, 8, 9, 10]. Benchmark experiments [11, 12] show that filter-based VIO has the advantages of high computational efficiency and small memory requirements, and has a good application prospect in payload-constrained embedded systems. However, compared with the optimization-based VIO, filter-based methods are more efficient and also have the problem of insufficient accuracy, which limits the application of this method in some scenarios. Taking MSCKF [10], the representative work of filter-based VIO, as an example, this method uses null space projection to simplify the residual model and then performs EKF update to estimate the corresponding pose. Because the 3D features used in the visual residual model are obtained only through a 3D reconstruction process and are not jointly optimized with the system’s pose, it is easy to cause the accumulation of linearization errors and lead to the decline of navigation accuracy. In addition, EKF is prone to filtering inconsistency problems, that is, misestimating the observability of the system leads to performance degradation, which is also an important factor affecting the accuracy of filter-based VIO [13]. In summary, it can be found that the main reason for the accuracy damage of filter-based VIO is the accumulation of linearization errors [14]. Therefore, if we can solve the problem that filter-based methods are susceptible to linearization errors, it will help to develop a VIO framework that is both highly precise and efficient. As mentioned above, an important problem of MSCKF-based VIO is that it is not completely decoupled from 3D features, which are still required to construct visual residuals. The newly proposed PO theory indicates that reprojection error can be obtained solely through camera pose, which is equivalent to traditional multi-view geometry description [15, 16]. Inspired by PO theory, we derive a visual residual model based only on pixel coordinates and relative pose, which is completely decoupled from 3D features to avoid being affected by inaccurate 3D reconstruction processes. Meanwhile, the new residual model does not need the simplification of left null space, and can be used for filter update directly. On the other hand, the current strategies to solve the problem of filtering inconsistency can be broadly classified into two categories. One starts from the perspective of maintaining the system observability, such as the first estimate Jacobian EKF (FEJ-EKF) [17, 13, 18] and the observability constrained EKF (OC-EKF) [19, 20]. Since different scenes have diverse effects on the system observability, such as static and dynamic scenes, thus this method is more demanding in practical applications [21]. The other one starts with the model definition, such as the invariant Kalman filter (IEKF) [22, 23, 24, 25, 26], which uses the matrix Lie group to define the state space more strictly. The advantage of IEKF is its universality in different scenarios, but the derivation and implementation are too complex [21]. ST-EKF is a special case of matrix Lie group application, which obtains accuracy improvement by state transformation of velocity error and is easier to implement than IEKF [21, 27, 28]. However, ST-EKF only has a more rigorous definition of the velocity error state, and there are still theoretical imperfections. Based on ST-EKF, we propose the double state transformation EKF (DST-EKF), which redefines the system’s position error state and further improves the accuracy and consistency. The above works are concerned with solving the accumulation of linearization errors caused by inaccurate system modeling. In addition, visual measurement information is also an important source of linearization errors. Due to the sensitivity of camera to the environment, it is easy to be disturbed in the actual motion, resulting in the visual tracking failure, which makes the system input discontinuous measurement information and affects the subsequent state estimation accuracy. The current common solution is the visual relocalization [29], but this method relies on matching with the existing localization scenes and does not work when the motion trajectory is not repeated, which limits the robustness of VIO under non-closed-loop motion trajectories. Rauch-Tung-Striebel (RTS) [30] is an efficient optimal backtracking smoothing method, which can optimize the cumulative error based on the historical state information, and is not limited by whether a closed-loop trajectory is formed. Therefore, we propose the DST-RTS backtracking smoothing method combining DST-EKF and RTS smoother to correct motion trajectories during visual interruption. This will improve the robustness of VIO system under visual deprived conditions. Figure 1: This figure illustrating the full pipeline of the proposed SP-VIO, and part of the performance test results of this algorithm on popular public datasets, including trajectory errors and time costs. (a) Framework of SP-VIO. (b) Trajectory error on EuRoC [31] and Tum VI [32]. (c) Average runtime on EuRoC and Tum VI. In conclusion, we propose the state transformation pose-only VIO (SP-VIO) algorithm, as shown in Fig. 1, which considers both high accuracy and efficiency, and has certain robustness under visual deprived conditions. To be specific, the contributions of this paper are shown as follows: • We propose a pose-only measurement model that can be used for filter-based VIO systems. This model is decoupled from 3D features and avoids the damage to accuracy caused by inaccurate 3D reconstruction while maintaining efficiency. • We propose a system model based on the double state transformation Kalman filter(DST-EKF), which enables the VIO system to obtain the improvement of robustness and consistency by defining the velocity error state and the position error state more strictly. • We propose a DST-RTS backtracking smoothing strategy that does not rely on loop closure. This strategy uses the velocity information provided by the VIO system after the visual observation is recovered to correct the motion trajectory by RTS backtracking, which can reduce the cumulative error caused by visual interruption. • A novel VIO algorithm named SP-VIO is proposed, which takes MSCKF [10] as the baseline system and integrates the above functions. We conduct comprehensive performance evaluations on both popular public datasets (EuRoC[31], Tum VI[32], Kitti Odometry[33]) and personal datasets. Experimental results show that SP-VIO has both high precision and robustness, with comprehensive performance better than SOTA VIO algorithms."
https://arxiv.org/html/2411.07550v1,"Learning Autonomous Docking Operation of Fully Actuated
Autonomous Surface Vessel from Expert data",This paper presents an approach for autonomous docking of a fully actuated autonomous surface vessel using expert demonstration data. We frame the docking problem as an imitation learning task and employ inverse reinforcement learning (IRL) to learn a reward function from expert trajectories. A two-stage neural network architecture is implemented to incorporate both environmental context from sensors and vehicle kinematics into the reward function. The learned reward is then used with a motion planner to generate docking trajectories. Experiments in simulation demonstrate the effectiveness of this approach in producing human-like docking behaviors across different environmental configurations.,"Autonomous docking of unmanned surface vessels remains a challenging problem due to the complex hydrodynamics, environmental disturbances, and constrained maneuverability. There are lots of expert data created from docking of vessels such as ferries, yachts etc in real world scenarios through Automatic Identification System (AIS) and its own logging system equipped in vessels. This data can effectively be used for teaching autonomous surface vessels (ASV) to perform autonomous operations such as docking, navigation, collision avoidance etc. Through this way the experience of human captains can be captured in ASVs to perform autonomous operations. This essentially enables such autonomous vessels to coexist with traditional vessels. Rule-based approaches typically rely on handcrafted heuristics or predefined algorithms, which may struggle to adapt to complex and dynamic environments. In contrast, Neural networks which can learn from expert demonstrations, allows the autonomous system to adapt its behavior based on real world data. This approach can be used in vessels such as yachts, ferries to collect data while in human operation mode and use that data for learning operations such as docking, collision avoidance from the human expert data, without using predefined rule based approaches. This can gradually make such vessels to learn policies for autonomous operations after it learns from human mode of operations, which can essentially lead to making those vessels autonomous. As the vessels collect more and more data, the model learns and adapts to more complex behaviours in daily operations of vessel which is out of the picture for traditional rule based approaches. Imitation learning approaches exists to capture the expert behaviour from data to perform that specific task. Here in this paper one such approach Inverse Reinforcement learning algorithm (IRL) has been employed to capture expert behaviour from the generated data through simulation. More specifically since the reward function which is highly non linear in nature, it can be better captured by a variant of IRL, Maximum Entropy Deep Reinforcement Learning (MEDIRL) has been used in this implementation. This implementation integrates both environmental context and vessel kinematics into a deep inverse reinforcement learning framework for predicting the appropriate reward function and generating a policy for docking maneuvers. By employing a two-stage neural network architecture, we effectively process and combine information about the surrounding environment (such as dock layout, occupied berths, and obstacles) with the vessel’s kinematic data. This allows our system to generate docking strategies that are both safe and efficient, adapting to various scenarios much like an experienced human operator would."
https://arxiv.org/html/2411.07534v1,Effective Virtual Reality Teleoperation of an Upper-body Humanoid with Modified Task Jacobians and Relaxed Barrier Functions for Self-Collision Avoidance,We present an approach for retartgeting off-the-shelf Virtual Reality (VR) trackers to effectively teleoperate an upper-body humanoid while ensuring self-collision-free motions. Key to the effectiveness was the proper assignment of trackers to joint sets via modified task Jacobians and relaxed barrier functions for self-collision avoidance. The approach was validated on Apptronik’s Astro hardware by demonstrating manipulation capabilities on a table-top environment with pick-and-place box packing and a two-handed box pick up and handover task.,"I INTRODUCTION Despite advances in robot autonomy, teleoperation [1, 2, 3] remains a practical approach for remote surveying and intervention [4, 5, 6]. While direct teleoperation is not viable with long network latencies, it remains a useful tool for human-to-robot imitation learning [7, 8] which will enable future robots to be more autonomous. A core problem with direct teleoperation is retargeting human-to-robot movements, which is an active area of research [9, 10, 11, 12, 13, 14]. In a previous work, three 6 degree-of-freedom (DoF) trackers comprising of a VR headset and two controllers were used to fully control the pelvis height, torso, arms, and head of the NASA Valkyrie humanoid [14, 15]. With this approach, since there are more joints than tracker DoFs (n_{j}>n_{t}), multiple solutions for retargeting exist. Redundancy resolution is done by adding biasing posture tasks [12] and appropriate weighting of end-effector pose tasks [16]. Another approach is to add more trackers to the operator with a full-body suit [11], here there are more tracker DoFs than robot joints (n_{t}>n_{j}). In either case, some form of weight tuning of tasks is required to obtain a desired retargeted behavior. An appropriate weight set can be difficult to identify and in some cases poor tuning of these weights can cause unwanted behaviors such as oscillations [12]. In contrast, we propose to utilize a minimum set of trackers (n_{j}=n_{t}) and assign only a set of joints for each tracker DoF by modifying the corresponding task Jacobian for the retargetting task. This minimizes the responsibility of each joint, removes operational space task conflicts, conditions the retargeting behavior, and also informs the operator apriori which trackers map to which joints (Fig. 1). This approach of proper task allocation was effective in certain bipedal locomotion approaches [17, 18] and appears to be effective for teleoperation as well. Finally, during direct teleoperation, it can be burdening and unsafe for the operator to also consider robot-self collisions on top of commanding the robot as part of regular operations. An easy approach is to reject joint commands that would cause the robot to self-collide using a collision library checker [19]. However, this tends to cause abrupt pauses when performing a task. A better approach is to include self-collision avoidance as part of the Inverse-Kinematics (IK) problem of retargeting. To our knowledge, most published works ignore the self-collision avoidance problem and rely on the operator to execute safe behaviors. The VR interface for the NASA Valkyrie robot [16, 15] is an exception as it uses repulsive potential fields. We propose that signed-distance and relaxed-barrier functions are an improved approach to handle self-collisions. Figure 1: Four 6 DoF trackers (headset, left controller, right controller, waist tracker) and the set of joints they control on the robot. Using modified task Jacobians, the headset orientation controls the neck joints, the hand controllers’ pose control only the arm joints, the waist tracker’s vertical axis controls the torso yaw joint, and the forward position of the headset controls the leaning angle of the robot using the torso pitch joint."
https://arxiv.org/html/2411.07442v1,Learned Slip-Detection-Severity Framework using Tactile Deformation Field Feedback for Robotic Manipulation,"Safely handling objects and avoiding slippage are fundamental challenges in robotic manipulation, yet traditional techniques often oversimplify the issue by treating slippage as a binary occurrence. Our research presents a framework that both identifies slip incidents and measures their severity. We introduce a set of features based on detailed vector field analysis of tactile deformation data captured by the GelSight Mini sensor. Two distinct machine learning models use these features: one focuses on slip detection, and the other evaluates the slip’s severity, which is the slipping velocity of the object against the sensor surface. Our slip detection model achieves an average accuracy of 92%, and the slip severity estimation model exhibits a mean absolute error (MAE) of 0.6 cm/s for unseen objects. To demonstrate the synergistic approach of this framework, we employ both the models in a tactile feedback-guided vertical sliding task. Leveraging the high accuracy of slip detection, we utilize it as the foundational and corrective model and integrate the slip severity estimation into the feedback control loop to address slips without overcompensating. Videos and demonstrations are available at: https://sites.google.com/uw.edu/lsds","I INTRODUCTION Tactile sensing plays a pivotal role in robotic manipulation, offering a rich source of information for understanding and interacting with the environment [1]. Manipulation tasks such as delicate handling of objects [2] and secure grasping [3] under dynamic conditions rely on effective slip detection [4]. Traditional approaches to slip detection have predominantly relied on binary indicators of slip occurrence [5, 6, 7, 8, 9], leveraging tactile data to discern stable grips from unstable ones. However, this binary treatment overlooks the nuanced spectrum of slip dynamics, potentially leading to inadequate control strategies. Furthermore, reliance solely on visual feedback for slip detection and manipulation tasks presents inherent limitations, such as occlusions, varying lighting conditions, and the need for external viewpoints that may not always be feasible in confined environments [10]. Figure 1: Summary of the Slip-Detection-Severity Framework: A robot executes an object handling task, during which tactile features from the GelSight Mini sensor are extracted in real time. These features simultaneously feed into the Slip Detection and Slip Severity models. Upon detecting slip, the feedback controller actively adjusts the gripper to mitigate slip severity. To overcome the identified limitations, this research seeks to answer the primary question of how slip detection methods can be improved to effectively manage the complexities of slip dynamics. Furthermore, it explores how can slip be quantified to aid in the development of feedback control algorithms for more precise slip handling and mitigation. Our contribution centers on the direct detection of slip occurrences and the simultaneous estimation of slip severity, utilizing real-time tactile sensor data. We introduce a framework for learned slip detection and severity assessment, derived from the tactile features of the deformation vector field extracted from GelSight sensors. By identifying essential tactile features that capture the non-linear surface dynamics associated with slip events, we employ machine learning to directly map these features for slip detection and severity assessment. The efficacy of our framework is demonstrated through its integration into a feedback gripper controller as shown in Figure 1, showcasing how the slip-detection-severity feedback enables precise gripper positioning without the need for explicit force control or prior knowledge of the object’s size, geometry, or texture. Background and Related Work: Research in tactile sensing was initiated in the 1970s with the introduction of piezoelectric elements as strain sensors [11], and has expanded to include a diverse array of sensor technologies. These technologies are capable of detecting various object properties such as mass, geometry, texture, slip, and hardness, utilizing piezoelectric, capacitive array, optical, and magnetic sensors [12]. Among these, optical marker-based tactile sensors, including TacTip [13], TouchRoller [14], and GelSight [15][16], represent significant advancements in sensing high-resolution surface features and texture. TacTip sensors employ a camera to monitor the movement of white pins within a membrane upon object contact, mirroring a biomimetic design. TouchRoller, designed as a rolling sensor, acquires geometric information by traversing an object’s surface. This study focuses on the GelSight sensor, which uses a reflective gel-coated elastomer and LED illumination to track marker displacement, enabling accurate measurements of contact deformation using the deformation vector field. Foundational work has been established by demonstrating the use of tactile data for slip detection [17]. Optical methods utilize the eccentricity of the contact surface to measure object deformation [18]. The integration of machine learning has expanded slip detection capabilities, with Support Vector Machines (SVMs) utilized alongside TacTip sensor data [19]. Neural networks have been applied for slip classification [20], and GelSight sensor data have been incorporated to enhance object shape detection [7]. Furthermore, entropy-based methods utilize learning algorithms and shear marker displacement to predict slip likelihood [9, 21]. In this study, we concentrate on extracting tactile features through vector field analysis of the deformation field and employing learned models to detect slip. Research on predicting the relative velocity between grippers and manipulated objects through tactile sensing alone remains limited due to the complexity involved. A method for calculating sliding velocity utilizes a nonlinear observer used with the SUNTouch tactile sensor [22]. However, this technique encounters difficulties with objects that have small curvature radii or high deformability. Another approach, employing linear regression and capacitive-based nib-structure tactile sensors for slip speed prediction [23], presents a promising direction despite limitations in the feature space and low correlation with the regression variable. Recent efforts have leveraged CNNs alongside BioTac sensor measurements to assess the slipping speed of objects [24]. This approach is particularly effective with rigid objects but faces challenges in generalizing to deformable ones. This study aims to establish a direct correlation between slip velocity and tactile features extracted from vector field analysis, using learned models. We demonstrate that extracting features from the optical tactile sensors enables the development of models that effectively generalize across different object types."
https://arxiv.org/html/2411.07405v1,Quality of Control based Resource Dimensioning for Collaborative Edge Robotics,"With the increasing focus on flexible automation, which emphasizes systems capable of adapting to varied tasks and conditions, exploring future deployments of cloud and edge-based network infrastructures in robotic systems becomes crucial. This work, examines how wireless solutions could support the shift from rigid, wired setups toward more adaptive, flexible automation in industrial environments. We provide a quality of control (QoC) based abstraction for robotic workloads, parameterized on loop latency and reliability, and jointly optimize system performance. The setup involves collaborative robots working on distributed tasks, underscoring how wireless communication can enable more dynamic coordination in flexible automation systems. We use our abstraction to optimally maximize the QoC ensuring efficient operation even under varying network conditions. Additionally, our solution allocates the communication resources in time slots, optimizing the balance between communication and control costs. Our simulation results highlight that minimizing the delay in the system may not always ensure the best QoC but can lead to substantial gains in QoC if delays are sometimes relaxed, allowing more packets to be delivered reliably.","Flexible automation and mass customization are key drivers in this era of smart manufacturing and industrial development [1], contributing to advancing the goals of Industry 4.0 and 5.0. They can enable systems to adapt to varying production needs with minimal reconfiguration, making handling diverse and complex tasks easier, and finally paving the way for producing personalized products at scale. With the emergence of flexible automation, revisiting the mostly wired robotic solutions prevalent in the industry is required, as they do not offer mobility, adaptability and restrict collaboration. For such joint tasks, wireless collaborative robotics is crucial [2]. Wireless collaborative robotics necessitate managed systems promising low latency, high reliability, and flexibility, where 5G networks in combination with local edge computing seem attractive [3]. These 5G-based systems offer the possibility to offload different computational tasks to the edge cloud. However, practical implementations of collaborative robotics [4] where communications and compute are provided by 5G and edge computing-based infrastructures, respectively, are not well understood, opening some questions: (a) What are the different information flows in a collaborative robotic setup and how can their looped interactions be mapped to a 5G-based implementation? (b) How would the robotic setup react to uncertainties in terms of delays, reliability, capacity constraints emanating from a 5G setup? (c) Can an acceptable quality-of-control (QoC) [5] tracking the energy and control expenditure, be maintained? and finally, (d) How to develop an abstraction of robotic workloads parameterized by these uncertainties, to jointly optimize system performance? There have been a few works addressing these questions: Some works such as [6], minimize the end-to-end latency for networked robotic applications like automated guided vehicles (AGVs) on a factory floor. In their work, provided a delay requirement for their system, they find an allocation satisfying that requirement with fixed reliability bounds. While such works minimize the end-to-end latency, they lack an understanding of network reliability and how that might affect the robotic system. [7, 8] provide orchestration frameworks for networked collaborative robotics that offload motion level control to the edge. They highlight the importance of minimizing the time for the successful completion of mission-critical tasks and assume packet error rate (PER) and delay thresholds, with [7] also indicating capacity constraints. All these works, however, have a binary value function, where as long as a certain latency, PER or task success rate target is met, the system is considered stable. This calls for a joint understanding of latency and reliability to characterize the performance of the control application with a measurable metric such as QoC. Authors in [5] propose a QoC metric that is sensitive to control loop instabilities, estimating positional errors in tactile-visual control applications. While they individually show how QoC is affected by end-to-end latency, jitter, and packet drops, they don’t provide a coherent abstraction of QoC based on their combined understanding for robotic workloads and still lack in indicating relevant tradeoffs and joint optimization potentials. [9] tackles this by providing such an abstraction based on tradeoffs between latency and reliability for the problem of estimating dynamical systems over communication channels. Using this abstraction they obtain an optimal code-block length for the state estimation problem. Their communication model, however, is based on information/coding theory for finite blocklengths. Overall, we observe a dearth in literature where an abstraction of a collaborative robotic system parameterized with both latency and reliability is used to jointly optimize control applications connected with a 5G network. We aim to fill this gap, assuming a multi-robot setup where they collaborate towards a joint task, having offloaded their motion-level control to the 5G edge, with our contributions: • We derive a QoC-based abstraction for collaborative robotics, considering both network delay and reliability. • We exemplify the applicability of our approach with a consensus-based scenario, by considering a joint robotic system and network optimization. • Our simulation results show that by relaxing the delays in some cases, we save up to 32% in energy consumption as compared to State-of-the-art (SOTA) evaluation schemes. The rest of the paper is structured as follows: In Section II we introduce the system model before we derive QoC for a collaborative robotic system in Section III. We present our optimization framework, evaluation methodology and results in Section IV, while the paper is concluded in Section V."
https://arxiv.org/html/2411.07382v1,Dynamic Zoning of Industrial Environments with Autonomous Mobile Robots,"This paper presents a scheduling algorithm that divides a manufacturing/warehouse floor into zones that an Autonomous Mobile Robot (AMR) will occupy and complete part pick-up and drop-off tasks. Each zone is balanced so that each AMR will share each task equally. These zones change over time to accommodate fluctuations in production and to avoid overloading an AMR with tasks. A decentralized dynamic zoning (DDZ) algorithm is introduced to find the optimal zone design, eliminating the possibility of single-point failure from a centralized unit. Then a simulation is built comparing the adaptability of DDZ and other dynamic zoning algorithms from previous works. Initial results show that DDZ has a much lower throughput than other dynamic zoning algorithms but DDZ can achieve a better distribution of tasks. Initial results show that DDZ had a lower standard deviation of AMR total travel distance which was 2874.7 feet less than previous works. This 68.7% decrease in standard deviation suggests that AMRs under DDZ travel a similar distance during production. This could be useful for real-world applications by making it easier to design charging and maintenance schedules without much downtime. Video demonstration of the system working can be seen here: https://youtu.be/yVi026oVD7U","With recent hardware advances in sensors and computing power, autonomous mobile robots (AMRs) have become more feasible in manufacturing and warehouse environments. For companies to stay competitive and become more Lean [1], many have turned to an automated production process that deploys a fleet of AMRs or automated guided vehicles (AGVs) to complete simple pick-up and drop-off tasks (see figure 1). This alleviates workers from repetitive tasks and frees up time for more valuable production. With the introduction of AGVs and AMRs to industrial environments, many scheduling algorithms have been developed to manage orders while they are moving through the floor. Many of these algorithms can find the optimal order schedule, however, most are tailored toward the warehouse environment. Unlike warehouses, manufacturing environments are not as predictable since there are many different types of parts going to different assemblies and machines. Often, there are daily inconsistencies with production, which are caused by temporary delays like broken tools, missing parts, repair, and personnel. Figure 1: Human-robot collaboration in the warehouse [2] One way to approach scheduling in manufacturing environments is by sectioning off areas of the warehouse floor into zones where a robot will operate. Each robot performs pick-up and drop-off tasks from one point to another, assuming that a human will be at the same point to physically manage the items. The system will continually monitor and record data to determine if there is an imbalance between zones. If an imbalance is detected and persists, it will be corrected by redrawing zones based on the current data. Ho and Liao [3] introduced a simulated annealing (SA) algorithm with dynamic zoning for use with AGVs. This paper builds upon that algorithm and adjusts it for use with AMRs. For comparison, a genetic algorithm (GA) is also implemented replacing SA from the dynamic zoning algorithm in Ho and Liao [3]. We also introduce a Decentralized Dynamic Zoning (DDZ) algorithm that focuses on designing zones around the average load between each robot. DDZ uses Weighted Average Consensus [4] to find the average load in the network and then adjusts zones to lower the load standard deviation by independently running SA between each robot in the network. The system assumes that robots have a limited communication range and can only build zones based on their neighbor’s information. Zone information is shared with workstations that push parts to the next robot once they are finished processing. Method Scheduling for use with multiple AMRs Adjusts to a dynamic work environment Used with a manufacturing environment Used with a warehouse environment Decentralized Simulated annealing[3] ✓ ✓ Min-Max Strategy[5] ✓ Two-stage heuristic[6] ✓ SAC and reinforcement learning[7] ✓ ✓ Dynamic task chain[8] ✓ ✓ Auction based method[9] ✓ ✓ ✓ ✓ GA Dynamic zoning ✓ ✓ ✓ ✓ DDZ ✓ ✓ ✓ ✓ ✓ Table 1: Scheduling algorithm comparison: Addressed ✓, Not addressed Table 1 compares the method proposed in this paper and that of other research. The proposed method has the advantage of being applicable in both warehouse and manufacturing facilities. This method is designed in unpredictable environments where robots are only informed of pick-up and drop-off locations. DDZ relies on finding a load balance among all robots so that tasks can be equally shared between each robot. The main contributions of this paper are as follows: • Introduce a DDZ algorithm that finds the best zone design by focusing on reducing the standard deviation between the robot load and the calculated average. Each robot can only communicate with the robots that are within communication range and the workstations within their zone. • Compare DDZ to GA and previous works with SA [3] using a realistic simulated manufacturing floor. Throughput and distance traveled are compared to highlight the advantages and disadvantages of each method. • Provide the community with open source code for AMR experimentation and for use as a platform to build manufacturing/warehouse simulations using NVIDIA ISAAC Sim [10]."
https://arxiv.org/html/2411.07375v1,Instance Performance Difference: A Metric to Measure the Sim-To-Real Gap in Camera Simulation,"In this contribution, we introduce the concept of Instance Performance Difference (IPD), a metric designed to measure the gap in performance that a robotics perception task experiences when working with real vs. synthetic pictures. By pairing synthetic and real instances in the pictures and evaluating their performance similarity using perception algorithms, IPD provides a targeted metric that closely aligns with the needs of real-world applications. We explain and demonstrate this metric through a rock detection task in lunar terrain images, highlighting the IPD’s effectiveness in identifying the most realistic image synthesis method. The metric is thus instrumental in creating synthetic image datasets that perform in perception tasks like real-world photo counterparts. In turn, this supports robust sim-to-real transfer for perception algorithms in real-world robotics applications.","There exist several commonly-used metrics used to evaluate the quality of synthetic images and define an associated simulation-to-reality (sim-to-real) gap that pertains to how humans respond to these images [1, 2, 3, 4, 5]. These metrics are not suitable for the case when the synthetic images, which have individual real counterparts, are used for training perception neural networks (NNs) employed in robotic systems. Specifically in such a case, each synthetic image can be one-to-one paired with a corresponding real photo, but pixel positions of each object are not aligned perfectly between the real and synthetic pictures due to slight variations in the camera and/or object poses. Note that because synthetic images are used to train and test NNs for visual perception tasks, we care more about algorithmic performance than visual fidelity in a human perception sense. From the perspective of simulation, we propose an Instance Performance Difference (IPD) metric to define the “difference” between the real and synthetic domains by using the perception algorithm outputs. This metric is extended from the Contextualized Performance Difference (CPD) [6] and is similar to the metric used in [7]. Unlike conventional approaches that focus on the performance of an algorithm trained on the synthetic dataset and tested on the real dataset, IPD focuses on performance similarity between instances tested on both synthetic and real datasets, making it a more accurate measure of simulation performance. The main idea of IPD is as follows: if a synthetic image closely resembles a real photo, when a perception algorithm poorly detects an object in the real photo, the same object in the synthetic image should also be detected poorly. Conversely, when the object is detected well in the real photo, the same should happen in the synthetic image."
https://arxiv.org/html/2411.07342v2,"Learning Dynamic Tasks on a Large-scale Soft Robot 
in a Handful of Trials","Soft robots offer more flexibility, compliance, and adaptability than traditional rigid robots. They are also typically lighter and cheaper to manufacture. However, their use in real-world applications is limited due to modeling challenges and difficulties in integrating effective proprioceptive sensors. Large-scale soft robots (\approx two meters in length) have greater modeling complexity due to increased inertia and related effects of gravity. Common efforts to ease these modeling difficulties such as assuming simple kinematic and dynamics models also limit the general capabilities of soft robots and are not applicable in tasks requiring fast, dynamic motion like throwing and hammering. To overcome these challenges, we propose a data-efficient Bayesian optimization-based approach for learning control policies for dynamic tasks on a large-scale soft robot. Our approach optimizes the task objective function directly from commanded pressures, without requiring approximate kinematics or dynamics as an intermediate step. We demonstrate the effectiveness of our approach through both simulated and real-world experiments.","Elephant trunks, snakes, and certain invertebrates are capable of highly dynamic and fast motion while maintaining their flexibility, prehensile nature, and compliance. However, despite being modeled after these biological entities, soft robots still struggle to perform highly dynamic control tasks. This is because soft robots are effectively infinite-dimensional dynamical systems, and it is difficult to derive accurate kinematic or dynamics models that describe their motion well enough to allow for the design of controllers that fully exploit their capabilities. Large-scale soft robots (a few meters in length) are attractive because of their high force-to-weight ratio [1]. However, their increased inertia, surface area, and related gravitational effects also present further modeling difficulties. Figure 1: The soft robot design used in this paper for simulation and hardware-based experiments. This is a 12 degree of freedom (DOF) pneumatically actuated, large-scale soft robot about 1.3 meters in length. It consists of three continuum joints connected by two rigid links. Previous innovations in soft robot modeling, such as constant curvature [2], PDEs [3], finite elements [4], first-principles [5], splines [6], and deep learning [7] have allowed soft robots to perform well enough at static control tasks. However, they generally require the robot to move slower to maintain lower tracking error, and fail when the robot is carrying non-uniform loads or operating at higher velocities. Furthermore, their tracking errors are higher than those of traditional rigid robots. Modeling approaches with high data requirements (i.e., deep neural networks), are less suitable for soft robots because the data collection process may exacerbate “wear and tear” (which occurs naturally and frequently over time) on the compliant parts of the robot. This may affect the robot’s dynamics and cause significant deviations from expected behavior. Other effects such as temperature may also affect the robot’s dynamics or contribute to measurement noise. Given these modeling challenges, we adopt a task-oriented, Bayesian optimization-based control learning approach that does not require an explicit model of the soft robot. Instead, we learn a direct mapping from the controller parameters to the task objective function and use this mapping to find control parameter settings that maximize the task objective. This approach improves the efficiency of building a controller by significantly reducing the number of necessary hardware evaluations on the robot. The wide variety of designs and underlying actuation principles (i.e., pneumatics, hydraulics, or cable-driven mechanisms) in soft robots for different applications have also made transferring modeling and control strategies between robot platforms challenging. In this regard, the Bayesian optimization (BayesOpt) approach presented here makes fewer platform-specific assumptions, making the approach easier to adapt for use on different soft robots. Our contributions can be summarized as follows: 1. We present a Bayesian optimization-based approach to soft robot control tuning, capable of learning dynamic, high-dimensional behaviors directly from low-dimensional control parameterizations. 2. We learn successful control policies with remarkably few interactions with the soft robot, minimizing possible wear and tear on the robot hardware. 3. We evaluate the approach on two challenging tasks: throwing and hammering, which require highly dynamic motions and complex velocity profiles for successful completion. 4. We demonstrate our approach on a physical large-scale soft robot (shown in Figure 1), successfully learning control parameters for a “pseudo-throwing” task despite uncertain objective function observations."
https://arxiv.org/html/2411.07309v1,"Proprioceptive and Exteroceptive Information Perception in a
Fabric Soft Robotic Arm via Physical Reservoir Computing","AbstractOver the past decades, we have witnessed a rapid emergence of soft and reconfigurable robots thanks to their capability to interact safely with humans and adapt to complex environments. However, their softness makes accurate control very challenging. High-fidelity sensing is critical in improving control performance, especially posture and contact estimation. To this end, traditional camera-based sensors and load cells have limited portability and accuracy, and they will inevitably increase the robot’s cost and weight. In this study, instead of using specialized sensors, we only collect distributed pressure data inside a pneumatics-driven soft arm and apply the physical reservoir computing principle to simultaneously predict its kinematic posture (i.e., bending angle) and payload status (i.e., payload mass). Our results show that, with careful readout training, one can obtain accurate bending angle and payload mass predictions via simple, weighted linear summations of pressure readings. In addition, our comparative analysis shows that, to guarantee low prediction errors within 10%, bending angle prediction requires less training data than payload prediction. This result reveals that balanced linear and nonlinear body dynamics are critical for the physical reservoir to accomplish complex proprioceptive and exteroceptive information perception tasks. Finally, the method of exploring the most efficient readout training methods presented in this paper could be extended to other soft robotic systems to maximize their perception capabilities.","Over the past decades, we have witnessed a rapid emergence of soft and reconfigurable robots because of their capability to interact safely with humans and operate in complex environments [1, 2, 3]. It is also widely recognized that materializing the full potential of soft robots requires multi-faceted efforts, ranging from their design and actuation all the way to control and autonomy. In this regard, there has been a proliferation of studies in soft robotic design, fabrication, and actuation mechanisms [4, 5], with several of them being successfully commercialized in healthcare and manufacturing. On the other hand, progress in the sensing and control of soft robots is relatively nascent [6, 7]. In particular, integrated and reliable sensing – the pre-requisite of effective control and autonomy – remains an open challenge [8]. Soft robots typically exhibit complex deformations internally and interact with the external environment, so the sensors embedded in their body need to provide two types of information. One is proprioception, related to the robotic body’s movement, shape, and location, and the other is exteroception, related to external stimulation such as touch, pressure, and temperature [9, 10]. Correspondingly, a variety of soft sensors has been developed based on different working principles. For example, resistive (and piezo-resistive) sensors can directly measure the local strain of a soft actuator [11, 12], capacitive sensors can sense the physical touch on the robotic skin [13], and flexible optical fiber sensors can be distributed throughout the robotic body to inform the overall shape changes [14]. Finally, different kinds of sensors can be integrated to obtain a comprehensive and accurate understanding of the robots’ working conditions (aka. multi-modal sensing or sensor fusion [15, 16, 17]). While the readings from these sensors are quite simple, such as resistance, pressure, and light intensity, they can be sent to a centralized processor to extract more complex information like body kinematics or the target object’s stiffness. This “information perception” is sometimes accomplished with the help of data-driven and machine learning methods [18], and the result can inform control and decision-making. However, adding soft sensors can complicate the overall robot design and fabrication setup, resulting in a significant increase in the robot’s cost and, sometimes, a decrease in mechanical flexibility. A large array of sensors could also produce substantial data, demanding high computational power and creating time delays. But most importantly, an over-reliance on these additional and complex sensors can lead us to neglect an important source of sensing information: the soft robotic body itself. Figure 1: A “big-picture” comparison of two different approaches for pneumatic soft robot sensing (a) One can embed different kinds of sensors into the soft robotic body and process their readings — using machine learning, for example — to obtain proprioceptive and exteroceptive information. In this case, pressure sensors are typically used for actuation control only. (b) One can also treat the robotic body as a reservoir computer (i.e., a “physical neural network”) and obtain complex information by simple, weighted linear summations of its pressure sensor readings. The soft robotic arm figure is adopted from [19]. As a soft robot is activated and interacts with the environment, its body’s deformation and dynamic responses inherently harbor many useful and complex proprioceptive and exteroceptive information [20] — That is, the soft robotic body itself can be part of the “sensory system” with embodied information. Therefore, one can potentially use only a few simple sensors (preferably those “already there”) to measure the body dynamics and then process the readings to acquire more complex and useful information. To this end, employing the framework of physical reservoir computing is a promising strategy to achieve such “information perception through body mechanics.” Reservoir computing is a branch within the discipline of artificial neural networks. In this architecture, the interconnection weights inside the neural network’s kernel remain fixed, and only readout weights are trained to reach the targeted outputs [21, 22, 23, 24, 25, 26, 27]. Since the neural network does not change during training, one can use a physical body as the reservoir and harvest its nonlinear and high-dimensional dynamic responses as the computational resource (essentially, the physical body becomes the neural network; see Fig. 1). Simple mechatronic components like embedded sensors are required to construct a physical reservoir, but computing occurs entirely in the mechanical domain. Offloading portions of computing burdens to the mechanical domain brings significant benefits like lower power consumption, fewer digital-analog conversions, and much better reliability against difficult working conditions. Most importantly, the physical reservoir framework provides a gateway to access the sensory information embodied in the mechanical body [28, 29]. Therefore, in this study, we adopt a fabric-based, pneumatic soft robotic arm as the physical platform to examine the feasibility and performance of using simple pressure reading, combined with physical reservoir computing, to achieve proprioceptive and exteroceptive information perception tasks (Fig. 2). Here, pressure sensors are inherently available in the pneumatic system, so using them would not significantly increase the robot’s complexity. Through extensive experimentation and readout training based on the physical reservoir computing framework, we uncovered that a simple, weighted linear summation of embedded pressure sensor readings can simultaneously offer us accurate predictions of bending posture (proprioceptive) and end payload mass (exteroceptive) (Figs. 1, 2). We also examined how to use the minimum amount of pressure reading to obtain these predictions accurately. In what follows, the second section of this paper briefly describes the experimental setup of the fabric-based pneumatic robotic arm and the corresponding physical reservoir computing framework. The third section details the results for single and multi-task information perception. This paper ends with a conclusion and a discussion of future work. Figure 2: Setting up the robotic arm as a physical reservoir computer. (a-b) In this study, we use a fabric-based robotic arm segment as the physical testbed, which has three columns of pressurized pillows. (b-c) Working principle of the soft robotic arm reservoir. There are two different inputs to the reservoir: actuation pressure P^{(i)} and end payloads M^{(j)}. Seven pressure sensors are distributed throughout the sensing column, and their readings (aka. the reservoir states) will be plugged into weighted linear summation for readout training and prediction. (d) Measured actuation pressures s_{in}(t) sent to the robotic arm. (e) The corresponding robot bending angle captured by the motion capture system (without any payload). (f) Pressure sensor readings from actuation pressure input P^{(1)} and P^{(7)} under payload input M^{(1)}, these data constitute pressure state vector \mathbf{S}^{(1,1)}(t) and \mathbf{S}^{(7,1)}(t)."
https://arxiv.org/html/2411.08027v1,LLMPhy: Complex Physical Reasoning Using Large Language Models and World Models,"Physical reasoning is an important skill needed for robotic agents when operating in the real world. However, solving such reasoning problems often involves hypothesizing and reflecting over complex multi-body interactions under the effect of a multitude of physical forces and thus learning all such interactions poses a significant hurdle for state-of-the-art machine learning frameworks, including large language models (LLMs). To study this problem, we propose a new physical reasoning task and a dataset, dubbed TraySim. Our task involves predicting the dynamics of several objects on a tray that is given an external impact – the domino effect of the ensued object interactions and their dynamics thus offering a challenging yet controlled setup, with the goal of reasoning being to infer the stability of the objects after the impact. To solve this complex physical reasoning task, we present \operatorname{LLMPhy}, a zero-shot black-box optimization framework that leverages the physics knowledge and program synthesis abilities of LLMs, and synergizes these abilities with the world models built into modern physics engines. Specifically, \operatorname{LLMPhy} uses an LLM to generate code to iteratively estimate the physical hyperparameters of the system (friction, damping, layout, etc.) via an implicit analysis-by-synthesis approach using a (non-differentiable) simulator in the loop and uses the inferred parameters to imagine the dynamics of the scene towards solving the reasoning task. To show the effectiveness of \operatorname{LLMPhy}, we present experiments on our TraySim dataset to predict the steady-state poses of the objects. Our results show that the combination of the LLM and the physics engine leads to state-of-the-art zero-shot physical reasoning performance, while demonstrating superior convergence against standard black-box optimization methods and better estimation of the physical parameters. Further, we show that \operatorname{LLMPhy} is capable of solving both continuous and discrete black-box optimization problems.","Many recent Large Language models (LLMs) appear to demonstrate the capacity to effectively capture knowledge from vast amounts of multimodal training data and their generative capabilities allow humans to naturally interact with them towards extracting this knowledge for solving challenging real-world problems. This powerful paradigm of LLM-powered problem solving has manifested in a dramatic shift in the manner of scientific pursuit towards modeling research problems attuned to a form that can leverage this condensed knowledge of the LLMs. A few notable such efforts include, but not limited to the use of LLMs for robotic planning (Song et al., 2023; Kim et al., 2024), complex code generation (Tang et al., 2024; Jin et al., 2023), solving optimization problems (Yang et al., 2024; Hao et al., 2024), conduct sophisticated mathematical reasoning (Trinh et al., 2024; Cherian et al., 2024), or even making scientific discoveries (Romera-Paredes et al., 2024). Figure 1: Frames from an example dynamical sequence in our TraySim dataset. The left-most frame shows the first frame of the scene with many objects on the tray and is going to be impacted by a black pusher (right-bottom). The subsequent frames show the state of the system at the 25-th, 50-th, and the 200-th time step (each step is 0.01s). Our task is for a model to reason through the dynamics of the system and predict the stability of each object on the tray at the end of the episode in a zero-shot manner, when provided as input only the first frame of the sequence. While current LLMs seem to possess the knowledge of the physical world and may be able to provide a plan for solving a physical reasoning task (Singh et al., 2023; Kim et al., 2024) when crafted in a suitable multimodal format (prompt), their inability to interact with the real world or measure unobservable attributes of the world model, hinders their capability in solving complex physical reasoning problems (Wang et al., 2023; Bakhtin et al., 2019; Riochet et al., 2021; Harter et al., 2020; Xue et al., 2021). Consider for example the scene in Figure 1. Suppose a reasoning model is provided as input only the first image (left-most) and is asked to answer the question: Which of the objects will remain upright when the tray is impacted by the black pusher with a velocity of 4.8 m/s?. To answer this question, the model must know the various physical attributes of the system, including the masses of the objects, friction coefficients between the contacts and their respective contact forces, stiffness of the objects, among others. While, a large sophisticated machine learning model (such as an LLM) may be able to provide an educated guess based on the intuitive physics of the system, a useful solution would demand a more intricate reasoning strategy in estimating the real-world physics and dynamics; such complex dynamics may be difficult or even impossible to be correctly learned solely from training data. Conversely, advancements in graphics hardware and software have led to the development of advanced physics engines capable of simulating realistic world models Makoviychuk et al. (2021); Bear et al. (2021); Todorov et al. (2012); Gan et al. (2020). Thus, rather than having the LLM to learn the world physics, our key idea is to consider using a physics engine in tandem with the LLM, where the LLM may use its world knowledge for generating scene-based reasoning hypotheses while the simulator is used to verify them within the physical world model. To study this problem, we consider the novel task of predicting the dynamics of objects and their stability under the influence of an impact – an important problem for a variety of robotic applications (Gasparetto et al., 2015; Ahmed et al., 2020). In this paper, we consider this problem in a challenging setting using our new dataset, TraySim, in which the impact is caused by a pusher colliding to a tray that holds several objects of varied sizes, masses, and centers of gravity, with the goal of predicting the dynamics of each of the object instances. We cast this task as that of answering physical reasoning questions. Specifically, as illustrated in Figure 1, TraySim includes simulated video sequences consisting of a tray with an arbitrary number of objects on it and given the first video frame of a given scene, the task of the reasoning model is to infer which of the objects on the tray will remain upright after the impact when the system has stabilized. As is clear from Figure 1, solving this task will require the model to derive details regarding the physical properties of each of the objects and their contacts, as well as have the ability to imagine the system’s dynamics through multi-body interactions influenced by the various internal and external forces from the impact. Our task presents a challenging reasoning setup for current machine learning models, including LLMs. To solve this task, we propose \operatorname{LLMPhy}, a black-box optimization setup combining an LLM with a physics engine that leverages the program synthesis abilities of the LLM to communicate with the engine for solving our task. \operatorname{LLMPhy} operates in two phases: i) a parameter estimation phase, where \operatorname{LLMPhy} is used as a continuous black-box optimization module towards inferring the physical parameters of the objects, including the friction, stiffness, damping, etc. from a given example video sequence, and ii) a scene understanding phase, where the LLM-simulator combination is used as a discrete black-box optimizer to reconstruct the problem layout for synthesizing the setup within the simulator for execution. Our framework builds a feedback loop between the LLM and the physics engine, where the LLM generates programs using its estimates of physical attributes; the programs are executed in the simulator, and the error from the simulations are fed back to the LLM as prompts to refine its estimates until a suitable convergence criteria is met. Note that we do not assume any differentiablity properties of the simulator, which makes our setup highly general. This allows the approach to function as a black-box optimization framework, enabling its use with a wide range of simulators without the need for gradient-based methods. While we may generate unlimited data using our simulation program, given the zero-shot nature of our setup, we synthesized 100 sequences in our TraySim dataset to demonstrate the effectiveness of \operatorname{LLMPhy}. Each sample in TraySim has two video sequences: i) the task sequence of which only the first frame is given to a reasoning agent, and ii) a parameter-estimation video sequence which has a lesser number of instances of each of the object types appearing in the task sequence; the latter sequence has an entirely different layout and dynamics of objects. To objectively evaluate the performance, we cast the task as a physical question answering problem, where the LLM is required to select the correct subset of answers from the given candidate answers. Our results on TraySim show that \operatorname{LLMPhy} leads to promising improvements in performance against alternatives, including using Bayesian optimization, CMA-ES, and solely using an LLM for physical reasoning. Interestingly, we also find that \operatorname{LLMPhy} estimates the physical parameters better and powerful recent LLMs – such as OpenAI o1-preview – show trends of superior optimization convergence. Before moving forward, we summarize below our main contributions: • We consider the novel task of reasoning over complex physics of a highly dynamical system by combining LLMs with non-differentiable physics engines. • We propose a zero-shot reasoning framework \operatorname{LLMPhy}, that combines the reasoning and program synthesis abilities of an LLM with the realistic simulation abilities of a physics engine. This approach is used to estimate the physical parameters of the model, the scene layout, and synthesizing the dynamical scene for inferring the solution. • We introduce a novel synthetic multi-view dataset: TraySim, to study this task. The dataset consists of 100 scenes for zero-shot evaluation. • Our experiments using \operatorname{LLMPhy} on the TraySim dataset demonstrate promising results against related baselines, highlighting its potential for tackling complex physics-based reasoning tasks that involves both discrete and continuous optimization sub-tasks."
https://arxiv.org/html/2411.07954v2,"Learning Memory Mechanisms for
Decision Making through Demonstrations","In Partially Observable Markov Decision Processes, integrating an agent’s history into memory poses a significant challenge for decision-making. Traditional imitation learning, relying on observation-action pairs for expert demonstrations, fails to capture the expert’s memory mechanisms used in decision-making. To capture memory processes as demonstrations, we introduce the concept of memory dependency pairs (p,q) indicating that events at time p are recalled for decision-making at time q. We introduce AttentionTuner to leverage memory dependency pairs in Transformers and find significant improvements across several tasks compared to standard Transformers when evaluated on Memory Gym and the Long-term Memory Benchmark. Code is available at https://github.com/WilliamYue37/AttentionTuner.","Partially Observable Markov Decision Processes (POMDPs) offer a framework for modeling decision-making in environments where the agent’s information is incomplete, a common situation in real-world scenarios such as a robot operating based on limited camera observations. Making effective decisions under such conditions necessitates incorporating the agent’s history, which can be encoded through memory mechanisms like Recurrent Neural Networks (RNNs) (Hausknecht & Stone, 2017; Karkus et al., 2017; Zhu et al., 2018; Igl et al., 2018; Hafner et al., 2019) or self-attention architectures such as Transformers (Esslinger et al., 2022). However, it’s not always straightforward for a fully automated system to identify which points in history are crucial to remember for a particular decision. On the other hand, when humans learn, we are often taught not just the actions we need to take at the moment, but also which past events and memories should be recalled. For instance, a survival instructor might instruct students to recollect previously observed landmarks for navigating back to base camp or a coach could ask an athlete to recall a past encounter with an opponent when making their next move. With this motivation in mind, this study concentrates on learning memory mechanisms essential for decision-making in POMDP tasks via expert demonstrations, also known as imitation learning. Standard imitation learning methods, which involve experts providing observation-action pairs, are insufficient in POMDP tasks as they do not capture the memory processes experts employ during decision-making. To capture memory mechanisms through demonstration, we introduce the use of memory dependency pairs (p,q), where p<q, indicating that the observation at time p ought to be recalled for decision-making at time q. These memory dependency pairs can be integrated into the widely-used Transformers (Vaswani et al., 2023) by applying a loss to the self-attention matrix to reinforce attention between tokens representing times p and q (Figure 1). The main contributions of this paper are as follows: • We introduce memory dependency pairs to incorporate memory mechanisms into demonstrations for imitation learning in POMDPs and to improve long-term credit assignment. • We develop the Long-term Memory Benchmark (LTMB) for evaluating long-term memory capabilities in decision-making tasks. • We introduce AttentionTuner, a novel method for leveraging memory dependency pairs in self-attention architectures, and benchmark it against vanilla Transformers on Memory Gym (Pleines et al., 2024) and LTMB. Empirical analyses show that AttentionTuner significantly improves success rates on four tasks and aids the optimizer in consistently navigating the loss landscape towards solutions with better generalizability compared to those found by optimizing the vanilla Transformer. Ablations reveal that these improvements in learning can be attained with as few as 0.1% of demonstrations annotated."
https://arxiv.org/html/2411.07862v1,Iterative Learning Control with Mismatch Compensation for Residual Vibration Suppression in Delta Robots,"Unwanted vibrations stemming from the energy-optimized design of Delta robots pose a challenge in their operation, especially with respect to precise reference tracking. To improve tracking accuracy, this paper proposes an adaptive mismatch-compensated iterative learning controller based on input shaping techniques. We establish a dynamic model considering the electromechanical rigid-flexible coupling of the Delta robot, which integrates the permanent magnet synchronous motor. Using this model, we design an optimization-based input shaper, considering the natural frequency of the robot, which varies with the configuration. We proposed an iterative learning controller for the delta robot to improve tracking accuracy. Our iterative learning controller incorporates model mismatch where the mismatch approximated by a fuzzy logic structure. The convergence property of the proposed controller is proved using a Barrier Composite Energy Function, providing a guarantee that the tracking errors along the iteration axis converge to zero. Moreover, adaptive parameter update laws are designed to ensure convergence. Finally, we perform a series of high-fidelity simulations of the Delta robot using Simscape to demonstrate the effectiveness of the proposed control strategy.","Over the past decades, pick-and-place parallel robots have been widely used in food, electronics, packaging and other industries, due to their high speed and high acceleration motion capability [1, 2]. As typical representatives, Delta robots possess three translational degrees of freedom [3]. However, in order to reduce the weight of robots to achieve higher acceleration and decrease the energy costs, the links of the robots are made from lightweight materials, such as carbon fiber, which inevitably results in the deterioration of the residual vibration and eventually undermines the positioning accuracy of the robots. The occurrence of vibration makes it more difficult to achieve high performance trajectory tracking control and may cause instability in extreme circumstances. Therefore, designing an effective controller and suppressing the residual vibration are of great significance for the application of Delta robots as well as other parallel robots. Generally speaking, the approaches of vibration suppression can be divided into two categories, passive vibration suppression approaches [4, 5] and active vibration suppression approaches [6, 7, 8]. The passive approaches utilize additional damping materials to increase the damping ratio of the system. Although these kinds of methods can reduce the vibration effectively, the use of extra materials increases the manufacturing costs. On the contrary, the active approaches involve designing a control system to achieve trajectory tracking and vibration suppression simultaneously. The active approaches can be classified into feedback methods and feedforward methods. The feedback methods have to acquire real time vibration signals to form the closed-loop controllers [9, 10], which requires the installation of additional sensors to measure the vibration signals. The low-level controllers have to be changed and redesigned by the users, which may not be allowed for some commercial parallel robots. The feedforward methods do not change the existing low-level controllers but just modify the input reference trajectory based on the dynamic behavior of the systems, which are obviously easier for realization. Thus, controller design and residual vibration suppression can be addressed separately. Among the feedforward methods, the input shaping techniques are very common and useful tools to suppress the residual vibration [11, 12, 13]. The principle of input shaping techniques is to modify the reference trajectory, which results in a deviation between the original reference trajectory and the modified trajectory. Therefore it may not be suitable for those task conditions where the reference trajectory is designed strictly and cannot be adjusted. However, for all high-speed parallel robots including Delta robots used to carry out pick-and-place operations, only start and end points are crucial. In order to achieve satisfactory working accuracy, numerous control methods had been proposed for Delta robots and other parallel robots in the past decades, including sliding mode control [14, 15], robust control [16, 17], intelligent control [18, 19], etc. However, all these mentioned control strategies only depend on the error information of the current operation period. For those applications where robots repeatedly perform the same tasks, the historical error information is a valuable resource to improve the control performance. Previous research has illustrated that the trajectory has a profound impact on the vibration of Delta robots. Therefore, it is crucial to ensure that the robot can track the reference trajectory to prevent unexpected vibration. Delta robots are used to perform the pick-and-place operation, which is repetitive along the fixed trajectory and within the same duration time. Under this circumstance, iterative learning control (ILC) [20, 21] as a suitable control approach to achieve a high performance and high accuracy trajectory tracking, has been widely employed in various fields, such as high-speed trains, precision motion stages, etc [22, 23]. From the few existing applications of ILC on parallel / Delta robots, the restrictions of resetting condition and repetitive trajectory are overcome in the ILC proposed in [24]. It has been further developed in [25] to achieve an adaptive robust proportional-derivatives ILC strategy in which a robust term is introduced to compensate the repetitive and nonrepetitive disturbance. However, the above robust ILC methods for Delta robots require the norm of disturbance to satisfy certain relationships, which causes difficulties in their application. Moreover, the selection of control gains is complexly restricted due to the requirement of stability guarantee. Uncertainty compensation is in this case a suitable approach to address this problem, particularly when the uncertainty arises from model mismatch. A model mismatch compensation method is proposed in [23] for precision motion stages, in which the Gaussian process regression is used to predict the unknown model mismatch, and the error is compensated using a repetitive control approach. To improve modeling, the coupling between the mechanical and electrical systems, which profoundly influences the dynamic behavior of robots [26], needs to be accounted for. Thus, for completeness, the model of PMSM should be integrated into the dynamic model of Delta robots. Furthermore, in order to guarantee the working safety, the robots’ velocity needs to be constrained. To introduce state constraints in the control problem, an adaptive neural network control algorithm is proposed in [27] for a wheeled mobile robot with velocity constraints, in which the barrier Lyapunov function (BLF) is introduced to guarantee the velocity constraints. The derivation of convergence guarantees for the BLF is inspired by [27] in this paper. For parallel robots, especially for Delta robot, studying the effect of velocity constraints control is not common. The excessive speed not only requires more energy consumption but also increases the risks associated with the operation. Therefore, it is of great importance to take angular velocity constraints into account. In this paper, we propose a control strategy combining input shaping techniques and ILC for Delta robot with PMSM and angular velocity constraints. The main contributions of this work are listed as follows. 1. We establish an integrated dynamic coupling mathematical model of Delta robots, where the flexibility of joints and links, as well as the dynamics of PMSM are taken into account. Based on the established model, we design an optimal input shaper by two global optimization objectives, which can suppress the residual vibration effectively. 2. According to the concept of the singular perturbation method (SPM), we propose an adaptive mismatch-compensated iterative learning controller (AMCILC) for the rigid-body motion coordinates of a Delta robot. The iterative learning method can effectively address the repetitive tasks of Delta robots. We introduce the FLS to approximate the model mismatch including the damping term of PMSM. By using two designed adaptive iterative update laws, we employ a Barrier Composite Energy Function (BCEF) to prove the convergence property of the tracking errors, in which the barrier Lyapunov function can ensure the velocity constraints to be satisfied. 3. Based on Simscape’s multi physical domain coupling function, we establish a high-fidelity simulation model of a Delta robot system to verify the performance of the proposed input shaper-based adaptive mismatch-compensated iterative learning controller (IS-AMCILC). The rest of this paper is summarized as follows. The coupling dynamic mathematical model of a Delta robot is established and the optimal input shaper is designed in Section II. Then, in section III, the design and stability proof of IS-AMCILC are conducted in which the velocity constraints are taken into account. A series of simulations is performed to verify the effectiveness of the proposed control strategy in Section IV. Finally, conclusions are drawn in Section V."
https://arxiv.org/html/2411.07830v1,Singularity-Avoidance Control of Robotic Systems with Model Mismatch and Actuator Constraints,"Singularities, manifesting as special configuration states, deteriorate robot performance and may even lead to a loss of control over the system. This paper addresses the kinematic singularity concerns in robotic systems with model mismatch and actuator constraints through control barrier functions (CBFs). We propose a learning-based control strategy to prevent robots entering singularity regions. More precisely, we leverage Gaussian process (GP) regression to learn the unknown model mismatch, where the prediction error is restricted by a deterministic bound. Moreover, we offer the criteria for parameter selection to ensure the feasibility of CBFs subject to actuator constraints. The proposed approach is validated by high-fidelity simulations on a 2 degrees-of-freedom (DoFs) planar robot.","I INTRODUCTION Robots are becoming increasingly prevalent across various industries, such as robotic arms used in industrial production and parallel-mechanism based legged robots. Singularities, arising from specific geometric relationships between links, can cause robots to lose or gain one or more DoFs, potentially leading to a loss of control over the system. Therefore, avoiding singular configurations is crucial to ensure safe operation for robotic systems. Directly modifying the reference trajectories is an effective method to avoid singularities. For example, a linear weighting method based post-processing non-singular trajectory generation method was proposed for a 5-DoFs hybrid machining robot in [1]. An algorithm based on output twist screws was presented in [2] to address type II singularity in parallel mechanisms by modifying trajectories. However, the trajectory modification method lacks sufficient flexibility, as one has to repeat the process for different trajectories. Moreover, non-singular reference trajectories are unable to guarantee robots not entering singularity regions, due to the presence of control errors. In recent years, advancements in safe optimization enabled by CBFs offer a promising alternative solution to the singularity avoidance problem. CBFs are powerful tools for handling various constraints, which enable their application in numerous safety-critical fields [3]. For example, one can leverage multiple CBFs to coordinate connected and automated agents at intersections [4], where the collision avoidance CBFs and velocity CBFs have to be jointly feasible under input constraints. A CBFs design methodology is proposed in [5] for Euler-Lagrange systems with position, velocity and input constraints. CBFs can be also used to ensure the safety of learned models for control in robotic systems [6], and to impose safety-critical constraints in a continuous-time trajectory generation process [7]. In order to handle model mismatch, robust CBFs are developed by introducing a compensation term based on the bound of uncertainty [8]. GP-based learning methods are another suitable method to tackle model mismatch [9], as they provide a quantification of the prediction uncertainty, which could be used to obtain the corresponding bound [10, 11]. There is little research regarding CBFs in addressing singularity concerns. In [12], CBFs were utilized to tackle singularity problem in passivity-based control. However, the feasibility of CBFs in [12] is based on the assumption that joints can provide unbounded torques, which does not precisely correspond to the capabilities of motors in practice. In addition, model mismatch has not been addressed in [12]. This paper proposes a methodology for CBFs construction to address singularity avoidance problem in robotic systems with model mismatch and subject to actuator constraints. The primary contributions of this work are summarized as follows: (i) the theoretical guarantee of the feasibility of CBFs with model mismatch and actuator constraints is obtained, as well as the parameter selection criteria is provided, (ii) the model mismatch is learned using GP regression combined with a deterministic error bound, and (iii) the proposed approach is validated by high-fidelity 2 DoFs planar robot simulations on Simscape. Notation: \mathbb{R} and \mathbb{R}_{\geq 0} denote the set of real, non-negative real numbers, respectively. The Euclidean norm is denoted by \left\|\cdot\right\|. \mathbb{N}_{n} denoting the set of natural numbers \{1,\cdots,n\}. The matrix inequality A\leq B for matrices A and B means that the matrix B-A is positive semidefinite. e_{i} denotes the ith column of nth-order identity matrix I_{n}."
https://arxiv.org/html/2411.07799v1,"Horticultural Temporal Fruit Monitoring via
3D Instance Segmentation and Re-Identification using Point Clouds","Robotic fruit monitoring is a key step toward automated agricultural production systems. Robots can significantly enhance plant and temporal fruit monitoring by providing precise, high-throughput assessments that overcome the limitations of traditional manual methods. Fruit monitoring is a challenging task due to the significant variation in size, shape, orientation, and occlusion of fruits. Also, fruits may be harvested or newly grown between recording sessions. Most methods are 2D image-based and they lack the 3D structure, depth, and spatial information, which represent key aspects of fruit monitoring. 3D colored point clouds, instead, can offer this information but they introduce challenges such as their sparsity and irregularity. In this paper, we present a novel approach for temporal fruit monitoring that addresses point clouds collected in a greenhouse over time. Our method segments fruits using a learning-based instance segmentation approach directly on the point cloud. Each segmented fruit is processed by a 3D sparse convolutional neural network to extract descriptors, which are used in an attention-based matching network to associate fruits with their instances from previous data collections. Experimental results on a real dataset of strawberries demonstrate that our approach outperforms other methods for fruits re-identification over time, allowing for precise temporal fruit monitoring in real and complex scenarios.","The challenge of meeting a growing demand for food requires advances in agricultural practices, with a focus on efficiency and sustainability. Autonomous robots offer new possibilities to automate labor-intensive tasks such as crop monitoring and management. Such systems have the potential to enhance agricultural production systems, and can enable continuous and large-scale monitoring [8, 38, 33]. In particular, these technologies support phenotyping, the process of evaluating plant characteristics by providing precise, high-throughput assessments and surpass the limitations of traditional manual methods [9, 40]. This shift towards automated phenotyping represents a critical step forward in optimizing crop selection and improving crop yield. Temporal matching, or fruit re-identification, on top of accurate instance segmentation, allows for tracking the development of single fruits over time, enabling the analysis of growth patterns and the estimation of maturation rates. Figure 1: Fruit re-identification on three point clouds acquired at three different points in time. Fruit instances are first segmented using an instance segmentation method, then they are temporally matched with fruit instances of a previous data collection (e.g., matching fruits in \mathcal{P}^{t} with fruits in \mathcal{P}^{t-1}). Green lines indicate matching between fruits, while fruits inscribed in a circle do not have a matching fruit in the previous data collection. In this paper, we aim to track fruits in a greenhouse using colored point clouds acquired by a robot equipped with a high-resolution LiDAR scanner. The goal is to identify, segment, and temporally match individual fruits at different points in time within the 3D space. 2D image-based approaches [30, 10, 13, 11, 19] lack the 3D structure, depth, and spatial information offered by point clouds. Conversely, point clouds lack a regular grid structure, making it difficult to apply traditional image processing techniques. The challenge is to process this sparse and irregular data to detect and associate individual fruits, which may vary in size, shape, orientation, and occlusion levels and may be harvested or newly grown. Once fruit instances have been segmented, the re-identification task involves recognizing and matching the same fruit instances across different point clouds captured at different points in time or from different viewpoints. Similar to loop closures in SLAM [35] and visual place recognition [37] systems, this task is usually performed assuming unique landmarks. In the context of fruit re-identification, there are no unique traits that make fruits easily distinguishable. As depicted in Fig. 1, fruits can be very similar, tightly packed, and their pose can change significantly over time, causing trivial solutions based on relative position to fail. The main contribution of this paper is a novel method for accurately performing fruit instance segmentation and re-identification on point clouds captured by an agricultural robot at different points in time, based on a learned descriptor encoder and an attentive matcher. We exploit dense high-precision point clouds recorded with a high precision Faro laser scanner. While these sensors are not commonly employed in robotic applications, their capability to scan detailed environments has recently garnered attention, leading to their integration into robotic systems [33]. We segment fruits using a learning-based instance segmentation, which infers fruit instances directly from the point cloud. Each fruit is then processed by a 3D sparse convolutional neural network to obtain a per instance descriptor. Then, we match each fruit with its corresponding instance from a previous data collection by using their descriptors and an attention-based matching neural network. To handle the possibility of a no-match scenario, where a query fruit is identified as a new instance, we represent it using a specific descriptor. We then predict a probability distribution over the candidate fruits from a previous data collection, including the no-match option. Each query fruit is subsequently matched with the previous instance that has the highest predicted probability, using a greedy assignment to determine associations. In sum, we make two key claims: (i) our approach is able to identify fruits in point clouds using an instance segmentation method; (ii) it outperforms baseline approaches on the re-identification task using real-world data collected in a greenhouse. These claims are backed up by the paper and our experimental evaluation. By harnessing 3D data and sparse convolution networks, our method significantly enhances the effectiveness and scalability of fruit segmentation and tracking, outperforming baseline approaches and offering new capabilities in automated crop monitoring. The implementation of our method is available at https://github.com/PRBonn/IRIS3D."
https://arxiv.org/html/2411.07711v1,OWLed: Outlier-weighed Layerwise Pruning for Efficient Autonomous Driving Framework,"The integration of Large Language Models (LLMs) into autonomous driving systems offers promising enhancements in environmental understanding and decision-making. However, the substantial computational demands of deploying LLMs locally on vehicles render this approach unfeasible for real-world automotive applications. To address this challenge, we introduce OWLed, the Outlier-weighed Layerwise Pruning for Efficient Autonomous Driving Framework that leverages outlier-weighted layerwise sparsity for model compression. Our method assigns non-uniform sparsity ratios to different layers based on the distribution of outlier features, significantly reducing the model size without the need for fine-tuning. To ensure the compressed model adapts well to autonomous driving tasks, we incorporate driving environment data into both the calibration and pruning processes. Our empirical studies reveal that the encoder component is more sensitive to pruning than the LLM, highlighting its critical role in the system. Experimental results demonstrate that OWLed outperforms existing methods in perception, action prediction, and language understanding while substantially lowering computational requirements. These findings underscore the potential of combining advanced pruning techniques with LLMs to develop efficient and robust autonomous driving systems capable of handling complex scenarios. Code will be made publicly available.","Autonomous driving technology has made significant progress in recent years, with an increasing number of autonomous vehicles being tested and deployed on public roads [1, 2]. There are generally two types of autonomous driving systems: one is a modular design with different sub-modules to complete various tasks such as perception, prediction, and planning [3, 4, 5, 6]. Such a decoupled design may lead to cumulative errors as the module cannot access the sensor data. The other is an end-to-end design that directly inputs sensor data into a series of neural networks to obtain control signals [7, 8, 9]. However, systems built with traditional neural networks often struggle to handle long-tail scenarios and complex urban environments, limiting their ability to process the diverse and unpredictable situations encountered in real-world driving [1, 2, 10]. Recently, large language models (LLMs) have exhibited capabilities approaching Artificial General Intelligence, including common sense understanding, reasoning, and knowledge retrieval [11, 12, 13]. LLMs’ remarkable capabilities make them a promising solution for addressing the aforementioned challenges in autonomous driving systems. This has sparked several attempts to integrate LLMs into autonomous vehicles to enhance understanding of the driving environment, explain perception results [14], generate textual driving decisions [15], and translate those decisions into executable commands [10]. These works primarily fall into two categories: The first category leverages pre-trained LLMs, such as ChatGPT, through API interfaces to process sensor data (i.e., camera and radar inputs) for environmental understanding and decision-making [14, 16, 17, 15, 18]. This approach can fully utilize state-of-the-art pre-trained LLMs to achieve optimal performance. However, its drawback lies in its dependence on network conditions and the data processing speed of servers, which limits its effectiveness in complex driving scenarios [10]. The second category involves training and deploying LLMs locally on autonomous vehicle systems. This method could address the autonomous driving system’s requirements for cloud computing resources and network communication quality [19, 20, 9, 21, 22, 23, 24]. However, as LLMs typically operate at the billion-parameter scale, this approach places substantial demands on local computational resources, thereby limiting their practical applications in autonomous vehicles. To address the computational challenges posed by deploying LLMs locally, numerous studies have explored various model compression techniques for applications with limited computational budget, including quantization [25], knowledge distillation [26] and network pruning [27]. Among them, network pruning has emerged as a promising approach to improve the efficiency of deep networks by reducing model size [28, 29, 30, 31, 32]. Specifically, pruning methods aim to reduce the model size by removing unnecessary weights and maintaining performance. However, its application to LLMs is limited because traditional pruning methods often require fine-tuning or retraining to obtain the optimal performance, which is prohibitively expensive for LLMs due to their massive size and requirements for sheer amount of computational resources [27, 33, 34, 35]. Recent advancements have explored pruning LLMs without fine-tuning. These studies have demonstrated that a substantial portion of the model’s parameters can be eliminated in a single step while incurring only minimal performance degradation. For example, sparseGPT [36] is a one-shot pruning method based on layer-wise reconstruction. It uses the Optimal Brain Surgeon (OBS) algorithm to determine which weights to prune. Wanda [37], on the other hand, is a simple yet effective pruning approach that considers both weight magnitudes and input activations. Wanda is computationally more efficient than SparseGPT as it doesn’t require the approximation of second-order information. Note that both SparseGPT and Wanda are designed to apply uniform layerwise sparsity when pruning LLMs. This means that each layer in the model is pruned to the same degree of sparsity, regardless of its position or function within the network. More recently, research has shown that non-uniform layerwise sparsity can potentially yield better performance [38]. Building on this insight, Yin et al. [38] proposed OWL as a non-uniform layerwise sparsity strategy based on the distribution of outlier features. OWL captures the impact of outlier features in LLMs and assigns sparsity ratios proportional to each layer’s outlier ratio. OWL has shown demonstrated superior performance compared to uniform pruning methods, particularly at high sparsity levels. Importantly, it can be combined with existing pruning techniques to further improve performance. While many attempts have been made to leverage LLMs in autonomous driving, the research of computationally efficient LLMs for end-to-end autonomous driving is still at the very preliminary stage. To address the computational challenges associated with local LLM deployment in autonomous vehicles, we propose the first effective LLM-assisted autonomous driving framework by utilizing outlier-weighted layerwise sparsity to compress LLMs, termed OWLed. To ensure effective adaptation of LLMs to the specific demands of autonomous driving, we incorporate downstream task data into both the calibration process and the optimization of the pruning procedure. This tailored approach allows for more precise and context-specific model compression. The main contributions of this paper are summarized as follows: ① We propose an effective LLM assisted autonomous driving framework that significantly enhances computational efficiency while maintaining the powerful reasoning capabilities of LLMs. By employing outlier-weighted layerwise sparsity, our approach effectively addresses the computational constraints inherent in deploying LLMs in vehicles. This framework is designed to process and reason over complex environmental data from multiple sensory modalities and natural language inputs, enabling more sophisticated and efficient decision-making in autonomous vehicles. ② We investigated the importance of the encoder and the LLM in the context of pruning for autonomous driving systems. Our empirical results show that the encoder plays a crucial role in these systems and exhibits higher sensitivity to pruning compared to the LLM component. ③ To further enhance the performance of OWLed, we introduce a novel calibration strategy tailored specifically for autonomous driving applications. While current pruning methods rely solely on generic text data for calibration, our approach leverages diverse data collected from real driving environments. This novel use of domain-specific calibration data allows us to better adapt the LLM to autonomous driving. ④ Compared to existing methods, our framework demonstrates superior perception, action prediction, and language understanding capabilities while significantly reducing computational requirements. We validate the effectiveness of our method through extensive experiments and ablation studies. The rest of this paper is structured as follows: Section II presents the related work for LLMs-based autonomous driving and pruning methods. The proposed method is detailed in Section III. Section IV presents the main experimental setup, results, and the ablation study. We conclude the paper in Section V."
https://arxiv.org/html/2411.07223v1,Grounding Video Models to Actions through Goal Conditioned Exploration,"Large video models, pretrained on massive amounts of Internet video, provide a rich source of physical knowledge about the dynamics and motions of objects and tasks. However, video models are not grounded in the embodiment of an agent, and do not describe how to actuate the world to reach the visual states depicted in a video. To tackle this problem, current methods use a separate vision-based inverse dynamic model trained on embodiment-specific data to map image states to actions. Gathering data to train such a model is often expensive and challenging, and this model is limited to visual settings similar to the ones in which data are available. In this paper, we investigate how to directly ground video models to continuous actions through self-exploration in the embodied environment – using generated video states as visual goals for exploration. We propose a framework that uses trajectory level action generation in combination with video guidance to enable an agent to solve complex tasks without any external supervision, e.g., rewards, action labels, or segmentation masks. We validate the proposed approach on 8 tasks in Libero, 6 tasks in MetaWorld, 4 tasks in Calvin, and 12 tasks in iThor Visual Navigation. We show how our approach is on par with or even surpasses multiple behavior cloning baselines trained on expert demonstrations while without requiring any action annotations.","1 Introduction Large video models (Brooks et al., 2024; Girdhar et al., 2023; Ho et al., 2022) trained on a massive amount of Internet video data capture rich information about the visual dynamics and semantics of the world for physical decision-making. Such models are able to provide information on how to accomplish tasks, allowing them to parameterize policies for solving many tasks (Du et al., 2024). They are further able to serve as visual simulators of the world, allowing simulation of the visual state after a sequence of actions (Brooks et al., 2024; Yang et al., 2024c), and enabling visual planning to solve long-horizon tasks (Du et al., 2023). However, directly applying video models zero-shot for physical decision-making is challenging due to embodiment grounding. While generated videos provide a rich set of visual goals for solving new tasks, they do not explicitly provide actionable information on how to reach each visual goal. To ground video models to actions, existing work has relied on training an inverse dynamics model or goal-conditioned policy on a set of demonstrations from the environment (Black et al., 2023; Du et al., 2024; 2023). Such an approach first requires demonstrations to be gathered in the target environment and embodiment of interest, which demands human labor or specific engineering (e.g. teleoperation or scripted policy). In addition, the learned policies may not generalize well to areas in an environment that are out-of-distribution of the training data. Recently, Ko et al. (2023) proposes an approach to directly regress actions from video models, without requiring any action annotations. In Ko et al. (2023), optical flow between synthesized video frames is computed and used, in combination with a depth map of the first image, to compute a rigid transform of objects in the environment. Robot actions are then inferred by solving for actions that can apply the specified rigid transform on an object. While such an approach is effective on a set of evaluated environments, it is limited in action resolution as the inferred object transforms can be imprecise due to both inaccurate optical flow and depth, leading to a relatively low success rate in evaluated environments (Ko et al., 2023). In addition, it is difficult to apply this approach to many robotic manipulation settings such as deformable object manipulation, where there are no explicit object transforms to compute. Figure 1: Grounding Video Models to Actions. Our approach learns to ground a large pretrained video model into continuous actions through goal-directed exploration in the environment. Given a synthesized video, a goal-conditioned policy attempts to reach each visual goal in the video, with data in the resulting real-world execution saved in a replay buffer to train the goal-conditioned policy. We propose an alternative manner to directly ground a video model to actions without using annotated demonstrations. In our approach, we learn a goal-conditioned policy, which predicts the actions to reach each synthesized frame in a video. We learn the policy in an online manner, where given a specified task, we use each intermediate synthesized frame as a visual goal for a goal-conditioned policy from which we obtain a sequence of actions to execute in an environment. We then use the image observations obtained from execution in the environment as ground-truth data to train our goal-conditioned policy. We illustrate our approach in Figure 1. In practice, directly using synthesized images as goals for exploration often leads to insufficient exploration. Agents often get stuck in particular parts of an environment, preventing the construction of a robust goal-conditioned policy. To further improve exploration, we propose to generate chunks of actions to execute in an environment given a single visual state. By synthesizing and executing a chunk of actions we can explore the environment in a more directed manner, enabling us to achieve a more diverse set of states. We further intermix goal-conditioned exploration with random exploration to further improve exploration. Overall, our approach has three contributions: (1) We propose goal-conditioned exploration as an approach to ground video models to continuous actions. (2) We propose a set of methods for effective exploration in the environment, using a combination of chunked action prediction for temporally coherent exploration along with periodic randomized actions for robust state coverage. (3) We illustrate the efficacy of our approach on a set of simulated manipulation and navigation environments."
https://arxiv.org/html/2411.07183v1,Probabilistic approach to feedback control enhances multi-legged locomotion on rugged landscapes,"Achieving robust legged locomotion on complex terrains poses challenges due to the high uncertainty in robot-environment interactions. Recent advances in bipedal and quadrupedal robots demonstrate good mobility on rugged terrains but rely heavily on sensors for stability due to low static stability from a high center of mass and a narrow base of support[1]. We hypothesize that a multi-legged robotic system can leverage morphological redundancy from additional legs to minimize sensing requirements when traversing challenging terrains. Studies suggest [2, 3] that a multi-legged system with sufficient legs can reliably navigate noisy landscapes without sensing and control, albeit at a low speed of up to 0.1 body lengths per cycle (BLC). However, the control framework to enhance speed on challenging terrains remains underexplored due to the complex environmental interactions, making it difficult to identify the key parameters to control in these high-degree-of-freedom systems. Here, we present a bio-inspired vertical body undulation wave as a novel approach to mitigate environmental disturbances affecting robot speed, supported by experiments and probabilistic models. Finally, we introduce a control framework which monitors foot-ground contact patterns on rugose landscapes using binary foot-ground contact sensors to estimate terrain rugosity. The controller adjusts the vertical body wave based on the deviation of the limb’s averaged actual-to-ideal foot-ground contact ratio, achieving a significant enhancement of up to 0.235 BLC on rugose laboratory terrain. We observed a \sim 50% increase in speed and a \sim 40% reduction in speed variance compared to the open-loop controller. Additionally, the controller operates in complex terrains outside the lab, including pine straw, robot-sized rocks, mud, and leaves.","Legged robots offer a compelling alternative to wheeled robots, particularly when navigating unstructured terrain. Recent advances in few-legged robots demonstrate their adaptability to diverse and unstructured terrains. Bipedal robots excel at obstacle avoidance and stair climbing [4, 5, 6], but their static instability necessitates substantial effort to maintain balance in an upright posture. Disruptions in body or leg trajectories can lead to instability [7, 8], limiting mobility on highly uneven terrains. Quadruped robots, known for greater stability, perform well on challenging terrains like snow, wet moss, mud, and rocky surfaces [9, 10, 11]. However, they rely on advanced control algorithms and complex sensing systems, including force sensors and accelerometers, to analyze foot-environment interactions and maintain balance [12, 13, 14, 15]. Additionally, they utilize cameras and LiDAR for estimating terrain geometric features for foothold planning [16, 17, 18, 19, 20]. This adds complexity to the sensing system and limits the robot’s scalability in confined spaces. In contrast, hexapod robots, with two additional legs, exhibit more robust mobility on rugged terrain[21, 22]. Multi-legged robots with more than six legs exhibit excellent static stability due to a low center of mass and broad base of support [2, 3, 23], minimizing the effort required to maintain balance. This stability gives the system significant potential for traversing rugged environments, allowing for a much simpler sensory system compared to fewer-legged robots. Recent studies [2, 3] indicate that a serially connected multi-legged robotic system with high static stability and morphological redundancy can reliably traverse noisy landscapes without requiring sensing and control. These systems successfully transport between two points on rough terrain without feedback control. However, their speed on such terrain with an open-loop controller is relatively low and depends on multiple legs to maintain it. To address these limitations and improve the effectiveness of multi-legged locomotion in complex environments, designing a feedback control framework is essential. Specifically, designing an simple feedback controller to enhance the robot’s performance with minimal sensing is a challenging yet fascinating problem. To the best of our knowledge, feedback control for such multi-legged robotic systems has been minimally exploration [24, 25], largely due to their high degrees of freedom (over 25), complex environmental interactions, and unclear key parameters requiring control. Figure 1: Multi-legged robots navigating rugged landscapes. To demonstrate the effectiveness of the proposed probabilistic model and feedback controller, we tested the robot in laboratory-based rough terrains (shown in the bottom right corner) and outdoor environments. Details of the lab-based terrain construction can be found in Section IV-A. The outdoor tests were conducted on rugged landscapes with a mixture of random tree debris, grass, boulders, mud, leaves, and rocks. Empirical evidence [2] suggests that multi-legged robots can benefit from vertical body motion modulation when navigating rugged terrains with uneven surfaces and various obstacles. However, a notable gap exists in the literature regarding a systematic investigation of how vertical body undulation affects a robot’s motion on rough terrain. Furthermore, the integration of vertical body undulation into a feedback control framework remains underexplored. This paper systematically investigates the benefits of vertical body undulation in enhancing the speed of multi-legged robots using stochastic analysis, a widely adopted method for addressing uncertainties in robot-environment interactions[26, 27, 28, 29, 30, 31, 32, 33]. Specifically, we develop two probabilistic models to analyze how the amplitude of vertical body waves affects the robot’s speed on rough terrain and integrate this modulation into a feedback controller to improve robot’s performance on rugged landscapes. Figure 2: Robophysical model description. A. Design of a single module of the robophysical model, includes four active degrees of freedom: leg up and down, leg swing, and body lateral and vertical rotation. B. The compliant leg feature is achieved by a connected rubber band in 1, enabling the robot to overcome obstacles like those in 2. C. (1) Overhead view of the robotic system. Snapshots of the forward gait illustrate the coordination between horizontal body undulation and leg stepping. (2) Side view of the robotic system. Snapshots depicts variations in the vertical body undulation wave with different amplitudes (A_{v}). To summarize our contribution, we first introduce a robotic system capable of generating coordinated body undulation and leg-stepping waves to enable forward motion. Next, we develop a binary contact sensing system to detect foot-ground contact. We then propose two probabilistic models to quantify the benefits of vertical wave modulation for multi-legged locomotion on rugged terrains. The first model predicts the robot’s forward speed based on the actual-to-ideal foot-ground contact ratio, while the second model estimates this contact ratio given the terrain distribution and vertical wave amplitude. By combining these models, we demonstrate that: 1) a higher contact ratio generally leads to increased speeds, and 2) appropriately adjusting the vertical wave amplitude can effectively achieve this higher contact ratio. Utilizing these models, we design a feedback controller that allows the robot to optimize its vertical amplitude based on terrain roughness, estimated in real-time through contact ratio measurements. Finally, we validate the controller’s effectiveness through laboratory and outdoor experiments."
https://arxiv.org/html/2411.07146v1,Lost in Tracking Translation: A Comprehensive Analysis of Visual SLAM in Human-Centered XR and IoT Ecosystems,"Advancements in tracking algorithms have empowered nascent applications across various domains, from steering autonomous vehicles to guiding robots to enhancing augmented reality experiences for users. However, these algorithms are application-specific and do not work across applications with different types of motion; even a tracking algorithm designed for a given application does not work in scenarios deviating from highly standard conditions. For example, a tracking algorithm designed for robot navigation inside a building will not work for tracking the same robot in an outdoor environment. To demonstrate this problem, we evaluate the performance of the state-of-the-art tracking methods across various applications and scenarios. To inform our analysis, we first categorize algorithmic, environmental, and locomotion-related challenges faced by tracking algorithms. We quantitatively evaluate the performance using multiple tracking algorithms and representative datasets for a wide range of Internet of Things (IoT) and Extended Reality (XR) applications, including autonomous vehicles, drones, and humans. Our analysis shows that no tracking algorithm works across different applications and scenarios within applications. Ultimately, using the insights generated from our analysis, we discuss multiple approaches to improving the tracking performance using input data characterization, leveraging intermediate information, and output evaluation.","Tracking systems are fundamental to immersive Extended Reality (XR) applications, facilitating accurate and real-time navigation and mapping that are crucial for creating immersive and interactive experiences (Billinghurst et al., 2015; Klein and Murray, 2007). However, various challenges must be addressed for accurate tracking, particularly in human-centered scenarios like XR (Cadena et al., 2016; Nwankwo and Rueckert, 2023). Tracking systems in XR face additional challenges due to human factors such as unpredictable movements, inter-individual variability, contextual factors, cognitive load, occlusions from body parts, physical safety concerns, adaptive requirements, and the need for real-time interaction. These elements introduce layers of unpredictability and complexity, further complicating the tracking process. These challenges are intertwined, involving (1) the environment’s complexity, (2) various locomotion demands, and (3) the inherent limitations of sensing and tracking systems. As a result, while tracking methods are often presented as generic, their performance significantly varies across different environments, locomotion scenarios, and application settings, such as drones (Burri et al., 2016), autonomous vehicles (Geiger et al., 2013), robotics (Zhang et al., 2022), and other human-centered (Ngo et al., 2022; Vávra et al., 2017; Fu et al., 2021) and non-human-centered environments (Aqel et al., 2016; Milz et al., 2018). To understand these challenges, it is essential to examine the specific factors contributing to the complexity of broader tracking systems and how human factors add to their complexity. First, the complexity of the environment can vary with the number of objects, lighting conditions, occlusions, weather, reflective surfaces, and scene changes. For instance, tracking in a crowded urban setting with changing lighting and reflective surfaces is particularly difficult. In XR applications, this complexity is heightened by the unpredictability of human interactions and the dynamic nature of the environment. Additionally, humans can seamlessly transition between different environments, such as walking from a room to a corridor to the outdoors, without a break. This continuous movement across varied settings introduces additional challenges for tracking systems, as they must constantly adapt to new conditions and maintain accuracy. Second, locomotion differs across applications. Vehicles, robots, and humans move differently, each posing unique challenges. In human-centric applications, such as XR, abrupt movements can cause blurred images. Even in-vehicle navigation typically involves fewer abrupt movements, maintaining tracking accuracy is difficult due to varying speeds and accelerations in parking lots, urban areas, and highways (Milz et al., 2018). Tracking human movement adds another layer of complexity, as humans frequently stop, walk, and change speeds unpredictably, making consistent speed maintenance challenging. Third, sensors such as IMU sensors, depth cameras, and RGB cameras each have specific issues (Chong et al., 2015). IMU sensors can drift over time (Anwar et al., 2019; Trippel et al., 2017), depth cameras struggle with lighting and reflective surfaces (Halmetschlager-Funek et al., 2018), and RGB cameras are affected by lighting variations. In XR applications, these sensor limitations are compounded by the need to integrate data from multiple sources in real-time (Ungureanu et al., 2020). To overcome these challenges, prior work has developed tracking algorithms that leverage various computational approaches. For example, traditional SLAM methods heavily depend on carefully engineered features and manually designed system components (Campos et al., 2021; Zubizarreta et al., 2020; Schops et al., 2019; Qin et al., 2018). These methods often lack robustness, meaning they struggle to maintain accuracy and reliability in dynamic and diverse real-world scenarios where conditions can vary significantly. Factors such as changing lighting conditions, moving objects, and varying environmental textures can degrade their performance. Conversely, end-to-end learning approaches (Zhou et al., 2018; Bloesch et al., 2018; Wang et al., 2020) learn system components directly from data, which can lead to improved adaptability. However, these approaches can also face robustness issues, as they may fail to generalize when encountering unfamiliar situations or environments not represented in their training data. Hybrid approaches (Tang et al., 2020; Krishna Murthy et al., 2020; Zhou et al., 2017) aim to enhance overall performance by combining traditional and learning-based methods, leveraging both strengths. While this improves the average case performance, it often sacrifices the best-case performance the individual approaches might achieve. To comprehensively address these challenges, it is important not only to evaluate tracking systems within XR environments but also to compare their performance against other application domains, such as autonomous vehicles and drones. Evaluating tracking methods across these varied domains provides a broader perspective on the strengths and weaknesses of different approaches. Autonomous vehicles and drones present unique challenges, such as high-speed movement and indoor-outdoor environmental variability, which can inform improvements in XR tracking systems. By understanding how these systems perform in different contexts, we can derive insights that contribute to developing more robust and versatile tracking solutions that can be applied across multiple domains, including but not limited to XR. Additionally, it is crucial to examine how these algorithms behaved in their original use cases (Valente et al., 2019; Zhang et al., 2021a; Milz et al., 2018; Wang et al., 2022; Chen et al., 2020) before XR became prominent. Understanding their foundational performance and limitations in traditional applications will provide a deeper insight into their adaptability and potential enhancements needed for XR environments. This paper aims to address these challenges by systematically understanding the challenges, technical requirements bottlenecks, and potential solution directions needed to enhance tracking performance in XR and beyond. In doing so, we make the following contributions: (1) Taxonomy of challenges. We categorize the algorithmic, environmental, and locomotion-related challenges tracking systems face and their impact on XR applications. This taxonomy provides a structured overview of the difficulties inherent in visual SLAM tracking by highlighting the specific issues that need to be addressed to improve tracking performance in various human-in-loop and other Internet of Things (IoT) systems. (2) Charting tracking performance. We quantitatively evaluate the performance of state-of-the-art tracking algorithms across three distinct datasets, each representing a different application domain, environment, motion, and tracking target with unique complexities, including representative IoT systems like autonomous vehicles and drones and human-in-the-loop systems such as XR. (3) Dataset characterization. Building on observations from our quantitative evaluation across traditional, end-to-end learning-based, and hybrid tracking systems, we conduct a preliminary proof of concept data characterization. This analysis highlights the importance of understanding how dataset properties impact tracking performance and identifies potential adaptive solutions for specific environments and use cases. Unlike existing surveys that focus on specific applications or isolated aspects of tracking systems, our comprehensive evaluation empirically examines a broader range of scenarios and system types. This approach systematically presents challenges and performance bottlenecks across diverse contexts, providing a robust foundation for developing adaptable and reliable tracking solutions. These insights are especially valuable for XR applications, where tracking systems must adapt to the unpredictability of human behavior. By addressing current challenges and conducting proof-of-concept case studies, this paper serves as both a reference point for researchers and a springboard for future innovations in Visual SLAM tracking in XR and beyond."
https://arxiv.org/html/2411.07104v2,Learning Multi-Agent Loco-Manipulation for Long-Horizon Quadrupedal Pushing,"Recently, quadrupedal locomotion has achieved significant success, but their manipulation capabilities, particularly in handling large objects, remain limited, restricting their usefulness in demanding real-world applications such as search and rescue, construction, industrial automation, and room organization. This paper tackles the task of obstacle-aware, long-horizon pushing by multiple quadrupedal robots. We propose a hierarchical multi-agent reinforcement learning framework with three levels of control. The high-level controller integrates an RRT planner and a centralized adaptive policy to generate subgoals, while the mid-level controller uses a decentralized goal-conditioned policy to guide the robots toward these sub-goals. A pre-trained low-level locomotion policy executes the movement commands. We evaluate our method against several baselines in simulation, demonstrating significant improvements over baseline approaches, with 36.0\% higher success rates and 24.5\% reduction in completion time than the best baseline. Our framework successfully enables long-horizon, obstacle-aware manipulation tasks like Push-Cuboid and Push-T on Go1 robots in the real world.","Recent advances in quadrupedal robots have significantly improved their ability to traverse challenging terrains [1, 2, 3, 4, 5, 6]. While many studies have focused on enhancing their mobility and stability of locomotion, the manipulation capabilities of these robots remain relatively limited. Efforts have been made to improve the quadrupedal capabilities in prehensile manipulation through attaching grippers or robotic arms on the robot [7, 8, 9, 10, 11, 12], and non-prehensile manipulation by using legs [13, 14, 15, 16] or the head [17, 18] as the end-effectors. Although these advancements enable quadrupeds to handle some routine tasks, their limited ability to manipulate large and heavy objects still restricts their usefulness in demanding fields like search and rescue, construction, industrial automation, and room organization, where both dexterity and strength are essential. To address these challenges, researchers have explored adding support structures to the robots [19, 20], coordinating whole-body movements [21], and using multiple robots [22, 23] to strengthen contact forces and expand operational dimensions. However, achieving long-horizon manipulation of large objects in cluttered environments remains a largely unexplored and challenging task for quadrupeds. In this work, we focus on addressing the challenge of obstacle-aware, long-horizon pushing by coordinating the whole-body motions of multiple quadrupedal robots. We build our work upon recent works of quadrupedal pushing that demonstrate impressive results. As shown in Table I, while many approaches utilize multiple robots to enhance manipulation abilities, few focus on long-horizon pushing and obstacle avoidance, both of which are critical for real-world tasks. Additionally, the limited use of whole-body motions (e.g., relying solely on heads to push) [18, 22, 23] restricts the contact patterns between robots and objects, making it difficult for the robots to perform diverse movements and avoid collisions with obstacles. TABLE I: Comparisons between our proposed method and previous methods of quadrupedal pushing. Method Collaborative Long- Horizon Whole- Body Obstacle- Avoidance Sombolestan et al. [18] ✗ ✗ ✗ ✗ Jeon et al. [21] ✗ ✗ ✓ ✗ Sombolestan et al. [22] ✓ ✗ ✗ ✗ Nachum et al. [23] ✓ ✓ ✗ ✓ An et al. [24] ✓ ✗ ✓ ✗ Xiong et al. [25] ✓ ✗ ✓ ✗ Ours ✓ ✓ ✓ ✓ To achieve collaborative, obstacle-aware, long-horizon quadrupedal pushing through whole-body motions, we propose a hierarchical multi-agent reinforcement learning (MARL) framework with three levels of controllers. The high-level controller integrates an Rapidly-exploring Random Tree (RRT) planner [26] and a centralized adaptive policy, which processes the reference trajectory, environment, and agent information to generate subgoals for the object. The mid-level controller learns a shared decentralized goal-conditioned policy, enabling multiple robots to coordinate and push the object toward the sequential subgoals proposed by the high-level controller. The low-level controller is a pre-trained locomotion policy that executes commands from the mid-level controller. We validate our approach through a series of experiments in both simulation and real-world tests on Go1 robots, a few of which are visualized in Figure 1. Our results demonstrate that the proposed method achieves a 36.0\% higher success rate and a 24.5\% reduction in completion time compared to the best baseline approach in simulation. Furthermore, our method can be deployed on real robots to successfully complete obstacle-aware, long-horizon Push-Cuboid and Push-T tasks. The main contributions of this paper can be summarized as follows. • We propose a hierarchical MARL framework with three hierarchies that can handle long-horizon collaborative quadrupedal pushing in an environments with obstacles. • We benchmark our proposed method against baselines on various long-horizon pushing tasks involving obstacles in IsaacGym [27], demonstrating that our method significantly outperforms the baselines. • We deploy our trained hierarchical policy on real robots, successfully completing the collaborative long-horizon Push-Cuboid and Push-T tasks with coordinated whole-body motions. Figure 2: Overview of the proposed hierarchical MARL framework for collaborative long-horizon pushing tasks by quadrupedal robots. The framework comprises three layers: a high-level controller, a mid-level controller, and a low-level controller. The high-level controller utilizes an RRT planner to generate a trajectory and an adaptive policy to assign subgoals based on the dynamic states of the environment, object, and robots. The mid-level controller employs decentralized pushing policies to convert a common subgoal into agent-specific velocity commands, which are then executed by the low-level locomotion policy on each robot. Each layer is trained independently, leveraging frozen lower-level policies."
https://arxiv.org/html/2411.07079v1,Robust Nonprehensile Object Transportation with Uncertain Inertial Parameters,"We consider the nonprehensile object transportation task known as the waiter’s problem—in which a robot must move an object balanced on a tray from one location to another—when the balanced object has uncertain inertial parameters. In contrast to existing approaches that completely ignore uncertainty in the inertia matrix or which only consider small parameter errors, we are interested in pushing the limits of the amount of inertial parameter uncertainty that can be handled. We first show how balancing constraints robust to inertial parameter uncertainty can be incorporated into a motion planning framework to balance objects while moving quickly. Next, we develop necessary conditions for the inertial parameters to be realizable on a bounding shape based on moment relaxations, allowing us to verify whether a trajectory will violate the balancing constraints for any realizable inertial parameters. Finally, we demonstrate our approach on a mobile manipulator in simulations and real hardware experiments: our proposed robust constraints consistently balance a 56 cm tall object with substantial inertial parameter uncertainty in the real world, while the baseline approaches drop the object while transporting it.","The waiter’s problem [1] is a nonprehensile manipulation task that requires a robot to transport objects from one location to another while keeping them balanced on a tray at the end effector (EE), like a restaurant waiter. This manipulation task is called nonprehensile [2] because the objects are not rigidly grasped: they are only attached to the robot by frictional contact and thus retain some independent degrees of freedom. Other examples of nonprehensile manipulation include pushing, rolling, and throwing [3, 4]. A nonprehensile approach avoids grasping and ungrasping phases and can handle delicate or unwieldy objects which cannot be adequately grasped at all [5]; such an approach for transporting objects is useful in industries including food service, warehouse fulfillment, and manufacturing. We build on our previous work on the waiter’s problem for mobile manipulators [6]. In contrast to [6], which focused on fast online replanning to react to dynamic obstacles while balancing objects with known properties, here we focus on offline planning for a balanced object with unknown inertial parameters—that is, the values of the mass, center of mass (CoM), and inertia matrix are not known exactly but rather lie in some set. Our approach is to plan trajectories to reach a desired EE position while satisfying constraints that ensure the balanced object does not move with respect to the tray (see Fig. 1). These balancing constraints depend on the geometric, frictional, and inertial properties of the object. The geometry of the object can in principle be estimated visually (e.g., using a camera), while frictional uncertainty can be reduced by using a high-friction material for the tray surface or by using a low friction coefficient in the planner [6]. However, the inertial properties can only be identified by moving the object around (see e.g. [7, 8]), which is time-consuming and could result in the object being dropped and damaged. Instead, we propose using robust constraints that successfully balance the object despite the presence of substantial inertial parameter uncertainty. Notably, we assume the CoM can be located at any height within the object, and that the inertia matrix can take any physically realizable value. Figure 1: The goal of this work is to move an object balanced on a tray to a desired position without dropping it, despite the inertial parameters of the object being uncertain. Here our mobile manipulator is balancing a tall box with uncertain contents. A video of our experiments is available at http://tiny.cc/upright-robust. We focus on balancing a single object which can slide and is tall enough to tip over. We assume that the geometry of the object is known but the mass and inertia matrix are unknown, and that the CoM lies within a known polyhedral convex set. We use the object’s known geometry to constrain the set of possible inertial parameters. A set of inertial parameters can only be physically realized on a given shape if there exists a corresponding mass density function which is zero everywhere outside that shape [8]. We develop necessary conditions for the inertial parameters to be physically realizable on a bounding shape based on moment relaxations [9]. These realizability conditions allow us to verify that a planned trajectory does not violate the balancing constraints for any physically realizable value of the inertial parameters, providing theoretical guarantees for the robustness of our planned trajectories. In summary, the contributions of this work are: • a planner for nonprehensile object transportation that explicitly handles objects with uncertain CoMs, extending the framework from [6]; • a theoretical analysis of the balancing constraint satisfaction in the presence of a bounded CoM and any physically realizable inertia matrix, based on moment relaxations [9]; • simulations and hardware experiments demonstrating that our proposed robust constraints successfully balance the object—despite using tall objects with high inertial parameter uncertainty—while baseline approaches drop the object; • an open-source implementation of our planner, available at https://github.com/utiasDSL/upright."
https://arxiv.org/html/2411.07056v1,Distributed Spatial Awareness for Robot Swarms,"Building a distributed spatial awareness within a swarm of locally sensing and communicating robots enables new swarm algorithms. We use local observations by robots of each other and Gaussian Belief Propagation message passing combined with continuous swarm movement to build a global and distributed swarm-centric frame of reference. With low bandwidth and computation requirements, this shared reference frame allows new swarm algorithms. We characterise the system in simulation and demonstrate two example algorithms.","Swarm robotics, inspired by swarms in nature, has the potential for resilient, robust, and redundant solutions to a wide range of problems such as mapping, logistics, search and rescue, disaster recovery, and environmental monitoring. Many relatively simple and cheap robots, each following simple rules, with local interactions between themselves and the environment are capable of producing a desired emergent swarm-level behaviour [1, 2, 3, 4, 5, 6]. There are many areas of swarm algorithm design where access to global information would be useful, but unless that information is inferred or constructed through purely local interactions, it does not fit within the distributed swarm paradigm. One example is spatial awareness, by which we mean that an agent within the swarm is aware of its own location with respect to the swarm as a whole; the swarm shares a spatial reference frame. The availability of a completely distributed, completely local, low cost, shared reference frame would open up algorithmic approaches previously not possible. By using Gaussian Belief Propagation, we can construct this within a swarm of robots based only on local observation and messaging. Robots move around, constructing a distributed, size-limited factor graph of observations of other robots and odometry information. Message passing within and between neighbouring robots results in convergence on a shared frame of reference; each robot knows where it is in relation to it. To demonstrate the potential of this, we focus on two applications, shape formation and logistics Swarm shape formation is a proxy for a number of real-world problems such as search and rescue or emergency communication. Many problems rely on the swarm maintaining a coherent shape or coverage of particular areas. This has been tackled in swarm robotics in a number of ways which in their essence involve the construction of a frame of reference, or coordinate system. These systems often rely on robots transitioning to a static state to serve as anchors for further extensions to the shape and coordinate system, or unrealistic assumptions of position knowledge. The use of swarms for intralogistics is an emerging area, where perhaps we can move beyond the lab into real-world applications. With our DOTS [7] robots we aim to demonstrate a simple but functional application. Specifically, we consider the potential for small scale out-of-the-box solutions for everyday environments; simply delimit an area of floor and add robots and small cargo carriers. Users would download an app to their phone and use this to call for a carrier to deposit an item. Via Bluetooth, any robot within range would respond and provide the carrier and take it and the item to be stored. To retrieve the item, the user would use the app again, robots would talk with their neighbours until a robot with recent knowledge of the item heard, which would pick up the carrier and take it to the user. Even simple random walk algorithms are capable of effective retrieval in logistics applications. We describe the implementation of a system, which we call Distributed Spatial Awareness (DSA), to provide a completely local and distributed shared frame of reference. We characterise its performance in simulation, examine the trade-offs with computational and communication cost, and to demonstrate it, we show simple but effective shape formation, and enhanced knowledge awareness within an intralogistics application. This paper is organised as follows, in the next section, we look at the background, Section 3 covers methods, Section 4 analyses and discusses the results, and Section 5 concludes."
https://arxiv.org/html/2411.07032v1,Scaling Long-Horizon Online POMDP Planning via Rapid State Space Sampling,"Partially Observable Markov Decision Processes (pomdps) are a general and principled framework for motion planning under uncertainty. Despite tremendous improvement in the scalability of pomdp solvers, long-horizon pomdps (e.g., \geq 15 steps) remain difficult to solve. This paper proposes a new approximate online pomdp solver, called Reference-Based Online pomdp Planning via Rapid State Space Sampling (rop-r a s 3 ). rop-r a s 3 uses novel extremely fast sampling-based motion planning techniques to sample the state space and generate a diverse set of macro actions online which are then used to bias belief-space sampling and infer high-quality policies without requiring exhaustive enumeration of the action space—a fundamental constraint for modern online pomdp solvers. rop-r a s 3 is evaluated on various long-horizon pomdps, including on a problem with a planning horizon of more than 100 steps and a problem with a 15-dimensional state space that requires more than 20 look ahead steps. In all of these problems, rop-r a s 3 substantially outperforms other state-of-the-art methods by up to multiple folds.","Motion planning in the partially observed and non-deterministic world is a critical component of reliable and robust robot operation. Partially Observable Markov Decision Processes (pomdps) [7, 22] are a natural way to formulate such problems. The key insight of the pomdp framework is to represent uncertainty on the effects of actions, perception and initial state as probability distributions, and then reason about the best strategy to perform with respect to distributions over the problem’s state space, called beliefs, rather than the state space itself. Although a pomdp’s systematic reasoning about uncertainty comes at the cost of high computational complexity [18], the pomdp framework is practical for many robotics problems [11], thanks in large part to sampling-based approaches. These approaches relax the problem of finding an optimal solution to an approximate one by sampling states from the belief space and computing the best strategies from only the samples. Scalable anytime methods under this approach (surveyed in [11]) have been proposed for solving large pomdp problems. However, computing a good solution to long-horizon (e.g., \geq 15 steps) pomdps remains difficult. Early results from the literature [12] indicate that Sampling-Based Motion Planning (sbmp)—sampling-based approaches designed for deterministic motion planning—help alleviate the challenges of long-horizon problems in offline pomdp planning. Specifically, sbmps can be used to generate suitable macro actions (i.e., sequences of actions) to reduce the effective planning horizon for a pomdp solver. Macro actions generated via sbmp automatically adapt to geometric features of the valid region of the state space and tend to cover a diverse set of macro actions. Although the above approach performs well for offline pomdp planning, it is often impractical for online planning for two reasons. First, the speed of sbmp, which historically required hundreds of milliseconds to tens of seconds to find a single motion plan. Second, most online pomdp planners [13, 21, 27] exhaustively enumerate each action at each sampled belief in computing the best action to perform. When macro actions are used, exhaustive enumeration is performed over all macro actions; thus, the number of macro actions should be kept low, thereby limiting macro action diversity in online pomdp planning. However, the recently proposed Vector-Accelerated Motion Planning (vamp) framework [25] enables sbmps to find solutions on microsecond timescales, multiple orders of magnitude faster than prior approaches. Concurrently, recently proposed reference-based pomdp planners [9] remove the requirement for exhaustive enumeration of actions at a small cost to full optimality. Leveraging these two advances, we propose an online pomdp solver, called Reference-Based Online pomdp Planning via Rapid State Space Sampling (rop-r a s 3 ), which is a reference-based pomdp planner that employs a vamp-enhanced macro action sampler as its underlying reference policy. We evaluate rop-r a s 3 on multiple long-horizon pomdps, including a problem that requires over 100 lookahead steps and a 15-dimensional problem with a planning horizon of over 20. Comparisons with state-of-the-art online pomdp planners—including pomcp [21], a pomcp modification that uses vamp to generate macro actions, and despot [27] with learned macro actions [15, 16]—indicate rop-r a s 3 substantially outperforms state-of-the-art methods in all evaluation scenarios."
https://arxiv.org/html/2411.07003v1,Enhancing Robot Assistive Behaviour with Reinforcement Learning and Theory of Mind,"The adaptation to users’ preferences and the ability to infer and interpret humans’ beliefs and intents, which is known as the Theory of Mind (ToM), are two crucial aspects for achieving effective human-robot collaboration. Despite its importance, very few studies have investigated the impact of adaptive robots with ToM abilities. In this work, we present an exploratory comparative study to investigate how social robots equipped with ToM abilities impact user’s performance and perception. We design a two-layer architecture. The Q-learning agent on the first layer learns the robot’s higher-level behaviour. On the second layer, a heuristic-based ToM infers the user’s intended strategy and is responsible for implementing the robot’s assistance, as well as providing the motivation behind its choice. We conducted a user study in a real-world setting, involving 56 participants who interacted with either an adaptive robot capable of ToM, or with a robot lacking such abilities. Our findings suggest that participants in the ToM condition performed better, accepted the robot’s assistance more often, and perceived its ability to adapt, predict and recognise their intents to a higher degree. Our preliminary insights could inform future research and pave the way for designing more complex computation architectures for adaptive behaviour with ToM capabilities.","Cognitive stimulation is crucial for maintaining and improving abilities such as memory, attention, and executive function [59]. Regular engagement in activities that challenge and stimulate the brain has been shown to positively impact cognitive health and can delay the onset of age-related declines [23]. The use of socially assistive robots (SARs) in memory exercises [1, 36] has the potential to provide a personalised and engaging platform for delivering cognitive stimulation, offering a unique and interactive experience for users [29]. Indeed, robots have been proven to be very effective in performing simple, and repetitive tasks, making them a perfect tool to support healthcare professionals and enhance their effectiveness during their daily working routine [52]. Nonetheless, for robots to be most effective in providing assistance, they must cater to the specific needs of users and aim to prevent negative emotions such as frustration (due to overly difficult exercises) or boredom (due to overly simple exercises) during cognitive exercises. It is important that the robot’s approach aligns precisely with the user’s requirements to achieve optimal results. In recent years, several studies have demonstrated the impact of robot adaptivity on users’ performance as well as on their engagement in assistive tasks [7, 53, 21]. However, adaptation in the robot’s behaviour may result in affecting people’s ability to understand and predict the robot, impacting their trust. To avoid this, it has to be noted that humans are more likely to cooperate with machines with whom they can share a mental representation. Hence, the robot should be able to have a Theory of Mind (ToM) of the users over time and personalise its behaviour according to the inferred beliefs and intentions [45, 48, 49]. Despite those works, very little is known about how to effectively combine adaptivity with ToM to improve task performance and increase the user’s perceived competence of the robot [11]. Therefore, in this work, we aim to fill this research gap. We build upon our previous knowledge, wherein we demonstrated how a robot can tailor its degrees of assistance to patients affected by cognitive decline [5]. Here, we go a step further by endowing the robot with the capability to understand users’ strategies through a process of mentalising and, therefore, use that information to provide qualitatively better assistance. This study takes an exploratory approach to address the following research question through a comparative study: would a robot endowed with adaptive socially assistive behaviour and ToM abilities have a different impact on users’ performance and perceived robot capabilities compared to an adaptive robot without ToM abilities? Figure 1: A user playing the memory game with the assistance of the Furhat robot. To tackle this research question, we propose a computational approach in which a robot learns the socially assistive behaviour that best fits the users while providing advice that relies on the user’s beliefs and their intended strategies to solve a memory game (see Figure 1). Specifically, we create a two-layer architecture. The upper level, namely the learning layer, includes a Q-learning-based agent trained in simulation to model an imperfect player and learn the policy that best fits the user’s needs (e.g., suggest the card). The lower level, namely the mentalising layer, consists of a heuristic-based ToM that based on the previous history of users’ moves is used to estimate their strategies and their beliefs about the card’s selection. Here, ToM has employed both for operationalising the assistance provided by the RL (e.g., suggest shark) and explain the rationale behind its hint (“You have seen the shark several times, the other card is in row 1 col 2, you should remember the location”). To evaluate our system, we carried out a user study in a real-world setting. N=56 untrained participants played a memory game with the assistance of the Furhat robot during a national fair. We found that participants who were assisted by a robot capable of ToM performed better, accepted the hints provided by the robot more frequently, and perceived the robot as more capable of adapting, predicting, and recognising their intentions in comparison to those participants who interacted with a robot without ToM abilities. The findings of this study provide insights that can inform future research into the design of robots that exhibit adaptive behaviour tailored to users’ capacities, while also incorporating ToM capabilities. In summary, the contributions of our study are the following: • Development of a hierarchical architecture that learns: i) socially assistive actions in simulation along, with ii) user’s intended strategies in real interactions, • Deployment and evaluation of a social robot endowed with such architecture in a real-world setting with 56 untrained participants."
https://arxiv.org/html/2411.06948v1,Bipedal walking with continuously compliant robotic legs,"In biomechanics and robotics, elasticity plays a crucial role in enhancing locomotion efficiency and stability. Traditional approaches in legged robots often employ series elastic actuators (SEA) with discrete rigid components, which, while effective, add weight and complexity. This paper presents an innovative alternative by integrating continuously compliant structures into the lower legs of a bipedal robot, fundamentally transforming the SEA concept. Our approach replaces traditional rigid segments with lightweight, deformable materials, reducing overall mass and simplifying the actuation design. This novel design introduces unique challenges in modeling, sensing, and control, due to the infinite dimensionality of continuously compliant elements. We address these challenges through effective approximations and control strategies. The paper details the design and modeling of the compliant leg structure, presents low-level force and kinematics controllers, and introduces a high-level posture controller with a gait scheduler. Experimental results demonstrate successful bipedal walking using this new design.","In biomechanics, there is broad consensus that elasticity –achieved through the compliance of muscles, tendons, and ligaments [1]– provides significant benefits for legged locomotion by dampening impacts, tuning the dynamics of periodic movements, and storing and releasing energy throughout the gait cycle [2]. On a whole-body scale, the advantages of elasticity are clearly demonstrated by simple mechanical models, which capture the fundamental dynamics of legged locomotion. The spring-loaded inverted pendulum (SLIP) model [3], for example, uses a single massless spring to explain key aspects of human running. When extended to two legs and equipped with a hip spring, this model can also describe the dynamics of many other bipedal gaits [4]. Research has also shown that integrating springs can help robots embody desired natural dynamics [5], and that they can facilitate the generation of energy-efficient gaits in bipeds [6, 7] and quadrupeds [8, 9]. A widely adopted approach to incorporating elasticity in legged robotic systems is the use of series elastic actuation (SEA) [10]. In SEA, an elastic element is placed between gearbox output and joint. It protects the gearbox from impacts and decouples the motor’s reflected inertia from the joint. Additionally, by measuring the deflection of the spring, joint torques can be estimated and regulated via admittance control. When highly compliant elastic elements are used, they can periodically store and release energy during a stride, improving locomotion economy [11]. Examples of electrically driven legged robotic systems that employ SEA include Anybotics’ ANYmal [12], Agility Robotics’ Digit, the emPower bionic ankle, and RAMone [13]. Despite the proven success of SEA, there is potential for significant improvements in its mechanical design. Current SEA robots are constructed from rigid segments connected by joints, in which an additional elastic element is integrated alongside the motor. This rigid structure adds mass to the robot’s leg, which is further increased by the inclusion of the elastic component. Moreover, co-locating motor, gearbox, and spring within a single rotational joint increases the overall design complexity. Figure 1: This paper demonstrates successful bipedal walking on continuously compliant legs. The hardware platform used for the experiments is based on the robot RAMone [13] in which the series compliance was removed from the actuators and the lower legs were replaced with a compliant semicircular structure made from spring steel. Bota Systems Rokubi F/T-sensors at the proximal end of the springs allow for measurement of the elastic forces. The robot is constrained to motion in the sagittal plane using a planarizer [14]. In our work, we propose an alternative approach that has the potential to fundamentally transform and enhance the SEA concept: we design the robot’s lower legs as deformable structures, allowing them to function as the series compliance element (Fig.1). This approach simplifies the actuation design significantly. Additionally, by using lightweight, slender materials for these compliant segments, we can reduce the weight at the distal end of the leg, where it is particularly advantageous for improving efficiency and performance [15]. To draw the parallels to a traditional SEA, the rigid upper part of the leg with the hip and knee motors constitute the ‘actuator’ which is positioned in series with the compliant lower leg which acts as the spring. Rather than controlling joint torques, however, we directly control the ground reaction forces at the foot. While this design paradigm may offer advantages in terms of reduced weight, simplified complexity, and improved manufacturability, it also introduces substantial challenges in terms of modeling, sensing, and control of the continuously compliant elements. Unlike traditional rigid robots, which rely on a finite number of discrete joints that can be easily equipped with sensors, a continuously compliant leg exhibits infinite dimensionality, necessitating appropriate approximations and simplifications for effective control. The aim of this paper is to demonstrate how these challenges can be addressed and how a very simple controller can achieve stable bipedal locomotion using this innovative leg design. Our work builds on several legged robots that integrate structural compliance into their designs. RHex used six fiberglass semicircular legs to navigate unstructured terrain [16]; SpaceBok was developed for lunar operations [17]; an initial design of a two-legged robot designed for high-speed running using structural compliance was introduced in [18]; and one-legged hoppers with continuously compliant legs were introduced in [19] and [20]. Although our design shares similarities with these previous robots, the motivation and application of continuous compliance in our system is distinct. RHex utilized compliant legs primarily for obstacle negotiation and rough terrain traversal. In SpaceBok, the feet were made from carbon fiber for weight reduction (and later replaced with aluminum feet to enable locomotion on granular media [21]). The one-legged hoppers demonstrated the use of resonance frequencies for energy-efficient locomotion. In contrast, our approach uniquely employs the compliant legs as a series elastic element, actively regulating their deformation for force control. This is an aspect not explored in the aforementioned robots. This allows us to integrate compliance into the control architecture in a novel way, optimizing both stability and efficiency in legged locomotion. We begin the remainder of this paper by detailing the new hardware (Section II) and present suitable methods for modeling the compliance (Section III). In Section IV, we first introduce effective low-level controllers for force and kinematics regulation, before detailing a high-level posture controller supported by a gait scheduler. Finally, the results of successful walking experiments, which confirm the overall viability of our proposed approach, are discussed in Section V, and further demonstrated in the supplementary video."
https://arxiv.org/html/2411.06920v1,Safe Planner: Empowering Safety Awareness in Large Pre-Trained Models for Robot Task Planning,"Robot task planning is an important problem for autonomous robots in long-horizon challenging tasks. As large pre-trained models have demonstrated superior planning ability, recent research investigates utilizing large models to achieve autonomous planning for robots in diverse tasks. However, since the large models are pre-trained with Internet data and lack the knowledge of real task scenes, large models as planners may make unsafe decisions that hurt the robots and the surrounding environments. To solve this challenge, we propose a novel Safe Planner framework, which empowers safety awareness in large pre-trained models to accomplish safe and executable planning. In this framework, we develop a safety prediction module to guide the high-level large model planner, and this safety module trained in a simulator can be effectively transferred to real-world tasks. The proposed Safe Planner framework is evaluated on both simulated environments and real robots. The experiment results demonstrate that Safe Planner not only achieves state-of-the-art task success rates, but also substantially improves safety during task execution. The experiment videos are shown in https://sites.google.com/view/safeplanner.","Robot task planning is a challenging temporal planning problem (Kaelbling and Lozano-Pérez 2013), aiming at obtaining a subtask sequence to accomplish a long-horizon target task, which is of great importance in the autonomous robot domain (Guo et al. 2023). Classical robot task planning methods are mostly based on search or STRIPS (Fikes and Nilsson 1993). However, these classical methods require much domain-specific knowledge and can hardly be applied to dynamic real-world environments. Following task-planning researchers pay attention to learning-based methods, e.g., reinforcement learning and imitation learning (Ceola et al. 2019; McDonald and Hadfield-Menell 2022; Jiang et al. 2019). Although these learning-based methods have shown promising results, they usually overfit the training scenes and cannot support natural languages as target task descriptions. Recently as large pre-trained models have demonstrated superior planning abilities in general settings, more and more researchers (Guan et al. 2023; Zhao, Lee, and Hsu 2024; Song et al. 2023) try to employ large pre-trained models to achieve autonomous robot task planning with languages as instructions. Recent robot task planning methods based on large pre-trained models mainly utilize vision-language models (VLMs) and large language models (LLMs) to understand the task scenes and decompose the challenging long-horizon tasks (Driess et al. 2023; Hu et al. 2023; Huang et al. 2023b). These models are pre-trained with large-scale data, and thus are endowed with great generalization abilities. However, as lacking real-world training data, it is quite difficult for these models to accurately understand the task scenes, which may lead to unsafe and unexecutable planning results, causing damage to the robots and the real-world environments. For example, in a moving-object task shown in Figure 1, a bunch of objects has been put on one table, and the task is to move object A to a target position. The pre-trained models without real scene knowledge may output a direct planning solution, e.g., picking object A, and then placing it at the target place. However, as object A is surrounded by other objects, directly moving A may damage the nearby objects or cause collisions. Therefore, only using the pre-trained models is not enough, and a smarter planning model with safety awareness is needed, which is expected to move the objects nearby object A to other places before moving the target object A. Figure 1: An illustrative example of safe planning. To achieve safe planning in general robotics tasks, we propose a novel Safe Planner framework, which empowers safety awareness in large model based robot task planning. This framework is comprised of two levels: the high-level planner, and the low-level executors (skills). The high-level planner takes natural language task instruction and visual observations as inputs, and outputs the next skill to execute. For a structured interface between the two levels, we use the planning domain definition language (PDDL) (Fox and Long 2003) to define skills. As only using the large models pre-trained with Internet data as planners suffer from the unsafe problem, we develop a safety prediction module, which can predict the safety of executing the low-level skills. Then, the safety prediction results are leveraged to guide the planning process of the large pre-trained models, incentivize the reasoning ability of the large models, and thus promote safe planning in general robot tasks. Note that in real-world robot tasks, most unsafety is caused by collisions. Therefore in this work, the safety measure is defined with collision numbers, and the proposed framework can be easily adapted with a broader safety definition. In the experiments, we compare the proposed framework with state-of-the-art large model planning methods in both the simulated environments and the real robot manipulation tasks. The experiment results demonstrate that Safe Planner not only achieves better task success rates, but also substantially improves safety during task execution. Besides, The safety prediction module trained with the simulated data can be effectively transferred to the real robot settings. To further investigate the safety module, we conduct thorough ablation studies on its design."
https://arxiv.org/html/2411.06789v1,AV-PedAware: Self-Supervised Audio-Visual Fusion for Dynamic Pedestrian Awareness,"In this study, we introduce AV-PedAware, a self-supervised audio-visual fusion system designed to improve dynamic pedestrian awareness for robotics applications. Pedestrian awareness is a critical requirement in many robotics applications. However, traditional approaches that rely on cameras and LIDARs to cover multiple views can be expensive and susceptible to issues such as changes in illumination, occlusion, and weather conditions. Our proposed solution replicates human perception for 3D pedestrian detection using low-cost audio and visual fusion. This study represents the first attempt to employ audio-visual fusion to monitor footstep sounds for the purpose of predicting the movements of pedestrians in the vicinity. The system is trained through self-supervised learning based on LIDAR-generated labels, making it a cost-effective alternative to LIDAR-based pedestrian awareness. AV-PedAware achieves comparable results to LIDAR-based systems at a fraction of the cost. By utilizing an attention mechanism, it can handle dynamic lighting and occlusions, overcoming the limitations of traditional LIDAR and camera-based systems. To evaluate our approach’s effectiveness, we collected a new multimodal pedestrian detection dataset and conducted experiments that demonstrate the system’s ability to provide reliable 3D detection results using only audio and visual data, even in extreme visual conditions. We will make our collected dataset and source code available online for the community to encourage further development in the field of robotics perception systems.","Pedestrian awareness is a crucial factor for many autonomous systems, such as delivery robots [1], COBOTs [2], and manipulators arms [3], in ensuring efficient planning and safety in complex and dynamic environments. The primary approach to achieving pedestrian awareness has been using exteroceptive sensors such as cameras, 3D-LIDARs [4], and UWB [5], which provides high-resolution environmental information. However, these sensors have certain limitations, such as vulnerability to lighting conditions and occlusion, high costs, and the need for additional operator hardware. In recent years, there has been a growing interest in utilizing audio sensors as a supplementary modality for robot navigation, as evidenced by numerous research studies [6, 7, 8, 9, 10, 11, 12, 13]. Audio sensors can offer valuable environmental information that may not be available through visual and LIDAR, such as sound source direction and distance, and acoustic properties of the surroundings. Additionally, audio sensors are less susceptible to occlusion and can function in low-light or complete darkness. Thus, integrating audio sensors with visual sensors can provide a more complete and robust perception of the environment, which can improve a robot’s ability to navigate effectively in challenging environments, and offer a more affordable solution as well. Figure 1: Our proposed method fully exploits the complementary information between audio and visual modalities, enabling 360-degree pedestrian detection. A promising approach for enhancing pedestrian awareness using audio modality is through self-supervised learning [6, 7, 9]. Thanks to the co-occurrence among various modalities, the audio-visual object detection model can be trained in a self-supervised mode without the need for any labels. Specifically, the 3D position of a person at a certain moment has natural correspondences in both the camera view and the associated sound. We aim to exploit the spatial and temporal relationships between different modalities cues to enable the model to associate sounds and images with the 3D location of the pedestrians. In this study, we introduce a new self-supervised cross-model distillation network for 3D pedestrian detection. The proposed network employs a teacher-student network structure, where the teacher network is a well-trained 3D detection model used to generate pseudo-labels for supervising the learning of the audio-visual network. During inference, the network only requires audio and visual signals as inputs to obtain the 3D position information of pedestrians. We evaluate the performance of the proposed approach on a recently collected multimodal dataset, and the experimental results demonstrate the effectiveness of our model in 3D pedestrian detection. To summarize, this paper presents three key contributions: • Our initial contribution is the presentation of a novel network that incorporates both audio and visual data to detect 3D objects. Our network is particularly noteworthy because it can accurately identify pedestrians within a complete 360-degree field of view, including those who are out of camera view and in the presence of challenging visual conditions. • Our second contribution is the proposal of a network that operates in a self-supervised learning paradigm, thereby reducing the need for labeled data during the training process. By leveraging the natural correspondence between different modalities, our approach achieves high levels of accuracy while minimizing the need for manual annotation. As a result, our proposed method is both efficient and effective, offering significant advantages over traditional supervised learning methods. • Our final contribution is the introduction of a multimodal dataset that includes point cloud, RGB image, and audio data of pedestrians. This dataset is a crucial resource for researchers investigating sound-based object localization, providing valuable data for future studies in this area. Importantly, to the best of our knowledge, this is the first dataset that incorporates audio data in combination with traditional modalities in this manner."
https://arxiv.org/html/2411.06782v1,QuadWBG: Generalizable Quadrupedal Whole-Body Grasping,"Legged robots with advanced manipulation capabilities have the potential to significantly improve household duties and urban maintenance. Despite considerable progress in developing robust locomotion and precise manipulation methods, seamlessly integrating these into cohesive whole-body control for real-world applications remains challenging. In this paper, we present a modular framework for robust and generalizable whole-body loco-manipulation controller based on a single arm-mounted camera. By using reinforcement learning (RL), we enable a robust low-level policy for command execution over 5 dimensions (5D) and a grasp-aware high-level policy guided by a novel metric, Generalized Oriented Reachability Map (GORM). The proposed system achieves state-of-the-art one-time grasping accuracy of 89% in real world, including challenging tasks such as grasping transparent objects. Through extensive simulations and real-world experiments, we demonstrate that our system can effectively manage a large workspace, from floor level to above body height, and perform diverse whole-body loco-manipulation tasks. See our robot at work: quadwbg.github.io.","I INTRODUCTION Quadrupedal loco-manipulation, which integrates legged locomotion with robotic arm manipulation, has emerged as a key research area due to its broad potential applications, including household assistance, urban maintenance, disaster relief, and autonomous field operations [1, 2, 3, 4]. Recent advancements in reinforcement learning (RL) have enabled the development of end-to-end policies for whole-body locomotion and manipulation [5, 6, 7, 8, 9], allowing robots to perform tasks that require seamless coordination of movement and object interaction. While end-to-end RL has substantially improved locomotion skills [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22], loco-manipulation remains highly challenging due to the increased action dimensionality and complex physical interactions involved. These challenges often result in loco-manipulation policies with mediocre accuracy and limited generalizability [5, 7], especially when grasping objects of different shapes, sizes, and materials, thereby restricting their effectiveness in real-world applications. To enhance both the performance and generalizability of whole-body grasping systems, we draw inspiration from the success of various grasp detection techniques [23, 24, 25, 26, 27, 28, 29]. These methods demonstrate robust performance in detecting grasp poses for diverse, unseen objects in cluttered environments, including challenging materials like transparent or specular surfaces. By integrating grasp pose detection with motion planning, these approaches consistently achieve impressive accuracy, typically exceeding a 90% grasping success rate across arbitrary object configurations in table-top settings, as illustrated in 1. Figure 1: Real-world examples of whole-body grasping objects from various locations. Top row: Grasping transparent and specular objects in cluttered environments. Bottom row: Grasping of objects at various heights. This inspires us to take the best of both worlds via integrating legged locomotion with grasp detection to achieve high-performance and highly generalizable loco-manipulation. However, this combination is highly nontrivial. Directly applying grasp detection results for arm motion planning in legged robots is insufficient, as it ignores the coordination required between the robot’s body and arm movements. To address these challenges, we introduce QuadWBG (Generalizable Quadrupedal Whole-Body Grasping), a modular system consisting of four key modules: perception, planning, locomotion, and manipulation (see Figure 2). The perception module integrates object segmentation and grasp detection, handling object tracking and grasp pose prediction. The planning and locomotion modules function as high-level and low-level controllers, respectively, guiding the robot to approach the grasp pose. Finally, the manipulation module leverages motion planning to move the arm and execute the grasp while maintaining the body stationary. At the core of this system is a key innovation: the Generalized Oriented Reachability Map (GORM). GORM acts as a metric for evaluating the reachability of the base pose relative to the target pose across six degrees of freedom. It efficiently guides the planning module during training by calculating the optimal base pose for grasping tasks. GORM also captures the robot’s overall reachability from various positions and orientations, enabling the policy to select base poses that optimize both arm reachability and robot stability. This ensures precise grasping while maintaining balance and locomotion stability. Our framework offers additional advantages. By incorporating highly robust grasp detection modules trained on large-scale real and synthetic data, we eliminate the sim-to-real gap commonly encountered in end-to-end visual policy learning for manipulation. Furthermore, our framework provides clearer insights into system performance and allows for targeted optimization of each module. Through extensive simulations and real-world experiments, we demonstrate that our system achieves state-of-the-art performance in both grasping accuracy and handling a wide range of objects. The system consistently delivers robust whole-body control across a variety of tasks. Notably, it achieves a remarkable one-time grasping accuracy of 89% in real-world scenarios, even excelling in challenging tasks such as grasping transparent objects. These results underscore the system’s high precision and adaptability in complex environments. Figure 2: Pipeline of our system. First, we train a teacher-student 5D low-level policy in simulation (A). The perception module (B) then continuously tracks the object, and generates grasping pose guiding manipulation module (C). This pose is also utilized by the planning module (D) to command the locomotion policy."
https://arxiv.org/html/2411.06766v1,"GenZ-ICP: Generalizable and Degeneracy-Robust 
LiDAR Odometry Using an Adaptive Weighting","Light detection and ranging (LiDAR)-based odometry has been widely utilized for pose estimation due to its use of high-accuracy range measurements and immunity to ambient light conditions. However, the performance of LiDAR odometry varies depending on the environment and deteriorates in degenerative environments such as long corridors. This issue stems from the dependence on a single error metric, which has different strengths and weaknesses depending on the geometrical characteristics of the surroundings. To address these problems, this study proposes a novel iterative closest point (ICP) method called GenZ-ICP. We revisited both point-to-plane and point-to-point error metrics and propose a method that leverages their strengths in a complementary manner. Moreover, adaptability to diverse environments was enhanced by utilizing an adaptive weight that is adjusted based on the geometrical characteristics of the surroundings. As demonstrated in our experimental evaluation, the proposed GenZ-ICP exhibits high adaptability to various environments and resilience to optimization degradation in corridor-like degenerative scenarios by preventing ill-posed problems during the optimization process.","Numerous researchers have studied light detection and ranging (LiDAR)-based odometry, which exploits high-precision distance measurement and is robust to light variations [1, 2]. LiDAR odometry employs various error metrics for iterative closest point (ICP), such as the point-to-point [3] and point-to-plane [4] error metrics, each having its strengths and weaknesses. However, existing LiDAR (-inertial) odometry systems [5, 6, 7, 8] typically rely on a single error metric, leading to performance variations depending on the geometrical characteristics of the surroundings. For instance, although the point-to-point error metric performs well across various environments, it becomes less accurate in structured environments because it does not utilize structured geometric features such as surface normals [4]. Conversely, the point-to-plane error metric can become less precise in unstructured environments owing to the impact of noisy 3D information on the normal estimation [5]. Generalized-ICP (G-ICP) [9] incorporates both error metrics within a probabilistic framework, assuming all surroundings as locally flat. However, this assumption can lead to significant approximation errors depending on the geometrical characteristics of the surroundings, resulting in performance degradation. Fig. 1: LiDAR mapping result of GenZ-ICP in the Long_Corridor sequence of the SubT-MRS dataset [10]. The accumulated map is shown in gray, while the current scan, classified by our proposed adaptive weighting, is colored in light blue where the point-to-plane error metric is applied for planar regions and in red where the point-to-point error metric is applied for non-planar regions. Note that all zoomed-in images are from the same scan, and the visualized coordinate corresponds to the robot’s body frame. Our GenZ-ICP adaptively utilizes both error metrics by reflecting the geometrical characteristics of the surroundings, achieving robustness across various environments, particularly in corridor-like environments. The camera image is included only for better understanding of the scene. This study proposes a novel ICP method called GenZ-ICP to address the drawbacks of relying on a single error metric. As illustrated in Fig. 1, we revisit both point-to-plane and point-to-point error metrics and propose a method that leverages their strengths in a complementary manner. Moreover, the proposed GenZ-ICP utilizes an adaptive weight that reflects the geometrical characteristics of the surroundings, achieving high adaptability to diverse environments. One of the main degeneracy cases for LiDAR odometry is the one-directional degeneracy typically found in corridor-like structures [11]. Therefore, as with Tagliabue et al. [12], we focus on corridor-like degeneracy cases in this study. This study makes the following three key claims: (i) Our approach performs on par with state-of-the-art LiDAR odometry methods in general environments. (ii) It shows superior performance in degenerative environments, such as long corridors, compared to the state-of-the-art approaches that rely on a single error metric. (iii) It prevents mathematically ill-posed problems in the optimization process, resulting in resilience to optimization degradation in corridor-like degeneracy cases."
https://arxiv.org/html/2411.06752v1,Learning from Feedback: Semantic Enhancement for Object SLAM Using Foundation Models,"Semantic Simultaneous Localization and Mapping (SLAM) systems struggle to map semantically similar objects in close proximity, especially in cluttered indoor environments. We introduce Semantic Enhancement for Object SLAM (SEO-SLAM), a novel SLAM system that leverages Vision-Language Models (VLMs) and Multimodal Large Language Models (MLLMs) to enhance object-level semantic mapping in such environments. SEO-SLAM tackles existing challenges by (1) generating more specific and descriptive open-vocabulary object labels using MLLMs, (2) simultaneously correcting factors causing erroneous landmarks, and (3) dynamically updating a multiclass confusion matrix to mitigate object detector biases. Our approach enables more precise distinctions between similar objects and maintains map coherence by reflecting scene changes through MLLM feedback. We evaluate SEO-SLAM on our challenging dataset, demonstrating enhanced accuracy and robustness in environments with multiple similar objects. Our system outperforms existing approaches in terms of landmark matching accuracy and semantic consistency. Results show the feedback from MLLM improves object-centric semantic mapping. Our dataset is publicly available at: jungseokhong.com/SEO-SLAM.","Simultaneous Localization and Mapping (SLAM) has evolved from focusing on geometric accuracy to integrating semantic information, enhancing its utility for downstream tasks such as navigation [1], manipulation [2], and planning [3]. This evolution aligns with advances in computer vision and deep learning [4], introducing richer and more accurate environmental representations. Recent developments in foundation models, such as large language models (LLM) [5, 6, 7], vision language models (VLM) [8, 9], and multimodal LLM (MLLM) [10, 11], have shown that they can extract semantic information from data in an open-vocabulary setting. Several studies [12, 13, 14, 15, 16, 17, 18, 19, 20] show that the foundation models can perform spatial reasoning from a given scene or map with semantic features embedded. Beyond mapping, [21, 22] present semantic SLAM using the foundation models. Confusion matrix update GroundingDINO-SAM Factor graph update Semantic label database update MLLM Supervision RAM \mathbf{l}_{j+1}\mathbf{l}_{j}\mathbf{x}_{i+1}\mathbf{x}_{i+1}\mathbf{x}_{i} Odometry Data Association \mathbf{x}_{i}\mathbf{x}_{i+1}\mathbf{x}_{i+1}\mathbf{l}_{j}\mathbf{l}_{j+1} 2 4 3 1 3 Landmark Projection MAP Estimate RGBD Image Figure 0: Overview of the SEO-SLAM system: The pipeline begins with an RGBD image input, from which odometry is derived. RAM processes the RGB image to generate object tag lists. GroundingDINO and SAM then localize and segment objects based on these tags. Geometric and semantic information from the RGBD image and odometry are fed into a factor graph. MAP estimation is obtained through factor graph optimization. Landmarks from the current map are projected onto the camera frame and overlaid on the image. This composite image is used as input for MLLM, which provides feedback on each landmark based on the current scene. The MLLM’s feedback is used to update: (1) the semantic label database for GroundingDINO, (2) the multiclass prediction confusion matrix, and (3) the list of erroneous factors. For example, if the feedback informs \textbf{l}_{j+1} is an erroneous landmark, factors related to \textbf{l}_{j+1} are removed. Despite these advances, critical challenges persist in semantic SLAM: (1) Distinguishing between similar objects in close proximity remains difficult when detectors only provide generic labels (e.g., “shoe” for all shoes). This results in the fusion of similar objects into a single landmark, as shown in (Fig LABEL:fig:concept1a). (2) Erroneous landmarks pose a significant challenge in maintaining consistent maps over time. This issue can be caused by sensor measurement uncertainties or scene changes, particularly in cluttered and dynamic environments. (3) Object detectors are vulnerable to inherent biases in their training datasets, causing consistent semantic errors in some objects. To address these challenges, we aim to leverage foundation models’ semantic understanding and SLAM’s spatial accuracy to build maps that are both semantically and spatially consistent. Foundation models have robust semantic understanding capabilities, but limited spatial reasoning without a prebuilt map with embedded semantic features. Conversely, SLAM systems excel at capturing spatial information, but often struggle with maintaining reliable semantic information. By combining these strengths, we propose Semantic Enhancement for Object SLAM (SEO-SLAM), a novel approach that leverages VLM and MLLM for semantic SLAM (Fig ‣ I). The main contributions of our work include: 1. Integrating image tagging, tag-based grounding, and segmentation models into the SLAM pipeline to enable descriptive open-vocabulary object detection and refine semantic information of landmarks. 2. Harnessing MLLMs for generating more descriptive labels for existing landmarks and rectifying erroneous landmarks to reduce perceptual aliasing. 3. Presenting a method to use MLLM responses to update a multiclass prediction confusion matrix and identify duplicated landmarks. 4. Experimental results demonstrating the improved object semantic mapping accuracy in challenging scenarios with multiple similar objects in close proximity. 5. Introducing datasets which have semantically similar objects in a single scene with odometry, ground truth trajectory data, and ground truth object information."
https://arxiv.org/html/2411.06751v1,DP and QP Based Decision-making and Planning for Autonomous Vehicle,"Autonomous driving technology is rapidly evolving and becoming a pivotal element of modern automation systems. Effective decision-making and planning are essential to ensuring autonomous vehicles operate safely and efficiently in complex environments. This paper introduces a decision-making and planning framework for autonomous vehicles, leveraging dynamic programming (DP) for global path planning and quadratic programming (QP) for local trajectory optimization. The proposed approach utilizes S-T graphs to achieve both dynamic and static obstacle avoidance. A comprehensive vehicle dynamics model supports the control system, enabling precise path tracking and obstacle handling. Simulation studies are conducted to evaluate the system’s performance in a variety of scenarios, including global path planning, static obstacle avoidance, and dynamic obstacle avoidance involving pedestrian interactions. The results confirm the effectiveness and robustness of the proposed decision-making and planning algorithms in navigating complex environments, demonstrating the feasibility of this approach for autonomous driving applications.","I-A Motivation and Problem Statement In recent years, autonomous driving technology has made significant advancements, becoming a cornerstone of modern transportation automation. Unlike drones or smart robots, autonomous vehicles are responsible for performing various ground transportation tasks while maintaining passenger comfort and vehicle stability. To ensure the safety and comfort of passengers, it is crucial that autonomous vehicles maintain stability in all driving conditions. Therefore, a reliable decision-making and planning system is required to navigate effectively in challenging scenarios, such as avoiding dynamically appearing obstacles, handling sharp turns, and ensuring pedestrian safety on roadways. Additionally, unstable driving and extra mileage can lead to increased tire wear and passenger discomfort, emphasizing the need for planning algorithms that can generate efficient, stable, and collision-free paths. Before executing decision-making and planning algorithms, it is essential to establish a comprehensive understanding of the driving environment using global perception techniques, such as those described in [1] [2]. These technologies enable autonomous vehicles to perceive and interpret their surroundings in real-time, accurately identifying obstacles and their positions, which is crucial for making informed decisions. Once the perception layer is in place, the focus shifts to designing effective path-planning and control strategies that ensure the vehicle’s safety and stability in dynamic environments. Planning algorithms for autonomous vehicles are responsible for generating safe and optimized routes that avoid potential collisions while remaining stable. In this context, dynamic programming (DP) and quadratic programming (QP) algorithms are essential, as they enable decision-making that balances optimal path selection with vehicle stability [3]. The S-T graph representation provides a visual framework for trajectory planning, allowing the vehicle to adapt to both static and dynamic obstacles. This paper presents a decision-making and planning framework for autonomous vehicles, integrating DP and QP algorithms to achieve safe and stable navigation. Global planning is handled through dynamic programming to generate optimal routes, while local trajectory optimization is performed using quadratic programming to ensure smooth and collision-free paths. The S-T graph approach is employed to facilitate real-time obstacle avoidance and path adjustments based on the perceived environment. The main contributions of this work are summarized as follows: • Decision-making and Planning Framework: This paper proposes an integrated decision-making and planning framework based on DP for global planning and QP for local trajectory optimization. The combined approach allows the autonomous vehicle to navigate complex environments safely and efficiently. • Dynamic and Static Obstacle Avoidance: The proposed framework leverages an S-T graph to represent the vehicle’s planned path and speed profile in real time, enabling effective avoidance of both dynamic and static obstacles. • Enhanced Navigation Capabilities: By incorporating DP and QP algorithms, this work enhances the vehicle’s ability to generate optimized routes and trajectories, improving overall navigation performance and operational safety. I-B Related Works Various controllers have been developed to achieve effective control of autonomous vehicles. Stanford University’s Stanley autonomous vehicle utilized a Proportional-Integral (PI) controller, which was designed based on a linearized model of the vehicle’s dynamics relative to the path [4]. A Radial Basis Function Neural Network (RBFNN)-based adaptive PID controller was proposed for longitudinal control to achieve precise speed tracking [5]. Moreover, fuzzy logic controllers were integrated with traditional PID controllers to enable parameter adaptation, improving control accuracy [6]. However, these methods often face challenges in achieving precise longitudinal control, as throttle and brake are separate components, and a single PID controller may not be efficient or accurate enough in coordinating both systems. For lateral control, alternative techniques such as sliding mode control (SMC) have been employed to counter parameter uncertainties and external disturbances, allowing better trajectory control [7]. Additionally, fuzzy PID controllers focusing on new self-adaptive fuzzy PID designs based on nonlinear multi-input multi-output (MIMO) structures have been explored [8] [9]. However, PID control remains a simple feedback mechanism with manually tuned parameters, making it difficult to achieve real-time and precise responses [10]. In contrast, Linear Quadratic Regulator (LQR) control is designed using a state-space model of the system, enabling it to leverage the system’s dynamic characteristics more effectively [11]. For decision-making and planning algorithms, commonly employed methods include the A* search algorithm and Dijkstra’s algorithm [12]. A typical decision-making and planning module can be divided into three levels: route planning, behavioral decision-making, and motion planning. Zhao et al. identified route planning as the process of generating a global path by combining map information after receiving a specified travel destination, which serves as a reference for subsequent detailed path planning [13]. The behavioral decision-making layer makes decisions based on perception data and, through motion planning, ultimately generates a trajectory that satisfies specific constraints. This paper adopts a combination of DP and QP, which is well-suited for multi-stage decision problems and efficiently handles scenarios with large state spaces and complex state transitions [14]."
https://arxiv.org/html/2411.06708v1,Cải thiện thời gian bay dựa trên mô hình điều khiển dự đoán thích nghi cho máy bay không người lái,"Các nền tảng trên không thông minh như máy bay không người lái (UAV) đang được kỳ vọng mang đến cuộc cách mạng trong hàng loạt lĩnh vực như vận chuyển và giao thông, giám sát hiện trường, sản xuất công nghiệp, quản lý nông nghiệp. Trong đó, điều khiển chính xác là một trong những nhiệm vụ quan trọng mang tính quyết định hiệu suất và khả năng làm việc của hệ thống máy bay không người lái. Tuy nhiên, các nghiên cứu hiện nay tập trung giải quyết vấn đề theo dõi quỹ đạo, giảm thiểu sai số trong quá trình bay mà ít quan tâm tới cải thiện thời gian bay. Trong bài báo này, chúng tôi đề xuất một mô hình điều khiển dự đoán (MPC) giảm thiểu thời gian bay đồng thời khắc phục những hạn chế của bộ điều khiển MPC cổ điển thường được sử dụng. Bên cạnh đó, phương pháp MPC và ứng dụng của nó cho điều khiển máy bay không người lái đã được trình bày chi tiết trong bài báo. Cuối cùng, kết quả đã chứng minh hiệu suất của bộ điều khiển đề xuất được cải thiện so với MPC tiêu chuẩn. Ngoài ra, hướng tiếp cận này có tiềm năng trở thành nền tảng cho việc kết hợp các thuật toán thông minh vào các bộ điều khiển cơ bản.","I GI´I THI.U Máy bay không người lái (UAVs) đang tạo ra một cuộc cách mạng trong nhiều ngành công nghiệp, nông nghiệp cũng như quân sự. Nhờ khả năng di chuyển linh hoạt trong các môi trường và thiết kế nhỏ gọn, máy bay không người lái có thể cắt giảm chi phí hoạt động đi 50%, giảm rõ rệt thời gian cần để thực hiện công việc nhất là với những hệ thống có quy mô lớn [1]. Để đạt được hiệu suất cao trong các ứng dụng, một yêu cầu quan trọng là phải duy trì được độ chính xác bay, trong khi giảm thời gian thực hiện chuyến bay đi ngắn nhất có thể. Điều đó đặt ra thách thức cho việc cải tiến các thuật toán điều khiển cổ điển được sử dụng, trong khi vẫn phải đảm bảo độ chính xác cũng như độ phức tạp để phù hợp cho hệ thống phần cứng hạn chế của máy bay không người lái. Gần đây, các phương pháp điều khiển dựa trên tối ưu hóa đặc biệt là mô hình điều khiển dự đoán (MPC) và những biến thể của nó thu hút nhiều sự chú ý cho điều khiển quadrotor nhờ những tiến bộ trong hiệu quả phần cứng, thuật toán và mô hình. MPC xem xét các giá trị hiện tại và quá khứ và tạo ra lệnh điều khiển theo đường chân trời lùi giúp tối ưu hóa sai khác trong tương lai theo đường chân trời. Các phương pháp MPC tuyến tính và phi tuyến đều đã được áp dụng để điều khiển mô hình quadrotor được tổng kết và đánh giá trong [2]. Ngoài ra, MPC thể hiện khả năng hoạt động với những ràng buộc về vật lý của hệ, cũng như thích nghi tốt với hệ thống đa cảm biến, phi tuyến tính [3, 4, 5]. Tuy nhiên, nhiều ứng dụng của MPC vẫn gặp phải những thách thức đáng kể, chẳng hạn như yêu cầu về mô hình tính toán chính xác và sự cần thiết của việc giải những bài toán tối ưu hóa quỹ đạo trực tuyến với khả năng tính toán hạn chế của máy tính quy mô nhỏ gắn trên máy bay. Việc cải tiến các tham số trong bộ điều khiển MPC từ lâu đã được các nhà khoa học quan tâm đến. Amos trong [6] đề xuất MPC khả vi, khi các trọng số trong hàm chi phí được cập nhật qua từng bước thời gian để thích nghi và nâng cao hiệu suất của bộ điều khiển. Angel trong [7] đã đề xuất và ứng dụng bộ điều khiển dự đoán biên mô hình, bằng sự kết hợp giữa MPC và các thành phần sai số biên [8], vừa xét tới tính chính xác của bộ điều khiển cũng như cải thiện thời gian bay. Nhìn chung, các thuật toán MPC cải tiến mặc dù đã được chứng minh là mang lại hiệu suất bay tốt hơn, nhưng vẫn có sự đánh đổi nhất là về độ phức tạp của thuật toán cải tiến, và phần lớn các thuật toán MPC cải tiến cho quadrotor chưa thể hoạt động được trong điều kiện thời gian thực [9]. Cải tiến thời gian tối ưu trong các thuật toán điều khiển là một hướng nghiên cứu tiềm năng trong tương lai. Các thuật toán điều khiển cơ bản mặc dù khai thác nhiều vào sai số giữa trạng thái và tín hiệu điều khiển hiện tại đối với trạng thái và tín hiệu điều khiển tham chiếu nhưng rất ít thuật toán xét tới việc tối ưu hoá thời gian thực hiện. Neunert và các cộng sự trong [10] đã đưa ra một mô hình MPC với một trong những đầu vào là thời gian tối ưu để điều khiển UAV. Thời gian tối ưu trong bài báo này được định nghĩa là thời gian từ khi bắt đầu chuyển động cho tới lúc đạt được một điểm tham chiếu quan trọng (thường là điểm đích). Trong bài báo này, chúng tôi đề xuất thuật toán MPC cải tiến nhằm tối ưu thời gian bay cho mô hình máy bay không người lái được mô tả trong Hình 1. Ngoài ra, chúng tôi cũng ứng dụng và trình bày toàn diện mô hình MPC trong điều khiển máy bay không ngưới lái để phù hợp với đặc trưng động học và động lực học của hệ thống. Cuối cùng, chúng tôi đánh giá thuật toán đề xuất với thuật toán MPC tiêu chuẩn, cho thấy hiệu quả trong việc đáp ứng vị trí, thời gian bay, góc lệch và đưa ra một số hướng phát triển tiềm năng của thuật toán trong tương lai. Figure 1: Tổng quan về thuật toán cải tiến đề xuất Cấu trúc của bài báo được sắp xếp theo thứ tự như sau: Hệ thống điều khiển đề xuất được mô tả trong phần II. Tiếp đến, phần III trình bày những kết quả đạt được, bao gồm cả việc cấu hình hệ thống với những ràng buộc động học và động lực học, cùng với đó là kết quả và đánh giá hiệu năng của thuật toán MPC cải tiến. Cuối cùng, những kết luận được nêu rõ trong phần IV."
https://arxiv.org/html/2411.06707v1,Theo dõi quỹ đạo Quadrotor sử dụng Linear và Nonlinear Model Predictive Control,"Theo dõi quỹ đạo chính xác là đặc tính quan trọng và cần thiết cho điều hướng an toàn của Quadrotor trong trường lộn xộn hoặc bị nhiễu loạn. Trong bài viết này, chúng tôi trình bày chi tiết hai nền tảng điều khiển dựa trên mô hình hiện đại nhất cho bám quỹ đạo: linear-model-predictive controller (LMPC) và nonlinear-model-predictive controller (NMPC). Bên cạnh đó, các mô hình động học, động lực học của quadrotor được mô tả đầy đủ. Cuối cùng, hệ mô phỏng được triển khai và kiểm nghiệm tính khả thi, cho thấy sự hiệu quả của hai bộ điều khiển.","I GI´I THI.U I-A Bối cảnh và động lực Quadrotor đã trở thành một trong những phương tiện bay không người lái (UAV) phổ biến nhất [1], góp phần định hình lại các ngành công nghiệp, hậu cần, nông nghiệp,… [2, 3]. Do cấu trúc đơn giản và khả năng bay linh hoạt, quadrotor dần đóng vai trò quan trọng trọng đời sống cũng như nghiên cứu, tiêu biểu như Hoa Kỳ đã có hơn 300.000 máy bay không người lái thương mại được đăng ký tính đến năm 2021 và theo dự kiến sẽ tiếp tục tăng từ 4,4 tỷ lên 63,6 tỷ USD trong giai đoạn 2018-2025 [2, 4]. Yêu cầu tối quan trọng khi điều khiển quadrotor dưới môi trường nhiễu động bên ngoài có thể ảnh hưởng nặng nề đến hiệu suất bay đặc biệt là ở những khu vực lân cận với các công trình [5]. Hơn nữa việc áp dụng máy bay không người lái trong trình diễn đặc biệt là trong các sự kiện lớn đòi hỏi cần có khả năng theo đõi đối tượng, quỹ đạo chuyển động một cách nhanh chóng và linh hoạt. Tuy nhiên, hầu hết các phương pháp đều gặp khó khăn trong việc xử lý các hiệu ứng chung trong các chuyến bay nhanh, chẳng hạn như động lực học phi tuyến, hiệu ứng khí động học và giới hạn truyền động [3]. Gần đây, model predictive control (MPC) và các biến thể của nó thu hút nhiều sự chú ý cho điều khiển quadrotor nhờ những tiến bộ trong hiệu quả phần cứng và thuật toán [6, 7]. Ngoài ra, MPC hoạt động đối với các vấn đề đa cảm biến, xem xét các hạn chế đối với đầu vào và trạng thái trong công thức cùng với đặc điểm mạnh mẽ, thích nghi tốt với rối loạn, phi tuyến tính và lỗi mô hình [8]. I-B Các nghiên cứu liên quan Vấn đề điều khiển của quadrotor đã được nghiên cứu rộng rãi trong đó có nhiều cách tiếp cận khác nhau. Các phương pháp điều khiển tuyến tính như điều khiển proportial-integral-derivative (PID), linear quadratic regulator (LQR) [9, 10] được triển khai với mục đích đạt được tầm bay ổn định và đạt được hiệu suất đủ tốt. Tuy nhiên, phương pháp điều khiển tuyến tính sẽ không còn hiệu quả nếu quỹ đạo là đường đi phức tạp. Các bộ điều khiển phi tuyến được đề xuất tiêu biểu như backstepping [11] và feedback linearzation [12]. Phương pháp MPC là một phương pháp điều khiển dựa trên tối ưu hóa, nó tạo ra các lệnh điều khiển theo kiểu đường chân trời rút lui giúp giảm thiểu lỗi theo dòng đường chân trời. Tuy nhiên MPC đòi hỏi rất nhiều về mặt tính toán so với các phương pháp kể trên. Các nghiên cứu linear MPC [13, 14] được sử dụng trong điều khiển vị trí hoặc điều khiển mô hình tuyến tính hóa. Các nghiên cứu Nonlinear MPC [5, 15] nhằm điều khiển vị trí xyz cũng như vị trí góc (yaw,pitch,roll). Các phương pháp Nonlinear MPC cho hiệu quả hoạt động tốt hơn so với Linear MPC. I-C Đóng góp của bài báo Đóng góp chính của bài báo là trình bày, so sánh giữa bộ điều khiển linear MPC và nonlinear MPC cho vấn đề theo dõi quỹ đạo đối với robot quadrotor bên cạnh việc mô tả chi tiết mô hình động học, động lực học của robot. Mục đích so sánh là nhấn mạnh lợi ích của việc xem xét mô hình động lực học đối với quỹ đạo chuyển động. Kết quả được mô phỏng và thực hiện trong môi trường MATLAB được hiển thị và xác minh tính hiệu quả của bộ điều khiển MPC. I-D Bố cục bài báo Cấu trúc của bài báo được sắp xếp theo thứ tự như sau: Đầu tiên, các vấn đề theo dõi quỹ đạo cùng với mô hình quadrotor được trình bày trong phần II. Tiếp theo, bộ điều khiển linear MPC và nonlinear MPC được mô tả chi tiết lần lượt trong các phần III và IV. Trong phần V đưa ra môi trường giả lập và thảo luận về kết quả bám quỹ đạo của thuật toán. Cuối cùng, kết luận được nêu rõ trong phần VI."
https://arxiv.org/html/2411.06643v1,Flight Demonstration and Model Validation of a Prototype Variable-Altitude Venus Aerobot,"This paper details a significant milestone towards maturing a buoyant aerial robotic platform, or aerobot, for flight in the Venus clouds. We describe two flights of our subscale altitude-controlled aerobot, fabricated from the materials necessary to survive Venus conditions. During these flights over the Nevada Black Rock desert, the prototype flew at the identical atmospheric densities as 54 to 55 km cloud layer altitudes on Venus. We further describe a first-principle aerobot dynamics model which we validate against the Nevada flight data and subsequently employ to predict the performance of future aerobots on Venus. The aerobot discussed in this paper is under JPL development for an in-situ mission flying multiple circumnavigations of Venus, sampling the chemical and physical properties of the planet’s atmosphere and also remotely sensing surface properties.","Venus, our enigmatic neighboring planet of similar size and solar distance, has evolved into a world with very different conditions as Earth with a dense carbon dioxide atmosphere, a cloud layer composed of sulfuric acid aerosols, and surface temperatures above 460∘C. The processes that governed Venus’s evolution from a rocky planet’s primordial state, and how and when Venus and Earth diverged, remain poorly understood despite a history of Venus exploration over the last fifty years. While large landed missions, probes, and orbiters have improved (and are planned to dramatically improve) our understanding of the planet, many science questions remain that can only be addressed with a well-instrumented long duration vehicle flying in the clouds. Figure 1: Subscale prototype of a JPL’s variable-altitude controlled aerobot, made from Venus-compatible materials, in flight over the Black Rock desert in Nevada, USA. Photograph taken July 2022. 1.1 Venus Aerobot Background Balloon experiments have operated on Venus before but nothing of the proposed capability. The Soviet Vega lander missions of 1985 included balloon probes with two small 7 kg gondolas that flew for approximately 48 hours in the cloud layer at a nominally constant altitude [1]. The challenge today is expand this early demonstrator to a larger and fundamentally more capable buoyant aerial robotic platform (aerobot), thereby enabling the greater science return promised by a dedicated mission. NASA’s Jet Propulsion Lab (JPL) has been developing large aerobots for specifically this purpose for over twenty years, including multiple Discovery proposals [2], carrying a 40-100 kg gondola below the aerobot with science instrumentation consistent with a much more complete characterization of the Venus cloud layer. These aerobots, like the Vega balloons before, were designed to maintain altitude through the use of ""superpressure"" - i.e. a high strength envelope inflated to pressure slightly above the ambient atmospheric pressure. Given its constant volume, a superpressure balloon loses buoyancy when displaced upwards and gains buoyancy when displaced downwards, making it altitude stable. More recently, a NASA study described in Cutts et al. [3] refocused JPL efforts on an aerobot that can instead actively control its altitude over a variable range (Figure 1) instead of staying at a constant altitude. A variable-altitude approach allows the reuse of a single payload over a wide range of conditions in the clouds, thereby more fully characterizing the cloud layer (and providing an even greater science return) for a modest complexity cost. The success of the Google Loon program [4] provided credence to the rapid development of variable altitude aerobots for Earth, and the NASA study prioritized the adaption of this technology for Venus. A number of variable-altitude Venus design points and their development are described in [5], with targeted altitude range on Venus from 52 to 62 km and a flight duration goal of 117 Earth-days (one Venus day). Such platforms are highly scaleable, with designs able to carry from 15 kg to 230 kg depending on mission ambition. This paper describes a significant milestone in the technology advancement of this Venus aerial plaform: JPL’s first outdoor flights of a prototype aerobot, made from Venus-compatible materials, at atmospheric densities within the proposed flight envelope for Venus. Table 2 describes the demonstrated flight parameters of flight test in comparison to Venus. Balloon flight dynamics are principally driven by atmospheric density, meaning that this flight recreates a relevant environment for flight dynamics in Venus’s cloud layer from 54-55 km above the surface. The targeted Venus temperatures are roughly 15^{\circ}C greater than those in Nevada however, so models must be employed to take this and other environmental differences into account. Accordingly, a key objective for this flight was to provide validation data for our flight dynamics model FLOATS (FLight Operations and Aerobot Trajectory Simulator), which we subsequently use to predict mission trajectories and operations on Venus. Venus: VIRA Reference Atmosphere [6] Earth: US Standard Atmosphere (incl. +20^{\circ}C local offset condition) [7] Altitude Density Temperature Altitude Density Temperature (km) (kg/m3) (∘C) (km ASL) (kg/m3) (∘C) 52 1.28 60.2 – – – 53 1.15 49.9 0.0 1.15 35 54\cellcolorblue!15 1.03\cellcolorblue!15 39.7\cellcolorblue!15 1.1\cellcolorblue!15 1.03\cellcolorblue!15 27.9\cellcolorblue!15 55\cellcolorblue!15 0.92\cellcolorblue!15 29.2\cellcolorblue!15 2.2\cellcolorblue!15 0.92\cellcolorblue!15 20.7\cellcolorblue!15 56 0.82 18.7 3.3 0.82 13.6 57 0.72 9.4 4.5 0.72 5.8 58 0.63 2.1 5.7 0.63 -2.1 59 0.54 -4.4 7.0 0.54 -10.5 60 0.47 -10.4 8.2 0.47 -18.3 61 0.41 -14.5 9.4 0.41 -26.1 62 0.34 -18.7 10.8 0.34 -35.2 Table 2: Atmospheric equivalencies between Venus and test flight in Nevada, USA. Altitudes flown highlighted in blue. Note exact match in atmospheric density, although equivalent densities at Venus are slightly warmer. While flights at higher altitudes are possible with our platform, balloons must trade altitude for payload mass for a given envelope volume, and a complete characterization of the balloon dynamics for FLOATS required more payload mass than the full altitude range could support on a prototype of this scale. 1.2 Programmatic Context The development of this platform by JPL reflects preparation for a Venus mission concept that addresses the documented needs of the Venus science community. After assessing the state of fundamental knowledge on our sister planet, NASA’s Venus Exploration Assessment Group (VEXAG) defined six overarching Venus science objectives that can be addressed through near-term Venus mission opportunities, such as ""Did Venus have temperate surface conditions and liquid water at early times?"" and ""What processes drive the global atmospheric dynamics of Venus?"" [8]. While the recent selection of two Venus orbiters (VERITAS & Envision) and the DAVINCI entry probe will go a long way towards fulfilling many of the objectives sought by the NASA Venus science community, the subsequent 2023-2032 Planetary Science and Astrobiology Decadal Survey Origins, Worlds, & Life (OWL) notes that other Venus science objectives remain largely unaddressed by these missions, fundamentally because they require long duration in-situ measurements from within the atmosphere [9]. OWL is the overarching guiding document for the scientific exploration of the planets for the next decade, so architectures that enable these long duration in-situ measurements will naturally be the focus of the next generation of Venus exploration. Specifically, OWL has focused the New Frontiers 6 mission opportunity for Venus (called VISE, the Venus In-situ Explorer) on “the processes and properties of Venus that cannot be characterized from orbit or from a single descent profile"" [9]. These processes and properties include: ""(1) Complex atmospheric cycles (e.g., radiative balance; chemical cycles, atmospheric dynamics, variations of trace gases, light stable isotopes, and noble gas isotopes, and the couplings between these processes); (2) Surface-atmosphere interactions (e.g., physical and chemical weathering at the surface, near-surface atmospheric dynamics, and effects upon the atmosphere by any ongoing geological activity); and (3) Surface properties (e.g., elemental and mineralogical composition of surface materials, heat flow, seismic activity, and any magnetization)” [9]. Future missions to Venus can address VISE through several different architectures, including landers and aerial platforms, depending on the their focused science objectives. While landed surface missions (at 460∘C) have their own distinct advantages and technology challenges, aerial platforms that operate in the cloud layer (60∘C to -20∘C) can also offer in-situ access within the atmosphere at much more clement thermal conditions - assuming the sulfuric acid aerosols can be held at bay. Specifically, buoyant aerial platforms can avoid the hot surface conditions for the entire mission duration, thereby allowing the use of high-heritage instruments, avionics, and power systems, and creating a faster mission infusion opportunity with few thermal challenges [3]. Additionally, OWL has continued endorsing the Discovery program for concepts outside of VISE constraints, and Venus’s nearby location in the solar system could make an ambitious aerobot mission as cost-effective as a less ambitious concept to farther planetary targets. The variable-altitude aerobot was accordingly adopted into two OWL-support mission concept studies for Venus [10, 11], and Cutts et al. [12] describe a variety of smaller missions that could be performed with a variable-altitude aerobot, including concepts for New Frontiers and Discovery proposals. The prototype flown in the flight tests described in this paper is approximately one-third scale of the favored design point of these previous mission studies. 1.3 Paper Outline Section 2 describes the flight experiment parameters, including the subscale variable-altitude aerobot design, the approximately 30kg of instrumentation carried, and an overview of the flight operations and performance. Section 3 describes the FLOATS simulation equations, an update since our prior description in [5], covering the balloon altitude dynamics, shape model, aerodynamics, thermodynamics, heat transfer, and atmospheric models. Section 4 compares the aerobot flight to the FLOATS reconstruction. Section 5 leverages FLOATS to predict flights on Venus, specifically a three-circumnavigation mission of a larger variable-altitude aerobot carrying a gondola of 100kg. Section 6 concludes the paper with lessons-learned, next steps, and framing of the results within the larger mission context of aerial platforms for Venus. A video of the test flights in this paper can be found at https://www.jpl.nasa.gov/news/jpls-venus-aerial-robotic-balloon-prototype-aces-test-flights."
https://arxiv.org/html/2411.06627v1,"Optimal Virtual Model Control for Robotics: 
Design and Tuning of Passivity-Based Controllers","Passivity-based control is a cornerstone of control theory and an established design approach in robotics. Its strength is based on the passivity theorem, which provides a powerful interconnection framework for robotics. However, the design of passivity-based controllers and their optimal tuning remain challenging. We propose here an intuitive design approach for fully actuated robots, where the control action is determined by a ‘virtual-mechanism’ as in classical virtual model control. The result is a robot whose controlled behavior can be understood in terms of physics. We achieve optimal tuning by applying algorithmic differentiation to ODE simulations of the rigid body dynamics. Overall, this leads to a flexible design and optimization approach: stability is proven by passivity of the virtual mechanism, while performance is obtained by optimization using algorithmic differentiation.","Passivity-based control methods are crucial for designing of stable and reliable robot controllers [1, 2, 3, 4, 5, 6, 7]. Passivity-based approaches take into account the energy of the robot-controller interconnection (energy shaping) and regulate its dissipation (damping injection) to guarantee stability. The goal is a passive controlled robot that is stable when interacting with other passive devices. The passivity theorem [8, 9] guarantees stability even if both the robot model and the environment are uncertain, as long as they are passive. A classical example is the ubiquitous proportional derivative (PD) control plus gravity compensation, in joint or end-effector space [10, 11]. The proportional action has the mechanical interpretation of a spring. Combined with gravity compensation, it shapes the potential energy of the controlled robot. The derivative action has the mechanical interpretation of a damper, which has the effect of slowing down the robot motion towards the minimum of the shaped potential energy (stabilization via damping injection). The resulting controlled robot is passive and very reliable. Classical PD control shows how passivity-based design for fully actuated robot manipulators is a relatively ‘easy’ task. Although passivity-based control is a well-established control approach, in applications few practical designs go beyond PD control and quadratic potentials. Likewise, established methods such as virtual fixtures [12] and impedance controllers too often remain within the comfort of proportional-derivative reaction to the position error. In general, it is difficult to connect the design of a complex controller with the specific features of the task. In control-theoretic terms, the challenge is finding the relevant task-dependent metric and then optimizing the structure and parameters of the controller for this metric. Picture the design of a controller for a surgical laparoscopic task [13, 14, 15], where the surgical tool is constrained to pass through a small, but somewhat compliant aperture into the patient’s body. Precise position or force control are insufficient to tackle the complex interaction required by the task. Attacking this through the mathematical derivation of a specific potential energy of passivity-based control appears to fall short. In fact, what kind of ‘energy-shaping’ is best for this task? Even when the controlled impedance of the robot is considered [16], the desired impedance is often chosen heuristically [17]. Pragmatically, the review [18] concludes that efforts are needed “(…) to establish more systematic guidance or methodology on how to specify impedance parameters [such as desired inertia, damping, and stiffness] to reflect the basic dynamic interaction requirements of robotic manipulations raised from customers (…)”. Our hypothesis is that passivity-based control remains a central design approach in these complex settings. The question is how to explore the space of passivity-based controllers in terms of task-oriented design and optimal tuning. We propose a two-stage approach: first, design the controller as a virtual mechanism connected to the robot; second, tune it using scientific machine-learning by differentiation of the rigid-body-dynamics through simulations. The design of controllers as virtual mechanisms is the philosophy of virtual model control [19, 20]. Conceptualizing the controller as a virtual mechanism falls within the framework of control through interconnection [3, 21, 11, 22], but opens the design space to mechanical intuition and the ability to customize the structure of the controller to the specific features of the task. For the case of laparoscopic surgery, we can design a compliant virtual mechanism that connects to the surgical tool, constraining its interaction with soft tissues. An illustration is provided in Fig. 1. This is neither position nor force control. It shows connections with impedance control, but without enforcing a specific desired impedance. The shaping of the energy is not reduced to the manipulation of mathematical terms, but is determined by the geometry and dynamics of the virtual mechanism and by the virtual interconnection with the surgical tool, both driven by mechanical intuition. Nonlinearities are often seen as problematic dynamics to be removed. However, in a virtual mechanism, a nonlinear spring serves as a tool to stiffen or saturate at different extensions. Nonlinear mechanical linkages become a tool for shaping the kinetic and potential energy of the system intuitively without designing custom distance functions for energy-shaping potential fields. Virtual mechanisms are easy to implement using virtual operation space forces. Their implementation is easy to debug, as the controller action can be understood in terms of the physics of the virtual mechanism. If the virtual mechanism is passive, then the closed loop is passive, guaranteeing convergence to a minima of the potential energy of the controlled robot. In fact, the virtual mechanism approach provides robustness for the sim-to-real transition: even with geometric/inertial modeling errors, so long as the robot and controller are passive, we have some level of stability guarantee. Figure 1: The virtual mechanism for laparoscopic surgery. The motion of the virtual ‘instrument’, in red, is constrained to a pre-defined remote centre of motion (RCM) through prismatic and rotatory joints. The virtual instrument is ‘attached’ to the robot through springs and dampers. Springs are shown in a stretched configuration, at equilibrium there would be no distance between the virtual instrument and the surgical tool. For optimal tuning, we are inspired by recent work on scientific-machine learning: the practise of combining scientific models with gradient-based optimization [23]. Recent developments in automatic differentiation of ordinary differential equation (ODE) solvers have opened the door to efficient optimizations involving continuous-time dynamical systems, attracting increasing attention in robotics [24, 25, 26, 27, 28, 29]. Scientific machine learning has been used in related applications in physical system identification [30, 31, 32]; to find optimal state feedback control policies [33]; to perform optimal energy shaping for a pendulum, tuning a controller which is a passive neural network [34]; to design an energy shaping controller to generate desired closed-loop oscillatory trajectories on an undamped double pendulum [35]. In this paper, we embrace this philosophy. We consider the robot as an open system whose input/output behavior must be optimized. The optimization of the controller does not rely on general-purpose architectures, such as neural networks, autoencoders, etc. In contrast, our ‘model’ (in the sense of machine learning) is the virtual mechanism designed by the user. The expressivity of the controller, that is, the ability to generate a wide family of control policies, is regulated by the level of parameterization of the virtual mechanism, ranging from stiffness and damping parameters to length, inertia, and other geometrical features of its mechanical linkages. These parameters are optimized using loss functions that are surrogates of classical performance control metrics, namely the \mathcal{L}_{2} and \mathcal{L}_{\infty} metrics [36]. Compared to [33], the use of virtual mechanisms leads to dynamic feedback policies. Passivity is not enforced through constraints as in [34], but inherited from the structure of the controller. Computing a gradient through an ODE solver ‘properly’ allows us to compute quickly, thus handle the optimization over the dynamics of the controlled robot efficiently. Using this approach to tune virtual mechanisms can meet the expectations of the review [18] for systematic parameter specification. At the same time, our approach guarantees stability/robustness certificates, which are often missing in state-of-the-art machine learning methods (sim-to-real gap), limiting their applicability in critical domains such as surgical robotics, where a higher level of certainty about controller stability/robustness is required. The contributions of this paper include: (i) Design of passivity-based controllers as virtual mechanisms, discussed in Sections II and III. These sections extend the early results of [37]. (ii) The adaptation of \mathcal{L}_{2} and \mathcal{L}_{\infty} performance metrics for tractable optimization based on automatic differentiation. This is discussed in Section IV-A. (iii) A tuning approach based on scientific machine learning implemented by algorithmic differentiation of continuous-time simulations, discussed in Section IV-C. This extends the approach of [38], based on linear matrix inequalities [39]. (iv) Simulation results for the 1DOF and 7DOF robot in a surgical setting. We also provide experimental validation, as shown in Section V-B."
https://arxiv.org/html/2411.06615v1,Field Insights for Portable Vine Robots in Urban Search and Rescue,"Soft, growing vine robots are well-suited for exploring cluttered, unknown environments, and are theorized to be performant during structural collapse incidents caused by earthquakes, fires, explosions, and material flaws. These vine robots grow from the tip, enabling them to navigate rubble-filled passageways easily. State-of-the-art vine robots have been tested in archaeological and other field settings, but their translational capabilities to urban search and rescue (USAR) are not well understood. To this end, we present a set of experiments designed to test the limits of a vine robot system, the Soft Pathfinding Robotic Observation Unit (SPROUT), operating in an engineered collapsed structure. Our testing is driven by a taxonomy of difficulty derived from the challenges USAR crews face navigating void spaces and their associated hazards. Initial experiments explore the viability of the vine robot form factor, both ideal and implemented, as well as the control and sensorization of the system. A secondary set of experiments applies domain-specific design improvements to increase the portability and reliability of the system. SPROUT can grow through tight apertures, around corners, and into void spaces, but requires additional development in sensorization to improve control and situational awareness.","I INTRODUCTION In the event of a building collapse, the reduction of a structure to an unstructured debris pile may create void spaces – internal, subsurface pockets where human survivors may be located [1]. Collapses present a myriad of electrical, chemical, and mechanical hazards to rescue crews resulting from exposed wires, gas leaks, sharp objects, and unstable debris among many other dangers [2]. Amidst these hazards, organized Urban Search and Rescue (USAR) teams, including canines and their handlers, structural engineers, and technical search specialists jointly assess the debris to locate survivors and identify the safest entry points in the structure using specialized search cameras to probe just beneath the surface. USAR teams have long employed rescue robots of varying morphologies to augment response efforts and accelerate response times [3]. Drones, unmanned ground vehicles [4, 5] and legged robots [6, 7, 8] enable above-ground monitoring and mapping of collapsed sites. Snake-like robots [9, 10, 11] have explored subsurface areas in collapsed buildings. Nonetheless, these established forms tend to be both expensive and rigid, which limits their adaptability to access small voids. In these environments where secondary collapse and robot loss are ever-present risks, there is an unmet need for a low-cost, capable platform to explore the void space for survivors of these tragedies and viable paths to reach them. Figure 1: External view of SPROUT growing into the void space inside a mock collapsed structure. Orange arrow shows the direction of growth from outside the void. Transparent images depict robot pointing its sensor head in multiple directions. (Inset) Egocentric view of space using tip-mounted camera. To address the challenges of this space, we consider vine robots: soft, growing structures that evert from their tips [12]. They are driven by air pressure, squeeze through small holes, move with minimal disturbance to the environment [13], and are steered with pneumatic actuators [14]. In sum, they travel along deliberate, complex paths through 3D space from user directions [15] and carry sensors at their tips [13]. In this paper, we adapt our robot previously used successfully in archaeological field studies [16] for integration with USAR teams. Our system, known as SPROUT – the Soft Pathfinding Robotic Observation Unit, is evaluated on the training site for Massachusetts Task Force 1 (MA-TF1). This study contextualizes recent developments in vine robots and their adaptability to the needs of the USAR community. The contributions of this work are: • A taxonomy of difficulty to categorize challenges presented by collapsed structure void spaces. • Analysis of state-of-the-art vine robot technology in a realistic debris pile with multiple points of entry. • System modifications of a vine robot for increased portability and on-pile operations."
https://arxiv.org/html/2411.06557v1,Real-time Deformation-aware Control for Autonomous Robotic Subretinal Injection under iOCT Guidance,"Robotic platforms provide repeatable and precise tool positioning that significantly enhances retinal microsurgery. Integration of such systems with intraoperative optical coherence tomography (iOCT) enables image-guided robotic interventions, allowing to autonomously perform advanced treatment possibilities, such as injecting therapeutic agents into the subretinal space. Yet, tissue deformations due to tool-tissue interactions are a major challenge in autonomous iOCT-guided robotic subretinal injection, impacting correct needle positioning and, thus, the outcome of the procedure. This paper presents a novel method for autonomous subretinal injection under iOCT guidance that considers tissue deformations during the insertion procedure. This is achieved through real-time segmentation and 3D reconstruction of the surgical scene from densely sampled iOCT B-scans, which we refer to as B5-scans, to monitor the positioning of the instrument regarding a virtual target layer defined at a relative position between the ILM and RPE. Our experiments on ex-vivo porcine eyes demonstrate dynamic adjustment of the insertion depth and overall improved accuracy in needle positioning compared to previous autonomous insertion approaches. Compared to a 35% success rate in subretinal bleb generation with previous approaches, our proposed method reliably and robustly created subretinal blebs in all our experiments.","I INTRODUCTION Subretinal injection is a surgical technique to deliver therapeutic agents, such as gene therapy vectors, transplanted cells or stems cells directly under the retina, specifically targeting genetic retinal dystrophies and degenerations. Such procedures enable viable treatment options for diseases such as age-related macular degeneration (AMD), which alone is expected to affect 288 million people by 2040 [1]. Compared to injections into the vitreous with current anti-VEGF [2] drugs, direct injection of the payload into the subretinal space has the potential to maximize the therapeutic efficacy and reduce the systemic toxicity potential [3]. Figure 1: Tissue deformation during subretinal injection with a target (pink dashed line), defined as a virtual layer between ILM (green) and RPE (blue). In comparison the fixed target point (yellow) is outside of the retina at the end of the insertion. Subretinal injection is performed by inserting a microsurgical cannula through the retinal surface, known as the internal limiting membrane (ILM), and slowly guiding its orifice between the photoreceptor cells and the retinal pigment epithelium (RPE) - Bruch’s membrane complex. For optimal therapeutic results, a bleb, which can measure several millimeters in diameter, is formed by injecting a fluid that creates a localized separation between the ILM and the RPE, allowing the medication to effectively reach the targeted area. Due to the fragile and non-regenerative nature of the photoreceptor and RPE cells, any tissue damage can cause irreversible harm to the patient’s eyesight [4]. Therefore, it is crucial to place the needle at an optimal depth within the, on average 250 \mu m [5] thin, human retina to minimize trauma while maximizing drug-tissue interaction. There are two major difficulties in repeatedly performing successful subretinal injection: (1) visualization of the anatomical target area, and (2) precise needle insertion without causing tissue damage. The conventional way to visualize ophthalmic procedures is through an operating microscope. In addition to the microscopic view, optical coherence tomography (OCT) can provide high-resolution cross-sectional imaging at high update rates to visualize the retinal layers and monitor the subretinal injection procedure. OCT enables depth-resolved imaging, acquiring 2D B-scans and 3D C-scans of the surgical site [6, 7, 8]. Figure 2: Overview of our real-time needle insertion pipeline. (a) B5-scan, consisting of five 2D B-scans, showing different cross sections of the needle and retina. (b) Segmented B-scans using our segmentation network, needle top surface (red), ILM (green), RPE (blue). (c) Point cloud generated from segmentation results without processing. (d) Point cloud generated from segmentation results after processing with inpainted ILM and RPE layers, removed outlier needle points and visualized virtual target layer (pink). (e) Needle tip area of the processed point cloud. Pink points are the virtual target layer, gray line represents the A-scan going through the needle tip. (f) Robot control adjustments based on needle position. The second major challenge is guiding the surgical cannula to the target location. Due to human physiological limits, such as the tremor of the surgeon’s hand which is around 180\mu m [9], precise needle insertion and positioning at the required micrometer accuracy is naturally challenging. On the other hand, robotic systems [10, 11, 12, 13] have been developed for ophthalmic applications, which are not affected by limitations of human dexterity and enable precise and repeatable needle control inside the eye. Paired with iOCT imaging, such robotic systems have been successfully employed in subretinal injections [14, 15, 16]. With advances in deep learning, OCT-guided robotic task autonomy has been enabled for subretinal injections [17, 18, 19]. While these approaches show promising results on ex-vivo porcine eye experiments by navigating the cannula along a calculated trajectory to a target location, dynamic tissue behavior during the insertion procedure has not yet been considered. Recent studies have demonstrated the effects of tool-tissue interaction during subretinal injection and captured the resulting dynamic changes of the tissue structures in OCT imaging [20]. Hence, real-time feedback mechanisms need to be developed to continuously monitor the needle insertion, update the control strategy according to tool-tissue interactions, and ensure correct targeting. To address this challenge, we propose a real-time, OCT-based feedback system. Our method utilizes OCT B5-scans, which we define as five densely sampled 2D OCT B-scans generating small 3D OCT volumes. B5-scans are smaller than conventional C-scans and can be updated at a high frequency, enabling real-time acquisition and processing that determines the robotic control strategy. Retinal deformations, as a result of tool-tissue interactions, cause the retinal tissue and thus also the target point to shift. Tracking the target point is a hard computer vision task due to the retina’s monotonous nature. Meanwhile, the target point drifts in the X and Y directions are less critical because of the large size of the bleb, however, maintaining accurate positioning along the Z-axis is crucial, as it ensures that injections consistently reach the intended tissue layer, particularly relative to the RPE, where depth precision is crucial for effective treatment without causing tissue damage. Thus, we define the target area as a virtual layer located at a relative depth level between the retina’s ILM and RPE layers rather than a fixed point in the 3D volume. This dynamic targeting method adapts to tissue deformation during the insertion procedure by continuously updating the target distance based on the current retinal conformation. This ensures successful needle insertion to the anatomical target as visualized in Fig. 1. Overall, the main contributions of this work are as follows: 1. We develop and evaluate a real-time method for dynamically controlling needle insertions under iOCT guidance in robotic subretinal injections. 2. We introduce a virtual target layer located at a relative level in between ILM and RPE that dynamically adapts to tissue deformations caused by the needle insertion procedure. To validate our proposed method, we conducted two experiments on ex-vivo open-sky porcine eyes. Additionally, we compared the accuracy and quality of subretinal injections using our virtual target layer method with the conventional fixed-point targeting approaches used in previous works utilizing a syringe pump."
https://arxiv.org/html/2411.06543v1,Magnetic Field Aided Vehicle Localization with Acceleration Correction,"This paper presents a novel approach for vehicle localization by leveraging the ambient magnetic field within a given environment. Our approach involves introducing a global mathematical function for magnetic field mapping, combined with Euclidean distance-based matching technique for accurately estimating vehicle position in suburban settings. The mathematical function based map structure ensures efficiency and scalability of the magnetic field map, while the batch processing based localization provides continuity in pose estimation. Additionally, we establish a bias estimation pipeline for an onboard accelerometer by utilizing the updated poses obtained through magnetic field matching. Our work aims to showcase the potential utility of magnetic fields as supplementary aids to existing localization methods, particularly beneficial in scenarios where Global Positioning System (GPS) signal is restricted or where cost-effective navigation systems are required.","I INTRODUCTION Autonomous navigation has been an extensively studied field over the past two decades. Most autonomous systems make use of GPS, Light Detection and Ranging (LiDAR), and Camera as sensory inputs to estimate their position in a particular environment, as elaborated in [5]. While these sensors and their combinations yield promising outcomes [12], [13], [14], the imperative to explore alternative or supplementary options remains paramount. Environments with restricted GPS signal or feature deficit surroundings present significant challenges for traditional localization systems. Additionally, the need for cost-effective systems encourages to re-examine usability of readily available on-board sensors like magnetometers. This impetus drives our investigation into utilizing the ambient magnetic field to augment vehicle localization in suburban settings. The magnetic field based localization has been studied extensively for indoor applications. The ambient magnetic field offers a robust feature-based map for indoor localization, addressing the limitations of conventional positioning systems such as GPS within enclosed environments [9, 10]. In [8], a comprehensive survey shows the diverse approaches explored in the context of mapping and leveraging the ambient magnetic field for indoor localization. Additionally, in [4], a particle filtering technique tailored for indoor localization using a magnetic one-dimensional map is shown to accurately estimate a robot’s position along indoor corridors. Along with these studies, there are some interesting studies done for the outdoor environment as well. For instance, [1] employs a particle filter technique to achieve localization utilizing a pre-existing magnetic field map. This method estimates longitudinal travel distance and utilizes azimuth angle errors to gauge lateral deviations. Similarly, [11] explores outdoor localization leveraging ambient magnetic fields with wheel encoders to determine azimuth angles and traveled distances. Magnetic field-based localization presents a viable solution for navigating GPS-denied areas such as tunnels, underground parking facilities, and multi-level roads, as illustrated in [3]. The promising results shown in these studies offer a possible solution to enhance outdoor localization capabilities, particularly in challenging environments where traditional navigation systems may be unreliable. Our research introduces a methodology for combining Inertial Measurement Unit (IMU) based pose predictions with efficient mapping and localization using magnetic field data. The method utilizes batches of magnetic field data for a continuous pose estimation (x,y) instead of relying on magnetic field spikes through the environment. The method utilizes the positions updated through magnetic field matching to infer accelerometer bias and scale factor. We validate the effectiveness of our approach by testing through on-road applications, employing an IMU and a magnetometer positioned within a vehicle while driving in suburban environments."
https://arxiv.org/html/2411.06542v2,Is Linear Feedback on Smoothed Dynamics Sufficient for Stabilizing Contact-Rich Plans?,"Designing planners and controllers for contact-rich manipulation is extremely challenging as contact violates the smoothness conditions that many gradient-based controller synthesis tools assume. Contact smoothing approximates a non-smooth system with a smooth one, allowing one to use these synthesis tools more effectively. However, applying classical control synthesis methods to smoothed contact dynamics remains relatively under-explored. This paper analyzes the efficacy of linear controller synthesis using differential simulators based on contact smoothing. We introduce natural baselines for leveraging contact smoothing to compute (a) open-loop plans robust to uncertain conditions and/or dynamics, and (b) feedback gains to stabilize around open-loop plans. Using robotic bimanual whole-body manipulation as a testbed, we perform extensive empirical experiments on over 300 trajectories and analyze why LQR seems insufficient for stabilizing contact-rich plans. The video summarizing this paper and hardware experiments is found here.","Dexterous manipulation is full of contact-rich interactions, enabling various tasks through complex frictional interactions [1]. Historically, the non-smooth nature of contact has precluded a range of planning and control methods that rely on gradients of the dynamics. Recent advances have utilized contact smoothing — where non-smooth dynamics are replaced by a continuously differentiable proxy — to great effect as surrogate dynamics models for planning through contact [2, 3, 4, 5]. One may hope, then, that smoothing enables the use gradient-based control. (a) LQR. (b) Open-loop controller. Figure 1: These figures show snapshots of hardware experiments using LQR and open-loop controllers under perturbations to initial conditions of cylinder. The thick and thin lines represent the desired frame at the terminal time step and the current frame of the cylinder, respectively. While LQR outperforms open-loop in this example, a more comprehensive evaluation shows that LQR generally performs poorly. This work suggests that the above hope may face significant obstacles. We (1) introduce LQR control for contact-manipulation via contact smoothing. Furthermore, we (2) present and analyze robust trajectory optimization, hoping that the generated trajectories are robust to the model errors accumulated by using a surrogate dynamics model for control, and thus more amenable to LQR. Then, we (3) extensively evalute the performance of these methods, both in simulation and in hardware, on a bimanual whole-body manipulation as shown in Fig. 1. In short, we find: Despite its efficacy in planning through contact, dynamical smoothing alone is unsatisfactory as a means to obtaining linear control policies. Finally, we (4) identify the key factors leading to the inadequacies of linear control; namely, the unilaterality of contact, and the tendency of controllers to “push and pull” unless the dynamics are only very-slightly smoothed."
https://arxiv.org/html/2411.06425v1,Results of the 2023 CommonRoad Motion Planning Competition for Autonomous Vehicles,"In recent years, different approaches for motion planning of autonomous vehicles have been proposed that can handle complex traffic situations. However, these approaches are rarely compared on the same set of benchmarks. To address this issue, we present the results of a large-scale motion planning competition for autonomous vehicles based on the CommonRoad benchmark suite. The benchmark scenarios contain highway and urban environments featuring various types of traffic participants, such as passengers, cars, buses, etc. The solutions are evaluated considering efficiency, safety, comfort, and compliance with a selection of traffic rules. This report summarizes the main results of the competition.","The CommonRoad Motion Planning Competition was established in 2021, aiming to bring together researchers working on motion planning for autonomous vehicles. This report summarizes the results from the 2023 edition. The main goal of the competition is to provide a fair comparison of different motion planning approaches on a large number of realistic traffic scenarios. To achieve this, all motion planners are executed on the same hardware and consider the same traffic scenarios. Moreover, the benchmark scenarios are realistic since real road networks are used and the behaviors of the traffic participants are either adapted from real-world recordings or simulated by state-of-the-art traffic simulators. Finally, the competition additionally also considers a realistic vehicle model with nonlinear dynamics and parameters taken from a real Ford Escort. In addition to presenting the results of the competition, this report also contains a short description of the motion planners submitted by the participants, which provides some insights into the different motion planning strategies used. The remainder of this report is organized as follows: First, the format of the competition is described in Sec. 0.2 and the rules for performance evaluation are presented in Sec. 0.3. Afterward, a description of the participating motion planners is provided in Sec. 0.4, before presenting the results of the competition in Sec. 0.5."
https://arxiv.org/html/2411.06414v1,Psycho Gundam: Electroencephalography based real-time robotic control system with deep learning,"The Psycho Frame, a sophisticated system primarily used in Universal Century (U.C.) series mobile suits for NEWTYPE ({CJK}UTF8minニュータイプ) pilots, has evolved as an integral component in harnessing the latent potential of mental energy. Its ability to amplify and resonate with the pilot’s psyche enables real-time mental control, creating unique applications such as psychomagnetic fields and sensory-based weaponry. This paper presents the development of a novel robotic control system inspired by the Psycho Frame [1], combining electroencephalography (EEG) and deep learning for real-time control of robotic systems. By capturing and interpreting brainwave data through EEG, the system extends human cognitive commands to robotic actions, reflecting the seamless synchronization of thought and machine, much like the Psyco Frame’s integration with a Newtype pilot’s mental faculties. This research demonstrates how modern AI techniques can expand the limits of human-machine interaction, potentially transcending traditional input methods and enabling a deeper, more intuitive control of complex robotic systems.","The notion of using mental energy to control complex machinery has long been a key theme in science fiction, especially in the U.C. Gundam series. The Psycho Frame, a technology specifically designed for Newtype pilots, epitomizes this concept by enabling the pilot to control a mobile suit via mental synchronization, enhancing not only their combat effectiveness but also the machine’s response to mental stimuli. This technology amplifies the pilot’s mental power, creating psychomagnetic fields and enabling long-range, remote-controlled attacks. With the evolution of the Psycho Frame into the NT-D system, the pilot’s mental abilities are further augmented, allowing for advanced applications such as quantum manipulation and conceptual interference with physical systems. Inspired by this futuristic vision, we explore the real-world implications of mental control using EEG-based robotic systems. Electroencephalography captures brainwave signals that are processed by deep learning algorithms to establish real-time control over robotic platforms [2]. This work builds upon the concept of the Psycho Frame to develop a control system where mental inputs can direct robot actions. Our system, drawing from advanced AI and neurotechnology, aims to emulate the seamless interaction between human thought and robotic behavior, as seen in the Gundam universe. The paper outlines the design, training, and deployment of this EEG-based control system, analyzing its potential for applications in fields such as prosthetics, robotics, and augmented human-machine interfaces. The contributions of this work are stated as follows:"
https://arxiv.org/html/2411.06408v1,Visuotactile-Based Learning for Insertion with Compliant Hands,"Compared to rigid hands, underactuated compliant hands offer greater adaptability to object shapes, provide stable grasps, and are often more cost-effective. However, they introduce uncertainties in hand-object interactions due to their inherent compliance and lack of precise finger proprioception as in rigid hands. These limitations become particularly significant when performing contact-rich tasks like insertion. To address these challenges, additional sensing modalities are required to enable robust insertion capabilities. This letter explores the essential sensing requirements for successful insertion tasks with compliant hands, focusing on the role of visuotactile perception. We propose a simulation-based multimodal policy learning framework that leverages all-around tactile sensing and an extrinsic depth camera. A transformer-based policy, trained through a teacher-student distillation process, is successfully transferred to a real-world robotic system without further training. Our results emphasize the crucial role of tactile sensing in conjunction with visual perception for accurate object-socket pose estimation, successful sim-to-real transfer and robust task execution.","Daily life activities inherently involve interacting with the environment through contact. For robots to perform real-world tasks effectively, they must navigate through unstructured, contact-rich environments. While humans execute various interaction tasks with relative ease, achieving similar levels of precision and reliability in such environments poses a significant challenge for robots [1]. Physical interaction of the robot with its surroundings can greatly enhance perception and understanding of the environment [2]. These interactions provide essential information that is often otherwise unavailable, playing a critical role in adapting to uncertainties and improving robotic performance. This is particularly true for precision manipulation, where robots must perceive, grasp, manipulate and accurately place objects according to task-specific requirements. Figure 1: Tight insertion of an object into a socket with a robotic arm and a three-finger compliant hand without hand proprioception. Two sensing modalities are used: vision provides a rough estimate of the object-socket poses and tactile sensors on the fingers deliver implicit contact information. Precise robotic manipulations are often performed with rigid hands [3]. These hands are usually fully-actuated making their state deterministic. Hence, having hand proprioception along with visual perception and tactile sensing may provide enough information for accurate pose estimation of a grasped object [4]. Nevertheless, rigid hands are often characterized by complex structures with numerous degrees of freedom [5, 6]. This complexity can lead to increased fragility, higher costs and challenges in control. An alternate solution is the use of compliant hands [7]. These hands often incorporate underactuated mechanisms, such as combinations of tendons and springs, to achieve flexibility and adaptability in grasping various objects without closed-loop control [8]. However, this same adaptability makes precise control more difficult due to the absence of accurate analytical models, inherent uncertainties and no proprioception feedback [9]. Despite ongoing research, developing robust manipulation skills with compliant hands is still a key challenge [10]. Figure 2: Overview illustration of the training steps in simulation. First, a teacher policy is trained using privileged information. Subsequently, a distillation process is employed to train a student policy that learns to imitate the teacher’s behavior, relying solely on visuotactile data and End-Effector (EE) pose. Unlike rigid hands, the true state of a compliant hand is usually not observable [11]. That is, finger joint angles and loads cannot be extracted during grasping and manipulation, making real-world manipulation with the hand much more challenging. To bypass this challenge, we explore the use of multimodal perception in compliant hands for contact-rich manipulation skills. Recently, multimodal perception has been advanced for manipulation learning with rigid hands and usually includes tactile, visual and proprioceptive sensing [12]. Visual perception provides a comprehensive view of the environment, aiding in the localization and orientation of objects while providing rough pose estimations; Tactile perception allows robots to better understand relative and local information, to detect and respond to subtle changes in contact forces and object configuration, facilitating delicate manipulations that are challenging with vision alone [13]; and proprioception is essential for the coordinated use of tactile and visual feedback [14]. As mentioned, proprioception is usually not available in compliant hands. Combining extrinsic vision and tactile sensing is often termed Visuotactile sensing, and have been widely explored with rigid hands in contact-rich tasks such as insertion and in-hand manipulation [12, 14], often with known objects [4]. However, in rigid hands, the state of the hand is rather deterministic and the visuotactile data sufficiently captures the state of the hand-object, exhibiting minimal uncertainty. Learning manipulation skills with compliant hands, on the other hand, presents significant challenges due to the uncertainties in the hand-object state [15]. The configuration of a grasped object can vary considerably during contact-rich tasks, requiring a robust policy capable of adapting to these changes. Recently, learning-based methods gained a lot of popularity in acquiring manipulation policies [16, 17, 18], offering the potential for greater generalization as more data becomes available. Recent progress in this area stems from two main sources: the use of expert demonstrations combined with imitation learning [19], and learning in simulation paired with sim-to-real techniques [20, 21]. Both approaches attempt to compensate for the lack of diverse real-world data. However, these have limitations when it comes to manipulation with compliant hands. Expert demonstrations are usually acquired through teleoperation while remote control of a compliant hand is unnatural and lacks hand proprioception feedback, limiting the ability to perform complex tasks beyond simple grasping. Meanwhile, sim-to-real methods, while showing strong generalization and robustness across tasks like in-hand manipulation, grasping and long-horizon manipulation, struggle to simulate the complex and uncertain dynamics of compliant hands [22]. In this letter, we investigate the essential requirements for multimodal perception in task-centric manipulation involving compliant hands. As a test case, we consider insertion tasks which often serves as an ideal benchmark for evaluating the effectiveness of contact management in robotic manipulation [2]. Insertion is common in applications like assembly [23] and dense packing [24]. Hence, achieving successful tight object insertion with visuotactile sensing validates the robustness of the robotic system, demonstrating its ability to handle complex real-world scenarios that demand precise positioning and delicate manipulation. Furthermore, we assert that incorporating structured visuotactile representations into policy design is crucial for improving the ability to manage contact variations. Effective policy design for compliant hand manipulation requires object-centric, spatial-aware and relative-sensitive visuotactile representations. These attributes enable focusing on task-relevant objects, reasoning in 3D space and detecting subtle contact variations. To the best of the author’s knowledge, this is the first exploration of visuotactile sensing for compliant hands without relying on proprioceptive feedback. In addition to understanding multimodal information for manipulation with compliant robotic hands, we explore simulative tools for ease of policy training with transfer to a real system. In particular, a simulation integrating visuotactile sensing and the compliant hand mechanism is used to train a teacher model with privileged information. Visuotactile sensing is composed of a masked point cloud observed by a depth camera, and the high-resolution AllSight [25] tactile sensor. Subsequently, an imitation learning approach is employed to distill the perceived data into its most essential components. Then, the distilled policy is deployed on a real system with no further training. The policy is agnostic to the specific robotic arm used. Our methodology, illustrated in Figure 2, highlights the importance of understanding multimodal information in enhancing the manipulation capabilities of compliant robotic hands. The simulation and deployment code are provided open-source111Open-source simulation, deployment code, and videos are available at: https://github.com/osheraz/IsaacGymInsertion for potential benchmarks and to advance research in the field."
https://arxiv.org/html/2411.06382v1,Hardware-in-the-Loop for Characterization of Embedded State Estimation for Flying Microrobots,"Autonomous flapping-wing micro-aerial vehicles (FWMAV) have a host of potential applications such as environmental monitoring, artificial pollination, and search and rescue operations. One of the challenges for achieving these applications is the implementation of an onboard sensor suite due to the small size and limited payload capacity of FWMAVs. The current solution for accurate state estimation is the use of offboard motion capture cameras, thus restricting vehicle operation to a special flight arena. In addition, the small payload capacity and highly non-linear oscillating dynamics of FWMAVs makes state estimation using onboard sensors challenging due to limited compute power and sensor noise. In this paper, we develop a novel hardware-in-the-loop (HWIL) testing pipeline that recreates flight trajectories of the Harvard RoboBee, a 100mg FWMAV. We apply this testing pipeline to evaluate a potential suite of sensors for robust altitude and attitude estimation by implementing and characterizing a Complimentary Extended Kalman Filter. The HWIL system includes a mechanical noise generator, such that both trajectories and oscillatinos can be emulated and evaluated. Our onboard sensing package works towards the future goal of enabling fully autonomous control for micro-aerial vehicles.","Autonomous flapping-wing micro-aerial vehicles (FWMAV) possess the unique combination of small size and high maneuverability, opening a wide range of potential real-world applications. With wingspans as small as 2.5 centimeters Wood (2008), FWMAVs have the potential of assisting in tasks such as artificial pollination, environmental monitoring, or search and rescue in small, difficult to reach areas. To accomplish such applications, fully autonomous flight of FWMAVs (which includes robust control and state estimation using an onboard electronics package) must be achieved. With the small scale of FWMAVs comes challenging dynamics that makes state estimation difficult. As vehicle size diminishes, the vehicle’s dynamics scale accordingly. For example, rotational acceleration rate scales as l^{-1} , resulting in the ability to perform rapid attitude changes, similar to saccades observed in flying insects and birds Kumar and Michael (2012); Dickinson (2005); Wissa (2022). The vehicle dynamics are inherently unstable, requiring active control to perform corrective maneuvers which produces low frequency oscillation modes in FWMAV flights McLean (2003). Furthermore, the induced oscillations from flapping wings generates additional disturbances to the observed sensor data. As a result, state estimators must carefully consider the vehicle’s dynamics and characteristic modes to provide accurate estimations. While solutions have been derived for similar flight dynamics, an additional challenge for the design of any state estimation algorithm is accounting for the power, weight, and compute restrictions of these vehicles Aurecianus et al. (2018). Figure 1: (a) The Harvard RoboBee, the target FWMAV, photographed with the IMU and ToF components utilized in the hardware experiments. (b) A RoboBee flight trajectory that the robot arm reproduces in (c), while carrying the IMU and ToF sensors to collect data. (d) The sensor package, mounted to the distal end of the arm, which contains the sensor components shown in (a) for state estimation. The first demonstration of an insect-scale microrobot to carry its own weight solved the problem of flapping-wing propulsion at the micro-scale by driving the wings with piezoelectric bimorph actuators Wood (2008). Due to the poor efficiency of motors at smaller scales (due to surface area to volume ratio scaling, resulting in a greater dominance of friction Trimmer (1989)), alternate propulsion techniques for insect-scale FWMAV have been developed, including dielectric elastomer actuators and piezoelectric bending actuators Chen et al. (2022); Ozaki et al. (2021). This paper considers the Harvard RoboBee, an 81~{}\mathrm{mg} FWMAV shown in Figure 1 (a). The Harvard RoboBee leverages piezoelectric bimorph actuators to generate a thrust-to-weight ratio greater than unity, sufficient to power agile flight with some overhead available for a small payload (40~{}\mathrm{mg}). However, the vehicle’s minimal payload, along with fast vehicle dynamics, has historically resulted in several tasks being performed off-board including computation and sensing. Specifically, current autonomous flights for the RoboBee rely on an array of external motion capture cameras to provide the necessary state estimates required for feedback control McGill et al. (2022). Research in control autonomy for larger systems has demonstrated onboard localization using a wide array of sensors, such as cameras and scanning laser range finders Bi et al. (2018) Hu et al. (2022). However, the considered vehicles are one to two order magnitudes larger than the RoboBee, which makes several of the considered sensors infeasible as they do not satisfy the RoboBee’s limited weight and power constraints. Nonetheless, biology offers evidence of a plethora of unique sensory mechanisms that enable birds and insects to maintain stable flight orientations Sane et al. (2007); Croon et al. (2022); De Croon et al. (2022). Thus, past research has similarly demonstrated the possibility of onboard sensor feedback for RoboBee control. Fuller et. al. considered several options for enabling stable orientation control for the RoboBee, using an onboard gyroscope and a bio-inspired ocelli (horizon detection sensor) Fuller et al. (2014, 2013). Helbling et. al. demonstrated onboard integration of a Time-of-Flight sensor for altitude estimation for the RoboBee Helbling et al. (2017). These prior research efforts informed the survey and selection of sensor components as shown in Figure 1 (a). However, the mechanical complexity of the RoboBee makes the integration and evaluation of potential sensing packages and algorithms challenging. As a result, the scale, manufacturing complexity, and limitations of the RoboBee prevented the prototyping of advanced estimation algorithms. This challenge along with computational limitations led to the use of relatively simple state estimation algorithms that relied on techniques like gyroscopic integration which produced significant drift up to 6 degrees in orientation estimation after only a 2 second flight experiment Fuller et al. (2014). Figure 2: Block diagram illustrating the overall pipeline for the evaluation of state estimation algorithms. Based on RoboBee flight trajectory data, which is captured with a suite of Vicon motion capture cameras, a candidate onboard sensor package is evaluated by reproducing the same flight trajectories in hardware (or simulation). Subsequently, the sensor data is fed into the state estimation algorithm which utilizes a complementary filter and EKF to estimate the orientation and altitude of the vehicle. These challenges motivated the development of a lightweight sensing suite, which integrates several of the components listed above, and a novel state estimation algorithm based on an Extended Kalman Filter by Talwekar et al. This algorithm simultaneously estimates pitch, altitude, and translational velocity Talwekar et al. (2022) for extended durations (20 seconds). To evaluate the proposed sensing package while avoiding the constraints imposed by insect-scale FWMAVs, Talwekar used an offboard experimental setup, in which he waved the sensing package by hand inside a motion capture arena to collect ground truth data for evaluation. Although this testing pipeline allowed for rapid evaluation and iteration, the dynamics of actual FWMAV flights are complex and introduce several sources of noise that were not reflected in their evaluation procedure. Although the majority of the previous sensing research for insect-scale FWMAVs do not explore more advanced state estimation algorithms, researchers have investigated novel estimators for vehicles with similar noisy dynamics. Mahony et. al derived a fundamental filtering architecture that applies to rotation matrices and similarly Madgwick et al. proposed a gradient-descent complementary filter for attitude estimation for quad-rotors Mahony et al. (2008); Madgwick et al. (2011). In particular, researchers have explored algorithms for oscillating environments using filter architectures such as Cascaded Complementary Filters (CCF) and Complementary Kalman Filters Chiella et al. (2019); C and Jain (2016). While these papers consider a similar challenge (i.e., state estimation for aerial robots), the performance of each algorithm is validated in a simulated environment, with minimal, if any, experiments on hardware. Given the notable hardware constraints, however, performing thorough hardware experiments to validate accuracy of potential state estimation algorithms and sensors under FWMAV flight dynamics is important for motivating the substantial efforts needed to bring the compute and sensors onboard. However, given the hardware complexity in the manufacturing of vehicles like Harvard’s RoboBee, evaluate state estimation algorithms in hardware settings is quite challenging. Thus, given insect-scale FWMAV’s limited accessibility and complex fabrication process, research has also been invested into developing methods of approximating RoboBee dynamics and controls for hardware-in-the-loop testing of more complicated software solutions that can only be solved with experimentation. Chen, et. al presented a method of simulating FWMAV dynamics on quadrotors to create a more reproducible experimental setup for software control/state estimation solutions Chen et al. (2017). While this example successfully reproduces RoboBee dynamics by mapping the control inputs of a FWMAV to quadrotor inputs, they do not simulate the body oscillation dynamics that makes state estimation for FWMAVs particularly challenging. Effective evaluation of potential state estimation algorithms and sensors requires a more complete reproduction of FWMAV flight dynamics. Specifically, we seek to incorporate a richer suite of dynamics, including noise from various oscillation modes, in addition to reproducing gross trajectories in order to subject sensors and algorithms to more realistic flight scenarios. Consequently, in this paper, we not only present a minimal sensor suite and a simplified state estimation algorithm for FWMAVs, but also design a hardware-in-the-loop pipeline to characterize estimation performance. This pipeline recreates FWMAV flight experiments (Figure 1 (b)) using a UR5e (Figure 1 (c)) and adds realistic noise observed in flight experiments. This setup allows for rapid evaluation of algorithm accuracy on the considered hardware sensor components (Figure 1 (d)), achieving more accurate state estimation results than previous efforts. 1.1 Our Contributions In this paper, we propose a comprehensive state estimation algorithm and sensor suite that satisfies the RoboBee’s hardware constraints. Although we do not deploy the suite onboard the robot, we constrain hardware choices to those with opportunities for miniaturization. Specifically, we limit sensor selection to those that would fit within the approximately 50~{}\mathrm{mg} mass budget. The sensor suite incorporates an off-the-shelf nine-axis inertial measurement unit (IMU) with an accelerometer, magnetometer, and gyroscope (InvenSense ICM20948) and a time-of-flight (ToF) sensor (VL6180), shown in Figure 1 (a), that provides sensor measurements for a Complementary Extended Kalman Filter (CEKF), based on RoboBee dynamics. Additionally, we develop a testing pipeline using both simulation and hardware experiments, leveraging RoboBee flight experiments (as outlined in Figure 2) to validate that the accuracy of the algorithm does not degrade with the presence of body oscillatory modes. The Supplemental Video provides an overview of replayed trajectories utilized in this work as well as the mechanical noise generator designed to induce body oscillatory dynamics on the sensor suite. We observe that under RoboBee flight dynamics, the CEKF achieves state estimates with less than 1^{\circ} RMSE error in orientation and less than 2~{}\mathrm{mm} error in altitude with 16-bit fixed-point precision. Additionally, we approximate that a cycle of the proposed CEKF will require less than 11 microseconds on an onboard microcontroller for computations, which satisfies real-time latency requirements for the proposed state estimation algorithms. The rest of the paper is organized as follows. In Section 2 we derive the Complementary Extended Kalman Filter architecture for state estimation and evaluate the performance on simulated sensor data generated from open-loop RoboBee flight experiments. In Section 3 we select sensors that meet the SWaP constraints of the vehicle and characterize their performance. Section 4 presents our hardware-in-the-loop evaluation approach and the performance of the sensor suite and algorithm with RoboBee closed-loop dynamics. Finally, given the real-time latency requirement of state feedback for controllers, section 5 evaluates the required floating point operations (FLOPS) and the rate at which CEKF accuracy degrades for less precise floating point bit representations."
https://arxiv.org/html/2411.06377v1,"SymmeTac:
Symmetric Color LED Driven Efficient Photometric Stereo 
Reconstruction Methods for Camera-based Tactile Sensors","Camera-based tactile sensors can provide high-density surface geometry and force information for robots in the interaction process with the target. However, most existing methods cannot achieve accurate reconstruction with high efficiency, impeding the applications in robots. To address these problems, we propose an efficient two-shot photometric stereo method based on symmetric color LED distribution. Specifically, based on the sensing response curve of CMOS channels, we design orthogonal red and blue LEDs as illumination to acquire four observation maps using channel-splitting in a two-shot manner. Subsequently, we develop a two-shot photometric stereo theory, which can estimate accurate surface normal and greatly reduce the computing overhead in magnitude. Finally, leveraging the characteristics of the camera-based tactile sensor, we optimize the algorithm to be a highly efficient, pure addition operation. Simulation and real-world experiments demonstrate the advantages of our approach. Further details are available on: https://github.com/Tacxels/SymmeTac.","I INTRODUCTION Tactile perception [1] is significant and indispensable for robots, especially for dexterous grasping and delicate manipulation in industry and daily life. Unlike classical electric-based tactile sensors [1], camera-based tactile sensor overcomes the challenge of density, signal stability and processing [2, 3], and has attracted widespread attentions. Camera-based tactile sensors utilize camera to observe the contact deformation of its soft layer under illuminations, reconstruct the geometry of target surface and further estimate the contact force distribution, which can provide high-density, stable and multi-modality information with low-cost hardware [4]. With the help of advanced algorithms, it can support rich downstream robot tasks [3, 5, 6] and drive the development of embodied AI [7]. Fast and accurate geometric reconstruction algorithm is significant for camera-based tactile sensors [8], which not only provides shape and texture of target surface [9], but also builds the basis for subsequent force estimation with mechanical model [10]. However, efficiently acquiring accurate surface geometries is still facing challenges for camera-based tactile sensors. There are mainly two approaches for camera-based tactile sensors to calculate the surface geometries. Model-based methods take photometric stereo (PS) and variants to accurately reconstruct the deformation surface of sensors [11]. However, the sequential observation process under different monochromatic lights seriously decreases the efficiency. Look-up table methods take chromatic lights (R,G,B) to illuminate the deformation surface from different directions, and build the mapping relationship between the observed intensity and surface gradient (G_{ix},G_{iy})=\mathcal{F}(r_{i},g_{i},b_{i}) for each pixel i [12, 13]. Nevertheless, the table calibration is easily disturbed by imaging error or edge detection errors [9], and the nonuniform illumination strongly affects the accuracy. If one wants to increase the accuracy, the table size will greatly expand and decrease the reconstruction accuracy. How to simultaneously achieve accurate, general and efficient reconstruction is still one of the biggest challenges of camera-based tactile sensors. Figure 1: (a) Camera-based tactile sensor with symmetric illuminations. (b) Camera response curve supports channel splitting. (c) Proposed two-shot, highly efficient PS method and the reconstructed surface normal map. To address these problems, we propose SymmeTac, a highly efficient PS reconstruction methods for camera-based tactile sensors with colorful symmetric illuminations, as shown in Fig. 1. Inspired by the response feature of the color camera CMOS, we could effectively split the Red channel and Blue channel as two independent observations under red and blue LEDs separately. Then, we design the illumination as two group orthogonal and symmetric colorful lights, each group has a Red and a Blue LED separated by 90 degrees, which can provide 4 circumferential observations with only 2 lighting. Finally, we build a two-shot photometric stereo model to reconstruct the surface normal, which can model the albedo of each wavelength band and achieve highly efficient reconstruction with simple calculations. With proposed illumination design and model, camera-based tactile sensors can reconstruct surface with general and accurate model-based method, and more importantly, it is highly efficient. The contributions of this work are concluded as follows: • A highly efficient two-shot PS method is proposed; • The spectral characteristics of camera and the positional prior of LED are fully explored and utilized; • The performance on simulation and real-world camera-based tactile sensor is comprehensively evaluated. The rest of this paper is organized as follows: Sec. 2 introduces the related works, Sec. 3 describes the theory of the proposed method, Sec. 4 presents the simulated and real-world experiments, and Sec. 5 concludes the contributions, limitations, and future works of this paper."
https://arxiv.org/html/2411.06319v1,"Impact-Aware Robotic Manipulation: 
Quantifying the Sim-To-Real Gap for Velocity Jumps","Impact-aware robotic manipulation benefits from an accurate map from ante-impact to post-impact velocity signals to support, e.g., motion planning and control. This work proposes an approach to generate and experimentally validate such impact maps from simulations with a physics engine, allowing to model impact scenarios of arbitrarily large complexity. This impact map captures the velocity jump assuming an instantaneous contact transition between rigid objects, neglecting the nearly instantaneous contact transition and impact-induced vibrations. Feedback control, which is required for complex impact scenarios, will affect velocity signals when these vibrations are still active, making an evaluation solely based on velocity signals as in previous works unreliable. Instead, the proposed validation approach uses the reference spreading control framework, which aims to reduce peaks and jumps in the control feedback signals by using a reference consistent with the rigid impact map together with a suitable control scheme. Based on the key idea that selecting the correct rigid impact map in this reference spreading framework will minimize the net feedback signal, the rigid impact map is experimentally determined and compared with the impact map obtained from simulation, resulting in a 3.1% average error between the post-impact velocity identified from simulations and from experiments.","Humans are naturally skilled in exploiting intentional impacts, as becomes apparent from many activities we perform daily, such as running, jumping, and catching a flying object. The growing field of impact-aware robotics focuses on having robots exploiting intentional impacts for a gain in performance, such as reduced execution time, for locomotion (i.e., jumping [1] or running [2, 3]) and for manipulation (i.e. hitting [4], catching [5] or fast grabbing [6, 7]). Dealing with intentional impacts requires tackling challenges in different areas of research, including motion planning [8], impact detection and classification [9], and control [10]. The availability of validated impact models is a key enabler to push beyond the state-of-the-art for each of these three research areas. Examples for impact-aware motion planning include [11], which uses an impact model to plan an impact motion that prevents immediate post-impact slippage, and [12], which uses an impact model to execute a hitting motion with a given desired post-impact velocity. For impact classification, [13] uses an impact model to distinguish between expected and unexpected impacts, with the aim of understanding whether a recovery action or safety stop should be triggered. In the field of impact-aware control, [14] and [15] use ante- and post-impact references compatible with an impact model in order to remove undesired jumps in the tracking error when transitioning from the ante- to the post-impact state. Figure 1: Depiction of the dual-arm robotic setup used throughout the validation approach presented in this work. Impacts between robots and their environment can be modeled at different levels of abstraction as highlighted by [16], which distinguishes between: (a) A compliant contact model with a flexible joint transmission model, taking into account the finite time duration of the impact (usually in the order of several milliseconds) and post-impact vibrations in the drivetrain. (b) A nonsmooth contact model [17, 18] with a flexible transmission model, assuming an instantaneous contact transition while still accounting for post-impact vibrations in the drivetrain. (c) A rigid impact map, which combines a nonsmooth contact model with a rigid transmission model, assuming an instantaneous contact transition without oscillatory response in the drivetrain. It is noticeably challenging to accurately predict the transient velocity response shortly after the impact due to uncertainty of contact stiffness parameters for model (a), uncertainty of drivetrain stiffness and damping parameters for models (a) and (b), and other unmodeled vibrations in e.g. the environment or the robot structure. This challenge further increases when dealing with ideally simultaneous impacts, as is the case with jumping, dual-arm grabbing, or tasks with impacts between large surfaces. An unavoidable loss of simultaneity of the impacts can cause unplanned and unpredictable intermediate jumps in the transient velocity response before the velocities converge to a more predictable post-impact state once the impact sequence is finished [19]. Especially in scenarios with simultaneous impacts, model (c) of the rigid impact map is therefore the only reliable and robust model one can have. A common rigid impact model, for example used in [11, 13, 14], is the analytical impact model, which maps the ante-impact configuration and velocity into a post-impact velocity through a closed-form expression, while assuming the post-impact configuration remains equal due to the infinitesimal impact duration. However, for complex manipulation tasks, such as dual-arm grabbing with two 7 degree of freedom (DOF) robotic arms as perfomed in this work and shown in Figure 1, an analytical impact model cannot be easily derived. An alternative is presented in [20], which computes the rigid impact map by simulating the impact event using a compliant contact model. Such a compliant contact model can, however, be computationally expensive for realistic use case scenarios involving many objects and robots due to the small timestep required for accurate results. Especially in use cases where impact maps are determined during robot operation, the resulting large computation time is undesired. Furthermore, compliant contact models demand challenging estimation of stiffness and damping contact parameters. Another alternative is presented in [15], where the rigid impact map is extracted from physical experiments. This, however, requires a training procedure and thus prevents the manipulation of previously unhandled objects or in unexplored impact configurations. This work is centered around the idea of extracting the impact map from a physics engine based on a time-stepping method [21], which uses the theory of nonsmooth mechanics to efficiently evaluate the inelastic rigid impact map without requiring an estimation of contact stiffness and damping parameters. The extracted rigid impact map should, however, be experimentally validated before it can be trusted to be used reliably, especially considering prior work questioning the validity of rigid partially elastic impact models for simultaneous impact events [22]. Prior work in validation for robot-environment impacts includes [23, 16], which focuses on a validation of an analytical impact map for single impacts. In particular, [23] found that the most accurate impact map was one where not only the scaled motor inertia but also the low-level torque control loop effect is accounted for through the reflected motor inertia. In [24], two physics engines (MuJoCo & Drake) are validated against real-life data for a jumping humanoid. Both these physics engines, however, use compliant contact models. A third physics engine, Bullet, which does use a nonsmooth contact model description, was also compared in [24] for cube tossing. However, no simulations for robot-environment impacts were presented in Bullet due to its inability to account for the reflected motor inertia of the robot. In [25], a validation of the physics engine AGX Dynamics, which uses a time-stepping approach with nonsmooth contact models, is performed for a box tossing application. However, no validation is performed for impacts between robots and their environment. The main contribution of this work is, firstly, a demonstration that rigid impact maps for impacts between robots and their environment can be extracted from a physics engine that uses nonsmooth contact models. Secondly, we present an experimental validation to check the quality of these predictions for different reflected motor inertia models. This validation will be provided for manipulation scenarios with simultaneous impacts for which, due to their complexity, computing an analytical contact model is undesirable or impossible, such as fast dual-arm grabbing of objects. Due to the presence of impact-induced oscillations in experiments that are not modeled in the rigid impact map, one cannot consider the velocity directly after an impact as representative for this impact map. Additionally, due to the challenging nature of the tasks, feedback control is required for successful task execution, causing the velocity to converge to whatever post-impact reference is provided before the impact-induced oscillations vanish. Hence, a validation of the impact map based solely on a comparison of the velocity jump between experiments and simulations, as in [16, 23], is not possible. The validation procedure followed in this work makes use of the framework of reference spreading (RS) [26, 27, 28]. RS presents a way to reduce control input peaks and steps caused by so-called error peaking in traditional tracking control. This phenomenon concerns the problem that, under traditional tracking control, an unavoidable mismatch between the actual impact time and the predicted impact time will cause peaks in the velocity tracking error, and in the control inputs as a result [29, 30, 31]. By combining 1) an ante- and post-impact reference formulation that matches the rigid impact map with 2) an extension of these references past the nominal impact time and 3) a switching control policy based on impact detection, input peaks and steps are removed, minimizing control feedback during and after the impact sequence. The introduction of an interim-impact mode has additionally extended RS to be effective in the presence of simultaneous impacts as has been demonstrated in numerical simulation studies [19, 32] and in experimental work [15]. This paper aims to validate rigid impact maps by experimentally validating whether using the post-impact velocity prediction obtained from a simulation in an RS-based control framework causes the minimal feedback control effort, compared to using artificially different post-impact velocity predictions. This analysis is based on the theory that, if the post-impact velocity prediction is correct, the post-impact reference velocity in the RS control framework will match the actual velocity. This in turn results in a control effort that is dominated by feedforward, and contains little feedback due to the low velocity tracking error. By evaluating the integral of the feedback signal during and after the impact sequence for the different post-impact velocity predictions, an indirect experimental estimation of the optimal post-impact velocity is obtained, which is then compared against the predicted post-impact velocity to assess the validity of the physics engine. We dedicate special attention to the effect that the motor inertia has on the rigid impact map by comparing the experimentally identified optimal post-impact velocity against the predicted post-impact velocities obtained through simulations using different motor inertia models, confirming the conclusions of [23] also for complex simultaneous impact tasks. In the remainder of this paper, we introduce in Section II the equations of motion of the robots used in the experimental evaluation, followed in Section III by an overview of the simulation framework used to generate the rigid impact map. In Section IV, we present the RS-based control strategy used in the validation approach with details on the reference generation procedure and the actual control approach. We describe the methodology regarding the validation of the rigid impact map in more detail in Section V, followed by the experimental results in Section VI and the conclusion and recommendations for future research in Section VII."
https://arxiv.org/html/2411.06306v1,Optimal Driver Warning Generation in Dynamic Driving Environment,"The driver warning system that alerts the human driver about potential risks during driving is a key feature of an advanced driver assistance system. Existing driver warning technologies, mainly the forward collision warning and unsafe lane change warning, can reduce the risk of collision caused by human errors. However, the current design methods have several major limitations. Firstly, the warnings are mainly generated in a one-shot manner without modeling the ego driver’s reactions and surrounding objects, which reduces the flexibility and generality of the system over different scenarios. Additionally, the triggering conditions of warning are mostly rule-based threshold-checking given the current state, which lacks the prediction of the potential risk in a sufficiently long future horizon. In this work, we study the problem of optimally generating driver warnings by considering the interactions among the generated warning, the driver behavior, and the states of ego and surrounding vehicles on a long horizon. The warning generation problem is formulated as a partially observed Markov decision process (POMDP). An optimal warning generation framework is proposed as a solution to the proposed POMDP. The simulation experiments demonstrate the superiority of the proposed solution to the existing warning generation methods.","I INTRODUCTION The road traffic plays an important role in people’s lives. With the development of the complexity of city road networks, it is crucial for an advanced driver assistance system to be able to alert the potential risks to the human driver during driving. As shown in the studies of the human driver behavior with the warning system [1, 2, 3], existing driver warning technologies, mainly the forward collision warning and unsafe lane change warning, can reduce the risk of collision caused by human errors. However, studies show that the human drivers’ reactions to warnings vary with the type of warning and different drivers [4, 5], while the existing methods have not addressed this phenomenon adequately. Most methods in the literature mainly generate the warning in a one-shot manner without modeling the ego driver’s reactions and surrounding objects [6, 7, 8], which reduces the flexibility and generality of the system over different drivers and scenarios. Meanwhile, the triggering conditions of warning are mostly rule-based threshold-checking based on the current state, such as the time-to-collision (TTC) and the minimum safety distance [9, 10, 11], which lacks the prediction of the potential risk in a sufficiently long future horizon. As a consequence, the current warning systems, while effective in preventing collisions, tend to prompt urgent and uncomfortable braking actions. Studies have emphasized the importance of executing smoother and more comfortable braking maneuvers to assist drivers in avoiding not only identified dangers but also collisions with subsequent vehicles [12]. This work seeks to address these issues by formulating an optimal warning generation problem that considers the relation between the generated warning and the driver’s reaction and also the interaction between the ego vehicle and other agents on a long horizon. The problem is modeled as a partially observed Markov decision process (POMDP), in which we quantify the value of warnings through the comfort and safety of the future ego vehicle trajectory in the context of surrounding objects, and the cost through their format and frequency. An optimal warning generation framework is proposed as a solution to the POMDP. The key contributions of this work are as follows: - We propose a novel formulation of the optimal warning generation problem that considers the driver and surrounding vehicle reactions, and both the safety and the comfort of future ego trajectories. - We propose a warning generation framework combining driver behavior estimation as the solution to the above problem. The framework has the flexibility to incorporate any prediction models of the driving scenario. - The proposed method is evaluated over comprehensive closed-loop simulation experiments, which demonstrates the superiority of the proposed solution to the existing warning generation methods."
https://arxiv.org/html/2411.06303v1,TiniScript: A Simplified Language for Educational Robotics,"TiniScript is an intermediate programming language designed for educational robotics, aligned with STEM principles to foster integrative learning experiences. With its minimalist single-line syntax, such as F(2, 80), TiniScript simplifies robotic programming, allowing users to bypass complex code uploading processes and enabling real-time direct instruction transmission. Thanks to its preloaded interpreter, TiniScript decouples programming from hardware, significantly reducing wait times. Instructions can be sent wirelessly from any Bluetooth-enabled device —such as tablets, computers, or phones— through a serial port, making TiniScript adaptable to a variety of robots. This adaptability and ease of use not only optimize iterative and collaborative learning but also allow students to focus on creative aspects of robotics. This paper explores TiniScript’s design principles, syntax, and practical applications, highlighting its potential to make robotics programming more accessible and effective in developing critical thinking skills.","Keywords TiniScript, Educational Robotics, STEM Education, Programming Languages, Block-Based Programming, Microcontroller Programming."
https://arxiv.org/html/2411.06294v1,Hierarchical Performance-Based Design Optimization Framework for Soft Grippers,"This paper presents a hierarchical, performance-based framework for the design optimization of multi-fingered soft grippers. To address the need for systematically defined performance indices, the framework structures the optimization process into three integrated layers: Task Space, Motion Space, and Design Space. In the Task Space, performance indices are defined as core objectives, while the Motion Space interprets these into specific movement primitives. Finally, the Design Space applies parametric and topological optimization techniques to refine the geometry and material distribution of the system, achieving a balanced design across key performance metrics. The framework’s layered structure enhances SG design, ensuring balanced performance and scalability for complex tasks and contributing to broader advancements in soft robotics.","Soft robotic grippers (SGs) leverage the flexibility and compliance of soft actuators, offering a safe and adaptable solution for robotic manipulation across diverse environments. Unlike traditional rigid grippers, which are limited by their mechanical constraints, SGs harness soft materials and compliant structures to perform complex tasks, handling objects of various shapes, sizes, and materials with care and precision. This adaptability has positioned SGs as essential tools in a range of applications, from delicate handling in food processing to dynamic, unstructured object manipulation in fields like healthcare, wearables, and haptics, and robotics for human-machine interaction [1, 2]. Despite advancements in SGs, most current research centers on individual gripper configurations, primarily verified through experimental trials [3, 4, 5]. While optimization is sometimes applied, these efforts are typically limited to single mechanical objectives, like enhancing motion or force distribution, rather than addressing SG performance in a holistic manner [6, 7, 8]. This narrow focus highlights the need for a systematic, performance-driven framework that balances key functional goals across a wide range of tasks, ensuring SGs are designed for versatile and reliable manipulation. Design SpaceTask SpaceMotion SpaceHigh-Level ObjectiveMiddle-Level BehaviorLow-Level OptimizationPerformance Indices Bending & Twisting Primitives Optimization Model and ImplementationOptimizing Dexterity, Adaptability, Stability, and Reachability Figure 1: A multi-layered framework for SGs optimization, integrating task-level objectives, motion behavior, and design-space refinement to improve dexterity, adaptability, stability, and reachability. This paper proposes a performance-based design optimization framework to address this gap. The framework prioritizes an overarching goal of high-performance SGs, focusing on key performance indices (PIs)—dexterity, adaptability, stability, and reachability—that are essential for versatile manipulation. By structuring the optimization process around these indices, we aim to provide a comprehensive approach that integrates multi-objective optimization techniques, balancing competing design goals without an explicit prioritization of any single index. To achieve this, we introduce a hierarchical design architecture comprising three interconnected levels as illustrated in Fig.1. At the highest level, the Task Space defines the primary performance objective, establishing the indices that guide the design. The Motion Space translates these indices into specific movement primitives, such as bending and twisting, which are critical for achieving practical SG performance. Finally, at the lowest level, the Design Space applies parametric and topological optimization methods to refine the SG’s physical structure, ensuring that the design meets the targeted indices through an efficient and adaptable configuration of segments, chambers, and materials. Through this structured approach, our framework systematically addresses the complex interactions between SG geometry, actuation, and task-specific requirements. By emphasizing performance as the foundation of design, this framework offers a pathway for advancing SG capabilities across a wide range of applications, enhancing their utility in scenarios requiring dexterity, stability, and adaptability. The remainder of this paper is organized as follows: Section II details the defined PIs and introduces the hierarchical architecture underlying the proposed framework. Section III presents the optimization model, which incorporates multi-objective goals that align with the established indices. Section IV outlines the implementation of parametric and topological optimization methods and sensitivity analysis to enhance SG design. Finally, Section V discusses key findings, limitations, and future directions, followed by concluding remarks in Section VI."
https://arxiv.org/html/2411.06223v1,"Predictability Awareness For Efficient and Robust 
Multi-Agent Coordination","To safely and efficiently solve motion planning problems in multi-agent settings, most approaches attempt to solve a joint optimization that explicitly accounts for the responses triggered in other agents. This often results in solutions with an exponential computational complexity, making these methods intractable for complex scenarios with many agents. While sequential predict-and-plan approaches are more scalable, they tend to perform poorly in highly interactive environments. This paper proposes a method to improve the interactive capabilities of sequential predict-and-plan methods in multi-agent navigation problems by introducing predictability as an optimization objective. We interpret predictability through the use of general prediction models, by allowing agents to predict themselves and estimate how they align with these external predictions. We formally introduce this behavior through the free-energy of the system, which reduces (under appropriate bounds) to the Kullback-Leibler divergence between plan and prediction, and use this as a penalty for unpredictable trajectories. The proposed interpretation of predictability allows agents to more robustly leverage prediction models, and fosters a ‘soft social convention’ that accelerates agreement on coordination strategies without the need of explicit high level control or communication. We show how this predictability-aware planning leads to lower-cost trajectories and reduces planning effort in a set of multi-robot problems, including autonomous driving experiments with human driver data, where we show that the benefits of considering predictability apply even when only the ego-agent uses this strategy. The code and experiment videos can be found in the following page: https://romanchiva.github.io/PAProjectPage/","Many modern robotics applications involve autonomous agents navigating multi-agent environments where they will be required to interact with humans and other robots without full knowledge or extensive communication capabilities Wang et al. (2022). This involves planning trajectories in a complex system governed by a mix of rational and non-rational, stochastic and possibly game theoretic behaviors. To achieve safe and efficient interactions, agents need to reason about each other and coordinate. However, this poses critical challenges due to the high uncertainty associated with estimating other agents’s objectives Fisac et al. (2018) and a computational complexity that renders problems intractable for more than a handful of agents. Receding Horizon Trajectory Optimization allows for flexible and anticipative planning while ensuring compliance with e.g. safety constraints in multi-agent navigation problems. However, planning a trajectory that explicitly accounts for interactions among agents generally requires solving a joint optimization problem. A variety of joint planning methods can be found in literature, e.g. Huang et al. (2024); Chen et al. (2023), of which game theoretic approaches best capturing agent interaction complexities Wang et al. (2022). By modelling other agents as rational actors, game theoretic approaches cast the joint optimization as a constrained dynamic game and seek to find equilibrium solutions. Although this often results in stable and coordinated interactions Fisac et al. (2018); Cleac’h et al. (2020), game theoretic approaches suffer from the curse of dimensionality, as the planning complexity grows exponentially with the number of agents Schwarting et al. (2018). Additionally, modelling other agents as rational is a strong assumption which will not hold in practice, especially when interacting with human agents Colman (2003); Bossaerts and Murawski (2017). Alternatively, predict-and-plan approaches scale well with number of agents, however they tend to perform poorly in interactive environments. By separating prediction and planning, the problem simplifies to a single-agent collision avoidance problem with dynamic obstacles Brito et al. (2020); de Groot et al. (2023). The accuracy of the prediction model limits how well agents can coordinate. A system of interacting agents is highly complex, making it difficult to predict the diversity of possible futures, especially when considering interactions. This can lead to ambiguous predictions, making agents unable to anticipate their environment, and thus have to re-plan more often or engage in riskier behaviors Wang et al. (2022). Ideally, every agent in the environment would be able to accurately anticipate surrounding agents’ future trajectories allowing for efficient and safe interaction. Sequential planning agents use prediction models to avoid collisions with others, however, this fails to acknowledge that surrounding agents also hold predictions about the ego-agent, and plan their trajectory based on these predictions. Unless the optimal avoidance strategy falls within the range of predicted behaviors, other agents will react to the unexpected avoidance strategy by modifying their own trajectory. To mitigate this issue, we propose the following: in the same way a prediction model is used to predict other agents, the ego-agent can use it to approximate how other agents expect it to behave. This information can be used in planning to introduce a penalty for trajectories other agents will find surprising, bringing the optimal trajectory closer to the expectation surrounding agents hold. Accounting for predictability in this way mirrors the principle of free-energy minimization in active inference Sajid et al. (2021) (and control systems Theodorou et al. (2010)), where an agent not only seeks to maximize reward but also aims to minimize the discrepancy between some prediction model and observations. In multi-agent interactions Hyland et al. (2024), agents hold probabilistic beliefs about the behavior of others, and the accuracy of these beliefs is directly influenced by the agent’s own actions. By minimizing free energy, the agent balances actions that reduce uncertainty and confirm its internal model of the world with those that maximize reward. This approach ensures that the agent’s behavior is not only goal-directed but also aligned with maintaining coherent and accurate beliefs about the surrounding agents. 1.1. Contribution We explore how sequential planning agents can improve their coordination capabilities by accounting for the predictability of their planned trajectories. When a group agents accounts for predictability, they are able to foster a ‘soft social convention’ dictated by the prediction model which results in a decrease of uncertainty about the environment for all agents in the group. This helps agents resolve coordination problems without having to explicitly model interactions. Formally, the contribution of this paper is threefold: (1) We exploit ideas on free-energy to formulate a cost function that uses feedback from a prediction model to include predictability as an objective and analyze how this cost function can be integrated with a planner. (2) We provide results showing how our predictability awareness mechanism leads to ‘soft social conventions’ forming-based interaction strategies encoded in prediction models for multi-robot navigation problems. This allows agents to achieve smoother coordination by improving the effectiveness of prediction models in interactive environments. (3) Accounting for predictability causes agents to adopt social norms and pro-social behaviors encoded in learned prediction models, allowing to more closely mimic experts’ behaviors without needing cost function learning. We provide evidence for these behaviors in an experiment where an agent interacts with human drivers in scenarios from the Waymo Open Motion Dataset."
https://arxiv.org/html/2411.06183v1,"Sampling-Based Model Predictive Control for Dexterous 
Manipulation on a Biomimetic Tendon-Driven Hand","Biomimetic and compliant robotic hands offer the potential for human-like dexterity, but controlling them is challenging due to high dimensionality, complex contact interactions, and uncertainties in state estimation. Sampling-based model predictive control (MPC), using a physics simulator as the dynamics model, is a promising approach for generating contact-rich behavior. However, sampling-based MPC has yet to be evaluated on physical (non-simulated) robotic hands, particularly on compliant hands with state uncertainties. We present the first successful demonstration of in-hand manipulation on a physical biomimetic tendon-driven robot hand using sampling-based MPC. While sampling-based MPC does not require lengthy training cycles like reinforcement learning approaches, it still necessitates adapting the task-specific objective function to ensure robust behavior execution on physical hardware. To adapt the objective function, we integrate a visual language model (VLM) with a real-time optimizer (MuJoCo MPC). We provide the VLM with a high-level human language description of the task, and a video of the hand’s current behavior. The VLM iteratively adapts the objective function, enabling effective behavior generation. In our experiments, the hand achieves an average ball rolling speed of 0.35 rad/s, successful ball flips, and catching with a 67% success rate. Our results demonstrate that sampling-based MPC is a promising approach for generating dexterous manipulation skills on biomimetic hands without extensive training cycles.111Video with experiments: https://youtu.be/6ivbd_jijHA","Robotic hands have the potential to achieve human-like dexterity, but effectively controlling them poses significant challenges due to their high-dimensionality and complex contact states. Biomimetic hands with compliant actuation offer advantages; however, they also bring additional challenges, e.g., the difficulty of estimating the state of compliant links. Overcoming these challenges would allow biomimetic robotic hands to become the ideal universal manipulation platform in human-centric environments. Reinforcement learning (RL) methods have shown success in dexterous manipulation tasks on robotic hands, but are limited by long training times and the need for task-specific retraining. Imitation learning is also emerging as an alternative, but as of now, it still requires several hours of robot teleoperation data to master dexterous manipulation tasks. Figure 1: Overview of our sampling-based model predictive control approach and demonstration on a biomimetic tendon-driven hand. Showing experimental results for in-hand ball rolling and ball flipping, both in simulation and on the physical hardware. Sampling-based MPC offers a flexible alternative that excels in contact-rich manipulation tasks without the need for retraining. However, its application has been largely limited to simulations and has not yet been shown to work on physical robot hands. Our work addresses this gap by integrating a visual language model (VLM) with sampling-based MPC, enabling rapid adaptation of task-specific objective functions and generation of robust manipulation behaviors on a biomimetic tendon-driven robotic hand. I-A State of the art in dexterous manipulation Learning-based methods have shown great promise in dexterous manipulation. OpenAI [andrychowicz2020learning] achieved a breakthrough by using RL to demonstrate cube rotation with the Shadow Hand [ShadowRobotCompany2023ShadowSeries]. They applied domain randomization, which varied the simulation environment to help the robot perform better in real life. OpenAI also introduced [akkaya2019solving] automatic domain randomization (ADR), enabling the robot to solve a Rubik’s cube. The use of GPU-based simulators like Isaac Gym has sped up the training process. Allshire et al. [allshire2022transferring] showed in-hand cube manipulation using the TriFinger [Wuthrich2020TriFinger:Dexterity] robot. Similarly, Arunachalam et al. [Arunachalam2022DexterousManipulation] and Qin et al. [Qin2022FromTeleoperation] utilized imitation learning (IL) to perform tasks on the Allegro Hand [W.Robotics2023AllegroRd] like cube rotation and object flipping, using demonstrations captured with RGB and depth (RGB-D) cameras. Handa et al. [handa2023dextreme] achieved cube reorientation using the Allegro Hand, with only 8 GPUs, outperforming earlier systems that relied on thousands of CPU cores. Their use of vectorized ADR enhanced the robot’s ability to generalize by adding random variations that did not depend on physics. Yin et al. [Yin2023RotatingTouch] used tactile feedback to enable the Allegro hand rotate objects without visual input. Finally, Toshimitsu et al. [Toshimitsu2023GettingJoints] developed the Faive Hand, a biomimetic tendon-driven robotic hand. They demonstrated in-hand ball rotation after just one hour of training on a single NVIDIA A10G GPU. A learning-based alternative to reinforcement learning, behavioral cloning (BC), is quickly emerging as human manipulation datasets are becoming available. Mandikal et. al. has even shown successful imitation learning of dexterous tasks from unstructured YouTube videos [mandikal2022dexvip], but these methods are mostly focusing on simulation and need hours of data from every single task, which might not be available. Sampling-based MPC offers a promising alternative for dexterous manipulation where per-task retraining is not necessary. Bhardwaj et al. [bhardwaj2022storm] successfully controlled high-degree-of-freedom robot arms for real-time tasks. Howell et al. [howell2022predictive] achieved in-hand cube rotation in simulation using the MuJoCo physics engine on CPUs. Pezzato et al. [pezzato2023sampling] used the GPU-parallelizable simulator Isaac Gym for non-prehensile manipulation with a physical robotic arm. While model-based control methods like MPC can solve complex tasks, they are computationally demanding and require precise modeling. Sampling-based methods do not depend on gradients, making them suitable for nonlinear and discontinuous dynamics. However, computational complexity limits its success in high-dimensional tasks. Current literature indicates that sampling-based MPC has not been applied to dexterous in-hand manipulation on physical robotic hands. I-B State of the art in objective function design via LLMs Objective functions are essential for directing robotic behavior and optimizing tasks. However, 92% of researchers rely on manual trial-and-error methods for reward design, often yielding sub-optimal results [Booth2023TheSpecifications]. Recent advancements in multimodal large language models (LLMs), and particularly VLMs, offer new solutions to these challenges. Ma et al. [ma2023eureka] introduced Eureka, which leverages GPT-4’s in-context learning to automate reward function design. This iterative process incorporates human language feedback to optimize rewards until satisfactory performance is achieved. Eureka successfully demonstrated pen spinning on a simulated Shadow Hand. Yu et al. [yu2023language] used GPT-4 for objective function design executed via sampling-based MPC. Their method facilitates human interaction by translating natural language commands into reward code, successfully achieving object grasping and drawer opening with a 7-degree-of-freedom arm and a jaw gripper. Liang et al. [liang2024learning] further streamlined this process by embedding motion descriptions within a single prompt for reward code. This improved the teaching success rate for unseen tasks by 26.9% and reduced human corrections, achieving successful object grasping with a physical 7-DoF arm and a jaw gripper. I-C Approach We use a tendon-driven robotic hand with bio-inspired rolling contact joints. This joint design enhances compliance but also introduces challenges in state estimation and modeling. We use sampling-based MPC with MuJoCo as the dynamic model, which simplifies task adaptation by allowing rapid modifications to task-specific objective functions. RL and BC methods require separate training for each task, while our approach can directly execute new objective functions. This allows an efficient integration into an evolutionary optimization loop without extensive retraining. We exploit the in-context learning capabilities of a VLM (GPT-4o) to perform evolutionary optimization of objective function weights with video feedback. We investigate dexterous manipulation tasks such as ball rolling, flipping, and catching. Our contributions to the existing literature are: • We demonstrate sampling-based MPC on a new set of dynamic dexterous manipulation tasks, both in simulation and on a physical robot. • We show that adaptive dynamic in-hand manipulation is possible with a tendon-driven bio-inspired hybrid rigid-soft robotic hand. • We introduce a framework that enables in-context learning to perform evolutionary optimization of objective function weights with video feedback."
https://arxiv.org/html/2411.06182v1,IDF-MFL: Infrastructure-free and Drift-free Magnetic Field Localization for Mobile Robot,"In recent years, infrastructure-based localization methods have achieved significant progress thanks to their reliable and drift-free localization capability. However, the pre-installed infrastructures suffer from inflexibilities and high maintenance costs. This poses an interesting problem of how to develop a drift-free localization system without using the pre-installed infrastructures. In this paper, an infrastructure-free and drift-free localization system is proposed using the ambient magnetic field (MF) information, namely IDF-MFL. IDF-MFL is infrastructure-free thanks to the high distinctiveness of the ambient MF information produced by inherent ferromagnetic objects in the environment, such as steel and reinforced concrete structures of buildings, and underground pipelines. The MF-based localization problem is defined as a stochastic optimization problem with the consideration of the non-Gaussian heavy-tailed noise introduced by MF measurement outliers (caused by dynamic ferromagnetic objects), and an outlier-robust state estimation algorithm is derived to find the optimal distribution of robot state that makes the expectation of MF matching cost achieves its lower bound. The proposed method is evaluated in multiple scenarios111https://youtu.be/pTRD7SZRuA8, including experiments on high-fidelity simulation, and real-world environments. The results demonstrate that the proposed method can achieve high-accuracy, reliable, and real-time localization without any pre-installed infrastructures.","Localization is a fundamental task in developing autonomous robotic systems which has the potential to enable extensive industrial applications, such as construction[1], logistics[2], and underground mine exploration[3]. All of these missions rely on reliable state estimation for mobile robots. During the last decade, simultaneous localization and mapping (SLAM) technology[4, 5, 6] has been well applied to mobile robots. However, localization in enclosed or partially enclosed environments (e.g. corridors, industrial warehouses, carparks) still remains challenging and intractable due to the inevitable drift of the SLAM system. Even though the loop closure[7, 8] and bundle adjustment (BA) [9, 10, 11] techniques are able to correct drift, the state can change drastically when the drift is eliminated. The state transition is unacceptable for the mobile robots due to the causing of destabilization. To overcome the aforementioned limitations of SLAM systems, pre-installed infrastructures, such as ultra-wideband (UWB) anchor[12], radio frequency identifications (RFIDs)[13], and QR codes[14], are typically deployed in industrial scenarios. However, the installation and maintenance process of infrastructures-based localization systems is inflexible and costly which significantly limits the industrial application of the mobile robot system. Hence, inspired by natural animals, such as spiny birds and lobsters, that can sense their position and orientation using information from the local anomalies of earth MF [15, 16], an infrastructure-free localization system is investigated for mobile robots by using the local ambient MF. I-A Related Works In recent years, infrastructure-free localization systems have been amenable for wide-scale commercial use. A serial of priori-map-based SLAM methods is investigated to realize smooth and drift-free localization without any infrastructures. In [17], a real‐time high‐precision visual localization system is designed for autonomous vehicles that employ only low‐cost stereo cameras to localize the vehicle with a priori 3D LiDAR map. The drift of the visual odometry is eliminated by registering the visual point cloud to the pre-build LiDAR map through a probabilistic weighted Normal Distributions Transformation (NDT) [18]. Considering that visual feature detection and matching are unstable in texture-less or repetitive scenarios, a semantic pre-build map is adopted in [19] which contains typical features in parking lots, such as guide signs, parking lines, speed bumps, etc. However, the priori-map-based SLAM methods typically degrade significantly or even become unobservable in conditions of the visual/LiDAR odometry unreliable caused by poor illumination or self-similar scenes. To overcome this, an ambient MF-based localization system has become as a viable alternative for infrastructure-free localization thanks to the distinctiveness and pervasiveness of MF distortions caused by ferromagnetic objects. The use of MF as a source of SLAM is a promising novel approach. To achieve MF-based SLAM, the MagSLAM [20] employs a grid-based spatial discretization methodology and assumes the MF intensity in one grid with respect to the same distribution. In [21], a pedestrian dead reckoning (PDR)-aided MF SLAM is investigated which represents the MF map with a reduced-rank Gaussian process[22] using the Laplace basis functions, and a Rao-Blackwellized particle filter (PF) is adopted to compensate for position drift in PDR. To further improve the real-time performance of [21], the PF is replaced by extended Kalman filter (EKF) in [23] thanks to the derivation of the MF gradient expressions. However, similar to the visual/LiDAR SLAM [6, 4], the MF SLAM also suffers from drift when the pre-build MF map is unavailable. A series of works investigate the process of offline MF map construction through the bilinear interpolation [24], Gaussian process [25], and learning-based MF prediction [26]. In [27], a drift-free MF-based localization algorithm is proposed by fusing the MF-based localization with PDR through the EKF, where the MF-based localization algorithm is designed to estimate the rigid transformation that optimally aligns the MF measurements with the pre-build MF map through the Gauss-Newton optimization. Considering the potential MF gradient flatness region in real-world environments, which can lead to the degeneracy of gradient-based state estimation (e.g. Gauss-Newton, Levenberg-Marquardt, and Kalman filter) [28], PF [29, 30, 31] is well-applied thanks to its excellent performance in non-convex optimization. In [32], a complete MF localization system is designed by integrating offline MF map construction, PDR odometry, and a dynamic time warping (DTW)-based MF sequence matching [33] into an indoor pedestrian localization framework. Both [27, 29, 30, 31, 32] are wheel odometry/PDR-aided MF localization systems, which limited them can only deploy on certain robot platforms (legged robots or robots equipped with the wheel encoder). To overcome this problem, a few works [34, 35, 36] paid attention to the pure MF localization algorithm. In [34] and [35] the MF-based localization is realized through the classical K-Nearest Neighbors (KNN) algorithm. However, the KNN-based MF matching is typically limited by the similar-sequential-route assumption (the online localization route should be similar to the route used in the offline MF map construction process) due to the 3D MF vector measured by a magnetometer at one single location is highly dependent on the orientation. In [36], a pure MF localization method is proposed with the maximum a posteriori (MAP) estimator which solves the similar-sequential-route limitation problem with a rotation-invariant MF descriptor. I-B Motivation and Contributions In view of the aforementioned analysis, despite the recent popularity of ambient MF-based state estimation research for infrastructure-free localization, most of the works are drift-suffered[20, 21, 23], auxiliary odometry-aided[27, 29, 30, 31, 32], or similar-sequential-route limited[34, 35]. However, drift is unacceptable for mobile robots in enclosed or partially enclosed environments, and the dependence on auxiliary odometry limits the application of the algorithm into certain robot platforms, while the similar-sequential-route limitation leads the robot to track with the fixed path. With the goal of designing a flexible drift-free localization system, in this work, a robust pure MF-based localization system is developed without any assistance from infrastructures. The key idea of the proposed infrastructures-free, drift-free localization system is to find the optimal distribution of state that makes the expectation of MF matching cost achieves its lower bound. The MF matching cost is designed to describe the difference between the real-time MF measurements and the pre-built MF map which integrates correct MF matching measurement and MF measurement outlier (ignored in traditional MF-based localization methods) simultaneously in a piecewise function. Different from the traditional MF-based localization methods[20, 21, 23, 27, 29, 30, 31, 34, 35, 36], which generally approximate the MAP problem with the Gaussian distribution assumption, the proposed method deals with non-Gaussian heavy-tailed noise (MF measurement outlier introduced) robustly by optimizing the expectation of cost function with a stochastic optimization algorithm. The main contributions of this paper are listed as follows: • A pure MF-based localization approach is proposed for mobile robots which is infrastructure-free, drift-free, and does not rely on any additional odometry information. • To deal with the non-Gaussian heavy-tailed noise, an outlier-robust state estimator is derived which can optimize the non-convex and noncontinuous state estimation problem with parallel sampling. • The practical implementations of the proposed stochastic estimator are illustrated in detail, including Monte-Carlo approximation, sampling dimension shrinking on manifolds, and cost-shifting strategy. The practicability and performance of the proposed MF-based localization system are extensively verified in both simulated and real-world experiments."
https://arxiv.org/html/2411.06121v1,SniffySquad: Patchiness-Aware Gas Source Localization with Multi-Robot Collaboration,"Gas source localization is pivotal for the rapid mitigation of gas leakage disasters, where mobile robots emerge as a promising solution. However, existing methods predominantly schedule robots’ movements based on reactive stimuli or simplified gas plume models. These approaches typically excel in idealized, simulated environments but fall short in real-world gas environments characterized by their patchy distribution. In this work, we introduce SniffySquad, a multi-robot olfaction-based system designed to address the inherent patchiness in gas source localization. SniffySquad incorporates a patchiness-aware active sensing approach that enhances the quality of data collection and estimation. Moreover, it features an innovative collaborative role adaptation strategy to boost the efficiency of source-seeking endeavors. Extensive evaluations demonstrate that our system achieves an increase in the success rate by 20\%+ and an improvement in path efficiency by 30\%+, outperforming state-of-the-art gas source localization solutions.","Rapid and accurate responses to gas leak incidents are essential for safeguarding human and environmental health, as leaked gases can rapidly create highly flammable or toxic conditions, posing significant risks of explosions and poisoning [1, 2, 3]. For instance, in the United States alone, 2,600 gas leakage incidents have been reported, with 328 resulting in explosions and 122 fatalities [4]. A key aspect of quick response requires localizing the gas source, which involves analyzing the concentration and distribution of the gas in the air to trace it back to its origin. With the knowledge of source locations, subsequent mitigation operations, such as shutting off valves or sealing the leaks, can be conducted more logically, efficiently, and safely [5]. Conventional gas source localization (GSL) solutions fall into two categories: (i) human expert-based solutions assign human operators to engage in affected areas. These laborious and perilous activities not only increase the risk of misinterpretation but also imperil the safety of the operators [6]; and (ii) wireless sensor network (WSN)-based methods utilize preinstalled static sensors to detect the gas source by monitoring gas concentration readings [7]. However, this approach is constrained by spatial resolution limitations, particularly in environments with extensive and intricate pipeline networks that present numerous potential leakage points. In this work, we aim to devise an effective strategy for collaboratively scheduling multiple olfactory robots to localize the gas source in real-world environments. Particularly, we utilize multiple robot’s activeness and collaborative capability to gather more information efficiently, enhancing the synergy between their sensing and scheduling abilities. Figure 1: Mobile olfactory robots autonomously search for and navigate towards the source of gas leakage. We take gas leakage in a factory with intricate pipelines as an example scenario, as shown in Fig. 1. Here, a fleet of robots equipped with gas concentration sensors and wind sensors are dispatched to search the surroundings and gather gas sensory data. By analyzing and interpreting the environmental conditions, the robots autonomously navigate to pinpoint the source of the gas leakage. However, translating this idea into a practical system is non-trivial and faces two challenges: \bullet The patchy nature of gas plumes confuses and traps the robots. The gas landscape is characterized by a patchy structure [8, 9], as evidenced by field measurements of gas concentration (see Fig. 2). As seen, gas plumes fragment into disjointed patches and form distinct and scattered areas. From the perspective of source-seeking robots, the concentration measured along the horizontal centerline exhibits pronounced fluctuations as the distance to the source increases, since gas patches are separated by regions where gas concentration is below detectable levels. This intermittent characteristic introduces uncertainty in determining the source direction when robots devise paths using noisy and local sensory data, ultimately leading to their entrapment within these patches and failure to complete GSL. Existing solutions, including bio-inspired [10, 11] and probabilistic model-based approaches [12, 13], either rely on local gas concentration gradient or oversimplified gas dispersion models, rendering them effective only in idealized scenarios with continuous gas concentration fields. \bullet Trade-off between source localization effectiveness and search efficiency for collaborative robots. Since gas distribution varies continuously over space and time, it is impossible to measure at every possible location all the time. Therefore, robots must strike a balance between two conflicting objectives within limited sampling: (i) identifying new potential gas source positions and (ii) excluding false positive source positions. Achieving these goals simultaneously complicates the localization of the true gas source efficiently and effectively. Specifically, the former goal necessitates extensive exploration and sampling of gas data, while the latter goal requires utilizing previously acquired information to reach and verify the estimated source position’s probability. Current multi-robot GSL solutions tend to exhibit either clustered or dispersed collaborative behaviors, thereby undermining the simultaneous achievement of both objectives [14, 11, 15]. Remark. As far as we are aware, previous works develop their operating principles ignoring the patchy nature of gas plumes, based on which they schedule agents without fully harnessing their versatility and collaborative capabilities at a system level. To tackle the above challenge, we design and implement SniffySquad, a collaborative olfactory robot scheduling system for gas source localization. Benefiting from SniffySquad, a team of olfactory robots can adapt to field patchiness and dynamically adjust their movements accordingly, enabling efficient and effective emission source localization. In general, SniffySquad excels in the following two aspects. \bullet At the individual level, we propose a Patchiness-aware active sensing method, which refines the probabilistic source estimation by incorporating the patchy characteristic of gas in each robot’s moving strategy. Inspired by the principles of Langevin MCMC [16], we adjust robots’ gradient-based moving direction with a regulation term, allowing them to escape false positive source positions and thereby collect more informative sensing data. \bullet At the team level, we design a Potential-instructed collaborative roles adaptation strategy that further enhances the source-seeking efficiency by adjusting robots’ roles based on their spatial distribution and measurements in the past. Each robot in the team adopts either a fine-grained search role to inspect spurious signals (i.e., exploiter) or a coarse-grained search role to discover potential new sources (i.e., explorer). These roles are adaptively adjusted based on the estimated probabilities of each robot’s proximity to the emission source, thereby parallelizing the exploration of the environment and exploitation of collected information in a flexible manner. Figure 2: Spatial characteristics of gas concentration. We conducted a proof-of-concept experiment to check gas characteristics in our indoor testbed, with a gas emission device at (x,y)=(1.0,0.0) and winds blowing along the x axis. The heatmap illustrates concentration values, representing the number of particles with a diameter >0.3um in 0.1L of air. The observed gas plume patches may mislead source-seeking robots into falsely identifying them as the actual gas emission source. We evaluate the performance of SniffySquad and compare it with state-of-the-art baseline methods through experiments on a real-time multi-robot testbed (12 hours) and extensive physical feature-based simulations (750 runs). Experiment results show that our system outperforms all baselines, achieving a 20\%+ success rate improvement and improving path efficiency by >30\%. We summarize the contributions of this paper as follows: • We propose SniffySquad, a collaborative multi-robot olfactory sensing system for accurate and efficient gas source localization, specifically designed to handle the patchy characteristics in real-world gas fields. • We introduce a patchiness-aware active sensing method that refines probabilistic source estimation by incorporating gas patchiness in each robot’s moving strategy. Building on this design, we further devise a potential-instructed collaborative roles adaptation strategy, which parallelizes the exploration of the environment and exploitation of collected information in an adaptive manner. • We develop a prototype system and evaluate SniffySquad through a real-world testbed and a physical-feature-based gas dispersion simulator. Extensive evaluation results show its effectiveness and superior performance. The remainder of the paper is organized as follows. We first introduce the background, motivation, and related works in Section II. In Section III, we present preliminaries, followed by the system overview in Section IV. In Sections V and VI, we introduce the algorithm design, involving detailed descriptions of the patchiness-aware active sensing and potential-instructed collaborative roles adaptation. Section VII showcases the implementation and evaluation. Finally, Section VIII concludes the paper."
https://arxiv.org/html/2411.06111v1,Energy-efficient Hybrid Model Predictive Trajectory Planning for Autonomous Electric Vehicles,"To tackle the twin challenges of limited battery life and lengthy charging durations in electric vehicles (EVs), this paper introduces an Energy-efficient Hybrid Model Predictive Planner (EHMPP), which employs an energy-saving optimization strategy. EHMPP focuses on refining the design of the motion planner to be seamlessly integrated with the existing automatic driving algorithms, without additional hardware. It has been validated through simulation experiments on the Prescan, CarSim, and Matlab platforms, demonstrating that it can increase passive recovery energy by 11.74% and effectively track motor speed and acceleration at optimal power. To sum up, EHMPP not only aids in trajectory planning but also significantly boosts energy efficiency in autonomous EVs.","The study of electric vehicles (EVs) has garnered significant attention from both industry and academia, driven by growing concerns about environmental issues [1]. Autonomous electric vehicles are presented as a practical solution to these challenges [2]. However, a widespread adoption faces hurdles such as limited battery life and prolonged charging times [3]. To tackle these challenges, researchers are exploring diverse solutions, including innovations in autonomous driving strategies and optimization of route planning algorithms to maximize energy utilization [4] [5] [6] [7]. Yet, current studies often overlook the interplay among the kinetic energy recovery systems (KERS), engine efficiency, and external environment. These interactions are distinguishing features of hybrid and electric vehicles, which can additionally be incorporated into vehicle driving strategies to improve energy-efficiency[8]. In prior energy strategy research, driving strategies to improving energy-efficiency have been widely proposed[9], but it is difficult for drivers to manually control vehicles according to these proposed strategies. The emerge of autonomous driving provides a satisfying solution to the above problems, effective vehicles controlled by algorithms can be more accurately planned and controlled based on strategies. The conversion of kinetic energy into electrical energy involves reversing the vehicle’s electric engine into functioning as a generator and subsequently charging the battery with the generated current [10]. This conversion of the kinetic energy losses serves as a viable method for energy recovery, enhancing system performance, improving energy conversion efficiency, and extending the mileage [11][12][13]. In recent years, kinetic recovery has emerged as a key focus of interest among researchers, designers, and manufacturers in the EV industry [14]. As a portion of the energy that is utilized to propel the electric vehicle is dissipated as braking energy loss during driving, recovering a portion of these losses can enhance the efficiency of energy extraction from the battery and prolong the mileage. An alternative approach to regenerative braking for kinetic energy recovery is the conversion of potential energy losses, which can be implemented by not activating the braking system when the vehicle decelerates. In such instances, the kinetic energy recovery system kicks in to recover as much kinetic energy as possible from the car. Therefore, KERS in energy recovery strategy is an important factor that has to be considered for improving the overall efficiency of EV operation. In this paper,we propose an energy-efficient Hybrid Model Predictive Planner (EHMPP), optimized based on the best energy efficiency strategies. This planner takes into account the operational state of the KERS during the planning process for EVs, enhancing their range and energy efficiency. We conducted simulation experiments in Prescan, Cars, and Matlab to validate the effectiveness of our proposed strategy. The results indicate that implementing EHMMP significantly enhances the energy efficiency of autonomous electric vehicles. Our main contributions to this article are as follows: • EHMPP is an EVs planner that operates within the constraints of existing hardware configurations, eliminating the need for supplementary hardware deployment. • Our simulation results demonstrate that the proposed strategy significantly enhances the vehicle’s energy efficiency during operation. Specifically, it boosts passive energy recovery by 11.74% during deceleration phases. Moreover, the strategy optimizes motor operation, ensuring it remains close to its ideal power state throughout acceleration, deceleration, and cruising phases, thereby improving overall energy efficiency. • EHMPP enhances flexibility by automatically selecting distinct cost functions for different motion states, surpassing traditional methodologies. This approach not only facilitates an adaptive planning but also serves as a valuable reference for deploying additional strategies."
https://arxiv.org/html/2411.06039v1,"To What Extent Does the Perceived Obesity Level of 
Humanoid Robots Affect People’s Trust in Them?","Despite obesity being widely discussed in the social sciences, the effect of a robot’s perceived obesity level on trust is not covered by the field of HRI. While in research regarding humans, Body Mass Index (BMI) is commonly used as an indicator of obesity, this scale is completely irrelevant in the context of robots, so it is challenging to operationalize the perceived obesity level of robots; indeed, while the effect of robot’s size (or height) on people’s trust in it was addressed in previous HRI papers, the perceived obesity level factor has not been addressed. This work examines to what extent the perceived obesity level of humanoid robots affects people’s trust in them. To test this hypothesis, we conducted a within-subjects study where, using an online pre-validated questionnaire, the subjects were asked questions while being presented with two pictures of humanoids, one with a regular obesity level and the other with a high obesity level. The results show that humanoid robots with lower perceived obesity levels are significantly more likely to be trusted.","A broad spectrum of literature has developed, emphasizing the importance of human-robot trust [1]. More importantly, studies suggest that the more important elements of trust are robot-related (e.g., robot attributes), namely, things that robot designers can directly manipulate [2]. Therefore, it is crucial to understand how simple changes in a robot’s design can improve, or at least not thwart, the potential for trust between humans and robots in their interaction. This paper presents an initial investigation of how a robot’s perceived obesity will affect people’s trust. Research in social sciences has tested the effect of physicians’ body weight on patients’ level of trust. For example, it has been found that health providers’ excess weight may negatively affect patients’ perceptions of their credibility, level of trust, and inclination to follow medical advice [3]. It should be noted that findings suggest the difference in trust is not necessarily related to medical counseling specifically regarding weight, but a general unwillingness of patients to follow the medical advice of overweight doctors, regardless of the medical field the medical advice is related to [4]. In this project, we aim to identify whether the same conclusions can be drawn in the context of robots; in other words, does weightism (bias or discrimination against people who are overweight) exist not only towards people but also towards humanoid robots? It should be noted that the effect of a robot’s size on people’s trust in it was addressed in previous HRI papers; various articles have researched the effects of a robot’s height on how people perceive and feel about the robot [5]. However, many articles have used the terms ”size” and ”height” interchangeably when referring to the dimensions of the robot [6]. In contrast, we wanted to study the effect of the perceived obesity of the robot."
https://arxiv.org/html/2411.05881v1,MIPD: A Multi-sensory Interactive Perception Dataset for Embodied Intelligent Driving,"During the process of driving, humans usually rely on multiple senses to gather information and make decisions. Analogously, in order to achieve embodied intelligence in autonomous driving, it is essential to integrate multidimensional sensory information in order to facilitate interaction with the environment. However, the current multi-modal fusion sensing schemes often neglect these additional sensory inputs, hindering the realization of fully autonomous driving. This paper considers multi-sensory information and proposes a multi-modal interactive perception dataset named ParallelBody, enabling expanding the current autonomous driving algorithm framework, for supporting the research on embodied intelligent driving. In addition to the conventional camera, lidar, and 4D radar data, our ParallelBody dataset incorporates multiple sensor inputs including sound, light intensity, vibration intensity and vehicle speed to enrich the dataset comprehensiveness. Comprising 126 consecutive sequences, many exceeding twenty seconds, ParallelBody dataset features over 8,500 meticulously synchronized and annotated frames. Moreover, it encompasses many challenging scenarios, covering various road and lighting conditions. The dataset has undergone thorough experimental validation, producing valuable insights for the exploration of next-generation autonomous driving frameworks. Data, development kit and more details will be available at https://github.com/BUCT-IUSRC/Dataset__MIPD.","The evolution of autonomous driving technology heralds a transformative era in transportation systems, where the depth and integration of environmental perception are central to enhancing road safety, easing traffic pressures, and optimizing energy utilization[1]. While autonomous driving has made commendable strides towards fully autonomous operations, its perceptual limitations continue to pose significant challenges for its broader adoption[2]. Thus, the incorporation of embodied intelligence into autonomous driving is critical, with interaction perception being a fundamental and pivotal concept. Interaction perception underscores the potential of multi-modal sensory data to enhance the safe and efficient interaction of vehicles with other traffic participants, enabling the acquisition and interpretation of environmental information. In the concept of interaction perception, vehicles not only passively receive information from their surroundings but also influence the environment and other traffic participants through their own behavior. This two-way interaction involves the acquisition, processing, and delivery of multidimensional sensory data. For example, interactive perception may include the vehicle receiving sensor data from its surroundings, including images, point clouds, light, and vibration. Furthermore, the vehicle is capable of influencing its surroundings and other traffic participants based on information about its own state, including data such as acceleration, steering angle, and so forth. It is possible for vehicles to predict the impact and to dynamically optimize and adjust their perceptual state based on their own state. The construction of a multidimensional perceptual dataset is a fundamental step in the realization of the goal of interactive perception. Notably, the deficiencies in current datasets to accurately capture and adapt to the dynamic variations of the real world represent formidable barriers to further advancements[3, 4]. These datasets often depend excessively on conventional visual and distance-sensing technologies like cameras, LiDAR, and radar[5, 6, 7, 8, 9]. Prominent datasets such as Kitti[10], NuScenes[11], Waymo[12], and Argoverse[13], although abundant in visual data, fall short in offering a holistic perception of the environment. This oversight limits autonomous systems from fully understanding the nuances of environmental changes, particularly the subtle variables like variations in lighting conditions across different weather scenarios or the differing responses of vehicles to varied road surfaces[19, 20]. Such limitations starkly curtail the systems’ capability to comprehend and adapt to intricate environments[22]. Furthermore, the complexity and variability of real-world driving conditions far exceed the scope of existing datasets, encompassing a spectrum of dynamic elements such as light intensity and road conditions[23, 24]. The existing data set is inadequate to meet the needs of autonomous driving technologies, which require a more comprehensive and dynamic compilation of environmental data[25]. This is essential for enhancing the adaptability and decision-making precision of autonomous driving technologies. In light of these challenges, an innovative multi-modal dataset design concept is introduced in this paper to address existing deficiencies. Considering that in complex transportation scenarios, people tend to perceive dynamic elements comprehensively through visual, auditory, and tactile multi-sensory synergies, the data collection platform was extended as shown in Fig. 1LABEL:sub@people+car. Our proposed dataset goes beyond conventional visual data to include camera imagery, lidar and 4D radar data, and importantly, it integrates diverse multi-dimensional information such as vibration and light intensity. This robust amalgamation of data aims to offer a more accurate and holistic environmental simulation, significantly bolstering the adaptive capabilities and situational awareness of autonomous driving systems. Our main contributions to this work are set out below: (1) A novel multi-modal dataset is proposed and constructed, integrating a variety of information such as camera data, point cloud information from lidar, 4D radar, sound, vibration, light intensity, and vehicle speed. This dataset is designed to comprehensively enhance perception tasks by fusing multi-sensory data to compensate for the lack of single-sensory information. (2) Our dataset consists of 126 consecutive sequences, most of which last more than twenty seconds and contain over 8,500 carefully synchronized and annotated frames. Additionally, our dataset includes challenging scenarios, such as various road conditions, weather conditions, lighting conditions, and more. (3) Experiments were conducted using multiple single-modal and multi-modal correlation models to validate the effectiveness of the collected dataset. The paper is organized as follows: In Section 2, a summary of previous research on different sensor configurations of multi-modal datasets, fusion perception models, and embodied perception is provided. Section 3 presents detailed information about the dataset, including sensor setup details, dataset labeling and visualization, and statistical analysis. The experiment details and the discussion of the results are presented in Section 4. In Section 5, the reasons for the significant degradation of the perception accuracy of existing perceptual models when confronted with complex scenarios are discussed. Furthermore, the limitations of the existing algorithms are analyzed and potential future research directions are outlined. Finally, in Section 6, a summary of the work is provided and the limitations of existing datasets in comparison to the strengths of the dataset are discussed."
https://arxiv.org/html/2411.05847v1,Federated Data-Driven Kalman Filtering for State Estimation,"This paper proposes a novel localization framework based on collaborative training or federated learning paradigm, for highly accurate localization of autonomous vehicles. More specifically, we build on the standard approach of KalmanNet, a recurrent neural network aiming to estimate the underlying system uncertainty of traditional Extended Kalman Filtering, and reformulate it by the adapt-then-combine concept to FedKalmanNet. The latter is trained in a distributed manner by a group of vehicles (or clients), with local training datasets consisting of vehicular location and velocity measurements, through a global server aggregation operation. The FedKalmanNet is then used by each vehicle to localize itself, by estimating the associated system uncertainty matrices (i.e, Kalman gain). Our aim is to actually demonstrate the benefits of collaborative training for state estimation in autonomous driving, over collaborative decision-making which requires rich V2X communication resources for measurement exchange and sensor fusion under real-time constraints. An extensive experimental and evaluation study conducted in CARLA autonomous driving simulator highlights the superior performance of FedKalmanNet over state-of-the-art collaborative decision-making approaches, in localizing vehicles without the need of real-time V2X communication.","Autonomous vehicles employ a variety of sensors such as cameras, LiDAR, GNSS (global navigation satellite systems), and IMUs (inertial measurement units) to perceive and interpret their environment. These vehicles are expected to be a fundamental component of future Intelligent Transportation Systems [1]. Moreover, vehicles enhance their perception capabilities beyond the individual sensor range through Vehicle-to-Vehicle (V2X) communication and 5G, allowing them to share crucial traffic information. Achieving precise 3D location awareness over time is essential for optimizing autonomous driving performance. A promising approach for enhancing location or situational awareness is to exploit collaboration among vehicles, either during training or the decision-making phase, relying on V2X information exchange [2, 3]. This approach becomes even more effective when the uncertainty of sensor measurements can be estimated using data-driven or deep learning techniques [4]. KalmanNet [5], a recurrent neural network (RNN) designed to estimate the uncertainty for a single agent through the principles of the Extended Kalman filter (EKF), will be used as a key module, exactly due to its interpretability and efficiency in capturing unknown system dynamics. This work will explore the potential of avoiding raw data exchange between vehicles, while still leveraging the information among connected agents. Specifically, we will investigate transitioning from collaboration during the decision-making phase to collaboration during the training phase, and the potential benefits of it. To be more concise, instead of exchanging raw information, the data collected by a group of vehicles can facilitate a continual learning paradigm in order to estimate sensor measurements uncertainty, thereby enhancing the accuracy of localization over time. Collaboration during training usually refers to a distributed scenario where clients jointly train models used for localization applications, using their own local models. In this federated learning (FL) scenario, a global server aggregates the local models and sends back to the clients the global model after some communication rounds [6, 7]. FedLoc [8, 9] is a very popular generic framework, focusing mainly on indoor localization scenarios of edge devices. However, despite its benefits, it requires extensive trainable parameters and large datasets, even for simple sequences, lacking the explainability of KalmanNet. Indoor localization based on WiFi measurements is also the focus of related FL works [10, 11]. Collaboration during the decision-making phase, usually refers to cooperative localization (CL) based on traditional optimization techniques. Understanding the statistics of measurement noise is crucial for enhancing location estimation accuracy [2]. Centralized methods, such as those using multidimensional scaling [12] or quadratically constrained quadratic problems [13], often either assume the noise covariance is known in advance or set it equal to the identity matrix. More practical distributed approaches, which utilize the concept of covariance intersection [14, 15], typically assume the true covariance matrices are known, without addressing how they can be estimated in practice. However, in all cases CL requires raw data exchange in order to localize vehicles, thus resulting in high communication costs and privacy issues. Thus, the challenge addressed in this paper is to design an explainable data-driven localization architecture that utilize the collaborative nature of FL in order to enhance autonomous vehicles localization. To address that challenge, this study combines FL with the inherent interpretable KalmanNet architecture. This novel integration promises high performance, due to diverse data shared by cooperating agents, as well as low computational complexity due to the explainable nature of KalmanNet. Moreover, motivated by distributed parameter estimation approaches [16], our method employs the adapt-then-combine (ATC) strategy. During the adaptation step, each vehicle utilizes its private dataset to train a local KalmanNet model. This model estimates the uncertainty of the specific vehicle’s measurements, which is then incorporated into the Kalman filter (KF) solution. In the combination step, we aim to develop a robust global KalmanNet model that integrates information across various vehicles operating in diverse environments, effectively learning the underlying system’s dynamics. Furthermore, our approach only requires sharing the weights of the local KalmanNet models. By exchanging and fusing the local models during training, we will achieve better performance than CL approaches. Therefore, the main contributions of this study can be summarized as follows: • Exploiting the ATC strategy, we reformulate standard KalmanNet to its FedKalmanNet counterpart, enabling the formulation of a highly efficient distributed learning framework for data-driven localization, limiting the need for data sharing and ensuring privacy protection. • The newly proposed collaborative training paradigm for autonomous vehicle localization is shown to outperform traditional optimization based CL approaches that require exchanging and fusing raw data in real-time V2X conditions. • Extensive numerical evaluations carried out in the renowned CARLA simulator [17] demonstrate the competitive advantages of the proposed federated data-driven localization approach, in terms of absolute pose error. Outline Section II introduces the preliminaries; Section III presents the proposed federated data-driven localization framework; Section IV is dedicated to experimental setup and results, while Section V concludes this work."
https://arxiv.org/html/2411.05821v1,"Benchmarking Vision, Language, & Action Models on Robotic Learning Tasks","Vision-language-action (VLA) models represent a promising direction for developing general-purpose robotic systems, demonstrating the ability to combine visual understanding, language comprehension, and action generation. However, systematic evaluation of these models across diverse robotic tasks remains limited. In this work, we present a comprehensive evaluation framework and benchmark suite for assessing VLA models. We profile three state-of-the-art VLM and VLAs —GPT-4o, OpenVLA, and JAT—across 20 diverse datasets from the Open-X-Embodiment collection, evaluating their performance on various manipulation tasks. Our analysis reveals several key insights: (1) current VLA models show significant variation in performance across different tasks and robot platforms, with GPT-4o demonstrating the most consistent performance through sophisticated prompt engineering, (2) all models struggle with complex manipulation tasks requiring multi-step planning, and (3) model performance is notably sensitive to action space characteristics and environmental factors. We release our evaluation framework and findings to facilitate systematic assessment of future VLA models and identify critical areas for improvement in the development of general-purpose robotic systems.","The development of generalizable robotic systems remains a central challenge in machine learning and robotics. While recent advances have enabled impressive capabilities in controlled settings, learned policies frequently fail to generalize beyond their training distribution. This limitation manifests across multiple dimensions: models struggle to respond appropriately to novel task instructions [4, 23], handle variations in object positions and orientations [3], adapt to changing lighting conditions or partially obscured scenes [6], interact with previously unseen objects, or maintain performance in the presence of distracting objects [25, 22]. This brittleness presents a significant barrier to deploying learned robotics systems in unconstrained real-world environments. Concurrent advances in foundation models, particularly in vision and language domains, offer a compelling potential solution to these challenges. Web-scale training has enabled models to realize SotA capabilities in visual recognition [15, 17], complex reasoning about object-agent interactions [2, 8, 24], code generation [5], and multimodal understanding. These models exhibit precisely the kinds of robust generalization capabilities that traditional robotics approaches have found challenging to achieve. The semantic reasoning, problem-solving, and visual interpretation capabilities demonstrated by these models would be tremendously valuable for developing generalist robots capable of performing diverse tasks in dynamic real-world environments. This direction aligns with an emerging trend in machine learning regarding the advantages of unified neural sequence models. Such models continue to show performance improvements even at the frontier of data, compute, and model scale [11, 10]. Moreover, historical trends suggest that generic models capable of effectively leveraging computation tend to eventually supersede specialized domain-specific approaches [21]. Unified sequence models offer several compelling advantages: they eliminate the need for hand-crafted policy architectures with domain-specific inductive biases, can leverage diverse training data through sequence serialization, and demonstrate consistent improvements with increased scale. However, significant challenges remain in applying these models to robotics. While foundation models are typically trained on billions of tokens and images from the web, collecting comparable quantities of robotic interaction data remains infeasible in the near term [7, 13]. Additionally, the direct application of foundation models to robotics tasks presents fundamental technical challenges: these models excel at semantic reasoning and high-level understanding, but robots require precise, grounded, low-level control actions, such as Cartesian end-effector commands. Recent work has attempted to bridge this gap by incorporating language models (LLMs) and vision-language models (VLMs) into robotics systems (Ahn et al., 2022; Driess et al., 2023; Vemprala et al., 2023). However, many of these approaches restrict foundation models to high-level planning, effectively using them as sophisticated state machines that parse commands into primitive actions, which are then executed by separate low-level controllers that cannot leverage the rich semantic knowledge encoded in the foundation models. More recent research has explored bootstrapping robotics representations using pretrained language and vision-language models [19, 18, 12]. Applying these modules in components in planning systems has also been explored [8, 20]. A promising direction has emerged in the form of vision-language-action models (VLAs), which often involve extending different kinds of pretrained foundation models for robotics by pretraining [3], cotraining [22] or fine-tuning [14, 4, 16] visually-conditioned language models to control actions for robots. These models have demonstrated notable transfer to new task scenarios, a compelling first step on the path towards generally useful policies for robots and agents. As these models continue to evolve, there is a critical need for systematic evaluation of their capabilities across both their intended multimodal training domains and out-of-distribution scenarios. Our primary contributions in this paper are: • Detailed profiling results for an initial set of VLM, VLA, and emerging ""generalist"" models, providing insights into their capabilities and limitations • Analysis of generalization • A systematic set of evaluation splits and metrics specifically designed for robotics learning tasks in the widely-used OpenX Dataset • A general framework for mapping VLMs to other modality classes, with particular emphasis on action spaces • Open-source software infrastructure for downloading, managing, and utilizing the benchmark data Through this work, we aim to provide the robotics learning community with robust tools and methodologies for assessing and comparing these emerging approaches, facilitating progress in this rapidly evolving field and helping to bridge the gap between foundation models and practical robotics applications. Importantly, this is the first foray into a new large scale generalist action model benchmark, which we discuss in the context of Future Work."
https://arxiv.org/html/2411.05799v1,"NeoPhysIx: An Ultra Fast 3D Physical Simulator 
as Development Tool for AI Algorithms","Traditional AI algorithms, such as Genetic Programming and Reinforcement Learning, often require extensive computational resources to simulate real-world physical scenarios effectively. While advancements in multi-core processing have been made, the inherent limitations of parallelizing rigid body dynamics lead to significant communication overheads, hindering substantial performance gains for simple simulations.This paper introduces NeoPhysIx, a novel 3D physical simulator designed to overcome these challenges. By adopting innovative simulation paradigms and focusing on essential algorithmic elements, NeoPhysIx achieves unprecedented speedups exceeding 1000x compared to real-time. This acceleration is realized through strategic simplifications, including point cloud collision detection, joint angle determination, and friction force estimation.The efficacy of NeoPhysIx is demonstrated through its application in training a legged robot with 18 degrees of freedom and six sensors, controlled by an evolved genetic program. Remarkably, simulating half a year of robot lifetime within a mere 9 hours on a single core of a standard mid-range CPU highlights the significant efficiency gains offered by NeoPhysIx. This breakthrough paves the way for accelerated AI development and training in physically-grounded domains.","I INTRODUCTION To develop effective robot controllers, optimize robots’ planning behavior, or enable offline learning techniques such as Reinforcement Learning, very fast and stable physical simulators are essential. These simulators must handle 3D rigid bodies, as well as various types of sensors and actuators, to accurately model complex robotic systems [1, 2, 3, 4, 5, 6, 7]. Over the years, simulation algorithms have been significantly optimized to deliver faster results [8]. Many simulators now meet the real-time performance criterion. However, with the 4.0 GHz clock frequency barrier for CPUs in place [9], further improvements in simulation speed have stagnated. Real-time simulation is particularly important for the gaming industry, where the most advanced simulators are optimized to meet real-time requirements even in complex scenarios. Despite these improvements, extremely fast simulation speeds are still required for applications such as evolving learning neuro-controllers, executing real-time planning in complex environments, or optimizing robot construction parameters. Simulators like PhysX (NVIDIA), ODE (Open Dynamics Engine), Havok, and Bullet are fine-tuned to deliver data at rates exceeding real-time. However, achieving a frame rate that is a multiple of real-time often requires substantial software restructuring or implementing unconventional solutions, such as using separate threads for each computed scene. These optimizations reflect the primary focus on meeting game physics requirements rather than the unique needs of robotics. Projects involving Artificial Intelligence, where algorithms require the capability to test behaviors across diverse scenarios, demand specialized simulators. These simulators must not only meet real-time performance requirements but also deliver results at the highest possible speeds. The following sections introduce a new simulator, NeoPhysIx, capable of achieving up to one million frames per second on a single thread of a standard Intel i5 processor. Key methods that enable this speedup, such as point cloud collision detection, joint angle calculation, and friction force estimation, will be discussed in detail."
https://arxiv.org/html/2411.07192v1,Data-Driven Predictive Control of Nonholonomic Robots Based on a Bilinear Koopman Realization: Data Does Not Replace Geometry,"Advances in machine learning and the growing trend towards effortless data generation in real-world systems has led to an increasing interest for data-inferred models and data-based control in robotics. It seems appealing to govern robots solely based on data, bypassing the traditional, more elaborate pipeline of system modeling through first-principles and subsequent controller design. One promising data-driven approach is the Extended Dynamic Mode Decomposition (EDMD) for control-affine systems, a system class which contains many vehicles and machines of immense practical importance including, e.g., typical wheeled mobile robots. EDMD can be highly data-efficient, computationally inexpensive, can deal with nonlinear dynamics as prevalent in robotics and mechanics, and has a sound theoretical foundation rooted in Koopman theory. On this background, this present paper examines how EDMD models can be integrated into predictive controllers for nonholonomic mobile robots. In addition to the conventional kinematic mobile robot, we also cover the complete data-driven control pipeline – from data acquisition to control design – when the robot is not treated in terms of first-order kinematics but in a second-order manner, allowing to account for actuator dynamics. Using only real-world measurement data, it is shown in both simulations and hardware experiments that the surrogate models enable high-precision predictive controllers in the studied cases. However, the findings raise significant concerns about purely data-centric approaches that overlook the underlying geometry of nonholonomic systems, showing that, for nonholonomic systems, some geometric insight seems necessary and cannot be easily compensated for with large amounts of data.","Autonomous mobile robots have become indispensable in many applications – be it as service robots for vacuuming and cleaning, for logistics and delivery, and in security and defence. In many of these applications, there is nowadays a tendency to move toward the application of whole robot fleets, where the individual robots are comparatively simple and mass-produced in an economical manner, e.g., by minimizing the number of moving parts and not relying on high-precision driveline components. This has multiple repercussions. On the one hand, whereas omnidirectional and fully actuated mobile robots exist on the ground and in the air, bringing advantages regarding maneuverability and thereby simplifying control and path planning, economic and simplicity considerations have led to a prevalence of underactuated, nonholonomic mobile robots [1, Ch. 8]. However, precise control of nonholonomic systems presents a challenge to this day [2, 3, 4, 5], in particular for optimal control [6, 7, 8]. Nevertheless, optimal control, such as model predictive control (MPC), has some key advantages making it very suitable for robotics, including straight-forward controller tuning, clear definition of the control goal, and the direct consideration of constraints, which allows robots to operate safely at the border of their operating regime for maximum task-solution performance. This motivates the usage of MPC as this paper’s control method of choice. On the other hand, cost-efficient, mass-produced robots that do not rely on high-precision parts may divert from a nominal, ideal behavior as it may be described by a nominal model. This can motivate to use a data-driven or data-augmented model for an MPC controller instead of a (purely) nominal one [9, 10]. Ideally, few data points are needed to obtain a model useful for model-based closed-loop control, so that newly produced robots can be ”calibrated” quickly and cost efficiently. As indicated before, model predictive control of nonholonomic vehicles is by no means simple if the control task is to ensure asymptotic stabilization of arbitrary setpoints. The latter can be practically useful, e.g., for a measurement robot that needs to reach certain poses with a very high accuracy to perform accurate measurements. It was only quite recently that it was shown that, for this task – even for the arguably simplest nonholonomic system, i.e., the differential-drive mobile robot – the canonically employed quadratic cost function provably does not lead to a functional MPC controller [7]. On the contrary, tailored non-quadratic stage costs [11, 12] or terminal conditions (either using non-quadratic terminal costs or non-convex terminal regions, see, e.g., [13, 14]) yield a guaranteed setpoint stabilization. The latter would be undesirable practically as it can introduce feasibility issues and control failure. Until very recently, it was even unknown how to generally design functioning cost functions for general nonholonomic vehicles, i.e., with higher degrees of nonholonomy and with drift [8, 15]. In [8], it has become clear that tailored, non-norm, mixed-exponents cost functions, which can be inferred from a homogeneous approximation of the nonholonomic system [16, 4], are key, and [15] has extended this to nonholonomic systems with certain kinds of actuation dynamics, i.e., if acceleration is not immediate and, thus, velocity setpoints are, in the model, not reached immediately. The respective stability analysis can be nicely embedded in the framework [16] for general nonlinear systems with null-controllable homogeneous approximations. In the above context, this paper provides several new contributions. Firstly, to the knowledge of the authors, it is the first time that purely data-driven model predictive control of a nonholonomic vehicle is performed and experimentally validated with a theory-conforming mixed-exponents cost function, without using a predefined nominal model. Secondly, as will be seen, we employ EDMD on a Koopman background to infer the model used for MPC. As far as the authors are aware, in this framework, this paper represents the first occasion where a second-order dynamics model of a nonholonomic mobile robot accurate enough for closed-loop MPC control is learned from real-world data. As will be seen, the drift present in this model has some repercussions on data acquisition, model learning, and on control design, which are all solved within this paper. This paper builds upon our previous work [17], in which a first-order kinematic model was considered, approximated, and its performance assessed regarding open-loop predictions only. Compared to the latter, novelties include studying closed-loop control in a model predictive controller with cost tailored to nonholonomic systems’ sub-Riemannian geometry [4] in conjunction with data-inferred models as well as data-inferred models capable of taking into account acceleration-level effects in the first place, including how to obtain and process real-world data for them. The paper is organized as follows. Section 2 introduces the considered system class, model predictive control, and the EDMD approach for control-affine systems. Section 3 then introduces the kinematic and second-order dynamics differential-drive robot, as well as the specialities of controlling such a nonholonomic vehicle by means of a model predictive controller. In Section 4, the data-driven models for the kinematic mobile robot, inferred from experimental measurement data, are used within predictive controllers, both in simulation, to allow large-scale comparative studies under ideal conditions, and in hardware experiments for validating whether the closed-loop controllers work as promised. In Section 5, real-world data is collected for the differential-drive robot with actuator dynamics, and insight is given into post-processing the data and identifying an EDMD-based model that accounts for actuator dynamics. This model is then used within predictive controllers showing the necessity of a cost function taking into account the system’s sub-Riemannian geometry arising from the nonholonomic constraints. Section 6 further points out how less data is needed when utilizing an EDMD-based predictive controller. A brief summary and outlook is given in Section 7. Subsequently, for integers n,m\in\mathbb{Z} with n\leq m, we define \mathbb{Z}_{n:m}\coloneqq\mathbb{Z}\cap[n,m]. Bold variables denote a vector, or in the case of a capital bold letter, a matrix, e.g., \bm{v} and \bm{A}, respectively. Furthermore, a part of a vector is denoted by specifying its component’s indices as a subscript, e.g., \bm{v}_{2:4} containing components 2 to 4 of vector \bm{v} in their original order."
https://arxiv.org/html/2411.07165v1,Acoustic-based 3D Human Pose Estimation Robust to Human Position,"This paper explores the problem of 3D human pose estimation from only low-level acoustic signals. The existing active acoustic sensing-based approach for 3D human pose estimation implicitly assumes that the target user is positioned along a line between loudspeakers and a microphone. Because reflection and diffraction of sound by the human body cause subtle acoustic signal changes compared to sound obstruction, the existing model degrades its accuracy significantly when subjects deviate from this line, limiting its practicality in real-world scenarios. To overcome this limitation, we propose a novel method composed of a position discriminator and reverberation-resistant model. The former predicts the standing positions of subjects and applies adversarial learning to extract subject position-invariant features. The latter utilizes acoustic signals before the estimation target time as references to enhance robustness against the variations in sound arrival times due to diffraction and reflection. We construct an acoustic pose estimation dataset that covers diverse human locations and demonstrate through experiments that our proposed method outperforms existing approaches.","Human pose estimation has diverse applications including rehabilitation support, elderly monitoring, and disaster relief efforts. Traditional approaches to 3D human pose estimation have primarily employed RGB videos and images [Martinez et al.(2017)Martinez, Hossain, Romero, and Little, Pavlakos et al.(2017)Pavlakos, Zhou, Derpanis, and Daniilidis], transient light [Isogawa et al.(2020)Isogawa, Yuan, O’Toole, and Kitani], event data [Scarpellini et al.(2021)Scarpellini, Morerio, and Del Bue, Chen et al.(2022)Chen, Shi, Ye, Yang, Sun, and Wang], radio frequency (RF)/Wi-Fi signals [Zhao et al.(2018b)Zhao, Tian, Zhao, Alsheikh, Li, Hristov, Kabelac, Katabi, and Torralba, Jiang et al.(2020)Jiang, Xue, Miao, Wang, Lin, Tian, Murali, Hu, Sun, and Su], and millimeter wave [Kong et al.(2022)Kong, Xu, Yu, Chen, Ma, Chen, Chen, and Kong, Xue et al.(2021)Xue, Ju, Miao, Wang, Wang, Zhang, and Su]. Additionally, methods that combine some of these approaches as a multimodal framework also exist [An et al.(2022)An, Li, and Ogras, Yang et al.(2024)Yang, Huang, Zhou, Chen, Xu, Yuan, Zou, Lu, and Xie]. However, optical signals face challenges such as obstruction and poor performance in low-light conditions [Lee et al.(2023)Lee, Rim, Jeong, Kim, Woo, Lee, Cho, and Kwak]. Furthermore, RGB video and images acquire high-resolution measured data, which raises concerns regarding the protection of personal information. Wireless signal-based methods are restricted in environments employing precision machinery, such as medical facilities or aircraft. One possible solution to these challenges is the utilization of acoustic signals. Acoustic signals have much longer wavelengths (meter scale) compared to optical signals (nanometer scale) or RF/Wi-Fi signals (centimeter scale). Therefore, acoustic signals are more susceptible to diffraction and less affected by obstruction. Moreover, acoustic signals offer consistent performance irrespective of lighting conditions and their usage is not hindered by the presence of precision machinery. Recent studies have explored passive acoustic sensing for gesture recognition and human pose estimation by leveraging human speech [Li et al.(2021)Li, Kang, Pei, Zhe, Zhang, He, and Bao, Ginosar et al.(2019)Ginosar, Bar, Kohavi, Chan, Owens, and Malik], ambient sounds [Gao et al.(2020)Gao, Oh, Grauman, and Torresani], or the sound of playing a musical instrument [Shlizerman et al.(2018)Shlizerman, Dery, Schoen, and Kemelmacher-Shlizerman]. These methods require sounds produced by the subjects themselves, which limits the use case. Alternatively, Shibata et al\bmvaOneDotproposed a 3D human pose estimation approach using active acoustic sensing with Time-Stretched-Pulse (TSP) signals [Shibata et al.(2023)Shibata, Kawashima, Isogawa, Irie, Kimura, and Aoki]. In this approach, a subject is positioned between a speaker and microphone (see Fig. 1(a)), where the speaker repeatedly emits TSP signals to create an acoustic field, and human poses are estimated based on how the acoustic field distorts as a subject moves. However, this method primarily relies on how the acoustic signal emitted from the speaker is obstructed by the human body to estimate the human pose. It implicitly assumes that the target subject is positioned on a straight line between the speaker and the microphone, although in the real world, meeting such constraints is extremely rare. Through the preliminary experiments, we found that the estimation accuracy significantly decreases when the subject deviates from this line, due to the difficulty of capturing subtle changes in sound signals caused by human body movements. Fig. 1(c) visualizes the acoustic features used as input to the model. The dimensions of these features are reduced by the Principal Component Analysis (PCA). The acoustic features in the settings without any subject and those shown in figures (a) and (b) are represented in different colors. From this figure, it is confirmed that the acoustic features when a person moves away from this line (blue dots) approach the features when there is no subject (green dots), indicating sound diffraction and reflection convey much less human pose information than sound obstruction caused by a person standing on the aforementioned line (red dots). To overcome this limitation, this paper proposes an acoustic-based 3D human pose estimation method, which remains effective regardless of the subject’s standing positions. While Shibata et al\bmvaOneDotprimarily relied on signal obstruction by the human body as their main clue, in this paper, we also consider cases where the position of the person is not on the straight line connecting the speaker and the microphone, as shown in Fig. 1(b). Therefore, it is necessary to consider signal diffraction and reflection from the subject as well as signal obstruction. From a technical perspective, this implies the need to solve two extremely challenging issues: (i) The relatively long wavelengths of acoustic signals tend to cause specular reflections off the surface of the human body. Consequently, the sound intensity of the reflected acoustic signals is greatly influenced by the positions of reflection and recording microphones. (ii) The arrival time of sound emitted from a speaker until it is recorded can vary due to signal diffraction and reflection. In this paper, we aim to develop methods capable of addressing these challenges. First, to enhance robustness against variations in the subject’s position, we introduce a position discriminator module. This module uses intermediate features of the pose estimation module to predict human positions, while the pose estimation module is trained to maximize the uncertainty of human positions, through adversarial training. Furthermore, to achieve robust pose estimation against changes in the arrival time of sound due to sound diffraction and reflection, we propose to introduce a reference window into the pose estimation module to consider signals prior to the target time to be estimated. Additionally, we perform data augmentation by shifting the phase of the acoustic signal, which allows for a reduction in the amount of data per subject location, enabling the preparation of a dataset that covers diverse positions. As the first attempt at non-invasive 3D human pose estimation regardless of the subject’s position, we construct a new dataset containing data from positions away from the straight line connecting the speaker and the microphones. In summary, the technical contributions of this study are as follows: (1) We have worked towards realizing a practical non-invasive 3D human pose estimation method based on active acoustic signals while subjects are placed in multiple positions. (2) We introduced a position discriminator module to enhance robustness against variations in the subject’s standing position. Additionally, we constructed a pose estimation model that considers acoustic signals prior to the estimation target time to achieve robust estimation against changes in sound arrival times due to signal diffraction and reflection. (3) To effectively learn from limited data, we performed data augmentation by shifting the phase of the acoustic signal. (4) As the first attempt to estimate non-invasively 3D human pose regardless of the subject’s position, we constructed a dataset containing data from multiple positions away from the straight line connecting the speaker and the microphones."
https://arxiv.org/html/2411.06733v1,GSL-PCD: Improving Generalist-Specialist Learning with Point Cloud Feature-based Task Partitioning,"Generalization in Deep Reinforcement Learning across unseen environment variations often requires training over a diverse set of scenarios. Many existing DRL algorithms struggle with efficiency when handling numerous environment variations. The Generalist-Specialist Learning (GSL) framework addresses this by first training a generalist on all variations, then creating specialists from the generalist’s weights, each focusing on a random subset of variations. Finally, the generalist refines its learning with assistance from the specialists. However, random task partitioning in GSL can impede specialist performance, as it often assigns vastly different variations to the same specialist, typically resulting in each specialist being assigned just one variation, which increases computational costs. To improve this, we propose Generalist-Specialist Learning with Point Cloud Feature-based Task Partitioning (GSL-PCD). This approach clusters environment variations based on features extracted from object point clouds, using balanced clustering with a greedy algorithm to assign similar variations to the same specialist. Evaluations on robotic manipulation tasks from the ManiSkill benchmark demonstrate that point cloud feature-based partitioning outperforms vanilla partitioning by 9.4% with a fixed number of specialists and reduces computational and sample requirements by 50% to achieve comparable performance.","Generalization remains one of the most fundamental challenges in reinforcement learning (RL) Cobbe et al. (2020); Jia et al. (2022); Ghosh et al. (2021). Ideally, RL agents should be capable of adapting and generalizing to unseen environment variations during deployment. To achieve this objective, RL agents need to focus on learning generalizable skills, rather than overfitting to specific environment setups. Recently, several benchmarks James et al. (2020); Ghosh et al. (2021); Gu et al. (2023) have been proposed to evaluate the generalization capabilities of RL agents, offering rich scene layouts, a large number of environment configurations, and diverse sets of objects. Many existing deep reinforcement learning (DRL) algorithms struggle to achieve high performance when faced with a wide range of environment variations, as demonstrated in Schulman et al. (2017). To address these challenges, the Generalist-Specialist Learning (GSL) framework Jia et al. (2022) was recently proposed. Inspired by how human organizations tackle complex problems, GSL first trains a generalist on all environment variations. It then deploys a large population of specialists, initialized with the generalist’s weights, each trained on a randomly selected subset of variations. Finally, GSL retrains the generalist, leveraging demonstrations from all the specialists. During specialist training, GSL randomly assigns environment variations to specialists (random task partitioning). While this approach is convenient to implement, our empirical observations show that it often assigns very different environment variations to a single specialist, significantly increasing the difficulty of specialist training. As a result, to ensure specialists can effectively master their assigned variations, the number of environment variations per specialist should be reduced. In practice, GSL assigns only one environment variation per specialist in most of its experiments. However, this low number of variations per specialist leads to an explosion in the number of specialists required, resulting in an unsustainable demand for online samples and computational resources. For instance, GSL trains 75 specialists for Procgen tasks Cobbe et al. (2020) and requires 1,024 specialists for the Plunder environment to achieve good performance. This level of sample and computational complexity severely limits GSL’s applicability in scenarios where online samples are costly and computational power is constrained. We draw inspiration from how humans learn to solve complex tasks. People typically begin by practicing on a group of similar task configurations, rather than randomly selected ones, and only after becoming proficient in that group do they move on to broader sets of tasks. Based on this observation, we believe the principle of ”starting with similar tasks first” can also be applied to the domain of Generalist-Specialist Learning (GSL). During specialist training, specialists should focus on subsets of similar task configurations to achieve better performance. In robotic manipulation tasks involving object variations, object appearance serves as the best indicator of task similarity Ling et al. (2023). Previous works like Ling et al. (2023); He et al. (2021); Eisner et al. (2022) have noted that point cloud features encode detailed depth and spatial information, providing the most effective 3D representations for reasoning about agent-object relationships. Therefore, we naturally ask: Can object point cloud features be leveraged to explore better task partitioning in Generalist-Specialist Learning (GSL) in robotic manipulation tasks with object variations? The answer is yes. In this paper, we propose Generalist-Specialist Learning with Point Cloud Feature-based Task Partitioning (GSL-PCD). Our method leverages a pre-trained PointNet++ model Qi et al. (2017) to encode point cloud features of all objects and employs a balanced clustering algorithm with a greedy approach to group objects with similar point cloud representations, assigning each cluster to a single specialist. By using point cloud features as an indicator of task similarity, our approach ensures that objects with similar characteristics are grouped under the same specialist. This significantly reduces the learning difficulty for each specialist, enabling them to handle a greater variety of environment variations. We evaluate our method on the Turn Faucet task from the ManiSkill benchmark Gu et al. (2023); Mu et al. (2021), a challenging robotic manipulation task featuring 60 different types of faucets. Empirical results demonstrate that our approach outperforms random partitioning by 9.4% with a fixed number of specialists and reduces computation and sample requirements by 50% to achieve similar performance. To summarize, our contributions are as follows: • We identify the limitations of random task partitioning within the vanilla Generalist-Specialist Learning (GSL) framework and advocate for a more structured approach to task partitioning. • We propose Generalist-Specialist Learning with Point Cloud Feature-based Task Partitioning (GSL-PCD), which leverages point cloud features extracted by a pre-trained PointNet++ model to indicate task similarity in robotic manipulation tasks with object variations. Balanced clustering with greedy algorithm is used to group objects, assigning those from the same cluster to a single specialist. • We evaluate our method on the challenging Turn Faucet task from the ManiSkill benchmark, featuring 60 different types of faucets, outperforms random partitioning by 9.4% with a fixed number of specialists and reduces computation and sample requirements by 50% to achieve similar performance."
https://arxiv.org/html/2411.06711v1,Anytime Probabilistically Constrained Provably Convergent Online Belief Space Planning,"Taking into account future risk is essential for an autonomously operating robot to find online not only the best but also a safe action to execute. In this paper, we build upon the recently introduced formulation of probabilistic belief-dependent constraints. We present an anytime approach employing the Monte Carlo Tree Search (MCTS) method in continuous domains. Unlike previous approaches, our method assures safety anytime with respect to the currently expanded search tree without relying on the convergence of the search. We prove convergence in probability with an exponential rate of a version of our algorithms and study proposed techniques via extensive simulations. Even with a tiny number of tree queries, the best action found by our approach is much safer than the baseline. Moreover, our approach constantly finds better than the baseline action in terms of objective. This is because we revise the values and statistics maintained in the search tree and remove from them the contribution of the pruned actions.","Casting decision-making under uncertainty as a Partially Observable Markov Decision Process (POMDP) is considered State-Of-The-Art (SOTA). Under partial observability the decision-making agent does not have complete information about the state of the problem, so it can only make its decisions based on its “belief” about the state. In a continuous domains in terms of POMDP state, the belief, in a particular time index, is the Probability Density Function (PDF) of the state given all concurrent information in terms of performed actions and received observations in an alternating manner, plus the prior belief. A POMDP is known to be undecidable [1] in finite time. Introducing various constraint formulations into POMDP is essential for, e.g., ensuring safety [2], [3] and efficient Autonomous Exploration [4]. Yet, the existing online approaches in anytime setting have problems and therefore fall short of providing reliable and safe optimal autonomy. This crucial gap we aim to fill in this paper. Similar to almost any online POMDP solver today such as MCTS, our method constructs a belief tree and uses the tree to represent the POMDP policy. We prune dangerous actions from the belief tree and revise the values and statistics that an MCTS tree maintains. Anytime, our search tree contains only the safe actions in accord to our definition of safe action, which will appear shortly. Our work lies in continuous domain in terms of actions and the observations. In such a setting, there are approaches to tackle averaged cumulative constraint using anytime MCTS methods [5], [6]. We now linger on the explanation of what the averaged constraint is. Under partial observability, namely in the POMDP setting, there are naturally two stages to consider in order to introduce a constraint. The first stage arises from the belief itself. Usually, at this stage, the state-dependent payoff operator is averaged with respect to the corresponding belief to obtain a belief-dependent one. It is then summed up to achieve a cumulative payoff. We use the term payoff to differentiate between reward operator and emphasize that a belief-dependent payoff constraint operator shall be as large as possible as opposed to the cost operator. The second stage arises from the distribution of possible future observations episodes. At this stage, commonly, the cumulative payoff is again averaged but with respect to future observations episodes and then thresholded, thereby forming an averaged cumulative constraint. Such a formulation is sufficient for ensuring safety in limited cases as we will further see in Section VI-A. This is because it permits deviations of the individual values within the summation. Let us now describe the MCTS methods mentioned above to tackle averaged cumulative constraint. The seminal paper in this direction is [7]. It leans on the rearrangement of the constrained objective using the occupancy measure described in [8]. Such a reformulation is appealing since it transforms the problem into linear programming bringing convexity to the table and enjoying from strong duality. The authors of [5] extend the approach from [7] to continuous spaces. Still, both papers [7] and [5] assure constraint satisfiability only at the limit of the convergence of the iterative procedure, namely in infinite time. Since these are iterative methods, to assure anytime constraint satisfiability we need to project the obtained occupancy measure at each iteration to the space defined by the constraint. If dual methods are involved [9] such a projection does not make much sense, e.g., the projection might lead to a step direction vector on the boundary of all the constraints, making it zero vector. Employing the primal methods in continuous spaces also appears to be problematic since the summations in [7] are transformed into integrals. The paper [6] provides some sort of anytime satisfiability by introducing high-level action primitives (options). Still, [6] suffers from limitations, e.g. it requires crafting low-level policies, meaning knowing how the robot shall behave a priori. In addition, the options shall be locally feasible. Additionally, for efficiency reasons, the duality based approaches perform a single tree query of the MCTS, instead of running MCTS until convergence in the maximization of the Lagrangian dual objective function phase (See section 8.5.2 in [9]) of dual ascend. In all three papers [7], [5], [6] the averaged cumulative constraint is enforced solely from the root of the belief tree. This is suboptimal since within a planning session it is not taken into account that the constraint will be enforced at the future planning sessions. In other words, the contemplation of a robot about the future differs from its actual future behavior. This aspect has been fixed by [10]. As we will further see in Section IV, our approach naturally handles this problem. Moreover, [10] assures fulfillment (admission) of the recursive averaged cumulative constraint anytime with respect to search tree constructed partially with the reward bounds and partially with rewards themselves. Yet, the algorithm presented in [10] requires that the value function is bounded on the way down the tree to assure the exploration. This is commonly achieved by assuming that the state-dependent reward is trivially bounded from above and below. This does not hold for general belief-dependent reward functions. Moreover, the exploration outlined in that paper is valid for discrete spaces only. All in all, the extension of that work to continuous spaces and belief-dependent rewards requires clarification. Support for general belief dependent rewards and payoff/cost operators and MCTS convergence We now clarify whether or not the mentioned above solvers support belief-dependent cost/payoff operators and rewards. It was suggested in [3],[4] that general belief-dependent payoff/cost operators are extremely important. As mentioned in [3] Value-at-Risk (VaR) and Conditional VaR (CVaR) over the distance to the safe space allow for control of the depth the robot can plunge into the obstacle. To rephrase that, these operators measure how bad the disaster (collision) will be. See Appendix D, for details. The Information Gain discussed in [4] is relevant for exploration. The paper [4] discussed the general belief-dependent averaged constraint of the form (38) in a high dimensional setting and in the context of Information Gain. The iterative schemes in [7], [5] lean on the convergence of MCTS. It has been shown in [11] that even in discrete spaces and with bounded rewards it can take a very long time for MCTS to converge. In the case of unbounded reward or the cost-augmented objective of [7], [5], the MCTS may converge slowly. If such an augmented reward has a large variance, it will be needed a huge amount of tree queries for action-value estimate (to be defined shortly) at each belief node of the belief tree to converge. The large variance can be the result of an unrestrained variability of the rewards or a large Lagrange multiplier. There are several constraint formulations for POMDP. Below we discuss the most prominent techniques one by one. Shielding POMDPs There is a growing body of literature on shielding POMDPs. The shield is a technique to disable the actions that can be executed by the agent and violate the shield definition. There are several shield definitions. Online methods [12], [13] in this category utilize Partially Observable Monte-Carlo Planning (POMCP) algorithm [14]. These works have the same problems we are solving in this paper: one way or another, the actions violating the shield definition participate in the planning procedure, yielding a suboptimal result. The work [13] enforces the shied outside the POMCP planning. As we further show, not considering safety in the future times, namely within the planning session, can lead to a suboptimal planning result. Chance Constrained (CC) Online Planning A recent work [15] tackles online planning with chance constraints in an anytime setting. This paper suggests using a Neural Network (NN) to approximate CC enforced, with an adaptive threshold, from each belief considered in the planning session. This work trains NN offline. Therefore the error stemming from the discrepancy of simulated and real data is unknown. Moreover, it is not clear how complex the NN shall be to achieve zero loss in training to ensure no error in CC approximation, so even if no discrepancy discussed before exists, the NN inference may be slow. In this method, dangerous actions do not participate in the planning session. Safe control Under Partial Observability There are a variety of robust control approaches natively tailored for continuous state/action/observation spaces [16],[17]. However, these methods are usually limited to very specific rewards/objectives and tasks, such as reaching a goal state or to be as close as possible to a nominal trajectory. Moreover, in both papers the system dynamics are control-affine. Without this assumption, it is not clear how to enforce the constraint through a derivative of the barrier function. I-A Contributions Below we list down our contributions in the same order as they appear in the manuscript. • By constraining directly the problem space and not the dual space we present an anytime MCTS based algorithm for safe online decision making with safety governed by a Probabilistic Constraint (PC). Our approach enjoys anytime safety guarantees with respect to the belief-tree expanded so far and works in continuous state, action and observation spaces. When stopped anytime, the action returned can be considered as the best safe action under the safe future policy (tree policy) expanded so far. Our search tree solely consists of safe actions. We prove convergence in probability with an exponential rate of our approach. • Another contribution on our end is constraining the beliefs with incorporated outcome uncertainty stemming from an action performed by the robot and without incorporating the received observation. This is alongside the constraint over the posterior belief with included last observation. To the best of our knowledge, no previous works do that. • We also spot a problem happening in duality based approaches arising from averaging unsafe actions in MCTS phase. Therefore, an additional contribution of ours is an analysis of this phenomenon. • We simulate our finding on several continuous POMDP problems. I-B Notation We use the \square as a placeholder for various quantities. The values in \square can be replaced by one of the respective options. We also extensively use the indicator function notation, which is \mathbf{1}_{A}(\square). This function equals to one if and only if \square{\in}A. By lowercase letters we denote the random variables of their realizations depending on context. By the bold font we denote vectors of operators in time of different lengths. We denote estimated values by \hat{\square}. I-C Paper Roadmap This paper proceeds with the following structure. Section II presents relevant background. Section III then formulates the problem. Section IV presents our approach. Section VI discusses our baseline. Section VII gives experimental validation of the proposed methodology. Finally, Section VIII concludes the paper."
https://arxiv.org/html/2411.06632v1,ACRA Format Instructions for Authors,"The ACRA Proceedings will appear in CD-ROM form only. To ensure that all papers in the Proceedings have a uniform appearance, authors are asked to adhere to the following instructions. In addition, we will accept, and in fact encourage, submissions in the final format. This file includes the style instructions for submissions.","Although ACRA is a CD-ROM only conference, papers should be prepared so that they can be printed out. 1.1 Corresponding Author Details The corresponding author is requested to email the following information along with the paper: 1. title of the paper, 2. name and postal address, email address. 1.2 Word Processing Software As detailed below, ACRA has prepared and made available a set of LaTeX macros and Word templates for use in formatting your paper. If you are using some other word processing software (such as WordPerfect, etc.), please follow the format instructions given below and ensure that your final paper looks as much like this sample as possible."
https://arxiv.org/html/2411.06575v1,Adaptive Kinematic Modeling for Improved Hand Posture Estimates Using a Haptic Glove,"Most commercially available haptic gloves compromise the accuracy of hand-posture measurements in favor of a simpler design with fewer sensors. While inaccurate posture data is often sufficient for the task at hand in biomedical settings such as VR-therapy-aided rehabilitation, measurements should be as precise as possible to digitally recreate hand postures as accurately as possible. With these applications in mind, we have added extra sensors to the commercially available Dexmo haptic glove by Dexta Robotics and applied kinematic models of the haptic glove and the user’s hand to improve the accuracy of hand-posture measurements. In this work, we describe the augmentations and the kinematic modeling approach. Additionally, we present and discuss an evaluation of hand posture measurements as a proof of concept.","Haptic gloves are wearable devices that provide force and/or tactile feedback to the fingertips and, optionally, the palm. Further, they track the movement of the fingers and, in some cases, the palm with multiple degrees of freedom (DoF) [1]. In recent years, haptic gloves have become increasingly popular. Current literature envisions that such haptic gloves, if properly designed, will enable dexterous manual interaction with a virtual reality (VR) that feels realistic [2]. However, haptic glove applications are not only limited to VR interaction. Other examples range from robotic teleoperation [3, 4] over surgical education [5] to behavioral analysis [6]. Haptic devices have also been applied for medical settings like motor rehabilitation [7, 8, 9, 10] and medical training [11, 12, 13]. In these biomedical contexts, such devices are helpful because, in addition to measuring hand and finger movements, they also provide performance test scenarios by haptically representing virtual objects. Further, they allow the creation of applications hand-tailored to a patient’s needs [14, 8]. However, as Wang et al. note, in practice, commercially available haptic gloves generally sacrifice the accuracy of motion tracking for a lower price and weight [2]. For example, the Dexmo glove111https://www.dextarobotics.com/en-us [15, 16] only measures the overall flexion of each fingertip relative to its respective metacarpophalangeal joint (MCP) with a bend sensor, as well as the abduction of the MCP. Still, a single bend value does not uniquely determine the flexion angles of all joints, i.e., MCP, proximal interphalangeal (PIP), and distal interphalangeal (DIP). To circumvent this problem, the Dexmo software uses a linear regression model that infers PIP, DIP, and MCP angles based on the bend value of a finger [15]. The drawback of this approach is that it constrains the measured position of individual fingertips to be on a circular trajectory between full extension and a power grasp. This constraint limits the quality of force feedback, which depends on a high-quality fingertip pose estimation, and drastically reduces the number of natural movements the device can track. For example, a precision pinch where PIP and DIP of the digit opposed to the thumb are fully extended, can not be captured. Figure 1: Data flow diagram of our model-based approach. The haptic gloves data is converted to fingertip positions by applying Forward Kinematics derived from a model of the glove. Based on a model of the user’s hand, the application of Inverse Kinematics yields a valid joint configuration. Another challenge we identify in replicating hand postures digitally as accurately as possible is choosing the dimensions of the digital hand model. The lengths of the bones in the human hand dictate the positions of the joints and their influence on the fingertip positions relative to the palm. If the user’s hand differs too much from the model, some postures might be physiologically impossible to reach with the digital hand. To the best of our knowledge, no efforts have been made in the haptics community to develop an adaptive human hand model for haptic glove postures with a fast and reliable calibration method. Most gloves are under-determined but still compute the full hand joint states [17]. Thus, they must resort to some variation of the mentioned coupling simplification of PIP and DIP to the MCP and use two extreme configurations to create a linear mapping between the joint of the real and digital hand that uses the digital model’s full range of motion. This work presents a modified version of the Dexmo haptic glove that allows us to estimate each finger’s MCP, PIP, and DIP angles without constraining them to a predefined trajectory. We propose a framework that computes the hand joint configuration using kinematic modeling of both the glove and hand, along with sensor data. As a part of this framework, we utilize the Hand Model Configuration Tool (HMCT) [18] to measure the size of the user’s hand and appropriately adapt the generic hand model employed for Inverse Kinematics. The framework can be split into a hardware-dependent and a hardware-agnostic part (see Fig. 1). The hardware-dependent part consists of the glove itself and the software component tied to it that sends out its measurements, as well as a reliable model of the glove, its links, and its joints. Section II-B1 discusses this part of our framework applied to Dexmo in more detail. The hardware-agnostic part consists of (1) a Forward Kinematics module, which – parameterized by the glove’s exoskeleton model – converts the incoming measurements into fingertip positions, and (2) an Inverse Kinematics module, parameterized by a configurable user hand model, that converts the fingertip positions into a hand joint configuration for downstream applications. This approach is detailed in section II-B. Figure 2: Concept of additionally measured angles for each finger. B denotes the bend sensor of Dexmo, which measures the flexion angle \alpha. F and T denote the newly added sensors to measure angles \beta and \gamma to compute the fingertip’s position. Inverse Kinematics then solves for the finger-joint angles \sigma, \delta, and \phi. 1. and 2. display two distinct configurations with different configurations of DIP and PIP, but the same values for angles \alpha and \beta, as an example of the under-determination problem, when only measuring \alpha."
https://arxiv.org/html/2411.06244v1,Grasping Object: Challenges and Innovations in Robotics and Virtual Reality,"In real-life, grasping is one of the fundamental and effective forms of interaction when manipulating objects. This holds true in the physical and virtual world; however, unlike the physical world, virtual reality (VR) is grasped in a complex formulation that includes graphics, physics, and perception. In virtual reality, the user’s immersion level depends on realistic haptic feedback and high-quality graphics, which are computationally demanding and hard to achieve in real-time. Current solutions fail to produce plausible visuals and haptic feedback when simulation grasping in VR with a variety of targeted object dynamics. In this paper, we review the existing techniques for grasping in VR and robotics and indicate the main challenges that grasping faces in both domains. We aim to explore and understand the complexity of hand-grasping objects with different dynamics and inspire various ideas to improve and come up with potential solutions suitable for virtual reality applications.","Over the past decades, with the advancement in virtual reality (VR) devices and Human-computer interaction (HCI) studies, the need for simulation of human behaviour with realistic interaction has become vital for many applications (see Fig. 1), such as industrial training, medical and surgical simulation, and rehabilitation [1, 2, 3]. High-fidelity graphics in such virtual environments have been used to provide users with a relatively immersive virtual experience [4]. However, obtaining a fully immersed experience requires realistic interaction with objects within the environment, where forces and masses of the objects can be felt during the interaction [5]. Haptic devices for both fingertips and/or the whole hand enable users to feel and manipulate the 3D objects and explore the virtual environment through a kinesthetic and cutaneous perception [6]. When the user interacts with an object, grasping is one of the main intuitive action behaviours. Although grasping behaviour is natural, human hands can grab a variety of objects with different shapes, weights, and frictions. Grasping in the virtual environment is a challenging task, as an ideal grasping action must take into account the geometry and dynamic characteristics of the virtual object [7]. Despite the recent achievements in grasping techniques in VR, which have led to the emergence of some technological devices such as glove-based devices [8] and controller-based devices [9, 1]. More technical methods need to be explored in order to ensure stable, controllable grasping in virtual reality. The grasping techniques in VR differ broadly in complexity according to the virtual object’s properties, such as mass, size, and materials. Moreover, the object’s stiffness plays a significant role in making the interaction with the object complex to simulate [10]. Most existing techniques focus on reducing the complexity of the grasping targets by simulating rigid bodies or objects with relatively simple and similar properties [11]. However, unlike rigid bodies, deformable bodies have high dynamic force attributes when the fingertips make contact with them. Therefore, obtaining stable grasping of deformable bodies while achieving realistic visuals and haptic feedback in virtual reality remains an open problem. Fig 1. Realistic interaction with virtual objects within an immersed environment is crucial for many applications, such as industrial and medical training, entertainment and virtual social interaction [2]. In this paper, we aim to review the existing methods for grasping from the perspectives of haptics and visuals, including the properties of the target objects. Further, to focus on the techniques of grasping in VR, we also generally introduce robotic grasping methods to compare the grasping methods in virtual environments with real-life robotics. Through this paper, we hope to summarise the main existing challenges in grasping simulation and improve the quality of VR grasping, potentially creating a direction for a new research agenda."
https://arxiv.org/html/2411.06219v1,RRT* Based Optimal Trajectory Generation with Linear Temporal Logic Specifications under Kinodynamic Constraints,"In this paper, we present a novel RRT*-based strategy for generating kinodynamically feasible paths that satisfy temporal logic specifications. Our approach integrates a robustness metric for Linear Temporal Logics (LTL) with the system’s motion constraints, ensuring that the resulting trajectories are both optimal and executable. We introduce a cost function that recursively computes the robustness of temporal logic specifications while penalizing time and control effort, striking a balance between path feasibility and logical correctness. We validate our approach with simulations and real-world experiments in complex environments, demonstrating its effectiveness in producing robust and practical motion plans. This work represents a significant step towards expanding the applicability of motion planning algorithms to more complex, real-world scenarios.","I INTRODUCTION Recent developments in motion planning have increasingly focused on handling intricate objectives and constraints using advanced formalisms, including temporal logic [1, 2]. Due to their expressive nature and precise semantics, temporal logics, such as Linear Temporal Logic (LTL) [3], Metric Temporal Logic (MTL) [4], and Signal Temporal Logic (STL) [5], have become vital tools in defining desirable behaviors for dynamic systems. The robust theoretical foundations and practical tools available for temporal logic have facilitated its widespread use in motion planning for robotics and control systems. Current approaches for generating trajectories that satisfy temporal logic specifications include symbolic control techniques [6], sampling-based methods [7, 8], graph search techniques [9], and optimization-based methods [10]. However, although symbolic control techniques can address input constraints, they typically rely on state space abstraction, which significantly increases computational complexity, especially as the dimensionality of the state space grows. Sampling-based techniques, for instance, abstract continuous state spaces into graph structures that are then used to search for feasible paths. However, they disregard the system dynamics and as these graphs become more complex, scalability issues arise, rendering these methods less effective for complicated planning problems. On the other hand, mixed-integer linear programming (MILP)-based techniques [11] encode LTL specifications directly into optimization problems, yielding optimal solutions. Despite their precision, these methods are often limited to specific subclasses of LTL formulas and heavily rely on user-defined parameters, which constrains their applicability in more general scenarios. Figure 1: Real-world demonstration of optimal, kinodynamically feasible trajectories, meeting LTL specifications. In recent years, tree-based algorithms have gained attention for incorporating temporal logic preferences into planning, leveraging spatial robustness as a key part of their cost function [12, 13, 14]. Robustness in this context refers to a quantitative measure of how well a trajectory satisfies or violates the given temporal logic specification. It provides a real-valued score that indicates the degree of satisfaction, thus enabling planners to prioritize paths that best meet the desired temporal properties. For example, Karlsson et al. [12] formulated a cost function that balances STL spatial robustness with trajectory duration, while others like Vasile et al. [14] aim to maximize the spatial robustness of STL specifications during motion planning. While these methods enhance the robustness of trajectory planning, they often do not account for system dynamics, state, and input constraints, which is essential for ensuring the feasibility of the generated paths. A trajectory that satisfies temporal logic specifications but fails to adhere to the system’s kinodynamic constraints may not be implementable in real-world scenarios. Kinodynamic RRT [15] addresses this issue by extending traditional Rapidly-exploring Random Trees (RRT) to consider system dynamics, generating trajectories that not only avoid obstacles but also comply with the system’s motion capabilities. In this work, we build on these principles by proposing a novel RRT*-based strategy to generate kinodynamically feasible paths that also satisfy temporal logic specifications. Unlike traditional methods, our approach not only focuses on meeting complex temporal logic requirements, through LTL, but also ensures that the trajectories align with the system’s physical constraints. This integration of temporal logic and kinodynamic considerations results in a more robust and practical motion planning solution, especially suited for dynamic and constraint-heavy environments."
https://arxiv.org/html/2411.06174v1,State Chrono Representation for Enhancing Generalization in Reinforcement Learning,"In reinforcement learning with image-based inputs, it is crucial to establish a robust and generalizable state representation. Recent advancements in metric learning, such as deep bisimulation metric approaches, have shown promising results in learning structured low-dimensional representation space from pixel observations, where the distance between states is measured based on task-relevant features. However, these approaches face challenges in demanding generalization tasks and scenarios with non-informative rewards. This is because they fail to capture sufficient long-term information in the learned representations. To address these challenges, we propose a novel State Chrono Representation (SCR) approach. SCR augments state metric-based representations by incorporating extensive temporal information into the update step of bisimulation metric learning. It learns state distances within a temporal framework that considers both future dynamics and cumulative rewards over current and long-term future states. Our learning strategy effectively incorporates future behavioral information into the representation space without introducing a significant number of additional parameters for modeling dynamics. Extensive experiments conducted in DeepMind Control and Meta-World environments demonstrate that SCR achieves better performance comparing to other recent metric-based methods in demanding generalization tasks. The codes of SCR are available in https://github.com/jianda-chen/SCR.","In deep reinforcement learning (Deep RL), one of the key challenges is to derive an optimal policy from highly dimensional environmental observations, particularly images [5, 12, 33]. The stream of images received by an RL agent contains temporal relationships and significant spatial redundancy. This redundancy, along with potentially distracting visual inputs, poses difficulties for the agent in learning optimal policies. Numerous studies have highlighted the importance of building state representations that can effectively distinguish task-relevant information from task-irrelevant surroundings [45, 46, 8]. Such representations have the potential to facilitate the RL process and improve the generalizability of learned policies. As a result, representation learning has emerged as a fundamental aspect in the advancement of Deep RL algorithms, gaining increased attention within the RL community [21]. Related Works. The main objective of representation learning in RL is to develop a mapping function that transforms high-dimensional observations into low-dimensional embeddings. This process helps reduce the influence of redundant signals and simplify the policy learning process. Previous research has utilized autoencoder-like reconstruction losses [43, 18], which have shown impressive results in various visual RL tasks. However, due to the reconstruction of all visual input including noise, these approaches often lead to overfitting on irrelevant environmental signals. Data augmentation methods [42, 22, 23] have demonstrated promise in tasks involving noisy observations. These methods primarily enhance perception models without directly impacting the policy within the Markov Decision Process (MDP) framework. Other approaches involve learning auxiliary tasks [33], where the objective is to predict additional tasks related to the environment using the learned representation as input. However, these auxiliary tasks are often designed independently of the primary RL objective, which can potentially limit their effectiveness in improving the overall RL performance. In recent advancements, behavioral metrics, such as the bisimulation metric [11, 10, 6, 4, 5, 47] and the MICo distance [7], have been developed to measure the dissimilarity between two states by considering differences in immediate reward signals and the divergence of next-state distributions. Behavioral metrics-based methods establish approximate metrics within the representation space, preserving the behavioral similarities among states. This means that state representations are constrained within a structured metric space, wherein each state is positioned or clustered relative to others based on their behavioral distances. Moreover, behavioral metrics have been proven to set an upper bound on state-value discrepancies between corresponding states. By learning behavioral metrics within representations, these methods selectively retain task-relevant features essential for achieving the optimal policy, which involves maximizing the value function and shaping agent behaviors. At the same time, they filter out noise that is unrelated to state values and behavioral metrics, thereby improving robustness in noisy environments. However, behavioral metrics face challenges when handling demanding generalizable RL tasks and scenarios with sparse rewards [20]. While they can capture long-term behavioral metrics to some extent through the temporal-difference update mechanism, their reliance on one-step transition data limits their learning efficiency, especially in the case of sparse rewards. This limitation may result in representations that encode non-informative signals [20] and can restrict their effectiveness in capturing long-term reward information. Some model-based approaches attempt to mitigate these issues by learning transition models [14, 16, 27]. However, learning a large transition model with long trajectories requires a large number of parameters and significant computational resources. Alternatively, N-step reinforcement learning methods can be used to address the problem [48]. Nevertheless, these methods introduce higher variance in value estimation compared to one-step methods, which may lead to unstable training. Contributions. To overcome the above challenges, we introduce the State Chrono Representation (SCR) learning framework. This metric-based approach enables the learning of long-term behavioral representations and accumulated rewards that span from present to future states. Within the SCR framework, we propose the training of two distinct state encoders. One encoder focuses on crafting a state representation for individual states, while the other specializes in generating a Chronological Embedding that captures the relationship between a state and its future states. In addition to learning the conventional behavioral metric for state representations, we introduce a novel behavioral metric specifically designed for temporal state pairs. This new metric is approximated within the chronological embedding space. To efficiently approximate this behavioral metric in a lower-dimensional vector space, we propose an alternative distance metric that differs from the typical L_{p} norm. To incorporate long-term rewards information into these representations, we introduce a “measurement” that quantifies the sum of rewards between the current and future states. Instead of directly regressing this measurement, we impose two constraints to restrict its range and value. Note that SCR is a robust representation learning methodology that can be integrated into any existing RL algorithm. In summary, our contributions are threefold: 1) We introduce a new framework, SCR, for representation learning with a focus on behavioral metrics involving temporal state pairs. We also provide a practical method for approximating these metrics; 2) We develop a novel measurement specifically adapted for temporal state pairs and propose learning algorithms that incorporate this measurement while enforcing two constraints; 3) Through experiments conducted on the Distracting DeepMind Control Suite [34], we demonstrate that our proposed representation exhibits enhanced generalization and efficiency in challenging generalization tasks."
https://arxiv.org/html/2411.06087v2,Cross-Domain Transfer Learning using Attention Latent Features for Multi-Agent Trajectory Prediction,"With the advancements of sensor hardware, traffic infrastructure and deep learning architectures, trajectory prediction of vehicles has established a solid foundation in intelligent transportation systems. However, existing solutions are often tailored to specific traffic networks at particular time periods. Consequently, deep learning models trained on one network may struggle to generalize effectively to unseen networks. To address this, we proposed a novel spatial-temporal trajectory prediction framework that performs cross-domain adaption on the attention representation of a Transformer-based model. A graph convolutional network is also integrated to construct dynamic graph feature embeddings that accurately model the complex spatial-temporal interactions between the multi-agent vehicles across multiple traffic domains. The proposed framework is validated on two case studies involving the cross-city and cross-period settings. Experimental results show that our proposed framework achieves superior trajectory prediction and domain adaptation performances over the state-of-the-art models.","I INTRODUCTION In the realm of intelligent transportation, the landscape of vehicle trajectory prediction has evolved significantly and gained immense traction in intelligent transportation systems, spanning applications from vehicle design to traffic forecasting and traffic control [1]. This transformation is driven by advancements in vehicle sensor hardware and traffic infrastructure. Consequently, this allows the acquisition of high fidelity sensory and positional data of multiple vehicles, which are crucial for data-driven modelling of the complex spatial-temporal interactions between vehicles in a multi-agent traffic network [2]. In particular, an accurate forecast of the future vehicular trajectories allows a ego vehicle to plan its optimal navigational route within the network and alleviate traffic issues such as congestion and accidents. Recent advances in the deep learning paradigm have tremendously enhanced vehicular trajectory prediction in many traffic networks such as arterial roads, boulevards, and interstate highways. A noteworthy example is the graph-based interaction-aware trajectory prediction (GRIP) model [3] which incorporates graph convolutional neural network (GCN) and a recurrent encoder-decoder architecture. The GRIP model exploited graph representation to model complex spatial inter-agent interactions and the sequential encoding modules in recurrent neural network (RNN) to model temporal correlation across the vehicle trajectories. Apart from that, attention mechanism of the Transformer networks [4, 5] has been harnessed to effectively model time-series data. The attention mechanism involves a global treatment of the time-evolving trajectory as a unified sequence, thus mitigating the deficiency of RNNs in retaining long-term temporal dependencies in long vehicle trajectory [6, 7]. Nevertheless, deep trajectory prediction models are often tailored to the available training data, which could be collected on a particular traffic configuration such as time period or fixed location. This adherence to a certain traffic domain inhibits the model from effectively generalizing its prediction to unseen traffic networks [8]. Current domain adaptation approaches for trajectory prediction predominantly rely on semi-supervised techniques [9]. These approaches serve as effective means to transfer existing knowledge within vision models pertaining to road geometry and topology, and in kinematic models for understanding driver maneuvering behaviors. For example, Xu et al. [10] have tackled the domain shift challenge by employing domain adaptation techniques such as similarity losses between source and target domains for distribution alignment in the context of pedestrian trajectory prediction. Nevertheless, a shift in geographical location and distinct traffic conditions could make these models ineffective when applied to comprehensive system-wide network. Moreover, traffic dynamics evolve over time and thus the validity of a trained model would be limited to the given temporal window. Motivated by these difficulties, in this paper, we propose a novel sequence-to-sequence graph Transformer-based model (namely Graph Embedded Transformer) to learn the spatial-temporal features of multi-agent trajectory data. Our proposed framework utilizes the embedding capabilities of GCN to help model the spatial features of multi-agent trajectories, while the Transformer performs temporal modelling of the trajectory sequence. To address the issue of co-variate domain shifting due to the differences in traffic distributions in different locations or periods, we introduce a domain adaptation training strategy on top of the spatial-infused attention embedding of the Transformer encoder to adapt the model’s attention across multiple traffic domains. To the best of our knowledge, this is the first work that investigates the feasibility of a domain-adaptable graph Transformer-based framework on generalizing trajectory prediction from source domain to target domain with limited training data. Our contributions are highlighted as follows: • We introduce the Graph Embedded Transformer, which first integrates a Graph Convolutional Network to construct spatial-aware non-Euclidean input embeddings for the Transformer [11]. The Transformer’s encoder module then perform temporal encoding of the spatial input embeddings to encode historical trajectory. The Transformer’s decoder module subsequently generates future trajectory based on the encoder’s latent features. • A domain adversarial training strategy is incorporated on top of the Transformer’s encoder module is to achieve cross-domain transfer learning across traffic domains. In particular, the latent representation of the encoders composed of its spatially-infused attention embeddings are input into a discriminatory layer to minimize the statistical discrepancy between the latent space in different domains. • The efficacy of the proposed Graph Embedded Transformer is validated against the state-of-the-art vehicle trajectory prediction models. Comparative results on the NGSIM-I80 and NGSIM-US101 datasets show that our proposed model achieves superior accuracy across the source and target domains, thus indicates effective domain generalization. The rest of this paper is organized as follows. In Section II, we briefly discuss related work followed by the problem formulation in Section III. In Section IV, we describe our base architectures used and implementation details for domain adaptation. We report our experimental results in Section V. Finally, we conclude this paper in Section VI. Figure 1: Graph Embedded Sequence-to-Sequence Transformer integrated with Domain Adaptation. The target is separated into (a) Training Mode: Last segment from Source + Output shifted right and (b) Inference Mode: Last Segment from Source + Iterative Predictions shifted right."
https://arxiv.org/html/2411.05898v1,"Integrating Object Detection Modality into 
Visual Language Model for Enhanced Autonomous Driving Agent","In this paper, we propose a novel framework for enhancing visual comprehension in autonomous driving systems by integrating visual language models (VLMs) with additional visual perception module specialised in object detection. We extend the Llama-Adapter architecture by incorporating a YOLOS-based detection network alongside the CLIP perception network, addressing limitations in object detection and localisation. Our approach introduces camera ID-separators to improve multi-view processing, crucial for comprehensive environmental awareness. Experiments on the DriveLM visual question answering challenge demonstrate significant improvements over baseline models, with enhanced performance in ChatGPT scores, BLEU scores, and CIDEr metrics, indicating closeness of model answer to ground truth. Our method represents a promising step towards more capable and interpretable autonomous driving systems. Possible safety enhancement enabled by detection modality is also discussed.","The rapid advancements in autonomous driving systems have led to an increased focus on developing end-to-end models capable of handling complex driving scenarios. Despite significant progress, current approaches still face challenges in generalisation, especially when faced with rare or unseen situations. Moreover, the ability to interact with human users and provide explanations for the model’s decisions is crucial for building trust and acceptance of autonomous vehicles. To address these challenges, the recently introduced DriveLM challenge Sima et al., (2023) aims to leverage the power of vision-language models (VLMs) and large language models (LLMs) in the context of autonomous driving. By combining the visual understanding capabilities of VLMs with the reasoning and natural language processing abilities of LLMs, DriveLM seeks to improve generalisation and enable interactive communication between autonomous vehicles and human users. Inspired by the DriveLM framework Sima et al., (2023), this paper presents a novel approach that integrates additional modalities into the LLMs to enhance its perception and reasoning capabilities for autonomous driving tasks. Our method builds upon the Llama-Adapter Zhang et al., 2023a , a parameter-efficient fine-tuning approach that allows for the incorporation of task-specific knowledge into the pre-trained LLM. The common practice for image perception capability in Llama-Adapter is to incorporate a pre-trained image embedder, specifically the CLIP Radford et al., 2021a model with trainable vision transformers (ViT) Dosovitskiy et al., (2020) to generate adaptation queries. The queries are then projected and appended onto layer-wise token embeddings. However, such a perceptual network has critical limitation, which is based on the employment of CLIP. Although CLIP is effective at capturing global contextual information, it struggles to accurately detect and locate objects in the image, as it is primarily trained on perceptual prompts rather than position-level annotations. To address this limitation, we propose the integration of a detection network into the Llama-Adapter framework. The detection network leverages pretrained YOLOS Fang et al., 2021b and postfix vision transformers to process multi-view camera inputs and generate rich feature representations that accurately capture object-specific information, such as positions and bounding boxes. Moreover, we introduced trainable ID-separator token to address confusion of object-camera relationship to concatenated YOLOS output. By incorporating the detection network, our approach enables: 1) The capability to sense local, fine-grained details in the driving scene, complementing the global understanding provided by the perceptual network. 2) Understanding of different viewpoints of BEV images via trainable ID tokens and detection-result-oriented fine-tuning. 3) enhancement of scene understanding robustness and potential defence against vulnerabilities from visual modality. We believe that a standalone detector is crucial for autonomous driving, which helps to ensure safer and more reliable decision-making by improving the capability to manage complex scenarios such as the detection of pedestrians, vehicles, and traffic signs. To evaluate the effectiveness of our approach, we conducted extensive experiments on the DriveLM challenge, comparing our model’s performance against state-of-the-art baselines. We demonstrate significant improvements in the ChatGPT score, the BLEU score, and the CIDEr score (see Sec. 4.2). The main contributions of this paper can be summarised as follows: (1) We identify the limitations of relying solely on CLIP-based features for perception in the Llama-Adapter framework and propose the integration of a detection network to overcome these challenges. (2) We leverage pretrained YOLOS and vision transformers in the detection network to accurately capture object-specific information and enhance the perceptual capabilities of the Llama-Adapter. (3) We demonstrate significant improvements in multiple matrices on overall driving performance through extensive experiments on the DriveLM challenge, showcasing the benefits of integrating the detection network."
https://arxiv.org/html/2411.04348v1,"UEVAVD: A Dataset for Developing UAV’s Eye View 
Active Object Detection","Occlusion is a longstanding difficulty that challenges the UAV-based object detection. Many works address this problem by adapting the detection model. However, few of them exploit that the UAV could fundamentally improve detection performance by changing its viewpoint. Active Object Detection (AOD) offers an effective way to achieve this purpose. Through Deep Reinforcement Learning (DRL), AOD endows the UAV with the ability of autonomous path planning to search for the observation that is more conducive to target identification. Unfortunately, there exists no available dataset for developing the UAV AOD method. To fill this gap, we released a UAV’s eye view active vision dataset named UEVAVD and hope it can facilitate research on the UAV AOD problem. Additionally, we improve the existing DRL-based AOD method by incorporating the inductive bias when learning the state representation. First, due to the partial observability, we use the gated recurrent unit to extract state representations from the observation sequence instead of the single-view observation. Second, we pre-decompose the scene with the Segment Anything Model (SAM) and filter out the irrelevant information with the derived masks. With these practices, the agent could learn an active viewing policy with better generalization capability. The effectiveness of our innovations is validated by the experiments on the UEVAVD dataset. Our dataset will soon be available at https://github.com/Leo000ooo/UEVAVD\_dataset.","I INTRODUCTION In recent years, Unmanned Aerial Vehicles (UAVs) have made a big splash in many applications like traffic monitoring [1], industrial facilities inspection [2], and post-disaster search and rescue [3] owing to its high flexibility and strong maneuverability. Behind these applications, target detection is an indispensable key technology [4], which aims to locate and identify targets from aerial images, providing important prior information for subsequent actions. Since the renaissance of deep learning, the Deep Neural Network (DNN) based methods, such as Faster-RCNN [5], YOLO [6], SSD [7], and their variants have gradually become the mainstream in the field of UAV target detection [8]. However, in the air-to-ground scenario, detecting a target would inevitably encounter challenge of the occlusion among terrestrial objects, which greatly diminishes the detection performance [9]. To address this difficulty, most existing methods make adaptive improvements to the detection model. For example, literature [10] uses the Soft-NMS method to suppress redundant prediction frames during post-processing to alleviate the occlusion problem. However, the detector’s ability of anti-occlusion is still far from satisfactory [11]. Fundamentally, the quality of the input data limits the upper bound of detection performance. If the UAV could autonomously change the viewing angle to get the observation more conducive to identifying the target, the detection performance would greatly improve. To achieve this, Active Object Detection (AOD) is a practical way. Figure 1: A demonstration of the AOD decision process by the UAV platform. The UAV’s policy network gives moving instructions based on observations. It can autonomously decide whether to move and how to move to acquire an ideal observation while minimizing the movement cost. AOD is a subfield that utilizes active vision in target detection, and its objective is to improve the detection result by endowing the mobile sensing platform the ability to actively adjust the viewing angle to better identify the target whose location is given at first [12]. As shown in Fig. 1, the UAV cannot determine the target identity under some viewpoints due to occlusion. At this time, the UAV has to decide whether to move and how to move to acquire an ideal observation while minimizing the movement cost. Deep Reinforcement Learning (DRL) has now become a mainstream framework for solving the AOD problems [13]. Ammirato et al. first used the REINFORCE algorithm for AOD tasks and released the Active Vision Dataset (AVD) collected in indoor environments for developing and benchmarking active vision algorithms [14]. Han et al. solved the AOD problem by a Dualing Deep Q-learning Network (DualingDQN) with prioritized experience replay for the first time [15]. Fang et al. used self-supervised representation learning to improve the sample efficiency of the DRL method [12]. Based on literature [16], Liu et al. incorporated the target crop into the state representation and designed a novel reward function to help the robot approach the target object more smoothly [17]. However, existing AOD researches mainly focu on indoor robotic applications, neglecting the outdoor air-to-ground circumstances. For the indoor environment, there are several datasets concerning active vision published, such as the AVD [14], T-LESS [18], and R3ED [19] dataset, all of which collect the multi-view target images indoor under various occlusion conditions. However, there is no similar counterpart in the scenario of the UAV’s eye view AOD. Although the existing datasets (e.g., VisDrone-DET dataset [20]) for UAV-based target detection commonly cover abundant environment settings and targets, they cannot be used for studying the air-to-ground AOD problem because of lacking densely collected multi-view images over the targets. To fill this gap, we release a UAV’s Eye View Active Vision Dataset (UEVAVD) and hope it can facilitate research on the AOD problem in the air-to-ground scenario. The dataset is collected in the simulated environment constructed by Unreal Engine (UE), focusing on five types of vehicular targets. UAV observes each target under different environment settings (different occlusions, different terrains), and its predefined sampling positions are densely and regularly arranged over the target. Various combinations of the samples can be seen as the continuous observation result by the UAV while flying along different routes, laying a foundation for future research on the UAV AOD problem. Besides, given that a good state representation can dramatically strengthen the effectiveness, robustness, and generalization capability of the agent’s policy [21], we improve the existing AOD method by incorporating inductive biases into the state representation learning process. Specifically, for the AOD task, two aspects of prior knowledge can be exploited. First, due to the partial observability, the state representation of the agent needs to be extracted from the historical observation sequence instead of relying on the single-view observation. Therefore, we utilize a combination of the convolutional neural network and the Gated Recurrent Unit (GRU) to extract state representation from the observation sequence. Second, what should be stressed are the characteristics of the target itself in terms of its appearance, pose, and the positional relationship between the target and its surroundings, whereas features such as the color and texture of the terrestrial objects can be discarded. Therefore, we pre-decompose the scene using the powerful Segment Anything Model (SAM) and filter out the irrelevant information with the masks derived. With these practices, the backbone of the decision network could learn better state representation, thus improving the performance of the agent’s policy. The main contributions of this paper are summarized as follows. 1. We release a new dataset, UEVAVD, aiming to promote the research on the UAV’s eye view AOD problem. On this basis, we could find out how to better exploit the UAV’s autonomy and maneuverability to overcome difficulties like occlusion in UAV-based object detection. 2. We improve the existing DRL-based AOD method by incorporating the inductive bias while learning the state representation. With the practice of scene pre-decomposition and memory based state estimation, the policy learned by the agent could get stronger generalization capability and perform better in the testing environment."
