URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10060v1,CMATH: Cross-Modality Augmented Transformer with Hierarchical Variational Distillation for Multimodal Emotion Recognition in Conversation,"Multimodal emotion recognition in conversation (MER) aims to accurately identify emotions in conversational utterances by integrating multimodal information. Previous methods usually treat multimodal information as equal quality and employ symmetric architectures to conduct multimodal fusion. However, in reality, the quality of different modalities usually varies considerably, and utilizing a symmetric architecture is difficult to accurately recognize conversational emotions when dealing with uneven modal information. Furthermore, fusing multi-modality information in a single granularity may fail to adequately integrate modal information, exacerbating the inaccuracy in emotion recognition. In this paper, we propose a novel Cross-Modality Augmented Transformer with Hierarchical Variational Distillation, called CMATH, which consists of two major components, i.e., Multimodal Interaction Fusion and Hierarchical Variational Distillation. The former is comprised of two submodules, including Modality Reconstruction and Cross-Modality Augmented Transformer (CMA-Transformer), where Modality Reconstruction focuses on obtaining high-quality compressed representation of each modality, and CMA-Transformer adopts an asymmetric fusion strategy which treats one modality as the central modality and takes others as auxiliary modalities. The latter first designs a variational fusion network to fuse the fine-grained representations learned by CMA-Transformer into a coarse-grained representations. Then, it introduces a hierarchical distillation framework to maintain the consistency between modality representations with different granularities. Experiments on the IEMOCAP and MELD datasets demonstrate that our proposed model outperforms previous state-of-the-art baselines. Implementation codes can be available at https://github.com/cjw-MER/CMATH.","Emotion Recognition in Conversation (ERC) aims to identify the corresponding emotion of each utterance in a conversation. It has become a critical problem in recent years due to its potential applications in social media analysis (Kumar et al., 2015), recommendation systems (Xu et al., 2018; Zheng et al., 2022), healthcare services (Pujol et al., 2019), and affective computing systems (Zhou et al., 2020). Early research works usually assume that the emotion of the current utterance depends on its surrounding utterances as well as the speaker’s emotional state. DialogueRNN (Majumder et al., 2019) presents a RNN-based neural network to keep track of individual party states. DialogueGCN (Ghosal et al., 2019) models inter- and self-party dependency with a graph convolutional network to capture emotional inertia of individual speakers. Although these methods demonstrates impressive performance in emotion recognition, they only focus on unique modality (i.e., textual modality). In recent years, multimodal emotion recognition in conversation has attracted considerable attention which attempts to model emotions of utterances by utilizing multiple modality information, such as textual, audio, and visual cues. Recent works mainly employ transformer mechanisms (Lian et al., 2021; Zou et al., 2023) or interaction graphs (Hu et al., 2021, 2022; Nguyen et al., 2024; Tu et al., 2024) to model interactive relationships across different modalities (Ma et al., 2024). Lian et al. (2021) apply a cross-modal transformer to capture intra- and inter-modal interactions. Zou et al. (2023) introduce a prompt transformer for cross-modal information interaction. Hu et al. (2021) propose a multimodal fused graph convolutional network to explore both multimodal and long-distance contextual information. Hu et al. (Hu et al., 2022) design a graph-based dynamic fusion paradigm to reduce redundancy and boost complementarity between different modalities. Nguyen et al. (2024) employ a directed acyclic graph to integrate multimodal information. Tu et al. (2024) develop an adaptive interactive graph network to enhance intra- and cross-modal interactions. Despite previous works have demonstrated promising results, they still suffer from several limitations: Figure 1: An example of a multimodal conversation scenario, in which the conversational contents are from textual, audio and visual modalities. Firstly, they usually treat different modalities as equivalent quality and employ symmetric architectures to fuse information in these modalities. However, in reality, modality qualities would be various, and the symmetric fusion strategy inevitable leads to inferior performance. Secondly, existing fusion methods mainly focus on fusing multimodal information in a unique information granularity by employing a unique-stage fusion strategy. They ignore the rich multi-granularity information embedded in different modalities, including the fine-grained feature representation and the coarse-grained semantic representation. To address the aforementioned issues, this paper proposes a novel Cross-Modality Augmented Transformer with Hierarchical Variational Distillation (CMATH). It consists of two major components: Multimodal Interaction Fusion module and Hierarchical Variational Distillation module. In the Multimodal Interaction Fusion module, we introduce a cross-modality augmented transformer (CMA-Transformer) for fine-grained feature-fusion. Different from previous symmetric fusion strategy, CMA-Transformer adopts an asymmetric fusion strategy, which treats one modality as the central modality and the others as auxiliary ones. To be specific, it attempts to enhance the quality of the central modality by exploring interaction information from these auxiliary modalities. We employ CMA-Transformer for each modality by taking it as the central modality, and obtain its augmented representation. In the Hierarchical Variational Distillation Module, we first introduce a variational fusion network for coarse-grained semantic information fusion, which takes the above augmented fine-grained feature representation for each modality as input and learns a multimodal Gaussian distribution. Then, we introduce a Hierarchical Distillation strategy to maintain the consistency between modality representations with different granularities to further enhance the effectiveness of the semantic distribution integration. To validate the effectiveness of the proposed model CMATH, we conducted extensive experiments on two widely used datasest, i.e., IEMOCAP and MELD. Experimental results demonstrate that CMATH significantly outperforms all baseline methods and achieves the state-of-the-art performance. Specifically, the proposed method obtains 73.90% and 73.96% in terms of two metrices ( i.e., accuracy and weighted F1-score) on the dataset IEMOCAP, with the relative performance improvement of 4.84% and 4.55% over the best performance baseline AdalGN, respectively. We further carry out experiments to analyze the effectiveness of each component of CMATH in depth in order to explore their contribution to the performance of multimodal emotion recognition in conversation. In summary, the main contributions of this work are as follows: • We propose to treat different modalities as nonequivalent quality during the fusing process and propose a novel CMA-Transformer which fuses multi-modality in an asymmetric strategy. We focus on enhancing the quality of each modality by taking it as the central modality and exploiting the remaining modalities as auxiliary information. • We introduce a hierarchical variational distillation framework, which first leverages a light variational fusion module to extract coarse-grained semantic information by fusing the Gaussion distribution of each modality, and then applies a hierarchical distillation strategy to maintain the consistency between modality representations with different information granularities. • We validate the effectiveness of our proposed method on two widely used benchmark datasets, i.e., IEMOCAP and MELD. Experimental results demonstrate its superiority over existing state-of-the-art methods."
https://arxiv.org/html/2411.10436v1,Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization,"Multimodal Large Language Models (MLLMs) are known to hallucinate, which limits their practical applications. Recent works have attempted to apply Direct Preference Optimization (DPO) to enhance the performance of MLLMs, but have shown inconsistent improvements in mitigating hallucinations. To address this issue more effectively, we introduce Hallucination-targeted Direct Preference Optimization (HDPO) to reduce hallucinations in MLLMs. Unlike previous approaches, our method tackles hallucinations from their diverse forms and causes. Specifically, we develop three types of preference pair data targeting the following causes of MLLM hallucinations: (1) insufficient visual capabilities, (2) long context generation, and (3) multimodal conflicts. Experimental results demonstrate that our method achieves superior performance across multiple hallucination evaluation datasets, surpassing most state-of-the-art (SOTA) methods and highlighting the potential of our approach. Ablation studies and in-depth analyses further confirm the effectiveness of our method and suggest the potential for further improvements through scaling up.","Large Language Models (LLMs) have been verified in various field OpenAI (2024); Dubey et al. (2024); Sun et al. (2024), while they encounter challenges such as hallucination. Multimodal Large Language Models (MLLMs) are also known to hallucinate Bai et al. (2024). Specifically, they often produce unfaithful content that does not align with the visual input, which undermines their reliability and practicality, particularly in critical applications such as autonomous driving Cui et al. (2024) or medical tasks Liu et al. (2023a). Hence, addressing MLLM hallucination (M-hallu) is essential. Recent efforts have aimed at mitigating M-hallu through various approaches, including inference-stage strategies like contrastive decoding Leng et al. (2024), and post-hoc corrections that employ external visual models to refine responses Yin et al. (2023). While these methods are simple and training-free, they do not fundamentally enhance the model’s intrinsic capabilities. Meanwhile, some pioneer preference optimization methods like Direct Preference Optimization (DPO) Rafailov et al. (2024) have been introduced, which encourage the model to learn from the comparisons between positive and negative samples, alleviating hallucinations Zhao et al. (2023); Pi et al. (2025). However, most current methods cannot deliver consistent improvements across all types of MLLM hallucination tasks (e.g., VQA and captioning tasks, as shown in our experiments of Table 1). Additionally, it appears that the model’s improvement on specific tasks is closely related to the format of the training data. For instance, the DPO data of SeVa Zhu et al. (2024) primarily consists of VQA, which explains its strong performance on VQA-related hallucination evaluation. However, its results on captioning tasks are relatively unsatisfactory. Moreover, these methods do not explicitly consider diverse sources of M-hallu. Hence, we argue that if we focus on mitigating multimodal hallucinations, we should be able to address diverse types of hallucination causes and tasks, and design hallucination-targeted preference pairs for DPO accordingly. Our goal is to comprehensively alleviate all multimodal hallucination problems, including both discriminative tasks (e.g., VQA) and generative tasks (e.g., image captioning). Different from the hallucinations in LLMs, M-hallu primarily arises from the following three aspects: (1) Insufficient visual capability: This occurs when the MLLM’s visual encoder lacks the necessary strength, being distracted by relatively unimportant visual information, leading to hallucinations; (2) Incapable long-context generation: We observe that hallucinations become more pronounced as the generated content grows longer, similar to long-range forgetting, which needs to be addressed in practical applications; (3) Multimodal conflicts: Multimodal conflicts frequently arise in real-world scenarios due to the inevitable noises in texts and images. MLLMs are more prone to hallucinations with conflicting information existing between text and image Liu et al. (2024b). To address the aforementioned challenges, we propose Hallucination-targeted Direct Preference Optimization (HDPO) to mitigate M-hallu. Our approach constructs hallucination-targeted preference pairs, specifically designed to address various forms and causes of hallucinations. Specifically, we design three types of DPO data reflecting the corresponding hallucination causes as follows: (1) For insufficient visual capability, during the model’s autoregressive decoding, we preserve only some visual tokens with the lowest attention scores to produce targeted negative responses that reflect incorrect visual information distraction, urging MLLMs to pay attention to more effective visual information. (2) For incapable long context generation, we specifically select positive examples from high-quality long-form captions, while creating negative examples where the latter part of the response deviates from the image content, simulating long-form hallucinations. (3) For multimodal conflicts, we add conflicting information with images into prompts to generate negative examples. We provide positive and negative pairs with questions featuring conflicting prefixes to train the model to correctly respond to the question even containing conflicting information. We conduct extensive experiments to evaluate our approach across various types of M-hallu tasks. The results demonstrate that our HDPO framework achieves the overall best performance in effectively mitigating MLLM hallucinations on various tasks. Our contributions are summarized as follows: • We analyze three key causes behind MLLM hallucinations from visual capability, long-context generation, and multimodal conflicts aspects, offering valuable insights to guide future advancements. • Based on these analyses, we propose a novel HDPO, aiming to jointly address all types of M-hallu tasks. To the best of our knowledge, we are the first to adopt hallucination-targeted DPO from diverse aspects with our novel DPO data construction strategies. • Through extensive experiments on different datasets, HDPO demonstrates consistent improvements in all types of M-hallu tasks."
https://arxiv.org/html/2411.10231v1,"A Low-Resolution Image is Worth 1x1 Words: Enabling Fine Image
Super-Resolution with Transformers and TaylorShift","Transformer-based Super-Resolution (SR) models have recently advanced image reconstruction quality, yet challenges remain due to computational complexity and an over-reliance on large patch sizes, which constrain fine-grained detail enhancement. In this work, we propose TaylorIR to address these limitations by utilizing a patch size of 1x1, enabling pixel-level processing in any transformer-based SR model. To address the significant computational demands under the traditional self-attention mechanism, we employ the TaylorShift attention mechanism, a memory-efficient alternative based on Taylor series expansion, achieving full token-to-token interactions with linear complexity. Experimental results demonstrate that our approach achieves new state-of-the-art SR performance while reducing memory consumption by up to 60% compared to traditional self-attention-based transformers.","Image Super-Resolution (SR) remains a foundational yet significant low-vision challenge, aiming to reconstruct High-Resolution (HR) images from Low-Resolution (LR) inputs. The applications encapsulated by SR are broad, spanning security, medical imaging, and even astronomical analysis [20, 21]. Despite the powerful advances made with deep learning, limitations persist, especially regarding high-frequency detail enhancements [19, 13, 9]. With the introduction of deep learning, SR methods have leaned heavily on Convolutional Neural Networks (CNNs) [30, 32, 12], delivering impressive performance. Short after, transformer-based architectures have demonstrated an aptitude for capturing intricate relationships across input sequences, making them a dominant choice for regression-based image SR [25, 6]. Prominent transformer-based architectures are SwinIR [11], Restormer [27], and HAT [3], which have demonstrated promising gains, applying self-attention mechanisms for precise context-aware upscaling. Yet, transformer-based SR methods face notable challenges: high memory requirements and quadratic time complexity associated with self-attention, limiting practicality for real-time and large-scale applications. As a result, current methods reduce the contextual scope within which attention is operating, e.g., 8\times8 windows, and sometimes operate within these windows with patch-sizes greater than 1\times1, leading to non-pixel level detail enhancement. This restriction compromises the ability to capture fine-grained dependencies across the entire image. Figure 1: Overview of TaylorIR’s impact on low-vision applications like image SR. By embedding the input as 1\times1 patches, TaylorIR transforms inputs to long, pixel-level sequences, allowing fine-grained detail enhancement. In addition, it exploits the TaylorShift [22] attention mechanism, which significantly reduces memory consumption compared to classical self-attention, making it a more efficient solution for image SR. To address these issues, we introduce TaylorIR, a novel transformer-based SR approach that makes use of pixel-level detail refinement. Moreover, it substitutes traditional self-attention with TaylorShift [22] attention, a memory-efficient mechanism inspired by Taylor series expansion that approximates full token-to-token interactions with linear complexity. By enabling fine-grained attention on a per-pixel level, our approach significantly enhances context information while improving computational efficiency. For Swin-based SR models like SwinIR, we showcase TaylorSwinIR, a version of SwinIR adapted with TaylorIR to support 1\times1 patch embeddings and large window sizes, i.e., from 8\times8 with 64 tokens to 48\times48 with 2304 tokens. TaylorShift [22] enables efficient, global attention at a lower memory footprint, supporting broader context and enhanced detail without the resource strain of traditional attention mechanisms. Compared to the baseline SwinIR, applying TaylorIR achieves state-of-the-art performance while reducing memory consumption by up to 60%, as demonstrated in extensive experiments across standard benchmarks and exemplified in Figure 1. Our key contributions are as follows: • Pixel-Wise Patch Embedding: We adopt a 1\times1 patch size approach for transformer-based SR, allowing for per-pixel processing and sharper detail reconstruction. • Efficient Large-Window Self-Attention: By enabling extended windows in SwinTransformer-based SR models, our employed TaylorShift [22] attention improves SR quality with reduced memory and computational costs associated with standard window attention. • Improving State-of-the-Art SR models: TaylorSwinIR outperforms current SR models in both PSNR and SSIM across multiple benchmark datasets, establishing a new efficiency-performance balance."
https://arxiv.org/html/2411.10034v1,EveGuard: Defeating Vibration-based Side-Channel Eavesdropping with Audio Adversarial Perturbations,"Vibrometry-based side channels pose a significant privacy risk, exploiting sensors like mmWave radars, light sensors, and accelerometers to detect vibrations from sound sources or proximate objects, enabling speech eavesdropping. Despite various proposed defenses, these involve costly hardware solutions with inherent physical limitations. This paper presents EveGuard, a software-driven defense framework that creates adversarial audio, protecting voice privacy from side channels without compromising human perception. We leverage the distinct sensing capabilities of side channels and traditional microphones—where side channels capture vibrations and microphones record changes in air pressure, resulting in different frequency responses. EveGuard first proposes a perturbation generator model (PGM) that effectively suppresses sensor-based eavesdropping while maintaining high audio quality. Second, to enable end-to-end training of PGM, we introduce a new domain translation task called Eve-GAN for inferring an eavesdropped signal from a given audio. We further apply few-shot learning to mitigate the data collection overhead for Eve-GAN training. Our extensive experiments show that EveGuard achieves a protection rate of more than 97\% from audio classifiers and significantly hinders eavesdropped audio reconstruction. We further validate the performance of EveGuard across three adaptive attack mechanisms. We have conducted a user study to verify the perceptual quality of our perturbed audio.","Loudspeakers are omnipresent in today’s technology-based society. Their use extends beyond facilitating phone calls and video conferencing for the exchange of private information. They have been widely integrated into intelligent mobile and IoT devices, enhancing human-machine interaction through speech recognition. The associated use cases are anticipated to reach a market size of \$150.68 billion by 2032 [57, 47]. As people increasingly rely on loudspeaker-equipped devices, voice privacy is becoming increasingly important. Unfortunately, the diverse sensors in intelligent devices are imposing an alarming risk to voice privacy. Although these sensors are not originally designed for voice recording, they can be repurposed by adversaries to serve as side channels to capture voice-induced vibrations, thereby facilitating unauthorized eavesdropping. For example, the prevalent accelerometers on smartphones have been exploited to eavesdrop on voice playout [58, 22]. Millimeter-wave (mmWave) radars can remotely detect vibrations from sound sources and recover speech signals through walls [20, 21, 54, 62, 61, 77]. Such side-channel speech eavesdropping attacks (SSEAs) lead to severe individual privacy breaches [1] and may compromise sensitive organizational intellectual property [12]. Figure 1: Overview of EveGuard, inserting imperceptible adversarial perturbations to the target speech to protect users’ voice communication from multi-sensor eavesdropping attacks. Existing research has devised hardware-based defenses against SSEAs. For instance, jamming-based methods [28, 59, 71] can block adversarial mmWave SSEAs. However, they may degrade the sensing function of legitimate mmWave devices. Moreover, jamming is generally prohibited in non-military applications [13]. Intelligent reflecting surface (IRS) has also been used as a security shield [53, 56], yet it can only protect its immediate vicinity. As for defending against accelerometer-based SSEAs, vibration motors have been used to generate low-amplitude vibrations that disrupt eavesdropping [76]. However, this method may cause user discomfort and hasten the depletion of smartphone batteries. We propose EveGuard, an innovative software-based defense mechanism to protect against privacy leakage from the loudspeaker-generated voice in SSEAs. As shown in Figure 1, EveGuard mitigates SSEA threats by introducing audio adversarial examples to the original audio signals prior to playback. EveGuard ensures that (i) the perturbed speech signals remain natural to human ears and microphones, and (ii) any attempt by SSEAs to capture and reconstruct the perturbed speech will produce content that is difficult to interpret, both for humans and automated speech recognition systems. Note that EveGuard cannot protect voice from a human speaker when SSEAs target eavesdropping on throat vibrations, a challenge also for state-of-the-art (SOTA) attacks [60]. To attain these salient properties, EveGuard must address four main design challenges. First, it is crucial to ensure the effectiveness of perturbations against SSEAs while maintaining the quality of legitimate voice communication. Existing adversarial speech generation methods commonly rely on additive perturbations, which can introduce noticeable, conspicuous noise [74]. In contrast, EveGuard leverages the distinct sensing mechanisms of the side channels versus traditional microphones or human hearing, i.e., the former only captures low-frequency vibrations whereas the latter senses the subtle changes in air pressure. EveGuard devises a two-stage Perturbation Generator Model (PGM) that integrates: (1) finite impulse response (FIR) convolution to perturb the low-frequency attributes of speech while preserving the speech quality and (2) inaudible low-frequency adversarial perturbations (LFAPs) to corrupt the eavesdropped signals. Second, to automate and optimize the perturbation signal generation, EveGuard requires a new differentiable computational model to represent the SSEA. To tackle this challenge, we propose Eve-GAN, a deep generative network aiming at learning an audio-to-SSEA translation that can map the source audio to the targeted eavesdropped audio. Once trained, Eve-GAN serves as a differentiable layer, enabling end-to-end training of our PGM. Yet training Eve-GAN requires collecting sufficient SSEA samples across various attack scenarios. Additionally, obtaining paired training data is tedious as it requires input-output pairs with the same speaker, speech attributes (e.g., prosody), and utterance content. To address this, we leverage advancements in few-shot unsupervised learning [38]. We propose a few-shot, unpaired audio-to-SSEA translation, which learns to convert source audio into eavesdropped audio by referencing an unpaired SSEA sample. By extracting domain features from the few-shot real-world SSEA samples, Eve-GAN facilitates a generalizable conversion applicable to unseen samples during training. Third, the rapid growth of ML empowers attackers to devise sophisticated SSEAs [21, 22, 54, 58]. For instance, the attacker can transcribe private conversations using speech recognition, and identify digits with audio classifiers. However, the defender has no prior knowledge of the SSEA model deployed by the eavesdropper. To overcome this hindrance, we utilize the transferability of adversarial examples, which means perturbations learned to fool an ensemble of diverse surrogate models can also be effective against unknown black-box models [39, 7]. To this end, we first build a set of surrogate ML models based on multiple hypothetical SSEAs. We then concatenate the PGM with Eve-GAN and ensemble surrogate models to encourage the PGM to learn robust perturbations in an end-to-end manner. Finally, an adaptive attacker who knows the existence of EveGuard may attempt to mitigate the effects of the perturbation. Thus, we apply three preventive techniques to PGM as follows: (1) the use of a discriminator inside the PGM to enforce undetectable constraints, (2) style diversification by integrating VAE-GAN [19] into FIR perturbation generator, and (3) ensuring the LFAP generator uses a random latent vector as input to produce diverse LFAP samples. We implement EveGuard by integrating the above solutions. To evaluate EveGuard, we reproduce white-box SSEAs based on representative eavesdropping sensors, mmWave radar, accelerometer, and optical sensor. Built upon these, we extensively validate EveGuard under various attack settings including distance, orientation, materials, hardware configurations, etc. Our experimental results show that EveGuard achieves a protection rate of more than 97\% from SSEA’s digit classifiers and hinders the recovery of eavesdropped audio with an MCD (Mel-Cepstral Distortion) of over 13.4, and a WER (Word Error Rate) of over 68.2\%. To show that our adversarial audio generated by PGM is imperceptible to humans, we verify the indistinguishability through a user study involving 24 participants. The main contributions of EveGuard are as follows. \bullet We introduce EveGuard, a novel software-driven defense framework that leverages black-box adversarial examples to protect loudspeaker-generated voice from SSEAs. \bullet We design a PGM that leverages the unique features of eavesdropping devices to ensure robust perturbations across diverse attack scenarios, including variations in distance, orientation, materials, and hardware configurations. \bullet We develop Eve-GAN, a differential framework that enables our PGM to learn the distribution of adversarial examples end-to-end, incorporating a few-shot, unpaired audio-to-SSEA translation framework to reduce data collection overhead for training Eve-GAN. \bullet We perform extensive experiments to verify the effectiveness of the EveGuard defense, using both objective metrics and subjective user studies. Audio samples are available at https://eveguard.github.io/demo/."
https://arxiv.org/html/2411.09955v1,Instruction-Guided Editing Controls for Images and Multimedia: A Survey in LLM era,"The rapid advancement of large language models (LLMs) and multimodal learning has transformed digital content creation and manipulation. Traditional visual editing tools require significant expertise, limiting accessibility. Recent strides in instruction-based editing have enabled intuitive interaction with visual content, using natural language as a bridge between user intent and complex editing operations. This survey provides an overview of these techniques, focusing on how LLMs and multimodal models empower users to achieve precise visual modifications without deep technical knowledge. By synthesizing over 100 publications, we explore methods from generative adversarial networks to diffusion models, examining multimodal integration for fine-grained content control. We discuss practical applications across domains such as fashion, 3D scene manipulation, and video synthesis, highlighting increased accessibility and alignment with human intuition. Our survey compares existing literature, emphasizing LLM-empowered editing, and identifies key challenges to stimulate further research. We aim to democratize powerful visual editing across various industries, from entertainment to education. Interested readers are encouraged to access our repository at https://github.com/tamlhp/awesome-instruction-editing.","Visual design tools have become essential in various multimedia fields, although they often require prior knowledge to use effectively. Recent research has emphasised text-guided image editing as a way to make these tools more accessible and controllable (Li et al., 2020a; Patashnik et al., 2021; Gal et al., 2022; Crowson et al., 2022), as in Fig. 1. Studies have shown the effectiveness of diffusion models in creating realistic images and their application in image editing through techniques like swapping latent cross-modal maps for visual manipulation (Ho et al., 2020; Kim et al., 2022). Additionally, specific region editing is made possible through guided masks (Nichol et al., 2022; Avrahami et al., 2022). Moving away from complex descriptions and masks, instruction-based editing has gained traction for its straightforward approach, allowing users to directly command how and what aspects of an image to edit (Hertz et al., 2023; Mokady et al., 2023; Kawar et al., 2023). This paradigm is noted for its practicality, aligning closely with human intuition (Fu et al., 2024; El-Nouby et al., 2019; Fu et al., 2020). The latest text-to-image generative models offer impressive image quality and accuracy in reflecting the given captions, marking a significant leap in content generation technologies (Alayrac et al., 2022; Ramesh et al., 2022; Rombach et al., 2022). Among these advancements, instructional image editing has emerged as a particularly promising application (Brooks et al., 2023). This method streamlines the editing process by eliminating the need for detailed before-and-after captions (Avrahami et al., 2022; Wallace et al., 2023). Instead, users can provide simple, human-readable instructions, such as “change the dog to a cat”, making the editing process more intuitive and aligned with how humans naturally approach image modification (Zhang et al., 2024f). In recent years, advancements in large language models (LLMs) (Touvron et al., 2023; Brown et al., 2020) have dramatically reshaped the landscape of image and video manipulation. The convergence of these technologies has enabled more intuitive, flexible, and high-fidelity editing processes, largely driven by natural language instructions (Wu et al., 2023c; Feng et al., 2024b; Chakrabarty et al., 2023a). These innovations span various applications, from fashion image editing and 3D scene manipulation to video-to-video synthesis and audio-driven editing, empowering users to achieve fine-grained control over visual content. Moreover, Multimodal large language models (MLLMs), building upon the foundational capabilities of traditional LLMs, have extended the boundaries of vision-language tasks (Zhang et al., 2024b). By integrating latent visual knowledge and treating images as input, MLLMs enhance performance in tasks requiring both textual and visual reasoning. The emergence of diffusion models, such as LLaVA (Liu et al., 2024a) and MiniGPT-4 (Zhu et al., 2024a), has further elevated the potential of these frameworks by improving image-text alignment through instruction tuning. These models, including GILL (Koh et al., 2024) and SEED (Ge et al., 2023), facilitate coherent image generation from textual input while preserving rich visual semantics, marking a pivotal evolution in instruction-based editing. This review paper explores the evolution and diversity of techniques underpinning instruction-based image and video editing, synthesizing cutting-edge approaches that integrate human feedback, multimodal signals, and advanced neural architectures. The focus spans from early models leveraging generative adversarial networks (GANs) (Patashnik et al., 2021) to the latest innovations using diffusion models, including frameworks like Pix2Pix (Brooks et al., 2023), InstructBrush (Zhao et al., 2024), and FlexEdit (Nguyen et al., 2024a). Additionally, specialized models for audio- and video-driven editing, such as Noise2Music (Huang et al., 2023a) and Fairy (Wu et al., 2023a), are examined, demonstrating the versatility and creativity unlocked by these methods. By analyzing over 100 recent key publications, this review delves into key technological breakthroughs, evaluates their effectiveness, and considers potential avenues for further innovation. From 3D image editing (Sabat et al., 2024) to fashion editing (Wang and Ye, 2024), this paper highlights how these models are reshaping industries ranging from entertainment and fashion to education and remote sensing (Han et al., 2024b). Through this comprehensive overview, we aim to identify emerging trends, challenges, and opportunities in the growing field of text-driven, instruction-guided image and video editing. Differences with Existing Surveys. Our survey differs from existing surveys in its specific focus on instruction-based image and video editing empowered by LLMs. While Li et al. (Li et al., 2024b) focus on the integration of various modalities for retrieval tasks, our paper highlights the use of instructions for precise visual editing. Qin et al. (Qin et al., 2024) evaluate instruction-following abilities in LLMs but does not address their application in visual manipulation, which is a key focus of our review. Similarly, Yin et al. (Yin et al., 2023) address instruction-following in language models with a broader emphasis on ethical concerns, whereas our review emphasizes the technical advancements in using these capabilities for visual content generation and editing across various domains, including image, video, and 3D manipulation. Closest to our review is (Zhan et al., 2023), which explores generative AI techniques but lacks the detailed exploration of instruction-following in visual editing contexts, as seen in our paper. Especially, we consider caption-based image editing (Chen et al., 2018; Couairon et al., 2022b; Lin et al., 2023a) is a part of instruction-based image editing but we do not fully focus on the former. Rather, we are interested in user-friendly instructions that have practical implications for broad audience when editing images. Table 1 summarises the difference between our surveys and existing ones. Table 1. A comparison between existing surveys Survey Focused Task Focused Modality Key Contents (Qin et al., 2024) Editing Text Instruction development, Evaluation concerns (Yin et al., 2023) Editing Text LLM-empowered instructions, Instruction tuning (Li et al., 2024b) Retrieval Image, Video, Audio Image-text composite retrieval, Multimodal composite retrieval (Zhan et al., 2023) Generation Image Text guidance, Audio guidance, Sketch guidance, etc. Ours Editing Image, Video, Audio Instruction mechanisms, Augmentations, Learning stragies, Model designs, Loss functions Paper Collection Methodology. To map the research landscape on this subject, we used a range of keyword searches and combinations such as “image editing”, “image manipulation”, “text-guided”, “instruction-followed”, and “instruction-guided”. Initially, we relied on platforms like Google Scholar, Semantic Scholar, and the AI-enhanced tool Scite.ai to compile an initial set of studies. We then expanded this collection by conducting backward searches, reviewing the references in the selected papers, and forward searches to identify works that cited them. To ensure accuracy, we manually evaluated the relevance of each study, given that some focused on related areas like image generation or retrieval but employed similar techniques. This thorough process ultimately resulted in the identification of over 100 pivotal papers relevant to the field. Contributions. The main contributions of this survey are: • Comprehensive Review: This study provides a comprehensive review of LLM-empowered image and media editing. We have gathered and summarised an extensive body of literature, including both published works and pre-prints up to October 2024. • Process-based Taxonomy: We have organised the literature according to the developmental stages of an image editing framework. Fig. 2 presents the taxonomy we developed to structure the existing works in the field. • Optimisation Tools: We have curated a set of optimisation tools for developing end-to-end image editing frameworks, covering model designs, learning strategies, instruction mechanisms, data augmentations, and loss functions. • Practical Applications: We discuss various practical applications across multiple domains, including style, fashion, face editing, scene manipulation, charts, remote sensing, 3D, speech, music, and video editing. • Challenges and Future Directions: Instruction-guided visual design remains an emerging area of research. Based on the surveyed literature, we identify several unresolved challenges and propose future research directions to explore more editing use cases and user-friendly editing controls. • Sources, Datasets, and Metrics: To support empirical research, we provide a comprehensive overview of available source codes, datasets, and evaluation metrics that have been utilised in the field. • Online Updating Resource: To support ongoing research in LLM-empowered visual design, we have created an open-source repository111https://github.com/tamlhp/awesome-instruction-editing, which consolidates relevant studies, including links to papers and available code. Figure 2. Process-based taxonomy of instruction-guided image editing."
https://arxiv.org/html/2411.09952v1,GGAvatar: Reconstructing Garment-Separated 3D Gaussian Splatting Avatars from Monocular Video,"Avatar modelling has broad applications in human animation and virtual try-ons. Recent advancements in this field have focused on high-quality and comprehensive human reconstruction but often overlook the separation of clothing from the body. To bridge this gap, this paper introduces GGAvatar (Garment-separated 3D Gaussian Splatting Avatar), which relies on monocular videos. Through advanced parameterized templates and unique phased training, this model effectively achieves decoupled, editable, and realistic reconstruction of clothed humans. Comparative evaluations with other costly models confirm GGAvatar’s superior quality and efficiency in modelling both clothed humans and separable garments. The paper also showcases applications in clothing editing, as illustrated in Figure 1, highlighting the model’s benefits and the advantages of effective disentanglement. The code is available at https://github.com/J-X-Chen/GGAvatar/.","Reconstructing realistic clothed digital humans and their garments is a significant task in computer graphics and computer vision. This type of work aims to synthesize high-resolution clothed human body images from an unprecedented view or generate human imagery in a novel pose. Previous research has delved into explicit modelling methods under costly capture systems to obtain suboptimal reconstruction outcomes (Seitz et al., 2006; Szeliski et al., 1996). Recent advancements have shifted towards direct construction from single RGB images or monocular videos, utilizing models with implicit representation such as Neural Radiance Field (NeRF)(Mildenhall et al., 2020) to capture fine textures on the surface. However, these models (Peng et al., 2021; Weng et al., 2022; Feng et al., 2022; Chen et al., 2021) require dozens of training hours. Consequently, current studies(Jiang et al., 2023; Geng et al., 2023; Qian et al., 2024; Lei et al., 2024; Hu et al., 2024b; Kocabas et al., 2024; Hu et al., 2024a) are increasingly focused on enhancing rendering speed and modelling efficiency by turning neural rendering techniques into Instant-NGP(Müller et al., 2022) or 3D Gaussian Splatting (3DGS)(Kerbl et al., 2023). Nevertheless, the lack of disentanglement functions in these existing avatar models may constantly limit their applicability in real-world scenarios. This paper argues that an ideal avatar model should not only produce high-quality, rapid, and thorough reconstruction results, but also possess the decoupling capability necessary for applications such as virtual try-ons. Unfortunately, creating a perfect editable and drivable avatar is a demanding task that presents several challenges. Firstly, to effectively disentangle the body and garments, integrity and anti-interference properties must be maintained between distinct components. Specific estimations are required for the unsupervised areas where the human body is obstructed. Secondly, a precise transformation between canonical space and various pose spaces must be established to locate the partitioned point cloud at the target position. Lastly, it is essential to capture diverse and intricate clothing details, including textures, and to achieve high-quality reconstructions from sparse monocular inputs, particularly for loose-fitting attire. However, works such as (Li et al., 2024; Feng et al., 2023; Corona et al., 2021; Jiang et al., 2020; Li et al., 2022; Kim et al., 2024; Pons-Moll et al., 2017) are limited to recovering geometry without providing corresponding appearance information. In response to these challenges, this paper proposes a novel framework, GGAvatar, designed to construct realistic avatars from monocular videos while effectively and completely separating the garments. Specifically, this paper builds and fits garment templates alongside the corresponding body template to achieve a preliminary state of separation and interference resistance, resulting in partitioned point sets. Phased trainable modules (isolation and joint training) reasonably prevent the intersection of point sets during the training process. Subsequently, the target Gaussian positions are ensured by constructing deformation fields based on a concentric skeleton. Simultaneously, high-quality rendering is accomplished using 3DGS. Notably, GGAvatar enables thorough separation of clothed humans in novel view synthesis tasks from monocular inputs—potentially a first in this field, to my knowledge. The paper evaluates the GGAvatar model by comparing it with baseline approaches and other works on the People Snapshot Dataset (Alldieck et al., 2018) or the ZJU Mocap Dataset (Peng et al., 2021). The results indicate that GGAvatar demonstrates a high level of reconstruction quality for clothed humans, comparable to that of other 3DGS-based models. Notably, the proposed model outperforms nearly every traditional NeRF-based model while exhibiting significantly faster training speeds—approximately hundreds of times faster than the NeRF counterparts. Furthermore, ablation studies are conducted to validate the effectiveness of each component. To highlight the superiority and practical utility of GGAvatar, this paper compares it with existing non-fully decoupled models on clothing transfer. The contributions are summarized as follows: • This paper proposes the GGAvatar model, based on phased training methods, to achieve high-quality and efficient construction for various viewpoints or pose synthesis tasks of clothed humans. • The method of constructing parameterized templates for garments is introduced to solve the challenge of complex clothes modelling. • The GGAvatar enables a thorough separation between different garments, allowing applications such as colour editing and clothing transfer."
https://arxiv.org/html/2411.08885v1,"Enhancing Lie Detection Accuracy: A Comparative Study of Classic ML, CNN, and GCN Models using Audio-Visual Features","Inaccuracies in polygraph tests often lead to wrongful convictions, false information, and bias, which have significant consequences for both legal and political systems. Recently, analyzing facial micro-expressions has emerged as a method to detect deception; however, current models have not reached high accuracy and generalizability. The purpose of this paper is to aid in remedying these problems. The unique multimodal transformer architecture used in this paper improves upon previous approaches by using auditory input, visual facial micro-expressions, and manually transcribed gesture annotations, moving closer to a reliable non-invasive lie detection model. Visual and auditory features were extracted using Vision Transformer and OpenSmile models respectively, which were then concatenated with the transcriptions of participants’ micro-expressions and gestures. Various models were trained for classification instances of lies and truth using these processed and concatenated features. The CNN Conv1D multimodal model achieved a 95.4% average accuracy. However, further research is still required to create higher-quality datasets and even more generalized models for more diverse applications.","Lie detection has been a recurring focus of research and technological innovation in law enforcement and criminal justice. According to a survey conducted by the University of Wisconsin-La Crosse, about 75% of survey respondents reported telling zero to two lies per day; lying comprised 7% of total communication, with 79% of the lies being told face-to-face and 21% being mediated [3]. Current technologies, such as polygraphs, have focused on biological responses like blood pressure to detect lies. However, these methods are unpredictable and easily flawed. Recently, research has begun to focus on various other indicators of deception, including facial micro-expressions and audio cues [4]. Facial micro-expressions (ME) are intentional or involuntary localized and momentary movements of the face, usually lasting less than 500 milliseconds [2]. Despite advancements in lie detection techniques, traditional methods remain intrusive, subjective, and often inaccurate. Detecting deception through ME and speech analysis presents a significant challenge due to the subtle and brief nature of these cues. As shown in Table I, traditional methods have high variance and relatively low accuracy. This study aims to address these limitations by developing a non-intrusive, objective, and highly accurate method for detecting deception using both ME and audio signals. Accurate lie detection is crucial in various fields, including security, legal systems, and psychological evaluations. The primary objective of this study is to establish an AI model that can differentiate between truth and deception with high accuracy by analyzing audio, visual cues in videos, and extracted gestures. Audio dialogue, visuals, and gestures all help to distinguish between deception and truthfulness, making them important features to consider [23]. Therefore, the Real-life Deception Detection Dataset from the University of Michigan was used, which includes 121 videos of deception and truthfulness and a CSV file for gestures. Visuals and audio were extracted from the videos, and OpenSMILE and Vision Transformer (ViT) were used to extract features from audio and video, respectively. Classical machine learning models like Random Forest Classifiers and Logistic Regression can serve as accurate baseline references for a binary classification task like truth and lie. Yet to build off of that, by leveraging advanced neural network models, such as Conv1D, Graph Convolutional Networks (GCN), and CNN LSTM, the accuracy can be increased. TABLE I: Estimated accuracy of different test types in detecting deception and truthfulness Test type Detecting deception Detecting truthfulness Laboratory studies CQT – Polygraph 74%–82% 60%–83% CIT – Polygraph 76%–88% 83%–97% ERP 68% 82% fMRI 84% 81% Field studies CQT – Polygraph 84%–89% 59%–75% CIT – Polygraph 42%–76% 94%–98% This study addresses the following research questions: How effective is the proposed AI model in detecting lies compared to traditional methods and some recent AI models? Which features carry the highest weights in prediction? Deception detection technology has the potential to revolutionize various fields. In law enforcement, it could improve interrogation outcomes and border security by identifying deceptive behavior. In the legal system, it could be utilized to assess the credibility of courtroom testimonies and negotiations. Additionally, applying this technology to financial services could aid in detecting fraudulent claims and reducing the risk of financial fraud. Previous studies have experimented with various machine learning models. For instance, a study by Soldner et al. implemented the Random Forest model, achieving the best accuracy of 69%, as shown in II [5]. Insights from this paper suggest expanding our dataset and exploring additional modalities to enhance the model’s accuracy and reliability in lie detection. Furthermore, Random Forest, being a machine learning technique, cannot handle complex relations as well as multimodal data, which is a limitation of the mentioned study. Moreover, most traditional AI models fall short in reliability and accuracy, often leading to false positives or negatives [1]. A study conducted by the University of Michigan in 2015 analyzed trial videos using micro-facial expressions and achieved a rudimentary accuracy rate of 83.05% using neural networks [6]. Aligning different data types and achieving 83.05% accuracy are two main advantages of the study. TABLE II: Best Results Of Study [2]. Features Acc. Linguistic 66% Dialog 57% Non-verbal 61% All Features 69% This paper is organized as follows: analyzing previous work, discussing the paper’s methods (data collection, data analysis, feature extraction, and implementation guide for the tested models), presenting the results of different tested models, comparing the paper’s results with other studies using the same dataset, and providing a discussion including limitations and recommendations. The paper concludes with a summary of key findings and a look forward."
https://arxiv.org/html/2411.08882v1,A Novel Multimodal System to Predict Agitation in People with Dementia Within Clinical Settings: A Proof of Concept,"Dementia is a neurodegenerative condition that combines several diseases and impacts millions around the world and those around them. Although cognitive impairment is profoundly disabling, it is the noncognitive features of dementia, referred to as Neuropsychiatric Symptoms (NPS), that are most closely associated with a diminished quality of life. Agitation and aggression (AA) in people living with dementia (PwD) contribute to distress and increased healthcare demands. Current assessment methods rely on caregiver intervention and reporting of incidents, introducing subjectivity and bias. Artificial Intelligence (AI) and predictive algorithms offer a potential solution for detecting AA episodes in PwD when utilized in real-time. We present a 5-year study system that integrates a multimodal approach, utilizing the EmbracePlus wristband and a video detection system to predict AA in severe dementia patients. We conducted a pilot study with three participants at the Ontario Shores Mental Health Institute to validate the functionality of the system. The system collects and processes raw and digital biomarkers from the EmbracePlus wristband to accurately predict AA. The system also detected pre-agitation patterns at least six minutes before the AA event, which was not previously discovered from the EmbracePlus wristband. Furthermore, the privacy-preserving video system uses a masking tool to hide the features of the people in frames and employs a deep learning model for AA detection. The video system also helps identify the actual start and end time of the agitation events for labeling. The promising results of the preliminary data analysis underscore the ability of the system to predict AA events. The ability of the proposed system to run autonomously in real-time and identify AA and pre-agitation symptoms without external assistance represents a significant milestone in this research field.","Dementia is a neurodegenerative condition that leads to a progressive decline in cognition and is one of the leading causes of death, disability, and hospitalization in Canada and worldwide. Currently, dementia is the seventh cause of death worldwide [1]. Globally, over 55 million individuals are living with dementia; as the ratio of older people increases, this number will grow to 78 million by 2030 and 139 million by 2050, making dementia a major global health crisis [1]. In addition to cognitive and functional decline, people living with dementia (PwD) also experience non-cognitive neuropsychiatric symptoms (NPS) during their illness [2]. NPS commonly includes agitation, aggression, apathy, symptoms of psychosis, delusions, hallucinations, and disturbances of sleep and appetite. Among NPS, agitation and aggression (AA) occur frequently in severe cases and are a common source of distress for patients and caregivers [3]. They commonly occur during care and are believed to be manifestations of perceived or real unmet needs [3]. Behaviors of AA include pacing, rocking, gesturing, restlessness, shouting, scratching, throwing objects, and destroying property [4]. These symptoms are the leading cause of hospitalizations, extended length of stay as inpatients, and increased demand for placement in long-term care facilities [5]. AA enormously burdens PwD, their families, caregivers, and healthcare systems. In current practices, AA are commonly assessed through caregiver reports. Many observational methods have been developed, including the Neuropsychiatric Inventory (NPI) [6] and the Cohen-Mansfield Agitation Inventory (CMAI) [7]. These assessments are based on manual observations, which are subject to potential bias depending on the caregiver’s memory or emotional state. It is possible to address these limitations by using Artificial Intelligence (AI) and predictive algorithms to predict episodes of AA in PwD before they occur. By 2025, AI technologies are expected to be worth an estimated $36 billion (US) [8]. There is growing evidence that combining AI and sensory technologies to develop a solution for NPS detection will guide the provision of personalized interventions for PwD [9, 10, 11, 12]. The timely detection of critical events in PwD using digital technologies is gaining wide acceptance. For example, smartwatches are being used to help people with dementia [13, 14] and detect epileptic seizures to prevent the development of severe complications [15, 16]. Multiple attempts have been made to create predictive algorithms to detect AA in PwD using several physiological parameters and/or environmental data [17, 18]. Such studies incorporate wearable sensors to capture patient data and use it in AA prediction using machine learning algorithms. Moreover, AI has also been employed in video-based monitoring systems to monitor and detect AA behavior in PwD [19, 20]. To the best of our knowledge, there is no current video surveillance system operating in a hospital setting to detect AA in PwD in real-time due to privacy concerns. The use of multimodal sensing, including wearable sensors and camera footage, for real-time detection of AA and pre-agitation behavior in PwD has not been extensively explored to date. The combination of multimodal sensing and artificial intelligence holds great promise in effectively detecting and predicting AA in real-time. This approach could lead to the timely implementation of preventive strategies or therapies, which could reduce care costs and decrease the frequency of critical incidents among this demographic [21, 22]. This study aims to understand the complicated behaviors of PwD and predict AA in PwD. We carried out this study in the Geriatric Dementia Unit (GDU) and the Geriatric Transitional Unit (GTU) at the Ontario Shores Center for Mental Health Sciences [23] for 5 years. We integrate a multimodal approach, combining biometric data from the EmbracePlus wristband [24] and video data from CCTV cameras installed in common areas in both units. These biometric signs are analyzed to determine the possible correlation with abnormal behaviors. Data collected by these devices, along with the results of the data analysis, is compared against the nurse notes collected via custom forms to confirm the AA and pre-agitation events. The cameras deployed in the designated places automatically detect AA behavior. The developed AI model detects abnormal behavior from body activity recognition in real-time using deep learning techniques. The cameras allow us to document the exact time of the incident for further analysis. The data and analysis then determine personalized pre-agitation conditions using our proposed classification system. To assess our system, we conducted a pilot study focusing on patient acceptance of wristbands, complemented by video camera validation and multimodal sensor data for predicting AA in PwD. The EmbracePlus wristband was crucial for collecting physiological signals like Electrodermal Activity (EDA), heart rate, skin temperature, and movement data. The camera system was also a key component in detecting body movements and recording agitation events. Both systems are tested on three participants who were successfully recruited at the Ontario Shores Centre for Mental Health Sciences. We achieved high accuracy in detecting AA through comprehensive data preprocessing, feature extraction, and the ExtraTrees classification algorithm. Additionally, AA detection was enhanced by analyzing real-time video feeds with OpenPose-generated skeletal keypoints and employing RNN-based neural networks, particularly LSTM and GRU [25]. These networks, optimized for real-time processing, facilitate timely interventions. The pilot study demonstrated the system’s effectiveness through both the wristband and video detection."
https://arxiv.org/html/2411.09266v1,"How Good is ChatGPT at Audiovisual Deepfake Detection: A Comparative Study of ChatGPT, AI Models and Human Perception","Multimodal deepfakes involving audiovisual manipulations are a growing threat because they are difficult to detect with the naked eye or using unimodal deep learning-based forgery detection methods. Audiovisual forensic models, while more capable than unimodal models, require large training datasets and are computationally expensive for training and inference. Furthermore, these models lack interpretability and often do not generalize well to unseen manipulations. In this study, we examine the detection capabilities of a large language model (LLM) (i.e., ChatGPT) to identify and account for any possible visual and auditory artifacts and manipulations in audiovisual deepfake content. Extensive experiments are conducted on videos from a benchmark multimodal deepfake dataset to evaluate the detection performance of ChatGPT and compare it with the detection capabilities of state-of-the-art multimodal forensic models and humans. Experimental results demonstrate the importance of domain knowledge and prompt engineering for video forgery detection tasks using LLMs. Unlike approaches based on end-to-end learning, ChatGPT can account for spatial and spatiotemporal artifacts and inconsistencies that may exist within or across modalities. Additionally, we discuss the limitations of ChatGPT for multimedia forensic tasks.","Figure 1: Illustration of audiovisual deepfake manipulations. Original video content is represented as RVRA (real video with real audio. Through deepfake manipulation techniques, three manipulated types are generated: FVRA (fake video with real audio), RVFA (real video with fake audio), and FVFA (fake video with fake audio). Blue text represents the “real modality” of the video content, while red text represents the “fake modality”. Synthetic multimedia content has become both innovative and a significant threat in recent years. Deepfake images and videos created using artificial intelligence (AI) and deep learning (DL) techniques have attracted public and academic attention. This synthetic content is generated by generative adversarial networks (GANs) [1] and more sophisticated AI techniques such as diffusion models [2]. While deepfake technology has many innovative applications in education, entertainment, and other fields [3], it is a double-edged sword that can be used for unethical purposes, such as pornography, political defamation, identity theft, fraud, misinformation, and disinformation [4, 5, 6]. Unethical use of this technology can lead to political instability and social violence [6]. On the one hand, deepfake technology continues to evolve to create more convincing and realistic fake multimedia content. Social media, on the other hand, plays a catalytic role in spreading such content. Therefore, timely detection of deepfake content is crucial to avoid any damage and loss to human society [4]. Audiovisual deepfakes that involve multimodal manipulation are a more convincing type of forgery, with attackers attacking audio, video, or both modalities. Unimodal video forgery detectors [7, 8, 9, 10] and spoofed audio detectors [11, 12, 13, 14] are generally unable to identify forgeries across multiple modalities, although they may be good at detecting forgeries in the specific modality they focus on. To address this challenge, the research community has developed sophisticated tools and algorithms to detect audiovisual forgeries in videos. These specialized tools require knowledge of multimedia forensics as well as knowledge of deep learning. Furthermore, these tools do not generalize well to other unseen datasets and manipulations. Large language models (LLMs) are a major advancement in the field of artificial intelligence. They are trained on a large amount of data and can perform well in various natural language processing (NLP) tasks such as text generation, summarization, classification, completion, sentimental analysis, machine translation, and question answering. Their applications even go beyond the aforementioned NLP tasks and can be used as writing assistants, learning tools, productivity tools, coding assistants, software development, healthcare, legal assistance, entertainment, and more. Despite being primarily designed for NLP tasks, OpenAI’s ChatGPT can analyze image, audio, and video content. Taking advantage of its support for multimodal input, we studied the potential and limitations of ChatGPT for audiovisual deepfake detection. The research questions we aimed to address in this study are as follows: • Can ChatGPT perform multimedia forensic tasks? • Is ChatGPT capable of detecting forgery based on artifacts in audio and visual modalities? • What is the role of prompt engineering in using ChatGPT to detect audiovisual deepfakes? • Which performs better at identifying forgeries in audiovisual deepfakes, ChatGPT, humans, or AI models? • How interpretable is ChatGPT for forgery detection? • What are the limitations of ChatGPT in detecting multimodal deepfakes? The main contributions of our work are threefold: • We explore for the first time the potential of ChatGPT for audiovisual forgery detection tasks. • We compare the performance of ChatGPT with human and state-of-the-art AI models on audiovisual forgery detection tasks. • We highlight the strengths and limitations of ChatGPT on audiovisual forgery detection tasks."
https://arxiv.org/html/2411.09053v1,Information Need in Metaverse Recordings - a Field Study,"Metaverse Recordings (MVRs) represent an emerging and underexplored media type within the field of Multimedia Information Retrieval (MMIR). This paper presents findings from a field study aimed at understanding the users information needs and search behaviors specific to MVR retrieval. By conducting and analyzing expert interviews, the study identifies application scenarios and highlights challenges in retrieving multimedia content from the metaverse. The results reveal existing application scenarios of MVRs and confirm the relevance of capturing time-series data from the graphical rendering process and related input-output devices, which are also highly relevant to user needs. Furthermore, the study provides a foundation for developing retrieval systems tailored to MVRs by defining use cases, user stereotypes, and specific requirements for MVR Retrieval systems. The findings contribute to a better understanding of information search behaviors in MVR Retrieval and pave the way for future research and system design in this field.","The growth rate of multimedia creation is high. Digital Cameras are ubiquitous and social media has led to an immense media generation, and, in recent years, boosted short form video content. Furthermore, the COVID crisis has given remote technologies for communication a push, such as increased use of video conferencing, virtual conferences. Another trend re-emerged in the last years, the idea of an everlasting virtual space, where people meet and life together - the metaverse. The growth rate of usage of platforms [1] like Roblox [2] or Minecraft [3] show, that people are heavily using virtual worlds. Trend reports assume an even higher usage in the future [4]. It is likely, that people will create recordings of experiences in the virtual world, like they do in the real world. Early versions of this can be seen as YouTube videos [5] for entertainment purposes. Multimedia Information Retrieval (MMIR) [6] is the field in computer science, which addresses indexing and retrieval of multimedia content. The metaverse is build on virtual worlds, which are basically computer generated multimedia. Therefore, we examine the integration of MVRs in MMIR. In earlier publications [7, 8] we have outlined the differences between metaverse content and other media types, i.e. format, structure and content. The analysis of the differences revealed a lack of support of MMIR for metaverse content. The further integration of MVR in MMIR should be grounded on user demands. There is a noted gap in the existing literature regarding the information needs specifically related to MVR Retrieval. Understanding these information needs is essential for developing effective MVR retrieval systems. MVR as a new multimedia type introduces challenges for integration in MMIR, related to the capture, organization, and retrieval of content generated in virtual environments. One open question is whether such user sessions are recorded, which would be indirectly recorded metaverse content, in the field and for which applications. Another significant challenge concerns the formats of data available in metaverse environments and how they align with user interests. Unlike traditional media recordings, MVRs can capture not only video and audio but also complex data formats such as movement patterns, eye-tracking information, and biosensor data. The potential for data capture in the metaverse is considerable, yet it remains unclear how these rich data formats align with users needs and interests. For example, while systems may be capable of reconstructing virtual scenes with mathematical precision, it is unclear whether users find such detailed data useful or necessary for their tasks. A further challenge lies in understanding users information needs and how they search for and retrieve MVRs. Little is known about the information searching behavior specific to MVRs, and existing search systems are not yet tailored to the unique attributes of virtual worlds. Traditional search filters, such as date ranges, location, and event types, may not fully capture the complexity of user needs in the metaverse. Moreover, it is unclear how users express their information needs when searching for MVRs, as past queries and behaviors have not yet been documented. The lack of understanding of the technical capabilities and user interests shows a critical research gap. Understanding which data types users value and how they search for MVRs is crucial for integrating MVR in MMIR and developing effective MVR Retrieval systems. In this paper, we present a field study conducted with a small expert group. Based on interviews, we describe application scenarios and search behaviors of users, and how MMIR can support them. The following sections present an overview of the metaverse and related technologies, Information Retrieval (IR), and MMIR in Section 2. Section 3, describes the field study design. Section 4 presents the results of the interviews. Finally, Section 5 summarizes the presented work and discusses future work."
https://arxiv.org/html/2411.08334v1,Enhancing Multimodal Query Representation via Visual Dialogues for End-to-End Knowledge Retrieval,"Existing multimodal retrieval systems often rely on disjointed models for image comprehension, such as object detectors and caption generators, leading to cumbersome implementations and training processes. To overcome this limitation, we propose an end-to-end retrieval system, Ret-XKnow, to endow a text retriever with the ability to understand multimodal queries via dynamic modality interaction. Ret-XKnow leverages a partial convolution mechanism to focus on visual information relevant to the given textual query, thereby enhancing multimodal query representations. To effectively learn multimodal interaction, we also introduce the Visual Dialogue-to-Retrieval (ViD2R) dataset automatically constructed from visual dialogue datasets. Our dataset construction process ensures that the dialogues are transformed into suitable information retrieval tasks using a text retriever. We demonstrate that our approach not only significantly improves retrieval performance in zero-shot settings but also achieves substantial improvements in fine-tuning scenarios. Our code is publicly available: https://github.com/yeongjoonJu/Ret˙XKnow.","With the growing demand for information retrieval across diverse applications, such as internet search and knowledge-based question answering, precise and efficient retrieval from multimodal queries involving pairs of images and text has emerged as a critical challenge. In such multimodal queries, each modality independently provides insufficient information to retrieve the desired passages within a knowledge base, necessitating the integrated understanding of the visual and textual queries. Existing Vision-Language (VL) retrievers (Qu et al. 2021; Luo et al. 2021; Gao et al. 2022; Lin et al. 2023) often depend on disjointed models for object detection or image captioning to provide visual information. The reliance on disjointed models complicates the training process (e.g., the models should be fine-tuned for separate tasks in domain adaptation) and increases the likelihood of propagating erroneous predictions. The utilization of the captioning model also lacks the fine-grained information embedded within the images. Previous approaches (Lin et al. 2023; Luo et al. 2023) have attempted to address these drawbacks. However, as shown in Fig. 1, they result in lower performance in a zero-shot setting than a text retriever that does not use image information despite their pre-training for the image-text alignment. Lin et al. (2023) introduce token-level embeddings and utilize two types of visual representations: textual description of the image and feature-based visual embeddings with regions of interest by an object detector. They pre-train the retriever to map token-level visual embeddings into the linguistic space of a text retriever and then fine-tune it by adding image captions to the textual queries. Such the retriever captures fine-grained features of the image by employing visual embeddings with captions. They also facilitate modality interaction between the textual query and the image by relying on textual information, but the mechanism also results in complex implementations and inefficient retrieval due to multiple steps. Figure 1: Zero-shot retrieval performance on OK-VQA (Google Search). Ret-XKnow outperforms the text-based retriever, while other multimodal retrievers fall short, relying on the textual query in the pre-training stage. Luo et al. (2023) present an end-to-end approach that projects multimodal features encoded via self-attention into linguistic space with a pre-training task called VL-ICT, to detach the dependency on the disjointed modules. They automatically construct a pre-training dataset by applying the Inverse Cloze Task (ICT) (Lee, Chang, and Toutanova 2019) to a multimodal knowledge base. However, this approach has significant limitations. First, the dataset does not adequately reflect the variety and complexity of real-world queries, as it only removes the title or caption from a sentence extracted as the query without considering the image. Second, in the constructed pairs of a multimodal query and the corresponding passage, the passage can often be matched solely with the textual content of the query. This occurs because the target passage is selected from the content following a sentence with a title or caption, thereby hindering learning rich image representations. To tackle these issues, we propose two approaches: (1) an end-to-end Retriever to eXpand visual Knowledge, Ret-XKnow, and (2) a Visual Dialogue-to-Retrieval (ViD2R) pre-training dataset constructed from visual dialogues containing distinct relevant passages for various queries related to the same image. Ret-XKnow endows a text retriever with the understanding of multimodal queries in the context of efficient information retrieval, inspired by the concept of partial convolutions (Liu et al. 2018), which fill undesired pixels with surrounding pixel information. We compress visual embeddings to focus on the visual information relevant to the textual query by leveraging the relevance scores between visual embeddings and textual query representations as an adaptive mask. We only attach a vision encoder to the text retriever with only a few layers, utilizing output embeddings of the penultimate layer in the vision model for fine-grained visual representations. Our model architecture does not allow the direct intervention of textual query features in the pre-training stage, achieving modality interaction without fusing text features with image features. Through this architecture, we introduce both the late-interaction mechanism (Khattab and Zaharia 2020) for pre-indexing documents and the modality interaction without requiring an additional document encoder and disjointed models. Recent advances in multimodal language models have produced several multimodal dialogue datasets (Zhu et al. 2023; Liu et al. 2023; Wang et al. 2023; Huang et al. 2023) for training models to perform tasks based on visual content. These datasets consist of multi-turn sessions with query-response pairs centered around a single image, providing precise and comprehensive information pertinent to the query and image. The response with detailed information can improve multimodal retrieval tasks by linking image understanding with complex textual queries. Whereas, such datasets are not appropriate for directly training retrievers due to the gap between explicit responses and broader passages. To bridge this gap, we transform the visual dialogue datasets into a format suitable for retrieval tasks through three simple steps: pre-processing, neural filtering, and response-to-passage conversion. Our construction process is applicable in diverse domains and modalities since our approach only requires multimodal dialogue datasets and sets of documents related to the target domain. Our retriever, Ret-XKnow pre-trained with the ViD2R dataset, outperforms various baselines in zero-shot retrieval performance across four multimodal datasets in an end-to-end manner. Furthermore, we demonstrate that the pre-training dataset curated via our construction method effectively mitigates the issue of overlooking visual features during the pre-training stage, leading to remarkable performance in fine-tuning settings. Our contributions are summarized as follows: • We propose Ret-XKnow, an end-to-end multimodal retriever that overcomes the limitations of disjointed models by dynamically focusing on visual features relevant to the textual query. • We introduce the ViD2R dataset, which transforms visual dialogue datasets into a format suitable for training VL retrievers, leading to significant improvements in zero-shot retrieval performance. • We demonstrate the comprehensive adaptability of Ret-XKnow by fine-tuning three downstream tasks. Our end-to-end retriever even shows comparable performance on baseline methods utilizing image captioning."
https://arxiv.org/html/2411.08307v1,Perceiver: A Multi-cale Perceiver with Effective egmentation for Long-Term Expressive Symbolic Music Generation,"Music generation has progressed significantly, especially in the domain of audio generation. However, generating symbolic music that is both long-structured and expressive remains a significant challenge. In this paper, we propose PerceiverS (Segmentation and Scale), a novel architecture designed to address this issue by leveraging both Effective Segmentation and Multi-Scale attention mechanisms. Our approach enhances symbolic music generation by simultaneously learning long-term structural dependencies and short-term expressive details. By combining cross-attention and self-attention in a Multi-Scale setting, PerceiverS captures long-range musical structure while preserving performance nuances. The proposed model, evaluated on datasets like Maestro, demonstrates improvements in generating coherent and diverse music with both structural consistency and expressive variation. The project demos and the generated music samples can be accessed through the link: https://perceivers.github.io.","Recent advancements in music generation, especially in audio generation models such as AudioLDM [1], MusicGen [2], and Jen-1 [3], have demonstrated significant progress, with these models capable of generating highly natural-sounding music. However, symbolic music generation, an area where models can generate and manipulate music in symbolic form, plays a crucial role in the field of music generation, particularly due to its editable nature. This allows for operations such as cutting and rearranging different sections or substituting instrument timbres, enabling human involvement in high-quality music production during the post-processing stage. Compared to audio generation, symbolic music offers a further level of abstraction, making it easier for machine learning models to capture deeper musical characteristics and understanding. However, the symbolic music generation still faces two key challenges. First, despite the advancements, many of the most expressive datasets, recorded from live performances and recording studios, are seldom used compared to manually created MIDI-file datasets. The primary reason is that they lack detailed annotations, making it harder for models to learn complex structures. Furthermore, due to computational limitations, these models cannot fully capture the context of an entire piece of music. Techniques, such as chunking and quantization, are often employed to reduce computational complexity, leading to the loss of crucial musical details and making it difficult for models to grasp the full structure of a composition. Second, the waterfall-like approaches that use abstract structural representations as conditions for music generation tasks have enabled the generation of structured music. However, such methods rely heavily on handcrafted feature engineering. Our objective is to design and develop a model that is capable of learning the long-range dependencies in music without relying on explicit structural annotations. The emergence of Transformer Attention technologies, such as Perceiver AR [4], has made it possible to access much longer contextual dependencies. It allows for the simultaneous learning of musical structure and the generation of expressive performances. Perceiver AR has demonstrated the ability to attend to a context length of up to 32,768 tokens using the Maestro dataset [5], where the query in cross-attention attends to significantly longer key/value pairs [4]. However, this approach has also introduced challenges. Specifically, the causal mask, when applied with the default input sequence segmentation, does not fully conceal tokens that should not be visible during autoregressive training and generation, which ultimately degrades the quality of the generated music. Additionally, when using ultra-long context as a condition, the model tends to generate identical or similar repetitive segments as the sequence length increases due to issues with high similarity in the context of neighboring tokens, which leads to a high token autocorrelation [6] tendency. To address the challenges mentioned above, in this paper, we propose PerceiverS, a novel model that addresses the causal masking issue by incorporating Effective Segmentation. Additionally, PerceiverS employs Multi-Scale attention to mitigate the high token autocorrelation problem that arises from relying solely on long-range dependencies. Specifically, by adjusting the input sequence segmentation to start from the head segment with an effective causal mask and aggressively increasing the segment length up to the maximum input sequence length, we resolve the learning limitations caused by the causal mask in Perceiver AR [4]. Additionally, by incorporating Multi-Scale masks across multiple layers of cross-attention, the model considers both ultra-long and short-range attention simultaneously. This approach addresses the limitation in Perceiver AR, which focuses solely on long-range attention [4]. Different from Perceiver AR, our approach enhances symbolic music generation by effectively capturing both long-term structural dependencies and short-term expressive details. Through improved segmentation and multi-scale attention mechanisms, PerceiverS generates coherent, diverse music without relying on extensive structural annotations. Extensive experiments have been conducted to evaluate the performance of the proposed PerceiverS. The experimental results demonstrate an average 40% improvement in Overlap Area when measured against the original training dataset, highlighting a substantial advantage of our approach over Perceiver AR [4] in generating high-quality symbolic music."
https://arxiv.org/html/2411.07899v1,Rendering-Oriented 3D Point Cloud Attribute Compression using Sparse Tensor-based Transformer,"The evolution of 3D visualization techniques has fundamentally transformed how we interact with digital content. At the forefront of this change is point cloud technology, offering an immersive experience that surpasses traditional 2D representations. However, the massive data size of point clouds presents significant challenges in data compression. Current methods for lossy point cloud attribute compression (PCAC) generally focus on reconstructing the original point clouds with minimal error. However, for point cloud visualization scenarios, the reconstructed point clouds with distortion still need to undergo a complex rendering process, which affects the final user-perceived quality. In this paper, we propose an end-to-end deep learning framework that seamlessly integrates PCAC with differentiable rendering, denoted as rendering-oriented PCAC (RO-PCAC), directly targeting the quality of rendered multiview images for viewing. In a differentiable manner, the impact of the rendering process on the reconstructed point clouds is taken into account. Moreover, we characterize point clouds as sparse tensors and propose a sparse tensor-based transformer, called SP-Trans. By aligning with the local density of the point cloud and utilizing an enhanced local attention mechanism, SP-Trans captures the intricate relationships within the point cloud, further improving feature analysis and synthesis within the framework. Extensive experiments demonstrate that the proposed RO-PCAC achieves state-of-the-art compression performance, compared to existing reconstruction-oriented methods, including traditional, learning-based, and hybrid methods.","Nowdays, the advent of 3D visualization techniques is revolutionizing the way we interact with digital content, offering a more immersive experience compared to traditional 2D formats. This technological shift has unlocked new possibilities for multimedia applications, including immersive communication [1], virtual and augmented reality experiences [2], and the preservation of cultural artifacts [3]. A key representation in 3D visualization is the point cloud, which consists of points in 3D space, often accompanied by attributes such as color and surface properties. The fidelity of a visual scene is closely tied to the number of points in the point cloud, which can range from thousands to billions, posing significant challenges for data management. To address this, various compression techniques [4, 5, 6, 7, 8, 9] have been developed to reduce point cloud data size while maintaining quality. In line with the push of the industry for efficient data handling, standardization bodies like the Joint Photographic Experts Group (JPEG) and the Moving Picture Experts Group (MPEG) have recognized the importance of point cloud data formats and have embarked on creating standards for their compression. This has led to the development of the Geometry-based Point Cloud Compression (G-PCC) and Video-based Point Cloud Compression (V-PCC) standards [10, 11, 12], both designed to meet the specific compression requirements of point cloud data. In addition, numerous deep learning-based methods [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23] have been developed for point cloud compression, showing promising performance compared to these traditional approaches. In point cloud visualization scenarios, the content rendered on the screen represents the final output seen by the end-user. Considering the compression process, the transformation of raw point clouds into visually coherent images is a complex process. Critical factors include the accuracy of point positioning, the richness of attribute information, and the effectiveness of rendering algorithms. Javaheri et al. investigated the impact of rendering on multiple MPEG point cloud coding solutions, motivated by the consideration that the perceived quality of point cloud data highly depends on the rendering solution [24]. Despite this, current point cloud compression methods focus on preserving the accuracy of point locations and associated attributes after reconstruction, often overlooking the effect of the rendering process. In this paper, we propose to integrate the compression and rendering modules within a single deep-learning framework to achieve an optimal trade-off between compression efficiency and the quality of rendered multiview images. The key feature of our framework, namely RO-PCAC, is the differentiability of the rendering module. After the rate-distortion loss is calculated, the rendering module generates necessary gradients that guide the network to jointly minimize bitrate and optimize image quality. The rate-distortion loss includes the estimated bitrate and the image error between the original and rendered multiview images. Additionally, inspired by the success of point cloud transformers [25, 26, 27] and sparse tensor-based convolution [28, 29, 30, 31], we propose a sparse tensor-based transformer, termed SP-Trans, to enhance feature analysis and synthesis. It adaptively constructs local neighborhoods within a fixed-size 3D window, aligning with local point density and ensuring a consistent number of neighbors for the enhanced local self-attention mechanism. Cosine similarity is employed within the local self-attention layer to handle varying point numbers and ensure its functionality in sparse regions. This design reduces computational complexity while leveraging the ability of the transformer to capture complex relationships and long-range dependencies between points. RO-PCAC outperforms state-of-the-art compression methods, including the traditional, learning-based, and hybrid approaches, on benchmark datasets such as 8i Voxelized Full Bodies (8iVFB) [32] and Owlii dynamic human mesh (Owlii) [33]. In summary, the main contributions of this paper are: 1. we make the first attempt to construct a general rendering-oriented compression framework for the color attribute of 3D point cloud data, termed RO-PCAC. RO-PCAC consists of a learning-based point cloud attribute compression module and a differentiable rendering module, enabling end-to-end training to jointly minimize the bitrate and optimize the quality of rendered multiview images. Furthermore, RO-PCAC supports the modular design for both the compression and rendering components. 2. we propose a sparse tensor-based transformer, SP-Trans, which captures the attribute relationships within local regions of point clouds and enhances the feature representation. SP-Trans adaptively constructs local neighborhoods and leverages a local self-attention mechanism, supported by cosine similarity, to process the density-varied regions, thereby improving compression efficiency. The remainder of this paper is organized as follows: Section II provides a review of related works. Section III describes the motivation of the proposed RO-PCAC and details the framework. Section IV first presents the training and testing setup, followed by objective and subjective quality comparison results, along with ablation studies. Finally, concluding remarks are offered in Section V."
https://arxiv.org/html/2411.07539v1,Harmonizing Pixels and Melodies: Maestro-Guided Film Score Generation and Composition Style Transfer,"We introduce a film score generation framework to harmonize visual pixels and music melodies utilizing a latent diffusion model. Our framework processes film clips as input and generates music that aligns with a general theme while offering the capability to tailor outputs to a specific composition style. Our model directly produces music from video, utilizing a streamlined and efficient tuning mechanism on ControlNet. It also integrates a film encoder adept at understanding the film’s semantic depth, emotional impact, and aesthetic appeal. Additionally, we introduce a novel, effective yet straightforward evaluation metric to evaluate the originality and recognizability of music within film scores. To fill this gap for film scores, we curate a comprehensive dataset of film videos and legendary original scores, injecting domain-specific knowledge into our data-driven generation model. Our model outperforms existing methodologies in creating film scores, capable of generating music that reflects the guidance of a maestro’s style, thereby redefining the benchmark for automated film scores and laying a robust groundwork for future research in this domain. The code and generated samples are available at https://anonymous.4open.science/r/HPM.","A film score - the original music accompanying a film - plays a pivotal role in enriching the film’s emotional landscape, deepening narrative complexity, character arcs, and thematic exploration. Creating a film score requires a harmonious, multidisciplinary effort that includes composers, orchestrators, musicians, sound engineers, and music editors working in concert to blend composition, arrangement, and recording with the film’s visual narrative. Automating the film score production process through artificial intelligence research represents a significant stride toward cost efficiency and innovation in film score production. Translating the visual modality into music has been a promising area in cross-modal generative modeling [52; 7; 54; 47]. Some works rely on Codebook [5] and have less flexibility to generate novel sounds outside it. Some works use pre-defined symbolic musical representations such as MIDI (Musical Instrument Digital Interface), REMI (revamped MIDI-derived events), and Piano-Roll that can be autoregressively generated [8; 7; 10]. Nevertheless, they require an excellent MIDI synth to render audio from generated MIDI and struggle to model timbre and expressiveness. Recently, some works directly produce spectrograms based on diffusion models [9; 11; 30] with textual prompts. Diffusion model-generated audio exhibits fidelity to prominent prompt features, including genre, tempo, instrumentation, and mood, while also capturing the fine-grained semantics of the prompt. Yu et al. [47] utilize a latent conditional diffusion probabilistic model to synthesize long-term conditional waveforms and generate soundtracks for dances and sports scenarios. Recent work111https://huggingface.co/spaces/fffiloni/image-to-music-v2 involves translating visual content into textual descriptions [24; 27], utilizing robust text-to-music diffusion models [19; 30; 11] for music generation. However, this two-step process (visual-to-text and then text-to-music) introduces additional complexity and points of potential error. Key visual elements crucial for emotion conveyance, like colors, lighting, and composition, may be inadequately represented in text. In contrast, a direct visual-to-music translation could more effectively capture and convert these visual cues into musical elements, preserving the original emotional tone of the visuals. While conceptually straightforward, generating music from film diffusion models faces notable challenges. 1) The field significantly lacks datasets that carefully pair film clips with their corresponding music. Compiling such datasets is challenging and resource-intensive. 2) Achieving the thematic musical pieces align with the film’s narrative and emotional tone presents a complex challenge, introducing integration difficulties within the current frameworks of diffusion models. 3) There is an absence of objective metrics to measure the quality of music generated for film clips, complicating the evaluation of progress and the refinement of models. To bridge the existing void in automated film scores, we establish a comprehensive dataset - FilmScoreDB. FilmScoreDB contains 32,520 film clip-music pairs, totaling 90.35 hours, featuring compositions from renowned film composers. This collection serves to infuse our data-driven diffusion model with targeted domain-specific insights. We present HPM, a novel approach tailored for generating film scores and transferring composition styles. Leveraging a diffusion model, our framework introduces a low-rank, parameter-efficient fine-tuning mechanism to ControlNet, complemented by a film encoder designed to assimilate semantic, emotional, and aesthetic dimensions of film content. Additionally, we introduce an enhanced metric incorporating originality and recognizability to assess the quality of generated music. Utilizing the design above, our model framework adeptly generates film scores, operating independently and under specific styles’ control, demonstrating a significant advancement in film score generation. In conclusion, our main contributions are three-fold: • We are the first to tackle the automatic film score and composition style transfer challenge, concentrating on generating music for specific film segments. • We introduce a novel benchmark that includes a comprehensive film score dataset, a refined set of evaluation metrics, and a well-defined baseline model to foster subsequent research efforts. • Through comprehensive experimental analysis, our framework demonstrates exceptional capability in generating film scores and in transferring composition styles. Our framework outperforms existing methods across all evaluated metrics and sets a new benchmark for the musical arts community."
https://arxiv.org/html/2411.07772v1,Automatic Album Sequencing,"Album sequencing is a critical part of the album production process. Recently, a data-driven approach was proposed that sequences general collections of independent media by extracting the narrative essence of the items in the collections. While this approach implies an album sequencing technique, it is not widely accessible to a less technical audience, requiring advanced knowledge of machine learning techniques to use. To address this, we introduce a new user-friendly web-based tool that allows a less technical audience to upload music tracks, execute this technique in one click, and subsequently presents the result in a clean visualization to the user. To both increase the number of templates available to the user and address shortcomings of previous work, we also introduce a new direct transformer-based album sequencing method. We find that our more direct method outperforms a random baseline but does not reach the same performance as the narrative essence approach. Both methods are included in our web-based user interface, and this—alongside a full copy of our implementation—is publicly available at https://github.com/dylanashley/automatic-album-sequencing","Album sequencing is the process of taking a music album and ordering it so that listening to it in that order produces a desired emotional response in the listener. Despite its importance in producing an impactful music album, album sequencing has received comparatively little attention from the artificial intelligence community. Our previous research [1] introduced a way to compress different kinds of media down into an ultra-low dimensional representation. This representation captures their relevancy to the overarching story induced by the ordering of the collection they belonged to, i.e., their narrative essence. This is accomplished by using neural networks and contrastive learning [2, 3]. Then, evolutionary algorithms are used to learn a set of template curves and a novel curve-fitting algorithm to fit the narrative essence of new media collections to these template curves. The above was principally done with music albums from the FMA dataset [4], though it is shown that this applies to other forms of media as well. There are two key issues with our previous work. First, our previous method requires knowledge of advanced machine learning techniques, making it inaccessible to many people who perform album sequencing. Second, it requires a complex pipeline with (1) a neural network to extract the narrative essence followed by (2) a separate evolutionary algorithm to learn a set of templates and then (3) a fitting algorithm to produce a final ordering. This is a highly complex and particularly problematic setup that does not allow information like the genre of an album to flow between the narrative essence and the final ordering, resulting in genre-agnostic templates. Here, we address both of the aforementioned issues. To address the latter issue, we introduce a new approach that replaces the full pipeline with a single Transformer [5, 6, 7]. While this does not outperform the more complicated pipeline proposed in our previous work, the new simpler pipeline still outperforms a random baseline, making it useful for automatic album sequencing. Next, to address the former issue, we implement and release a dedicated user-friendly web-based interface that allows a less technically inclined user to run both the narrative essence-based and the new simplified album sequencing approaches on the user’s own music. We release this interface alongside a complete implementation of our approach publicly at https://github.com/dylanashley/automatic-album-sequencing In summary, our contributions are as follows: (1) We introduce a new direct method to perform automatic album sequencing. (2) We show that, despite the simpler pipelineethod outperforms a random baseline. (3) We release a web-based user interface tool that makes automatic album sequencing accessible to a less technical audience."
https://arxiv.org/html/2411.07751v1,SAV-SE: Scene-aware Audio-Visual Speech Enhancement with Selective State Space Model,"Speech enhancement plays an essential role in various applications, and the integration of visual information has been demonstrated to bring substantial advantages. However, the majority of current research concentrates on the examination of facial and lip movements, which can be compromised or entirely inaccessible in scenarios where occlusions occur or when the camera view is distant. Whereas contextual visual cues from the surrounding environment have been overlooked: for example, when we see a dog bark, our brain has the innate ability to discern and filter out the barking noise. To this end, in this paper, we introduce a novel task, i.e. Scene-aware Audio-Visual Speech Enhancement (SAV-SE). To our best knowledge, this is the first proposal to use rich contextual information from synchronized video as auxiliary cues to indicate the type of noise, which eventually improves the speech enhancement performance. Specifically, we propose the VC-S2E method, which incorporates the Conformer and Mamba modules for their complementary strengths. Extensive experiments are conducted on public MUSIC, AVSpeech and AudioSet datasets, where the results demonstrate the superiority of VC-S2E over other competitive methods. We will make the source code publicly available. Project demo page: https://AVSEPage.github.io/","In our daily living environments, speech signals are often distorted by various environmental background noises during their propagation. Speech enhancement (SE) is a task aiming at isolating the clean speech in the presence of noise interference, resulting in improved speech intelligibility and perceptual quality [1, 2, 3, 4]. It enables natural and effective Human-Robot Interaction (HRI) and plays a crucial role in various applications, such as hearing aids, mobile communication, automatic speech recognition [5, 6, 7], speaker verification [8], and speaker tracking [9, 10, 11]. These applications underscore the importance of SE in realistic scenarios. Traditional signal processing-based SE approaches, which are derived from the assumed properties on speech and noise, are incapable of suppressing highly non-stationary noise sources [12, 13, 14]. In the past decade, with the advent of deep learning technology and increased computational resources, supervised speech enhancement solutions has achieved great success [2]. Figure 1: Our proposed SAV-SE task where outputs from the audio and visual encoder are fused to refine and generate the enhanced audio. By incorporating the visual context from noise environments, it significantly enhances speech quality, particularly in situations where traditional audio-only techniques falter. Despite the significant strides made in the field, the challenge of noise reduction without inflicting artifacts on the speech signal persists, particularly in dynamic environments characterized by non-stationary and multi-source noise [15]. This difficulty is further compounded by the need to maintain the integrity of the speech signal, ensuring that the naturalness of the human voice is preserved. To address this challenge, researchers have been exploring cutting-edge signal processing methodologies and sophisticated machine learning paradigms. One promising solution involves the use of neural networks, which has demonstrated great capabilities in extracting features and separating signals from complex acoustic environments. A variety of network architectures are trained to learn the underlying patterns in noisy audio data, thus accomplishing the objective of speech enhancement [16]. Each of these models contributes unique strengths to the task of learning and generalizing from noisy audio data. For example, Multi-Layer Perceptrons (MLPs) are proficient in detecting intricate, non-linear data patterns, whereas Recurrent Neural Network (RNN) effectively manage the sequential dependencies in audio signals. Temporal Convolutional Network (TCN) excel in capturing long-range dependencies without suffering from the vanishing gradient problem that plagues standard RNN. The Transformer architecture, featuring self-attention, has transformed the field by allowing models to process any part of the input sequence, which is crucial for tasks involving widespread noise-speech relationships. The Mamba architecture [17], as the latest advancement, further extends the capabilities of noise reduction and speech enhancement. Researchers have increasingly acknowledged the importance of maintaining semantic, temporal, and spatial coherence between audio and video sources [18, 19]. This motivates attempts to use video information as a complement of audio input to recover details that are lost in audio-only scenarios. Existing Audio-Visual Speech Enhancement (AVSE) schemes often exploit temporal synchronized facial and lip movements to improve the clarity and perception of enhanced speech [20, 21, 22]. Despite outperforming audio-only SE systems, they are infeasible in many practical scenarios (e.g., outdoors or pandemic period) where human visual cues are not available. Moreover, inaccurate face or lip detection (e.g., in low-quality videos) may also result in degraded performance. In contrast, visual cues of environmental information, such as noise scenes or background objects emitting the noise, are easier to capture. It is more practical to use visual environmental cues to provide a valuable complement to speech enhancement. Thus, to fully leverage audio-visual information to enhance uni-modal learning, it is essential to consider these modality-specific attributes. In this paper, we introduce a novel AVSE framework, as illustrated in Figure 1, which uses visual information of the surrounding scenes as an auxiliary prompt to improve SE performance. Specifically, it addresses the limitations of current technologies, particularly in scenarios where an accurate capture of facial or lip information is not available. The contributions of this paper are summarized as follows: 1. We introduce a novel and more practical scene-aware AVSE task, namely SAV-SE. Unlike existing AVSE studies that rely primarily on visual facial and lip movements, this paper explores auxiliary visual contextual cues from the surrounding scenes to mitigate environmental background noise. 2. We are the first to explore selective State Space Model (SSM) for audio-visual speech enhancement. Specifically, we propose a Visual-prompting ConMamba for Scene-aware Speech Enhancement (VC-S2E), a novel approach that leverages audio-visual modalities to improve speech quality and intelligibility. Built upon innovative hybrid convolution-SSM architecture, ConMamba can capture both long-range global interactions and localized fine-grained feature patterns. 3. We comprehensively evaluate our proposed method across three widely used AV datasets. The results consistently confirm the superiority of our \text{VC-}\text{S}^{2}\text{E} over other competing methods in speech quality and intelligibility. Meanwhile, the visualization analysis illustrates that visual focal areas locate at the sounding object, demonstrating the contribution of visual scene information."
https://arxiv.org/html/2411.07650v1,"Understanding Audiovisual Deepfake Detection:
Techniques, Challenges, Human Factors
and Perceptual Insights","Deep Learning has been successfully applied in diverse fields, and its impact on deepfake detection is no exception. Deepfakes are fake yet realistic synthetic content that can be used deceitfully for political impersonation, phishing, slandering, or spreading misinformation. Despite extensive research on unimodal deepfake detection, identifying complex deepfakes through joint analysis of audio and visual streams remains relatively unexplored. To fill this gap, this survey first provides an overview of audiovisual deepfake generation techniques, applications, and their consequences, and then provides a comprehensive review of state-of-the-art methods that combine audio and visual modalities to enhance detection accuracy, summarizing and critically analyzing their strengths and limitations. Furthermore, we discuss existing open source datasets for a deeper understanding, which can contribute to the research community and provide necessary information to beginners who want to analyze deep learning-based audiovisual methods for video forensics. By bridging the gap between unimodal and multimodal approaches, this paper aims to improve the effectiveness of deepfake detection strategies and guide future research in cybersecurity and media integrity.","The proliferation of smart digital devices such as mobile phones, laptops, tablets, and other digital gadgets, coupled with the accessibility of social media platforms, has promoted the exponential growth of multimedia content (images, videos, and audio) on the internet. This growth is further fueled by technological advances [1], including various deep generative networks [2] [3]. However, this accessibility heightens the need for caution because it can lead to the prevalence of disinformation. Despite this, many people still stick to the trend of the antiquated phrase “seeing is believing” and share multimedia content without considering its authenticity or verifying its digital integrity. Deepfake technology, or sophisticated Artificial Intelligence (AI) models, enable deep learning (DL) tools to manipulate media (images, videos, and audio) to generate hyper-realistic fake content that deceives viewers. Deepfake is AI-generated media that has been deceptively altered by superimposing a source face in a video onto a target face, manipulating the speech in an audio clip, or both. The vast amount of data available online in the form of images, videos, and audio to train such models makes detecting such forgeries increasingly challenging. The impact of deepfakes is critical because we still trust photographic and audio recording evidence. The emergence of realistic and subtle production tools makes fake content incredibly believable and harder to distinguish from genuine content [4]. The rapid spread of harmful and uncontrolled content from fake media has serious imminent impacts and reduces trust in journalism and news providers [5] [6]. Deepfake media content can be exploited to fuel political or religious tensions between countries [7], spread misleading information or rumors between political parties [5] [8], deceive the public [5], engaging in revenge porn [8], defame celebrities [8], promote fraud and identity theft [9], and create political chaos or publicity in a campaign [10]. Generative Adversarial Networks (GAN) [2] and Variational Autoencoders (VAE) [3] are sophisticated DL models for generating counterfeit content. In GAN, the generator network and the discriminator network are the two main components, and these two networks are opposed to each other. The generator aims to generate plausible data, while the discriminator determines the real data from the fake data generated by the generator. Similarly, VAE is an unsupervised learning method consisting of encoder and decoder architectures. VAE is used to create high-quality, hyper-realistic fake content by merging and/or superimposing existing media (images or videos) onto source media for the purpose of deception. Currently, AI-synthesized videos are mainly divided into three different generation types [11] [12]. (1) Head puppetry/puppet master is a counterfeit video generation technique based on the target person animating like a puppet. (2) Face swap aims to generate a video of the target person by swapping the target person’s face with that of the source person while retaining the same facial expression as the target person. (3) Lip-sync is another deepfake video generation method whose main goal is to transform a person’s lips to be synchronized or consistent with the target audio. This technique tends to manipulate the lip region in such a way that the target of the attack appears to be saying things they never said in reality. In the past few years, immense progress in the field of automatic video editing and a great interest in face manipulation techniques have been noticed. Advances in manipulation tools and open-source codes allow even naive users to use deepfake technology like an expert in a few simple steps. This technological advancement has a wide range of positive applications in the fields of visual effects, photography, education, film industry, virtual reality, video games, cinema, and entertainment. However, it also poses significant challenges in terms of authenticity verification and prevention of malicious use. To overcome these challenges, researchers have made many attempts and proposed DL-based unimodal forgery detection methods [13, 14, 15, 16, 17]. Figure 1: Volume of research into audiovisual deepfakes between 2017 and 2023. The detection of visual manipulation in videos has long been the focus of researchers, while the identification of audio forgeries has often been overlooked. Recently, however, the trend of sound manipulation has grown rapidly along with visual alterations, leading to bimodal fabrication that enhances the authenticity of fake content and makes detection difficult [18, 19, 20, 21]. Fig. 1 highlights the research community’s growing interest and concern in audiovisual deepfakes. The number of publications on audiovisual deepfakes has increased significantly in recent years, demonstrating both beneficial progress and growing concerns. The integration of multimodality is proven to be beneficial in various research fields [22, 23, 24]. Consequently, researchers have used various DL techniques that exploit audio and visual features for video forgery detection. Nonetheless, existing media forensics research is lacking in investigations that analyze methods for generating and detecting video deepfakes using audio and visual modalities. Table I lists an overview of relevant studies. Our study was strongly motivated by the lack of attention paid to audiovisual deepfakes in surveys, highlighting the urgent need to focus research on audiovisual deepfakes, including their generation, how to mitigate their harmful effects, and a summary of existing audiovisual deepfake detection methods. TABLE I: Comparison of survey studies related to deepfake detection. Reference Year Contribution Verdoliva [25] 2020 A discussion of video deepfakes from a forensic perspective, with an emphasis on the limitations of current forensic detection methods. Mirsky et al. [26] 2021 An in-depth analysis of field-specific generation techniques and a brief discussion of detection methods. Yu et al. [27] 2021 A detailed analysis of forged video synthesis and detection techniques, with a focus on face manipulation. Rana et al. [28] 2022 A comprehensive review of deepfake detection methods proposed during 2018-2020. Nguyen et al. [29] 2022 A comprehensive overview of deepfake generation and detection techniques and a discussion of challenges and future research directions in the field. Masood et al. [30] 2023 An analysis of the generation and detection of audio and visual deepfakes and a discussion of datasets. Mubarak et al. [31] 2023 An analysis of audio, visual, and text-based deepfakes, with a focus on detection methods. Figure 2: Taxonomy of deepfakes. Generally speaking, as shown in Fig. 2, there are four types of deepfakes, namely text deepfakes, audio deepfakes, visual deepfakes, and audiovisual deepfakes. Audiovisual deepfakes are a combination of acoustic and visual manipulation that can enhance manipulated videos to make them look more believable, and have received a lot of attention in recent years. This study specifically provides an in-depth review of the latest audiovisual deep learning solutions to improve the detection of challenging video deepfakes. To the best of our knowledge, we are the first to perform a comprehensive analysis of existing DL-based methods that exploit audio and visual manipulations in videos for automatic deepfake detection. Our important contribution also includes a comprehensive discussion of publicly available datasets relevant to this task. The main contributions of our work are as follows: • We provide an unprecedented survey that systematically analyzes key detection and generation methods for audiovisual deepfakes, with a special emphasis on automatic video deepfake detection methods. • We highlight the challenges, limitations, and human perception in the field of audiovisual deepfake detection. Furthermore, we outline research directions for future developments in this field. • We summarize and present publicly available datasets that can be used to train multimodal/audiovisual deepfake detectors. The remainder of this paper is organized as follows. Section II introduces different types of deepfakes. Section III discusses video deepfake detection methods. Section IV classifies detection methods that exploit visual and acoustic streams. In Section V, we review publicly available datasets for audiovisual deepfake detection. Section VI presents performance metrics and evaluation. Section VII examines human perception of audiovisual deepfakes. Section VIII discusses several aspects of the deepfake challenge and potential research directions. Finally, Section IX concludes this survey."
https://arxiv.org/html/2411.07428v1,"Just label the repeats 
for in-the-wild audio-to-score alignment","We propose an efficient workflow for high-quality offline alignment of in-the-wild performance audio and corresponding sheet music scans (images).111Video examples: https://bit.ly/jltr-ismir2024 Code: https://github.com/irmakbky/jltr-alignment Corresponding author: Irmak Bukey ¡ibukey@cs.cmu.edu¿ Recent work on audio-to-score alignment extends dynamic time warping (DTW) to be theoretically able to handle jumps in sheet music induced by repeat signs—this method requires no human annotations, but we show that it often yields low-quality alignments. As an alternative, we propose a workflow and interface that allows users to quickly annotate jumps (by clicking on repeat signs), requiring a small amount of human supervision but yielding much higher quality alignments on average. Additionally, we refine audio and score feature representations to improve alignment quality by: (1) integrating measure detection into the score feature representation, and (2) using raw onset prediction probabilities from a music transcription model instead of piano roll. We propose an evaluation protocol for audio-to-score alignment that computes the distance between the estimated and ground truth alignment in units of measures. Under this evaluation, we find that our proposed jump annotation workflow and improved feature representations together improve alignment accuracy by 150\% relative to prior work (33\%\to 82\%).","Sheet music has been used as a primary means of communicating musical ideas for centuries. Accordingly, sheet music is a profoundly important modality for MIR, not only because of the breadth of musical knowledge and history contained within, but also because sheet music constitutes a vital interface between MIR systems and musicians. However, while multimodal MIR systems are rapidly improving at tasks like music transcription [3, 4, 5, 6] and controllable generation [7, 8, 9], these systems typically operate on MIDI as a symbolic music format. This may be less useful to musicians, e.g., a musician might prefer transcription systems to output sheet music instead of MIDI. We conjecture that the scarcity of fine-grained alignment data linking sheet music to corresponding performance audio is a key bottleneck to incorporating sheet music into multimodal MIR systems. Alignments allow multimodal MIR data to be segmented into input-output chunks of tractable length for training models, and the lack of sheet music alignments may partially explain why sheet music is mostly overlooked. Moreover, alignments have practical utility outside of multimodal MIR, e.g., they may be used by musicians to practice along with pre-recorded accompaniments. Unfortunately, collecting alignments is deceptively tricky. For example, one could have a musician use a touch screen to point to the current location in sheet music while listening to a recording in real time. However, their tracking may be imprecise (due to expressive performance timing) and lack non-obvious details that are essential for segmentation (bar line locations, number of active staves). In this work, we investigate the task of alignment of offline in-the-wild performance audio and corresponding sheet music scans (images), with a long-term goal of aligning large corpora of sheet music and performance recordings at scale. Much of the past work on audio-to-score alignment make at least one of several common assumptions that inhibit their practicality for collecting aligned data at scale: (i) the presumed availability of digital scores like MIDI or MusicXML as opposed to sheet music images [10, 11, 12, 13, 14], (ii) the alignment of MIDI performances or synthesized audio instead of real audio recordings [15, 12, 16], (iii) limitations in instrument diversity, commonly piano only [10, 16, 17], or (iv) dependence on time-consuming human annotation [18, 19]. Here we propose an audio-to-score alignment procedure that makes none of these assumptions, potentially offering a path forward for large-scale data collection. Most closely related to our approach is that of Shan et al. [16, 17], who examine offline alignment of in-the-wild piano sheet music images and performance recordings by aligning feature representations derived from the score and audio via MIR methods. In addition to operating on more diverse ensembles, our work has two primary distinctions: (1) we take a different approach to handling jumps in scores, and (2) we modify their feature representations. A key challenge in audio-to-score alignment is handling inter-measure jumps in scores induced by repeat signs. Shan et al. [16, 17] propose extensions to DTW that are capable of automatically handling jumps. Here we propose a pragmatic alternative: a workflow and interface that allows humans to quickly annotate jumps, and a system that incorporates these jump labels. We find that this approach can yield much higher-quality alignments than the automatic one, costing only seconds of annotator time. We additionally extend the bootleg score feature representations used by Shan et al. [17], first proposed by Yang et al. [1]. Creating a bootleg score involves detecting noteheads and staff lines to produce a simple binary representation of a score that is conducive to alignment. We find that the use of measure bounding box detection as a preprocessing step improves the quality of underlying notehead and staff line detection algorithms. Additionally, motivated by findings in [2], we find that using raw onset probabilities predicted by a music transcription model as the audio feature representation produces higher quality alignments than using the MIDI transcriptions—see Figure 1 for a summary. Motivated by our long-term goals of bringing sheet music into multimodal MIR, we also propose a new measure-aware evaluation scheme for comparing alignments. We speculate that measure-level alignment granularity is necessary for tractable training of multimodal MIR systems in the short term and that human perception of alignment quality is tied to measures. Accordingly, we prescribe new measure-aware alignment metrics for this task, such as an accuracy metric which reports the proportion of time where the estimated alignment is within a half measure radius of the ground truth alignment. On a small but diverse dataset of in-the-wild sheet music and aligned audio [18], we observe that our proposed system achieves an accuracy of 120\% relative to that of Shan et al. (33\%\to 72\%). By providing repeat labels, we improve the absolute accuracy of our system from 20\%\to 83\% on a subset of pieces that have repeats. Our work makes the following contributions: • A system capable of high-quality in-the-wild alignment of sheet music images and performance audio. • A pragmatic workflow we call Just Label The Repeats that further improves alignment accuracy. • An interface that enables rapid jump annotation."
https://arxiv.org/html/2411.07335v1,"Multimodal Fusion Balancing Through
Game-Theoretic Regularization","Multimodal learning can complete the picture of information extraction by uncovering key dependencies between data sources. However, current systems fail to fully leverage multiple modalities for optimal performance. This has been attributed to modality competition, where modalities strive for training resources, leaving some underoptimized. We show that current balancing methods struggle to train multimodal models that surpass even simple baselines, such as ensembles. This raises the question: how can we ensure that all modalities in multimodal training are sufficiently trained, and that learning from new modalities consistently improves performance? This paper proposes the Multimodal Competition Regularizer (MCR), a new loss component inspired by mutual information (MI) decomposition designed to prevent the adverse effects of competition in multimodal training. Our key contributions are: 1) Introducing game-theoretic principles in multimodal learning, where each modality acts as a player competing to maximize its influence on the final outcome, enabling automatic balancing of the MI terms. 2) Refining lower and upper bounds for each MI term to enhance the extraction of task-relevant unique and shared information across modalities. 3) Suggesting latent space permutations for conditional MI estimation, significantly improving computational efficiency. MCR outperforms all previously suggested training strategies and is the first to consistently improve multimodal learning beyond the ensemble baseline, clearly demonstrating that combining modalities leads to significant performance gains on both synthetic and large real-world datasets.","Exploiting multimodal data has made significant progress, with advances in generalizable representations and larger datasets enabling solutions to previously unattainable tasks [27; 29; 37; 43; 42; 44; 50; 53; 62]. However, studies indicate that multimodal data is often utilized suboptimally, underperforming compared to ensemble unimodal models or even the best single modality [55; 60]. The expectation that adding a modality should improve performance, assuming independent errors and above-chance predictive power [17], is frequently contradicted in practice. Huang et al. [20] attribute this issue to modality competition, where one modality quickly minimizes training error, misdirecting and suppressing the learning of others. Factors like noise levels, relationship complexity with the target, feature dimensionality, and data quality can cause one modality to fit faster than another. This imply that adding task-relevant information doesn’t guarantee better performance, primarily due to complications during training. To address these issues, it’s crucial to monitor each modality’s contribution during training and apply corrective measures. Several balancing strategies have been proposed to tackle this issue [5; 6; 9; 10; 21; 26; 28; 40; 41; 54; 55; 57; 60]. A central aspect of these methods is estimating each modality’s contribution to the output. Most assume distributional independence between modalities on predicting the target, measuring contribution via unimodal performance [60; 26; 40; 6; 54]. Some methods bypass this assumption by estimating influence based on prediction differences between original and perturbed inputs [28; 21; 10]. Perturbations can take various forms, such as zeroing values [28], adding Gaussian noise [10], or using task-specific augmentations [21; 31]. These methods aim to amplify a modality’s influence by increasing the impact of perturbations on the output. However, this also makes the network more sensitive to these changes (e.g., noise), risks becoming overly reliant on the perturbations, and struggles to scale when multiple perturbations are required. Additionally, increasing the contribution of one modality can be achieved by overshadowing others, leading to an imbalance that undermines overall performance and makes the objective counterproductive. Given these challenges, how can we design an efficient regularization method that addresses multimodal competition, ensuring balanced and effective learning across all modalities? Figure 1: (Left) Illustration of the conditional mutual information (\operatorname{CMI}) terms, \operatorname{CMI}_{1}:I(X_{1};Y\mid X_{2}) and \operatorname{CMI}_{2}:I(X_{2};Y\mid X_{1}), representing the unique contributions (U_{1} and U_{2}) of each modality to the target. The shared task-relevant information (S) between the modalities is defined as I(X_{1};X_{2})-I(X_{1};X_{2}\mid Y). (Right) Accuracy as a function of the ratio between the unique information (U_{1}) from modality X_{1} and the shared information (S) between the modalities. Synthetic data are generated as X_{1}=N_{1}+Y,X_{2}=N_{1}+Y where N_{1},N_{2} are independent noise for each modality. We consider S the percentage of the datapoints that both modalities have information about the label Y, U_{1} and U_{2} when only one has with the other modality equating to noise for those datapoints. In the experiment we keep U_{2} constant while changing U_{1} and S. As U_{1} increases and S decreases, accuracy deteriorates, reflecting intensified multimodal competition. Among the various methods, including Singleloss, Multiloss, Ensemble, unimodally pretrained and finetuned encoders (Uni-Pre Fine), OGM [40], AGM [28], and MLB [26]our regularization method MCR demonstrates a slower decline in accuracy. For further detals prease refer to Section 4.1 In this paper, we introduce the Multimodal Competition Regularizer (\operatorname{MCR}), a loss function designed to promote the exploration of task-relevant information across all available modalities. By decomposing the joint mutual information (\operatorname{MI}), we separately model shared and unique task-relevant information within the modalities. To efficiently capture the unique information from each modality, we employ a computationally inexpensive permutation-based approach. Our method maximizes the lower bounds of each MI term to encourage the network to learn both shared and unique information, while minimizing upper bounds on terms to suppress task-irrelevant information. We frame the problem in a game-theoretic setting, exploring strategies that involve both collaboration and competition among modalities to address the conflicting objectives that arise when increasing all modalities’ contributions simultaneously. This approach allows their contributions to adapt dynamically, achieving balance during training. We extensively evaluate \operatorname{MCR} on synthetic datasets and several established real-world multimodal benchmarks, including action recognition on AVE [49] and UCF [45], emotion recognition on CREMA-D [4], human sentiment on CMU-MOSI [61], human emotions on CMU-MOSEI [63] and egocentric action recognition on Something-Something [15]. Our results demonstrate that \operatorname{MCR} is the first balancing method to significantly improve supervised multimodal training over the ensemble baseline across a variety of datasets and models. Our key contributions are summarized as follows: 1. An analysis of multimodal competition, defining the error increase caused in multimodal training, while demonstrating in our results that most previous methods do no outperform simple baselines, such as unimodal ensembles. 2. A novel multimodal training strategy, \operatorname{MCR}, designed to regularize multimodal competition, which includes: • Defining lower and upper bounds of the \operatorname{MI} terms, encouraging the exploration of information across all modalities. • Introducing a game-theoretic perspective where modalities form the players that compete for training resources, assisting the regularization through balancing the corresponding MI terms. • Suggesting latent-space perturbations as an efficient way to estimate the lower bound of the \operatorname{CMI} reducing the computational cost of multiple forward passes."
https://arxiv.org/html/2411.07155v1,"Low Complexity Learning-based 
Lossless Event-based Compression","Event cameras are a cutting-edge type of visual sensors that capture data by detecting brightness changes at the pixel level asynchronously. These cameras offer numerous benefits over conventional cameras, including high temporal resolution, wide dynamic range, low latency, and lower power consumption. However, the substantial data rates they produce require efficient compression techniques, while also fulfilling other typical application requirements, such as the ability to respond to visual changes in real-time or near real-time. Additionally, many event-based applications demand high accuracy, making lossless coding desirable, as it retains the full detail of the sensor data. Learning-based methods show great potential due to their ability to model the unique characteristics of event data thus allowing to achieve high compression rates. This paper proposes a low-complexity lossless coding solution based on the quadtree representation that outperforms traditional compression algorithms in efficiency and speed, ensuring low computational complexity and minimal delay for real-time applications. Experimental results show that the proposed method delivers better compression ratios, i.e., with fewer bits per event, and lower computational complexity compared to current lossless data compression methods.","An event camera, also known as a dynamic vision sensor (DVS), is a type of sensor that differs from traditional cameras in the way it captures visual information. Instead of capturing entire frames at fixed intervals like a conventional camera, an event camera operates at the pixel level by detecting changes in brightness (events). This results in a highly efficient method of capturing visual data with extremely low latency and high dynamic range, making it ideal for applications requiring fast responses and minimal computational resources. Event cameras are important because they offer significant advantages in tasks such as high-speed motion tracking, low-latency robotics, and dynamic scene analysis, where traditional cameras may struggle, especially in high-speed motion scenes and scenes with uncontrolled illumination conditions. An event is a single occurrence captured by a pixel within the event camera’s sensor, represented as a 4D tuple (t_{s},x,y,p), where t_{s} denotes the precise timestamp of the event’s occurrence, (x,y) specifies the spatial coordinates within the sensor’s pixel array, and p indicates the polarity of the brightness change (increase or decrease) at that location. Figure 1 depicts an event sequence in 3D space (x,y,t_{s}) and illustrates several benefits of event-based data, such as time-space continuity, absence of blur, and rapid acquisition to minor changes in the scene. Figure 1: Visualization of event data in 3D space: (a) Ground truth image captured by the camera (b) A set of events produced by an event camera, events in red (positive polarity) and blue (negative polarity) with the first events in black to show scene structure. Lossless compression of an event sequence is highly desirable to preserve the integrity of the captured data. Event cameras generate a continuous stream of asynchronous events, each representing a precise spatio-temporal change in brightness. Losing any events or reducing its accuracy could lead to missed details or inaccuracies in the processing of visual scenes, potentially compromising the performance of computer vision algorithms and other applications relying on this type of data. Furthermore, many applications that leverage event data, such as autonomous vehicles, robotics, and augmented reality systems, demand real-time processing and analysis of visual information. Consequently, fast and low-latency encoding and decoding of the event data are crucial. Any significant delays in encoding or decoding the event sequence could result in delayed decision-making or outdated information, potentially leading to safety hazards or sub-optimal performance. In the context of autonomous vehicles, for instance, real-time processing of event data is essential for timely detection and tracking of obstacles, pedestrians, and other moving objects on the road. Even a brief delay in processing this information could have severe consequences, as vehicles traveling at high speeds cover significant distances in a short time. Real-time encoding and decoding of event data ensures that the vehicle’s perception and decision-making systems have access to the most up-to-date visual information, enabling prompt and appropriate responses to dynamic road conditions. In this context, this work aims to introduce a novel lossless event data compression method that leverages deep learning techniques (which are also referred as learning-based), while maintaining low computational requirements and supporting low-latency streaming. The focus is on scenarios where real-time execution of an application from a live sensor stream takes place. In this case, event data is instantly encoded by the sensor, streamed to the application processor, and then decoded and processed in real-time by the application. The proposed method organizes the event sequence into a 2D event frame, which stores events that occur at spatial coordinates (x,y) at a given time instance t_{s}. To improve data representation efficiency, a quadtree structure is used for the first time in this work to provide adaptive partitioning of the 2D frame of events. This structure enables a more compact binary representation of event data, which is then processed using simple but yet efficient entropy coding techniques. A key contribution of this work is the development of a deep neural network that learns to model the data distribution, thereby optimizing the performance of the entropy coder. Experimental results show that this method overcomes traditional lossless data compression techniques, achieving improvements in both computational complexity (especially at the encoder) and coding efficiency. The rest of the paper is structured as follows: Section II briefly reviews background work on lossless event data compression, while the proposed compression method is described in Section III. Performance evaluation is presented and analysed in Section IV, and finally, Section V concludes the paper."
https://arxiv.org/html/2411.05794v1,Beyond Correlation: Evaluating Multimedia Quality Models with the Constrained Concordance Index,"This study investigates the evaluation of multimedia quality models, focusing on the inherent uncertainties in subjective Mean Opinion Score (MOS) ratings due to factors like rater inconsistency and bias. Traditional statistical measures such as Pearson’s Correlation Coefficient (PCC), Spearman’s Rank Correlation Coefficient (SRCC), and Kendall’s Tau (KTAU) often fail to account for these uncertainties, leading to inaccuracies in model performance assessment. We introduce the Constrained Concordance Index (CCI), a novel metric designed to overcome the limitations of existing metrics by considering the statistical significance of MOS differences and excluding comparisons where MOS confidence intervals overlap. Through comprehensive experiments across various domains including speech and image quality assessment, we demonstrate that CCI provides a more robust and accurate evaluation of instrumental quality models, especially in scenarios of low sample sizes, rater group variability, and restriction of range. Our findings suggest that incorporating rater subjectivity and focusing on statistically significant pairs can significantly enhance the evaluation framework for multimedia quality prediction models. This work not only sheds light on the overlooked aspects of subjective rating uncertainties but also proposes a methodological advancement for more reliable and accurate quality model evaluation.","The evaluation of multimedia quality models [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], is essential for assessing the efficacy of signal processing algorithms across various domains such as inpainting, enhancement, codecs, voice call standards, and media streaming. The performance of objective quality models is typically evaluated by comparing the predicted Mean Opinion Score (MOS) with human-rated MOS, employing statistical measures for accuracy. The MOS can be derived by averaging individual ratings on a 5-point Absolute Category Rating (ACR) scale, with the possibility of extending this scale to capture more nuanced user perceptions (e.g., a 10-point ACR scale). Following the ITU-T P.1401 guidelines [11], the evaluation process involves statistical tools like the root mean squared error* (RMSE*), Pearson’s correlation coefficient (PCC), and the outlier ratio (OR) to analyze the average error, linearity, and the prevalence of outliers, respectively. In addition, Spearman’s rank correlation coefficient (SRCC) and Kendall’s tau correlation coefficient (KTAU) offer insights into monotonic relationships and the balance between concordant and discordant pairs. These methodologies are widely used to evaluate regression tasks, but they neglect that subjective MOS ratings are highly influenced by several factors such as rating inconsistency, bias from MOS distribution [12], order-effect, and rater-specific biases like culture and language. Ignoring this uncertainty can lead to inaccuracies in the performance evaluation of quality models. The exploration into evaluating instrumental quality models with a focus on rater uncertainty is limited, particularly neglecting factors such as the restriction of range, sample size, and variability among rater groups, which are critical for a comprehensive evaluation of multimedia quality prediction. These factors can be summarised as follows: • Small sample size impacts the robustness of statistical measures like PCC, SRCC, and KTAU, particularly when the evaluations are based on averaging across a limited set of conditions111We use the term condition to indicate a particular degradation at a specific intensity e.g., gaussian noise added to images at 20 dB signal-to-noise ratio (SNR). This can introduce a sampling bias, as these measures may not accurately reflect the population’s characteristics with a small dataset, leading to potential inaccuracies in the evaluation of objective quality models [13, 14]. • Restriction of range refers to the issue arising when statistical metrics like PCC or SRCC are calculated over a subset of the full data range, leading to potential deviations in their values. This problem is accentuated in uncontrolled environments, such as user-generated content or in-the-wild assessments, where the quality range is uncontrolled and might not accurately represent the entire spectrum of quality [15, 16, 17]. • Variability among rater groups emerges, especially in crowdsourced settings, because MOS is a relative scale as shown in several studies [18, 19, 12, 20, 21]. In crowdsourcing [22, 23, 24, 25, 26, 27, 28, 29], different stimuli are assigned to different groups due to the large size of the dataset that needs to be annotated. As a consequence, every group will necessarily introduce a group bias effect. Krasula et al.’s work [15] marks a significant step forward in proposing a performance evaluation metric that identifies significantly different pairs and quantifies uncertainty in objective metric predictions, thus enhancing the evaluation framework to account for the statistical significance of subjective scores and the integration of data from varied subjective experiments. Another study that questions the reliability of PCC has been proposed for speech quality models [30]. Here, the authors propose a Bayesian model selection to evaluate instrumental quality models, showing that PCC does not have high explanatory power compared to their approach. Both studies confirm the inadequateness of statistical metrics but do not address the three issues mentioned above: small sample sizes, restriction of range, and variability among rater groups. This paper seeks to address this gap by evaluating the performance of instrumental quality models under these three scenarios. We conduct a thorough examination of statistical metrics for instrumental quality models to evaluate their robustness in these scenarios. In addition, we introduce the Constrained Concordance Index (CCI), a novel metric designed to assess the performance of instrumental quality models. CCI measures the capability of these models to accurately rank pairs where MOS has high precision and ignores the ones with uncertain MOS. Our findings demonstrate that typical statistical metrics (PCC, SRCC, and KTAU) lack robustness in the three scenarios mentioned above, whereas the proposed CCI effectively addresses this issue. We proposed the CCI metric in our previous study to evaluate objective quality models for sound archives [31]222We introduced the CCI metric in [31] with another name: pairwise ranking accuracy (PRA). In our previous paper [31] we have only used the CCI and motivated its usage but we did not compare it against traditional metrics and show its robustness for multimedia quality model evaluation. In this paper, we slightly modify the CCI metric (see Section 3) and we do an extensive comparison of the CCI metric against traditional statistical metrics (PCC, SRCC, and KTAU) to evaluate the robustness against the three scenarios mentioned above. One of the contributions of this paper is also the evaluation of traditional statistical metrics in these three scenarios. To achieve this we select raters and samples from laboratory-based speech and image quality datasets through bootstrapping. By extracting a subset of stimuli and raters from these lab-based quality databases, we examine the extent to which statistical metrics diverge from their values obtained when using the full set of raters and stimuli. In Section 2 we outline the motivations for proposing a new metric that overcomes the issues of traditional statistical metrics for the three studied scenarios: small sample size, restriction of range, and rater group variability. Section 3 is dedicated to the description of the proposed metric CCI. Experiment setup and results comparing all the statistical metrics are shown in Section 4 while a description of how to interpret and visualise the proposed CCI metric is described in Section 5. Finally, we provide a paper discussion in Section 6 and our conclusions in Section 7. The Python code to reproduce the experiments and the CCI metric are available on GitHub333https://github.com/alessandroragano/muqeval."
https://arxiv.org/html/2411.06976v1,A Hierarchical Compression Technique for 3D Gaussian Splatting Compression,"3D Gaussian Splatting (GS) demonstrates excellent rendering quality and generation speed in novel view synthesis. However, substantial data size poses challenges for storage and transmission, making 3D GS compression an essential technology. Current 3D GS compression research primarily focuses on developing more compact scene representations, such as converting explicit 3D GS data into implicit forms. In contrast, compression of the GS data itself has hardly been explored. To address this gap, we propose a Hierarchical GS Compression (HGSC) technique. Initially, we prune unimportant Gaussians based on importance scores derived from both global and local significance, effectively reducing redundancy while maintaining visual quality. An Octree structure is used to compress 3D positions. Based on the 3D GS Octree, we implement a hierarchical attribute compression strategy by employing a KD-tree to partition the 3D GS into multiple blocks. We apply farthest point sampling to select anchor primitives within each block and others as non-anchor primitives with varying Levels of Details (LoDs). Anchor primitives serve as reference points for predicting non-anchor primitives across different LoDs to reduce spatial redundancy. For anchor primitives, we use the region adaptive hierarchical transform to achieve near-lossless compression of various attributes. For non-anchor primitives, each is predicted based on the k-nearest anchor primitives. To further minimize prediction errors, the reconstructed LoD and anchor primitives are combined to form new anchor primitives to predict the next LoD. Our method notably achieves superior compression quality and a significant data size reduction of over 4.5\times compared to the state-of-the-art compression method on small scenes datasets.","3D Gaussian Splatting (GS) [3DGS] has demonstrated substantial advances in the field of novel view synthesis due to its impressive visual quality with ultra fast training speed. Different from Neural Radiance Field (NeRF) [nerf] with implicit representations, 3D GS uses serial explicit scattered isotropic ellipsoids to reconstruct the 3D scene. Each Gaussian consists of a 3D point center and several attributes, including scale vector, rotation quaternion, Spherical Harmonic (SH) coefficients, and opacity. Leveraging a highly optimized CUDA-based rendering implementation, 3D GS enables rapid training and rendering, making it highly suitable for practical applications. Additionally, its explicit data format is not only easy to understand and analyze, but also facilitates downstream processing (e.g., MGA [streaming]), making it a promising candidate for industrial application and standardization efforts. However, explicit point-based representations inherently result in significant storage overhead, as each point and its associated attributes must be stored independently. For example, reconstructing a large scene typically requires several million Gaussians, which can consume more than one gigabyte of memory. Therefore, compression of 3D GS becomes an essential technology to mitigate storage and transmission overhead. Currently, research on 3D GS compression can be categorized into two distinct branches: generative compression and traditional compression [GGSC]. The majority of studies focus on generative compression, employing techniques such as pruning, codebooks, and entropy constraints to produce more compact data representations during 3D GS generation. For example, LightGaussian [lightgaussian] prunes insignificant Gaussians based solely on the opacity parameter, and HAC [hac] introduces a hash-grid assisted context model to reduce spatial redundancy. However, although traditional compression has hardly been studied now, it remains an equally important area of research. GGSC [GGSC] is the first work to address this gap by proposing a simple but effective graph-based compression anchor. However, the performance of GGSC is limited as it does not fully exploit the spatial redundancy of 3D GS. In this paper, we introduce a Hierarchical GS Compression (HGSC) technique. We first prune Gaussians based on primitive importance scores assessed by both global significance and local significance: global significance is determined by each Gaussian’s contribution to rendering color across different views, while local significance is associated with the volume of each Gaussian, both of which are crucial for maintaining final rendering quality. Then, an Octree [octree] structure is employed to compress 3D positions. To reduce the influence of point merging within a voxel in Octree, the reconstructed points are recolored by applying the attributes of the nearest Gaussian from original 3D GS to ensure consistency. Based on the 3D GS Octree, we implement a hierarchical compression strategy. Specifically, we use a KD-tree [KD-tree] to split the 3D GS into multiple blocks and apply Farthest Point Sampling (FPS) [FPS] to select anchor primitives in each block and then generate varying Levels of Details (LoDs) primitives. These anchor primitives serve as references for predicting non-anchor primitives across different LoDs to reduce spatial redundancy. For anchor primitives, we employ the region adaptive hierarchical transform (RAHT) [RAHT] to achieve near-lossless compression of various attributes to enhance prediction accuracy. For non-anchor primitives, each is predicted by the k-nearest anchor primitives. Subsequently, the discrepancies between the predicted and actual attributes are quantized and subsequently encoded using the LZ77 [LZ77] codec. To minimize prediction errors, the current reconstructed LoD and anchor primitives are combined to form the new anchor primitives for predicting the next LoD. Overall, our method achieves better compression efficiency and reduced processing time compared to the benchmark GGSC."
https://arxiv.org/html/2411.06810v1,JPEG AI Image Compression Visual Artifacts: Detection Methods and Dataset,"In recent years learning-based image compression methods have significantly improved and started to outperform conventional codecs. However, neural network approaches can unexpectedly introduce visual artifacts in some images. In this work, we propose methods to separately detect three types of artifacts (texture and boundary degradation, color change, and text corruption), to localize the affected regions, and to quantify the artifact strength. We consider only those regions which have been distorted just by the neural compression, while being recovered successfully by a conventional codec at a comparable bitrate. The proposed methods have been employed to collect artifacts for the JPEG AI verification model with respect to HM-18.0, the H.265 reference software. We processed about 350,000 unique images from the Open Images dataset with different compression quality parameters and created a dataset of 46,440 artifacts with validation using crowd-sourced subjective assessment. The proposed dataset and methods are valuable for testing neural network-based image codecs, identifying bugs in the codecs, and enhancing their performance. We make source code of the methods and the dataset publicly available.","Since 2018, researchers have actively pursued neural-based image compression and have demonstrated the superiority of neural codecs over classical models. In 2023 alone, more than 500 papers on neural compress methods have been published [], [], []. The capabilities of neural codecs have attracted considerable attention from both the scientific and industrial communities. As a result, the development of the JPEG AI standard for neural-based image compression has begun []. A neural codec based on the JPEG AI standard is already in development and has demonstrated 40% better performance compared to the advanced VVC intra codec. The standard is set to be published in October 2024, which will bring even more attention to the field. Neural codecs use deep learning models to compress images, that is, they learn nonlinear transformations, providing a more compact bit representation and better coding performance than traditional methods that use a predefined set of algorithms to compress data. As a result, neural codecs can produce higher quality images, but they can also introduce artifacts that are not present in traditional codecs. Therefore, the challenge arises to study the shortcomings of neural compression methods, develop metrics to detect neural compression artifacts, and create a dataset of such images. The assembled dataset can be used to test neural methods, identify their shortcomings and further improve them. Existing image quality assessment methods such as PSNR, SSIM and others are not suitable for this task for several reasons: 1. These algorithms respond poorly to small area neural compression artifacts that are nevertheless noticeable to humans, i.e., they correlate poorly with human perception of the image. 2. They are mainly aimed at obtaining a single number - an estimate of image quality, which does not allow to achieve artifact localization. 3. They are used as target metrics in most works on new compression methods, so it is incorrect to evaluate methods by them. 4. They assess image quality in general, i.e. they do not give any information about the type of artefact. The objective of this project was to develop metrics sensitive to neural artifacts, such as distortion of text, color, textures, and borders in compressed images. The metrics are designed to detect even minor distortions that significantly degrade the perception of the image. An artifact, in this context, is a distortion relative to the original image, present in an image compressed by a neural codec and absent or less noticeable in an image compressed by a classical codec. These metrics identify images that are less resilient to neural compression techniques compared to traditional algorithms. Using these metrics, a dataset was compiled, containing 53260 images with various types of artifacts and compression ratios. Additionally, a subjective verification of the automatically detected artifacts was performed to ensure accuracy. Our main contributions are: 1. Development of detection methods for three types of neural network compression artifacts. 2. Conducted subjective comparisons to validate identified artifacts. 3. Compilation of a dataset containing examples of neural compression artifacts. 4. Demonstrated higher correlation of our methods compared to existing image quality assessment methods with subjective evaluations."
https://arxiv.org/html/2411.06742v1,Loss-tolerant neural video codec aware congestion control for real time video communication,"Because of reinforcement learning’s (RL) ability to automatically create more adaptive controlling logics beyond the hand-crafted heuristics, numerous effort has been made to apply RL to congestion control (CC) design for real time video communication (RTC) applications and has successfully shown promising benefits over the rule-based RTC CCs. Online reinforcement learning is often adopted to train the RL models so the models can directly adapt to real network environments. However, its trail-and-error manner can also cause catastrophic degradation of the quality of experience (QoE) of RTC application at run time. Thus, safeguard strategies such as falling back to hand-crafted heuristics can be used to run along with RL models to guarantee the actions explored in the training sensible, despite that these safeguard strategies interrupt the learning process and make it more challenging to discover optimal RL policies.The recent emergence of loss-tolerant neural video codecs (NVC) naturally provides a layer of protection for the online learning of RL-based congestion control because of its resilience to packet losses, but such packet loss resilience have not been fully exploited in prior works yet. In this paper, we present a reinforcement learning (RL) based congestion control which can be aware of and takes advantage of packet loss tolerance characteristic of NVCs via reward in online RL learning. Through extensive evaluation on various videos and network traces in a simulated environment, we demonstrate that our NVC-aware CC running with the loss-tolerant NVC reduces the training time by 41% compared to other prior RL-based CCs. It also boosts the mean video quality by 0.3 to 1.6dB%, lower the tail frame delay by 3 to 200ms, and reduces the video stalls by 20% to 77% in comparison with other baseline RTC CCs.","Real-time video communication including video conferencing (MacMillan et al., 2021), live video/VR broadcasting (Web, 2021a, 2020b; Hopkins, 2017), IoT applications (Web, 2021b; Mob, 2024), and cloud gaming (Web, 2021c, 2020a) has been a key component of our daily lives (Blum et al., 2021) and carries a dominant amount of traffic in today’s internet (Cisco, [n. d.]). These RTC applications require high network bandwidth and low network latency to deliver seamless and high quality experience to users, pushing the telecommunication infrastructure upgrade to meet the demands and forcing the congestion control algorithms to promptly adapt to the constantly changing network conditions. Unlike traditional congestion controls (Ha et al., 2008; Brakmo et al., 1994; Jacobson, 1988) which are designed for reliability and in-order delivery through retransmissions instead of realtimeliness, plenty of hand-crafted congestion controls for real-time video communication (Carlucci et al., 2016; Fouladi et al., 2018; Zhu et al., 2020; Johansson and Sarker, 2017; Nagy et al., 2014; Ray et al., 2022) have been proposed to boost bandwidth utilization, suppress packet delays, and avoid packet losses. However, these pre-programmed rule-based congestion control algorithms are not panacea in all network settings as they fall short of adapting to the highly heterogeneous network conditions. To save the human effort optimizing a rule-based congestion control algorithm for numerous network conditions, researchers have made huge effort to explore the data-driven approaches to design congestion control and rate adaptation (Mao et al., 2017; Jay et al., 2019; Zhang et al., 2020, 2021; Zhou et al., 2019; Xia et al., 2022; Gilad et al., 2019) and have shown great potential over the handcrafted heuristics. The other approach designed to bridge the gap between training network environments and the real networks is using The “learning online, running online” strategy is often adopted to train a RL-based solution in order to bridge the gap between training network environments and the real network environments at the deployment stage. RL models directly interact with the real network environments to collect experience and then update themselves during runtime. However, the trial-and-error behavior of online RL training will unavoidably take risky actions which might disturb the system performance. A safeguard policy, typically a handcrafted heuristics, is often used to substitute the RL model once an erroneous action is detected or the system is in a risky state (Zhang et al., 2020; Mao et al., 2019). After the safeguard policy recovers the system to a safe state, the RL-based model takes the control back. The main design philosophy behind safeguarding a RL-based CC as well as behind traditional CCs in RTC applications is based on an implicit assumption on video codec that delayed or lost packets can lead to incomplete frames received which then block video decoding at the receiver side and hurt users’ QoE. The recent loss-tolerant neural video codecs (Dasari et al., 2022; Hu et al., 2021; Lu et al., 2019; Cheng et al., [n. d.]; Chen et al., [n. d.]; Sivaraman et al., 2022) breaks the implicit assumption on video codecs as these NVCs can decode incomplete frames and still deliver decent frame quality. They have shown strong loss tolerance ability across a wide range of packet loss rates on top of its high compression efficiency and good generalization over various video content. Figure 1 shows a state-of-art neural video codec, GRACE, has a smoother and slower video quality drop with increasing packet loss rate than commonly used encoder-side forward error correction (FEC) and decoder-side error concealment (EC). Figure 1. Loss-tolerant neural video codec has slow and smooth video quality drop with increasing packet loss rate. In this paper, we present NVC-CC, a RL-based RTC congestion control algorithm which can be trained online without the help of safeguard policies by taking advantage of the NVCs’ packet loss tolerance properties. Our key insight is that the safeguard policies run along the RL model hinders the RL online learning efficiency. Comprehensive experiments (§5) on a diverse set of videos and network traces show that our NVC-aware CC running with the loss-tolerant NVC reduces the training time by 41% compared to other prior RL-based CCs. It also boosts the mean video quality by 0.3 to 1.6dB%, lower the tail frame delay by 3 to 200ms, and reduces the video stalls by 20% to 77% in comparison with other baseline RTC CCs. Contributions: Our work makes the following contributions. 1) We reveal the inefficiency of training in RL-based RTC congestion control solutions trained by online learning with safeguard policies and introduce the trade-off between learning efficiency and QoE in RL training (§3.2). 2) We analyze how the loss-tolerant NVCs can help improve training efficiency by allowing RL-based CCs to learn without the restriction of safeguard policies and not not hurting QoE (§3.3). 3) We propose NVC-CC, which, to the best of our knowledge, the first RL-based congestion control aware of and taking advantage of the loss-resilient properties of neural video codecs (§4) and validate its remarkable performance gain over the state-of-the-art solutions (§5)."
https://arxiv.org/html/2411.05832v1,"Diversify, Contextualize, and Adapt:
Efficient Entropy Modeling for Neural Image Codec","Designing a fast and effective entropy model is challenging but essential for practical application of neural codecs. Beyond spatial autoregressive entropy models, more efficient backward adaptation-based entropy models have been recently developed. They not only reduce decoding time by using smaller number of modeling steps but also maintain or even improve rate–distortion performance by leveraging more diverse contexts for backward adaptation. Despite their significant progress, we argue that their performance has been limited by the simple adoption of the design convention for forward adaptation: using only a single type of hyper latent representation, which does not provide sufficient contextual information, especially in the first modeling step. In this paper, we propose a simple yet effective entropy modeling framework that leverages sufficient contexts for forward adaptation without compromising on bit-rate. Specifically, we introduce a strategy of diversifying hyper latent representations for forward adaptation, i.e., using two additional types of contexts along with the existing single type of context. In addition, we present a method to effectively use the diverse contexts for contextualizing the current elements to be encoded/decoded. By addressing the limitation of the previous approach, our proposed framework leads to significant performance improvements. Experimental results on popular datasets show that our proposed framework consistently improves rate–distortion performance across various bit-rate regions, e.g., 3.73% BD-rate gain over the state-of-the-art baseline on the Kodak dataset.","Most neural image codecs [18, 17, 8, 9, 11, 15] first transform an image into a quantized latent representation. It is then encoded into a bitstream via an entropy coding algorithm, which relies on a learned probability model known as the entropy model. According to the Shannon’s source coding theorem, the minimum expected length of a bitstream is equal to the entropy of the source. Thus, accurately modeling entropy of the quantized latent representation is crucial. Entropy models estimate a joint probability distribution over the elements of the quantized latent representation. Generally, it is assumed that all elements follow conditionally independent probability distributions. To satisfy this, the probability distributions are modeled in context-adaptive manners, which is key to accurate entropy modeling [18]. Recent methods are based on the joint backward and forward adaptation where the probability distributions adapt by leveraging contexts in two different ways: directly using previously encoded/decoded elements (i.e., backward adaptation), and extracting and utilizing an additional hyper latent representation (i.e., forward adaptation). Here, the type of contexts leveraged can be diverse depending on the spatial range they cover. First, each element has dependencies with other elements in the same spatial location along the channel dimension. Since the channel-wise dependencies correspond to the local image area (e.g., a 16\times 16 patch), we denote them as the “local” context. Second, dependencies exist among spatially adjacent elements, and we refer to them as the “regional” context. Lastly, long-range spatial dependencies span the entire image area, referred to as the “global” context. For the backward adaptation, the modeling order, i.e., which elements are modeled first, is an important factor, and the key lies in how effectively we can utilize diverse contexts in the modeling process. Early studies employ spatial autoregressive (AR) models that access regional context including the most spatially adjacent elements. However, they suffer from significantly slow decoding times due to the inevitably large number of modeling steps, which is equal to the spatial dimensions [18]. To enhance efficiency in entropy modeling, several attempts reduce the number of modeling steps while leveraging diverse contexts: a 10-step channel-wise AR model [17], a 2-step spatial non-AR model with a checkerboard pattern [8], and a 4-step non-AR model that operates across spatial and channel dimensions using a quadtree partition [14]. Figure 1: DCA diversifies the hyper latent representations and contextualizes the current elements by leveraging the diverse hyper latent representations along with the previous elements. As a result, the probability distributions adapt effectively, leading to accurate entropy modeling. Entropy models based on the efficient backward adaptation methods have led to significant improvements. However, they are still limited in fully leveraging contexts for forward adaptation. Since they use multiple neural layers with downsampling and upsampling for modeling hyper latent representation, they can only access the regional context. This limits the performance improvement due to the insufficient contexts (Figure 1a). In particular, this limitation is exacerbated at the first step where only forward adaptation is utilized due to the absence of previous elements (Figure 6). Therefore, it is necessary to develop effective forward adaptation in synergy with the efficient backward adaptation. In this paper, we propose a simple yet effective entropy modeling framework, called DCA (Diversify, Contextualize, and Adapt), leveraging sufficient contexts for forward adaptation without compromising on bit-rate (Figure 1b). Building on the quadtree partition-based backward adaptation [14], we introduce a strategy of diversification, i.e., extracting local, regional, and global hyper latent representations unlike only a single regional one in the previous approach. Note that simply using more contexts for forward adaptation does not guarantee performance improvements because forward adaptation requires additional bit allocation unlike backward adaptation. Then, we propose how to effectively utilize the diverse contexts along with the previously modeled elements for contextualizing the current elements to be encoded/decoded. To consider step-wise different situations, e.g., increased number of previous elements over steps, our contextualization method is designed to utilize each hyper latent representation separately in a step-adaptive manner. Additionally, our contextualization method proceeds in the sequence of regional, global, and local hyper latent representations. Similarly to backward adaptation, we empirically observe that modeling order also matters in forward adaptation. Our main contributions are summarized as follows: • We propose a strategy of diversifying contexts for forward adaptation by extracting three different hyper latent representations, i.e., local, regional, and global ones. This strategy can provide sufficient contexts for forward adaptation without compromising on bit-rate. • We introduce how to effectively leverage the diverse contexts, i.e., previously modeled elements and the three hyper latent representations. We empirically show that the modeling order of three types of contexts affects the performance. • Through the diversification and contextualization methods, our DCA effectively adapts, resulting in significant performance improvements. For example, DCA achieves 3.73% BD-rate gain over the state-of-the-art method [14] on the Kodak dataset."
