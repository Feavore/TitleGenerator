URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10228v1,Path Assignment in Mesh Networks at the Edge of Wireless Networks,"We consider a mesh network at the edge of a wireless network that connects users with the core network via multiple base stations. For this scenario we present a novel tree-search based algorithm that determines the optimal communication path to the core network for each user by maximizing the signal-to-noise-plus-interference ratio (SNIR) for each chosen path. We show that for three mesh networks with differing sizes, our algorithm chooses paths whose minimum SNIR is 3 dB to 18 dB better than that obtained via an algorithm that disregards the effect of interference within the network, 16 dB to 20 dB better than a random algorithm that chooses the paths randomly, and 0.5 dB to 7 dB better compared to a recently introduced genetic algorithm (GA). Furthermore, we show that our algorithm has a lower complexity compared to the GA in networks where its performance is within 2 dB.","Higher throughput and latency demands drive the need for an increased signal dimensions, which either are provided by using a larger number of antennas to exploit the spatial dimension or by increasing the signal bandwidth [1]. Large signal bandwidth is typically made available by moving to higher carrier frequencies. High carrier frequencies in this context means upper mm-wave band and beyond. The challenge at higher carrier frequencies is an increased signal path-loss and that it is becoming more difficult to generate the output power required for long distances wireless links [2]. If higher frequencies are going to be used, both ends of a communication link have to be moved closer to each other. For applications where coverage is important this ultimately leads to network densification. Densification of wireless networks therefore drive a need for efficient roll-out and backhaul to keep the deployment and cost of ownership low. Different solutions to address this problem of last-mile backhaul have been investigated and introduced into standards such as self-backhaul [3]. Self-backhaul has the advantage that hardware can be re-used between access and backhaul, but has the disadvantage that backhaul and access compete for the the same frequency resources. In this paper we investigate a network topology, where backhaul and access operate on different carrier frequencies. The underlying assumption is that the access-link operates at a lower carrier frequency while the backhaul operates at a higher carrier frequency where larger bandwidth is available. For the backhaul network we consider a mesh network topology, which is particularly attractive from a network roll-out and deployment perspective. The network can easily be expanded by adding new micro base stations (BSs) without a need to add more fiber or macro BS sites for backhaul. In the investigated concept each BS can serve as an access-node for users, as well as a forwarding node in the backhaul network. This system solution simplifies roll-out and densification, but increases complexity in the network layer where routing path through the mesh have to be chosen. Routing in wired mesh networks have been well studied. However, in contrast to wired mesh networks, optimization of wireless networks is more challenging since the cost of different links in the mesh is inter-dependent due to interference between different backhaul links. Routing choices impact interference levels and vice versa. This paper aims to solve this problem by introducing a tree based algorithm that finds the optimal route for each user in a mesh network by considering aforementioned interference present within the network. Furthermore, the algorithm can trade off between performance and scalability, depending on the requirement of the network operator. Significant research efforts to optimize the network have been undertaken in [4, 5, 6, 7, 8, 9]. While [4] and [5] employ algorithms rooted in numerical optimization principles, [6, 7] present innovative approaches hinging on machine learning techniques. In [8], the authors study the problem of topology optimization and routing for integrated access and backhaul networks. They optimize the network using a genetic algorithm. In [9] topology optimization is made through considering latency gain and maximum number of hops in a mmWave, full-duplex backhaul network. In contrast to the above works, this paper introduces a novel tree search based algorithm. Notably, our algorithm stands out by taking network interference into careful consideration as compared to [4, 6]. Unlike [5], where interference mitigation is largely associated with building reflections in urban scenarios, our algorithm considers the impact of interference when BSs communicate amongst each other. Moreover, our algorithm is versatile, accommodating networks with any number of BSs that are connected to the core network, while [6], [7] and [9] exclusively focus on scenarios involving a single core BS. Furthermore, different to [7], we do not assume that the BSs communicate at fixed times."
https://arxiv.org/html/2411.09812v1,Edge Caching Optimization with PPO and Transfer Learning for Dynamic Environments,"This paper addresses the challenge of edge caching in dynamic environments, where rising traffic loads strain backhaul links and core networks. We propose a Proximal Policy Optimization (PPO)-based caching strategy that fully incorporates key file attributes such as size, lifetime, importance, and popularity, while also considering random file request arrivals, reflecting more realistic edge caching scenarios. In dynamic environments, changes such as shifts in content popularity and variations in request rates frequently occur, making previously learned policies less effective as they were optimized for earlier conditions. Without adaptation, caching efficiency and response times can degrade. While learning a new policy from scratch in a new environment is an option, it is highly inefficient and computationally expensive. Thus, adapting an existing policy to these changes is critical. To address this, we develop a mechanism that detects changes in content popularity and request rates, ensuring timely adjustments to the caching strategy. We also propose a transfer learning-based PPO algorithm that accelerates convergence in new environments by leveraging prior knowledge. Simulation results demonstrate the significant effectiveness of our approach, outperforming a recent Deep Reinforcement Learning (DRL)-based method.","With the rapid growth of mobile applications and services, the demand for data has surged, placing significant pressure on backhaul links and core networks. The exponential rise in video streaming, cloud services, and Internet of Things (IoT) devices has led to an overwhelming number of content requests, often from centralized data centers. To address this, edge caching frequently requested data closer to users, has become a key strategy for reducing network congestion and minimizing transmission delays. By caching popular content locally, edge caching not only alleviates the load on core networks but also improves the overall user experience by reducing latency. Edge caching faces significant challenges in dynamic and unpredictable environments, where content popularity is not static and fluctuates based on factors like time, location, and social trends [1, 2, 3, 4]. A viral video, for instance, may experience a sudden spike in requests, followed by a rapid decline in demand. Similarly, request rates can change unpredictably, influenced by factors such as user mobility, device connectivity, and network congestion. For instance, during peak hours in a densely populated area, a surge in user activity may dramatically increase the request rate for certain content. On the other hand, during off-peak times, the request rate may drop significantly. Events like sports games or breaking news can also cause rapid shifts in content requests. When it comes to dynamic environments, most existing research primarily focuses on changes in content popularity trends while overlooking the equally important fluctuations in request rates. In real-world scenarios, Users’ behavior and usage patterns can shift over time, which directly influences the rate at which files are requested. For instance, during peak hours, when many users are accessing the network at once, demand for content rises sharply. This increased activity results in shorter intervals between file requests, as users require quick access to different resources. In contrast, during off-peak periods, when fewer users are online, the intervals between file requests tend to lengthen. With reduced demand for immediate access, users are less likely to request content frequently, leading to extended gaps between consecutive requests. Existing caching strategies, particularly those relying on fixed policies, often fail to account for these dynamic shifts, leading to inefficient use of cache resources. Therefore, the ability to learn new policies quickly and efficiently becomes crucial for maintaining high cache performance in such environments. Most of the existing work on reinforcement learning in dynamic environments assumes that changes occur gradually, giving the agent sufficient time to adapt its policy incrementally. This assumption, however, is not always valid, especially in real-world scenarios where sudden shifts in content popularity or request rates are common. In such cases, relying on gradual adaptation may lead to poor performance. Therefore, detecting when these abrupt changes happen becomes critical to maintaining high system efficiency. By quickly identifying the moment of change, essential steps can be taken to adjust the Reinforcement Learning (RL) policy and prevent the agent from making decisions based on outdated information. Detecting these shifts promptly allows the agent to recalibrate and ensure that caching decisions remain relevant. While training caching policies from scratch after every environmental change is one option, it is not practical. Learning a new policy from the ground up each time a shift occurs would result in significant delays, during which the system operates suboptimally. This can lead to increased latency and reduced cache efficiency. For this reason, it is important to converge to the optimal policy faster in the new environment, as this ensures the system can restore its high level of performance without unnecessary delays or inefficiencies. Building on this, we introduce a caching strategy based on PPO [5], a reinforcement learning (RL) technique known for its stability and efficiency. In our approach, we incorporate not only content popularity but also key file attributes such as size, lifetime, and importance to ensure that the cache is used optimally. In this work, we propose a mechanism for detecting changes in content popularity and request rates, designed to identify shifts in the environment quickly. This mechanism has two functions. The first function focuses on popularity change detection by leveraging cosine similarity between recent and past request patterns. This method effectively captures shifts in content popularity by comparing how closely current requests align with historical trends. The second function addresses request rate changes using a simple moving average over a defined window of time. By continuously monitoring and averaging request rates, it quickly detects deviations from the norm, signaling a change in the environment. Both functions are designed to detect changes rapidly, allowing the system to adapt swiftly and maintain optimal performance in dynamic scenarios. To quickly adapt to changing environments, we integrate transfer learning into the PPO algorithm by transferring the learned knowledge in the form of transitions to the new environment. In the new environment, we assign priorities to both the transferred state transitions and the new transitions collected from the current environment. These priorities are determined using a combination of the Temporal difference (TD) error associated with each transition and the difference between the average reward and the instant reward tied to that transition. By assigning higher priority to more relevant transitions, the caching agent is able to focus on learning from the most significant experiences, thus accelerating convergence. This approach is particularly advantageous in dynamic edge caching environments, where frequent changes demand swift adaptation, and leveraging prior knowledge minimizes the time and computational resources required for the agent to adjust to new conditions. In summary, our approach tackles the limitations of traditional caching strategies by dynamic change detection, and transfer learning. This results in a caching system that is flexible, efficient, and capable of adapting quickly to changing environments, ensuring optimal performance even in highly dynamic scenarios. The contributions of this paper are summarized as follows. • We introduce a mechanism with two functions specifically designed to rapidly detect changes in the environment. These functions are engineered to enhance the responsiveness of the system to dynamic shifts in content popularity and request rate, ensuring timely and accurate identification of environmental changes. • We propose a novel algorithm that combines transfer learning with PPO. This innovative approach facilitates accelerated adaptation to new environments by effectively leveraging previously acquired knowledge, thereby enhancing the algorithm’s ability to quickly adjust and optimize caching strategies in dynamic settings. • We conducted simulations where the environment underwent sudden changes, and our change detection approaches demonstrated remarkable speed in identifying these change points. Furthermore, our proposed transfer learning algorithm outperforms several benchmark methods, including learning from scratch, learning from demonstrations [6], and another recently published work [7]. This superiority highlights the effectiveness of our approach in rapidly adapting to new conditions and achieving better performance in dynamic environments. The remainder of the paper is structured as follows: Section II provides an overview of related work. Section III introduces our system model, while Section IV outlines our proposed caching algorithm. In Section V, we discuss our experimental setup and results, and Section VI concludes the paper. Table I summarises the most important notations used in this paper. TABLE I: Table of notations Symbol Description f_{r} the requested file F The total number of file types f_{f} file type f \lambda The request rate \eta Skewness of popularity distribution N length of the sliding window to store the history of the file requests \gamma The discount factor |\Lambda| The size of the replay buffer before change in the environment |\Lambda^{\prime}| The size of the replay buffer after the new environment \beta Controls the importance sampling d_{f} The number of requests for file type f over the last N requests l_{f} Lifetime of file type f i_{f} Importance of file type f z_{f} Size of file type f h^{f}(t) Freshness of file type f y_{f}(t) Utility of file type f b The files cached in the edge router’s cache \pi Policy \theta Weights of the actor network \theta^{\prime} Weights of the critic network s Current state s^{\prime} Next state r(t) The instant reward at time t \tau State transtition time r_{i} The instant reward of transition i r_{(n)} The immediate reward at the n^{th} step p_{i} The priority of sampling transition i P(i) The probability of sampling transition i L_{P} The size of the moving average window for popularity change detection L_{R} The size of the moving average window for request rate change detection th_{R} Threshold for the acceptable variance of the request rate th_{c} Threshold for the acceptable variance of the cosine similarity"
https://arxiv.org/html/2411.10385v1,"Low-Latency Task-Oriented Communications with Multi-Round, Multi-Task Deep Learning","In this paper, we address task-oriented (or goal-oriented) communications where an encoder at the transmitter learns compressed latent representations of data, which are then transmitted over a wireless channel. At the receiver, a decoder performs a machine learning task, specifically for classifying the received signals. The deep neural networks corresponding to the encoder-decoder pair are jointly trained, taking both channel and data characteristics into account. Our objective is to achieve high accuracy in completing the underlying task while minimizing the number of channel uses determined by the encoder’s output size. To this end, we propose a multi-round, multi-task learning (MRMTL) approach for the dynamic update of channel uses in multi-round transmissions. The transmitter incrementally sends an increasing number of encoded samples over the channel based on the feedback from the receiver, and the receiver utilizes the signals from a previous round to enhance the task performance, rather than only considering the latest transmission. This approach employs multi-task learning to jointly optimize accuracy across varying number of channel uses, treating each configuration as a distinct task. By evaluating the confidence of the receiver in task decisions, MRMTL decides on whether to allocate additional channel uses in multiple rounds. We characterize both the accuracy and the delay (total number of channel uses) of MRMTL, demonstrating that it achieves the accuracy close to that of conventional methods requiring large numbers of channel uses, but with reduced delay by incorporating signals from a prior round. We consider the CIFAR-10 dataset, convolutional neural network architectures, and AWGN and Rayleigh channel models for performance evaluation. We show that MRMTL significantly improves the efficiency of task-oriented communications, balancing accuracy and latency effectively.","In the rapidly advancing field of NextG communication systems, there is an increasing focus on task-oriented (or goal-oriented) communications. This approach is gaining prominence as it addresses the specific needs of various applications by ensuring that the transmission process is aligned with the ultimate objective of the task at hand [1, 2, 3, 4, 5, 6, 7, 8, 9]. Unlike traditional communication paradigms that focus on delivering raw data, task-oriented communications (TOC) aims to transmit only the information necessary to accomplish a specific task. Deep learning plays a crucial role in optimizing the encoding and decoding processes for TOC, allowing for efficient and effective transmission of information that directly contributes to the task’s success. By leveraging deep learning-driven TOC, NextG communication systems can achieve significant improvements in both performance and resource utilization [10, 11, 12], making them well-suited for the demands of modern applications such as the Internet of Things (IoT), augmented reality/virtual reality (AR/VR), and vehicle-to-everything (V2X) network systems. In IoT networks, sensors generate vast amounts of data that need to be processed and analyzed to make real-time decisions, such as in smart cities and industrial automation. TOC can significantly reduce the communication overhead by transmitting only the essential information required for decision-making, rather than the raw sensor data. Similarly, in AR/VR applications, low latency and high accuracy are critical to delivering immersive experiences. TOC can help achieve this by optimizing the transmission of visual and sensory data to meet the application’s specific needs. In V2X systems, vehicles need to communicate with each other and with infrastructure to ensure safe and efficient transportation. TOC can enhance these interactions by focusing on the transmission of critical information, such as collision warnings and traffic updates, thereby improving response times and reducing network congestion. One of the primary challenges in TOC is balancing task accuracy and latency objectives and requirements. To that end, the age of task information for TOC was studied in [13]. Achieving high accuracy often requires transmitting a large amount of data, which can lead to increased delay (measured by the number of channel uses) and higher bandwidth usage. Conversely, minimizing delay and bandwidth usage can compromise accuracy. This accuracy-delay tradeoff is a significant hurdle that needs to be addressed to realize the full potential of TOC. We propose a novel multi-round, multi-task learning (MRMTL) approach to address this challenge by dynamically updating the number of channel uses in iterative transmissions of TOC. MRMTL involves an encoder at the transmitter that learns compressed latent representations of input data (e.g., images), which are transmitted over a wireless channel. At the receiver, a decoder performs a machine learning task, specifically classifying the received signals. MRMTL is different from the autoencoder-based communications, where the typical setting has the source-coded data symbols as the input and the reconstruction of those symbols as the output [14]. On the other hand, MRMTL starts with (raw) input data and performs a machine learning task such as classification. The deep neural networks (DNNs) corresponding to the encoder-decoder pair are jointly trained, considering both channel and data characteristics to achieve high task accuracy with minimal channel uses. MRMTL introduces a dynamic update mechanism where the transmitter incrementally sends an increasing number of encoded samples over the channel. The receiver utilizes signals from a previous round to enhance task performance, rather than relying solely on the latest transmissions. The multi-round process utilizes multi-task learning that jointly optimizes accuracy across multiple rounds, with each configuration treated as a distinct task performed with a different number of channel uses. When the receiver’s confidence in the task decisions is low, it can then allocate additional channel uses to improve task accuracy. We demonstrate that MRMTL achieves the accuracy of conventional methods with single-round, single-task learning (SRSTL) for TOC that requires a large number of channel uses, but with significantly reduced delay by incorporating signals from a prior round. To evaluate MRMTL, we consider the CIFAR-10 dataset as the input, convolutional neural network (CNN) architectures as the DNNs, and AWGN and Rayleigh channel models. Our results show that MRMTL significantly improves the efficiency of TOC, effectively balancing accuracy and delay. This study represents a significant step forward in the development of efficient and effective TOC systems for NextG networks. The MRMTL approach can be extended to incorporate semantic communications [15] and integrated sensing and communications [16, 17] by adding tasks of reconstruction and sensing, respectively. The remainder of the paper is organized as follows. Section II describes SRSTL for TOC. Section III presents MRMTL for TOC. Section IV introduces the MRMTL’s process of dynamic initiation of multiple rounds in TOC. Figure 1: System model of SRSTL for TOC."
https://arxiv.org/html/2411.09996v1,"Building 6G Radio Foundation Models 
with Transformer Architectures","Foundation deep learning (DL) models are general models, designed to learn general, robust and adaptable representations of their target modality, enabling finetuning across a range of downstream tasks. These models are pretrained on large, unlabeled datasets using self-supervised learning (SSL). Foundation models have demonstrated better generalization than traditional supervised approaches, a critical requirement for wireless communications where the dynamic environment demands model adaptability. In this work, we propose and demonstrate the effectiveness of a Vision Transformer (ViT) as a radio foundation model for spectrogram learning. We introduce a Masked Spectrogram Modeling (MSM) approach to pretrain the ViT in a self-supervised fashion. We evaluate the ViT-based foundation model on two downstream tasks: Channel State Information (CSI)-based Human Activity sensing and Spectrogram Segmentation. Experimental results demonstrate competitive performance to supervised training while generalizing across diverse domains. Notably, the pretrained ViT model outperforms a four-times larger model that is trained from scratch on the spectrogram segmentation task, while requiring significantly less training time, and achieves competitive performance on the CSI-based human activity sensing task. This work demonstrates the effectiveness of ViT with MSM for pretraining as a promising technique for scalable foundation model development in future 6G networks.","Foundation models (FMs) are first trained on a large, often unlabeled dataset, allowing them to build broad, adaptable representations that can be finetuned for various downstream tasks. This initial pretraining stage is done using self-supervised learning (SSL), where the model learns underlying patterns and relationships within the data without relying on labeled examples [1, 2, 3]. The model ideally develops a robust understanding of its target modality, which, in our case, is radio spectrograms. In fields like computer vision and natural language processing, FMs have set new benchmarks [4, 5, 6, 7], often surpassing supervised learning models, specifically designed for individual tasks. This is largely due to their ability to generalize: FMs learn flexible and transferable representations that make them better suited to handle variations in data, perform across diverse tasks, and adapt to new contexts. Generalization is especially valuable when labeled data is scarce, as foundation models can perform well with minimal additional labeled samples. Deep learning (DL) has demonstrated strong potential when applied to individual wireless tasks, including automatic modulation classification [8], channel estimation [9], constellation and waveform design [10], among others. However, these models are highly specialized, and there are concerns about their ability to generalize effectively in real-world scenarios. Wireless signals are subject to time-varying impairments, and the communication environment is constantly changing, which can degrade a DL model’s performance if it fails to adapt. Introducing the concept of FMs for wireless can potentially overcome these limitations [11, 12]. We propose FMs for wireless signals as a solution to address these challenges. By capturing over-the-air radio signals and pretraining FMs through SSL, there is no need for labeled data. Additionally, these pretrained models can then serve as backbones for multiple tasks, reducing computational costs. Most importantly, FMs are expected to achieve better generalization by leveraging their broad, transferable representations, making them well-suited to handle diverse and dynamic wireless environments. The primary contributions of our paper are: • We propose and demonstrate the effectiveness of a Vision Transformer (ViT) as a radio foundation model for spectrogram learning. Adopting ViT as the FM offers enhanced flexibility, particularly in handling variable input sequences, and increased scalability, as training and evaluation can be parallelized. ViT also captures long-term dependencies through its attention mechanisms. • We introduce a Masked Spectrogram Modeling (MSM) approach to pretrain the ViT in a self-supervised fashion, and thoroughly evaluate key design considerations of the masking procedure and transformer size on performance. • By finetuning across two downstream tasks, we demonstrate that the ViT radio FM effectively learns features that generalize across diverse domains, achieving competitive—or even superior—performance with 4x smaller model sizes compared to baselines. • We demonstrate the effectiveness of the proposed foundation model by utilizing a real-world dataset that is captured over-the-air in a software-defined radio testbed. Upon acceptance, the datasets and code will be publicly available to encourage further research within the community on FM for wireless. The remainder of the paper is structured as follows: Section II presents the datasets utilized for pretraining the foundation model, and for the CSI-based human activity sensing and spectrogram segmentation tasks. Section III outlines the ViT architecture and algorithm of the self-supervised foundation model. Section IV presents numerical experiments conducted to evaluate the proposed methodology. Finally, section V concludes the paper."
https://arxiv.org/html/2411.09913v1,"A Graph-based Strategic Sensor 
Deployment Approach for k-coverage in WSN","This paper studies a graph-based sensor deployment approach in wireless sensor networks (WSNs). Specifically, in today’s world, where sensors are everywhere, detecting various attributes like temperature and movement, their deteriorating lifetime is indeed a very concerning issue. In many scenarios, these sensors are placed in extremely remote areas, where maintenance becomes challenging. As a result, it is not very wise to depend on a single sensor to obtain data from a particular terrain or place. Hence, multiple sensors are deployed in these places, such that no problem arises if one or few of them fail. In this work, this problem of intelligent placement of sensors is modelled from the graph theoretic point of view. We propose a new sensor deployment approach here, which results in lesser sensor density per unit area and less number of sensors as compared to the existing benchmark schemes. Finally, the numerical results also support our claims and provide insights regarding the selection of parameters that enhance the system performance.","With the rapid evolution of applications like Internet of Things (IoT), wireless sensors are everywhere. Wireless sensor network (WSN) is a reality today; the global IoT connections is forecast to reach around 38.9 billion in 2029 [1]. As a result, the lifetime of these sensors is a very critical issue and hence, they need to be regularly monitored and recharged. In this context, there has been research regarding efficient environment aware wireless transmission techniques, which extend the sensor lifetime by avoiding unwanted transmissions [2],[3]. However, in many applications, they are placed in very remote and challenging areas, where their maintenance is extremely challenging. Thus, it is not wise to rely solely on a single sensor placed in such areas for data. In graph theoretical terms, this is termed as 1-coverage, i.e., every point in the region is monitored or covered by at least one sensor. This leads to a more evolved concept of graph theory, which is k-coverage, i.e., all the points in a concerned ‘region’ is covered or monitored by at least k sensors. Note that, the value of k depends on the application at hand; a higher value of k implies that a higher degree of reliability is required, and vice-versa [4]. The simplest possible strategy to attain a certain coverage is the one, where we deploy all the available sensors in the region of concern, such that the defined set of targets are totally covered. However, as stated earlier, these sensors have a limited lifetime and hence, this method is not desirable from the application point of view [5]. Also, this leads to unwanted wastage of resources. The authors in [6] proposed a bound on the sensor density in order to cover a certain region of interest. The work in [7] proposed a sensor deployment strategy, where they used regular hexagons of a certain side length to cover the entire region and accordingly, place the sensors inside. In this context, we propose an intelligent graph-based sensor deployment strategy. Specifically, Section II introduces the essential terminology required throughout the work and also the respective system model. Section III discusses the proposed strategy by modeling the sensor deployment problem as a k-coverage problem, where it is assumed that the sensors are placed in the environment as a regular polygon tiling. To do so, we assess and examine various planar convex regular polygons, which possess the ability to tile the entire Euclidean plane. As hexagons prove to be the most efficient of the regular polygons, we assume hexagonal tiling. Based on hexagonal tiling of the region of interest, we strategically place the sensors inside a regular hexagon, and by employing this method for every hexagon, we cover the entire Euclidean plane. Moreover, we characterize the performance of the proposed strategy in terms of sensor density and number of sensors required to cover a certain region of interest. We show that our proposed strategy outperforms the state of the art deployment strategy [7] in terms of both sensor density and total number of sensors required. Finally, Section IV presents the numerical results and Section V concludes our work."
https://arxiv.org/html/2411.09849v1,"Self-Supervised Radio Pre-training: Toward 
Foundational Models for Spectrogram Learning","Foundational deep learning (DL) models are general models, trained on large, diverse, and unlabelled datasets, typically using self-supervised learning techniques have led to significant advancements especially in natural language processing. These pretrained models can be fine-tuned for related downstream tasks, offering faster development and reduced training costs, while often achieving improved performance. In this work, we introduce Masked Spectrogram Modeling, a novel self-supervised learning approach for pretraining foundational DL models on radio signals. Adopting a Convolutional LSTM architecture for efficient spatio-temporal processing, we pretrain the model with an unlabelled radio dataset collected from over-the-air measurements. Subsequently, the pretrained model is fine-tuned for two downstream tasks: spectrum forecasting and segmentation. Experimental results demonstrate that our methodology achieves competitive performance in both forecasting accuracy and segmentation, validating its effectiveness for developing foundational radio models.","A foundational model is a general model pretrained on a large-scale - usually unlabeled - dataset, typically through self-supervised learning [1]. Through this training, the model develops a solid understanding of the target modality, such as text in natural language processing (NLP) or images in computer vision. This understanding allows the model to be fine-tuned for diverse downstream tasks. Foundational models in NLP [2, 3] and computer vision [4] have driven significant advancements through leveraging the knowledge encoded in their pretrained representations. This facilitates quicker experimentation, more efficient resource utilization, and potentially, improved performance on downstream tasks that smaller models or those with more limited domain knowledge cannot achieve. Deep learning has showcased promising results when applied in wireless communication [5]. The effectiveness has been demonstrated across various tasks, including automatic modulation classification [6], channel estimation [7], constellation and waveform design [8], among others. However, these models are highly specialized, echoing the early stages of deep learning’s evolution in NLP and computer vision. The reliability of these models across data distribution shifts and their ability to generalize is also usually limited. Introducing the concept of foundational models into wireless communication holds substantial promise to overcome these limitations [9]. We argue that as in NLP and computer vision, where a wealth of unlabeled data exists — communication signals can be harnessed for pretraining such foundational models through self-supervised learning, mitigating the expense associated with data labeling. Moreover, leveraging a foundational model as a backbone for multiple downstream tasks, which utilize its pretrained representations in subsequent processing, reduces computational demands. This approach can also improve generalization by leveraging the broader knowledge encoded within foundational model representations compared to highly specialized models which suffer from limited scope. Drawing inspiration from these advancements, particularly in [2, 10, 4], we introduce a foundational radio model pretrained using masked spectrogram modelling (MSM) — a novel technique, we propose for wireless signals. This model is then fine-tuned to perform two different downstream tasks: spectrogram forecasting, which involves predicting future spectrogram based on past data, and spectrogram segmentation, which consists of distinguishing between background noise and other signal activities within the spectrogram. These tasks, while different, are complementary in the context of spectrum analysis and constitute a usage scenario for a foundational model integrated in a opportunistic spectrum access system. The primary contributions of our paper are: • We propose and develop a novel self-supervised learning approach, MSM, for pre-training foundational models on radio signals. To the best of our knowledge, this work represents the first demonstration of radio foundational models for spectrogram learning using unlabeled data. • We demonstrate the effectiveness of the proposed approach utilizing a real-world dataset that we collected over a software-defined radio testbed. The recordings are time-domain IQ samples received between 2.4 to 2.65 GHz. • Our results show that the developed MSM approach is able to learn features that generalize to both related and unrelated downstream tasks. Fine-tuning the foundational model demonstrated competitive results for spectrum forecasting and spectrum segmentation which had a distinctly different and unseen data distribution. The results of this paper highlight the significant potential that radio foundational models have to effectively enable multiple downstream spectrogram tasks. It is envisioned that such models will foster wider adoption of AI to enable reliable network performance and services. The remainder of the paper is structured as follows: Section II presents the two datasets utilized for pretraining the foundational model, and for the spectrum forecasting and segmentation tasks. Section III outlines the architecture and algorithm of the self-supervised foundational model. Section IV presents numerical experiments conducted to evaluate the proposed methodology. Finally, Section V concludes the paper."
https://arxiv.org/html/2411.09660v1,"Capacity and Power Consumption of Multi-Layer
6G Networks Using the Upper Mid-Band","This paper presents a new system model to evaluate the capacity and power consumption of multi-layer 6G networks utilising the upper mid-band (FR3). The model captures heterogeneous 4G, 5G, and 6G deployments, analyzing their performance under different deployment strategies. Our results show that strategic 6G deployments, non-co-located with existing 5G sites, significantly enhance throughput, with median and peak user rates of 300 Mbps and exceeding 1 Gbps, respectively. We also emphasize the importance of priority-based cell reselection and beam configuration to fully leverage 6G capabilities. While 6G implementation increases power consumption by 33%, non-co-located deployments strike a balance between performance and power consumption.","We stand on the cusp of a new era—the sixth generation (6G) of mobile communication technology. The International Telecommunication Union (ITU) has already outlined the requirements for 6G [1], and 3rd Generation Partnership Project (3GPP) recently agreed on a development timeline [2]. Once again, exciting times lie ahead. To gain a perspective on 6G, let us first reflect on the key advancements of the fifth generation (5G). From our viewpoint, 5G adoption has been built on three main pillars [3]: • A key software development: network slicing. • An appealing technology: ultra-reliable low latency communication (URLLC), which has seen limited success due to Wi-Fi’s dominance in indoor use cases [4, 5]. • A hardware and signal processing breakthrough: massive multiple-input multiple-output (mMIMO). Other features specified in 5G, while relevant and promising, are yet to gain widespread market traction. A notable example is millimeter wave (mmWave) technology, whose spectrum licenses did not sell as expected globally, indicating a lack of interest from operators in this power-hungry technology. We predict that 6G development and market adoption will follow a path similar to 5G. While many foresee a vast array of new functionalities, we anticipate three key components: a software breakthrough, a compelling new use case, and a hardware evolution: • Integrating artificial intelligence (AI) will be central across all protocol stack levels and control systems [6]. • Joint communication and sensing will likely serve as the key marketing driver for 6G[7]. However, similar to URLLC, we remain cautious about its market adoption, given the strong position of incumbents in this segment. • Hardware advancements will include breakthroughs in system-on-chip technology for AI processing and further advancements in mMIMO, leading to extreme mMIMO operating in the upper mid-band (7–24 GHz), referred to as frequency range 3 (FR3) in 3GPP[8]. Commercial 6G will center around FR3 and advancements in mMIMO. The introduction of multiple 400 MHz channels per operator in the upper mid-band offers significant potential to support demanding 6G use cases, providing an ideal balance between coverage and capacity for both outdoor and indoor deployments. The success of this band will depend on advanced mMIMO technology, with up to 256 transceivers and many more passive antennas required to achieve median speeds of up to 1 Gbps per user. However, further developing mMIMO as we did for 5G may result in prohibitive power consumption, and it remains unclear whether co-located 6G deployments alongside existing 5G sites will be effective. In this paper, we evaluate the capacity and power consumption of multi-layer 6G networks using FR3. Our analysis highlights the superior performance of strategic 6G deployments non-co-located with 5G, achieving median user rates of up to 300 Mbps and peak rates exceeding 1 Gbps. We also emphasize the critical role of priority-based cell reselection and beam codebook configuration in fully leveraging the capabilities of 6G. While 6G implementation results in a 33% increase in power consumption, non-co-located deployments offer the best trade-off between performance gains and power consumption. Our findings underscore the need for a targeted approach to 6G deployment, paving the way for sustainable next-generation networks."
https://arxiv.org/html/2411.09380v1,Connecting the Unconnected: A DT Case Study of Nomadic Nodes Deployment in Nepal,"This paper addresses the challenge of robust cellular connectivity in dense, underdeveloped urban environments, specifically focusing on Kathmandu, Nepal. As cities grow, existing cellular infrastructure struggles to meet the demand for reliable, high-throughput, and low-latency communication services. The lack of investment in new technologies and the intricacies of the cities’ landscape pose even more difficulties for robust connectivity. This work addresses the above challenges in a cost-effective and flexible way. We investigate the deployment of LTE Nomadic Nodes (NNs) at scale in order to enhance network capacity and coverage. Utilising a Digital Twin (DT), we simulate and optimise NN placement, considering Kathmandu’s physical and environmental characteristics. Our approach leverages the DRIVE DT framework, which enables the systemic evaluation of various network configurations and user mobility scenarios. The results demonstrate that NNs significantly improve signal strength and expected user datarates, presenting a viable solution for enhancing urban cellular connectivity.","As urbanisation proliferates, the density of urban environments is unprecedented. Estimates show that by 2050, two-thirds of the world’s population will inhabit urban areas [1]. Cellular connectivity is crucial for densely populated cities, enhancing urban living through advancements in transportation, healthcare, public safety, and environmental monitoring [2, 3]. Connectivity is also integral for day-to-day activities such as e-banking, staying connected with family and friends, navigation, and infotainment. All use cases require increased throughput, reliable connectivity, and low latency [4]. Although the penetration of the public digital infrastructure is increasing in developing countries [5, 6], the growing demand for new applications and the increasing number of users outperform the capacity of the infrastructure. The lack of investment in installing additional Base Stations (BSs) worsens the connectivity problem [7, 8]. Addressing these challenges is crucial [1], as a diverse array of critical functions and services that form the socio-economic foundation of contemporary cities depend on seamless connectivity [9]. Inspired by the above, this work will focus on the cellular connectivity problems of Kathmandu, Nepal. We aim to describe a solution that will be cost-effective, easily applicable, and viable in densely populated areas with intricate infrastructure, such as Nepal. We address the problem by investigating the feasibility of creating heterogeneous cellular networks by employing Nomadic Nodes (NNs) [10]. NNs are usually self-contained, mobile, and flexible in their deployment and are frequently used to extend existing infrastructure and enhance connectivity across large geographical areas [10]. As in almost all urban areas of Asia-Pacific, Nepal mainly depends on 4G cellular connectivity [5, 7]. Therefore, we focus on exploring the 4G cellular connectivity in a dense urban scenario in the heart of Kathmandu, Nepal. This scenario needs to be investigated multimodally, considering all the intricacies of dense network deployment, such as user density and mobility, the optimal placement of the NN in conjunction with the existing BSs, the signal attenuation from physical impediments and tall buildings, and more. We employ Digital Twin (DT)-ing, where we model various physical objects in the digital world, emulate different interactions, and run various what-if scenarios at a large scale. Our DT of choice is the Digital Twin for self-dRiving Intelligent VEhicles (DRIVE) [11, 12], a framework for large-scale communication scenarios, allowing traffic participants and the communication infrastructure to interact through various communication planes. DRIVE is a “Digital Network Oracle” that maintains snapshots of the states of the “virtual world”. These snapshots can be leveraged for various optimisations or to validate performance changes across large-scale scenarios without the need for resource-expensive simulations [11]. As the current version of DRIVE did not meet the requirements of the investigated scenario, the framework was extended, as discussed in the rest of the paper. The NN placement is based on a real map from OpenStreetMaps [13] while taking heed of the existing BS placement extracted from CellMapper [14]. Kathmandu’s physical landscape (buildings, parks, etc.) is considered for both the optimal placement of NNs and the realistic channel propagation characteristics in such a scenario. Moreover, our dense data traffic conditions are based on spatio-temporal user mobility models to recreate a realistic scenario. DRIVE brings all models and interactions within the same framework, enabling systemic evaluation across the entire city’s plane. Our evaluation explores the parameter space, and the results indicate improved network connectivity in terms of the signal strength or the users’ perceived datarates. The rest of the paper is structured as follows. Sec. II overviews the state-of-the-art. Our system model is presented in Sec. III explaining various extensions to DRIVE. The NN placement and the way we calculate the overall datarate are described in Sect. IV. Sec. V summarises our results and findings. Finally, the manuscript concludes in Sec. VI."
https://arxiv.org/html/2411.09148v1,Toward Democratized Generative AI in Next-Generation Mobile Edge Networks,"The rapid development of generative AI technologies, including large language models (LLMs), has brought transformative changes to various fields. However, deploying such advanced models on mobile and edge devices remains challenging due to their high computational, memory, communication, and energy requirements. To address these challenges, we propose a model-centric framework for democratizing generative AI deployment on mobile and edge networks. First, we comprehensively review key compact model strategies, such as quantization, model pruning, and knowledge distillation, and present key performance metrics to optimize generative AI for mobile deployment. Next, we provide a focused review of mobile and edge networks, emphasizing the specific challenges and requirements of these environments. We further conduct a case study demonstrating the effectiveness of these strategies by deploying LLMs on real mobile edge devices. Experimental results highlight the practicality of democratized LLMs, with significant improvements in generalization accuracy, hallucination rate, accessibility, and resource consumption. Finally, we discuss potential research directions to further advance the deployment of generative AI in resource-constrained environments.","The rapid growth of generative AI technologies, including large language models (LLMs) and other generative models, has led to significant breakthroughs across various domains, enabling impactful applications such as conversational systems (e.g., ChatGPT) and image generation tools (e.g., DALL-E) [1]. These models have demonstrated remarkable capabilities in tasks such as text generation, image creation, and multimodal understanding, accelerating digital transformation in various fields such as healthcare, education, and content creation. Despite these impressive developments, deploying large generative models remains highly challenging, particularly on mobile and edge devices. Specifically, training advanced foundation models such as GPT-4 typically requires massive computational resources, involving thousands of GPUs running for weeks, which far exceeds the capacity of conventional hardware. For example, training GPT-3, with over 175 billion parameters, requires consuming hundreds of petaflops of computational power over several weeks on advanced supercomputers111https://www.trgdatacenters.com/resource/ai-chatbots-energy-usage-of-2023s-most-popular-chatbots-so-far/. During deployment, transferring the foundation models to mobile edge devices results in significant bandwidth consumption. During inference, running the foundation models also demands substantial computational and memory resources, surpassing the limits of most mobile CPUs and GPUs. For instance, deploying GPT-3 on a standard smartphone would be impractical due to its memory requirement of around 350 GB, whereas most mobile devices have only a few gigabytes of available memory222https://www.osinto.com/post/scale-is-all-you-need. Furthermore, mobile devices are subject to strict energy constraints, as executing complex AI models can rapidly drain battery life, making them unsuitable for practical use [2]. For instance, running intensive AI inference tasks on a typical smartphone can deplete its battery within a few hours, which makes continuous or prolonged use unfeasible. These challenges are compounded by limited bandwidth, which makes frequent communications with cloud servers or model updates impractical in low-bandwidth environments such as rural areas. Thus, novel approaches are required to make these advanced capabilities feasible in resource-limited environments. To address these challenges, the concept of democratized generative AI has emerged [3]. Democratized generative AI aims to make generative AI technologies broadly accessible by overcoming barriers such as high computational costs, energy consumption, memory limitations, and reliance on centralized infrastructure. This concept encompasses approaches such as TinyML and Small ML, which optimize models to operate efficiently on resource-constrained devices such as mobile phones and edge nodes. TinyML, for instance, emphasizes running machine learning models on small, power-efficient microcontrollers. From social and economic perspectives, democratized AI fosters inclusivity by reducing entry barriers, allowing a diverse group of users, from individual consumers to small organizations, to benefit from advanced AI capabilities without requiring extensive infrastructure333https://www.splunk.com/en_us/blog/learn/democratized-generative-ai. In the context of next-generation mobile and networking systems, democratized generative AI is becoming increasingly crucial. AI technologies are integral to optimizing network performance, managing bandwidth, and automating network configurations to achieve efficient communication [4]. However, deploying these resource-intensive models on mobile and edge devices, where computational power, energy, and bandwidth are often limited, presents significant challenges. Therefore, there is a need for innovative compact model strategies that make AI accessible without significantly compromising performance. Based on these considerations, this article aims to provide a forward-looking exploration of model-centric compact model strategies for deploying generative AI on mobile and edge devices with LLMs as an example. To the best of our knowledge, this is the first article to explore a comprehensive framework for democratizing LLM deployment in mobile networks. The main contributions of this work can be summarized as follows: • We propose a comprehensive democratized generative AI framework that incorporates compact model strategies, including quantization, model pruning, and knowledge distillation, to facilitate the deployment of generative AI on resource-constrained mobile and edge devices. • We conduct a systematic analysis and comparison of various compact model strategies, focusing on their ability to reduce computational demands, memory usage, and energy consumption while preserving model performance. Moreover, we define key performance metrics to assess their practical impact on generative AI applications in constrained environments. • We present experimental evaluations demonstrating the effectiveness of these compact model strategies in improving generalization accuracy, hallucination rate, accessibility, and resource consumption on actual mobile edge devices. The results validate the practicality of deploying democratized LLMs on real mobile devices."
https://arxiv.org/html/2411.09134v1,ABCI 3.0: Evolution of the leading AI infrastructure in Japan,"ABCI 3.0 is the latest version of the ABCI, a large-scale open AI infrastructure that AIST has been operating since August 2018 and will be fully operational in January 2025. ABCI 3.0 consists of computing servers equipped with 6128 of the NVIDIA H200 GPUs and an all-flash storage system. Its peak performance is 6.22 exaflops in half precision and 3.0 exaflops in single precision, which is 7 to 13 times faster than the previous system, ABCI 2.0. It also more than doubles both storage capacity and theoretical read/write performance. ABCI 3.0 is expected to accelerate research and development, evaluation, and workforce development of cutting-edge AI technologies, with a particular focus on generative AI.","AI Bridging Cloud Infrastructure (ABCI) is the world’s first large-scale open AI infrastructure, designed and developed by AIST with the aim of accelerating the development of AI technologies in Japan [1, 2, 3]. It has been installed in the AI Data Center located at AIST’s Kashiwa Center, and began operating in August 2018. To date, many organizations have achieved remarkable results by using ABCI, including the successful construction of Japanese large language models (LLMs) such as PLaMo [4], Swallow LLM [5], and so on. On the other hand, as the demand for generative AI in industry, academia, and government in Japan is growing rapidly, ABCI cannot provide enough resources to meet such user demands in a timely manner. In addition, the development of generative AI is still at an early stage and currently focuses mainly on natural languages. In the near future, it is important to develop real world multimodal foundation models that are built using large amounts of image, audio and sensor data obtained from the real world, such as manufacturing, transportation, and so on. In order to demonstrate such cutting-edge AI technologies, it is imperative to improve the computational capability of ABCI. Since November 2024, AIST has been gradually starting to operate the latest system in the ABCI series, ABCI 3.0, while leveraging the existing technical assets of ABCI. ABCI 3.0 will be fully operational in January 2025. This paper is organized as follows. Section 2 introduces the hardware configuration of ABCI 3.0. In Section 3, we describe the initial software installed. The data center facility where we have upgraded for ABCI 3.0 is presented in Section 4. Finally, Section 5 briefly mentions the future perspective. Table 1: ABCI series AAIC a ABCI 1.0 ABCI 2.0 b ABCI 3.0 Start of operation 2017 2018 2021 2024 Number of nodes 50 1088 120 766 GPU NVIDIA P100 NVIDIA V100 NVIDIA A100 NVIDIA H200 Number of GPUs 400 4352 960 6128 CPU Intel Xeon E5 v4 Intel Xeon Gold 6148 Intel Xeon Platinum 8360Y Intel Xeon Platinum 8558 FP64 (PFLOPS) 2.2 37.2 19 415 FP32 (PFLOPS) 4.4 74.4 150 3000 FP16 (PFLOPS) 8.6 550 300 6220 a AAIC is the prototype system of ABCI. b ABCI 2.0 is the extension based on ABCI 1.0, and this table only shows the specification of the extension part. We usually refer to the performance of ABCI 2.0 as the performance of ABCI 1.0 with the extension part."
https://arxiv.org/html/2411.08907v1,From Simulators to Digital Twins for Enabling Emerging Cellular Networks: A Tutorial and Survey,"Simulators are indispensable parts of the research and development necessary to advance countless industries, including cellular networks. With simulators, the evaluation, analysis, testing, and experimentation of novel designs and algorithms can be executed in a more cost-effective and convenient manner without the risk of real network service disruption. Additionally, recent trends indicate that the advancement of these Digital System Models (DSM), such as system-level simulators, will hold a pivotal role in advancing cellular networks by facilitating the development of digital twins. Given this growing significance, in this survey and tutorial paper, we present an extensive review of the currently available DSMs for 5G and beyond (5G&B) networks. Specifically, we begin with a tutorial on the fundamental concepts of 5G&B network simulations, followed by an identification of the essential design requirements needed to model the key features of these networks. We also devised a taxonomy of different types of 5G&B network simulators. In contrast to existing simulator surveys, which mostly leverage traditional metrics applicable to legacy networks, we devise and use 5G-specific evaluation metrics that capture three key facets of a network simulator, namely realism, completeness, and computational efficiency. We evaluate each simulator according to the devised metrics to generate an applicability matrix that maps different 5G&B simulators vis-à-vis the different research themes they can potentially enable. We also present the current challenges in developing 5G&B simulators while laying out several potential solutions to address the issues. Finally, we discuss the future challenges related to simulator design provisions that will arise with the emergence of 6G networks.","Computer-aided numerical simulations, or simulators, are used as a first-tier assessment tool to evaluate the diverse features of cellular networks [1]. For instance, cellular network operators traditionally rely on cellular network simulators such as Atoll [2], Planet [3], and Asset [4] to assist in the design, planning, and optimization stages of network rollout. Academic researchers, on the other hand, use simulators such as MATLAB [5], Vienna [6], and ns-3 [7] to design, analyze, and test new protocols, architectures, and features. Compared to alternative strategies such as analytical models, testbeds, and field trials, simulators are more practical when considering factors such as risk to real networks, benefits to industry, utilities to research and development, and resources needed to develop and perform, as summarized in Fig. 1. Compared to analytical modeling, simulators have the ability to generate results even for non-tractable mathematical problems. Although testbeds and field trials can provide a more realistic and practical assessment of wireless networks, simulators pose minimal risk to live cellular networks and require fewer resources to implement and develop. As cellular network technology evolves to enable an expanding number of emerging use cases and as the demand for mobile data grows at an unprecedented magnitude, simulators have become increasingly important for both industry and academia. For instance, simulators are being considered as a promising solution to address the data scarcity challenge in the cellular network domain [8]. However, amidst the increasing complexity and stringent requirements of future cellular networks, simulators must continue to evolve to remain relevant. Figure 1: Spider web diagram of different cellular network evaluation tools using a penta-prong metrics including practicality of the results, utility to industry, utility to academic R&D, risk to the live network, and required resources. Figure 2: Digital system model utilized as digital twin kernel. Recent trends suggest that to provide a more advanced method of network modeling, the cellular network industry is moving towards digital twins (DTs), which are virtual and up-to-date representations of a physical system (i.e., a cellular network) [9, 10]. This emphasis on DTs has gained traction within the cellular network community, particularly in shaping the foundation of 6G networks, as highlighted by several studies [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]. These collective endeavors signify a substantial shift towards leveraging DTs as a fundamental enabler in the evolution towards 6G networks. DTs are increasingly recognized to be valuable assets in addressing diverse challenges encountered within cellular networks, including resource allocation [23], energy efficiency [24], cellular edge networks [25, 26, 15], optimizing communication and computation costs associated with DTs [27] and the deployment of Open Radio Access Network (O-RAN) [19]. As DT closely mirrors a real cellular network, it serves as a reliable platform for developing and benchmarking new research directions accurately. Furthermore, DTs can serve as platforms for network management and optimization, mitigating the risk of service degradation. Leveraging DTs as the testing ground for modifications, such as parameter adjustments, provides a safeguard against potential disruptions, as changes are only implemented in the real network after successful validation within the digital twin. DTs are composed of three unique components: the physical product, the digital or virtual model of the product, and the interconnections represented by the data that reflects the current state of a live system, as shown in Fig. 2. While creating a digital copy of a real system is relatively straightforward for simple systems with low dynamicity, such as static machinery, the process becomes exceptionally challenging when dealing with complex and highly dynamic systems like cellular networks. In such cases, cellular network digital system models (DSMs) play a pivotal role. These DSMs take on a critical function in modeling the intricacies of cellular networks, including different network components (i.e., base station and antenna), radio propagation, protocols, user mobility, traffic patterns, and network performance, to name a few. The concept of evolving DSMs into comprehensive digital twins is exemplified in [28], wherein the authors showcase the potential of a state-of-the-art emulator to transition into a fully realized digital twin. A DSM is a versatile term that can encompass various simulation types, such as system-level or link-level simulators, as well as digital twins, depending on the specific features. For instance, DSM with high fidelity can become a kernel in developing live DT models of cellular networks [10, 29]. The distinction lies in the set of attributes that characterize a DSM as either a simulator or a digital twin. While there are overlaps between DSMs and DTs in terms of their functionalities, several differences exist between the two. These distinctions are summarized in Table I. DSMs, although functional independently, cannot be classified as DTs as they lack connectivity with the real network. The presence of a feedback loop between the digital representation and the real network is one of the major disparities between the two. This connection is pivotal for facilitating seamless data exchange between the DT and the real network. This link is imperative as DTs heavily rely on data-driven models to function [17] unlike simulators which rely more on pre-defined rules and deterministic models. The reliance of DTs on data-driven modeling is crucial for mitigating the complexity of modeling numerous real-time network functionalities. [11]. Consequently, this makes the requirements for efficient predictive modeling capabilities, more stringent in DTs than in simulators. The DTs of 6G wireless systems must rely on efficient AI schemes tailored for handling extensive datasets [9]. This underscores AI’s pivotal role as an enabler of DTs and is expected to be natively integrated into their design, unlike in simulators where its implementation remains optional [18]. Finally, in terms of visualization, DTs demand more advanced capabilities to provide richer representations of physical systems, such as 3D maps, buildings, vegetation, etc. Table I: Distinguishing factors between DSM and DT Digital Twins DSMs (i.e., simulators) Real-time interaction with real network Required Nonexistent Reliance on data-driven models High Low Predictive modeling capabilities High Low AI implementation Native Optional Visualization Advanced Less advanced Figure 3: Summary of available literature on simulator comparison in wireless networks. Inside the circles are the different domains of wireless networks. Inside the connected ellipses are the survey papers available in the literature. The outline of these ellipses represents the year of publication. The figures inside the square represent the number of simulators included in each survey paper. In this survey paper, these attributes are presented using a set of criteria that we call the ""iron triangle"" of cellular networks. The iron triangle encompasses three crucial factors: realism, completeness, and computational efficiency, which together play a critical role in determining whether DSMs can function as simulators or serve as digital twin kernels. By leveraging these criteria, our paper offers valuable insights into the current state of various DSMs and their progress towards becoming digital twin-ready by tackling the challenges posed by the iron triangle. Furthermore, our study aims to raise awareness about the challenges involved in developing these DSMs and explore potential solutions. We also conduct an analysis of future requirements for DSMs to meet the evolving demands of next-generation cellular networks. However, it is essential to clarify that our work does not cover the two-way feedback between the digital model and digital twin, as that aspect falls beyond the scope of this article. Instead, we focus on providing a comprehensive understanding of the key attributes and challenges associated with DSMs, contributing to the broader discourse on digital twin development in the cellular network domain. I-A Related Work Wireless networks (WN) have had a colossal impact on various sectors of human society, including communication, education, defense, security, healthcare, agriculture, and manufacturing, among others. It is a broad area of research and can be broken down into several sub-topics, as shown in Fig. 3. These specific domains include Wireless Sensor Network (WSN), Wireless Mesh Network (WMN), Cellular Network, Mobile Ad-hoc Network (MANET), and Vehicular Ad-hoc Network (VANET), which is a sub-category of MANET. The overwhelming demand to advance wireless communication has led to the development of several simulators that perform a broad array of functionalities. There are several comparative studies on simulators for WNs in the literature. Fig. 3 shows a summary of the related literature we assimilate in this survey paper. Currently, several surveys and comparative studies are focused on WN in general[30, 31, 32, 33, 34, 35]. Meanwhile, other works also exist highlighting the simulators for specific sub-categories of WN. For instance, [36, 37, 38, 39, 40, 41, 42] are surveys on simulators used for MANETs,[43, 44] are works dedicated to VANET simulators, while [45] confined the simulator discussions for WMN. Moreover, the bulk of the papers on simulator surveys and comparative studies are concentrated on WSN, with 11 papers as of this writing [46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56]. While some of these survey papers are comprehensive, including [50, 51, 49, 32, 30, 37, 41] wherein the authors evaluated 10 or more simulators, they can be deemed outdated with regards to emerging cellular networks as most were published between 2008 and 2015. Unlike other branches of WN, such as WSN, WMN, and MANETS, review papers dedicated to simulators pertinent to emerging cellular networks are scarce, as shown in Fig. 3. A review paper that is most related to the scope of our paper is [57], which offers a comparative study on simulators specific to 4G and 5G cellular networks. However, this review paper has a very limited scope as it considers only four simulators, namely, ns-3, OMNeT++, Riverbed Modeler, and NetSim. While there are other review papers available on cellular network simulators, such as those in [58, 59, 60, 61], they primarily concentrate on the specifications of 5G simulators rather than their evaluation. These papers discuss various requirements for simulators, such as reusability, scalability, flexibility, multiple levels of abstraction, parallel processing, and the integration of link-, system-, an d network-level simulators. However, it is worth noting that most of these requirements are not exclusive to 5G networks, and they provide only a cursory assessment of the challenges in developing 5G simulators. While [61] discusses the challenges associated with developing 5G simulators, the study is not comprehensive and does not offer insights into overcoming the challenges inherent in developing 5G&B simulators. Along with the aforementioned articles on cellular network simulators, there exist publications that delve into simulator development [1, 62, 63]. In such works, authors not only expound on the features and capabilities of their respective simulators but also perform comparative analyses with other 5G simulators, highlighting the strengths and advantages of their own tools. For instance, the authors in [62] compared their simulator with ten others with respect to supported features such as massive multi-input multi-output (MIMO), flexible numerology, random access procedure, millimeter wave (mmWave) propagation, and general information such as programming language and license type. Similarly, authors in [1] developed a simulator and compared it to existing simulators using almost similar metrics as [62] with the inclusion of cloud computing capabilities. By discussing its primary features, the authors in [63] juxtaposed the simulator they developed with six other simulators. Table II provides a comparative summary of the existing literature in tabular form, highlighting the distinctions between our work and other studies in this domain. Notably, our study stands out for its comprehensive analysis, which encompasses the broadest range of simulators, including industry-grade tools. Furthermore, our work is distinguished by the diversity of analyses performed, particularly our unique focus on how simulators can facilitate digital twin generation. Unlike previous studies that relied solely on traditional evaluation metrics, our research introduces new metrics, further setting it apart from existing literature. I-B Contributions The examination of the available literature on WN simulators reveals that, in comparison to other domains, cellular network simulators lag in the availability of review papers. Meanwhile, those that are now available are either insufficiently comprehensive, have limited discussion of simulator development challenges and potential solutions, or do not use 5G-specific evaluation measures. Given the shortcomings of the current relevant work on cellular network simulators, there is a dire need for a comprehensive survey that focuses on 5G&B network simulators. In summary, the main contributions of this work are as follows: 1. This paper provides a comprehensive survey of the current simulators for 5G&B networks. In contrast to current literature, which concentrates on general-purpose simulators (shown in Fig. 4), this survey covers simulators targeted for specific 5G use cases. Our discussion includes over 35 link-level, system-level, and network-level simulators. Moreover, this survey is the first to discuss current state-of-the-art, industry-grade commercial simulators. The presented analysis can assist researchers in selecting the proper simulation tool for their needs. 2. We provide potential strategies to reduce the complexity of simulator development by investigating simulator design requirements in light of the nuances and peculiarities of 5G&B communication. We also present several research topics that can be investigated in greater depth after these design requirements are met. 3. We present a new and insightful evaluation metric tailored specifically for 5G&B networks, aimed at discerning the operational status, degree of implementation fidelity, and underlying assumptions inherent to each simulator. This metric serves as a decisive benchmark, enabling the categorization of these DSMs into either potential candidates for enabling DTs or retaining their status as conventional simulators. In contrast to conventional and generic assessment methods, which often involve comparisons based on criteria such as graphical user interface, language platform, licensing model, and modularity, as depicted in Figure 4, our innovative metric focuses on a comprehensive evaluation framework composed of three pivotal criteria: realism, comprehensiveness, and computational efficiency. This refined metric is instrumental in not only gauging the capabilities of each simulator but also probing their applicability across different fields of research in 5G&B. 4. Simulators meeting the three mentioned criteria can be regarded as possessing high fidelity and good quality, however, they cannot be classified as DTs. Another essential aspect that must be present is the connection between the DSM and the real network. While this interconnection is beyond the scope of this paper, existing literature provides insights into what can be communicated through this link. Hence, we introduce a framework outlining the necessary steps to utilize DSM as a core component for generating DTs. This framework offers readers insights into the required information and the role of DSM in DT generation, while also emphasizing the significance of the three metrics. 5. We outline the challenges that may impede the development of realistic, comprehensive, and computationally efficient DSMs for 5G&B networks, preventing them from becoming DT kernels. This analysis is guided by insights from 5G&B network requirements, use cases, and enablers. It is essential to recognize these challenges in order to accelerate research and development toward more accurate and reliable DSMs and transform them into operational DTs rather than just conventional simulators. Furthermore, we identify potential approaches to addressing these challenges based on insights gleaned from academic literature and industrial practice. Finally, based on an array of related articles projecting what 6G will look like, we highlight the upcoming issues of designing 6G-specific DSMs. Table II: A Comparison of Existing Simulator Survey Papers Related Work Wireless Technology Number of Simulators Evaluated Year of Publication Comparison Metrics Used Development Requirements Development Challenges and Solutions Type of Simulators (Applicable only to CN Simulators) Digital Twin Considered Traditional New Link Level System Level Industry-grade NI Sarkar et al. [30] WN 10 2011 ✓ ✗ ✓ ✗ N/A N/A N/A ✗ S. Gupta et al. [35] WN 4 2013 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ S. Siraj et al. [33] WN 8 2012 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ MH Kabir et al. [32] WN 14 2014 ✓ ✗ ✓ ✗ N/A N/A N/A ✗ L. Ronit et al. [34] WN 7 2018 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ AS Toor et al. [31] WN 6 2017 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ H Sundani et al. [51] WSN 14 2011 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ C.P. Singh et al. [49] WSN 13 2008 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ M.Imran et al. [46] WSN 8 2010 ✓ ✗ ✓ ✗ N/A N/A N/A ✗ F. Yu et al. [53] WSN 7 2011 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ M.Korkalainen et al. [47] WSN 5 2009 ✓ ✗ ✓ ✗ N/A N/A N/A ✗ A. Stetsko et al. [56] WSN 4 2011 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ J.Lessmann et al. [48] WSN 4 2008 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ M Jevtic et al. [52] WSN 4 2009 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ B Musznicki et al. [50] WSN 28 2012 ✓ ✗ ✓ ✗ N/A N/A N/A ✗ M. A. Khan et al. [55] WSN 5 2014 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ S Yadav et al. [54] WSN 8 2012 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ L Hogie et al.[37] MANET 11 2006 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ MN Jambli et al.[41] MANET 15 2012 ✓ ✗ ✓ ✗ N/A N/A N/A ✗ J Malhotra et al. [39] MANET 8 2014 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ G. Chengetanai et al. [40] MANET 7 2015 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ A. Kumar et al. [36] MANET 6 2012 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ N. Bhatt et al. [42] MANET 4 2013 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ I. Dorathy et al. [38] MANET 8 2018 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ E Spaho et al. [43] VANET 5 2011 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ FJ Martinez et al. [44] VANET 4 2011 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ P Owczarek et al. [45] WMN 6 2014 ✓ ✗ ✗ ✗ N/A N/A N/A ✗ C. Bouras et al. [57] CN 4 2020 ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✗ P. K. Gkonis et al. [58] CN 8 2020 ✗ ✗ ✗ ✓ ✓ ✓ ✗ ✗ Y. Wang et al. [60] CN ✗ 2014 ✗ ✗ ✗ ✓ ✗ ✓ ✗ ✗ S. Cho et al. [61] CN ✗ 2017 ✗ ✗ ✓ ✓ ✓ ✓ ✗ ✗ S. M. A. Zaidi et al. [1] CN 14 2020 ✓ ✗ ✗ ✗ ✓ ✓ ✗ ✗ S. Martiradonna et. al [62] CN 11 2020 ✓ ✗ ✓ ✗ ✓ ✓ ✗ ✗ R. I. Tinini et al. [63] CN 7 2020 ✓ ✗ ✗ ✗ ✓ ✓ ✗ ✗ This survey & tutorial paper CN 35 TBD ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Figure 4: List of the simulators evaluated and comparison metrics used in relevant literature. I-C Paper Organization The structure of the paper, visualized in Fig. 5, is organized as follows: Section I provides the introduction and relevant work. Section II presents a tutorial focusing on the roles, strengths, and limitations of wireless network simulators. This section also includes an overview of the different types of simulators used in 5G&B networks, such as link-, system-, and network-level simulators, as well as the interplay between these simulators. In Section III, we discuss the key components of the 5G&B network and the impact of these components on the design requirements of the simulators. Section IV presents a taxonomy of the different types of simulators and their evaluation using traditional metrics. This section also includes a brief discussion of around 35 simulators that can be leveraged by both academia and industry for research, network planning, and optimization. Figure 5: Structure of the paper. We narrow down the discussion to DSMs consisting of system-level simulators and evaluate them in Section V. In this section, we present the new insightful metrics for simulator evaluation based on three factors: realism, completeness, and computational complexity. Section VI deals with the open challenges in the development of DSMs capable of functioning as DT kernels. Additionally, this section also presents several potential solutions to address the challenges. In Section VII, we extend the discussion of the challenges specific to 6G. Finally, Section VIII concludes the paper. To provide assistance to the readers, we provide a list of the acronyms used in this paper in Table III. Table III: List of Acronyms Acronym Description 3GPP 3rd Generation Partnership Project 4G Fourth Generation 5G Fifth Generation 5G&B Fifth Generation and Beyond 5GC 5G Core 5G PPP 5G Infrastructure Public Private Partnership AI Artificial Intelligence AMF Access and Mobility Management Function BLER Block Error Rate BS Base Station C-RAN Cloud Radio Access Network CAPEX Capital Expenditures CQI Channel Quality Indicator DSM Digital System Model EPC Evolved Packet Core FBMC Filter Bank Multicarrier FSPL Free Space Pathloss GIS Geographic Information Systems GW Gateway HARQ Hybrid Automatic Repeat Request HetNet Heterogeneous Network I2I Indoor–to-indoor InH Indoor-Hotspot IoT Internet of Things KPI Key Performance Indicator LOS Line-of-Sight LTE Long Term Evolution LUT Look-up Tables MAC Medium Access Control MANET Mobile Ad-hoc Network MCS Modulation and Coding Scheme ML Machine Learning MME Mobility Management Entity MU-MIMO Multi-user Multiple Input Multiple Output NFV Network Function Virtualization NLOS Non Line-of-Sight NR New Radio NSA Non-standalone O2I Outdoor–to-indoor O2O Outdoor–to-outdoor OFDM Orthogonal Frequency-division Multiplexing OPEX Operational Expenditures PDCP Packet Data Convergence Protocol PHY Physical Layer RAT Radio Access Technology RAN Radio Access Network RLC Radio Link Control RMa Rural Macro RRC Radio Resource Control RW Random Walk RWP Randon Waypoint SDN Software Defined Networking SINR Signal-to-Interference-plus-Noise Ratio SLAW Self-similar Least Action Walk SMa Suburban Macro SMF Session Management Function SA Standalone SUMO Simulation of Urban Mobility TIA Telecommunications Industry Association UAV Unmanned Aerial Vehicles UFe Urban Femto UMa Urban Macro UMi Urban Micro UPF User Plane Function VANET Vehicular Ad-hoc Network VoIP Voice Over IP WMN Wireless Mesh Network WSN Wireless Sensor Network ZF Zero Forcing"
https://arxiv.org/html/2411.09433v1,Conference Paper Title*,"This document is a model and instructions for LaTeX. This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, or Math in Paper Title or Abstract.",This document is a model and instructions for LaTeX. Please observe the conference page limits.
https://arxiv.org/html/2411.08918v1,Wireless Federated Learning over UAV-enabled Integrated Sensing and Communication,"This paper studies a new latency optimization problem in unmanned aerial vehicles (UAVs)-enabled federated learning (FL) with integrated sensing and communication. In this setup, distributed UAVs participate in model training using sensed data and collaborate with a base station (BS) serving as FL aggregator to build a global model. The objective is to minimize the FL system latency over UAV networks by jointly optimizing UAVs’ trajectory and resource allocation of both UAVs and the BS. The formulated optimization problem is troublesome to solve due to its non-convexity. Hence, we develop a simple yet efficient iterative algorithm to find a high-quality approximate solution, by leveraging block coordinate descent and successive convex approximation techniques. Simulation results demonstrate the effectiveness of our proposed joint optimization strategy under practical parameter settings, saving the system latency up to 68.54% compared to benchmark schemes.","Unmanned aerial vehicles (UAVs), commonly referred to as drones, have been revolutionizing next-generation wireless networks with their versatile capabilities including line-of-sight (LoS) connections, 3D mobility, and flexibility. UAVs can function as airborne base stations (BSs) for delivering communication, computation, and caching services to overcome traditional infrastructure limitations, while can also serve as mobile users for tasks like remote sensing, delivery services, target tracking, and virtual reality support. More recently, UAVs have been integrated with machine learning (ML) to deliver intelligent services such as classification of aerial images captured by UAV’s cameras. To ensure data privacy during ML model training in UAV networks, federated learning (FL) has been recently employed where UAVs can train an ML model and only share the trained model updates to a cloud server without data exchange. Previous studies have mainly focused on implementing conventional FL architectures with UAVs, where UAVs typically serve as FL clients with pre-stored datasets or as aerial base stations responsible for model aggregation. More recent research, such as [1, 2], has integrated UAV deployment into the FL system, enabling UAVs to function as relays for efficient data collection from other devices or as aggregators with adaptive positioning to improve FL performance. Furthermore, much of the prior work in this domain has primarily concentrated on communication and/or computation aspects, assuming that the data for training is readily available without accounting for the data sensing process. This assumption overlooks a critical factor, as the sensing process competes with communication and computation for limited resources, which can significantly affect overall learning performance. In FL, sensing, computation, and communication are highly coupled and must be seamlessly integrated to fully unlock its potential. This has given rise to the increasingly prominent research field of integrated sensing and communication (ISAC) [3]. Several studies have explored FL-UAV, FL-ISAC networks. In [4], the authors investigated a UAV-FL system for image classification in area exploration scenarios, while [5] proposed UAV-empowered wireless power transfer to enable sustainable FL-based wireless networks. The works in [6], [7], [8] studied FL-ISAC frameworks where they focus on jointly optimizing sensing, communication, and computation resource allocation. Despite these research efforts, the problem of latency minimization in UAV-enabled FL system with ISAC remains under-explored. Given UAV’s limited computational power and battery capacity, optimizing the round-trip ML model training latency, such as ML model training and ML model communication, is crucial to achieve efficient and timely FL while preserving UAVs’ resources. Hence, this paper studies a new latency minimization problem for UAV-enabled FL with ISAC. The contributions of this paper are two-fold: (1) We formulate a new latency minimization problem for UAV-enabled FL with ISAC, by jointly optimizing resource allocation and trajectory of UAVs along with resource allocation of the BS (Section II); (2) This latency minimization problem is computationally challenging, thus we propose a new iterative optimization approach to obtain the optimal solutions by applying block coordinate descent (BCD) and successive convex approximation (SCA) techniques (Section III). Numerical simulations show the superior performance of our joint optimization method compared to baseline approaches (Section IV)."
https://arxiv.org/html/2411.08902v1,A Range-Free Node Localization Method for Anisotropic Wireless Sensor Networks with Sparse Anchors,"In sensor networks characterized by irregular layouts and poor connectivity, anisotropic properties can significantly reduce the distance estimation accuracy between nodes, consequently impairing the localization precision of unknown nodes. Since distance estimation is contingent upon the multi-hop paths between anchor node pairs, assigning differential weights based on the reliability of these paths could enhance localization accuracy. To address this, we introduce an adaptive weighted method, AW-MinMax, for range-free node localization. This method involves constructing a weighted mean nodes localization model, where each multi-hop path weight is inversely proportional to the number of hops. Despite the model’s inherent non-convexity and non-differentiability, it can be reformulated into an optimization model with convex objective functions and non-convex constraints through matrix transformations. To resolve these constraints, we employ a Sequential Convex Approximation (SCA) algorithm that utilizes first-order Taylor expansion for iterative refinement. Simulation results validate that our proposed algorithm substantially improves stability and accuracy in estimating range-free node locations.","Wireless Sensor Networks (WSNs) are typically composed of numerous distributed sensor nodes that continuously monitor, detect, and collect data across various scenarios, such as urban surveillance, post-disaster recovery, and smart home environments [1, 2, 3]. In these contexts, accurately determining the location of each node is crucial for effective data processing [4, 5]. Although the Global Positioning System (GPS) is the predominant technology for localization, each node equipped with GPS is often impractical due to the high costs and energy consumption [6]. Consequently, a growing body of research focuses on developing localization methods that do not rely on GPS. I-A Related Work Currently, localization schemes that do not require GPS support can be broadly classified into two categories: range-based and range-free [7]. Range-based localization algorithms, such as Received Signal Strength (RSS) [8], Time of Arrival (ToA) [9], and Angle of Arrival (AoA) [10], utilize distance measurement devices within nodes to first ascertain distances between nodes, and subsequently use these distances for localization [11, 12]. In contrast, range-free methods rely on network connectivity rather than direct distance measurements to localize nodes [13, 14, 15]. Since classical range-free methods work on the assumption that nodes are uniformly distribution [16], their performance may deteriorate in WSNs with anisotropic factors such as irregular radio propagation and physical obstructions. For example, by supposing nodes following Poisson distribution, the EHP model estimates inter-node distances firstly by selecting the path with the fewest hops and then employs those distance estimations to localize nodes [17]. When actual network nodes do not follow Poisson distribution, for example, in anisotropic WSNs, employing the EHP model may result in poor localization accuracy [18]. Although a two-dimensional hyperbola to model path distances can better accommodate anisotropic network characteristics [19], this approach typically uses the mean value of multiple propagation path lengths, leading to significant localization errors. X. Yan, et al. [20] attempted to mitigate this by using a weighted average of multiple propagation path lengths to approximate inter-node distances, which somewhat improves localization accuracy. However, the method still incurs significant cumulative errors as the number of hops increases. To address this, the DV-maxHop algorithm was introduced to limit the number of hops between nodes, thus reducing cumulative errors [21]. It is clear that these methods all employ the connectivity of the WSNs to localize nodes in scenarios where the WSNs structure is sparse—due to factors like a low density of anchor nodes or small communication radii—localization error of these methods are exacerbated with connectivity decreasing. Selecting anchor nodes strategically may be feasible to counteract the reduced accuracy caused by diminished connectivity. Although the AnSuper algorithm selects suitable anchor node pairs by supposing uniform node distribution, its accuracy declines when nodes are non-uniformly distributed [22]. While using the Geometric Dilution of Precision (GDOP) threshold can lessen the negative impacts on localization accuracy induced by the non-uniform distribution of anchor nodes, it does not address accuracy reductions due to obstacles [23]. The Reliable Anchor Pair Selection (RAPS) algorithm selects anchor node pairs based on a predefined average hop distance threshold to circumvent localization accuracy deducing induced by obstacles, achieving path distances that more closely resemble actual distances [24]. However, since it also utilizes multi-hop distances to approximate straight-line distances, the RAPS algorithm still inevitably generates cumulative errors. X. Liu, et al. [25] proposed a new algorithm with an objective function to minimize cumulative errors between unknown nodes and anchor nodes. However, in scenarios where multi-path distances between some anchor nodes are significantly greater than their real distances [26], such as when obstacles are present, Liu’s method may experience accuracy deterioration. I-B Contributions This paper addresses the inaccuracies arising from the disparity between the actual distances of anchor pairs and their measured distances, which are often influenced by the multi-hop path characteristics of the anchor pair in anisotropic WSNs. To enhance node localization accuracy, our approach involves assigning different weights to multi-hop paths based on their distance estimation accuracy. Building on this concept, we introduce a weighted mean nodes localization model and its corresponding solver. The primary contributions of our work include: I-B1 We observe that a greater hop count between the given anchor pair typically leads to larger errors in distance estimation. To counter this, we have developed a weighted mean nodes localization model where each multi-hop path is assigned a weight inversely proportional to its hop count, aiming to mitigate the localization accuracy loss caused by multi-hop paths with great hop count. I-B2 To handle the proposed model’s inherent non-convexity and non-differentiability, we utilize matrix transformations to reformulate the proposed model into an optimization problem with a convex objective function and non-convex constraints. I-B3 To address the non-convex constraints, we first apply a first-order Taylor expansion to tighten them. Subsequently, we introduce a Successive Convex Approximation (SCA) algorithm designed to solve the proposed model iteratively. This algorithm enhances the feasibility and accuracy of solving the non-convex optimization problem, progressively improving the model’s performance through iterations. I-C Paper Organization The remainder of this paper is structured as follows: Section II outlines the system architecture and discusses the localization challenges within irregular networks. Section III provides a detailed introduction to the proposed algorithm, including its pseudo-code, computational complexity, and additional relevant aspects. Section IV demonstrates the feasibility and effectiveness of the proposed algorithm through simulation experiments. Finally, Section V presents conclusions drawn from the research findings. Figure 1: WSNs model in underwater."
https://arxiv.org/html/2411.08896v1,"Demand-Aware Beam Hopping and Power Allocation
for Load Balancing in Digital Twin empowered LEO Satellite Networks","Low-Earth orbit (LEO) satellites utilizing beam hopping (BH) technology offer extensive coverage, low latency, high bandwidth, and significant flexibility. However, the uneven geographical distribution and temporal variability of ground traffic demands, combined with the high mobility of LEO satellites, present significant challenges for efficient beam resource utilization. Traditional BH methods based on GEO satellites fail to address issues such as satellite interference, overlapping coverage, and mobility. This paper explores a Digital Twin (DT)-based collaborative resource allocation network for multiple LEO satellites with overlapping coverage areas. A two-tier optimization problem, focusing on load balancing and cell service fairness, is proposed to maximize throughput and minimize inter-cell service delay. The DT layer optimizes the allocation of overlapping coverage cells by designing BH patterns for each satellite, while the LEO layer optimizes power allocation for each selected service cell. At the DT layer, an Actor-Critic network is deployed on each agent, with a global critic network in the cloud center. The A3C algorithm is employed to optimize the DT layer. Concurrently, the LEO layer optimization is performed using a Multi-Agent Reinforcement Learning algorithm, where each beam functions as an independent agent. The simulation results show that this method reduces satellite load disparity by about 72.5% and decreases the average delay to 12ms. Additionally, our approach outperforms other benchmarks in terms of throughput, ensuring a better alignment between offered and requested data.","Low Earth Orbit (LEO) satellite communication systems, which play a crucial role in 6G, have advanced quickly due to their wide coverage, low latency, high transmission capacity, and flexible resource scheduling [1, 2, 3, 4]. To provide broadband transmission and seamless coverage, LEO constellations must ensure multiple coverage, where some areas are served by multiple satellites. However, the uneven geographic distribution of ground users and the high mobility of LEO satellites lead to temporal and spatial variability in traffic demands within a satellite’s coverage area. Beam Hopping (BH) technology, based on time slicing, achieves the traditional multi-beam coverage with fewer beams. This makes LEO satellites equipped with BH antennas highly flexible, which is crucial for addressing the varying demands across different regions [5]. Nonetheless, if beams are powered uniformly, the transmission capacity will still fall short of meeting the uneven traffic demands across different beams [6, 7, 8]. Additionally, onboard power in LEO satellites is extremely limited [9], making the intelligent and real-time joint BH and power allocation (PA) a critical research focus in LEO satellite communication systems. Nevertheless, there are still some critical issues which are closely related to practical implementations and have not been well addressed in effectively scheduling beams and power resources in LEO satellite constellations [6, 8]. First, the constantly changing topology of LEO satellite networks leads to fluctuations in inter-beam interference and satellite-to-ground link conditions, demanding adaptive rather than static resource management [10, 11]. Second, managing resource scheduling is key to reducing beam interference, both within and between satellites. Third, joint optimization of beam and power resources expands the system’s state-action space, exacerbating the ”curse of dimensionality.” Additionally, this joint optimization creates a discrete-continuous hybrid action space, where beam allocation is discrete, and PA is continuous [12]. Approximating continuous actions using finite discrete sets diminishes the natural structure of continuous actions, while relaxing discrete actions into continuous sets significantly complicates the action space. Finally, scheduling beams and power resources must not be viewed as a simple single-objective optimization, but should concurrently consider multiple performance metrics, such as network throughput, latency, and fairness. In reviewing the literature, it is evident that most studies concentrated on flexible radio resource allocation with a fixed beam direction, applicable to either a single satellite [13, 14, 6, 15] or multiple satellites [16, 17, 18, 19]. Some studies (e.g., [20, 21]) primarily utilize BH technology to address the uneven geographical distribution of users. However, these studies overlook the importance of adaptive beam resource allocation, opting instead for a static allocation strategy, such as equal distribution across beams. Although certain works [22, 23, 24] are dedicated to joint optimization of PA and beam direction, they are limited to single-satellite scenarios. In summary, existing research cannot be directly applied to multi-beam, multi-LEO satellite networks to address the aforementioned key issues. Furthermore, addressing all of these issues simultaneously presents new challenges for the dynamic resource management in LEO satellite networks due to the following aspects. a. Inter-beam interference is unavoidable when multi-beam LEO satellites transmit downlink data through BH. This interference includes both intra-satellite and inter-satellite interference, leading to a strong interdependence between resource allocation decisions across satellites. Therefore, to fully exploit the spectrum reuse benefits in multi-beam LEO satellite communications, fostering cooperation among satellites is crucial to mitigate interference and enhance system capacity. However, the limited computational capacity and energy resources of satellites significantly increase the difficulty of satellite cooperation. b. The resource allocation decisions (including BH and PA) should be jointly optimized to maximize overall network performance. Since BH and PA are closely intertwined, their joint optimization is typically NP-hard, making it difficult to achieve an optimal solution within polynomial time. c. With the constant movement of LEO satellites, the connections between satellites and ground terminals frequently change. Consequently, resource allocation decisions must be adaptable to this dynamic environment, further increasing the complexity of resource management. To address these complex challenges, we note that Digital Twin (DT) technology provides an opportunity to represent the real world in a virtualized manner and has emerged as a promising tool for guiding resource deployment and scheduling in the real world. As an effective method for bridging the gap between physical entities and the digital realm, DT has garnered significant attention from both industry and academia [25]. In industries such as manufacturing, power grids, and transportation management, DT has proven its ability to enhance operational efficiency and reduce resource waste through precise system modeling and predictive capabilities [26]. In multi-LEO satellite communication systems, introducing DT can provide a global perspective for LEO satellite systems. By virtually replicating and monitoring the status and resource dynamics of physical satellite networks, DT can fully capture the real-time status changes of satellites, ground users, and the environment [27]. This global perspective can support the overall optimization of BH and PA, not only helping to coordinate resource allocation for multiple satellites, but also effectively dealing with interference management and load balancing issues between satellites. Motivated by these considerations, we design a DT-empowered multi-beam, multi-satellite cooperative service network and formulate a resource management problem for the downlink scenario that integrates both BH and PA. Our objective is to enhance the fairness of cell service while simultaneously improving throughput. To address this esource management problem, we decompose the original problem into two sub-problems: demand prediction-based BH at the DT layer and beam PA at the LEO satellite layer. We propose two advanced reinforcement learning algorithms to solve these sub-problems. The key contributions of this paper are summarized as follows: • We propose a DT-empowered resource allocation framework for LEO satellite networks, addressing the joint BH and PA problem in a dynamic multi-beam, multi-satellite downlink scenario. The objective is to maximize throughput while ensuring fairness across cells. Given the mixed-integer, non-convex nature of the optimization problem, we decompose it into two simpler sub-problems: a DT layer multi-LEO satellite cooperative BH problem focused on load balancing, and a multi-beam power resource allocation problem at the LEO layer. • For optimization at the DT layer, the cloud predicts future time slot requirements based on historical demand data, considering the overlapping coverage of LEO satellite systems to design a BH scheme that balances communication demands among satellites. We employ the A3C algorithm, deploying an actor-critic network on each satellite and a global critic network in the cloud to improve training efficiency and stability. • To optimize resource allocation at the LEO layer, each satellite beam is modeled as an agent, and an on-demand beam PA problem is established. The Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm is utilized to facilitate cooperation and competition among these agents, further enhancing cell service fairness and system throughput. • Extensive simulation results demonstrate that the proposed joint BH and PA algorithm significantly outperforms baseline algorithms in terms of throughput, load balancing, and cell fairness. The rest of this paper is organized as follows. Section II reviews the related literature. Section III describes the system model and presents the joint BH and power resource allocation optimization problem for multiple LEO satellites. Section IV decomposes the optimization problem into two sub-problems and provides a detailed explanation of the solution methods for these sub-problems. Section V presents the simulation results, demonstrating the superior performance of the proposed method. Section VI presents a summary of this paper and discusses future research prospects."
https://arxiv.org/html/2411.08767v1,"SANDWICH: Towards an Offline, Differentiable, Fully-Trainable Wireless Neural Ray-Tracing Surrogate","Wireless ray-tracing (RT) is emerging as a key tool for three-dimensional (3D) wireless channel modeling, driven by advances in graphical rendering. Current approaches struggle to accurately model beyond 5G (B5G) network signaling, which often operates at higher frequencies and is more susceptible to environmental conditions and changes. Existing online learning solutions require real-time environmental supervision during training, which is both costly and incompatible with GPU-based processing. In response, we propose a novel approach that redefines ray trajectory generation as a sequential decision-making problem, leveraging generative models to jointly learn the optical, physical, and signal properties within each designated environment. Our work introduces the Scene-Aware Neural Decision Wireless Channel Raytracing Hierarchy (SANDWICH), an innovative offline, fully differentiable approach that can be trained entirely on GPUs. SANDWICH offers superior performance compared to existing online learning methods, outperforms the baseline by 4e^{-2} \unit in RT accuracy and only fades 0.5 \unit away from toplined channel gain estimation.","Wireless ray-tracing (RT) approaches model interactions between electromagnetic (EM) waves and their physical environment. Recent years have seen increased interest in wireless RT due to its potential to capture complex signal behaviors, especially in B5G and 6G scenarios [amiot2013pylayers, choi2023withray, orekondy2023winert, zhao2023nerf2, hoydis2023sionna, he2018design, lai2024real, zhang2024wisegrt, tarneberg2023towards]. Traditionally, RT relies on computationally intensive Maxwell equations (ME) or specialized wireless raytracers, where complex solvers [rtem] are introduced by integrating heuristics from geometrical optics (GO) and uniform theory of diffraction (UTD). Thanks to computational EM theory [18706], one can use shooting and bouncing rays sequence (SBS) while being GO/UTD-compliant to approximate EM waves in high frequency. Wireless RT Surrogate. Recently research has been exploring learning-based methods as RT surrogates [orekondy2023winert], aiming to reduce the computational costs of traditional wireless ray tracers and improve their scalability. In this context, Wireless RT is typically formulated as a sparsely-supervised and non-differentiable objective. Since the transmitter (Tx) emits a large volume of GO/UTD-feasible rays, only a small, countable subset of these rays are relevant for wireless reception at the receiver (Rx). Figure 1: Schematic Representation of SANDWICH: 1, Segment wireless RT and 3D environment. 2, Sequentialize RT into SBS. 3, Train neural surrogate & apply auto-regressive generation. 4, Apply generated RT to channel model. Several approaches have been proposed in the literature to address such challenges. For example, \citetorekondy2023winert mitigate these issues through online, continuous feedback mechanisms, such as temporal difference (TD) learning. Alternatively, other approaches, such as those in [hoydis2023sionna] and [choi2023withray], employ monte carlo (MC) methods to collect and refine a diverse set of ray trajectories. Nevertheless, existing methods are hindered by several significant limitations: 1. Dependence on real-time feedback from radio environment modeling or RT engines during training, 2. Incompatibility with GPU-based training, and 3. Lack of support for vectorization, thus limiting the ability to process batches of rays efficiently. Proposed Method. In this paper, we address wireless RT in indoor environments, considering Tx/Rx locations, signal-surface interaction, and surface texture information. We propose scene-aware neural decision wireless channel raytracing hierarchy (SANDWICH): a novel transformer-based offline learning method to perform wireless RT. SANDWICH addresses the aforementioned limitations by allowing learning from a limited set of collected RT results, without assumptions on the radio environment or supervision density. Additionally, our goal is to develop efficient and scalable wireless RT surrogates that eliminate the dependency on online supervision, and enable a vectorized, GPU-trainable neural wireless RT surrogate. To achieve our objectives, and inspired by the success of generative pre-trained transformers (GPTs), we suggest that wireless RT can be modeled in a sequence modeling space, where SBS in wireless ray can be generated similarly to token sequences, utilizing the transformer’s [vaswani2017attention] capability of learning wireless RT patterns in such sequence. We house the sequence auto-regressive generation task within an offline reinforcement learning (RL) scheme, enforcing the model to internalize the radio environment as a constraint during SBS generation to unhook the requirement for online supervision from well-parameterized radio environment in state-of-the-arts (SOTA). We illustrate a schematic representation of the proposed solution in Fig. 1, including 4 major steps: 1. Segment raw data into 3D environment and channel information. 2. Create tailored gym environment for test-time verification with OOD Tx & Rx locations. The ray’s trajectory is also transformed into true & augmented SBS for transformer models. 3. Train the proposed model and generate SBS on novel (Tx, Rx) distribution. 4. Besides the generated wireless RT geometrical accuracy, the generated SBS are utilized as priors for channel model to enhance performance in downstream tasks within a Turbo Learning [zhao2023nerf2] framework. Our method leverages a customized decision transformer (DT) [chen2021decision] to integrate sparse supervision through expected returns in a markov decision process (MDP), thereby decoupling the need for continuous online supervision from the environment. Additionally, we propose a data augmentation technique based on Fibonacci sphere [rogne2022raytracing] to generate stochastic trajectories that enhance DT’s generalization on out-of-distribution (OOD) test samples, alongside state supervision to enforce environment awareness. Our Contribution. We evaluate our approach using metrics on wireless ray accuracy and validating its effectiveness as a prior for channel estimation tasks. SANDWICH outperforms the online learning solution by 4e^{-2} \unit in RT accuracy and only fades 0.5 \unit away from GT-ray-powered channel gain estimation, while outperforming all non-RT based baselines with generated wireless RT results. Our method is end-to-end GPU-trainable, fully differentiable, and vectorizable."
https://arxiv.org/html/2411.08672v1,Joint Model Caching and Resource Allocation in Generative AI-Enabled Wireless Edge Networks,"With the rapid advancement of artificial intelligence (AI), generative AI (GenAI) has emerged as a transformative tool, enabling customized and personalized AI-generated content (AIGC) services. However, GenAI models with billions of parameters require substantial memory capacity and computational power for deployment and execution, presenting significant challenges to resource-limited edge networks. In this paper, we address the joint model caching and resource allocation problem in GenAI-enabled wireless edge networks. Our objective is to balance the trade-off between delivering high-quality AIGC and minimizing the delay in AIGC service provisioning. To tackle this problem, we employ a deep deterministic policy gradient (DDPG)-based reinforcement learning approach, capable of efficiently determining optimal model caching and resource allocation decisions for AIGC services in response to user mobility and time-varying channel conditions. Numerical results demonstrate that DDPG achieves a higher model hit ratio and provides superior-quality, lower-latency AIGC services compared to other benchmark solutions.","I-A Background and Overview Recent advancements in generative artificial intelligence (GenAI) have revolutionized the capabilities of traditional AI, which primarily focuses on discriminative tasks, pattern recognition, and decision-making [1]. Technically, GenAI excels at creating a diverse array of AI-generated content (AIGC), such as images, music, and natural language, based on human-provided instructions by learning the underlying patterns and distributions of training data. For instance, ChatGPT111https://openai.com/index/hello-gpt-4o/ is a large language model that can effectively understand and generate human-like text, responding interactively to given prompts. Additionally, DALL\cdotE-3222https://openai.com/index/dall-e-3/ is an image generation model capable of creating high-quality images from textual descriptions. The remarkable achievements in GenAI have greatly enhanced productivity across various domains, such as art, advertising, and education. Nowadays, benefiting from the growth of data and model size, GenAI can capture more comprehensive distributions that closely align with reality, resulting in more realistic and high-quality AIGC. However, these large foundation model architectures necessitate substantial storage capacity for caching and extensive computational resources for inference, placing significant pressure on practical deployment. For example, GPT-3 has 175 billion parameters and requires 8 × 48GB A6000 GPUs to perform inference for generating responses [2]. These challenges are further exacerbated in the emerging Metaverse paradigm, which demands continuous AIGC from resource-limited personal devices. Consequently, the growing footprint and complexity of GenAI models, along with extensive resources required for deployment, extremely limit the accessibility of AIGC service provision. Fortunately, cloud data centers with powerful computing resources, such as Amazon Lightsail333https://aws.amazon.com/lightsail/ and Alibaba Cloud444https://www.alibabacloud.com/, are capable of offering cloud-assisted AIGC services to users through the core network. However, migrating AIGC services to the cloud not only leads to prohibitive traffic congestion on backhaul links [3] but also raises concerns about privacy exposure over public cloud infrastructure. As an alternative, deploying GenAI models at the wireless edge (i.e., on edge servers co-located with base stations or roadside units) has proven to be a promising solution. First, due to the physical proximity between edge servers and users, edge-assisted AIGC service provisioning significantly reduces end-to-end latency. Second, keeping sensitive data local rather than transmitting it to cloud data centers enhances privacy protection, addressing growing concerns around data security. I-B Motivation and Main Challenges Although provisioning AIGC services at wireless edge networks offers several advantages, there remain significant issues to address. First, unlike traditional content caching, caching GenAI models requires the careful coordination of both storage and computing resources. Researchers in [4] investigated a content caching and user association problem in heterogeneous cellular networks, proposing a cubic exponential smoothing algorithm to determine the content cached in base stations (BSs). Researchers in [5] proposed a freshness-aware content refreshing scheme for mobile edge caching systems, where the BS refreshes cached content items based on the age of information (AoI) upon user requests. However, compared to content caching, GenAI model caching requires substantial computing capabilities for inference, complicating resource allocation schemes as edge servers must balance both storage and computational resources – where larger models offer better performance but demand more resources to run. Second, conventional optimization methods are limited by both high computational complexity and the need for complete information. Researchers in [6] proposed a holistic strategy for joint task offloading and resource allocation in mobile edge computing networks, where the original problem was decomposed into an RA sub-problem and a TO sub-problem, each solved using quasi-convex and convex optimization techniques, respectively. Researchers in [7] investigated a resource allocation problem for latency-aware federated learning in industrial Internet of Things systems, proposing a heuristic algorithm to select appropriate devices that balance the trade-off between interference and convergence time. However, convex optimization methods require extensive computations, making it difficult to meet real-time decision-making requirements. Additionally, conventional methods rely on precise knowledge of the system state to formulate and solve the optimization problem, which is often unavailable in practical networks. I-C Summary of Contributions Motivated by these challenges, this paper investigates the problem of joint GenAI model caching and resource allocation in wireless edge networks, with the goal of delivering low-latency, high-quality AIGC services to diverse users. The contributions of this paper are summarized as follows: • We formulate the joint model caching and resource allocation problem in GenAI-enabled wireless edge networks as a mixed-integer nonlinear programming (MINLP) problem, which is known to be NP-hard and particularly challenging to solve. • To address this problem, we employ a deep deterministic policy gradient (DDPG)-based reinforcement learning approach. After training, DDPG can efficiently map the problem’s state space (e.g., user mobility and varying AIGC requests) to optimal model caching and resource allocation decisions for AIGC service provisioning. • We assess the effectiveness of our approach through experiments across diverse simulation settings, showing that DDPG achieves a superior model hit ratio and provides higher-quality, lower-latency AIGC services in comparison to benchmark methods."
https://arxiv.org/html/2411.08341v1,"Generative AI for Data Augmentation in Wireless Networks: Analysis, Applications, and Case Study","Data augmentation is a powerful technique to mitigate data scarcity. However, owing to fundamental differences in wireless data structures, traditional data augmentation techniques may not be suitable for wireless data. Fortunately, Generative Artificial Intelligence (GenAI) can be an effective alternative to wireless data augmentation due to its excellent data generation capability. This article systemically explores the potential and effectiveness of GenAI-driven data augmentation in wireless networks. We first briefly review data augmentation techniques, discuss their limitations in wireless networks, and introduce generative data augmentation, including reviewing GenAI models and their applications in data augmentation. We then explore the application prospects of GenAI-driven data augmentation in wireless networks from the physical, network, and application layers, which provides a GenAI-driven data augmentation architecture for each application. Subsequently, we propose a general generative diffusion model-based data augmentation framework for Wi-Fi gesture recognition, which uses transformer-based diffusion models to generate high-quality channel state information data. Furthermore, we develop residual neural network models for Wi-Fi gesture recognition to evaluate the role of augmented data and conduct a case study based on a real dataset. Simulation results demonstrate the effectiveness of the proposed framework. Finally, we discuss research directions for generative data augmentation.","As an indispensable and fundamental technology, wireless networks enable users to access the Internet, connect devices, and communicate wirelessly. The growing complexity and diversity of wireless networks have facilitated the development of Deep Learning (DL)-based wireless communications and networking[1]. Since wireless networks generate large amounts of data from user activities, channel states, and network conditions, DL can efficiently distill high-level features and information from these data, which have complex structures and inner correlations[1], and enable intelligent applications such as Digital Twins (DTs), autonomous driving, and metaverse. Nevertheless, the availability of vast amounts of high-quality wireless data is a major determinant of the effectiveness of DL models[2]. Specifically, • Dynamic wireless channels: Due to multipath fading, shadowing, and interference, the environment in wireless channels is highly variable[3], which makes it difficult to collect high-quality wireless data. • Limited measuring equipment: High-quality wireless data collection often requires extensive deployment of specialized hardware and software equipment[3], which is costly and time-consuming. Therefore, obtaining high-quality and diverse wireless data for the progress of DL-based wireless communications and networking is challenging[2, 3]. Data augmentation is an effective technique to solve the problem of limited labeled datasets in model training[4]. The core idea of data augmentation is to synthetically increase the size of training datasets by modifying existing data, thus enhancing the robustness and generalization of learning algorithms[4]. Data augmentation techniques have been extensively applied in fundamental domains. For example, in Computer Vision (CV), data augmentation methods apply geometric or color transformations, such as cropping, flipping, and color channel changes[5]. However, traditional data augmentation methods may not be applicable to wireless data. Specifically, traditional data augmentation methods are carefully designed for certain domains and only perform simple transformations on the original datasets, rather than enriching data features from existing data[4]. Most importantly, traditional data augmentation methods lack consideration of the inherent characteristics and structures of wireless data[5], which cannot guarantee the quality and diversity of augmented wireless data. Hence, novel techniques for effective wireless data augmentation are urgently needed. Generative Artificial Intelligence (GenAI) is obtaining the full spotlight after the release of Large Language Model (LLM)-based chatbots by tech giants such as OpenAI’s ChatGPT, Google’s Bard, and Microsoft’s Bing Chat[6]. Unlike discriminative AI models that focus primarily on explicitly learning decision boundaries between classes, GenAI models excel at learning the underlying distribution, patterns, and features of input data, thus generating new data instances that resemble the original dataset[7]. Due to its transformative power in data generation, GenAI has recently gained significant attention in the realm of wireless networks[6], where real-world wireless data is often scarce, incomplete, costly to acquire, and difficult to model or comprehend[2], enabling the emergence of GenAI-driven data augmentation. Specifically, GenAI models can generate high-quality and diverse wireless data as a supplement, and the synthetic data can be combined with real data to augment the training dataset[6], which solves the wireless data acquisition challenge and can effectively improve the performance of DL models in wireless communication and networks. There are currently some preliminary studies on using GenAI models for data augmentation in wireless networks[2, 4, 8, 3]. For instance, the authors in [2] utilized conditional latent diffusion models to generate high-diversity and high-quality Radio Frequency (RF) data at low costs. In [3], the authors leveraged a Denoising Diffusion Probabilistic Model (DDPM) to generate channel data in multiple speed scenarios, where the DDPM can capture the underlying distribution of wireless channel data with limited data volume. Finally, a CsiNet model was used to validate the effectiveness of the proposed approach. However, the above works do not provide a general tutorial on how to implement data augmentation using GenAI technology in wireless networks. To this end, this paper focuses on answering two questions, i.e., “Why GenAI is conducive to data augmentation in wireless networks” and “How to use GenAI techniques to achieve wireless data augmentation.” To the best of our knowledge, this is the first work that systemically explores the potential and effectiveness of generative data augmentation in wireless networks. Our main contributions are summarized as follows: • We begin with a brief review of data augmentation techniques in the basic domains of images, text, and graphics, then discuss potential advantages brought by effective data augmentation techniques in wireless networks, and finally present the limitations of traditional data augmentation techniques in wireless networks. • We review typical GenAI models and their applications in data augmentation, and summarize the benefits of generative data augmentation. We then explore the effectiveness of GenAI-driven data augmentation for wireless applications from the physical, network, and application layers, which presents a specific GenAI-driven data augmentation architecture for each application. • We propose a general Generative Diffusion Model (GDM)-based data augmentation framework for Wi-Fi gesture recognition, where we use transformer-based diffusion models to generate high-quality and diverse Channel State Information (CSI) data. We finally conduct a case study based on a real dataset, where we develop Residual neural Network (ResNet) models to evaluate the performance gains achieved by augmented CSI data. Simulation results demonstrate the effectiveness of the proposed framework. TABLE I: A Summary of Traditional Non-AI and GenAI Methods for Typical Data Augmentation."
https://arxiv.org/html/2411.08300v1,EDM: An Ultra-Low Latency Ethernet Fabric for Memory Disaggregation,"Achieving low remote memory access latency remains the biggest challenge in realizing memory disaggregation over Ethernet inside the datacenter. We present EDM that tries to overcome this challenge using two key ideas. First, while the existing network protocols for remote memory access over Ethernet, such as TCP/IP and RDMA, are implemented on top of Ethernet’s MAC layer, EDM takes a rather radical approach of implementing the entire network protocol stack for remote memory access within the Physical layer (PHY) of the Ethernet. This overcomes fundamental latency and bandwidth overheads imposed by the MAC layer, especially for small memory messages. Second, EDM implements a centralized, fast, in-network traffic scheduler for memory traffic within the PHY of the Ethernet switch. Inspired by the classic Parallel Iterative Matching (PIM) algorithm, the scheduler dynamically reserves bandwidth between compute and memory nodes by creating virtual circuits in the switch’s PHY, thus eliminating the queuing delay and layer 2 packet processing delay at the switch for memory traffic, with high bandwidth utilization. Our FPGA testbed shows that EDM’s network fabric incurs a latency of only \sim300 ns for remote memory access in an unloaded network, which is an order of magnitude lower than state-of-the-art Ethernet-based solutions such as RoCEv2 and comparable to the emerging PCIe-based solutions such as CXL. Larger-scale network simulations show that even at high network loads, EDM’s latency is within 1.3\times its unloaded latency.","Memory disaggregation is a computing architecture where compute and memory are physically separate blades of resources connected via a network fabric such as the Ethernet. Memory disaggregation promises high compute density, fine-grained memory pooling and provisioning, and elastic memory scaling. Hence, not surprisingly, memory disaggregation has garnered significant interest in recent years, both in industry (hp-the-machine, ; intelrackscale, ; huawei, ) and in academia (osdi16:disaggregated, ; shoal, ; legos, ; mind, ). As the Ethernet link bandwidth reaches 400 Gbps, with Tbps Ethernet on the horizon (terabit-eth, ), prior works (osdi16:disaggregated, ; mind, ) have argued that the Ethernet bandwidth inside datacenters is already plentiful to carry both the memory and traditional IP and storage traffic with minimal bandwidth contention. However, despite the promise of plentiful bandwidth, remote memory access latency remains the key bottleneck in realizing memory disaggregation over Ethernet. As a result, there have been several proposals for alternate fabrics (cxl, ; novakovic14sonuma, ; infiniband, ) for memory disaggregation inside datacenters, most recently, PCIe-based CXL.mem fabric (cxl, ), that promise ultra-low latency for remote memory access. However, such fabrics scale poorly in terms of bandwidth, distance, cost and power compared to Ethernet (§ 2.2). Furthermore, even their latency scales poorly with network load (§ 4.3) due to ineffective mechanisms to handle fabric congestion (aurelia, ). Hence, an ultra-low latency Ethernet fabric, if realized in practice, has the potential to enable scalable, high bandwidth and low latency memory disaggregation at low cost and power. Using Ethernet as the interconnect between the processor and the memory introduces two sources of latency for memory access (Figure 1)—(i) the processor/memory to NIC interconnect latency, and (ii) the Ethernet fabric latency, which includes the latency of the host network protocol stack for remote memory access over the Ethernet (e.g., TCP/IP, RDMA) and the latency of the Ethernet switching network. The traditional PCIe interconnect between the NIC and processor/memory can have latency of the order of \mus (pcie_perf, ). However, in the past several years, various new interconnect specifications (cxl, ; ccix, ; nvlink, ; opencapi, ; ucie, ) have been proposed that either replace or build upon the PCIe physical layer, to dramatically reduce the processor/memory to NIC latency. Most notably, PCIe-based Compute Express Link (CXL.io) (cxl, ) point-to-point peripheral interconnect latency can be of the order of 100 ns (li2023pond, ). The demand for lower latency for cloud services has also prompted a tighter integration of processor and memory with the network controller, that promises to reduce the processor/memory to NIC latency to sub-100 ns. Such designs exist both in academia (e.g., nanoPU (nanopu, ), soNUMA (novakovic14sonuma, ), FAME-1 RISC-V RocketChip SoC (firesim, )) and industry (e.g., Intel Omni-Path (omnipath, )). Further, cloud providers are increasingly offloading processing from CPUs to accelerators, such as FPGAs, for cost efficiency and higher performance (microsoft-fpga, ; gft, ; disagg-nf, ). Such accelerators (stratix10, ; stratixv, ; netfpgasume, ) have processor, memory, and network controller integrated on the same motherboard, thus reducing the processor/memory to NIC latency to as few as 10s of nanoseconds (§ 4.1). Figure 1. A generic architecture for Ethernet-based memory disaggregation. In light of the above (and ongoing (cc-nic, )) architectural and system optimizations to reduce the processor/memory to NIC latency, the Ethernet fabric becomes the primary source of latency for remote memory access. Intra-server (or local) memory access latency typically varies from few 10s of nanoseconds to few 100s of nanoseconds (dram-latency-2, ; dram-latency-1, ), depending upon factors such as memory access pattern and memory location relative to the processor. However, in existing Ethernet-based networks, even with just a single Ethernet switch and highly optimized host network protocol stacks for remote memory access (e.g., RDMA over Converged Ethernet (RoCEv2) (rocev2, )), the Ethernet fabric latency can be as high as few \mus (osdi16:disaggregated, ) even in an unloaded network, which is already an order of magnitude higher than the local memory access latency. And at higher network loads, this baseline latency may easily increase by another order of magnitude due to network queuing and congestion (shoal, ). Given this, we present EDM (Ethernet Disaggregated Memory), which is an ultra-low latency Ethernet fabric for memory disaggregation inside datacenters. Akin to prior works (mind, ), EDM targets a rack or cluster scale memory disaggregation, with a single Ethernet switch connecting hundreds of compute and memory blades. To achieve ultra-low latency with high bandwidth utilization, EDM uses two key design ideas, as discussed below. First, we note that existing Ethernet-based network protocol stacks for remote memory access, such as TCP/IP and RDMA, are implemented on top of Ethernet’s Media Access Control (MAC) layer. Unfortunately, MAC layer imposes fundamental latency and bandwidth overheads for small memory messages due to its operation at frame granularity. In particular, MAC layer imposes a minimum frame size of 64 B and an Inter-frame gap (IFG) of at least 12 bytes. This results in high bandwidth overhead for small memory messages, such as a remote memory read request that only contain the control information for reading from remote memory, e.g., a 64-bit (8 B) remote memory address (§ 2.4). Further, MAC layer does not allow inter-frame preemption. As a result, a small, latency-sensitive memory message cannot preempt the transmission of a large Ethernet frame carrying, e.g., IP data, resulting in significant latency overhead (§ 2.4). To overcome these challenges, EDM employs a rather radical approach of completely bypassing the MAC layer and implementing the entire network protocol stack for remote memory access inside the Physical Coding Sublayer (PCS) of Ethernet’s Physical layer (PHY) at both the host (§ 3.2.1) and the switch (§ 3.2.2). The key insight is that PCS operates at the granularity of 66-bit blocks, that can be leveraged to both reduce the bandwidth overhead for small memory messages as well as enable very fine-grained multiplexing of memory messages and non-memory Ethernet frames (e.g., IP) for lower latency. Further, IFG is accessible inside PCS, which one could repurpose to carry memory messages. EDM’s network stack for remote memory access runs in parallel with the traditional Ethernet network stack for IP and storage traffic. The applications (on the compute node) and the memory controller (on the memory node) sending/receiving memory messages interface directly with EDM in the PHY. EDM encodes/decodes the memory messages to/from a series of 66-bit PHY blocks, all the while ensuring compliance with the Ethernet PHY standard. Furthermore, EDM is the first system to enable intra-frame preemption (§ 3.2.3), by multiplexing the transmission of memory messages and non-memory Ethernet frames at the granularity of 66-bit PHY blocks, while ensuring the higher sublayers in PCS at the receiver still receive the PHY blocks of an Ethernet frame in contiguous clock cycles as required by the Ethernet standard. The second key idea in EDM is to implement a centralized traffic scheduler for memory traffic in the Ethernet switch’s PHY (§ 3.1.1). The scheduler takes as input the current memory traffic demand matrix and implements the classic Parallel Iterative Matching (PIM) (pim, ) to dynamically reserve bandwidth between compute and memory nodes by creating virtual circuits in the switch’s PHY. This proactively ensures no queuing and layer 2 packet processing delay at the switch for memory traffic, while guaranteeing high bandwidth utilization. Furthermore, to achieve near-optimal latency under bandwidth contention, EDM augments PIM with priority-based scheduling, such as SRPT (srtf, ). Implementing EDM’s scheduler in practice requires overcoming two key challenges. First, acquiring the traffic demand matrix for memory traffic accurately and with low overhead. For this, EDM leverages the unique request-reply nature of remote memory reads, where the read request, containing the remote memory address the number of bytes to be read (as required by the memory controller interface, e.g., DDR4), implicitly provides a demand estimation for the corresponding read reply message. Further, by implementing the scheduler in the switch, EDM is able to intercept the read request messages inline and extract the demands without incurring any extra bandwidth or latency overhead. For remote writes, however, EDM does incur bandwidth and latency overhead of sending explicit demand messages to the switch before sending the actual write data. However, EDM manages to keep the bandwidth overhead to minimal through techniques such as batching, and the latency overhead of RTT/2 is nominal at rack or cluster scale. The second challenge is to design a scheduler that can schedule at line rate with small scheduling latency. In a naive implementation, each iteration of the priority-based PIM would take O(log(n)) clock cycles per destination port (in parallel), where n is the number of demand messages per destination port. In contrast, EDM is able to compute this in constant number of clock cycles, owing to EDM’s novel hardware design (§ 3.1.2), that intelligently trades-off hardware resource for time, by using a combination of constant time ordered list data structure that allows for parallel reads, comparisons, and writes, and a fast priority encoder. We implement EDM’s design on FPGAs by extending the PHY of standard 25G Ethernet (§ 4.1). Using a small network testbed of EDM-capable FPGA-based switch and NICs, we show that EDM only incurs a latency of \sim300 ns in an unloaded network, for both remote memory reads and writes (§ 4.2). The read (write) latency is 3.7\times (1.9\times), 6.8\times (3.4\times), and 12.6\times (6.4\times) lower than the latency of raw Ethernet (standard Ethernet MAC + PHY only), RDMA over Converged Ethernet (RoCEv2), and hardware offloaded TCP/IP network stacks respectively. Furthermore, EDM’s latency is comparable to both an intra-server two hop NUMA (dram-latency-1, ) as well as PCIe-based CXL fabric with a single switch hop (li2023pond, ) (while being more scalable and cost efficient than CXL (§ 2.2)). Using larger-scale network simulations over a wide variety of disaggregated network traffic traces from real applications, we show that even at high network loads, EDM’s average latency is within 1.3\times its unloaded latency, and over 7\times lower than CXL (§ 4.3), whose underlying flow control mechanism fails to handle fabric congestion effectively (aurelia, )."
https://arxiv.org/html/2411.07791v1,PB02: A classification model for sentiment analysis of tweets,The abstract goes here.,"This demo file is intended to serve as a “starter file” for IEEE conference papers produced under LaTeX using IEEEtran.cls version 1.8b and later. I wish you the best of success. mds August 26, 2015 I-A Subsection Heading Here Subsection text here. I-A1 Subsubsection Heading Here Subsubsection text here."
https://arxiv.org/html/2411.07485v1,Decentralized Network Topology Design for Task Offloading in Mobile Edge Computing,"The rise of delay-sensitive yet computing-intensive Internet of Things (IoT) applications poses challenges due to the limited processing power of IoT devices. Mobile Edge Computing (MEC) offers a promising solution to address these challenges by placing computing servers close to end users. Despite extensive research on MEC, optimizing network topology to improve computational efficiency remains underexplored. Recognizing the critical role of network topology, we introduce a novel decentralized network topology design strategy for task offloading (DNTD-TO) that jointly considers topology design and task allocation. Inspired by communication and sensor networks, DNTD-TO efficiently constructs three-layered network structures for task offloading and generates optimal task allocations for these structures. Comparisons with existing topology design methods demonstrate the promising performance of our approach.","With the advancement in Internet of Things (IoT) technology, many delay-sensitive yet computing-intensive applications have emerged, such as automotive driving, face recognition and virtual reality[1]. However, IoT devices typically have limited computing power, making it difficult to meet the demands of these tasks. Centralized cloud computing is traditionally used to process these tasks, but offloading them to the remote cloud can cause significant transmission delays that degrade user Quality of service (QoS). To address this issue, Mobile Edge Computing (MEC) solutions were introduced[2], where MEC places servers closer to end users at the network edge, such as at base stations, to reduce transmission delays. To better serve end users, extensive research has been conducted in the field of MEC, addressing various design aspects such as system deployment, task offloading, resource allocation, mobility management, and privacy and security [3]. Nevertheless, little attention has been given to optimizing network topology to enhance computational efficiency. Most existing studies primarily focus on offloading tasks from users to one or more nearby MEC servers within communication range [4, 5]. A few studies [6, 7] have explored offloading tasks to servers multiple hops away, but these designs did not consider the impact of network topology. In our prior work [8], we proposed a multi-layered task offloading framework and demonstrated that computational efficiency can be improved by leveraging layered network structures. We also showed that computational efficiency is influenced not only by the computing and communication characteristics of the servers but also by their network topology. In this paper, we aim to further investigate the joint design of layered network structure and task allocation. Layered structures have been widely used in communication and practical sensor networks due to energy efficiency and network management simplicity [9]. In these networks, a base station is typically present alongside several clusters of sensors or communication devices, with each cluster comprising a Cluster Head (CH) and multiple Cluster Members (CMs). The CH collects data from its CMs and transmits it to the base station. Methods for selecting CHs and CMs can be broadly categorized into two types: cluster-based [10, 11, 12, 13] and grid-based [14, 15, 16, 17]. In cluster-based methods, CHs are selected directly based on certain criteria. In contrast, grid-based methods first divide the network into grids, and then select CHs within each grid. Despite the widespread use of layered structures in communication and sensor networks, their design for task offloading remains underdeveloped. In this paper, we introduce a decentralized network topology design strategy for task offloading (DNTD-TO). We explore three-layered network structures, similar to those commonly used in communication and sensor networks, to facilitate computing. In this setup, tasks are offloaded from the root node (referred to as master) to servers in the second layer (CHs), which then distribute the tasks to their child nodes in the third layer (CMs). To select CHs and CMs, our strategy iterates through two nested phases: a local cluster formation phase, where each server within master’s communication range selects CMs in a decentralized manner, and a cluster selection phase where the master selects CHs and their associated CMs. The selection of CHs and CMs is based on servers’ task processing capacities and their potential to enhance computational efficiency. Optimal task allocation is integrated into every step of the selection process. The rest of the paper is organized as follows. Sec. II covers system modeling and problem formulation. Our approach and simulation studies are presented in Sec. III and Sec. IV, respectively. Sec. V concludes the paper."
https://arxiv.org/html/2411.07410v1,Control Protocol for Entangled Pair Verification in Quantum Optical Networks,"We consider quantum networks, where entangled-photon pairs are distributed using fibre optic links from a centralized source to entangling nodes. The entanglement is then stored (via an entanglement swap) in entangling nodes’ quantum memories until used in, e.g., distributed quantum computing, quantum key distribution, quantum sensing, and other applications. Due to the fibre loss, some photons are lost in transmission. Noise in the transmission link and the quantum memory also reduces fidelity. Thus, entangling nodes must keep updated records of photon-pair arrivals to each destination, and their use by the applications. This coordination requires classical information exchange between each entangled node pair. However, the same fibre link may not admit both classical and quantum transmissions, as the classical channels can generate enough noise (i.e., via spontaneous Raman scattering) to make the quantum link unusable. Here, we consider coordinating entanglement distribution using a standard Internet protocol (IP) network instead, and propose a control protocol to enable such. We analyse the increase in latency from transmission over an IP network, together with the effect of photon loss, quantum memory noise and buffer size, to determine the fidelity and rate of entangled pairs. We characterize the relationship between the latency of the non-ideal IP network and the decoherence time of the quantum memories, providing a comparison of promising quantum memory technologies.","Quantum networks can revolutionize the way we transmit information securely and efficiently. By exploiting the principles of quantum mechanics, these networks enable applications such as quantum cryptography, distributed quantum computing, and high-precision sensing[1]. A fundamental resource in these networks is quantum entanglement, when multiple particles become strongly correlated and the state of any subsystem cannot be regarded separately from the entire system. This leads to correlation between the particles that extends beyond those accounted by classical statistics. Entanglement can be distributed across distant nodes in a quantum network by optical photons, as they are highly resistant to environmental noise. These photons can be readily entangled with other photons, or atomic and solid state quantum memories, enabling entanglement consumption by various applications. The practical implementation of quantum optical networks for entanglement distribution faces significant challenges due to losses and noise. For example, photons can be absorbed or scattered as they travel through optical fibers. Additionally, environmental noise can degrade the quality of the quantum bits (qubits). Unlike classical signals, quantum signals cannot be amplified because of the no-cloning theorem, which prohibits creating identical copies of an unknown quantum state. This challenges the maintenance of entanglement over long distances or through complex network topologies. Furthermore, when one photon of an entangled pair is lost in transmission, the remaining photon is unusable. This necessitates a protocol to manage entanglement distribution and storage. Given that quantum memories, which store qubits until needed, have limited capacity and finite coherence times, efficient resource management is crucial. The high arrival rate of photons relative to available memory necessitates that nodes promptly identify and discard unusable qubits in the memory to free up space for transferring entanglement from incoming photons into memory qubits. In addition, quantum applications require entangled pairs to maintain a certain fidelity between nodes. Therefore, each node must verify that its qubits have counterparts in the partner node’s quantum memory with the required fidelity before they can be served to application. Here, we introduce a quantum optical network control protocol. Existing protocols [2] are for entanglement identification and tracking in a meet-in-the-middle architecture, using a reactive entanglement distribution strategy [3]. Contrarily, we adopt a proactive strategy using a midpoint-source architecture, where entangled photon pairs are continuously distributed to ensure an uninterrupted entanglement supply. Our protocol enables nodes to verify the integrity of qubits and accurately pair them with their entangled counterparts. We analyse the network latency impact on the verification protocol, and, ultimately, on the entangled qubits’ fidelity. This is important, as verification should ideally use the existing Internet protocol (IP) networks, rather than the quantum fibre link, as the noise generated by the classical channel [4] could render the quantum channel unable to distribute entanglement. Indeed, latency impacts the execution time of the protocol and, as a result, the quality of entangled qubits. Quantum systems, including memories, are susceptible to decoherence, and parameters such as T_{1} (longitudinal relaxation time, which measures the duration for spontaneous decay from the excited to the ground state) and T_{2} (transverse relaxation time, which quantifies how long phase coherence is maintained in a superposition state) dictate how long qubits can preserve their coherence [5]. Classical control delays can thus degrade the fidelity of entangled pairs by increasing the verification time. We analyze how this latency affects the fidelity of the entangled qubits, the rate of entanglement (i.e., the rate at which qubits with a maximally-entangled bipartite state are retained in the memory after verification), and the consumption of quantum memory resources needed to buffer entangled qubits until protocol completion. This analysis considers latency and noisy quantum memories under practical network conditions. The remainder of the paper is organized as follows: Section II provides an overview of the network model, integrating classical communication infrastructure with quantum networks. Section III describes our quantum memory noise model using Lindblad dynamics. Section IV discusses the results of our study, and concludes the paper with a summary of our findings."
https://arxiv.org/html/2411.07345v1,"High-Fidelity Cellular Network Control-Plane Traffic Generation
without Domain Knowledge","With rapid evolution of mobile core network (MCN) architectures, large-scale control-plane traffic (CPT) traces are critical to studying MCN design and performance optimization by the R&D community. The prior-art control-plane traffic generator SMM heavily relies on domain knowledge which requires re-design as the domain evolves. In this work, we study the feasibility of developing a high-fidelity MCN control plane traffic generator by leveraging generative ML models. We identify key challenges in synthesizing high-fidelity CPT including generic (to data-plane) requirements such as multi-modality feature relationships and unique requirements such as stateful semantics and long-term (time-of-day) data variations. We show state-of-the-art, generative adversarial network (GAN)-based approaches shown to work well for data-plane traffic cannot meet these fidelity requirements of CPT, and develop a transformer-based model, CPT-GPT, that accurately captures complex dependencies among the samples in each traffic stream (control events by the same UE) without the need for GAN. Our evaluation of CPT-GPT on a large-scale control-plane traffic trace shows that (1) it does not rely on domain knowledge yet synthesizes control-plane traffic with comparable fidelity as SMM; (2) compared to the prior-art GAN-based approach, it reduces the fraction of streams that violate stateful semantics by two orders of magnitude, the max y-distance of sojourn time distributions of streams by 16.0%, and the transfer learning time in deriving new hourly models by 3.36\times.","1 Introduction The recently introduced Control-/User-Plane Separation (CUPS) in 3GPP Release 14 for 4G (3GPP, 2016a) and in 3GPP Release 15 for 5G (3GPP, 2016c), combined with a significant increase in control-plane traffic alongside explosive growth in data-plane traffic (Networks, 2016; Analytics, 2021), challenge mobile network operators and designers to innovate on mobile network architectural design not only for the data plane but also for the control plane in order to provide sustained mobile user experience. Indeed, an increasing number of efforts have focused on open-source development of 3GPP-compliant MCN implementations (Ericsson, 2023) and novel MCN designs for improved performance (Jain et al., 2022; Ferguson et al., 2023; Hasan et al., 2023). Accurately assessing the performance of such open-source implementations or new MCN designs under real deployment scenarios, however, critically relies on using high-fidelity, large-scale control-plane traffic to drive the MCN operations. Despite the need, large-scale control-plane traffic traces in MCN are only accessible by mobile network operators, who are unlikely to share their traffic traces due to business and privacy concerns. As a result, the lack of public MCN control-plane traffic hinders the in-depth study of MCN design and performance optimization by the broad networking and systems communities. A classic approach to mitigating the lack of real network traffic traces is to generate synthetic traces. However, the large body of traffic synthesis work for LTE and 5G has focused on the data plane of MCN, i.e., on modeling data-plane traffic (e.g., (Lindberger, 1999; Li et al., 2010b, a; Checko et al., 2012; Jailani et al., 2012)). On the control-plane side, Dababneh et al. (Dababneh et al., 2015) modeled the total control-plane volume on LTE’s MCN but did not model the fine-grained inter-arrival time of successive events or the intricate event dependence specified by 3GPP for each individual UE; they also ignored the traffic diversity in device types and time-of-day. Compared to Internet data traffic generation, cellular network control-plane traffic generation faces some common fidelity requirements but also unique new requirements. A control-plane traffic trace consists of multiple streams of control events (samples), one stream per UE, typically throughout a day. (1) Stateful semantics. First and foremost, the traffic generator must accurately capture the rigid inter-dependence of sample features within a stream, adhering to domain-specific rules, i.e., 3GPP UE state machines, ensuring only semantically correct generated streams will be used to drive the MCN. (2) Multi-modal features. Like data traffic, control-plane traffic encompasses multiple fields. Specifically, each sample includes a control event type and a timestamp, and thus the traffic generator is tasked not only with producing realistic distributions for individual fields, but also generating realistic correlations between fields within each sample. (3) Variable flow length. As in data traffic, the traffic generator needs to capture diversity in UE flow length. For instance, the number of events within a fixed time window could vary among clients, which follows from the diversity in UE activity levels. (4) Long-term data drifts. As control-plane traffic lasts throughout the day, the traffic generator needs to capture data distribution drifts that occur over time, such as variations throughout a day. To meet these requirements, the state-of-the-art cellular control-plane traffic generator SMM (Meng et al., 2023) employs a traditional modeling methodology. In particular, it derives a statistical model, i.e., a Semi-Markov model, based on the 3GPP UE state machines and fits the model parameters (state transitions probabilities and sojourn times) on the real trace. Such a traditional modeling approach, however, critically relies on domain knowledge, e.g., the 3GPP standard, and hence requires re-design as the domain knowledge evolves. First, a separate model needs to be designed for each generation of cellular technology (4G or 5G). Second, even for a particular generation of technology, standards evolve continuously over time as new 3GPP specifications are published. For example, the development of LTE was carried out through 3GPP Release 8 (2008) to Release 16 (2020) (3GPP, 2024). Therefore, continuous manual effort is required to keep such domain-knowledge-dependent generators up to date with the latest standards. An additional limitation of the above traditional modeling approach is that the model parameters are rigid, and a single model cannot capture the diversity of control-plane traffic trace. As a result, the authors had to cluster control-plane-traffic and instantiate 20,216 models (one per cluster per hour) which are inconvenient to maintain and deploy. In light of the limitation of the traditional modeling approach, in this work, we study the feasibility of developing a high-fidelity MCN control plane traffic generator by leveraging generative machine learning (ML) models. Our approach was inspired by the recent work on using Generative Adversarial Networks (GAN) combined with LSTM in synthesizing data-plane traffic traces (Lin et al., 2020; Yin et al., 2022) which showed GAN-based approaches effectively learn the distributions and correlations of packet- or flow-level header fields in Internet traffic. However, when we adapted prior-art GAN-based approaches to generate control-plane traffic of the cellular network, we found it suffers a number of limitations. (1) The prior-art approaches such as NetShare (Lin et al., 2020) cannot capture the stateful semantics, as 22.10% of the streams in the synthesized trace contain one or more events that violate state transition rules stipulated by the 3GPP protocols. (2) NetShare fails to capture fine-grained temporal properties, such as the distribution of sojourn times, i.e., the duration that a UE stays in a particular state of the 3GPP state machine. For example, the maximum y-distance between the CDFs of the real and synthesized traces’ sojourn times reaches 61.7%. (3) The GAN-based approach cannot efficiently adapt to data distribution drifts that occur through time via transfer learning. In particular, training a model for one hour and applying transfer learning for each of the next 5 hours takes almost 2X as long as direct training a 6-hour model from scratch. (4) These GAN-based LSTM approaches require complex and specialized enhancements to alleviate mode collapse (Kodali et al., 2017). (5) Finally, these approaches require complex enhancements to the model design to address LSTM’s tendency to forget past states (Schak and Gepperth, 2019; Arora et al., 2019) in order to capture long-term dependencies. To overcome these drawbacks, we develop a transformer-based framework, CPT-GPT, that accurately captures complex dependencies among the events in each stream to meet the fidelity requirements (stateful semantics and distribution of multiple features). In doing so, it not only achieves comparable fidelity in traffic generation without domain knowledge as needed in SMM, but also achieves higher fidelity than prior-art GAN/LSTM-based approaches (Lin et al., 2020; Yin et al., 2022) without the need to use GAN or model enhancements such as dealing with long-term dependencies and mode collapse. Instead, the design of CPT-GPT only has to focus on a few challenges in the input layer and output layer design: (1) How to design tokens for the input layer to capture multi-modal features per sample? (2) How to introduce generation stochasticity at the output layer for multi-modal features? (3) How to efficiently update the model over time? We implement the complete cellular network control-plane generation model CPT-GPT in PyTorch, based on a decoder-based transformer model (Vaswani et al., 2017). We extensively evaluate CPT-GPT by comparing it with the traditional modeling-based SMM (Meng et al., 2023) and the SOTA GAN-based NetShare (Yin et al., 2022), on a large-scale control-plane traffic trace (for 380K UEs) from a leading mobile operator. Our evaluation shows that (1) Compared to SMM, CPT-GPT requires no domain knowledge, yet it can achieve close-to-zero semantic violations, similarly accurate sojourn time distribution in terms of the max y-distance of the CDFs, and more accurate percentage breakdown of different event types. For example, averaged over the three types of UEs (phones, connected cars, and tablets), CPT-GPT’s differences compared to the real dataset in the breakdown of the two dominant event types (SRV_REQ and S1_CONN_REL) are 1.01% and 1.21% lower than SMM, respectively. (2) Compared to NetShare, CPT-GPT achieves much higher fidelity in control-plane traffic generation. (i) It reduces the fraction of streams that violate stateful semantics by two orders of magnitude, from 22.1%, 11.5%, and 16.9% to 0.2%, 0.4%, and 1.5% for phones, connected cars, and tablets, respectively. (ii) It also improves the sojourn time distribution of the synthesized traces. For the three device types, the max y-distance of sojourn time distribution from the real trace is reduced by 10.7%, 9.1%, and 28.3% respectively, averaging over the two 3GPP states. (iii) CPT-GPT is more efficient in being applied transfer learning to adapt a pretrained model for a given hour-of-day to the subsequent hours. Specifically, it reduces the time needed to derive hourly models by 3.36\times. (3) CPT-GPT generates diverse control-plane traffic without memorizing individual sequences from the training set. Specifically, when examining generated sub-sequences of length 20, none are repeated from the training set. In summary, this work makes the following contributions: • We develop the first control-plane traffic generator that does not require domain knowledge yet achieves comparable accuracy as the prior-art Semi-Markov-Model-based traffic generator which heavily relies on domain knowledge. • We show how transformer-based models can overcome the low fidelity limitations of prior-art GAN/LSTM-based models in synthesizing control-plane traffic, more efficiently employ transfer learning to deal with traffic shifts over time, and does not memorize individual sequences from the training set."
https://arxiv.org/html/2411.07702v1,"A Call to Reconsider 
Certification Authority Authorization (CAA)","Certification Authority Authentication (CAA) is a safeguard against illegitimate certificate issuance. We show how shortcomings in CAA concepts and operational aspects undermine its effectiveness in preventing certificate misissuance. Our discussion reveals pitfalls and highlights best practices when designing security protocols based on DNS.Keywords: Web PKI, DNS, CAA, Protocol Security","Web security 1 relies on X.509 certificates. Fundamental to the underlying security model is the correct issuance of certificates. This trust is challenged by unrestricted certification. Certification Authorities (CA) are generally allowed to certify any arbitrary resource, i.e., to bind a public key to a domain name by creating a certificate. If certification occurs without the consent of a name owner, the misissued certificate can be used to impersonate resources of that name owner. Causes for misissuance are manyfold. Rogue or compromised CAs 2, spoofed DNS records or compromised name servers 3, and malicious traffic rerouting 4, 5 have been part of attack vectors in the past. There are two principal directions to harden the security of the certificate issuance process. (i) Prevention by enabling a CA to verify whether the issuing request is valid. (ii) Mitigation by enabling the name owner or any third party on behalf to identify and revoke an incorrectly issued certificate. Both directions have been covered in standardization and deployment. Certificate transparency (CT) 6 makes certificates public and DNS-Based Authentication of Named Entities for TLS (DANE TLSA) 7 binds certificates to services available below a specific domain name. Prevention is specified in CA authorization (CAA) 8, which allows a name owner to note in the DNS which CA is allowed to issue a certificate. In this paper, we argue that both prevention and mitigation are important, but that prevention should be the first-class citizen because it addresses a root cause (§ 2). Unfortunately, CAA, which is not only an IETF standard but also the solution the CA/Browser Forum agreed on to be mandatory, is flawed. We revisit design decisions and deployment behaviors (§ 5) to refine on-going standards and inform our community to guide fundamentally different approaches (§ 6), hoping that these insights prevent common pitfalls in the future. Our data-driven approach (§ 4) is based on more than 4.6M unique certificates from CT logs, which we test whether they conform to CAA policies. Our major findings read: 1. Implicit semantics overshadow expressiveness. 2. Underspecified syntax allows misinterpretation. 3. Loose policy scoping raises security risks. 4. Misaligned procedures defeat reliability and trust."
https://arxiv.org/html/2411.06870v1,AI-Native Multi-Access Future Networks - The REASON Architecture,"The development of the sixth generation of communication networks (6G) has been gaining momentum over the past years, with a target of being introduced by 2030. Several initiatives worldwide are developing innovative solutions and setting the direction for the key features of these networks. Some common emerging themes are the tight integration of AI, the convergence of multiple access technologies and sustainable operation, aiming to meet stringent performance and societal requirements. To that end, we are introducing REASON - Realising Enabling Architectures and Solutions for Open Networks. The REASON project aims to address technical challenges in future network deployments, such as E2E service orchestration, sustainability, security and trust management, and policy management, utilising AI-native principles, considering multiple access technologies and cloud-native solutions.This paper presents REASON’s architecture and the identified requirements for future networks. The architecture is meticulously designed for modularity, interoperability, scalability, simplified troubleshooting, flexibility, and enhanced security, taking into consideration current and future standardisation efforts, and the ease of implementation and training. It is structured into four horizontal layers: Physical Infrastructure, Network Service, Knowledge, and End-User Application, complemented by two vertical layers: Management and Orchestration, and E2E Security. This layered approach ensures a robust, adaptable framework to support the diverse and evolving requirements of 6G networks, fostering innovation and facilitating seamless integration of advanced technologies.","The forthcoming generation of communication networks, colloquially referred to as Sixth Generation (6G), represents a transformative leap beyond the existing paradigms of mobile communications [1]. This leap is essential to meet burgeoning demands for individual users and vertical industries, covering both indoor and outdoor environments and spanning multiple sectors and applications like automotive, manufacturing, public safety, eHealth, immersive media, and more [2]. These applications shift away from conventional Key Performance Indicators (KPIs), like faster data rates, reduced latency, or broader coverage, and move towards more complex classes of enablers for advanced services [3]. For example, the Fifth Generation (5G) Infrastructure Association (5G IA) [4] provides a comprehensive list discussing “integrated sensing and communications”, “cognition and connected intelligence”, “trustworthy and sustainable infrastructures”, and more. Similarly, other initiatives, like Hexa-X [5], discuss enablers like “network of networks”, “global service coverage”, “extreme experiences”, etc. A comprehensive comparison of the different visions can be found at [6]. Overall, all visions converge to a future network that architects a seamlessly interconnected world, where the integration of digital, physical, and human systems unfolds new dimensions of experience, efficiency, and societal transformation. TABLE I: List of Acronyms. Acronym Description 5G 5th Generation 5GPPP 5G Infrastructure Public Private Partnership 6G 6th Generation AI Artificial Intelligence AIaaS AI-as-a-Service AT Access Technology CIA Confidentiality, Integrity, and Availability CI/CD Continuous Integration / Continuous Delivery CNF Container Network Function CPaaS Communications-Platform-as-a-Service DAO Distributed Autonomous Organisation DLT Distributed Ledger Technology DT Digital Twin E2E End-to-End eMBB enhanced Mobile Broadband FDRL Federated Deep Reinforcement Learning FL Federated Learning FSO Free Space Optical GEO Geostationary Orbit IaC Infrastructure-as-Code IMT International Mobile Telecommunications IoT Internet of Things IT Information Technology ITU-T ITU Telecommunication KPI Key Performance Indicators KVI Key Value Indicators LEO Low Earth Orbit LiFi Light Fidelity MANO Management and Orchestration mATRIC Multi-access Technology Real-Time Intelligent Controller MEO Medium Earth Orbit mMTC massive Machine Type Communication MLOps Machine Learning Operations NaC Network-as-code NF Network Function NFV Network Function Virtualisation NGMN Next Generation Mobile Networks Alliance NIO Network Intelligence Orchestration NPN Non-Public Network NRE Network Resource Elasticity NTN Non-Terrestrial Network OPEX Operational Cost O-RAN Open Radio Access Network OT Operational Technology QoS Quality of Service QoE Quality of Experience RAN Radio Access Network RAT Radio Access Technologies REASON Realising Enabling Architectures and Solutions for Open Networks RINA Recursive InterNetwork Architecture RF Radio Frequency SBA Service-based Architecture SDG Sustainable Development Goals SLA Service-Level Agreement SFC Service Function Chain UPF User Plane Function URLLC Ultra-Reliable and Low-Latency Communication VIM Virtual Infrastructure Manager VM Virtual Machine VNF Virtual Network Function XAI Explainable AI However, shifting from traditional KPIs to a more complex set of enablers is only part of the wider paradigm shift shown in 6G ecosystems. We witness a transition from “govern by performance” to “govern by value” [3]. In other words, what is needed is not an indication of performance but an indication of value for the developed new ecosystems. This is how the term Key Value Indicators (KVIs) was born [7]. 6G is expected to not only bring innovative solutions for the above complex problems but also address other societal challenges, like environmental sustainability, ethical implications of technological advancements, and more [8]. A comprehensive, collaborative, and multifaceted approach is imperative to ensure progress that benefits all sectors of society and positively contributes to the global community. The architecture design is one of the most important aspects of each new generation of a communications network system. It is considered its foundation and describes what and how services are provided and how they are integrated. Based on that, this paper presents Realising Enabling Architectures and Solutions for Open Networks (REASON) project111REASON Project: https://reason-open-networks.ac.uk/ and its envisaged architecture. REASON aims to address the above technical and non-technical challenges and deliver an End-to-End (E2E) reference Open Network blueprint, considering all segments and functions of a future network. REASON will pursue to influence the future technology roadmap, making openness, interoperability and Artificial Intelligence (AI)-native capabilities the default standards in network architectures and systems. Reference architectures are usually driven by three factors [9], i.e., the “new scenarios and requirements to be delivered”, the “new technologies that need to be integrated”, and the “inspiration from existing systems”. This is the scope of this paper. Analysing the challenges identified and several use cases defined, we will provide a comprehensive set of requirements to drive future 6G networks. These requirements will later be used to define a novel reference architecture that enables principles like interoperability, agility, sustainability, resilience, and security, all key to future Industrial, Entertainment and Smart Cities applications [10]. Our use case proposition is also expected to demonstrate not only the capabilities of the REASON architecture but also to motivate future scenarios with strong market demand and commercial opportunities. As an expected outcome, the REASON architecture aims to enable multi-technology access network integration to meet the emerging 6G KPIs. Moreover, it will promote network densification to ensure support of the described 6G use cases. The architecture will be AI-native, enabling service and network optimisations in an E2E fashion. Cognitive tools and task-based networking are considered integral parts of the architecture, and we will discuss how they are incorporated into our system. Finally, the aspects of energy-awareness, E2E monitoring, seamless edge-cloud computing continuum, openness, interoperability and security will drive many of our architectural decisions, and we explain how the proposed design addresses them. This paper is structured as follows. Sec. II compares 5G and expected 6G capabilities and comprehensively compares REASON with other future networking projects. The technical and societal challenges identified in future 6G networks and some expected usage scenarios are presented in Sec. III. Sec. IV provides insight into the use cases identified within REASON. Building upon the challenges and use cases, Sec. V presents all the requirements that should be addressed from future network architecture and implementation. Following the requirements, the envisaged REASON architecture is detailed in Sec. VI. Finally, Sec. VIII provides our final remarks and some future directions within the REASON project. A table summarising all the acronyms can be found in Table I."
https://arxiv.org/html/2411.06742v1,Loss-tolerant neural video codec aware congestion control for real time video communication,"Because of reinforcement learning’s (RL) ability to automatically create more adaptive controlling logics beyond the hand-crafted heuristics, numerous effort has been made to apply RL to congestion control (CC) design for real time video communication (RTC) applications and has successfully shown promising benefits over the rule-based RTC CCs. Online reinforcement learning is often adopted to train the RL models so the models can directly adapt to real network environments. However, its trail-and-error manner can also cause catastrophic degradation of the quality of experience (QoE) of RTC application at run time. Thus, safeguard strategies such as falling back to hand-crafted heuristics can be used to run along with RL models to guarantee the actions explored in the training sensible, despite that these safeguard strategies interrupt the learning process and make it more challenging to discover optimal RL policies.The recent emergence of loss-tolerant neural video codecs (NVC) naturally provides a layer of protection for the online learning of RL-based congestion control because of its resilience to packet losses, but such packet loss resilience have not been fully exploited in prior works yet. In this paper, we present a reinforcement learning (RL) based congestion control which can be aware of and takes advantage of packet loss tolerance characteristic of NVCs via reward in online RL learning. Through extensive evaluation on various videos and network traces in a simulated environment, we demonstrate that our NVC-aware CC running with the loss-tolerant NVC reduces the training time by 41% compared to other prior RL-based CCs. It also boosts the mean video quality by 0.3 to 1.6dB%, lower the tail frame delay by 3 to 200ms, and reduces the video stalls by 20% to 77% in comparison with other baseline RTC CCs.","Real-time video communication including video conferencing (MacMillan et al., 2021), live video/VR broadcasting (Web, 2021a, 2020b; Hopkins, 2017), IoT applications (Web, 2021b; Mob, 2024), and cloud gaming (Web, 2021c, 2020a) has been a key component of our daily lives (Blum et al., 2021) and carries a dominant amount of traffic in today’s internet (Cisco, [n. d.]). These RTC applications require high network bandwidth and low network latency to deliver seamless and high quality experience to users, pushing the telecommunication infrastructure upgrade to meet the demands and forcing the congestion control algorithms to promptly adapt to the constantly changing network conditions. Unlike traditional congestion controls (Ha et al., 2008; Brakmo et al., 1994; Jacobson, 1988) which are designed for reliability and in-order delivery through retransmissions instead of realtimeliness, plenty of hand-crafted congestion controls for real-time video communication (Carlucci et al., 2016; Fouladi et al., 2018; Zhu et al., 2020; Johansson and Sarker, 2017; Nagy et al., 2014; Ray et al., 2022) have been proposed to boost bandwidth utilization, suppress packet delays, and avoid packet losses. However, these pre-programmed rule-based congestion control algorithms are not panacea in all network settings as they fall short of adapting to the highly heterogeneous network conditions. To save the human effort optimizing a rule-based congestion control algorithm for numerous network conditions, researchers have made huge effort to explore the data-driven approaches to design congestion control and rate adaptation (Mao et al., 2017; Jay et al., 2019; Zhang et al., 2020, 2021; Zhou et al., 2019; Xia et al., 2022; Gilad et al., 2019) and have shown great potential over the handcrafted heuristics. The other approach designed to bridge the gap between training network environments and the real networks is using The “learning online, running online” strategy is often adopted to train a RL-based solution in order to bridge the gap between training network environments and the real network environments at the deployment stage. RL models directly interact with the real network environments to collect experience and then update themselves during runtime. However, the trial-and-error behavior of online RL training will unavoidably take risky actions which might disturb the system performance. A safeguard policy, typically a handcrafted heuristics, is often used to substitute the RL model once an erroneous action is detected or the system is in a risky state (Zhang et al., 2020; Mao et al., 2019). After the safeguard policy recovers the system to a safe state, the RL-based model takes the control back. The main design philosophy behind safeguarding a RL-based CC as well as behind traditional CCs in RTC applications is based on an implicit assumption on video codec that delayed or lost packets can lead to incomplete frames received which then block video decoding at the receiver side and hurt users’ QoE. The recent loss-tolerant neural video codecs (Dasari et al., 2022; Hu et al., 2021; Lu et al., 2019; Cheng et al., [n. d.]; Chen et al., [n. d.]; Sivaraman et al., 2022) breaks the implicit assumption on video codecs as these NVCs can decode incomplete frames and still deliver decent frame quality. They have shown strong loss tolerance ability across a wide range of packet loss rates on top of its high compression efficiency and good generalization over various video content. Figure 1 shows a state-of-art neural video codec, GRACE, has a smoother and slower video quality drop with increasing packet loss rate than commonly used encoder-side forward error correction (FEC) and decoder-side error concealment (EC). Figure 1. Loss-tolerant neural video codec has slow and smooth video quality drop with increasing packet loss rate. In this paper, we present NVC-CC, a RL-based RTC congestion control algorithm which can be trained online without the help of safeguard policies by taking advantage of the NVCs’ packet loss tolerance properties. Our key insight is that the safeguard policies run along the RL model hinders the RL online learning efficiency. Comprehensive experiments (§5) on a diverse set of videos and network traces show that our NVC-aware CC running with the loss-tolerant NVC reduces the training time by 41% compared to other prior RL-based CCs. It also boosts the mean video quality by 0.3 to 1.6dB%, lower the tail frame delay by 3 to 200ms, and reduces the video stalls by 20% to 77% in comparison with other baseline RTC CCs. Contributions: Our work makes the following contributions. 1) We reveal the inefficiency of training in RL-based RTC congestion control solutions trained by online learning with safeguard policies and introduce the trade-off between learning efficiency and QoE in RL training (§3.2). 2) We analyze how the loss-tolerant NVCs can help improve training efficiency by allowing RL-based CCs to learn without the restriction of safeguard policies and not not hurting QoE (§3.3). 3) We propose NVC-CC, which, to the best of our knowledge, the first RL-based congestion control aware of and taking advantage of the loss-resilient properties of neural video codecs (§4) and validate its remarkable performance gain over the state-of-the-art solutions (§5)."
https://arxiv.org/html/2411.06455v1,Enhancing Emergency Communication for Future Smart Cities with Random Forest Model,"This study aims to optimise the ‘spray and wait’ protocol in delay tolerant networks (DTNs) to improve the performance of information transmission in emergency situations, especially in car accident scenarios. Due to the intermittent connectivity and dynamic environment of DTNs, traditional routing protocols often do not work effectively. In this study, a machine learning method called random forest was used to identify ‘high-quality’ nodes. High-quality’ nodes refer to those with high message delivery success rates and optimal paths. The high-quality node data was filtered according to the node report of successful transmission generated by the One simulator. The node contact report generated by another One simulator was used to calculate the data of the three feature vectors required for training the model. The feature vectors and the high-quality node data were then fed into the model to train the random forest model, which was then able to identify high-quality nodes. The simulation experiment was carried out in the ONE simulator in the Helsinki city centre, with two categories of weekday and holiday scenarios, each with a different number of nodes. Three groups were set up in each category: the original unmodified group, the group with high-quality nodes, and the group with random nodes. The results show that this method of loading high-quality nodes significantly improves the performance of the protocol, increasing the success rate of information transmission and reducing latency. This study not only confirms the feasibility of using advanced machine learning techniques to improve DTN routing protocols, but also lays the foundation for future innovations in emergency communication network management.","In modern communication networks, in the face of network disruption, congestion, or disaster, the traditional connection-based network model often fails to ensure the effective transmission of information. Fault-tolerant networks(DTNs)are emerging as an important solution for complex communication scenarios due to their excellent performance in the face of unstable or intermittent network connections. DTN networks [1] employ a store-and-forward mechanism, whereby information is persistently stored in the intermediate nodes of the DTN. In this way, it can transmit information efficiently even in the face of intermittent network connections. They are widely used in emergency scenarios such as natural disasters and car accidents. As part of the Smart City project in the city of Porto, Portugal, a number of DTN protocols have been used in urban bus networks to address the problem of high mobility and low connection density between vehicles. Sensor data is transmitted through these networks to a central server for city management and environmental monitoring [2]. The focus of this research is to improve the performance of DTN networks in emergency situations, especially in the specific scenario of car accidents. The data involved is geo-temporal, dynamic and complex. These data characteristics make information delivery variable and unpredictable. In such situations, timely and reliable transmission of information is important for coordinating rescue operations and providing early warning to surrounding pedestrians and vehicles. In this scenario spray-and-wait protocol is a useful routing strategy for information delivery in DTN networks because of its simplicity and effectiveness in reducing transmission delays. The protocol ensures that information is delivered to the target node as soon as possible by propagating copies of the information among multiple nodes. However, in high-risk and unstable environments such as car accident scenes, the basic implementation of the Spray and Wait protocol may not be successful in delivering information in a timely manner.Tornell et al. (2015) published an application of Delay Tolerant Networks (DTNs) in in-vehicle networks, which demonstrated that by optimising the DTN routing strategy ( such as the Spray and Wait protocol), the success rate of message delivery in high-risk and unstable environments can be significantly improved [3]. The core idea of my research is to identify ‘high quality’ nodes, i.e., those nodes that have a higher probability of successfully delivering a message and a shorter path to successfully deliver a message, based on specific characteristics observed in the simulations. These nodes will be predicted by the Random Forest model. By increasing the number of message copies carried by these high quality nodes, the overall message delivery success rate of a DTN network can be significantly improved, especially in dynamic and unpredictable environments such as car accident scenes. The rest of the paper is organised as follows: section 2 discusses global research trends in improving the performance of routing protocols in fault-tolerant networks (DTNs). Section 3 introduces the basic concepts of DTN networks and optimisation of the protocol. Section 4 describes the experimental design for setting up an emergency scenario (e.g., a car crash in Helsinki). Section 5 discusses the experimental results and analyses the reasons. Section 6 summarises the main results of the study and future research directions."
https://arxiv.org/html/2411.06348v1,Optimal Algorithm for Multiple-Processor Multitask Scheduling,"The efficient scheduling of multi-task jobs across multiprocessor systems has become increasingly critical with the rapid expansion of computational systems. This challenge, known as Multiprocessor Multitask Scheduling (MPMS), is essential for optimizing the performance and scalability of applications in fields such as cloud computing and deep learning. In this paper, we study the MPMS problem under both deterministic and stochastic models, where each job is composed of multiple tasks and can only be completed when all its tasks are finished. We introduce \mathsf{NP}-\mathsf{SRPT}, a non-preemptive variant of the Shortest Remaining Processing Time (SRPT) algorithm, designed to accommodate scenarios with non-preemptive tasks. Our algorithm achieves a competitive ratio of \ln\alpha+\beta+1 for minimizing response time, where \alpha represents the ratio of the largest to the smallest job workload, and \beta captures the ratio of the largest non-preemptive task workload to the smallest job workload. We further establish that this competitive ratio is order-optimal when the number of processors is fixed. For stochastic systems modeled as M/G/N queues, where job arrivals follow a Poisson process and task workloads are drawn from a general distribution, we prove that \mathsf{NP}-\mathsf{SRPT} achieves asymptotically optimal mean response time as the traffic intensity \rho approaches 1, assuming the task size distribution has finite support. Moreover, the asymptotic optimality extends to cases with infinite task size distributions under mild probabilistic assumptions, including the standard M/M/N model. Experimental results validate the effectiveness of \mathsf{NP}-\mathsf{SRPT}, demonstrating its asymptotic optimality in both theoretical and practical settings.","Scheduling is fundamentally about the optimal allocation of resources over time to perform a collection of jobs. With widespread applications in various fields, scheduling jobs to minimize the total response time (also known as flow time [1], sojourn time [2] and delay [3]) is a fundamental problem in computer science and operation research that has been extensively studied. As an important metric measuring the quality of a scheduler, response time, is formally defined as the difference between job completion time [4, 5] and releasing date, and characterizes the amount of time that the job spends in the system. Optimizing the response time of single-task jobs has been considered both in offline and online scenarios. If preemption is allowed, the Shortest Remaining Processing Time (SRPT) discipline is shown to be optimal in single machine environment. Many generalizations of this basic formulation become NP-hard, for example, minimizing the total response time in non-preemptive single machine model and preemptive model with two machines [1]. When jobs arrive online, no information about jobs is known to the algorithm in advance, several algorithms with logarithmic competitive ratios are proposed in various settings [6, 1]. On the other hand, while SRPT minimizes the mean response time sample-path wise, it requires the knowledge of remaining job service time. Gittins proved that the Gittins index policy minimizes the mean response time in an M/G/1 queue, which only requires the access to the information about job size distribution [7]. However, traditional scheduling problems have evolved significantly from single-task models to complex multitask scenarios. In the contemporary landscape of computational resources, the efficient scheduling of tasks across multiple processors has emerged as a critical challenge, driven by the exponential growth of data and the complexity of applications. To give the simplest example, for the objective of computing matrix vector product, we can divide matrix elements and vector elements into groups of columns and rows respectively, then the tasks correspond to the block-wise multiplication operations. Tasks can also be map, shuffle and reduce procedures in MapReduce framework [8]. Multi-processor Multitask Scheduling (MPMS) [9], where a job is considered to be completed only when all the tasks within the job are finished, addresses the allocation of jobs and tasks, each with potentially varying computational demands, to a set of processors in a manner that optimizes overall system performance metrics. Though much progresses have been made in single-task job scheduling, there is a lack of theoretical understanding regarding MPMS. The extension of traditional scheduling principles to multi-task systems, presents significant challenges. The heterogeneity of task durations and the dynamic nature of job arrivals seems further complicate the analysis. Thus a natural question that arises is, How can we design an efficient scheduling algorithm to minimize the total amount time that the multitask jobs spend in the multi-processor system? The significance of MPMS problem is also underscored by its direct impact on deep learning workflow [10, 11, 12], several different parallel strategies, such as data parallelism, pipeline parallelism, etc, have been proposed to accelerate the training process. Deep learning models benefit from parallel execution during training and inference phases, necessitating effective job and task scheduling to maximize the utilization of available computational resources. Inefficient scheduling can lead to increased training times, higher operational costs, and suboptimal model performance. Therefore, developing scheduling algorithms that can handle the complexities of modern multiprocessor and multi-task systems is paramount for advancing the field of deep learning. 1.1 Contributions. In this paper, we investigate how to minimize the total response time of multitask jobs in a multi-server system and answer the aforementioned question. Our contributions are summarized as follows. • In Section 3, we propose \mathsf{NP}-\mathsf{SRPT} algorithm [13], for minimizing the total response time. \mathsf{NP}-\mathsf{SRPT} algorithm achieves a competitive ratio of \ln\alpha+\beta+1, where \alpha is the maximum-to-minimum job workload ratio, \beta represents the ratio between maximum non-preemptive task workload and minimum job workload. It can be shown that no o(\ln\alpha+\beta)-competitive algorithm exists when the number of machines is constant. In addition, O(\ln\alpha+\beta^{1-\varepsilon}) is the best possible competitive ratio for the class of work-conserving algorithms. • Besides the worst case relative ratio above, we further prove our main result, absolute performance guarantees for \mathsf{NP}-\mathsf{SRPT} algorithm under certain probabilistic structure on the input instances, in which the relevant and remaining workload bound established for the adversarial inputs contributes significantly to the stochastic analysis. Assuming that jobs arrive according to a Poisson process, i.e., in M/G/N system, in Section 4 we prove that the average response time incurred by \mathsf{NP}-\mathsf{SRPT} algorithm is asymptotic optimal when load \rho\rightarrow 1, as long as the task size distribution has finite support. As shown in Section 5, the assumption of finite task workload can be removed for exponentially distributed job size, i.e., M/M/N, together with other job size distributions with certain properties on the tail of the distribution, by establishing an upper bound on the busy period in M/G/N. In addition, we also further validate the optimality of \mathsf{NP}-\mathsf{SRPT} through experiments in Section 6, confirming the efficacy of the algorithm. 1.2 Related Work Single and multiple task scheduling. There has been a large literature on single-task job scheduling, with parallel developments taking place in competitive analysis and queuing theory. For example, recently Hong and Scully [14] showed Gittins’s heavy-traffic optimality in G/G/N. However, little is known about multitask scheduling. Scully et. al [15] presented the first theoretical analysis of single-processor multitask scheduling problem, and gave an optimal policy that is easy to compute for batch arrival, together with the assumption that the processing time of tasks satisfies the aged Pareto distributions. Sun et al. [16] studied the multitask scheduling problem when all the tasks are of unit size, and proved that among causal and non-preemptive policies, fewest unassigned tasks first (FUT) policy, earliest due date first (EDD) policy, and first come first serve (FCFS) are near delay-optimal in distribution (stochastic ordering) for minimizing the metric of average delay, maximum lateness and maximum delay respectively. Wang et. al [3] established results of asymptotic independence of queues in multitask multi-processor system by developing a new technique named Poisson oversampling. To model the scenario when the scheduler has incomplete information about the job size, Scully et. al [17] introduced the multistage job model and proposed an optimal scheduling algorithm for multistage job scheduling in M/G/1 queue. The closed-form expression of the mean response time is also given for the optimal scheduler. In addition to the aforementioned work, there are also studies that further extend the understanding of scheduling by considering parallelizable jobs represented as Directed Acyclic Graphs (DAGs) [18]. Performance and optimality of SRPT and its variants. While SRPT is optimal at minimizing average response time in single-server systems [19], its performance is suboptimal in multi-server environments. However, SRPT remains a highly regarded method in the context of multi-server systems. It has shown that the competitive ratio of SRPT is in logarithmic order and is the best possible [1]. With the stochastic arrivals in M/G/k systems, SRPT is shown to be optimal in heavy traffic scenarios [20]. Another notable contribution by [21] introduces the Intermediate-SRPT algorithm, which addresses jobs with intermediate parallelizability and establishes an optimal O(\log P)-competitive ratio concerning average flow time, where P represents the ratio of maximum to minimum job sizes. Paper organization. The remainder of this paper is organized as following. We introduce the problem definition, notations and necessary background in Section 2. In Section 3 we formally present \mathsf{NP}-\mathsf{SRPT} algorithm, together with the analysis of its competitive ratio and lower bounds. Section 4 is devoted to the proof of the asymptotic optimality of \mathsf{NP}-\mathsf{SRPT} in heavy traffic regime, and the optimality is extended to infinite task size in Section 5. We conduct the experimental validation in Section 6, and conclude our work in Section 7."
https://arxiv.org/html/2411.06334v1,"A Multicast Scheme for Live Streaming Courses in Large-Scale, Geographically Dense Campus Networks","Video courses have become a significant component of modern education. However, the increasing demand for live streaming video courses places considerable strain on the service capabilities of campus networks. The challenges associated with live streaming course videos in campus network environments exhibit distinct spatial distribution characteristics. The audience for specific video courses may be highly concentrated in certain areas, leading to a large number of users attempting to access the same live stream simultaneously. Utilizing a Content Delivery Network (CDN) to distribute videos in these campus scenarios creates substantial unicast pressure on edge CDN servers. This paper proposes a two-layer dynamic partitioning Recursive Bit String (RBS) virtual domain network layer multicast architecture specifically designed for large-scale, geographically dense multicast scenarios within campus networks. This approach reduces redundant multicast messages by approximately 10-30% compared to the two-layer fixed partitioning method. Additionally, it establishes multicast source authentication capabilities based on Source Address Validation Improvement (SAVI) and facilitates secure multicast group key exchange using a concise exchange protocol within the WebRTC framework. In the next-generation data plane of programmable software-defined networks, the RBS stateless multicast technology can be integrated with the unique characteristics of large-scale, geographically dense campus network scenarios to dynamically and efficiently extend multicast coverage to every dormitory.","In recent years, advancements in Internet technology and the emergence of services such as wired 4K, wireless 2K, and augmented reality/virtual reality (AR/VR) have ushered the world into an era of ""video everywhere"". Video services have evolved from singular media applications to a diverse array of interactive video applications that depend on robust network capabilities. Video courses have become an essential component of modern education. However, the growing demand for large-scale high-definition, ultra-high-definition, and even 3D video course live streaming is challenging the service capabilities of campus networks. Currently, commercial online teaching platforms often utilize video servers in conjunction with Content Delivery Networks (CDN) and unicast networks for video transmission. As the number of viewers increases, these servers must implement load balancing to effectively manage demand. Limited network bandwidth adversely affects the quality and real-time performance of video playback. In the future, an increasing number of courses will be conducted via live streaming, which will exacerbate the existing bottleneck issues related to network bandwidth. The target audience for campus video courses is typically concentrated within a few campuses, resulting in significant unicast pressure on edge CDN servers, while other CDN servers struggle to alleviate this bandwidth strain. Under the CDN video distribution architecture, streaming a 1080p (60 FPS) H.265 encoded video (with a bitrate of 5 to 12 Mbps) requires approximately 117 servers with 10 Gbps bandwidth to support around 3,333 groups of 30-person course live streams, or about 468 servers with 2.5 Gbps bandwidth. Figure 1: Class Video Live Streaming Based on CDN (1080P, 60FPS, H.265 Encoding). Network layer multicast technology is an efficient one-to-many transmission method. By sending a data packet only once, multiple receivers can receive the same data, significantly reducing network bandwidth usage and server load. However, implementing large-scale multicasting still presents several challenges, including complex and scalable network topologies, the selection of appropriate multicast protocols, device compatibility issues, and security concerns. The challenges of live video streaming for courses in campus network environments are characterized by their unique features, including large scale and dense spatial distribution. Different types of audience groups for video courses exhibit varying spatial distribution characteristics. Certain specific audience groups may demonstrate a particularly high density in their spatial distribution, such as: • The audience groups for the video course will be concentrated on 1 \sim 2 floors of 1 \sim 2 dormitory buildings; • The video course audience groups, organized by colleges, will be concentrated in several nearby dormitory buildings; • The video course will target audience groups based on grade levels, covering nearly all dormitory buildings and the majority of dormitories within those buildings; • The audience groups for video courses based on university lectures consist of a large number of individuals, with their spatial distribution being both scattered and dense. In this paper, we focus on enhancing live streaming services within campus networks and propose an IPv6+ multicast architecture designed for large-scale, geographically dense multicast scenarios. This scheme enables elastic scaling of IPv6 video multicasting within the campus network without requiring modifications to the client protocol stack. Additionally, it ensures the authenticity of multicast members and content, as well as the secure exchange of group keys."
https://arxiv.org/html/2411.06021v1,"Advanced Network Planning in 
6G Smart Radio Environments","The growing demand for high-speed, reliable wireless connectivity in 6G networks necessitates innovative approaches to overcome the limitations of traditional Radio Access Network (RAN). Reconfigurable Reconfigurable Intelligent Surface (RIS) and Network-Controlled Repeater (NCR) have emerged as promising technologies to address coverage challenges in high-frequency millimeter wave (mmW) bands by enhancing signal reach in environments susceptible to blockage and severe propagation losses. In this paper, we propose an optimized deployment framework aimed at minimizing infrastructure costs while ensuring full area coverage using only RIS and NCR. We formulate a cost-minimization optimization problem that integrates the deployment and configuration of these devices to achieve seamless coverage, particularly in dense urban scenarios. Simulation results confirm that this framework significantly reduces the network planning costs while guaranteeing full coverage, demonstrating RIS and NCR ’s viability as cost-effective solutions for next-generation network infrastructure.","The demand for data in 6G networks and the advancements in communications require new frequency bands in millimeter wave (mmW) spectra (24-100 GHz). Although offering high capacity, these bands have a limited range and are sensitive to blockages, posing challenges to coverage and reliable connectivity. Consequently, next-generation Radio Access Network (RAN) designs are evolving to address these limitations with innovative devices such as Reconfigurable Intelligent Surface (RIS) s and Network-Controlled Repeater (NCR) s, which form the basis of the emerging concept of a Smart Radio Environment (SRE) [1]. In a SRE, the environment becomes an adaptable entity capable of enhancing signal propagation. This is possible by introducing novel network devices, namely RIS and NCR. RIS is a quasi-passive device that manipulates incident electromagnetic (EM) waves, altering their direction through programmable reflection coefficients [2]. Typically structured as two-dimensional arrays of tunable meta-atoms, RIS s are deployed to strategically redirect signals and thus improve coverage, especially in urban areas where line-of-sight (LoS) paths are obstructed. In contrast, NCR is an active device with beamforming and amplification capabilities and the ability to optimize signal routing [3, 4]. Recent studies have assessed the optimal positioning and orientation of RIS s and NCR s for improved coverage in simplified scenarios. For example, positioning RIS s near the transmitter (Tx) or receiver (Rx) has been shown to mitigate path loss, as demonstrated in [5, 6]. Furthermore, studies using stochastic geometry models suggest that RIS s enhance coverage, notably in environments with high obstacle density, a common characteristic of urban settings [7]. The effect of RIS deployment density and interference on performance has also been considered, indicating that increasing the number of RIS devices requires careful planning to balance desired and interference levels of signal [8]. The performance of RIS s is further influenced by their orientation and phase configuration, which are optimized in studies to improve indoor coverage within shadowed regions [9]. Recent research on NCR s has demonstrated their effectiveness in increasing coverage, especially for cell edge users and managing interference in dense networks [10]. Comparative analyzes between RIS s and NCR s suggest that each has unique advantages depending on the specific propagation environment. While large-scale RIS s can provide high-capacity links, NCR s often perform better when real-time signal amplification is required, as shown in scenarios considering propagation and geometric constraints [11]. Network planning for Heterogeneous SRE (HSRE) deployment in real environments remains a significant challenge [12]. Previous studies often considered idealized setups, ignoring the constraints imposed by buildings, user mobility, and installation feasibility. For example, planning strategies using simplified urban layouts have shown that the jointly deploying RIS and NCR can significantly improve reliability and mitigate blockage in urban networks [13, 14, 15]. However, a comprehensive network planning strategy for realistic environments is essential to maximize coverage efficiently and minimize deployment costs. To address these gaps, this paper presents a practical network planning framework for HSRE devices, designed with realistic environmental constraints and advanced planning tools. Specifically, this work combines the physical layer considerations of [11], which includes precise channel and propagation modeling, with an optimization approach inspired by the network planning model in [14], though using a more advanced optimization method. Our model minimizes network deployment costs while ensuring full coverage, providing an enhancement over prior planning techniques. It accounts for the physical and geometric characteristics of RIS s and NCR s, leveraging realistic urban maps to capture the impact of environment-specific factors on network performance. Additionally, we incorporate the statistical effects of static blockages common to urban scenarios and design the model to adapt to dynamic blockages that may emerge in live network operations, thereby supporting robust coverage and connectivity. Notably, we assume a cost model where device costs scale with the configurations of SRE components, introducing a practical approach to analyzing deployment costs based on device specifications. Our findings reveal that optimal planning—specifically the strategic placement and tailored configuration of RIS and NCR devices—significantly reduces deployment costs while ensuring full coverage. This work demonstrates how careful planning and configuration can meet high connectivity demands in urban environments without excessive infrastructure investment. The rest of the paper is organized as follows: Section II introduces the role of HSRE in network planning. Section III details the system model, including RIS and NCR device descriptions. Section IV presents the optimization problem, specifying the cost-minimization model for full coverage. Section V discusses numerical results, deployment insights, and cost analysis. Finally, Section VI concludes the paper and suggests future research directions."
https://arxiv.org/html/2411.06015v1,Multi-hop RIS-aided Learning Model Sharing for Urban Air Mobility,"Urban Air Mobility (UAM), powered by flying cars, is poised to revolutionize urban transportation by expanding vehicle travel from the ground to the air. This advancement promises to alleviate congestion and enable faster commutes. However, the fast travel speeds mean vehicles will encounter vastly different environments during a single journey. As a result, onboard learning systems need access to extensive environmental data, leading to high costs in data collection and training. These demands conflict with the limited in-vehicle computing and battery resources. Fortunately, learning model sharing offers a solution. Well-trained local Deep Learning (DL) models can be shared with other vehicles, reducing the need for redundant data collection and training. However, this sharing process relies heavily on efficient vehicular communications in UAM. To address these challenges, this paper leverages the multi-hop Reconfigurable Intelligent Surface (RIS) technology to improve DL model sharing between distant flying cars. We also employ knowledge distillation to reduce the size of the shared DL models and enable efficient integration of non-identical models at the receiver. Our approach enhances model sharing and onboard learning performance for cars entering new environments. Simulation results show that our scheme improves the total reward by 85\% compared to benchmark methods.","As urbanization and population growth continue, the road traffic in central cities worldwide has become overloaded. To relieve traffic congestion and infrastructure expenses, automakers are exploring flying cars, which expand transportation into Near-Ground Space (NGS) [1]. Without the constraints of road infrastructure, flying cars promise faster commutes and reduced costs associated with road construction and maintenance. Urban Air Mobility (UAM) is the transportation paradigm that emerges from this development, facilitating the movement of people and goods via near-ground airspace. According to a report from Morgan Stanley [2], the UAM industry could reach a value of $1.5 trillion by 2040, comparable to the potential market size for autonomous driving. While flying cars offer the promise of efficient transportation, they also introduce significant challenges to aviation safety. The most notable challenge is the need for complex 3D environmental detection, which is essential for autonomous flying. Flying cars must accurately detect both aerial (e.g., other flying cars, drones, birds) and ground entities (e.g., pedestrians, ground vehicles) [3]. Fortunately, point cloud data generated by onboard LiDAR systems helps meet this challenge. Point clouds provide detailed geometric information about the surroundings, allowing for accurate 3-Dimensional (3D) object detection [4]. However, flying cars encounter various environments during a single journey, making it impractical to collect and process all relevant data. A feasible alternative is for vehicles to acquire well-trained DL models from local cars in the area. These models can either be used directly for detection or further retrained. However, sharing DL models between flying cars is challenging because of the large safe separation for fast aviation as well as obstacles, such as skyscrapers, that can block signal transmission [5]. Reconfigurable Intelligent Surfaces (RIS) offer a solution to these challenges. RIS is a transformative 6 Generation (6G) technology that manipulates radio waves to customize signal propagation. Each RIS element can combine and reflect signals in a specific direction, which enables better signal transmission between distant flying cars, even in environments with non-line-of-sight (NLoS) conditions [6]. However, despite the promise of RIS technology, several difficulties remain for DL model sharing in UAM. First, the onboard DL models of flying cars are often complex and contain numerous parameters, making them difficult to transmit over wireless networks. Second, different flying cars may embrace different DL model structures. For instance, flying cars in urban environments may require more complex models than those in rural areas, where there are fewer obstacles and entities to detect. These differences make it hard to directly integrate DL models in a federated learning fashion, which requires identical parameter structures [7]. To overcome these difficulties, we exploit knowledge distillation (KD) to compress DL models for transmission and facilitate the integration of non-identical models. KD allows a larger, well-trained model (the teacher) to transfer its knowledge to a smaller, lightweight model (the student) [8]. This process reduces the size of the shared DL model, enabling efficient transmission, and allows for non-identical model integration without requiring the same structure for all models. Our proposed DL model sharing scheme includes three key components: model compression, propagation, and integration. KD is used to generate a lightweight model for transmission. Multi-hop RIS technology is employed to transmit the compressed model between flying cars. Finally, the received models are integrated into the onboard DL model of the receiver using KD method. Our scheme significantly improves onboard learning performance for cars entering new environments. The main contributions of this paper are as follows: • We design a multi-hop RIS-aided DL model sharing scheme. This scheme improves long-distance communication between flying cars by reflecting signals through multi-hop RIS communication, overcoming obstacles like buildings. We also develop two phase shift optimization algorithms for RIS. One algorithm uses the Semi-Definite Relaxation (SDR) method to achieve optimal phase shifts, while the other offers a fast suboptimal solution with lower computational complexity. These options balance performance and time consumption, making the scheme adaptable for various UAM scenarios. • We propose a KD-based DL model compression and integration scheme. The local DL model is compressed into a lightweight version for transmission, significantly reducing overhead without sacrificing much performance. We design three types of propagated DL models (lightweight version) to meet different performance and transmission delay requirements. Additionally, we develop a multiple-teacher, single-student KD framework to facilitate the integration of diverse models at the receiver. • We formulate a joint optimization problem that minimizes both transmission delay and final learning performance after DL model sharing. To solve this, we propose a Block Coordinate Descent (BCD) method that iteratively optimizes the KD-based model selection and multi-hop RIS phase shift control. This approach balances transmission performance and DL model integration quality in resource-constrained systems. Simulations illustrate that our proposed algorithms outperform the benchmarks in terms of transmission rate, model sharing performance, and optimization time consumption. The remainder of this paper is organized as follows. Section II reviews related works. Section III presents the system model. Section IV proposes our solution for optimizing model propagation and integration. Section V provides simulation results and performance analysis. Finally, Section VI concludes the paper."
https://arxiv.org/html/2411.06004v1,Do Data Center Network Metrics Predict Application-Facing Performance?,"Applications that run in large-scale data center networks (DCNs) rely on the DCN’s ability to deliver application requests in a performant manner. DCNs expose a complex design and operational space, and network designers and operators care how different options along this space affect application performance. One might run controlled experiments and measure the corresponding application-facing performance, but such experiments become progressively infeasible at a large scale, and simulations risk yielding inaccurate or incomplete results. Instead, we show that we can predict application-facing performance through more easily measured network metrics. For example, network telemetry metrics (e.g., link utilization) can predict application-facing metrics (e.g., transfer latency). Through large-scale measurements of production networks, we study the correlation between the two types of metrics, and construct predictive, interpretable models that serve as a suggestive guideline to network designers and operators. We show that no single network metric is universally the best predictor (even though some prior work has focused on a single predictor). We found that simple linear models often have the lowest error, while queueing-based models are better in a few cases.","Large-scale applications are distributed across multiple machines in multiple racks, and thus their performance depends on behaviors, such as latency and throughput, of the data center networks (DCNs) where they run. These behaviors depend on multiple aspects of the underlying DCN, as well as the choices made by designers and operators. We would like to evaluate how different aspects of network design and operation affect application performance. For example, when fabric planners are choosing between DCN designs, or provisioning DCN capacity, they want to know how their choices will affect application performance. Network operators need to know whether a DCN is reaching an operating point where application performance could be hurt. Traffic-engineering optimizers need network-level goals that preserve application performance. These use cases, and others, can depend on being able to predict application performance as a function of network behavior. However, we cannot always make direct measurements of application performance, such as web page load times or banking transactions per second. The reasons include scale, data privacy, the intrusiveness of instrumenting applications to measure top-level (e.g., user-facing) performance, and our lack of knowledge of future workloads. Also, large-scale DCNs often support a complex mix of applications, which vary in how they use network, compute, and I/O. Instead, we can leverage metrics that indicate when applications are bottlenecked by the DCN, such as flow completion times (FCTs), RPC latencies, and bulk-transfer goodput; we call these application-facing performance metrics (AFMs). We can then manage a DCN to preserve service-level objectives (SLOs) stated in terms of AFMs, under the reasonable assumption that this avoids DCN-bottlenecked applications (but we could not validate that assumption in this study). This viewpoint converts our problem of directly predicting application performance into one of predicting AFMs from our knowledge of DCN design and workload characteristics, which is more practical but still difficult. The gold-standard method to predict AFMs is to run controlled experiments, and then directly measure AFMs such as flow completion times. But those experiments are often infeasible: especially at scale, it can be difficult to replicate realistic application mixes and workloads, or even obtain the necessary hardware resources. Alternatively, we could actively perturb the network to discover how this affects a production workload, but the associated risks are usually unacceptable. We could also run packet-level simulations based on application traces and record the simulated AFMs (e.g., (Zhang et al., 2021a; Zhao et al., 2023a)). However, simulations cannot always produce accurate results: First, simulators cannot always faithfully reflect the full complexity of a real DCN’s dataplane behavior. Second, realistic application traces are not always available, for reasons of scale or privacy. Third, traces capture application behavior under a specific set of network conditions, and might not represent behavior under different simulated conditions. In this paper, we explore a different approach: we show how to create relatively simple and intuitive predictive models that relate AFMs to easily-measured network-level metrics (NLMs). Most production networks are already instrumented to collect NLMs, such as link utilizations and discard rates, via mechanisms such as sFlow, SNMP, or OpenConfig. Creating models that use NLMs to predict AFMs avoids the challenges of directly collecting application-facing metrics through expensive (or infeasible) experiments. Using AFM and NLM traces for a large fraction of the applications in a set of production datacenters, we demonstrate that these models can indeed predict AFMs in many cases, with useful accuracy. They can also predict how operating a network in an overloaded regime can lead to unpredictable and hard-to-bound AFMs. Figure 1. Hypothetical latency vs. link utilization. We simplify the prediction challenge by considering network performance in three regimes (Figure 1) : lightly-loaded, where small changes in network metrics such as link utilization have little effect on AFMs; moderate-queueing, where changes in NLMs have significant but predictable impacts on AFMs; and congested, where large queuing delays and/or packet losses lead to severe and unpredictable application impacts. Stakeholders want to avoid the congested regime, and manage their tradeoffs within the other two regimes. In our approach, we first model whether a network will be in the congested regime, by detecting knees in the curves representing AFM vs. NLM, via a modified version of the Kneedle algorithm (Satopaa et al., 2011) (§5.1). Our goal for this is not to predict application performance under severe congestion, but rather to predict when congestion will set in, so that this “danger zone” can be avoided. Once we have identified the knee (if any), we then try to make predictions for the non-congested regimes. For these regimes, we create models using quantile regression (Koenker and Hallock, 2001) (§5.2, which exposes sensitivity in both linear (lightly-loaded) and non-linear (moderate-queueing) regimes of a network, and is robust to noise and outliers in the measurements. Users can add asymmetric bias §5.2.3) to steer results towards overprediction or underprediction. This paper makes the following contributions: • A method to produce trustworthy, actionable models that predict the relationships between a variety of AFMs and NLMs (§5). This methodology is transferable and can be applied by DCN designers and operators using their own datasets. • An in-depth case study (§6) on a real-world fabric, validated on data from multiple production fabrics (§7). • We identify which network metrics are good predictors, and show that NLMs and AFMs are correlated even in non-congested regimes. • We further validate our approach on data from multiple production fabrics (§7). • We empirically show that no single NLM is the best predictor for the AFMs studied (§6.2.1)."
https://arxiv.org/html/2411.07179v1,"Joint Age-State Belief is All You Need: 
Minimizing AoII via Pull-Based Remote Estimation","Age of incorrect information (AoII) is a recently proposed freshness and mismatch metric that penalizes an incorrect estimation along with its duration. Therefore, keeping track of AoII requires the knowledge of both the source and estimation processes. In this paper, we consider a time-slotted pull-based remote estimation system under a sampling rate constraint where the information source is a general discrete-time Markov chain (DTMC) process. Moreover, packet transmission times from the source to the monitor are non-zero which disallows the monitor to have perfect information on the actual AoII process at any time. Hence, for this pull-based system, we propose the monitor to maintain a sufficient statistic called belief which stands for the joint distribution of the age and source processes to be obtained from the history of all observations. Using belief, we first propose a maximum a posteriori (MAP) estimator to be used at the monitor as opposed to existing martingale estimators in the literature. Second, we obtain the optimality equations from the belief-MDP (Markov decision process) formulation. Finally, we propose two belief-dependent policies one of which is based on deep reinforcement learning, and the other one is a threshold-based policy based on the instantaneous expected AoII.","Age of information (AoI) metric has recently been proposed to capture information freshness in remote estimation problems [1]. The AoI metric quantifies information freshness by a monitor which keeps track of how long ago the latest received information packet in the system had been generated. However, it is argued in [2] that AoI may fall short of capturing freshness in certain estimation problems since it does not consider the dynamics of the sampled process since even though the latest received packet may have been generated a long time ago, it is possible that the source may not have changed since then, and therefore, the packet can still be fresh. Similarly, a recently received packet may contain stale information if the source has already changed its state after the packet was generated. Stemming from this drawback of AoI, [2] proposes an alternative freshness metric, namely age of incorrect information (AoII) that penalizes the mismatch between the source and its estimation over time, and regardless of when it is sampled, it defines the estimation as fresh if it is the same as the source. Another interesting feature of AoII in contrast to AoI is that the monitor is not required to get a new sample to bring the age down to zero since the mismatch condition between the source and the monitor may as well be brought to end with a transition of the source to the estimated value at the monitor. In this paper, we consider the following AoII minimization problem in which an information source observes a DTMC process, and a remote monitor estimates the process from the updates received by the monitor from the source. We consider a pull-based scheme such that the source transmits its current state whenever a pull request arrives at the source, and the transmission is completed in the next time slot. The monitor updates its estimation by considering the source dynamics with the MAP estimator. Notice that the monitor does not have full information on AoII including the very same time slot the most recent update is received. Additionally, we consider a sampling rate constraint on the monitor that limits the average number of pull requests it can send. Therefore, we aim to find an AoII-minimizing policy at the monitor with the timely generation of pull requests based on partial observations. Generally, AoII is investigated for symmetric Markov chains, and a single threshold policy is proposed to minimize the average AoII [2, 3, 4]. Additionally, in these works, the latest received information is used, termed as the martingale estimator [5], since it would be optimum only if the source were a martingale. On the other hand, in our previous works [6, 7, 8], we have shown that if the source process is a general asymmetric Markov chain, a simple threshold policy would not be guaranteed to perform optimally. More specifically, in [7, 8] it is shown that the optimum transmission policy should take into account all the estimation, source, and age values. Similarly, it was shown in [6] that the monitor can reduce the average AoII value if the pull request rates are allowed to be dependent on the latest received information. However, in that work, we have considered a preemption mechanism at the transmitter which aborts the transmission of the current information packet if the source process changes before the packet is received. Similarly, the pull-based AoII minimization problem in [4] considers an immediate transmission that allows the monitor to keep track of AoII. On the other hand, in the system model here, the decision maker (the monitor) has no direct information on the source process and hence the current AoII. In this paper, a sufficient statistic corresponding to their joint probability distribution under the MAP estimation rule is derived. Therefore, this paper motivates to obtain policies based on this distribution. The closest MDP formulation to our problem is the partially observable Markov decision process (POMDP) formulation that considers states which are not observable directly, but their probability distributions can be obtained with observations using partial information [9]; this distribution is called belief. The main assumption of this formulation is that an MDP can be defined from the unobserved states, and the expected reward of the system can be calculated from this distribution. Because of the curse of dimensionality of the underlying POMDP formulation, exact solutions [10, 11] for POMDPs are not tractable for problems with large state and action spaces. An approximate solution is the so-called myopic policy which selects the action that minimizes the expected reward by ignoring its effects on future rewards [12, 13]. In [12], it is shown that a myopic policy is optimum for POMDPs under certain conditions. Similarly, the Whittle index approach can be adapted for POMDPs to obtain an index policy [14, 15]. Because of the dependency of the estimator on the belief, it is no longer straightforward to use the POMDP formulation for the problem of interest. However, we can formulate our problem as a belief-MDP such that the belief in unobserved states is viewed as a fully-observable continuous-valued state of the belief-MDP, and its equivalence to a POMDP is shown in [11]. In some cases [16, 15], the belief-MDP can further be converted to an MDP with observable and finite states, and optimum policies can be obtained accordingly. Deep reinforcement learning (DRL) has recently gained popularity for solving MDPs using the exploration-exploitation trade-off [17]. Indeed, the value function for a POMDP can be approximated from the belief using DRL [18], or from the observation [19], and the action that minimizes the value function is applied as a sub-optimal policy. POMDP formulation has been used to minimize AoI in several works [15, 20, 16, 21, 22, 23, 24]. In [15, 20, 24, 22], system models involving multiple sensors and a single monitor have been studied for different scenarios. In these works, the monitor is only aware of the AoI of the sensor that successfully transmits at that time slot and estimates the AoI of other sensors based on its previous observations. In [15], [24] and [22], sub-optimal policies are obtained via the index policy, the myopic policy, and the particle filter, respectively. On the other hand, the authors of [22] convert the POMDP into a fully-observable discrete space MDP problem, and obtain an optimum policy. A similar system model has been studied in [25]. In that work, an entropy-based metric, namely uncertainty of information, is minimized by employing an index policy. Additionally, the AoI minimization problems in [21, 23, 16] consider failure status, channel availability, and the battery level as unobservable states, respectively. Under the assumption that each update includes a timestamp of the generation time, the monitor can access the correct AoI value for the time slot the update is received. On the other hand, when the delay on the channel is considered, the source process may change before the update arrives, thus no updates guarantee resetting AoII and the monitor never has the correct AoII value including the time slot an update arrives. Additionally, AoII metric depends on the dynamics of the source process, and it is upper bounded even if there is no sampling. These aspects make AoII problems different and more challenging from AoI-based formulations. The contribution of this paper can be summarized as follows: i) We propose a MAP estimator to be used at the monitor in place of the simple martingale estimator, for which the monitor updates its estimation with the MAP rule. ii) We derive a sufficient statistic, namely belief, that corresponds to the joint distribution of AoII and source state, for general asymmetric Markov chains. iii) We propose two belief-dependent policies, one of which is based on DRL, and we compare these two policies against two baseline belief-agnostic policies."
https://arxiv.org/html/2411.07168v1,Enhancing Predictive Maintenance in Mining Mobile Machinery through a TinyML-enabled Hierarchical Inference Network,"Mining machinery operating in variable environments faces high wear and unpredictable stress, challenging Predictive Maintenance (PdM). This paper introduces the Edge Sensor Network for Predictive Maintenance (ESN-PdM), a hierarchical inference framework across edge devices, gateways, and cloud services for real-time condition monitoring. The system dynamically adjusts inference locations—on-device, on-gateway, or on-cloud—based on trade-offs among accuracy, latency, and battery life, leveraging Tiny Machine Learning (TinyML) techniques for model optimization on resource-constrained devices. Performance evaluations showed that on-sensor and on-gateway inference modes achieved over 90% classification accuracy, while cloud-based inference reached 99%. On-sensor inference reduced power consumption by approximately 44%, enabling up to 104 hours of operation. Latency was lowest for on-device inference (3.33 ms), increasing when offloading to the gateway (146.67 ms) or cloud (641.71 ms). The ESN-PdM framework provides a scalable, adaptive solution for reliable anomaly detection and PdM, crucial for maintaining machinery uptime in remote environments. By balancing accuracy, latency, and energy consumption, this approach advances PdM frameworks for industrial applications.","THE mining sector is a vital component of the global resource economy, providing the raw materials essential for industrial production and infrastructure development. By 2026, the industry is projected to reach a market value of $3.36 trillion [1], driven largely by the growing demand for minerals such as lithium, cobalt, and copper, which are critical components in renewable energy technologies [2], electric vehicles [3], and consumer electronics [4]. Mining operations are inherently complex, encompassing multiple stages like exploration, extraction, processing, and transportation. To maintain efficiency and safety, the sector greatly depends on a wide range of machinery and mobile and semi-mobile equipment, including drilling rigs, shovels, excavators, haul trucks, front-loaders, and other auxiliary equipment [5]. These operations often occur in harsh, remote environments, exposing assets to extreme conditions such as high temperatures, humidity, dust, and heavy loads [6]. Prolonged exposure to these conditions leads to equipment degradation, reduced remaining useful life, increased maintenance costs, and elevated safety risks. PdM has become critical for optimizing mining operations, offering benefits such as improved system availability, cost savings, and enhanced failure prediction [7].By leveraging data from continuous condition monitoring sensors, PdM enables proactive decision-making and timely maintenance, reducing the risk of unplanned downtime [8]. The rise of Artificial Intelligence (AI) has further advanced PdM, as Machine Learning (ML) and Deep Learning (DL) algorithms can analyze vast datasets, identify patterns, and predict equipment failures more accurately than traditional methods [9]. Internet of Things (IoT) technologies, particularly Wireless Sensor Networks (WSNs), have become key components for collecting real-time data on machinery performance [10, 11, 12]. WSNs consist of spatially distributed sensor nodes and gateways that communicate wirelessly to collect and transmit data [13]. Typically, these sensor nodes are small-size, light-weight, energy-efficient, cost-effective and remarkably flexible to deploy, making them ideal for mining environments [14]. PdM frameworks are structured methodologies that encompass the entire PdM process: from data collection, preprocessing, and communication to ML and DL model development, training, and deployment. Traditional PdM frameworks often rely on a fixed inference location, either at the cloud in a dedicated serverless service (in a server on-premise far away from the operation instead) or closer to the edge on a gateway or sensor node [15, 16, 17]. Each approach has its advantages and limitations. Cloud-based inference offers superior accuracy and scalability at the cost of high latency and the need for stable network connectivity [10, 18]. Edge-based inference, both on gateways and nodes, minimizes latency and enables real-time decision-making but may increase power consumption and limited model complexity [11, 12, 19, 20]. Motivation: Each condition monitoring approach and PdM solution has its own strengths and weaknesses. Regardless of the inference approach, there is not a single technique that can detect, diagnose and predict all types of faults optimally. This call for more flexible and adaptive solutions for faster and accurate maintenance predictions under uncertainty in operational conditions. In this context, the inference location is critical to ensure high the system’s performance. The inference location in a hierarchical inference network with several levels directly determinates both the speed and accuracy of detected events. A fixed inference location may be inadequate due to dynamic changes in operational conditions over time. For instance, different expertise of machinery operators or mining fronts with different shape and leveling, expose equipment to different stress and wear levels. By leveraging on-cloud, on-gateway, and on-device inference capabilities, the system dynamically can adjusts inference locations based on trade-offs between real-time demands and conditions such as accuracy, latency, and battery range. By adapting the inference location dynamically, the PdM can be optimized the condition monitoring process, leveraging cloud resources when accuracy is critical and shifting to edge computing when real-time decision-making is mandatory. This paper presents a PdM framework that integrates edge inference approaches (such as on-gateway and on-sensor-based inference) and cloud computing services into an unified hierarchical inference system to enhance real-time condition monitoring of heavy machinery. The proposed framework leverages the strengths of each inference level to provide real-time and energy-efficient condition monitoring, adapting the inference location based on operational demands and conditions such as latency, accuracy, and energy consumption. The ESN-PdM system is evaluated through a case-study in a real-world industrial scenario, where vibration data from mining equipment is used to evaluate the operational state of the machinery and triggering alarms when anomalies arise. The main contributions of this work are as follows: • An open-source, end-to-end framework for condition monitoring and PdM of mobile mining machinery in non-stationary operations. • A novel adaptive inference mechanism that dynamically updates the inference location for a node based on operational conditions. • A guide on how to use state-of-the-art TinyML optimization approaches to achieve optimal accuracy and model compression for efficient deployment of DL models on limited hardware resources of IoT edge devices. • A comprehensive evaluation of the proposal in terms of operational status classification accuracy, inference latency, and node energy consumption through a real-world industrial case-study. This paper is organized as follows: Section II discusses about the applicability of time-varying classification of multivariate time series from mechanical and/or electrical systems, and addresses the further implementation challenges on TinyML and PdM frameworks. Section III presents the proposed ESN-PdM framework, describing its architecture, components, and adaptive inference mechanisms. Section IV introduces a case study based on DL strategies for PdM. In Section V, the ESN-PdM framework is evaluated in terms of classification accuracy for PdM, inference latency, and energy consumption. Finally, Section VI presents a summary of main findings and conclusions, and outlines future research directions."
https://arxiv.org/html/2411.06490v1,Hermes: A Large Language Model Framework on the Journey to Autonomous Networks,"The drive toward automating cellular network operations has grown with the increasing complexity of these systems. Despite advancements, full autonomy currently remains out of reach due to reliance on human intervention for modeling network behaviors and defining policies to meet target requirements. Network Digital Twins (NDTs) have shown promise in enhancing network intelligence, but the successful implementation of this technology is constrained by use case-specific architectures, limiting its role in advancing network autonomy. A more capable network intelligence, or “telecommunications brain”, is needed to enable seamless, autonomous management of cellular network. Large Language Models (LLMs) have emerged as potential enablers for this vision but face challenges in network modeling, especially in reasoning and handling diverse data types. To address these gaps, we introduce Hermes, a chain of LLM agents that uses “blueprints” for constructing NDT instances through structured and explainable logical steps. Hermes allows automatic, reliable, and accurate network modeling of diverse use cases and configurations, thus marking progress toward fully autonomous network operations.","Since the inception of large-scale cellular networks, researchers and the industry have aimed to automate their operation and management due to the costly and extensive human labor involved. However, communication systems are notorious for their dynamic nature, spanning from wireless channel and network load to unpredictable faults and errors that require network adaptation. Due to these characteristics, full network autonomy has not yet been achieved, and human presence in the operation loop is still prevalent at multiple levels of the hierarchical network stack. On the network autonomy scale [1], where at level 0 humans perform network operations entirely using manual procedures and level 5 refers to full network automation, current network operations lie in the middle, aiming to reach level 3 automation before 2026 [2]. Network Digital Twin (NDT) has emerged as a highly promising candidate to enhance the design, analysis, operation, automation, and intelligence of future mobile networks [3]. However, the impact of this technology toward full network autonomy is limited by the current design approach of NDTs where different use cases are mapped to distinct NDT architectures [4], as processing different types of data and modeling and optimizing distinct network functionalities within a unique piece of software is extremely complex. To break this barrier and take autonomy beyond this level, a more capable type of network intelligence is needed, which encompasses extensive knowledge about network operations and functionalities, and can streamline these operations – essentially functioning as a telecommunications brain. The notion of a telecommunications brain has been utilized in the literature before to refer to a large-scale intelligent entity capable of understanding the intricacies of the network, the various cause-and-effect relationships in its functionalities, and the ability to plan and predict the network behavior in advance. This intelligence continuously monitors the network state, promptly reacts to any unforeseen changes, and seamlessly adapts the network operations to new scenarios as they arise. Arriving at such a realm of telecommunications brain is an ambitious goal that would elevate network autonomy to new heights. Although the end goal is clear, the path to realizing a telecommunications brain is still an open challenge. Then, Large Language Models have emerged, revolutionizing the Artificial Intelligence (AI) field, especially Natural Language Processing (NLP), by propelling text generation, comprehension, and interaction to unprecedented levels of sophistication. Promptly, researchers in the network domain have identified LLMs as key enablers to pave the way to the telecommunications brain [5]. Particularly, researchers envisioned a realm where LLMs take over the driving seat of network operations and management. However, to this day, the application of LLMs in the telecommunications domain has been mostly successful as human add-ons, such as Retrieval Augmented Generation (RAG) systems [6] that fetches Third Generation Partnership Project (3GPP) standards information and conversational chatbot tools for wireless communication specifications [7]. There have also been recent successful implementations of LLMs in embedding network commands, such as setting the transmit power of a base station to a specific value, into actionable configuration files for network management [8]. In [9], the authors propose an LLM-based multi-agent framework designed to convert user requests into optimized power scheduling vectors by selecting from a set of predefined equations and solvers the most suitable for the intended task. These lines of work leverage the alignment between such translation tasks and the natural language proficiency of LLMs. In another line of work, to overcome the limitations of LLMs in the network domain, researchers advocated for multi-modal LLMs trained on wireless signals and network measurements to augment their capabilities [10]. However, creating such large multi-modal models presents significant challenges. These challenges include the need for extensive datasets of measurements and wireless signals, often proprietary to operators and vendors, the complexity of integrating diverse modalities, and the inherent weakness of LLMs in managing numerical operations and relationships [11]. Therefore, despite the current research hype around LLMs, the question remains open: do LLMs truly hold the key for achieving the so-called telecommunications brain and leading to full autonomy in telecommunication networks? This work aims to address this question through a multi-step approach. As a first step, we posit that, fundamentally, an LLM can be considered inching closer to becoming a telecommunications brain if it can grasp the causal relationships between network components, configurations, parameters, and their impact on network performance. In other words, this capability would allow LLMs to construct end-to-end network models– NDTs– to effectively handle new environments and unseen circumstances. Moreover, we demonstrate that, today, the most powerful LLMs, e.g., GPT-4o, struggle to perform well on understanding and modeling the network behavior, even when using advanced prompting techniques like chain-of-thought [12]. We shed light on the common pitfalls LLMs fall into and the typical mistakes they make. Our findings illustrate that despite their impressive capabilities, current LLMs are far from being autonomous agents capable of taking the driving seat for telecommunications network management and operations on their own. To address these pitfalls, we introduce Hermes: a comprehensive chain-of-agents LLM framework that tackles network modeling and automation through the elaboration of “blueprints” of NDTs. In this context, a blueprint is a set of step-by-step logical blocks autonomously designed and coded by the LLMs using their parametric knowledge of the telecommunications domain, rather than relying on the direct interpretation of network measurements as multi-modal models do [10]. By incorporating key components such as self-reflection steps and feedback mechanisms, along with a granular step-by-step logical approach, Hermes ensures the validity of these blueprints and their associated code to realize a NDT tailored to the tasked intent. We demonstrate how leveraging the blueprints of NDT significantly increases the reliability of the LLM in addressing diverse network modeling tasks, resulting in a more robust comprehension of network dynamics and operations."
https://arxiv.org/html/2411.06042v1,Personalized Hierarchical Split Federated Learning in Wireless Networks,"Extreme resource constraints make large-scale machine learning (ML) with distributed clients challenging in wireless networks. On the one hand, large-scale ML requires massive information exchange between clients and server(s). On the other hand, these clients have limited battery and computation powers that are often dedicated to operational computations. \Acsfl is emerging as a potential solution to mitigate these challenges, by splitting the ML model into client-side and server-side model blocks, where only the client-side block is trained on the client device. However, practical applications require personalized models that are suitable for the client’s personal task. Motivated by this, we propose a personalized hierarchical split federated learning (PHSFL) algorithm that is specially designed to achieve better personalization performance. More specially, owing to the fact that regardless of the severity of the statistical data distributions across the clients, many of the features have similar attributes, we only train the body part of the federated learning (FL) model while keeping the (randomly initialized) classifier frozen during the training phase. We first perform extensive theoretical analysis to understand the impact of model splitting and hierarchical model aggregations on the global model. Once the global model is trained, we fine-tune each client classifier to obtain the personalized models. Our empirical findings suggest that while the globally trained model with the untrained classifier performs quite similarly to other existing solutions, the fine-tuned models show significantly improved personalized performance.","I Introduction Given the massive number of wireless devices that are packed with onboard computation chips, we are one step closer to a connected world. While these devices perform many onboard computations, they usually have limited computational and storage resources that can be dedicated to training ML models. Conversely, cloud computing for ML models raises severe privacy questions. Among various distributed learning approaches, FL [1] is widely popular as it lets the devices keep their data private. These distributed learning algorithms are not confined to theory anymore; we have seen their practical usage in many real-world applications, such as the Google keyboard (Gboard) [2]. \Ac fl, however, has its own challenges [3], which are mostly the results of diverse system configurations, commonly known as system heterogeneity, and statistical data distributions of the client devices. On top of these common issues, varying wireless network conditions also largely affect the FL training process when the devices are wireless: the model has to be exchanged using the wireless channel between the devices and server(s). While such difficulties are often addressed jointly by optimizing the networking and computational resources (e.g., see [4, 5] and the references therein), traditional FL may not be applied directly in many practical resource-constrained applications if the end devices need to train the entire model [6]. \Ac sl [7] brings a potential solution to the limited-resource constraints problem by dividing the model into two parts: (a) a much smaller front-end part and (b) a bulky back-end part. The front-end part --- also called the client-side model --- is trained on the user device, while the bulky back-end part --- also called the server-side model --- is trained on the server. \Acsl thus can enable training extremely bulky models at the wireless network edge, which typically incurs significant computational and communication overheads in traditional FL (e.g., see [8] and the references therein). For example, as large foundation models [9] --- that can have billions of trainable model parameters --- are becoming a part of our day-to-day life and are also envisioned to be an integral part of wireless networks [10], split learning (SL) can facilitate training these large models at the wireless edge. While SL can be integrated into the FL framework [11], it still has several key challenges, particularly when the clients’ data distributions are highly non-IID (independent and identically distributed). While FL typically seeks a single global model that can be used for all users, the performance reduces drastically under severe non-IID data distributions. This is due to the fact that general FL algorithms, like the widely popular federated averaging (FedAvg) [1], may inherently push the global model toward the local model weights of a client who has more training samples: these samples, however, may not be statistically significant. A such-trained global model underperforms in other clients’ test datasets, raising severe concerns about this ‘‘one model fits all"" approach. To empirically illustrate this, we implemented a simple hierarchical split federated learning (HSFL) algorithm with 100 clients and 4 edge servers that follows the general architecture of [12] and performs 5 local epochs 3 edge rounds and 100 global rounds. The test performances, as shown in Fig. 1, show that the globally trained model yields very different test accuracies in different clients’ test datasets. Figure 1: Globally trained model’s performance on CIFAR10: 65.36\%, 83.93\% and 33.33\% mean, maximum and minimum test accuracy, respectively, across 100 clients, when data samples are distributed following \mathrm{Dir}(\boldsymbol{\alpha}=\mathbf{0.1}) [4] I-A State of the Art Many recent works extended the idea of SL [7] into variants of FL [1] algorithms [11, 13, 12, 14, 15, 16, 17]. Xu et al. proposed a split federated learning (SFL) algorithm leveraging a single server with distributed clients [11]. In particular, the authors assumed that the client-side model is aggregated only after finishing the local rounds, while the server can aggregate the server-side models in each local training round. Liu et al. proposed hybrid SFL, where a part of the clients train their entire models locally, while others collaborate with their serving base station (BS) to train their respective model following SL [13]. Ao et al. proposed SFL assuming the BS selects a subset of the clients to participate in model training [16]. In particular, [16] jointly optimizes client scheduling, power allocation, and cut layer selection to minimize a weighted utility function that strikes a balance between the training time and energy overheads. Khan et al. proposed an HSFL algorithm leveraging hierarchical federated learning (HFL) and SL [12]. In particular, the authors aimed at optimizing the latency and energy cost associated with HSFL. Liao et al. proposed a similar HSFL algorithm that partitioned clients into different clusters [14]. The authors leveraged grouping the clients into appropriate clusters to tame the system heterogeneity. However, statistical data heterogeneity is well known to cause client drifts [18]. Lin et al. proposed a parallel SL algorithm that jointly optimized the cut layer splitting strategy, radio allocation, and power control to minimize per-round training latency [17]. As opposed to SFL, parallel SL did not aggregate client-side models. On the personalized SFL side, Han et al. proposed weighted aggregation of the local and global models during the local model synchronization phase [19]. Similar ideas were also explored in [20], where the authors mixed partial global model parameters with the local model parameters during the local model synchronization. However, [20] did not explore SL. Chen et al. proposed a 3-stage U-shape split learning algorithm [21]. More specifically, the authors divided the model into front, middle and back parts, where the clients retained the front and back parts, while the server had the bulky middle part. Such U-shape architecture incurs additional communication burden compared to [19]. I-B Research Gaps and Our Contributions The existing studies [11, 13, 12, 14, 15, 16, 17] considered SFL, HSFL and parallel SL extensively without addressing the need for personalization. While [19, 21] addressed joint personalization and split FL, these works were based on a traditional single server with distributed clients case. Therefore, weighted aggregation in multi-tier/hierarchical networks may lose personalization ability if the learning rate is not significantly low. Moreover, training the entire model is proven to have poor personalization capability [22]: even though the model was split into client-side and server-side parts, all model blocks on both sides of the models were updated in [19, 21]. Motivated by the above facts, we propose a PHSFL algorithm that integrates HFL and SL. The designed algorithm lets distributed clients train only the body part of the ML model to learn feature representations while keeping the output layer (e.g., the classifier) frozen during the training process inspired by the fact that globally trained model works great for generalization, while performs poorly for personalization. Besides, we perform extensive theoretical analysis to find the theoretical bound of the average global gradient norm of the global loss function. Our simulation results suggest that while the global trained model performs similarly to HSFL in generalization, with the similarly fine-tuned models for both cases, our proposed solution achieves significantly better personalization performance."
