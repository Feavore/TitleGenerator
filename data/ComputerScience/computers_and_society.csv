URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10225v1,Virtual Reality in Teacher Education: Insights from Pre-Service Teachers in Resource-limited Region,"This study explores the perceptions, challenges, and opportunities associated with using Virtual Reality (VR) as a tool in teacher education among pre-service teachers in a resource-limited setting. The research specifically addresses (1) how participation in a VR-facilitated lesson influences pre-service teachers’ perceptions of VR in education and (2) the unique challenges and opportunities identified by these pre-service teachers regarding VR’s use in education. Utilizing a qualitative case study design, the study draws on the experiences and reflections of 36 Ghanaian pre-service early childhood educators who engaged with VR in a facilitated lesson for the first time. Findings reveal that initial exposure to VR generated a positive perception, with participants highlighting VR’s potential as an engaging and interactive tool that can support experiential learning. Notably, many participants saw the VR-facilitated lesson as a promising alternative to synchronous online learning, particularly for its ability to simulate in-person presentations. They believe VR’s immersive capabilities could enhance both teacher preparation and learner engagement in ways that traditional teaching often does not, especially noting that VR has the potential of addressing expensive educational field trips. Despite these promising perceptions, participants identified key challenges, including limited infrastructure, unreliable internet connectivity, and insufficient access to VR equipment as perceived challenges that might hinder the integration of VR in a resource-limited region like Ghana. These findings offer significant implications for educational policymakers and institutions aiming to leverage VR to enhance teacher training and professional development in similar contexts to consider addressing the perceived challenges for successful VR integration in education. We recommend further empirical research be conducted involving pre-service teachers use of VR in their classrooms.","As a rapidly advancing educational tool, virtual reality (VR) holds significant potential to transform teaching practices, especially within teacher education. With its immersive and interactive capabilities, VR fosters experiential learning that extends beyond traditional instructional methods, allowing pre-service teachers to engage deeply with realistic, simulated teaching environments [1, 2]. In teacher education, this interactivity can be particularly impactful, as VR enables pre-service teachers to navigate complex classroom dynamics and apply pedagogical strategies in a risk-free, controlled setting, enhancing their engagement and preparedness for real-world teaching [3, 4]. However, despite the promise VR holds, its application in teacher education within developing countries, such as Ghana, remains underdeveloped, with limited exposure and resources to support its implementation [5]. The limited application of VR in Ghanaian teacher training reveals critical gaps, including a lack of foundational exposure to immersive technology, insufficient infrastructure, and an absence of systematic training programs to support educators in integrating VR meaningfully into their practices [6, 7]. While existing studies underscore VR’s benefits in enhancing engagement and competency development among trainee teachers [8], they also highlight that these benefits are contingent on institutional support, cultural relevance, and the availability of technical resources [9]. These barriers are particularly pronounced in resource-limited settings, where access to reliable internet and VR equipment is often constrained, posing challenges to VR’s sustained use and effectiveness [8, 6]. This study seeks to explore the perceptions and perceived challenges of PSTs in resource-limited regions who participated in a VR-facilitated lesson for the first time. In doing so, the study contributes to the growing body of research on VR in education, highlighting critical considerations for expanding VR’s accessibility and effectiveness in global teacher training contexts. The study was guided by the following research questions: 1. How does participation in a VR-facilitated lesson influence pre-service teachers’ perceptions of VR in education in resource-limited regions? 2. What key challenges and opportunities do pre-service teachers perceive in using VR in education in resource-limited regions?"
https://arxiv.org/html/2411.10142v1,First Steps towards K-12 Computer Science Education in Portugal — Experience Report,"Computer scientists Jeannette Wing and Simon Peyton Jones have catalyzed a pivotal discussion on the need to introduce computing in K-12 mandatory education. In Wing’s own words, computing represents a universally applicable attitude and skill set everyone, not just computer scientists, would be eager to learn and use.The crux of this educational endeavor lies in its execution. This paper reports on the efforts of the Ensico association to implement such aims in Portugal. Starting with pilot projects in a few schools in 2020, it is currently working with 4500 students, 35 schools and 100 school teachers. The main aim is to gain enough experience and knowledge to eventually define a comprehensive syllabus for teaching computing as a mandatory subject throughout the basic and secondary levels of the Portuguese educational system.A structured framework for integrating computational thinking into K-12 education is proposed, with a particular emphasis on mathematical modeling and the functional programming paradigm. This approach is chosen for its potential to promote analytical and problem-solving skills of computational thinking aligned with the core background on maths and science.","Computer scientists Jeannette M. Wing (USA) and Simon Peyton Jones (UK) have catalyzed a pivotal discussion on the need to introduce a K-12 computing syllabus into mandatory education (Wing, 2006; Kolling et al., 2013). This trend was galvanized by Wing’s influential paper, ’Computational Thinking’ (CT), in which she posits that computing ’[…] represents a universally applicable attitude and skill set everyone, not just computer scientists, would be eager to learn and use.’ However, the crux of this educational endeavor lies not in its justification but in its execution, and the literature does not yet show consensus on the broader inclusion of Computer Science (CS) in mandatory K-12 education (Webb et al., 2017; Xu et al., 2023; Vegas and Fowler, 2020). Quoting (Sun et al., 2021): […] however, there is no unified conclusion on how to design programming activities to promote the acquisition of CT skills more effectively. Ensico (Ensico Association, 2024) is a private, non-profit association founded in Portugal (2019) to address the challenge mentioned above that believes that incorporating computing into the first twelve years of education can promote equity and enhance scientific and technological literacy. Moreover, this has the potential to foster creativity and multidisciplinary learning, as well as to improve oral and written communication skills, understanding of formal concepts, and awareness of scientific history. As a member of the Informatics for All coalition, Ensico follows the Informatics Reference Framework (for All, 2022), which advocates that […] informatics should exist as a discipline at all stages of the school curriculum, starting early in primary school and continuing to exist and develop through upper secondary school. Moreover, we suggest that education in informatics should be compulsory for all pupils from primary through secondary education, having a status and standing similar to that of language and mathematics. Well-educated teachers and teacher-teachers are essential to realise this vision. Starting with pilot projects in a few schools, Ensico is currently working with 4500 students, 35 schools and 100 school teachers. Its main aim is to gain enough experience and knowledge to eventually define a comprehensive program for teaching computing as a mandatory subject throughout the basic and secondary levels of the Portuguese educational system. However, this raises some questions: should the focus be on technology or science, programming or general computing, concepts or procedures, formative or informative education, training or entertainment? This paper aims to address these pivotal questions and propose a structured framework for integrating computational thinking into K-12 education, with a particular emphasis on mathematical modeling and the functional programming paradigm. This approach is chosen for its potential to promote the analytical and problem-solving skills that are central to computational thinking, naturally aligned with the core background on maths and science. Although the bulk part of Ensico’s activity takes place in Portugal, an international arm was developed in parallel between 2020 and 2023 through the ERASMUS+ project CS4All (for All, 2023), in partnership with The Open University, Cisco ASC in the UK and Colectic in Spain. The remainder of this paper is structured as follows: section 2 briefly addresses basic principles concerning the overall teaching plan. Details about Ensico’s efforts to frame itss plan into the Portuguese K-12 educational system are given in section 3. The efforts since 2020 to fulfill this plan are presented in section 4. Section 5 provides details about classes and their different teaching styles. The organization of events and Ensico’s presence on social media are the subject of section 6. Finally, section 8 concludes and gives an outlook for future activities."
https://arxiv.org/html/2411.09710v1,An IoT Based Smart Waste Management System for the Municipality or City Corporations,"The population of the urban areas is increasing daily, and this migration is causing serious environmental pollution. A larger population is creating pressure on the municipality’s waste management and the city corporations of developing countries such as Bangladesh, further threatening human health. New generation technologies, such as the Internet of Things (IoT)-based waste management systems, can help improve this serious issue. IoT-enabled smart dustbins and mobile applications-based interactive management can effectively solve this problem. In this article, we combine these two technologies to offer an acceptable solution to this problem. The proposed waste management model enables smart dustbins to communicate with waste collectors or waste control centers whenever it is necessary. Additionally, city dwellers can use mobile applications to report their observations in their neighborhoods. As a result, both sensors and humans are involved directly in the development loop. We have conducted a detailed survey to study the acceptance of such a system in the community and received some encouraging results.","The waste that the tumid population produces is primarily to blame for the environmental pollution of the urban environment. Food waste is one of them. It is increasing the environmental and health risk of urban people [1, 2]. We have to spend a huge amount of money on removing this urban waste every year. It has become difficult to remove this excessive waste from people’s unconsciousness. For this, experts are looking for a smart solution to remove waste. From the World Bank’s report every year 1.3 billion tons of household waste is generated. And it will be 2.2 billion tons in 2025 [3]. Our era is technology-based and at present, the internet is very common in Bangladesh. The price of the internet is decreasing day by day and the use of the internet is much easier than before. So, an interconnected system will be a good solution for a waste management system. Interconnected systems may decrease cost and time. This will make waste management more efficient and convenient. For an increasing number of people, extra waste is being produced. But waste management system capability has not increased for this growing population. As compared to the population, our waste management system manpower is very low and there is a shortage of garbage bins and vehicles. As a developing country, Bangladesh can provide a minimum amount of money for the waste management system. But most of them are used for buying vehicles or paying the waste collector. So our waste management department manages some open space garbage locations. It is very unhygienic as most of the waste of our country is rotting garbage. As most of the garbage bins are open so easily it pollutes the air. So a large number of people are suffering from air pollution diseases. So we need a proper and efficient waste management system. Now we are living in the era of IoT. A great revelation has occurred in the industrial and residential sectors through IoT service. People are moving to IoT devices [4]. IoT-based engineering and scientific application is also involved [5]. Smart city-based applications are one of them. There is no alternate smart city-based IoT application for giving services and monitoring an upgrowing population [6]. Figure 1: Smart waste management system Governments of every country are trying to make everything digitized and smart through IoT services. Because IoT is a trendy technology in recent times [7]. IoT-based monitoring systems have become popular all over the world. Not only for home or office management systems but it is also used broadly in development for environmental monitoring systems [8, 9]. But our South Asian countries are still backward. In every country, the Internet is available in any place and cheaper than before. And today most of people are using the internet [10]. So, we made a smart system for waste management using IoT, which can be easily implemented. Our system may be a bit costly but it is very sustainable and efficient. And it can lead any country’s next vision SMART CITY one step ahead (Figure 1). Figure 2: Working flow chart of a smart dustbin. In this paper, we present a smart waste management model for the municipality and the city corporations where IoT-enabled sensors collect garbage status from the dustbins, and city dwellers can also participate in improving their neighborhood status. The rest of the paper is organized as related works in section II, proposed system architecture in section III, system implementation and experimental analysis in section IV, and the conclusion in the final section."
https://arxiv.org/html/2411.10294v1,Static network structure cannot stabilize cooperation among Large Language Model agents,"Large language models (LLMs) are increasingly used to model human social behavior, with recent research exploring their ability to simulate social dynamics. Here, we test whether LLMs mirror human behavior in social dilemmas, where individual and collective interests conflict. Humans generally cooperate more than expected in laboratory settings, showing less cooperation in well-mixed populations but more in fixed networks. In contrast, LLMs tend to exhibit greater cooperation in well-mixed settings. This raises a key question: Are LLMs about to emulate human behavior in cooperative dilemmas on networks? In this study, we examine networked interactions where agents repeatedly engage in the Prisoner’s Dilemma within both well-mixed and structured network configurations, aiming to identify parallels in cooperative behavior between LLMs and humans. Our findings indicate critical distinctions: while humans tend to cooperate more within structured networks, LLMs display increased cooperation mainly in well-mixed environments, with limited adjustment to networked contexts. Notably, LLM cooperation also varies across model types, illustrating the complexities of replicating human-like social adaptability in artificial agents. These results highlight a crucial gap: LLMs struggle to emulate the nuanced, adaptive social strategies humans deploy in fixed networks. Unlike human participants, LLMs do not alter their cooperative behavior in response to network structures or evolving social contexts, missing the reciprocity norms that humans adaptively employ. This limitation points to a fundamental need in future LLM design—to integrate a deeper comprehension of social norms, enabling more authentic modeling of human-like cooperation and adaptability in networked environments.","Over the last few years, the advances in artificial intelligence have ignited hopes of new methods for the behavioral and social sciences[1, 2]. In particular, chatbots powered by so-called large language models (LLM)[3] are believed to be able to emulate humans in conversations well enough to eventually replace them in experiments[4]. Even though experiments with human participants are ultimately needed to advance the behavioral and social sciences, there are expectations AI agents could aid in experimental design and provide experimental agents with programmable behavior (so-called “digital twins”) [1, 2]. LLMs are trained on extensive datasets of human-generated text and semantic knowledge from various societies [1, 4]. These models operate as conditional probability distributions, where altering the context or narrative can steer them toward more desirable outcomes by influencing the likelihood of specific responses while reducing others [4]. Consequently, LLMs are highly skilled at following instructions and embodying assigned personalities [5]. When given personalities, LLMs can display traits that resemble human nature, almost as if they possess their mind [6, 7]. This training process can also improve LLMs’ understanding and reasoning regarding cooperation, defection, and balancing individual and collective interests [8]. Recent behavioral experiments have shown that LLM agents can effectively substitute human participants in certain contexts, particularly within Western, Educated, Industrialized, Rich, and Democratic (WEIRD) societies [4]. However, this substitution does not extend to other cultural contexts [9]. Beyond cultural differences, within the same cultures, individuals’ social and strategic preferences depend on the network they are part of [10, 11, 12, 13]. Individuals can infer underlying network structures and adapt their social behavior, even without a complete overview of the network. Through interactions with neighbors and social learning, individuals discern the network they are part of, and this structure significantly influences their social behavior. A fascinating question arises: can large language models discern these network structures and adjust their behavior similarly to humans? This inquiry becomes especially intriguing when we consider their potential to help solve societal challenges, like promoting cooperation in social dilemmas. For this vision to come to life, it is crucial that AI agents consistently demonstrate behavior akin to that of humans. We have some understanding of LLMs’ capabilities in repeated prisoners’ dilemma games in well-mixed populations [5, 14, 8, 15, 16, 17], but we lack insight into how they perform in network settings. There, individuals consider not only strategies but also their interactions with specific network members [18, 19]. Previous research has shown that individuals can establish cooperation when they have the opportunity to adjust their social ties based on their experiences with neighbors [19]. Interestingly, it has been found that in a prisoner’s dilemma game, individuals can achieve stable cooperation within specific static network structures when the benefit-to-cost ratio exceeds the number of connections. However, the same parameter settings do not foster cooperation in a well-mixed population. The findings suggest that when individuals are aware of their neighborhood and the consequences of their cooperative actions, they can adapt their strategies based on the success of their neighbors, leading to stable cooperation even in networks with a few defectors [13]. In this study, we investigate whether LLMs can reliably adjust their social behavior in response to the network structure they operate within, as humans do. We study the capabilities of LLM agents, as of 2024, in the context of networked social dilemmas [18, 20, 5]. These stylized situations are designed to explore how humans prioritize between short-sighted egoistic and long-term prosocial choices. Such dilemmas have a wide range of social science applications, from behavioral economics, which explores questions like how to best incentivize people to follow policies, to anthropology and evolution, which search for the unique behavioral mechanisms behind human cooperation among known lifeforms."
https://arxiv.org/html/2411.10080v1,"Understanding The Effect Of Temperature On Alignment With Human
Opinions","With the increasing capabilities of LLMs, recent studies focus on understanding whose opinions are represented by them and how to effectively extract aligned opinion distributions. We conducted an empirical analysis of three straightforward methods for obtaining distributions and evaluated the results across a variety of metrics. Our findings suggest that sampling and log-probability approaches with simple parameter adjustments can return better aligned outputs in subjective tasks compared to direct prompting. Yet, assuming models reflect human opinions may be limiting, highlighting the need for further research on how human subjectivity affects model uncertainty.","Large Language Models (LLMs) have shown notable skills across a diverse range of tasks. Their increasing prevalence underscores the need for effectively modeling and representing a diverse range of human values and perspectives [1]. Several studies investigated the extent to which LLMs represent multiple human opinions [2, 3, 4, 5, 6, 7]. While studies [4] and [5] explored whose opinions are reflected by LLMs on a subjective opinion QA task, study [7] assessed their capability to capture human opinion distributions insubjective classification tasks. In [6] and [3] on the other hand, the authors analysed whether LLMs align with human opinion distributions in NLI (natural language inference) tasks. Recent studies observed misaligned LLM and human opinion distributions [5, 6, 7]. Others found methods that improve the alignment with human distributions [3, 2]. However, none of the studies examined the effect of temperature. We consider that allowing models to chose beyond the maximum likelihood token (0 temperature) may reveal more diverse opinions that could align with broader human views."
https://arxiv.org/html/2411.09856v1,InvestESG: A multi-agent reinforcement learning benchmark for studying climate investment as a social dilemma,"InvestESG is a novel multi-agent reinforcement learning (MARL) benchmark designed to study the impact of Environmental, Social, and Governance (ESG) disclosure mandates on corporate climate investments111Github repo: https://github.com/yuanjiayiy/InvestESG. Supported by both PyTorch and GPU-accelerated JAX framework, the benchmark models an intertemporal social dilemma where companies balance short-term profit losses from climate mitigation efforts and long-term benefits from reducing climate risk, while ESG-conscious investors attempt to influence corporate behavior through their investment decisions. Companies allocate capital across mitigation, greenwashing, and resilience, with varying strategies influencing climate outcomes and investor preferences. Our experiments show that without ESG-conscious investors with sufficient capital, corporate mitigation efforts remain limited under the disclosure mandate. However, when a critical mass of investors prioritizes ESG, corporate cooperation increases, which in turn reduces climate risks and enhances long-term financial stability. Additionally, providing more information about global climate risks encourages companies to invest more in mitigation, even without investor involvement. Our findings align with empirical research using real-world data, highlighting MARL’s potential to inform policy by providing insights into large-scale socio-economic challenges through efficient testing of alternative policy and market designs.","Climate change poses a persistent threat to global stability, with droughts, storms, fires, and flooding becoming more intense and frequent (Field et al., 2012), leading to disruption of the natural ecosystem and significant impacts on the global economy. Addressing climate change requires coordinated efforts across multiple sectors, particularly from large corporations, which are reportedly responsible for over 70% of global industrial greenhouse gas emissions (Griffin and Heede, 2017). While adaptation—preparing for the inevitable consequences of climate change—tends to be party-specific and often driven by financial incentives, mitigation—reducing emissions—presents a de facto social dilemma (Leibo et al., 2017), where the benefits of reduced emissions are shared globally yet the costs are borne locally (Olson Jr, 1971; Dahlman, 1979; Buchanan and Stubblebine, 2006). As corporations are inherently self-interested, they are unlikely to reduce emissions voluntarily without external incentives or regulations. Numerous policies have been proposed to address this challenge. Among these, mandatory Environmental, Social, and Governance (ESG) disclosures have recently been hotly debated in the United States. The Securities and Exchange Commission’s (SEC) proposal, which would require publicly traded companies to disclose climate-related risks, mitigation strategies, and greenhouse gas emissions from their operations, has attracted over 15,000 comments, making it one of the most contentious proposals in the SEC’s history (SEC, 2024a; CNBC, 2024). This has resulted in an indefinite delay in enactment of the policy to allow for further discussion (SEC, 2024b). The US is not alone in facing such a pushback; similar delays are unfolding in the European Union and Korea (Bloomberg News, 2024; Korea Economic Daily, 2023). This highlights the need for thorough research to effectively inform the design and implementation of these policies. Traditional economics and policy research relies on either empirical analysis—which does not enable testing possible new policies (Doshi et al., 2013; Li and Wu, 2020; Krueger et al., 2021)—or theoretical economics models, which are often limited to scenarios with only two agents (e.g., Friedman et al. 2021), or single-period games (e.g., Pástor et al. 2021). In contrast, Multi-Agent Reinforcement Learning (MARL) enables simulating complex interactions between multiple agents over extended time periods, under diverse hypothesized policy settings. Leveraging MARL to address large-scale socio-economic questions is a growing field (Hertz et al., 2023). Prior work has demonstrated the potential of MARL to design effective taxation schemes that enhance both equality and productivity (Zheng et al., 2021), highlighting its relevance for tackling real-world social challenges. We propose using multi-agent reinforcement learning (MARL) to explore the impact of the ESG disclosure policy. We introduce InvestESG, an open-source MARL benchmark, to examine how profit-driven corporations balance short-term profits with long-term climate investments and whether ESG-informed investor choices influence corporate behavior. The simulation involves two agent types: companies and investors. Companies allocate funds to mitigation, greenwashing, and resilience, while investors decide whom to invest in based on their preferences for financial returns versus ESG benefits. This creates an intertemporal social dilemma (Hughes et al., 2018), where both agent classes must weigh immediate and local costs against long-term and global gains. Using Schelling diagrams, we demonstrate that in a fully profit-driven environment, a social dilemma arises, but with sufficiently ESG-conscious investors with enough capital, climate mitigation becomes optimal for some or all corporations. However, if companies are able to greenwash to cheaply increase ESG scores without genuine mitigation, the environment once again becomes a social dilemma. Our experiments with a state-of-the-art MARL implementation yield findings that align with real-world empirical evidence and provide novel insights. First, a sufficient number of highly ESG-conscious investors are needed to incentivize corporate mitigation efforts. In such cases, climate-focused companies emerge and attract most climate-conscious investments, while others prioritize profit maximization. When only some investors are climate-focused, the market bifurcates, with mitigating companies aligning with conscious investors. Additionally, sharing climate risk information helps companies to increase mitigation efforts, even when investors are not present. Figure 1: The InvestESG Environment. Corporations choose how much to invest in mitigating emissions, which affects their ESG Score. Climate-conscious investors can see ESG Scores when deciding how much to invest in each company. However, companies can engage in greenwashing to inexpensively and falsely improve ESG scores without actually mitigating climate change. InvestESG is a social dilemma, where selfish, profit-motivated corporations will not invest in mitigation without further incentives, leading to increased climate risks and decreased global wealth. More broadly, we demonstrate how a MARL framework can inform policy debates in the field of climate change. Assessing the effectiveness of a policy is inherently challenging, due to the fact that policy experiments are often prohibitively expensive and impractical to conduct, and even when they are feasible, it can be extremely time-consuming. However, the findings revealed by our environment match existing empirical results, suggesting the simulation has high ecological validity. Given the urgency of addressing climate change, our work provides a new vector for studying this problem, creating a simulated environment where a broad range of regulations can be explored and tested efficiently to provide novel insights into the problem. We present InvestESG as a challenging benchmark for the machine learning community; for researchers that are interested in developing MARL algorithms that can solve social dilemmas, InvestESG represents a social dilemma environment that could have real-world impact. We will provide the code for the benchmark and agent baselines in open-source. We aim to show that MARL algorithmic design can inspire real-world actions that can be leveraged to tackle climate change."
https://arxiv.org/html/2411.09788v1,"AI-Driven Human-Autonomy Teaming in Tactical Operations: Proposed Framework, Challenges, and Future Directions","Artificial Intelligence (AI) techniques, particularly machine learning techniques, are rapidly transforming tactical operations by augmenting human decision-making capabilities. This paper explores AI-driven Human-Autonomy Teaming (HAT) as a transformative approach, focusing on how it empowers human decision-making in complex environments. While trust and explainability continue to pose significant challenges, our exploration focuses on the potential of AI-driven HAT to transform tactical operations. By improving situational awareness and supporting more informed decision-making, AI-driven HAT can enhance the effectiveness and safety of such operations. To this end, we propose a comprehensive framework that addresses the key components of AI-driven HAT, including trust and transparency, optimal function allocation between humans and AI, situational awareness, and ethical considerations. The proposed framework can serve as a foundation for future research and development in the field. By identifying and discussing critical research challenges and knowledge gaps in this framework, our work aims to guide the advancement of AI-driven HAT for optimizing tactical operations. We emphasize the importance of developing scalable and ethical AI-driven HAT systems that ensure seamless human-machine collaboration, prioritize ethical considerations, enhance model transparency through Explainable AI (XAI) techniques, and effectively manage the cognitive load of human operators.","The convergence of AI and autonomous technologies has revolutionized various industries, including defense and tactical operations. The rise of HAT can be attributed to several factors, including rapid advancements in autonomous technologies and AI [1], the increasing complexity of tasks and environments, the development of more capable autonomous systems, and the increasing availability of data and computing power [2]. As these technologies have become more sophisticated and capable, there has been a growing recognition of the potential collaborations that can be achieved by combining human cognitive abilities with the computational power and efficiency of autonomous systems [3]. The rise of modern HAT systems has also been driven by the need to address the complexities and challenges of rapidly evolving and dynamic environments. As tasks become more complex, time-sensitive, and data-intensive, the collaboration between humans and autonomous agents becomes crucial for effectively navigating and responding to these challenges. HAT is an emerging field that explores collaborative partnerships between humans and autonomous systems to perform tasks or achieve common goals [2, 4, 5, 6]. This involves a collaborative arrangement in which at least one human worker collaborates with one or more autonomous agents [2]. This collaborative approach has the potential to revolutionize how tasks are accomplished across various sectors and pave the way for a future where humans and intelligent autonomous systems will work hand in hand to tackle complex problems and achieve shared goals. HAT systems are designed to allow humans to delegate tasks to intelligent autonomous agents while maintaining overall mission control [7]. Autonomous agents, in this context, refer to computer entities with varying degrees of self-governance in decision-making, adaptation, and communication. This definition has been supported by studies conducted by the research works in [8, 9]. The integration of human cognitive capabilities with the computational power and efficiency of autonomous systems in HAT enhances performance, decision-making, and overall system capabilities. Here, we define and clarify some key concepts that are fundamental to understanding the scope and context of this study. These concepts include AI, Autonomy, Autonomous Systems, and Tactical Autonomy. By providing clear definitions and distinguishing between these terms, we aim to establish a common understanding among our readers. Autonomy. Autonomy in the context of HAT describes the ability of intelligent autonomous systems or agents to operate and make decisions independently in a team setting with varying degrees of self-governance [3, 10]. This involves a higher degree of decision-making capability in autonomous systems based on learning, adaptation, and reasoning. It is a property of a system, not a technology itself [10]. An autonomous entity can perceive, reason, plan, and act in pursuit of specific goals or objectives without constant human intervention. It is important to note that the level of autonomy can vary, ranging from fully autonomous systems that make all their decisions to semi-autonomous systems that require human input at certain points [10]. In the context of tactical autonomy, HAT involves the integration of autonomous capabilities into tactical operations. This integration can include various applications, such as using autonomous systems to gather intelligence, perform surveillance, and perform other critical activities. Autonomy enables systems to operate in complex and uncertain environments, learn from experience, and make decisions without explicit human intervention in every scenario. However, it is important to distinguish this from traditional automation, which typically follows pre-programmed rules, decision trees, or logic-based algorithms to perform tasks or make decisions. Traditional automation has limited adaptability and flexibility to handle dynamic or unforeseen situations without explicit programming. This paper discusses how AI-driven autonomy differs from traditional automation by emphasizing learning, adaptation, and decision-making capabilities. These capabilities ultimately enhance the overall effectiveness and agility of human-autonomy teaming in tactical operations. Autonomous Systems. Autonomous systems can perform tasks or operations without constant human control. They utilize AI algorithms and sensors to perceive and navigate their environment, achieving a high degree of autonomy [11]. Tactical Autonomy. In this study, tactical autonomy refers to autonomous systems’ ability to make real-time decisions and take actions in dynamic and complex operational environments [12]. This involves the seamless coordination and interaction between humans and autonomous systems, enabling them to function as a unified team with complementary strengths [12]. HAT focuses on achieving shared mission goals through seamless coordination and collaboration between human operators and intelligent autonomous systems [13]. This paper introduces an AI-driven HAT, which integrates AI into HAT frameworks. This approach improves decision-making, situational awareness, and operational effectiveness by combining the strengths of human expertise and AI capabilities. Tactical autonomy, which combines human cognitive abilities, such as adaptability, intuition, and creativity, with the computational power, precision, and dynamic execution of autonomous systems, has the potential to revolutionize various fields, including defense, emergency response, law enforcement, and hazardous environments [12]. It is important to differentiate between tactical and strategic autonomy to clarify how AI-driven human-autonomy teaming contributes to both levels of autonomy in military and operational contexts. Strategic autonomy refers to a nation or organization’s ability to make autonomous choices regarding broad security goals, whereas tactical autonomy, in contrast to strategic autonomy, focuses on individual units or teams acting independently within a specific mission [14]. Strategic autonomy involves higher-level decision-making and planning that considers long-term goals, overall mission objectives, and broader situational awareness. It addresses the coordination, allocation of resources, and strategic decision-making processes that guide the overall mission or campaign [14]. Tactical Operations. Tactical operations involve coordinated activities in a specific area or environment, typically in a military, law enforcement, or strategic context, focusing on achieving short-term objectives through rapid decision-making, adaptation to dynamic situations, and the application of military skills and resources within a localized area and timeframe [15]. In recent years, advancements in AI, Machine Learning (ML), robotics, and sensor technologies have paved the way for realizing the potential of tactical autonomy [12]. These technological advancements have enabled autonomous systems to perform complex tasks, process vast amounts of data in real-time, make informed decisions, and collaborate with human team members seamlessly [12]. This has opened new possibilities for augmenting human capabilities, optimizing resource allocation, and improving overall operational efficiency. However, effective tactical autonomy requires a comprehensive understanding of the dynamics between humans and autonomous systems. Human factors, including trust, communication, shared situational awareness, and decision-making, play a vital role in ensuring successful HAT. Challenges such as establishing appropriate levels of trust, addressing potential cognitive biases, managing workload distribution, and maintaining effective communication channels must be carefully addressed to ensure seamless collaboration and maximize the potential benefits of tactical autonomy. HAT for tactical autonomy is a collaborative approach to using humans and autonomous systems to operate and control weapons and other military systems. In HAT, the human operators and autonomous systems work together to achieve common goals. The human operators are responsible for the overall mission and making high-level decisions. Autonomous systems are responsible for performing assigned tasks. As explained in detail in Section IV, human operators contribute strategic insight, context, and high-level decision-making capabilities based on their experience and understanding of the mission’s goals. The interaction and communication represent the interfaces and communication channels through which each component exchanges information, collaborates, and makes joint decisions. Within the context of a shared decision-making process, human operators and autonomous systems engage in a collaborative decision-making process, sharing insights, data, and recommendations to formulate effective strategies. The autonomous system is responsible for real-time data processing, analysis, and execution of specific tasks supporting human operators with timely and pertinent information. Subsequently, once decisions are made, the autonomous system performs specific tasks, including reconnaissance, navigation, or data collection, in alignment with the directives of the shared decision-making process. This paper comprehensively explores the historical development and current state of HAT and delves into the opportunities, challenges, and potential future directions in leveraging AI for tactical autonomy. It emphasizes the transformative impact of AI on tactical autonomy and presents opportunities for improved decision-making, situational awareness, and resource optimization. By acknowledging and addressing the challenges associated with AI adoption, and by charting future directions for research, we can pave the way for a future where humans and autonomous systems seamlessly collaborate, ultimately leading to safer, more efficient, and successful missions in tactical environments. I-A Scope and Contributions The main contribution of this paper is its forward-looking study of the applications, trends, and disruptive technologies that will drive the HAT revolution in complex and dynamic environments. This provides a clear picture of HAT services and practical recommendations for future work. I-B Contributions This paper makes the following key contributions to the field of HAT. • We propose a comprehensive conceptual framework for AI-driven HAT in tactical operations, describing critical components such as trust and transparency, function allocation, situational awareness, and ethical considerations. The proposed framework provides a foundation to understand and advance the integration of AI into HAT for tactical environments. • We provide a comprehensive overview of the opportunities and key challenges associated with incorporating AI-driven HAT into tactical operations. • We explore the symbiotic relationship between AI and HAT, presenting a thorough analysis of how AI-driven HAT enhances decision-making, situational awareness, and operational effectiveness in tactical environments. • We identify several research directions for future work in AI-driven HAT, emphasizing ethical considerations, building transparent AI models, and advancing human-centric design principles to fully realize the potential of tactical autonomy. Table I compares our work to existing studies. In this paper, we explore and address research questions related to AI-driven HAT to enhance tactical operations, covering various aspects and challenges. • How do AI and HAT benefit each other when achieving tactical autonomy? • What are the main opportunities and challenges associated with incorporating AI-driven HAT in the context of tactical operations? • How can AI-driven HAT be best used in tactical operations to improve success and decision-making? • What is the plan for AI-driven HAT and how can it improve the collaboration between humans and autonomous systems in tactical situations? • How can AI-driven HAT help humans and autonomous systems work together smoothly to achieve common goals in tactical environments? • What ethical concerns must be considered when developing and using AI-driven HAT systems? • How can we make AI models in HAT more understandable, and why does this matter for better decision-making and trust in autonomous systems? • What design principles should be followed to create user-friendly AI-driven HAT systems for human operators in tactical settings? TABLE I: Comparison of our work to existing works. Year Publications Main Research Focus and Scope 2018 Ref [16] • Explores the relationship between team coordination dynamics and team performance for human-autonomy teams using an extended version of nonlinear dynamical systems methods. 2018 Ref [17] • Proposed a framework for HAT, incorporating three key tenets: transparency, bi-directional communication, and operator-directed authority. 2019 Ref [18] • Discusses what function allocation and challenges in allocating tasks between humans and autonomous machines. 2020 Ref [19] • Provides a framework for practitioners to make informed decisions regarding the integration and training of human-autonomy teams in applied settings. 2020 Ref [20] • Proposes a new approach to using ML agents in real-time strategy games to collaborate with human players rather than competing against them. 2021 Ref [3] • Examines the differences between automation and autonomy and how insights from human-human teaming can be applied to HAT. The authors have identified research gaps that need to be addressed to improve the understanding of HAT. 2022 Ref [2] • Provides a comprehensive understanding of the research environment, dependent variables, independent variables, key findings, and future research directions related to human-autonomy teamwork. 2022 Ref [21] • Emphasizes the need for humans and AI to work together effectively, particularly in complex situations. It examines the factors affecting the design and implementation of AI systems for human interaction. In addition, it provides a detailed roadmap for future HAT research, particularly emphasizing the perspectives of human factors, which aligns well with our focus on enhancing tactical operations through AI-driven HAT. 2024 Our Paper • Proposes a comprehensive conceptual framework for AI-driven HAT in tactical operations, detailing critical components, such as trust and transparency, function allocation, situational awareness, and ethical considerations. • Explores the advantages and challenges associated with integrating AI-powered HAT into tactical operations. • Provides a thorough exploration of the symbiotic relationship between AI and HAT in the context of tactical operations. • Identifies several research directions, including ethical considerations, building transparent AI models, and advancing human-centric design principles, for future work in AI-driven HAT. I-C Methodology This study investigates the potential of AI-driven HAT to revolutionize tactical operations. To achieve this, we conducted a systematic literature review to identify and analyze relevant academic research. Our search primarily targeted prominent academic databases such as Google Scholar, IEEE Xplore, ACM Digital Library, and ScienceDirect for scholarly articles published up to 2024. We focused on studies published up to May 2024 that emphasized empirical research and theoretical frameworks to explore the application of AI in human-autonomy teaming for tactical operations. Note that studies that focused on general AI applications without a tactical operation context were excluded. We employed a combination of keywords, including “AI-driven human-autonomy teaming,” “tactical operations,” “situational awareness,” “automated decision-making,” “Integrating AI and HAT,” “situation models,” and “shared situational awareness in HAT.” We included studies that focused on the application of AI in HAT for tactical operations, explored the use of Natural Language Processing (NLP) and reinforcement learning for improved communication, collaboration, and threat assessment, and addressed challenges related to trust, explainability, and ethical considerations. Furthermore, we included studies that explored the impact of AI-driven HAT on trust, explainability, and ethical considerations. We employed thematic analysis to identify key themes emerging from the reviewed literature, focusing on the opportunities and challenges associated with AI-driven HAT, with a particular emphasis on enhancing situational awareness, decision-making, and human-machine collaboration. The remainder of this paper is organized as follows. Section II discusses the integration of AI solutions into HAT. In Section III, we discuss the concept of delegated autonomy in HAT, exploring different levels and the balance between human decision-making and automated systems in teaming scenarios. Section IV presents the key components and characteristics defining HAT systems. Next, Section V identifies and discusses the practical applications of HAT, presenting real-world examples where HAT has proven advantageous. Section VI explores the economic aspects of AI integration in HAT. VII provides a detailed discussion of situation models and shared situational awareness in HAT. Section VIII outlines the specific roles and contributions of AI in enabling tactical autonomy in HAT, emphasizing its ability to enhance human decision-making. The opportunities and challenges associated with using AI to enhance HAT in tactical autonomy are discussed in Section X. The design of user interfaces and interaction mechanisms for HAT systems in tactical autonomy settings is explored in Section IX. Section XI introduces a proposed framework for AI-driven HAT in tactical operations, describes the key components, and provides guidance for future research and development. Finally, Section XII provides practical recommendations for implementing and optimizing HAT systems. The paper concludes in Section XIII with indications for future work."
https://arxiv.org/html/2411.09261v1,Automating Autograding: Large Language Models as Test Suite Generators for Introductory Programming,"Automatically graded programming assignments provide instant feedback to students and significantly reduce manual grading time for instructors. However, creating comprehensive suites of test cases for programming problems within automatic graders can be time-consuming and complex. The effort needed to define test suites may deter some instructors from creating additional problems or lead to inadequate test coverage, potentially resulting in misleading feedback on student solutions. Such limitations may reduce student access to the well-documented benefits of timely feedback when learning programming.In this work, we evaluate the effectiveness of using Large Language Models (LLMs), as part of a larger workflow, to automatically generate test suites for CS1-level programming problems. Each problem’s statement and reference solution are provided to GPT-4 to produce a test suite that can be used by an autograder. We evaluate our proposed approach using a sample of 26 problems, and more than 25,000 attempted solutions to those problems, submitted by students in an introductory programming course. We compare the performance of the LLM-generated test suites against the instructor-created test suites for each problem. Our findings reveal that LLM-generated test suites can correctly identify most valid solutions, and for most problems are at least as comprehensive as the instructor test suites. Additionally, the LLM-generated test suites exposed ambiguities in some problem statements, underscoring their potential to improve both autograding and instructional design.","Autograding of programming assignments offers a number of well-documented benefits to both students and instructors. In particular, the instant feedback provided by autograders has been shown to help students correct their errors and solve more problems (Pieterse, 2013; Sherman et al., 2013; Duch and Jaworski, 2018). Students perceive instant feedback as a positive contribution to their learning (Färnqvist and Heintz, 2016; Rao, 2019), and this is reflected in their improved overall performance (Gordillo, 2019; Bai et al., 2016). Autograding also minimizes the variability in grading decisions which may be subjective when grading is performed by instructors or teaching assistants (Parihar et al., 2017), and it helps reduce the workload of teaching staff significantly, by eliminating the need to manually evaluate each submission (Bai et al., 2016; Venables and Haywood, 2003). However, preparing problems for autograding systems requires more than just a problem statement; instructors must also provide a test suite that thoroughly covers the different scenarios of the problem. Preparing this test suite is not always straightforward. For it to be thorough, it must at least contain tests for multiple random inputs, the minimum and maximum limits of the problem’s inputs, as well as other edge-cases that are dependent on the problem. Furthermore, some problems require unique tests that capture specific constraints in the problem, e.g., an array of distinct numbers, or an array of non-increasing numbers, and sometimes random tests must be validated to ensure they actually contain a valid answer for the problem. This can all be very time consuming, even for low-level courses (Lobb and Harlow, 2016). Moreover, failing to provide sufficient tests might result in misleading feedback to students, as invalid solutions could be graded as valid. Recent advances in Generative AI (GenAI) have introduced exciting new possibilities for using AI in Computing Education (Denny et al., 2024c). Large Language Models (LLM) in particular, which are capable of generating human-like outputs in response to text prompts, can be integrated in various Computing Education settings (Becker et al., 2023; Prather et al., 2023). They have been applied to generating new programming exercises (Sarsa et al., 2022), explaining code (Denny et al., 2024b; Liffiton et al., 2023; Leinonen et al., 2023a; Phung et al., 2023), providing feedback and improving the readability of programming error messages (Leinonen et al., 2023b; Phung et al., 2023), and powering novel pedagogical approaches (Smith et al., 2024; Denny et al., 2024a). While they have also been used to grade solutions based on a set of criteria provided to the LLM (Nilsson and Tuvstedt, 2023; Bengtsson and Kaliff, 2023), we are not aware of prior work that uses LLMs in a Computing Education context to help instructors generate test suites for autograders to address the challenges stated earlier. In this study, we explore the efficacy of using LLMs to minimize the time and effort required of instructors who want their assignments to be submitted to and evaluated by autograders. We run tens of thousands of student solutions on LLM-generated test suites for CS1 problems, and compare their results to running them on traditional instructor-generated test suites with respect to their correctness and thoroughness. We also look into the LLM-generated test suites’ ability to uncover ambiguities in instructor-written problem statements. Our analysis is guided by the following three research questions: [] (1) RQ1: To what extent do LLM-generated test suites correctly identify valid solutions to CS1 problems? (2) RQ2: How comprehensive are LLM-generated test suites compared to instructor-generated test suites? (3) RQ3: What types of ambiguities can LLM-generated test suites help uncover in problem statements? We start with a literature review of studies on autograding and LLMs in Computing Education. We also show how our work differs from the large field of work undertaken on generating unit-tests for code. Then we describe the design of our study, present the results, and discuss the broader implications of this work."
https://arxiv.org/html/2411.09222v1,Toward Democracy Levels for AI,"There is increasing concern about the unilateral power of the organizations involved in the development, alignment, and governance of AI. Recent pilots — such as Meta’s Community Forums and Anthropic’s Collective Constitutional AI — have illustrated a promising direction, where democratic processes might be used to meaningfully improve public involvement and trust in critical decisions. However, there is no standard framework for evaluating such processes. In this paper, building on insights from the theory and practice of deliberative democracy, we provide a “Democracy Levels” framework for evaluating the degree to which decisions in a given domain are made democratically. The framework can be used (i) to define milestones in a roadmap for the democratic AI, pluralistic AI, and public AI ecosystems, (ii) to guide organizations that need to increase the legitimacy of their decisions on difficult AI governance questions, and (iii) as a rubric by those aiming to evaluate AI organizations and keep them accountable.","How should we navigate disagreements about the roles and responsibilities of AI systems, AI organizations, and AI regulators in a pluralistic and multipolar world? Similar questions have emerged with previous technological advances [46, 16, 36], and existing institutions and power structures will clearly play a significant role in adjudicating these questions. However, with AI, the pace of change, ubiquity, market incentives, geopolitical incentives, and jurisdictional arbitrage opportunities pose unprecedented challenges [1]. Thankfully, recent innovations in collective decision-making point towards a new generation of processes, infrastructure, and institutions to navigate these challenges [33, 12, 40]. We focus here on approaches to collective decision-making that are “characterized by a kind of equality among the participants at an essential stage of the decision-making process” [11], which we refer to as democratic processes. Beyond the potential normative and pluralistic benefits of such processes, the legitimacy and buy-in engendered by such processes are increasingly valuable to those involved in the development and governance of AI across corporations, governments, and multilateral NGOs [40]. As a result, most leading AI organizations have begun experimenting with such processes for policy or alignment decisions — including Anthropic’s Collective Constitutional AI [2], OpenAI’s Democratic inputs to AI grant program [18], Meta’s Community Forums [8], and Google DeepMind’s STELA project [6] — and there is increasing pressure to use such processes for the development of international regulation [13, 33]. While these early steps are clearly imperfect, they are developing internal organizational capacity to understand the potential of this direction, and may act as a stepping stone toward something truly impactful. However, to fulfill the potential of these processes and ensure they are credible, we need a shared language to describe and evaluate progress. Contribution. In this paper, we provide a “Democracy Levels Framework” for understanding the maturity of collective decision-making processes relating to AI, building on innovations from modern deliberative democracy [32] and pluralistic technology [44]. This framework defines a set of levels and dimensions which can: 1. Be used as milestones in a concrete roadmap (or “tech tree”) for the democratic AI [12], pluralistic AI [39], and public AI [43, 35] ecosystems — a rapidly evolving set of organizations, institutions, and initiatives focused on ensuring that we have the necessary “democratic infrastructure” for navigating the transition to a world with highly-capable AI systems. 2. Help guide organizations and institutions that need to increase the legitimacy of their decision-making on difficult AI governance questions. 3. Be used as a rubric by those aiming to keep those AI organizations accountable. We see this framework as being applicable to each of the yellow components in Figure 1: AI systems, AI organizations, and AI regulators (and the decision-making processes that feed into these). The ultimate intent is to provide a clear map of what it would take to enable meaningful democratic governance and alignment of AI, in a way that is useful both internally to organizations making decisions about AI, and externally to those supporting this work and providing accountability. Background. The framework we describe is agnostic to the kind of democratic process employed for decision-making. However, the examples we provide here generally rely on modern deliberative democratic processes [20]. This is due to their ability to work with jurisdictions of arbitrary size, infrastructure, and political structure (including globally), and effectively incorporates the knowledge of diverse participants and subject matter experts [24, 33, 32] (e.g., [30, 19, 17]). A democratic process of this form has as its input a remit and constituent population, and as its output a decision. The remit is a prompt which scopes the decision that needs to be made (and may specify the structure and properties required for the output), and at an essential stage in the process, decisions are made by a representative subset of the constituent population (generally selected by sortition [32]). The process is often conducted by a third-party democracy-as-a-service provider [33], analogous to a polling organization, who have expertise in conducting deliberative processes (e.g., [23, 31, 28, 26]). To develop this framework, we have drawn inspiration from existing frameworks for evaluating democratic-ness (e.g., [3, 25, 22, 38]), as well as frameworks for evaluating degrees of responsible behavior and autonomy in AI systems [7, 37]. Our work relates to explorations and assessments of democratic [12], participatory [15, 14, 41], pluralistic [39], and public AI [35, 43]. We also provide a brief overview of analogous innovation in traditional governments in Appendix A."
https://arxiv.org/html/2411.08977v1,Robustness and Confounders in the Demographic Alignment of LLMs with Human Perceptions of Offensiveness,"Large language models (LLMs) are known to exhibit demographic biases, yet few studies systematically evaluate these biases across multiple datasets or account for confounding factors. In this work, we examine LLM alignment with human annotations in five offensive language datasets, comprising approximately 220K annotations. Our findings reveal that while demographic traits, particularly race, influence alignment, these effects are inconsistent across datasets and often entangled with other factors. Confounders—such as document difficulty, annotator sensitivity, and within-group agreement—account for more variation in alignment patterns than demographic traits alone. Specifically, alignment increases with higher annotator sensitivity and group agreement, while greater document difficulty corresponds to reduced alignment. Our results underscore the importance of multi-dataset analyses and confounder-aware methodologies in developing robust measures of demographic bias in LLMs.","A growing body of literature explores LLMs as a quick, cheap, and dependable alternative to human annotators Chiang and Lee (2023); Törnberg (2023); Zhu et al. (2023); Gilardi et al. (2023). The eventuality of annotating data with LLMs is more than simple speculation: professional annotators already rely on LLMs to speed up their work Veselovsky et al. (2023). Since good-quality data annotations are a primary concern for any machine-learning application, its essential to assess the quality of LLM-generated annotations. In particular, tasks that reflect annotators’ subjective judgment and experiences, such as the perceived offensiveness of a message Davani et al. (2023), raise the question of what kind of subjectivities are reflected in LLM-generated annotations. Recent research finds evidence of demographic bias, that is, systematic alignment between LLMs’ annotations and those from select demographic groups of human annotators. If LLMs replicate the views of one demographic group over others, the resulting data applications risk perpetuating structural harms like marginalization of minority views. Although LLMs’ demographic bias has been identified for a variety of subjective constructs including resume screening Wilson and Caliskan (2024), medicine Omiye et al. (2023); Zack et al. (2023), political opinions Motoki et al. (2024), and offensiveness Sun et al. (2023); Santy et al. (2023), we know little about whether such bias holds beyond individual datasets. Moreover, many of these studies focus on different NLP tasks and datasets. For example, for offensiveness annotations, Sun et al. find that LLMs align most with White and female annotators in the POPQUORN dataset, while Schäfer et al. use the same dataset and do not find alignment between LLMs and women. Using a different dataset, Santy et al. find LLM alignment on offensiveness to be highest with Asian Americans. Thus, current findings are often contradictory due to the use of single datasets or idiosyncratic alignment measurement approaches, even if the same LLMs are studied. Understanding which demographic biases are consistent in LLM annotations is fundamental to tackling them. Our work fills this gap with a systematic study of LLM alignment on offensiveness labeling with different genders and ethnicities across five datasets, while also investigating factors beyond annotator demographics that might drive human-LLM misalignment. First, we verify RQ0: to what extent LLMs can substitute human annotators in detecting offensive language, to establish that LLMs’ bias may not simply be attributed to low performance. Indeed, LLMs are strong performers: correlations with aggregate human labels are positive and significant—ranging from 0.4 and 0.8. Through permutation and bootstrapping tests, we show that LLMs surpass individual human annotators in three out of five datasets, while the performance in the remaining cases is comparable. Next, we test RQ1: which demographic biases are consistently reproduced in LLM-generated annotations across datasets. Demographic biases exist within each dataset; however, most of these biases lack consistency. In our experiments we used two methods—rounded average and majority vote—to calculate the final label for a post. We found that only the bias between the White and Black demographics remained consistent when using the rounded average across all five datasets. Other differences between demographic pairs either appeared in only some datasets or even showed opposite results in different datasets. We, therefore, explore RQ2: to what extent confounding factors explain demographic bias, to understand how different characteristics of the datasets may lead to varying measures of demographic bias. We consider three alternative hypotheses: HPa) how difficult documents to annotate may be assigned unevenly across demographics; HPb) how individual annotators may have idiosyncratic annotation preferences that dominate those of the demographic; and HPc), how annotators in a demographic sample may have varying levels of agreement, in turn affecting LLMs’ alignment. We find them significantly related by modeling the relationship between demographic bias and confounders via logistic regression. Confounders explain a large fraction of the variance of LLM–human alignment, and thus can help us unpack cases when demographic bias is inconsistent. Overall contributions and novelty. Unlike past work which has mainly focused on single datasets for assessing human-LLM alignment for data annotation, we investigate alignment between humans and LLMs for labeling offensive content across five different datasets. Depending on the dataset and contrary to past work Sun et al. (2023); Santy et al. (2023), we do not find consistent gender alignment effects or better alignment with Asian Americans. While, we do find consistent patterns of better alignment with White people vs. Black people, demographic variables only drive a fraction of misalignment. We unpack this through a regression analysis with other confounding factors, like annotation difficulty. Finally, we contribute a harmonized, preprocessed, and merged version of five different NLP datasets modeling offensive language spanning 219,359 annotations, disaggregated by annotator groups that we make openly available for further community use.111https://github.com/shayanalipour/llm-alignment-bias"
https://arxiv.org/html/2411.08912v1,Can Large-Language Models Help us Better Understand and Teach the Development of Energy-Efficient Software?,"Computing systems are consuming an increasing and unsustainable fraction of society’s energy footprint, notably in data centers. Meanwhile, energy-efficient software engineering techniques are often absent from undergraduate curricula. We propose to develop a learning module for energy-efficient software, suitable for incorporation into an undergraduate software engineering class. There is one major problem with such an endeavor: undergraduate curricula have limited space for mastering energy-related systems programming aspects. To address this problem, we propose to leverage the domain expertise afforded by large language models (LLMs). In our preliminary studies, we observe that LLMs can generate energy-efficient variations of basic linear algebra codes tailored to both ARM64 and AMD64 architectures, as well as unit tests and energy measurement harnesses. On toy examples suitable for classroom use, this approach reduces energy expenditure by 30–90%. These initial experiences give rise to our vision of LLM-based meta-compilers as a tool for students to transform high-level algorithms into efficient, hardware-specific implementations. Complementing this tooling, we will incorporate systems thinking concepts into the learning module so that students can reason both locally and globally about the effects of energy optimizations.","Global climate change poses a serious threat to societal well-being. Data centers, which account for an estimated 4% of annual energy consumption in the United States (Institute, 2024) and 3% in the European Union (Commission, 2024), contribute significantly to this issue. As data center energy consumption is expected to grow, immediate action is needed. Although advancements have been made in enhancing data center computing system performance along various metrics — such as reducing latency (Hua et al., 2017), increasing throughput (Zhan et al., 2012), and improving parallelism (Mondal and Dutta, 2015) — knowledge of improving their energy efficiency remains limited (Manotas et al., 2016). There is little literature on training software engineers to use data center resources with an eye toward energy efficiency (Mullen et al., 2017; Qasem, 2019). While energy optimizations have been proposed for individual components (Anagnostopoulou et al., 2012; Weiser et al., 1996; Sehgal et al., 2010b, a; Vasić et al., 2009) and entire systems (Heath et al., 2001; Wu et al., 2015; Townend et al., 2019; Chi et al., 2021), they have largely focused on hardware. We have identified a need for improvement in software as well. Implementing energy-efficient software methods remains challenging, as current approaches rely on heavyweight design approaches (Te Brinke et al., 2013, 2014), pattern catalogs (Pinto et al., 2016; Maleki et al., 2017), programming languages (Couto et al., 2017), or decision frameworks (Manotas et al., 2014). Figure 1. Research overview: we are developing techniques and pedagogy to support a two-part learning module. We argue that all software engineers should have access to training on how to write more energy-efficient software. They should be able to learn not only what lightweight tools are available to them, but also how systems-level thinking can drive energy-conscious design decisions. Our preliminary results suggest that large language models (LLMs) might be a useful tool to realize this vision. We are examining two educational research questions: (1) RQ1: How can we apply LLMs to help engineers learn to write energy-efficient software? (2) RQ2: Is systems thinking an effective approach for design-level reasoning about energy-efficient software engineering designs?"
https://arxiv.org/html/2411.08910v1,Automated Feedback in Math Education: A Comparative Analysis of LLMs for Open-Ended Responses,"The effectiveness of feedback in enhancing learning outcomes is well documented within Educational Data Mining (EDM). Various prior research has explored methodologies to enhance the effectiveness of feedback. Recent developments in Large Language Models (LLMs) have extended their utility in enhancing automated feedback systems. This study aims to explore the potential of LLMs in facilitating automated feedback in math education. We examine the effectiveness of LLMs in evaluating student responses by comparing 3 different models: Llama, SBERT-Canberra, and GPT4 model. The evaluation requires the model to provide both a quantitative score and qualitative feedback on the student’s responses to open-ended math problems. We employ Mistral, a version of Llama catered to math, and fine-tune this model for evaluating student responses by leveraging a dataset of student responses and teacher-written feedback for middle-school math problems. A similar approach was taken for training the SBERT model as well, while the GPT4 model used a zero-shot learning approach. We evaluate the model’s performance in scoring accuracy and the quality of feedback by utilizing judgments from 2 teachers. The teachers utilized a shared rubric in assessing the accuracy and relevance of the generated feedback. We conduct both quantitative and qualitative analyses of the model performance. By offering a detailed comparison of these methods, this study aims to further the ongoing development of automated feedback systems and outlines potential future directions for leveraging generative LLMs to create more personalized learning experiences.","The growing integration of online learning platforms into traditional educational settings has influenced the development and direction of educational research. The global pandemic, COVID-19, resulted in the adoption of Online Learning Platforms (OLP)[3]. Consequently, various OLPs, especially in math education, have gained popularity over the recent years [34]. With the popularity of these platforms, there has been various research investigating effective teaching strategies, with many reporting on the benefit of timely and immediate feedback [14, 32, 19]. Feedback plays a crucial role in facilitating effective learning experiences, offering more than just assessments on the correctness of their answer by providing student-specific guidance. Timely feedback, in particular, can be highly effective in enabling students to rectify misunderstandings, bridge gaps in knowledge, or navigate to subsequent stages of their learning requirements. Prior exploration of effective feedback has reported on the effectiveness of feedback in enhancing learning outcomes, including the use of hints [53], explanations [39], worked-out examples [13], and common wrong answer feedback [22, 23], while others caution against the use of certain feedback designs, suggesting that poorly designed feedback can inadvertently impede student progress [23]. Automated scoring has been a focus for numerous online learning platforms, with extensive research spanning various fields, including mathematics [6], writing[43, 36], and programming [41, 51]. The initial works emphasized automating the grading of close-ended questions. However, recent advancements have extended these methodologies to include open-ended problems as well [18]. While early applications of automated scoring primarily focused on augmenting teacher resources in evaluating student responses, more recent explorations have begun to implement these techniques directly within classroom environments [38] to support students dynamically in real-time. The recent advancement and innovation in Large Language Models (LLMs), such as ChatGPT, have introduced a transformative approach to crafting automated feedback systems within educational platforms [2]. These developments in LLM technology have demonstrated significant potential in creating diverse mathematical content, providing support for math tutoring, offering detailed explanations, and facilitating the development of automated tutoring systems and educational chatbots that are adept at adapting to a wide range of contextual nuances. In this study, we delve into the application of pre-trained Large Language Models (LLMs) for both scoring and providing feedback on students’ open-ended responses. We particularly assess a fine-tuned LLM derived from Mistral—a Llama variant optimized for mathematics—and compare its efficacy with a leading non-generative model [11], currently used for the automated assessment of open-ended responses in mathematics. Additionally, we explore how these methods stack up against the capabilities of the GPT-4 model. Given the current limitations on training and fine-tuning GPT-4, we adopt a zero-shot strategy by providing the GPT-4 model with specific rubrics related to the open-ended questions. Toward this, we explore the following research questions: 1. How does an LLM fine-tuned (GOAT) with a dataset of students’ responses and teacher-provided scores compare to the previous state-of-the-art, SBERT-Canberra method in predicting teacher scores for student open-responses? 2. How does the pre-trained GPT4 model compare to the finetuned LLM (GOAT) in the auto-scoring task for open-ended questions? 3. Which of the three models, SBERT-Canberra, GOAT, or GPT4 is preferred for the feedback they generate, according to a detailed assessment protocol and human evaluators with prior teaching experience?"
https://arxiv.org/html/2411.08894v2,Temporal Patterns of Multiple Long-Term Conditions in Individuals with Intellectual Disability Living in Wales: An Unsupervised Clustering Approach to Disease Trajectories,"Identifying and understanding the co-occurrence of multiple long-term conditions (MLTC) in individuals with intellectual disabilities (ID) is crucial for effective healthcare management. These individuals often experience earlier onset and higher prevalence of MLTCs compared to the general population, highlighting the urgency of this issue. Despite established research on the high prevalence of MLTCs in the ID population, the specific patterns of co-occurrence and temporal progression of these conditions remain largely unexplored. This study presents an innovative unsupervised approach for examining and characterising clusters of MLTC in individuals with ID, based on their shared disease trajectories. Using a dataset of electronic health records (EHRs) from 13069 individuals with intellectual disabilities, encompassing primary and secondary care data in Wales from 2000 to 2021, this study analyses the time sequences of ordered disease diagnoses. The study population comprised 52.3% males and 47.7% females, with a mean of 4.5 \pm 3 long-term conditions (LTCs) per patient. Distinct MLTC clusters were identified in both males and females, stratified by age groups (< 45 and \geq 45 years). For males under 45, a single cluster dominated by neurological conditions (32.4%), while three clusters were identified for older males, with the largest characterised by circulatory (51.8%). In females under 45, one cluster was found with digestive system conditions (24.6%) being most prevalent. For females \geq 45 years, two clusters were identified: the first cluster was predominantly defined by circulatory (34.1%), while the second cluster by digestive (25.9%) and musculoskeletal (21.9%) system conditions. Mental illness, epilepsy, and reflux disorders were prevalent across all groups. This study reveals complex multimorbidity patterns in individuals with ID, highlighting age and sex differences. The identified clusters provide new insights into disease progression and co-occurrence in this population. These findings can inform the development of targeted interventions and risk stratification strategies, potentially improving personalised healthcare for individuals with ID and MLTCs with the aim of improving health outcome for this vulnerable group of patients, i.e., reducing frequency and length of hospital admissions and premature mortality.","People with intellectual disability (ID) face a significantly higher risk of developing a range of physical and mental health conditions compared to the general population. These conditions often occur at a younger age and lead to poorer outcomes, owing to a combination of genetic, behavioural, and social factors [1, 2, 3]. Studies show a much higher occurrence of multiple long-term conditions (MLTCs) in this population [2]. MLTCs, defined as two or more conditions in addition to ID, is linked to premature death and poorer quality of life [4]. Despite this, there appear to be only a few studies reporting the prevalence of MLTCs conducted on a large scale [2, 5], but no studies were found to reveal patterns of MLTCs and conditions more likely to co-occur together in this population. The growing use of electronic health records (EHRs) has enabled significant advances in addressing clinical challenges, enhancing diagnostic capabilities, and improving patient outcomes [6, 7, 8]. In addition to enabling studies on co-occurring conditions, the longitudinal nature of EHR provides a unique opportunity to uncover temporal associations and trajectories between conditions. Importantly, chronic health conditions frequently co-occur more than expected by chance, often as a consequence of shared risk factors, pathogenicity, or their treatment [9]. However, most prior studies have not incorporated the time dimension due to the short time span of the available data [10, 11]. Only recently have a few large-scale analyses assessed disease trajectories by evaluating temporal ordering of co-morbidity pairs over time in general population [12, 13, 14]. Many studies further developed the framework initially proposed by Jensen et al. [12], who described general principles for temporal trajectory analysis using Danish national data. For instance, Siggaard et al. [15] published a browser of these results, while Jørgensen and Brunak [16] focused on chronic obstructive pulmonary disease (COPD) trajectories. Hu et al. [17] linked the data to a cancer registry to investigate pre-diagnosis trajectories. Jensen et al.’s [12] approach has been applied, with modifications, to other populations including post-depression trajectories in UK Biobank [18], and end-of-life trajectories in California [19]. Furthermore, Giannoula et al. [13, 20] proposed a framework to detect and cluster co-morbidity pairs and shared trajectories over time using a dynamic time warping (DTW)-based unsupervised algorithm in EHRs, and later extended this to include genetic information in the clustering step. Unsupervised algorithms discover natural patterns in data without learning predefined outcomes or classifications. Trajectory analyses can reveal complex, time-ordered condition associations, as well as MLTCs patterns to enable better understanding of disease progression for improved prediction outcomes. In this study, we propose a computational framework for the analysis of temporal MLTCs on EHRs in 13069 adults diagnosed with ID in Wales, incorporating 40 long-term conditions (LTCs) from both primary and secondary care data. While most prior studies have applied temporal trajectory analysis to secondary care data and ICD-9 or -10 codes [12, 13, 15, 16, 17, 18, 19, 20], with only one study using primary care data [21], our approach utilises both to fully capture MLTCs, as most chronic conditions are treated in general practice. This approach highlights several differences in MLTC patterns between male and female sub-populations across different age groups, acknowledging sex and age as crucial factors in understanding MLTCs. The primary contributions of this research include statistical analysis to identify significant temporal condition pairs, identification of shared MLTCs trajectories, construction of a network of all shared trajectories, and identification of trajectory clusters using an unsupervised machine learning algorithm."
https://arxiv.org/html/2411.08888v1,Exploring Capabilities of Time Series Foundation Models in Building Analytics,"The growing integration of digitized infrastructure with Internet of Things (IoT) networks has transformed the management and optimization of building energy consumption. By leveraging IoT-based monitoring systems, stakeholders such as building managers, energy suppliers, and policymakers can make data-driven decisions to improve energy efficiency. However, accurate energy forecasting and analytics face persistent challenges, primarily due to the inherent physical constraints of buildings and the diverse, heterogeneous nature of IoT-generated data. In this study, we conduct a comprehensive benchmarking of two publicly available IoT datasets, evaluating the performance of time series foundation models in the context of building energy analytics. Our analysis shows that single-modal models demonstrate significant promise in overcoming the complexities of data variability and physical limitations in buildings, with future work focusing on optimizing multi-modal models for sustainable energy management.","The deployment of AI in digital infrastructure presents significant opportunities for enhancing energy efficiency and advancing net-zero strategies. Traditional building energy research relies on post-processed datasets, which typically include aggregated load data and weather station-monitored temperature—referred to as rough granularity data. While these higher-level data offer convenience for data scientists by providing a broad overview, they compromise the granularity necessary for a detailed description of activities. The introduction of the BLDG59(Luo et al., 2022) and BTS datasets(Prabowo et al., 2024) based on the centralized management tool, Brick Schema(Balaji et al., 2018), addresses this limitation by offering comprehensive IoT sensing and metering point data, enabling the exploration of detailed usage patterns in digitalized buildings. However, the increased data texture complicates the building activity modeling due to the varied architecture preferences, energy efficiency measures, local climate conditions, usage patterns, and engineering practices, all contributing to heterogeneous sensing data among buildings(Lin et al., 2024). Accurate cross-building forecasting using IoT point data necessitates models that can adapt to unseen physical constraints and variations in ontology usage. Foundation models, which utilize large, pre-trained models as the base for fine-tuning specific tasks, are receiving increasing attention in time series analysis due to their capabilities in generalization. Existing time series foundation models are typically summarized into two categories. Prompt-based methods, such as PromptCast(Xue and Salim, 2023) and LLMTime (Gruver et al., 2024), typically convert time series to sentences using a manually defined template and feed to language models. Methods following this logic benefit from the easy implementation, but specific prompt templates are required based on targeted tasks and datasets. Tuning-based models typically employ fine-tuning techniques for LLMs to adapt the time series inputs. One-Fits-All (GPT4TS)(Zhou et al., 2023) and LLM4TS(Chang et al., 2023) approach multivariate time series by treating them as univariate sequences, which are then segmented into patches. These patches are encoded using specific encoders, concatenated, and subsequently fed into the pre-trained LLMs. To enhance forecasting by incorporating domain knowledge, GPT4MTS(Jia et al., 2024) introduces a prompting template that combines time series data with textual prompts. TimeLLM(Jin et al., 2023) initially proposes the cross-modal attention mechanism, which aligns the time series with pre-train LLM’s word embedding. Inspired by this work, LLaTA(Liu et al., 2024) designed a dual-branch architecture to process the textual and temporal modalities. Each branch fine-tunes a GPT-2 backbones with the Low-rank adaptation technique (LoRA)(Hu et al., 2021) to adapt to the task, aligning the distillation knowledge and temporal embedding by reducing the distribution discrepancy between the modalities. TS foundation models are typically evaluated on well-processed datasets, which differ significantly from real-world building IoT datasets. Building IoT contains branches of information about energy usage, comfort parameters, and environmental factors, but at the same time, the complex dependencies and data heterogeneity among the IoT network present challenges for machine learning and deep learning techniques. In the context of energy-efficient digital infrastructures, the application of TS models still needs extensive experimentation to validate their effectiveness. This comparative study aims to assess the performance of TS foundation models in the building energy domain, benchmarking them against traditional machine learning and deep learning approaches. Additionally, the study explores architectural ablations of TS foundation models to understand their strengths and limitations. The remainder of the paper is structured as follows: Section 2 evaluation methodology adopted by this benchmark study. Section 3 presents the results, followed by a discussion of the findings and future research directions in Section 4. Section 5 provides a concise conclusion."
https://arxiv.org/html/2411.08884v1,Quantifying Risk Propensities of Large Language Models: Ethical Focus and Bias Detection through Role-Play,"As Large Language Models (LLMs) become more prevalent, concerns about their safety, ethics, and potential biases have risen. Systematically evaluating LLMs’ risk decision-making tendencies and attitudes, particularly in the ethical domain, has become crucial. This study innovatively applies the Domain-Specific Risk-Taking (DOSPERT) scale from cognitive science to LLMs and proposes a novel Ethical Decision-Making Risk Attitude Scale (EDRAS) to assess LLMs’ ethical risk attitudes in depth. We further propose an novel approach integrating risk scales and role-playing to quantitatively evaluate systematic biases in LLMs. Through systematic evaluation and analysis of multiple mainstream LLMs, we assessed the ""risk personalities"" of LLMs across multiple domains, with a particular focus on the ethical domain, and revealed and quantified LLMs’ systematic biases towards different groups. This research helps understand LLMs’ risk decision-making and ensure their safe and reliable application. Our approach provides a tool for identifying and mitigating biases, contributing to fairer and more trustworthy AI systems. The code and data are available.","Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and generating human language, showing significant potential across various domains. The outstanding performance of LLMs has sparked hope that they might be the AGI of our era Bubeck et al. (2023). As LLMs become more widely adopted, ranging from everyday use to specialized fields, the need for comprehensive evaluation, especially regarding safety, ethics, and biases, becomes increasingly urgent Chang et al. (2024). Currently, the widespread popularity of LLMs has led to the development of numerous evaluation benchmarks, tasks and metrics that examine LLMs from different angles Chang et al. (2024). Evaluations of LLMs’ risk attitudes are crucial for ensuring their safe and reliable application, especially in critical decisions such as health and finance Chang et al. (2024), particularly in modes where the LLM acts as an agent Zhao et al. (2024). In the field, while several benchmarks have been proposed to explore LLMs’ propensity to engage in harmful activities Yuan et al. (2024); Wang et al. (2023); Perez et al. (2022); Shi and Xiong (2024), such work remains relatively scarce. Compared to prior work, we not only innovatively apply interdisciplinary tools to evaluate LLMs’ risk attitudes across multiple domains and deeply assess ethical risk decision-making, but we are also, to our knowledge, the first to evaluate the biases within LLMs based on the risk analysis and role-play. Moreover, our work fills the gap in research AI psychology and cognitive science. Figure 1: Examine LLMs’ risk attitudes through various risk events across multiple domains (such as social and financial domains). What are the things they accept, and what are the things they have zero tolerance for? In this paper, We explore four key questions: (1) Do LLMs exhibit stable and measurable risk attitudes? (2) Are there specific differences or consistent patterns in risk attitudes across multiple domains among different LLMs? (3) What are LLMs’ risk propensities in the ethical domain, and how do these impact LLM safety? (4) Do LLMs exhibit systematic biases in their perception of risk attitudes and ethical levels for different social groups? We innovatively applies risk attitude assessment tools from human psychology, cognitive science and behavioral economics to AI systems, conducting a systematic, standardized, and quantitative evaluation of risk preferences in mainstream LLMs. Studies have shown that using standard psychometric inventories for LLMs is feasible and effective (Pellert et al., 2023; Li et al., 2024). We have chosen the DOSPERT Weber et al. (2002), widely used in social science researches Farnham et al. (2018); Shou and Olney (2020), as our assessment tool. DOSPERT places risk assessment within specific contexts across different domains, allowing for a comprehensive evaluation. DOSPERT has been widely validated and applied across different age groups and cultural backgrounds Blais and Weber (2006). In summary, we find that DOSPERT provides a promising framework for multi-dimensional analysis of LLMs’ ""risk personality"". Applying DOSPERT to LLMs represents an innovative interdisciplinary attempt. Futhermore, given that risk scores in the ethical domain are directly related to the safety of LLMs, we propose EDRAS to comprehensively and specifically evaluate LLMs’ risk attitudes in ethical domain. However, our research does not stop at simply assessing the risk propensities of LLMs. In the field of AI, systematic biases and stereotypes in algorithms have been a significant concern Mehrabi et al. (2021); Blodgett et al. (2020). These not only potentially exacerbate the spread of biases and promote social inequality but may also cause harm to certain groups. How to detect and quantify potential biases in LLMs, which may contain racial, gender, or other biases due to training from human text, is a crucial issue Demszky et al. (2023). We have discovered a novel approach using risk scale and role-play as a bridge to detect and quantify bias in LLMs, indirectly reflecting LLMs’ differential views on various social identities, occupations, ethnicities, and genders. The main contributions of this study include: • We verified that specific LLM possess differentiated and relatively stable risk propensities through multiple tests. • We conducted domain-specific risk propensities evaluations on multiple mainstream LLMs, explored specific differences and consistent patterns in LLMs’ risk attitudes. • We propose a novel EDRAS to further delve into the assessment of LLMs’ ethical decision-making risk attitudes. • We designed different role hypotheses, using risk scales to quantitatively explore what LLMs consider to be the different risk attitudes for various social groups, and discuss potential systematic biases. Models \mathbf{\mu_{5}} \mathbf{M_{5}} \mathbf{{max}_{5}} \mathbf{{min}_{5}} \mathbf{\Sigma^{\mathbf{2}}_{\mathbf{5}}} \mathbf{\Sigma_{5}} \mathbf{\Gamma_{5}} \mathbf{\kappa_{5}} Claude 3.5 Sonnet 38.80 38.40 40.00 38.00 0.51 0.72 0.63 -1.05 ERNIE 3.5 47.36 47.20 48.80 45.20 1.83 1.35 -0.35 -1.18 moonshot-v1 49.04 49.20 50.40 46.80 1.83 1.35 -0.52 -1.08 GPT-4o mini 52.24 52.00 53.60 50.80 1.38 1.18 0.11 -1.71 GPT-3.5 Turbo 55.20 55.20 55.60 54.80 0.13 0.36 0.00 -1.75 Llama 3.2-3b 60.88 61.20 64.00 58.00 4.95 2.23 0.02 -1.47 Table 1: In basic DOSPERT, the scores obtained include the mean \mu, median M, maximum \mathrm{max}, minimum \mathrm{min}, variance \Sigma^{2}, standard deviation \Sigma, skewness \Gamma, and kurtosis \kappa. These scores are calculated by dividing the raw scores by the total possible score of 250. The \mathrm{max} and \mathrm{min} of the various models differ slightly, and the \Sigma^{2} and \Sigma are small, indicating a relatively high stability in scores. This suggests that the LLMs have a relatively stable risk trait. Additionally, the scores among the LLMs also show stable differences, reflecting distinct ""risk personalities."""
https://arxiv.org/html/2411.08881v1,Can We Trust AI Agents? An Experimental Study Towards Trustworthy LLM-Based Multi-Agent Systems for AI Ethics,"AI-based systems, including Large Language Models (LLMs), impact millions by supporting diverse tasks but face issues like misinformation, bias, and misuse. Ethical AI development is crucial as new technologies and concerns emerge, but objective, practical ethical guidance remains debated. This study examines LLMs in developing ethical AI systems, assessing how trustworthiness-enhancing techniques affect ethical AI output generation. Using the Design Science Research (DSR) method, we identify techniques for LLM trustworthiness: multi-agents, distinct roles, structured communication, and multiple rounds of debate. We design the multi-agent prototype LLM-BMAS, where agents engage in structured discussions on real-world ethical AI issues from the AI Incident Database. The prototype’s performance is evaluated through thematic analysis, hierarchical clustering, ablation studies, and source code execution. Our system generates around 2,000 lines per run, compared to only 80 lines in the ablation study. Discussions reveal terms like bias detection, transparency, accountability, user consent, GDPR compliance, fairness evaluation, and EU AI Act compliance, showing LLM-BMAS’s ability to generate thorough source code and documentation addressing often-overlooked ethical AI issues. However, practical challenges in source code integration and dependency management may limit smooth system adoption by practitioners. This study aims to shed light on enhancing trustworthiness in LLMs to support practitioners in developing ethical AI-based systems.","Artificial Intelligence (AI) is emerging as a transformative force, reshaping industries, economies and everyday life. Despite its rapid development and adoption, a number of negative reports on its use highlight the importance of adhering to ethical norms and principles [39]. Several ethical guidelines are available with plenty ethical principles providing high level and abstract guidance for developers and stakeholders on AI ethics [3]. These are important, but there is a lack of practical guidance for developers to operationalise ethics in AI [3]. As new AI-based technologies emerge, ethics in AI will also become increasingly critical, such as the latest breakthrough: Large Language Models (LLMs). LLMs are becoming ubiquitous and have significant impact on decision-making processes and human interactions [20, 33]. LLMs are advanced AI algorithms capable of generating, interpreting, and predicting text based on vast amounts of data they have been trained on [4]. Of these LLMs, Generative Pre-trained Transformer (GPT) LLMs, such as ChatGPT, have showcased unprecedented proficiency in the human language, coding, logic, reasoning, and other associated natural language tasks [33]. They excel at guiding complex conversations and are used in countless endeavours such as software engineering (SE) - as in AI for Software Engineering (AI4SE) [22] - and qualitative research [26]. Within the AI4SE field, the capabilities of LLMs are being explored in the software development, maintenance, and evolution [28, 30, 26, 22], namely LLM4SE. Accordingly, they find applications across various stages of the software development life cycle, including requirement analysis, software design, code implementation, testing, refactoring, defect detection, and maintenance [30]. Albeit the notable use of LLM in SE, to the best of our knowledge there are no studies that focus on the use of LLM in the ethical AI development. In qualitative research, LLMs are also gaining prominence, particularly as a supplement to tasks traditionally performed by humans, like analysing qualitative data [12, 32, 24]. Specifically, in the coding process - where qualitative data is organised and interpreted by assigning codes or labels to text segments or different data forms - LLMs have demonstrated significant utility [36, 32]. However, several concerns arise, especially related with trustworthiness, as more practitioners are relying on LLMs to perform their task [22]. Several studies are drawing attention to possible problems in adopting LLMs for SE, in particular when syntactically correct but non-functional code is produced, which affects the reliability and effectiveness of LLM-based code generation [16]. Pearce et al. [29] used GitHub Copilot to produce 1,689 programs, and found that 40% of them have security vulnerabilities. Liu et al. [21] systematically analysed ChatGPT code generation reliability identifying quality issues as many of the programs generated provided wrong output or contained compilation or runtime error. Effective AI4SE should be trustworthy and synergistic with the practitioner’s workflow, otherwise “such AI4SE solutions risk becoming obstacles rather than facilitators"" [22]. Currently, there is a growing need for context-dependent empirical studies that explore how trust in LLMs affects their adoption in software engineering tasks [22]. Furthermore, there remains a significant gap in research focused on the practical operationalisation of AI ethics, particularly through the application of LLMs. To date, no studies have attempted to explore the development of ethical AI-based systems using LLMs. In this paper, we aim to explore trustworthiness in LLMs in the ethical AI development through an experiment. Our Research Question is: To what extent does the trustworthiness of LLM-based systems contribute to achieving ethically aligned AI-based systems? To this end, an empirical study was carried out, following the Design Science Research (DSR) method [13]. We identify techniques that can improve trustworthiness in LLMs, apply them to develop a prototype, use it in the context of the development of ethically aligned AI-based systems, and evaluate the output. The evaluation is conducted through four different approaches. First, a thematic analysis is performed by an LLM, which identifies and categorizes key themes within the data. Second, we apply hierarchical clustering dendogram to group related text to compare and support the thematic analysis. Third, we perform an ablation study. Fourth, we run the source code produced to assess its functionality and correctness. The contributions of this paper to the current state of knowledge on LLM4SE and ethics in AI are threefold. First, we showcase the effectiveness of various techniques in enhancing trustworthiness in LLM4SE. Second, we provide empirical insights into how these techniques can be leverage in developing ethically-aligned AI-based systems. Finally, by applying a multi-faceted evaluation approach - including thematic analysis, hierarchical clustering, and direct source code execution - we provide an initial framework for assessing the reliability and trustworthiness of LLMs in software engineering. With this experiment, we hope to shed light both on how to improve trustworthiness in LLM and on how to help practitioners develop ethical AI-based systems."
https://arxiv.org/html/2411.09549v1,Quantum computing inspired paintings: reinterpreting classical masterpieces,"We aim to apply a quantum computing technique to compose artworks. The main idea is to revisit three paintings of different styles and historical periods: “Narciso”, painted circa 1597–1599 by Michelangelo Merisi (Caravaggio), “Les fils de l’homme”, painted in 1964 by René Magritte and “192 Farben”, painted in 1966 by Gerard Richter. We utilize the output of a quantum computation to change the composition in the paintings, leading to a paintings series titled “Quantum Transformation I, II, III”. In particular, the figures are discretized into square lattices and the order of the pieces is changed according to the result of the quantum simulation. We consider an Ising Hamiltonian as the observable in the quantum computation and its time evolution as the final outcome. From a classical subject to abstract forms, we seek to combine classical and quantum aesthetics through these three art pieces. Besides experimenting with hardware runs and circuit noise, our goal is to reproduce these works as physical oil paintings on wooden panels. With this process, we complete a full circle between classical and quantum techniques and contribute to rethinking Art practice in the era of quantum computing technologies.","Art and science have long gone hand in hand, and quantum mechanics has attracted the interest of artists from the very beginning. However, even though quantum computing technologies have slowly started to emerge in the late 90s [1], their use in the Arts has only started sprouting recently. This integration has been proposed in the visual arts [2] and in music [3], and has inspired several collaborations, see e.g. Ref. [4, 5, 6] and exhibitions [7, 8]. The interplay between being inspired by quantum and making use of quantum computing has been discussed in Ref. [9], where it was also proposed the concept of Quantum-Computing Aided Composition (QAC) and Quantum-Computing Aided Design (QAD) to articulate the use of quantum technologies in Art. In our work we add another step to this approach by using the quantum computer-generated image as a template to actually paint the digital quantum computer result on a real canvas. In this way, a human element is added to the process, leading also to wanted inaccuracies, reflecting the human part of the process. Here, we present a detailed example of this idea to use quantum computing to aid the creative process of human artistic oil painting. In particular, in this project, we consider three masterpieces: “Narciso”, painted circa 1597–1599 by Michelangelo Merisi (Caravaggio) [10], “Les fils de l’homme”, painted in 1964 by René Magritte [11] and “192 Farben”, painted in 1966 by Gerard Richter [12]. We use quantum computing, in particular the IBM quantum hardware [13], to process digital images. To develop the quantum algorithm we use IBM’s quantum computing programming framework, Qiskit [14]. The original paintings are analyzed and divided partly or as a whole into a lattice. The discrete units, i.e. the lattice tiles, are then encoded to qubits, the fundamental constituents of a quantum computer. We then employ a Hamiltonian of a physical system as the observable we measure: the Ising model [15]. By quantum time evolving this operator, we can follow the transformation of the initial lattice from a selected starting row or column of the lattice. Running the algorithm on a real quantum computer results in a quantum ordering of the lattice tiles. This means that the tiles are placed only with a certain probability given by the underlying quantum mechanical principles of superposition and entanglement. In this way, the obtained digital image generated by the quantum computer serves, in a twofold way, as a template for the human interface. First, from a number of digital examples the one that finds the consent of the authors as aesthetically the best is selected as the final template. Second, the selected digital image is then taken to be reproduced manually on wooden panels, using oil paint. As said above, in this way we are realizing our idea of keeping the artwork still “human”. Our choice of paintings follows a line from the Naturalism of Caravaggio to the Surrealism of Magritte and, lastly, the Abstraction of Richter. We consider our work from various angles. The first is to explore, how quantum computing renders the original paintings from the three epochs differently leading to new versions of the paintings. The second is the aesthetical reception of the digital images produced by the authors. And, the third is the human intervention by reproducing the digital template on a wooden panel with oil, giving it an additional human element. Finally, through using a quantum computer, and thus employing the quantum mechanical principles, we are able to represent a contemporary concept, i.e. how reality is losing its form and becoming abstract and evanescent. We titled the paintings series as “Quantum Transformation I, II and III”111Note that we have added a watermark in the pictures of these paintings presented here.. The paper is structured as follows: In Sec. II, we introduce the Hamiltonian of the Ising model and the time evolution via a Trotterization. We define the quantum circuit used for the computation with its parameterized gates (i.e. the building blocks of quantum circuits). The results are reported in Sec. III, for the painting “Quantum Transformation I: Caravaggio”, Sec. IV for the painting “Quantum Transformation II: Magritte”, and Sec. V for the painting “Quantum Transformation III: Richter”. In each section, we describe each of the three paintings, with the results from the quantum computation, and show the final pictures of the oil paintings222The painting “Quantum Transformation II: Magritte” is still in production and will be included in a forthcoming version of this paper. We reported here the digital image with the quantum computing results.. The source code used to work on these paintings is available in Ref. [16]. To conclude the paper, Sec. VI gives a summary and the outlook for this project."
https://arxiv.org/html/2411.09546v1,Architectural Exploration of Application-Specific Resonant SRAM Compute-in-Memory (rCiM),"While general-purpose computing follows Von Neumann’s architecture, the data movement between memory and processor elements dictates the processor’s performance. The evolving compute-in-memory (CiM) paradigm tackles this issue by facilitating simultaneous processing and storage within static random-access memory (SRAM) elements. Numerous design decisions taken at different levels of hierarchy affect the figure of merits (FoMs) of SRAM, such as power, performance, area, and yield. The absence of a rapid assessment mechanism for the impact of changes at different hierarchy levels on global FoMs poses a challenge to accurately evaluating innovative SRAM designs. This paper presents an automation tool designed to optimize the energy and latency of SRAM designs incorporating diverse implementation strategies for executing logic operations within the SRAM. The tool structure allows easy comparison across different array topologies and various design strategies to result in energy-efficient implementations. Our study involves a comprehensive comparison of over 6900+ distinct design implementation strategies for EPFL combinational benchmark circuits on the energy-recycling resonant compute-in-memory (rCiM) architecture designed using TSMC 28 nm technology. When provided with a combinational circuit, the tool aims to generate an energy-efficient implementation strategy tailored to the specified input memory and latency constraints. The tool reduces 80.9% of energy consumption on average across all benchmarks while using the six-topology implementation compared to baseline implementation of single-macro topology by considering the parallel processing capability of rCiM cache size ranging from 4KB to 192KB.","Cache memory remains one of the critical components in our computing system, enhancing overall performance by bridging the speed gap between the main memory (RAM) and the central processing unit (CPU). Besides, in recent years, static random access memory (SRAM)-based in-memory computing paved a promising direction to enable energy-efficient computation. However, the lack of design and automation tools to map computation on optimal SRAM architecture increases design time-to-market, resulting in higher engineering costs. This research resolves this issue by proposing an architectural exploration tool that efficiently maps logic computations to optimal cache architecture. Figure 1: (a) Conventional Von Neumann architecture, where an operation f is performed on data D within the CPU, incurs high data movement overhead, which can be reduced using (b) a CiM architecture, where f is computed directly within the memory, with the CPU primarily functioning as a control unit. Computing-in-memory (CiM) architectures have emerged as highly promising solutions for data-intensive applications. They minimize data movement, enhance computational capabilities, and improve the system’s overall energy efficiency by processing and storing data within cache memory. As shown in Figure 1 (a), the traditional Von Neumann architecture relies on data communication between the arithmetic logic unit (ALU) and cache memory through address and data buses. However, as the CPU performance is significantly higher than the memory performance, the Von Neumann architectures often create memory bottlenecks. CiM architectures, as shown in Figure 1 (b), mitigate the impact of large memory access latencies by performing the computations within the memory. By reducing data movement and exploiting parallelism within the memory, CiM architectures significantly enhance computational efficiency and performance. SRAM-based CiM architectures have been heavily investigated for performing various operations, such as matrix-vector multiplication (MVM) [1, 2], multiply-and-accumulate (MAC) operations [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], boolean logic operations [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30], and content-addressable memory (CAM) [31, 32, 33, 34, 35, 36] operations for fast searching operations. However, none presents a generic energy-saving architecture that spans across various applications. This work utilizes a novel series-resonance-based resonant CiM (rCiM) architecture that reduces dynamic power consumption by recycling the wasted energy during writing operations. This work proposes an agile architectural exploration tool to map various logical operations to an optimal SRAM macro cache size. The primary objective of the tool is to facilitate the development of novel energy-efficient SRAM-based energy-recycling rCiM implementations individually designed for specific boolean logical applications. In particular, the main contributions of the paper are as follows: • A novel resonant Compute-in-Memory (rCiM) structure that incorporates a series inductor to recycle energy dissipated during write operations. • An architectural exploration toolflow that integrates open-source synthesis tools (Berkeley-ABC [37] & YOSYS [38]) to identify the optimal SRAM configuration within a specified range of SRAM cache memory and map efficient logical operations tailored to an optimal rCiM macro size. • Comprehensive analysis of 6900+ distinct logical design implementations for EPFL combinational benchmark circuits [39] using 12 different SRAM topologies."
https://arxiv.org/html/2411.09389v1,Less is More: Unseen Domain Fake News Detection via Causal Propagation Substructures,"The spread of fake news on social media poses significant threats to individuals and society. Text-based and graph-based models have been employed for fake news detection by analyzing news content and propagation networks, showing promising results in specific scenarios. However, these data-driven models heavily rely on pre-existing in-distribution data for training, limiting their performance when confronted with fake news from emerging or previously unseen domains, known as out-of-distribution (OOD) data. Tackling OOD fake news is a challenging yet critical task. In this paper, we introduce the Causal Subgraph-oriented Domain Adaptive Fake News Detection (CSDA) model, designed to enhance zero-shot fake news detection by extracting causal substructures from propagation graphs using in-distribution data and generalizing this approach to OOD data. The model employs a graph neural network-based mask generation process to identify dominant nodes and edges within the propagation graph, using these substructures for fake news prediction. Additionally, CSDA’s performance is further improved through contrastive learning in few-shot scenarios, where a limited amount of OOD data is available for training. Extensive experiments on public social media datasets demonstrate that CSDA effectively handles OOD fake news detection, achieving a 7%\sim16% accuracy improvement over other state-of-the-art models.","The popularity of social media has enabled rapid news dissemination, for both true and fake news. Given the potential impact of fake news, robust fake news detection methods are needed to debunk such news in a timely manner. In real-world scenarios, out-of-distribution news from unseen domains emerges over time. This brings substantial challenges to fake news detection models. Graph-based fake news detection methods using graph neural networks (GNN) have garnered much attention recently for modelling news propagation patterns (Gong et al. 2023a). Despite their success, existing GNN-based methods are generally built on the assumption that both training and testing data are independently sampled from an identical data distribution (i.i.d.), which often does not hold true nor reflect the real challenges of fake news detection (Li et al. 2022). Emerging and hitherto unseen fake news and their associated propagation graphs can and do appear. From an empirical perspective, these methods focus on minimising the average training error and incorporating correlations within the training data (which is considered to be in-distribution) to improve fake news detection accuracy (Liu et al. 2021). However, real-world graph-based fake news data is often mixed with biased domain-specific information in the training data. The detection model may thus learn these domain-specific biases resulting in misclassification of cross-domain news items (Li et al. 2022). To detect fake news across different domains (e.g., sports and politics), some early studies (Ma, Gao, and Wong 2018; Bian et al. 2020) focus on capturing content-independent propagation patterns. However, it has been shown (Min et al. 2022) that not only the news contents but also the propagation patterns can vary across different news domains. More recent approaches (Li et al. 2023; Lin et al. 2022) collect and manually label a small dataset from emerging news domains. They utilise domain adaptation methods to adapt the trained models to the emerging domains in a few-shot manner. However, these approaches require labelled data from emerging domains which is not always available and could be expensive and time-consuming. To address the limitations above, we focus on extracting causal subgraphs from news propagation graphs to eliminate potential domain biases. The patterns of such subgraphs are learnt for fake news detection in emerging domains. News from an emerging domain is considered as the out-of-distribution (OOD) data, and we generalise our model trained on in-distribution data to OOD data by capturing causal subgraphs in an unsupervised manner. From a causal analysis perspective, each propagation graph is composed of causal subgraph and biased subgraph which are initially entangled. Our intuition is that not all nodes in the propagation graph of a given news item are helpful for fake news detection. Instead, only some causal subgraphs of the propagation graph carry critical clues that can be used to identify fake news, as illustrated in Fig. 1 with an example. If we can identify and capture such causal subgraphs, we can improve fake news detection accuracy and subsequently improve the way we generalise the model to OOD data. Figure 1: Illustration of the causal subgraphs and the Structure Causal Models (SCMs). In the SCMs, the grey and white variables represent unobserved and observed variables. Further explanations on SCMs are given in Preliminaries. Based on this intuition, a cross-domain model – the Causal Subgraph Oriented Domain Adaptive Fake News Detection (CSDA) model, is proposed. This model extracts subgraphs from propagation graphs and performs detection based on the subgraphs. In CSDA, a binary mask is learned for each node and each edge of the propagation graph of a news item to classify them into causal or biased elements. For the subgraph formed by each type of element, a graph encoder and a multilayer perceptron (MLP) classifier together encode the subgraphs and classify the news item according to the subgraph embeddings. In the training process, we utilise a data augmentation strategy by concatenating the causal subgraph embedding and the permuted biased subgraph embedding. We then train CSDA with both embeddings to enhance the effectiveness of causal subgraph learning. In the testing process, only the causal branch of the CSDA model is utilised to predict news veracity. Following recent works (Li et al. 2023; Lin et al. 2022), we also consider a scenario where limited OOD data becomes available through manual labelling. In this scenario, CSDA’s performance on OOD data is further enhanced with a supervised contrastive learning-based approach and achieves state-of-the-art (SOTA) classification accuracy. In summary, our contributions include: • We propose a zero-shot cross-domain fake news detection model named CSDA based on extracting causal subgraphs related to news propagation patterns. • We further explore a few-shot scenario in cross-domain fake news detection where a small number of OOD examples are available, and we utilise contrastive learning to enhance CSDA’s cross-domain performance. • Extensive experiments are conducted on four real datasets. The results confirm the effectiveness of CSDA in cross-domain fake news detection, outperforming SOTA models by 7.69\sim 16.00\% in terms of accuracy."
https://arxiv.org/html/2411.09168v1,Theory of Mind Enhances Collective Intelligence,"Collective Intelligence plays a central role in a large variety of fields, from economics and evolutionary theory to neural networks and eusocial insects, and it is also core to much of the work on emergence and self-organisation in complex systems theory. However, in human collective intelligence there is still much more to be understood in the relationship between specific psychological processes at the individual level and the emergence of self-organised structures at the social level. Previously psychological factors have played a relatively minor role in the study of collective intelligence as the principles are often quite general and applicable to humans just as readily as insects or other agents without sophisticated psychologies. In this article we emphasise, with examples from other complex adaptive systems, the broad applicability of collective intelligence principles while the mechanisms and time-scales differ significantly between examples. We contend that flexible collective intelligence in human social settings is improved by our use of a specific cognitive tool: our Theory of Mind. We identify several key characteristics of psychologically mediated collective intelligence and show that the development of a Theory of Mind is a crucial factor distinguishing social collective intelligence from general collective intelligence. We then place these capabilities in the context of the next steps in artificial intelligence embedded in a future that includes an effective human-AI hybrid social ecology.","All intelligence is collective intelligence. [70] Collectives are capable of achieving things that individuals alone cannot. Notwithstanding the simplicity or complexity of the individuals, their aggregate behaviour can often be understood as a complex processing of information that individuals store, modify, and transfer between each other producing ‘useful’ collective behaviour at the scale of the whole collective. In most instances of Collective Intelligence (CI), where the agents might be ants in an ant colony, bees in a beehive, or neurons in a neural network, the individual is not aware of the drivers of their behaviour or the behaviour of other agents. For example, a single neuron is neither aware of its own internal processes nor that of a neuron it is connected to, nor is it aware of the end goal to which its activity contributes. Despite both this lack of awareness and the lack of a centralised controller, evolutionary and learning processes have produced an intricate, precise, and highly adaptive system that is capable of functional behaviour that would be impossible for any single neuron to achieve. In other instances of CI, such as teams of humans, or businesses interacting in economic markets, the agents themselves may be highly complex and exhibit varying degrees of purposefulness and awareness. Within this context, we draw attention to the role of psychological factors in improving the CI of human social collectives and quantifying the intelligence of social collectives, both natural and artificial. In order to understand how collectives process information, we first consider the variety of ways in which agents interact. The topology of the network describing agent-to-agent interactions is well known to be important for the proper functioning of social groups [83, 79]. In particular it has been shown that mammalian social groups exhibit patterns of fractal-like topologies [40, 51] that are a result of a cognitive ability to form discrete social connections between conspicifics [49]. These links are often both spatially and temporally transient; people meet for a while, go their separate ways, and come back together later. Despite this transience, individual connections are often the basis of long term social relationships between specific individuals as in pair-bonding and friendships. Consequently an important distinction can be made regarding connections between agents in complex adaptive systems: they can be more fluid-like or more solid-like [101]. For example the links between neurons in the brain are relatively fixed in nature when compared to the brief communicative interactions between ants, either instantaneous interactions between individual ants or via transient pheromone trails that coordinate the behaviour of large numbers of ants. Solé and colleagues [101, 88] identify a distinction between solid brains, in which interactions between agents fixed in place are highly persistent in time (e.g. neural networks, spin glasses) and liquid brains, in which interactions between highly mobile agents are much more short-lived (e.g. ants, immune cells). As Solé et al. note regarding liquid brains [101]: “Here there are no neural-like elements and yet in many ways these systems solve complex problems, exhibit learning and memory, and make decisions in response to environmental conditions.” All biological agents are composed of sub-units such as organs, cells, and molecular networks [67, 69, 71]. Cells in particular are the simplest living organisms with individual intelligence, or competencies [67, 33], within their native contexts. Here, we briefly focus on the archetypal single-cell intelligence, the neural cells. It is well understood that the central nervous system is a highly developed, adaptive, complex system that exhibits emergent computational characteristics [52], both in biological and artificial neural networks. Naturally the artificial models are simplifications but the extent to which they are simplifications is not so well understood. In a 2021 study, Beniaguev et al. [9] concluded that between five and eight layers of an artificial deep neural network are required to approximate the input–output mapping of a (single) cortical neuron and that the dendritic branches can be understood as spatiotemporal pattern detectors. This demonstrates that a single neural cell can be modelled as an artificial agent with highly complex computational capabilities situated within an adaptive, complex network of other highly complex agents, all signalling to one another. These results can be compared with earlier studies in which neurons were modelled as a Bayesian agent that is trying to infer the state of a hidden variable [25]. In each of these interpretations, a single cell can be seen as an agent with computational competencies situated within the context of a network that is slowly and adaptively changing around it. We can also compare the competencies of neural cells in networks to the individual competencies of ants in an ant colony. In a recent study [56] it was shown that social structures of some ant colonies are conserved between species that are separated by more than 100 million years of evolution. In the five species studied by Kay et al. [56], they found two social clusters and similarities in the division of labour that are preserved between the species. In a different study, Richardson et al. [91] showed that individuals within an ant colony play an important leadership role and that the behaviour of these individuals significantly improved the collective performance of the ants. Ants are also capable of changing their social structure in the event of pathogenic infestation of their colony. In a 2021 article, Stockmaier and colleagues [102] review the research on social distancing and other social restructuring that occurs with conspecifics in order to reduce the impact of pathogens by changing their social cues, signals, and other behaviours for the collective benefit of the colony. These two very different systems, neural networks and ant colonies, are examples of complex collective intelligences where the individuals (neurons, ants) are complex in their own right, but they signal each other in order to restructure their relationships so as to adapt their collective competencies to external signals. The neural networks are prototypical solid brains and ant colonies are prototypical liquid brains. Human social interactions can also be viewed as a form of liquid intelligence. Migliano et al. [79] discuss the ‘fluidity’ of social relations in early human societies: “Quantification and mapping of hunter–gatherers’ social networks has revealed details of a fluid and multilevel sociality, where friendship links connect unrelated mobile households into camps of temporary composition”. They describe the key characteristics of early human society, such as egalitarianism, division of labour, cooperative living with unrelated individuals, multi-locality, fluid social structures, and high mobility between campsites, which might be thought of as a liquid brain composed of social interactions that both cluster and disperse in order to store, modify, and transfer information via social networks. The notion that human social interaction might be a form of computation is not new: Mirowski, Axtell and colleagues [82, 60, 81] have suggested that economic markets are a form of computation by which prices can be derived, and Harré recently hypothesised [45] that this could be measured using information theory as had been done earlier for financial markets [47, 42]. As Axtell et al. [60] wrote: “There is a close connection between agent computing in the positive social sciences and distributed computation in computer science, in which individual processors have heterogeneous information that they compute with and then communicate to other processors.” The emergence of computation in multi-agent systems is a well-studied area of complex adaptive systems [64, 84]. For example neuroscience has used information theory to describe the storage, transfer, and modification of bits of information in biological neural processes [117]. More broadly, Integrated Information Theory (IIT) [107, 77] has been put forward as a measure of the emergence of ‘consciousness’ in generic (non-biological, non-neural) systems. In this case, some forms of IIT explicitly use information theory [8, 78] to measure the amount of non-trivial computation a system is carrying out. More generally, there is a move towards understanding complex adaptive systems in computational terms [89, 74] by empirically measuring the inter-agent flow of information [12]. In this article we use information theory to quantify how much computation in a CI is ‘emergent’ and how much is simply independent information processing by single agents. In general, we wish to capture the notion of the whole (computational process) being greater than the sum of the (independent) parts. We translate this to the simple notion that to the extent to which this inequality holds: Whole - \sum(Parts)>0 is the extent to which we will say a system exhibits non-trivial CI, noting that there are multiple possible implementations of this approach [54]. The Parts is how much computation a single agent is carrying out from one time step to the next such that the sum is the total of all agents’ independent computations. The Whole is the totality of computation in the system, it includes all single agent computations, pairwise computations, and higher order interactions between agents. Our measure will not be unique in any of its specifics, but it serves to quantify the CI of a system for comparative analysis. This approach also has much in common with that of Moore et al. [84] in which information theory is used to measure the collective intelligence in biological systems. Not only is there diversity in the types of systems that can show positive measures of CI, but the ways in which agents manipulate a system’s computations is diverse as well. Take for example Watson and Levin’s discussion of a scientist manipulating the intercellular signalling in order to change their collective outcome [110]: This framework [of collective cellular intelligence] makes a strong prediction: if intercellular signalling (not genes) is the cognitive medium of a morphogenetic individual, it should be possible to exploit the tools of behavioural and neuro-science and learn to read, interpret and re-write its information content in a way that allows predictive control over its behaviour (in this case, growth and form) without genetic changes. A counter question is: How can single agents, such as human leaders, have predictive control over a social group? Just as a scientist external to a cell collective can manipulate inter-cellular signalling to control the outcomes of the cell collective, a leader internal to a human collective can manipulate inter-personal behaviours to control the outcomes of the human collective. In both cases, an agent with a goal-directed psychology is acting on inter-agent relationships, i.e. inter-cellular or inter-personal, to control outcomes at the next level higher, i.e. organism-scale or societal-scale. In this work we will ask an analogous question of human agents: What is there in human psychology that allows us to learn to read, interpret, and re-write our interpersonal information content in a way that allows predictive control over our collective behaviour? We will not be able to explore all of the possible interpretations of this question here, but we posit that our Theory of Mind (ToM) is a suite of cognitive skills that allows individuals to have goal directed control over collective outcomes. Originally ToM was used to describe our ability to infer the unobserved mental states of other people [34] such as desires and beliefs, an ability humans are particularly good at and other animals much less so [86, 59]. But recently it has been shown that ToM is predictive of group performance as well [121, 31], empirically demonstrating the role of ToM in going beyond representations of the internal states of others to using that knowledge in a social setting to improve the collective outcomes for the group. In order to model ToM in a tractable fashion, we will focus on the narrower game theory of mind [123], and the Beliefs, Preferences, and Constraints (BPC) interpretation of game-theoretic decisions put forward by Gintis [35]. In this approach, what agents understand of other agents’ hidden states are the BPC that structure their observable behaviours. We will consider this question in the framework of agent interactions that extend agent utilities in a simple but novel way. We quantify our results using information theory to show the impact that a correctly deployed ToM has to direct agents’ behaviours in order to increase our CI. The models are simple but they illustrate the central notion that understanding the “beliefs, preferences, and constraints” [36, 37] of others can be used to improve the CI of a complex social system. In Section 2, we describe the liquid–solid dichotomy of interacting agents, review extant models of ToM, and provide perspective on the interplay between social network structures and ToM. In Section 3, we provide illustrative examples supporting different aspects of our argument, introducing our measure of computation and applying it to a simple empirical example. In Section 4 we review the psychology of social fluidity and the variety of social outcomes that this fluidity makes possible. We also use a simple multi-agent system to describe how a ToM can be used to improve the computational processes, i.e. the CI, of interacting agents. Finally, in Section 5, we discuss the broader implications of this approach."
https://arxiv.org/html/2411.09050v1,"The Systems Engineering Approach in Times of
Large Language Models","Using Large Language Models (LLMs) to address critical societal problems requires adopting this novel technology into socio-technical systems. However, the complexity of such systems and the nature of LLMs challenge such a vision. It is unlikely that the solution to such challenges will come from the Artificial Intelligence (AI) community itself. Instead, the Systems Engineering approach is better equipped to facilitate the adoption of LLMs by prioritising the problems and their context before any other aspects. This paper introduces the challenges LLMs generate and surveys systems research efforts for engineering AI-based systems. We reveal how the systems engineering principles have supported addressing similar issues to the ones LLMs pose and discuss our findings to provide future directions for adopting LLMs.","Large Language Models (LLMs) leverage neural network architectures trained on large amounts of data to learn underlying language patterns. LLMs generate content in formats humans understand based on these architectures \parencitefeuerriegel2024generative. Such ability creates novel human-machine interfaces \parencitecabrera2024self for adopting AI at different levels of our society. Generative AI technologies promise new applications for addressing critical problems in diverse domains. The complexity of socio-technical systems and the LLMs’ nature challenge the realisation of this vision. Social problems have critical requirements that demand reliable systems. LLMs rely on probabilistic models that make systems’ components based on these technologies non-deterministic, data-driven, and prone to hallucinations \parencitedantonoli2024large impacting the alignment and reliability of the systems. LLMs operate as black-boxes \parencitefeuerriegel2024generative, which impact systems’ accountability and interpretability as designers and users do not control and understand the systems (i.e., intellectual debt). Social problems usually appear in resource-constrained environments. Building LLMs is an expensive process that causes significant environmental concerns because it generates an immense carbon footprint \parenciteschwartz2020greenai. The outlined challenges require interdisciplinary research to align societal problems, systems, and AI technical advances. The systems engineering approach is equipped with principles to facilitate this alignment by prioritising the problems and their context before considering the technologies for their resolution. We envisage an ecosystem where systems engineering and LLMs mutually benefit instead of the naive belief that benefits come from the LLMs to the domains in one direction. This paper surveys how researchers have used the system engineering approach to design AI-based systems since 2017 (i.e., when current LLM technologies emerged) as a first step to building such an ecosystem. The main research question we want to answer is how does current research use the systems engineering approach to address challenges similar to the ones LLMs impose on socio-technical systems?"
https://arxiv.org/html/2411.08717v1,"Short note on the mapping of heritage sites impacted by the 2024 floods in Valencia, Spain","This short note presents preliminary findings on the impact of the October 2024 floods on cultural heritage sites in Valencia, Spain. Using publicly available data, we assess the extent of potential damage by overlaying flood maps with heritage site coordinates. We identify that 3.3% of heritage sites in the region have been potentially impacted, with churches and shrines (81), outdoor religious iconography (78), and historic irrigation features (45) being the most heavily affected. Our analysis utilizes data from OpenStreetMap and listings from the Generalitat Valenciana, suggesting that while OpenStreetMap’s crowd-sourced data can provide useful estimates of the proportion of impacted sites, it may not be suitable for a detailed damage assessment. By sharing this data openly, we aim to contribute to international efforts in preserving cultural heritage after the disaster and provide a foundation for future assessments of heritage site vulnerability to climate-related events.","In early November 2024, devastating flash floods struck the Valencia region of eastern Spain, primarily affecting the southern outskirts of Valencia city and surrounding areas. The floods, which began on October 29th, were caused by intense rainfall that led to the overflow of the Magro and Turia river basins, as well as the Poyo riverbed. The disaster resulted in at least 219 fatalities, extensive damage to infrastructure, homes, and businesses. We have written this brief note to present preliminary findings on the heritage impacts of the 2024 floods in Valencia. We are sharing this data openly, hoping it may save time to others and contribute, in some small way, to international efforts in solidarity with the affected communities. The data, and the files used for the analysis, are available here: Visit the project repository: https://github.com/jgraubove/danamaps Rapid responses to heritage sites in crisis have increased in recent decades, supported by digital technologies like GIS, and often involving a component of crowdsourcing and volunteer-led efforts. The work of the Heritage Guard Network,a collaborative project between Wikimedia Sweden, Wikimedia Poland, Wikimedia Ukraine and Wikimedia Georgia, is a prominent and recent example [4]. Other initiatives worth noting are Kathmandu Cultural Emergency Crowdmap, an initiative led by ICCROM during the 2015 Nepal earthquake [7] and an initiative led by Wikimedia after the 2018 National Museum of Brazil fire [5]. As the climate crisis intensifies, these initiatives will be more necessary. A scientific objective of this paper is to determine whether open, crowd-sourced data sources are useful for an initial appraisal of the potential damage to heritage."
https://arxiv.org/html/2411.08608v1,Comparative study of random walks with one-step memory on complex networks,"We investigate searching efficiency of different kinds of random walk on complex networks which rely on local information and one-step memory. For the studied navigation strategies we obtained theoretical and numerical values for the graph mean first passage times as an indicator for the searching efficiency. The experiments with generated and real networks show that biasing based on inverse degree, persistence and local two-hop paths can lead to smaller searching times. Moreover, these biasing approaches can be combined to achieve a more robust random search strategy. Our findings can be applied in the modeling and solution of various real-world problems.","Random walk is a ubiquitous concept that describes wandering in certain space in which the location where the walker will be in the next moment is chosen randomly. In complex networks it can applied for modeling diverse phenomena like searching through information networks [1], diffusion of information, ideas and viruses in social networks, stock market fluctuations, and solving various problems such as page ranking in the web [19], semi-supervised graph labeling [29, 10], link prediction in graphs [2], and graph representation learning [13, 17]. Since the onset of interest in complex networks, various models of random walk on top of them have been proposed. The standard uniform random walk is based on randomly choosing the next node in the walk with equal probability from all neighbors of the node where the walker currently is. By applying master equation approach [18] or Markov chain theory [12] one can obtain theoretical results for a key quantity in the random walk – the mean first passage time (MFPT), that represents the expected number of steps needed for the walker to reach randomly chosen target for the first time. Using the same formalism, various modifications of the uniform random walk have been applied that exploit the local properties of the network, aimed at improving the search time. One approach is based on the degrees of the neighbors [9], particularly when biasing proportionally to the inverse degree of the next node [6, 4]. Some authors have considered local neighborhood exploration by random walks using marking as well as biasing based on neighbors degrees[5]. In another approach memory is applied where the probability to jump to some next node depends on the current, but also on the previously visited one [3, 4, 7]. Other problems that have recently received attention are random walk on networks with resetting [21], multiple simultaneous random walks [20], and random walk on hypergraphs [8]. The theoretical expressions for calculating MFPT in random walks with one-step memory presented in [4] provide a useful testbed that can be employed for comparing various biasing strategies in relatively small networks. Nevertheless, the findings can be then applied to networks with arbitrary sizes. In this work, we aim to study and combine different approaches with local information in order to see whether further improvement is possible. We study five types of random walks with one-step memory: simple forward going, inverse degree biased, two-hop paths based, persistent, and we introduce a combination of persistent and inverse degree biased. For comparison in our study we also include two standard random walks without memory: uniform and inverse degree biased. Our findings can be applied for potential improvements in the study of a wide range of problems mentioned at the beginning of this introduction. In Section II we describe the theoretical expressions for calculating MFPTs in random walks with one-step memory on complex networks represented as graphs. Several graph searching strategies using such random walks are described in Section III. In Section IV we present the results obtained with the theoretical expressions and numerical simulations on several synthetic and real complex networks, while in Section V we give some general conclusions."
https://arxiv.org/html/2411.08363v1,On Algorithmic Fairness and the EU Regulations,"The paper discusses algorithmic fairness by focusing on non-discrimination and a few important laws in the European Union (EU). In addition to the EU laws addressing discrimination explicitly, the discussion is based on the EU’s recently enacted regulation for artificial intelligence (AI) and the older General Data Protection Regulation (GDPR). Through theoretical case analysis, on one hand, the paper demonstrates that correcting discriminatory biases in AI systems can be legally done under the EU regulations. On the other hand, the cases also illustrate some practical scenarios from which legal non-compliance may follow. With these cases and the accompanying discussion, the paper contributes to the algorithmic fairness research with a few legal insights, enlarging and strengthening also the growing research domain of compliance in software engineering.","Predictive machine learning models and automated decision-making (ADM) systems have been widely developed and deployed in recent years. Alongside the developments and deployments have been a long-standing debate over biases these models and systems have or may have. Recently, the debate has moved toward revolving around “formalists” and “theorists” (for a lack of better terms), the former seeking to approach biases through formal algorithmic modeling and the latter connecting fairness to broader legal, philosophical, and ethical reasoning [8, 14]. The paper follows the latter group: the goal is to elaborate algorithmic fairness and practical bias correction in relation to a few important EU regulations. Before continuing further, three brief terminological remarks should be made. First, to narrow the scope of concepts and to align with the regulations, a term “AI system” is taken as a synonym with ADM systems based on machine learning. The cases considered in Section IV are all contextualized with AI systems nowadays widely used in recruitment of employees. Second, the difficult concept of algorithmic fairness is narrowed by only focusing on discrimination in the recruitment context considered. In other words, someone might be discriminated by an AI recruitment system because of his or her religion, gender, mother tongue, or even political opinions. Third, the other difficult concept of a bias is framed with a conventional statistical bias involving a non-representative sample with respect to a wider population [14]. This choice simplifies the case analysis and aligns it toward the GDPR because the underlying presumption is that hypothetical developers of an AI recruitment system would seek to correct biases by collecting further personal data of prospective employees. With the partial focus on data protection, the paper contributes to the discussion on the GDPR’s relation to algorithmic fairness. Recently, an argument was raised that the GPDR might need a new exemption for this particular case [19]. As will be elaborated, the EU’s new artificial intelligence regulation has largely addressed this concern. The case analysis highlights also other scenarios under which bias correction can be done under the GDPR. From a broader perspective, the paper also contributes to the growing research domain of compliance in software engineering [10]. To this end, the opening Section II introduces the EU laws considered in the case analysis. Then, the analytical approach for the case analysis is briefly discussed in Section III. After presenting the cases in Section IV, a conclusion follows in Section V."
https://arxiv.org/html/2411.08294v1,Collaborative Participatory Research with LLM agents in South Asia: An Empirically-Grounded Methodological Initiative and Agenda from Field Evidence in Sri Lankan,"The integration of artificial intelligence into development research methodologies presents unprecedented opportunities for addressing persistent challenges in participatory research, particularly in linguistically diverse regions like South Asia. Drawing from an empirical implementation in Sri Lanka’s Sinhala-speaking communities, this paper presents an empirically-grounded methodological framework designed to transform participatory development research, situated in the challenging, multilingual context of Sri Lanka’s flood-prone Nilwala River Basin. Moving beyond conventional translation and data collection tools, this framework deploys a multi-agent system architecture that redefines how data collection, analysis, community engagement are conducted in linguistically and culturally diverse research settings. This structured, agent-based approach enables participatory research that is both scalable and responsive, ensuring that community perspectives remain integral to research outcomes. Our field experiences reveal the immense potential of LLM-based systems in addressing long-standing issues in development research across resource-limited regions, offering both quantitative efficiencies and qualitative improvements in inclusivity. At a broader methodological level, this research agenda advocates for AI-driven participatory research tools that maintain ethical considerations, cultural respect, and operational efficiency, highlighting the strategic pathways for deploying AI systems that reinforce community agency and equitable knowledge generation, potentially informing broader research agendas across the Global South.","The convergence of artificial intelligence and development research heralds a transformative paradigm shift in participatory methodologies, particularly through the emergence of Large Language Models (LLMs) and their potential to revolutionize community engagement practices Mohamed et al. (2024); Skirgård et al. (2023). As these technologies rapidly evolve, their application to development research presents both unprecedented opportunities and complex methodological challenges that demand careful examination Roberts et al. (2024). This intersection becomes particularly significant in linguistically diverse regions like South Asia, where traditional research approaches have long struggled to bridge communication gaps and cultural divides Kshetri (2024); Hassan et al. (2023). The limitations of conventional participatory research methodologies, heavily dependent on human intermediaries and constrained by resource availability, have historically impeded the scale and effectiveness of development initiatives Göpferich and Jääskeläinen (2009). These constraints are particularly evident in regions characterized by complex linguistic landscapes and limited technological infrastructure Magueresse et al. (2020); Nekoto et al. (2020). However, recent advances in LLM architectures, particularly in few-shot learning and cross-lingual transfer capabilities, offer promising solutions to these longstanding challenges Raiaan et al. (2024); Wu et al. (2023). The integration of LLM-based systems into participatory research frameworks raises fundamental questions about the nature of community engagement and knowledge democratization Hadi et al. (2024); Diab Idris et al. (2024). While these technologies offer powerful tools for bridging linguistic and cultural divides, their deployment must be carefully orchestrated to enhance rather than diminish the participatory nature of development research Rane et al. (2023); Kovač et al. (2024). This necessitates a nuanced approach that balances technological capabilities with ethical considerations and community agency Sabarirajan et al. (2024); Ray (2023). In this paper, we introduces and tested a novel framework for leveraging LLM-based multi-agent systems in participatory development research, drawing from empirical evidence in Sri Lanka’s Sinhala-speaking communities Hashmi et al. (2024); Urwin et al. (2023). Our approach moves beyond simple technological integration to address fundamental questions of community empowerment and knowledge production in Global South contexts Pfeffer et al. (2013). The urgency of this work is underscored by the increasing complexity of development challenges and the growing need for scalable, culturally sensitive research methodologies van Rensburg and van der Westhuizen (2024); Awad et al. (2016). Through critical analysis of both opportunities and challenges, we demonstrate how thoughtfully deployed AI technologies can enhance human capabilities in development research, potentially leading to more inclusive and impactful outcomes Ferdaus et al. (2024). Our framework provides a structured approach for implementing LLM-based multi-agent systems while maintaining core principles of participatory research, offering insights for researchers, practitioners, and policymakers working at the intersection of technology and development. This paper contributes to ongoing discussions about the role of artificial intelligence in development research by providing a structured framework for implementing LLM-based multi-agent systems in participatory research contexts. We argue that these technologies, when thoughtfully deployed, can enhance rather than replace human capabilities in development research, potentially leading to more inclusive, efficient, and impactful research outcomes."
https://arxiv.org/html/2411.08241v1,A Social Outcomes and Priorities centered (SOP) Framework for AI policy,"[Abstract]Rapid developments in AI and its adoption across various domains have necessitated a need to build robust guardrails and risk containment plans while ensuring equitable benefits for the betterment of society. The current technology-centered approach has resulted in a fragmented, reactive, and ineffective policy apparatus. This paper highlights the immediate and urgent need to pivot to a society-centered approach to develop comprehensive, coherent, forward-looking AI policy. To this end, we present a Social Outcomes and Priorities centered (SOP) framework for AI policy along with proposals on implementation of its various components. While the SOP framework is presented from a US-centric view, the takeaways are general and applicable globally.","Developments in the field of AI have been rapid and continue to gather momentum. The application of AI spans a variety of domains in both public and private sectors, and continue to accelerate. Further, recent developments in AI such as Generative AI have resulted in a fundamental evolution of the types of AI systems – systems that are not just stochastic but also manifest themselves in ways that exhibit previously unknown characteristics and behaviors (e.g., hallucinations 1, 2). Approaches that rely on studying these systems as natural systems are bound to be very deficient in understanding their behavior, let alone quantifying them, since both the uses and coverage of these systems are neither limited nor well-understood. To add to that, the underlying system design isn’t static and continues to evolve. We are sorely lacking a good understanding on the workings as well as proper, principled, tractable evaluation and validation of these AI systems. Unlike even a couple of decades ago, this rapid evolution is accompanied by parallel developments in other relevant areas such as computing, semiconductors, large scale data availability, rapid adoption and integration capabilities, and relatively much shorter time to market -– i.e., reach to both enterprise and retail consumers. There are market competitive forces further pushing the AI technology makers (interestingly both in the industry and academia) to release products and services powered by these technologies at an increasing pace. Hence, the cumulative effects of these developments, market forces, and the competitive pressures are posing a unique set of challenges – quite unlike what we have seen with critical technologies in the past where either the knowledge, ability to develop, or access (to the technology itself, or its building blocks) was typically restricted. Consequently, maturity of AI models and systems notwithstanding, their adoption has picked up significant pace 3 (the estimates in 3 are likely overly optimistic for GenAI adoption but the overall intent and industry efforts certainly continue to push for further penetration of technology, and other AI methods and techniques are already deeply entrenched in many areas). As adoption grows so do the challenges. These challenges are both specific and potentially systemic. More importantly, the debate on the effects and impact of AI on humanity tends to largely be held in abstract. On one end, proponents maintain AI as a type of silver bullet solution to everything that ails us, while the arguments on the other end maintain that AI poses existential risk in mostly overt ways. It can be argued that AI poses systemic risks in various ways that can have significant deleterious implications for our society – in fact they are manifesting already. However, most of the AI risk debate is around some hypothetical future scenario when AI systems will take over and make decisions bypassing human control (an advanced AGI scenario which we are far from and neither have a realistic timeline nor understanding of) that can bring about catastrophic consequences on humanity – scenarios such as nuclear warfare and bio-weapon risks.†††These and other catastrophic risks are very real in today’s world but currently contingent on human actions not a consequence of AI’s automated-decision-making risk. We humbly contend that these two extreme projections on the spectrum of AI’s opportunities and risks not just misrepresent the stakes but distract us collectively from both much needed urgent actions and an informed lasting policy framework – one that can address our present and immediate needs and realistically and pragmatically scale with the growth of AI and other developments. Neither the hyperbole around the all-encompassing benefits of AI, nor the doomsday scenarios of AGI taking over human decision making seem imminent and realistic enough currently so as to be actionable upon, but the risks and challenges arising from various relevant applications of AI are already here, and presently. Even when there are relatively more calibrated efforts in outlining the risks from autonomous AI systems 4, 5, these are typically futuristic scenarios for which the current state and sophistication of AI doesn’t suffice. And this is an important point. Both – the intentional hype and/or overestimation of AI’s current capability and the haste in putting immature, unreliable and untested AI capabilities in production to automate various critical tasks – are themselves dangerous. They not just expand the risk vectors but also open up possibilities for unintended system failures leading to catastrophic consequences. Among the present challenges and imminent risks from AI potentially in conjunction with other concurrent technological developments that confront us are: • Introduction of risks to products and services. Both due to a lack of proper testing framework and lack of incentive or requirement to do so, AI products and capabilities often lack robust testing, evaluation, validation and verification (V&V). This results in a lack of rigorous vetting of the products entering the market and decision-making workflows. Among the most well-known recent examples of such risks are the hallucination and deepfake issue with GenAI products 6, 7, 8. However, there are several additional examples leading to safety- and privacy-concerns among others 9, 10, 11, 12. • Security vulnerability. Rapid introduction of half-baked products in business critical applications have opened up new fronts on data and security risks, and cyber-security lines of attacks. These risks are not just in terms of adversarial attacks and system compromise but also data compromise resulting from the manner in which the AI technologies are modeled and deployed 13, 14, 15, 9, 16. • Impact on society and democracy. From potential for social engineering, election interference, mis- and dis-information, (misleading) partial information, to sowing division are only some of the risks that we need to contend with because of the combination of AI, mobile, and data technologies introduced and adopted across the world 17, 18. • National security implications on important areas including IP, business competitiveness, cyber-security, and critical infrastructure 19, 20, 21. • Lack of our collective capability to sufficiently understand and foresee, let alone address, the impact of AI-powered products and services be it on labor and workforce, social equity and equality, markets and consumers, or corporate responsibility 22, 23, 24. • Ethics and fairness challenges and concerns. These range all the way from data utilization, governance, and IP violation of human generated content to responsible use and societal implications around bias, discrimination, potential for social engineering, intended and unintended deleterious consequences 25, 26. • Risks and concerns emanating from relatively widespread open-source availability of novel AI technological developments (both in the US and globally) 27, 28, 29, 30, 31. • Challenges from AI in conjunction with other technology areas including quantum computing, blockchain, robotics, and Internet of Things (IoT) that may currently be escaping our focus and foresight 32, 10. • Less understood systemic and systematic effects of AI-driven products on existing functions and areas such as education, healthcare, psychology, and social harmony 33, 34, 35. Various policy efforts across the globe at both industry and government levels have been and continue to be proposed. While most of these efforts are mainly guidance and lack teeth, the others are frameworks that focus on the specific sub-areas and/or technical aspects. All of them further are decoupled from the intended outcomes and lack clear definition and metrics of success. Consequently, we continue to discuss, hype, and claim advances on the AI policy front but have little to show for it in terms of intended social and societal results. Even when the proposals discuss the outlines of the expected outcomes or benefits to society, they are grossly missing in specifics and roadmap to achieve desired outcomes. Different policy elements and areas continue to be proposed independently without necessarily informing or coordinating with each other. Furthermore, we lack a core strategy to manage and reconcile them. Consequently, even though there is a lot of activity and efforts around building robust AI policies, standards, and guardrails, they are bound to miss in most cases since they are neither anchored in the expected outcomes nor systematically managed within a clear framework. Our current policy efforts fall under two broad categories. The first set of efforts focuses on placing very high-level, rather vague guidance that AI systems should benefit society and do no harm. While this is a worthy vision, it doesn’t give any clarity or specifics on how to achieve that goal – what outcomes and effects are desirable? What are the ones that society can adapt to and accommodate? Which outcomes are deleterious? Which are outright dangerous and risky, to be avoided at all costs?. There is also an inherent subjectivity involved in such specifics based on the application, domains, user-groups, sensitivity of the technologies involved, sensitivity and the level of risk involved, and short-term and accumulated risk scenarios. They do not clarify what type of overall policy approach can advance our objective of maximizing AI’s benefits while minimizing the risks, and how it would be achieved. That is, we are missing a consensus social prioritization of the intended outcomes and an associated policy framework along with regulatory and standardization mechanism. Naturally, in the absence of any anchor or specificity, combined with the vagueness of guidance, these sets of policy suggestions have been rendered inactionable. As a consequence, most of the focus and energy of the AI policy discourse has been usurped by the second set of efforts - those focused just on the technical aspects of AI, resulting in a solely technology-centered approach. However, there is little on which this approach is anchored in terms of its ability to achieve the intended outcomes. Unfortunately, society is mostly missing from the work on AI policy for society. What we need is a comprehensive policy framework guided by social priorities and associated outcomes. This should account for various aspects of AI development and deployment life-cycle, AI’s implications on different application areas and domains, resultant effects and risks on various realms of society, implications on national and corporate security, accountability and risk mitigation from AI-driven offerings, and guardrails against unwanted and unintended near- and long-term consequences, all anchored in outcomes that support the societal priorities including democracy, fairness, equity and equality, information provenance, privacy, and security. That is, the intended outcomes should inform the policy priorities and the use of AI in various areas. This would not just help us optimally secure against the risk of AI but will also help maximize the use and adoption of AI for social benefits. Moreover, the innovation in the field can be guided productively and efficiently against this backdrop of social priorities. As a simple example, hallucinations in LLM’s is a well recognized problem and likely impossible to be completely solved in the way GenAI algorithms are designed currently 1, 2. Hallucinations can pose a very high risk in various areas such as defense or safety-critical application and should be managed both in technical and utilization sense. However, there are areas where they may not pose a critical risk –- areas such as creative art (assuming concerns around IP and copyrighted materials are resolved). Hence, policy framework should be flexible on where and under what circumstances is AI adoption helpful and permissible, and under what conditions they should have guardrails or can be outright impermissible. Also, various application and domains would demand different levels of stringency of such guardrails but this can be achieved only if there is a clear understanding and agreement on what types of outcomes are desired and allowed. Similarly, AI recommender systems can have fewer guardrails when applied to shopping recommendation but significantly stringent ones for critical areas such as content recommendation on social media to minimize the unintended and undesired consequences. There is an additional dimension on permissible uses of AI despite the guardrails and an associated accountability and enforcement mechanism - that of adversarial or unethical actors. For instance, there are various proposals around dealing with deepfakes relying mostly on the developers adhering to the established protocols around using AI in a principled manner. This, of course, doesn’t protect us from nefarious or unethical uses for which the legal framework remains unprepared. An example would be the role of deepfakes in degradation of women as detailed in 36, 8. Not only does this exemplify the unwanted use of AI but also the limits of our societal safeguards in extending protection against such uses. It would be very difficult to address such issues if the view on AI’s use is entirely technology-centered and decoupled from broader social and societal priorities. The social implications and costs are immense and AI in such cases is just an enabling technology that accelerates the technical path for these unwanted efforts (see, for instance, 33 for various dangers to the youth from social media that need our immediate and continued attention). A framework that reconciles our social priorities with the associated guardrails against AI’s use as well as the accountability-, legal- and enforcement-apparatus is critically important. This paper proposes a Social-Outcomes and Priorities based (SOP) framework for AI Policy. The SOP framework re-frames the discussion of AI policy in a society-centered approach, a departure from a technology-centered approach that has been adopted so far. The paper outlines the core components of the SOP framework along with suggestions on their implementation, recommendations on how the framework can leverage the existing policy, regulatory, and legal apparatus and further advises on how these can be enhanced, strengthened and updated in the context of an evolving AI landscape. The rest of the paper is organized as follows: Sec. 2 provides background on the main arguments currently driving the policy discussion resulting in fragmented technology-centered efforts, and their limitations. Sec. 3 sets up the discussion on the main constituents that an effective policy framework should cover. Sec. 4 then introduces the main proposal – a social-outcomes and priorities centered (SOP) framework for AI policy. It also details the main functional components of this framework along with proposals on their implementation. Even though the implementation proposals for various functions are presented from a US-centric viewpoint, the core framework is applicable globally. Sec. 5 then details how an SOP framework can be beneficial and contextualizes it with the application. We finally conclude with a call for action in Sec. 6 highlighting both the urgency and importance of implementing such an effective policy framework."
https://arxiv.org/html/2411.08088v1,"Safety case template for frontier AI: 
A cyber inability argument","Frontier artificial intelligence (AI) systems pose increasing risks to society, making it essential for developers to provide assurances about their safety. One approach to offering such assurances is through a safety case: a structured, evidence-based argument aimed at demonstrating why the risk associated with a safety-critical system is acceptable. In this article, we propose a safety case template for offensive cyber capabilities. We illustrate how developers could argue that a model does not have capabilities posing unacceptable cyber risks by breaking down the main claim into progressively specific sub-claims, each supported by evidence. In our template, we identify a number of risk models, derive proxy tasks from the risk models, define evaluation settings for the proxy tasks, and connect those with evaluation results. Elements of current frontier safety techniques—such as risk models, proxy tasks, and capability evaluations—use implicit arguments for overall system safety. This safety case template integrates these elements using the Claims Arguments Evidence (CAE) framework in order to make safety arguments coherent and explicit. While uncertainties around the specifics remain, this template serves as a proof of concept, aiming to foster discussion on AI safety cases and advance AI assurance.","Frontier artificial intelligence (AI) systems offer many benefits, but they are also being used to cause harm. For example, AI-generated synthetic media are increasingly used to fabricate false narratives for political manipulation or to produce non-consensual deepfake pornography [66, 70]. AI systems enable cybercriminals to personalise and automate phishing campaigns [30] and could allow authoritarian governments to enhance their surveillance capabilities [1]. Additionally, the growing integration of large language models (LLMs) across different sectors and functions risks perpetuating various social biases embedded in the underlying datasets [26]. It is probable that more capable AI systems will entail heightened risks [15, 23, 41, 58, 71]. Concerns have been raised that future systems could aid malicious actors in the development of biological weapons or lower the level of expertise required for executing increasingly sophisticated cyberattacks [24, 29, 68]. More speculatively, AI systems might become challenging to evaluate or control [15, 23, 41, 58, 71]. For these reasons, it is increasingly important for AI developers to demonstrate that their systems are sufficiently safe to deploy.111Our safety case template focuses on deployment decisions, though assurances around pre-deployment decisions (e.g. whether to train a model) are also important. One assurance strategy gaining traction for frontier AI is safety cases [7, 11, 17, 34, 73]. A safety case provides a structured and substantiated argument for why the risk associated with a safety-critical system is acceptable [38]. This method has been used in other sectors, including nuclear energy, offshore development, aviation, railway, software, and autonomous vehicles [10, 20, 25, 51, 72, 74]. Several scholars have recently discussed the possibility of applying safety cases to frontier AI [11, 6, 64, 5]. Frontier AI developers, including Anthropic [3] and Google DeepMind [27], have also expressed interest in moving in this direction. Additionally, safety cases are consistent with the Frontier AI Safety Commitments announced at the AI Seoul Summit 2024, which emphasise the need for comprehensive safety assessments to ensure responsible AI development and deployment [22]. That said, there is no readily available safety case methodology for frontier AI. While practices in other industries may provide useful guidance, the particularities of AI systems and associated risks require a tailored approach. Given the state of AI safety research, producing a holistic and scalable safety case—a comprehensive argument that addresses all potential risks across the system's lifecycle—does not seem feasible today for a frontier AI model. Still, we believe it is useful to develop safety case templates [7]. Such templates detail the argument structure and evidence for various parts of an AI safety case and may serve as building blocks for a full safety case in the future [34]. This article seeks to contribute to AI safety case methodology by sketching a safety case for cyber capabilities. This is a structured argument explaining why a given AI system does not cause a significant increase in cyber risk due to limited capabilities. We focus on cyber because the near-term risk is relatively established and because we have a relatively developed understanding of risk models [53, 54, 77, 79]. We focus on inability (‘the system is not capable of X, even absent any safeguards’) because it offers the simplest and best understood argument for why a system will not behave in a certain way. For current systems, developers largely rely on implicit or explicit inability arguments to assure safety. Other arguments for why a model will not behave in a certain way include control (‘the system is not capable of X, given existing safeguards’) and trustworthiness (‘even if the system is capable of X, it will consistently behave in a desirable way, thus avoiding X’) [17]. Finally, this safety case is structured to inform deployment decisions. We expect that safety cases for more capable systems will also be useful for different decisions, such as whether to start or continue training runs. This template serves as a proof of concept, illustrating that such a safety case could be viable in principle, while acknowledging we have significant uncertainties around the specifics. It builds on current frontier AI safety techniques, in particular model evaluations, which are already integral to safety efforts of major frontier developers [2, 57, 60]. This safety case template should be seen as an attempt to make explicit the implicit argument for safety based on these model evaluations. It does not guarantee safety; some of the claims in our template could fail to hold true in reality, invalidating the conclusion. Still, we expect that even these imperfect safety cases serve to increase the level of rigour in reasoning about development or deployment decisions. This can improve the confidence of developers—and, by extension, governments and the public—in claims about the safety of frontier AI systems. Being explicit about safety reasoning can also unveil specific points of disagreement about safety approaches. In the same vein, we hope this work lays the foundation for structured discussions on how to write AI safety cases, thereby advancing the frontier of AI assurance. The article proceeds as follows. Section 2 reviews related work on AI safety cases. Section 3 provides an overview of the structure and components of our safety case template. Section 4 details a safety case template for offensive cyber capabilities. Section 5 concludes with a summary of the article’s main contributions and suggestions for further research."
https://arxiv.org/html/2411.08450v1,"DecentPeeR: A Self-Incentivised & Inclusive 
Decentralized Peer Review System","Peer review, as a widely used practice to ensure the quality and integrity of publications, lacks a well-defined and common mechanism to self-incentivize virtuous behavior across all the conferences and journals. This is because information about reviewer efforts and author feedback typically remains local to a single venue, while the same group of authors and reviewers participate in the publication process across many venues. Previous attempts to incentivize the reviewing process assume that the quality of reviews and papers authored correlate for the same person, or they assume that the reviewers can receive physical rewards for their work. In this paper, we aim to keep track of reviewing and authoring efforts by users (who review and author) across different venues while ensuring self-incentivization.To this end, we introduce DecentPeeR, a system that captures the interactions of users who use a peer review system as decentralized reputation scores. We show that our system incentivizes reviewers to behave according to the rules, i.e., it has a unique Nash equilibrium in which virtuous behavior is rewarded. Furthermore, we detail how our design ensures inclusivity, i.e., giving everyone a fair chance to publish, especially when facing dishonest users. We also report on empirical results that show the incentive mechanism works: dishonest individual and group behavior are penalized, but it is possible to recover from a poor score over time.","Peer review systems are widely used and have an extensive impact in today’s academia. Use cases of peer-review ranges from the scientific publication process [1, 2] to open source software development [3, 4]. With the popularity of peer review systems, various methods have been proposed to make the peer review procedure more inclusive. With inclusivity, authors have a chance to publish their work solely based on its quality. Ensuring inclusivity is challenging in the academic peer review process: submissions on different research topics may not be comparable; reviewers may have personal opinions depending on the topic of the submission; due to large amounts of published papers, evaluations from only few reviewers can be used decide on the quality of a submission. Previous efforts to ensure inclusivity range from enforcing prior-announcement of conflict-of-interest [5, 6], double-blindness [7, 8], and actions from the editor to promote quality, integrity, and fairness [9]. Most traditional solutions are focused on how to make a single conference more inclusive. However, authors and reviewers likely take part at multiple venues over their careers. Thus, a cross-venue measure would be more viable today, given the advancements in decentralized technologies. In this work, we keep track of the actions of users over time using a reputation system to ensure inclusivity. We build a decentralized system where users tend to follow the rules of the system based on their best interests. While incentivizing users who behave rationally, the system should not punish academic work of good quality and thus violate inclusivity. To this end, we develop a self-incentivized system based on a game theoretical approach, showing that achieving the unique Nash equilibrium is only possible by adhering to the rules of the system. We detail our system, DecentPeeR, from the perspective of an academic who wants to contribute to or organize a conference. In doing so, we also benefit from the decentralized storage mechanisms provided by blockchain technology [10, 11] that is used to keep a history of peer review systems’ data across different venues. We detail our system, DecentPeeR, from the perspective of an academic who wants to contribute to or organize a conference. In doing so, we also benefit from the decentralized mechanisms provided by blockchain technology (e.g., to keep a history of peer review systems’ data across different venues). I-A Design Goals Our aim is to design our peer-review system that satisfies the following main goals: Self-incentivization. Typically, peer review systems assume that participants are well-behaved [12], that reviewers can be assigned a reliability score [13], or that at most a small percentage of reviewers is biased [14, 15]. However, these rules might be neglected, resulting to the raise of adversarial reviews [16]. As a remedy, we aim to ensure that it is in the reviewer’s own best interest to respect rules of the system, i.e., through a provable self-incentivization. Inclusivity. A reputation system should be flexible and adaptive and in particular also support new users of the system. Our goal is to design a system where everyone would have a chance to contribute good quality work, mostly independent of their scores, and where users with bad scores have a chance to recover from a failure. Cross-venue evaluation. Academic users do not encounter peer-review systems only once: they participate in multiple venues (conferences, journals, workshops, etc.) throughout several years. Hence we aim to design a system that benefits from this fact and also aggregates data over time. I-B Contribution In this paper, we design a self-incentivized and inclusive peer review system, DecentPeeR, that works across venues and tracks. Honest reviewing behavior is rewarded with a positive influence on future borderline-scored submissions by the authors. In addition, the reviewing score provides committee chairs with a criterion to select the committee. We present a peer review game in this paper and show that it has a unique Nash equilibrium where the users play honestly. We then analyze the desired properties of the peer review game, such as inclusivity and cross-venue evaluation. Our method incorporates three mechanisms that lead to a high level of inclusivity: • Our system only considers the reputation score in borderline cases: if the high quality of a paper is already agreed upon, we consider that paper as accepted. • Our system uses a randomness mechanism to form a program committee and a reviewing team: the weighted randomness ensures that selected users can be trusted while giving chance to every user to participate. • A reputation score function has been implemented with the goal of ensuring fast recovery for users who have limited misbehavior. We show that the cross-venue aspect allows us to quickly detect adversarial behavior by reviewers. Once an adversarial reviewer however behaves correctly again, the corresponding score of the reviewer recovers. We conclude by showing that a majority attack, where adversarial reviewers collaborate in order to evaluate the paper dishonestly, is unlikely under the uniformly chosen reviewers in the review assignment. Our paper is organized as follows: we follow by relating our paper to previous works. We then detail DecentPeeR design in §II and analyze it in §III. Finally, we go over a few case studies in §IV and conclude our work in §V. I-C Related Work Peer review is a broad research topic that has been investigated from many aspects over the past years. For example, empirical studies have been conducted on peer review for classroom use [17, 18], for conference reviews [19, 20], and for funding applications [21]. In the following, we discuss different perspectives on peer review systems, coming from theoretical research as well as practical systems. Blockchain-based peer review. Our peer review system can be implemented on a blockchain-based system that supports smart contracts, like Ethereum. The decentralized nature of blockchain-based protocols has previously motivated many researchers to utilize its advantages for new designs of peer review systems. Our work is also a building block towards a Decentralized Science (DeSci) future [22]. Initial proposals for an alternative blockchain-based peer review systems [23, 24] focus on providing an alternative coin instead of Bitcoin. These attempts to alter a financial system for peer review, however, are inherently flawed: wealthy participants can game the system to their benefit. Another set of blockchain-based peer review systems focused on providing a decentralized platform to store information exchanged during a peer review process [25, 10], mostly leveraging the advantages provided by IPFS. Although it is possible to use the proposed peer review systems across venues, the systems lack the essential requirements that ensure fair treatment of all users when being used across venues. More recent systems [26, 27] aimed to tackle the challenge of a collaborative system. In doing so, they showed what are possible ways to provide self-invitation based on a game-theoretic perspective. In a nutshell, they showed that the allowed rules of the game are in the best interest of all users. However, they did not show what happens when a failure happens and how the system could recover. Furthermore, they did not consider the fact that not only a single venue exists, and a system should not be restarted whenever a new request for a venue appears. We summarize the main properties of the presented distributed peer review systems in Table I. System Cross-venue Self-incentivising Inclusive PubChain [24] ✓ ✗ ✗ Blockchain and Kudos [23] ✓ ✗ ✗ IPFS-based [25] ✓ ✗ ✗ Data Marketplaces [26] ✗ ✓ ✗ Collaborative Research [27] ✗ ✓ ✗ DecentPeeR (our work) ✓ ✓ ✓ TABLE I: Comparing DecentPeeR with previous decentralized peer review systems. Social choice perspective on peer review. In social choice studies, peer review has been investigated as an assessment method for grading homework and exams[28], programming classes[29], and conferences[30]. Some of these works focus on finding the right aggregation rules, by investigating cardinal voting rules instead of ordinal grading [31, 32] or by showing specific properties of peer review protocols, such as strategyproofness [30]. Other papers focus on determining truthful reviewers by assuming that the grading is performed with assistance [33]. There are also studies on reputation-based peer grading. Leaning on PageRank, Walsh proposed a PeerRank system [34]. In this system, each reviewer gets a reputation score, which is computed by the grades given by the reviewer weighted by the reputation score of the reviewer. Note that a reputation score calculation is connected to the grade of the submitted work, which is not necessarily true in conference reviewing. Reputation systems. Reputation systems are one of the key tools to establish trust in an untrustworthy environment [35]. For example, they have an established place in designing distributed systems, especially in connecting peers in a peer-to-peer network [36]. They have also been considered in other applications such as e-commerce [37] and transportation [38]. Most of the previous work that considers a reputation system on blockchain [39, 40] uses the reputation score as an alternative mining protocol, which is orthogonal to how we used it in our system. Similarity detection mechanisms. The digital age has made it easier to reuse the efforts of others, and hence, from early on, it was critical to create similarity detection mechanisms [41]. Also, for the peer review process similarity detection is inevitable, as it enables editors to verify the integrity of a research work. With the rise of generative AI tools, advanced similarity detection mechanisms received even more attention, especially because it became possible to rehash ideas of others into undetectable results (even by trained eyes) [42]. To detect such behavior, context-oblivious methods have been proposed, such as fingerprinting [43], text matching [44], or compression [45]. The latter one can run fast but might have low accuracy due to false negatives. In general, context-aware similarity detection approaches allow us to reduce false negatives, but they require much more computational power. As an alternative, latent semantic analysis [46] or transformer-based tools [47] have been considered in the literature. In this work, we do not focus on a particular similarity detection mechanism, but rather assume that such methods exist and can be deployed by the system designer."
https://arxiv.org/html/2411.08425v1,Properties of fairness measures in the context of varying class imbalance and protected group ratios,"Society is increasingly relying on predictive models in fields like criminal justice, credit risk management, or hiring. To prevent such automated systems from discriminating against people belonging to certain groups, fairness measures have become a crucial component in socially relevant applications of machine learning. However, existing fairness measures have been designed to assess the bias between predictions for protected groups without considering the imbalance in the classes of the target variable. Current research on the potential effect of class imbalance on fairness focuses on practical applications rather than dataset-independent measure properties. In this paper, we study the general properties of fairness measures for changing class and protected group proportions. For this purpose, we analyze the probability mass functions of six of the most popular group fairness measures. We also measure how the probability of achieving perfect fairness changes for varying class imbalance ratios. Moreover, we relate the dataset-independent properties of fairness measures described in this paper to classifier fairness in real-life tasks. Our results show that measures such as Equal Opportunity and Positive Predictive Parity are more sensitive to changes in class imbalance than Accuracy Equality. These findings can help guide researchers and practitioners in choosing the most appropriate fairness measures for their classification problems.","Machine learning systems are increasingly being used to make decisions that affect people. Despite the benefits often associated with improving or speeding up tasks, there is also an awareness of the risks associated with such systems. In particular, there is a general agreement that machine learning models need to be controlled to maintain fairness and avoid biased decisions. These issues are reflected in recent AI guidelines, such as the proposed EU regulation on AI (Commission, 2021) and the recently accepted UNESCO recommendation on ethics in AI (UNESCO, 2021). The fairness of machine learning models is also a key research driver toward Responsible and Trustworthy AI (Wing, 2021). Fairness in machine learning refers to the idea that predictive systems should be designed and operated in a way that is fair and just to all individuals and groups (Mehrabi et al., 2021). This means that machine learning models should not discriminate against people based on their race, gender, age, or other personal characteristics. Therefore, approaches that mitigate unfairness are based on the notion of protected attributes (sometimes also called sensitive attributes) that define protected and unprotected groups, i.e., groups that are disproportionately less or more likely to be positively classified. Practical machine learning applications with protected attributes include automated hiring procedures (Schumann et al., 2020), credit scoring (Khandani et al., 2010), and criminal justice (Berk et al., 2021). In this context, several measures that assess fairness towards protected groups have been put forward (Mehrabi et al., 2021). However, current research focuses mainly on proposing methods that improve the fairness of machine learning models (Friedler et al., 2019) or studying correlations between fairness measures in practical applications (Anahideh et al., 2021), rather than investigating general dataset-independent properties. Therefore, providing general advice on which fairness measures are best suited for a given case study is hard. Moreover, by focusing on dataset-oriented evaluations, current studies do not answer questions concerning the behavior of fairness measures in the presence of different types of bias in the data (representation bias, skewed distributions, feature bias). In particular, there are no studies connecting theoretical properties of fairness measures with different levels of class imbalance, i.e., situations where at least one of the target classes contains a much smaller number of examples than the other classes (He and Garcia, 2009). Such a study would be valuable in guiding machine learning practitioners on the use of fairness measures, as many real-world datasets are naturally imbalanced (Branco et al., 2016). Finally, there has been no investigation into how interactions between class imbalance and disproportions between protected and unprotected groups might affect the properties of fairness measures. In this paper, we define general dataset-independent properties of fairness measures that assess their behavior for varying levels of class imbalance and protected group bias. To identify these properties in popular group fairness measures, we analyze their distributions in the context of class and protected group imbalance. As a result, we provide guidelines on which fairness measures are applicable to which types of datasets. The detailed contributions of this paper are as follows: • In Sections 3 and 4, we recall six popular group fairness measures and put forward a method for analyzing fairness measures on the basis of their probability mass functions. • In Section 5, we propose a set of general (dataset-independent) fairness measure properties related to their behavior in the presence of different levels of class imbalance and protected group ratios. We then verify whether the studied six fairness measures possess the proposed properties. • In Section 6, by training six different classifiers in a controlled experiment case study using real-world data with varying class and protected group ratios, we verify whether the identified dataset-independent properties apply to practical classification scenarios. • In Section 8, we formulate guidelines on using fairness measures in different data scenarios and draw lines for future research."
https://arxiv.org/html/2411.08243v1,Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset,"In an effort to mitigate the harms of large language models (LLMs), learning from human feedback (LHF) has been used to steer LLMs towards outputs that are intended to be both less harmful and more helpful. Despite the widespread adoption of LHF in practice, the quality of this feedback and its effectiveness as a safety mitigation technique remain unclear. This study addresses these issues by auditing the widely-used Helpful and Harmless (HH) dataset by Anthropic. Our work includes: (1) a thorough investigation of the dataset’s content through both manual and automated evaluation; (2) experiments demonstrating the dataset’s impact on models’ safety; and (3) an analysis of the 100 most influential papers citing this dataset. Through our audit, we showcase how conceptualization failures and quality issues identified in the HH dataset can create additional harms by leading to disparate safety behaviors across demographic groups. Our findings highlight the need for more nuanced, context-sensitive approaches to safety mitigation in LLMs.Warning: This paper contains model outputs that may be considered offensive.","Learning from Human Feedback (LHF) has gained in popularity in recent years as a strategy to mitigate the harms of large language models (LLMs) while preserving their utility. LHF consists of having human annotators assign relative preferences between model outputs, then training a model to generate outputs which are more likely to be preferred by the annotators. Although the first instances of LHF were in robotics Christiano et al. (2017) and text summarization Stiennon et al. (2020), it has been adopted as a harm reduction strategy by companies like Anthropic Bai et al. (2022), Google DeepMind Gemini Team et al. (2024) and OpenAI (OpenAI, 2023). Among the guiding principles for aligning LLMs, Askell et al. (2021) define an AI system as aligned if it is helpful, honest, and harmless (HHH). Helpfulness refers to the LLM’s capacity to perform a task or answer a question. Honesty pertains to factual accuracy and truthfulness. Harmlessness encompasses a range of considerations. Following the adoption of the HHH principles, recent studies have shifted towards a more integrated approach, recognizing these challenges as part of a broader “safety bundle” of misaligned behaviors (Ouyang et al., 2022a; Bai et al., 2022; Touvron et al., 2023). Anthropic’s Helpful and Harmless dataset Bai et al. (2022) serves as a prototypical example of this trend—a human preferences dataset designed for safety mitigation, cited over 1000 times and used to train more than 200 models.***https://huggingface.co/models?dataset=dataset:Anthropic/hh-rlhf This presents a shift in paradigm, as earlier research in responsible natural language processing (NLP) focused on distinct methods for identifying, quantifying, and mitigating issues such as stereotypical bias (Chu et al., 2024), toxicity (Gehman et al., 2020), and privacy leakage (Huang et al., 2022). Despite the widespread adoption of the HH dataset as a benchmark for models’ safety alignment, it is not clear to what extent this dataset truly succeeds at making LLMs more harmless and more helpful. In fact, recent work shows that models trained with LHF with HH preferences are more likely to exhibit safety failures associated with these preferences and the trade-offs they create (i.e. a safer model is less likely to respond to a query, making it less helpful, and vice versa) (Röttger et al., 2023; Bianchi et al., 2024; Chehbouni et al., 2024) or to showcase superficial safety alignment (Zhou et al., 2023a; Lin et al., 2024) — meaning that the model’s alignment predominantly improves the style of its outputs rather than influencing its underlying knowledge. Furthermore, while various work has looked at the shortcomings of open-source corpora (Gehman et al., 2020; Dodge et al., 2021; Birhane et al., 2023), to the best of our knowledge, none has investigated the content of preference datasets. In this work, we conduct a comprehensive audit to examine how the dataset embodies the HH principles and explore the connection between the conceptualization of these principles and the safety failures reported in the literature. We investigate the following three research questions by evaluating the quality of the dataset and its effectiveness for safety mitigation: (RQ1) What’s in the HH dataset? We provide a thorough audit of the HH dataset through an exploratory analysis as well as a human evaluation and show its various shortcomings, including a failure to conceptualize harmlessness and quality issues (§3); (RQ2) What can models learn from the HH dataset? We train models using different variations of the HH dataset and evaluate them for safety (Röttger et al., 2023). Our analysis reveals that using LHF with the HH dataset can lead to disparate exaggerated safety behaviors across demographic groups (§4); and (RQ3) How did the HH dataset impact the community? We conduct a survey of the 100 most influential papers that reference the original study and offer insights into how the dataset has been adopted by the broader research community as well as how some inherent limitations of the dataset are framed as an inevitable trade-off between helpfulness and harmlessness (§5). Through this multidimensional audit, we showcase the limitations of the “safety bundle” introduced by the HHH principles and operationalized by the HH dataset. In light of these findings, we highlight the need for more nuanced and context-sensitive approaches for safety mitigation as well as a shift in how these considerations are addressed by the community."
https://arxiv.org/html/2411.07705v1,dpvis: A Visual and Interactive Learning Tool for Dynamic Programming,"Dynamic programming (DP) is a fundamental and powerful algorithmic paradigm taught in most undergraduate (and many graduate) algorithms classes. DP problems are challenging for many computer science students because they require identifying unique problem structures and a refined understanding of recursion. In this paper, we present dpvis, a Python library that helps students understand DP through a frame-by-frame animation of dynamic programs. dpvis can easily generate animations of dynamic programs with as little as two lines of modifications compared to a standard Python implementation. For each frame, dpvis highlight the cells that have been read from and written to during an iteration. Moreover, dpvis allows users to test their understanding by prompting them with questions about the next operation performed by the algorithm.We deployed dpvis as a learning tool in an undergraduate algorithms class, and report on the results of a survey. The survey results suggest that dpvis is especially helpful for visualizing the recursive structure of DP. Although some students struggled with the installation of the tool (which has been simplified since the reported deployment), essentially all other students found the tool to be useful for understanding dynamic programs. dpvis is available at https://github.com/itsdawei/dpvis.","Dynamic programming (DP) is an algorithmic technique taught in many introductory algorithms courses as a fundamental part of undergraduate computer science curricula (on Computing Curricula Association for Computing Machinery, ACM; Kleinberg and Tardos, 2005; Cormen et al., 2001). DP exploits the recursive structure of combinatorial problems to break them into subproblems. Then, DP reduces redundant calculations by storing the solutions to prior subproblems in a subproblem array and referencing them to solve later subproblems. As a result, DP often yields polynomial-time algorithms for difficult-looking problems. (For a more in-depth refresher on DP, see Sec. 2.) Anecdotal experience of most algorithms instructors and past research have pointed to pervasive difficulties among students when learning DP (Enström, 2013; Shindler et al., 2022; Zehra et al., 2018). We attribute this to DP’s close relation to recursion and strong demand for high-level mathematical thinking. For instance, many students who are uncomfortable with recursion find it challenging to identify the recursive patterns of the problem (Shindler et al., 2022). Moreover, students often do not evaluate the subproblems in the correct order, resulting in redundant calculations and suboptimal worst-case runtimes (Enström, 2013). To assist students learning DP, we developed dpvis, a library that empowers its users with instructive and interactive visualizations of DP algorithms. dpvis provides three major classes of features: (1) an instructive step-by-step visualization of the dynamic program as it finds solutions in the subproblem array, (2) an interactive self-testing feature that verifies and reinforces the user’s understanding of DP, and (3) customization options for instructors to populate the visualization screen with additional annotations. dpvis is designed with both students and teachers in mind. For students, the first two types of features will likely be most useful: they allow students to use dpvis as a convenient debugging and visualization tool, and to test their own understanding of the execution of a DP algorithm. In particular, both active engagement and leading prompts have been identified as key features towards effectiveness of algorithm visualization tools (Saraiya et al., 2004). For teachers, the third class of feature allows for easy creation of carefully annotated examples for their students’ self-study and testing; again, careful instructions have been identified as a key feature of effective algorithm visualization tools. The central design philosophy of dpvis is for the programming experience to be as similar as possible to implementing a dynamic program in “vanilla” Python. If a user simply wants a basic visualization of their DP array without the need for any additional features, the code is usually not substantially modified. In many cases, the user only needs to change two lines of code to generate a visualization of the dynamic program (Fig. 1). We emphasize that the goal of dpvis is not to implement visualizations for a few particular DP problems, although it provides an easy way to do so. Instead, the library constructs a visualization of the execution of any 1D or 2D DP algorithm, and thus also serves as an important debugging tool for anyone (in particular, students) implementing DP solutions to new problems. We deployed dpvis as a learning tool in an undergraduate algorithms class, and asked students about their experience in an optional survey. Most respondents reported that dpvis had a positive impact on their learning experience (Sec. 4). We believe that the visualization features, interactive testing mode, and flexibility of dpvis allow it to effectively assist instructors in classroom settings. 1.1. Related Work 1.1.1. Existing DP Visualization Tools We discuss several previous DP visualization tools. Unlike other visualizers, dpvis implements an interactive self-testing mode that quizzes the students about the outcome of the next operation performed on the subproblem array (Sec. 3). Other visualizers (Banerji, 2021) may implement test cases (i.e., pass-fail checks to ensure that the implementation solves specific instances of the problem), but none place an emphasis on ensuring the student has a thorough understanding of the recurrence. Easy Hard DP Visualizer (Fraser, 2015) takes arbitrary 1D and 2D dynamic programs written in nearly “vanilla” JavaScript and shows a visualization of the subproblem array while highlighting the dependencies for each subproblem. However, after the initial animation, there is no option to go rewind the animation nor to pause the animation at a specific frame to view the computation in the array at any specific iteration. VisuAlgo (Halim, 2024) illustrates dynamic programs as a directed acyclic graph (DAG) where the nodes represent the subproblems, and edges represent dependencies between them. VisuAlgo topologically sorts the subproblems to iteratively calculate the values of each subproblem in the correct order. Although the graph representation clearly articulates dependencies and the ordering over the subproblems, we observed that the subproblem graph often became cluttered even for moderately sized problem instances. Strictly speaking, both Easy Hard DP Visualizer and VisuAlgo work with recursive functions as opposed to actual dynamic programs. They automatically perform memoization111The process of saving the solution to subproblems is known as memoization. on the given recursive function and produce either an animation of an array (for Easy Hard DP Visualizer) or a DAG (for VisuAlgo). This implicitly hides the array of subproblems, leading to ambiguity over when and where the solutions to the subproblems are stored. By requiring that the memoization be made explicit by the user, dpvis encourages users to have a clear understanding of their recurrence. Algorithm Visualizer (Alg, 2023) and Dynamic Programming Visualization (Banerji, 2021) offer a similar user interface as dpvis, with a controllable frame-by-frame animation showing the subproblem array as it is populated. However, in Algorithm Visualizer, the programmer is expected to manually update an array tracer with the operations that are performed on the array for each frame of the animation. dpvis is significantly more user-friendly by implicitly — and non-intrusively — tracking all READ and WRITE operations performed on the array and automatically determining which operations are visualized in which step of the animation (Sec. 3). Dynamic Programming Visualization (Banerji, 2021) collects a list of DP problems and asks the students to implement a DP algorithm. Akin to Leetcode (Lee, [n. d.]) and competitive programming environments (Cod, [n. d.]; Hac, [n. d.]), students verify the correctness of their implementation through a set of test cases checking the final subproblem array. Although this approach to testing verifies correctness, it does not allow students to demonstrate a thorough understanding of every step of the algorithm, and it restricts the user to the preselected set of problems. 1.1.2. Algorithm Visualization Algorithm visualization (AV) is a well-studied topic in the CS education community. A long line of work in computer science education has provided evidence that the most effective algorithm visualizers are interactive (Hundhausen et al., 2002). Further explorations (Naps et al., 2002) have provided taxonomies for the interactivity of algorithm visualizers (not viewing AVs, viewing AVs, responding to questions about AVs, changing parts of AVs, having students construct AVs, and presenting AVs). Studies have shown that although CS educators resoundingly agree that algorithm visualizations are effective pedagogical tools, they still hesitate to use them in classroom settings (Shaffer et al., 2011; Naps et al., 2002). Only about half of the instructors surveyed by Shaffer et al. (2011) responded that they had used algorithm visualizers in their class over the past two years. The two most common reasons for lack of use were “Trouble with finding suitable AVs” (13 responses) and “Trouble with integrating AV material into course” (11 responses).222For reference, the next most common reason was “Difficulty of making AVs” (3 responses). The survey by Naps et al. (2002) echoes the disconnect between instructors’ beliefs and practice, with similar obstacles named. dpvis is designed specifically for ease of adoption to lower these hurdles. 1.1.3. Dynamic Programming Comprehension Dynamic Programming comprehension has also been studied in the CS education community. Broadly, Zehra et al. (2018) identified three types of thematic difficulties that students face when first learning DP, i.e., subproblem identification, solution technique, and defining a recurrence. A later replication study by Luu et al. (2023) replicates these findings and identifies further difficulties experienced by students. For instance, some of the thematic difficulties arise from a failure to comprehend the applicability of DP, an inadequate understanding of recursion, and difficulty in determining the correct order to compute subproblems. Our library is designed to visualize computation of the subproblems, which will help students understand recursion and determine the ordering of the subproblems. ⬇ 1arr = [-1] * n # a list of length n, initialized to -1 2# sort by f[i] and precompute p[i] values (omitted here) 3arr[0] = 0 4for i in range(1, n): 5 arr[i] = max(arr[i-1], w[i] + arr[p[i]]) (a) Standard Python implementation without dpvis. ⬇ arr = DPArray(n) # sort by f[i] and precompute p[i] values (omitted here) arr[0] = 0 for i in range(1, n): arr[i] = max(arr[i-1], w[i] + arr[p[i]]) display(arr) (b) Python implementation with dpvis visualization. Figure 1. Python implementations of the Weighted Interval Scheduling dynamic program with and without dpvis visualizations enabled. dpvis visualizations can be enabled in two lines: the first line replaces the Python List object with the dpvis DPArray object, and the second line simply generates the visualization for the DParray object. Line changes from the standard Python implementation are highlighted."
https://arxiv.org/html/2411.07512v1,"Ética para LLMs: o compartilhamento 
de dados sociolinguísticos","The collection of speech data carried out in Sociolinguistics has the potential to enhance large language models due to its quality and representativeness. In this paper, we examine the ethical considerations associated with the gathering and dissemination of such data. Additionally, we outline strategies for addressing the sensitivity of speech data, as it may facilitate the identification of informants who contributed with their speech.","1 Introdução O uso da inteligência artificial (IA) tem se tornado cada vez mais presente na vida da população brasileira. Particularmente, o uso de IA na educação é uma realidade (de Oliveira Figueiredo et al., 2023; Leão et al., 2021) a qual, como educadores, tivemos que nos adaptar rapidamente tentando extrair o melhor delas para tornar o processo de ensino e aprendizagem mais interessante. Contudo, o uso dessas tecnologias em quaisquer áreas do conhecimento perpassa por questões éticas cujo debate no cenário brasileiro ainda é incipiente. Em 2023, a Confederação de Organizações Europeias de Proteção de Dados reconheceu que a Inteligência Artificial Gerativa (IAG) é uma tecnologia muito nova e que as organizações que cuidam da proteção de dados devem remodelar suas normativas para atender às diferentes demandas que estão surgindo junto com a incorporação desse tipo de tecnologia em diversos espaços. Isso se deve ao fato de que os modelos de IAG são treinados em grandes volumes de dados, em sua maioria mal documentados, codificando, como consequência, viéses, estereótipos, discursos nocivos, além de os reproduzir em suas respostas (Bender et al., 2021). No Brasil, o Governo Federal elaborou a Proposta de Plano Brasileiro de Inteligência Artificial 2024-2028, que prevê o investimento de 23 bilhões de reais. Sob a premissa de uma “IA para o bem de todos"", são destacadas visões como a centralidade no ser humano, prevenindo desigualdade e viéses, e a transparência e responsabilidade, garantindo a privacidade e a sobreania dos dados. Contudo, apenas 0,45% do orçamento é destinado ao “Apoio ao Processo Regulatório e de Governança da IA"", evidenciando ainda mais a necessidade de discussões urgentes sobre a ética na IA no Brasil111Proposta de Plano Brasileiro de Inteligência Artificial 2024-2028. Disponível em: https://www.gov.br/mcti/pt-br/acompanhe-o-mcti/noticias/2024/07/plano-brasileiro-de-ia-tera-supercomputador-e-investimento-de-r-23-bilhoes-em-quatro-anos/ia_para_o_bem_de_todos.pdf/view. A preocupação ética na alimentação de grandes modelos de linguagem (large language models - LLMs) atravessa também o trabalho de linguistas. Como já dito, essas tecnologias precisam de grandes volumes de dados, no caso de chatbots, por exemplo, dados linguísticos são necessários para que as máquinas possam ter um fluxo de fala (ou escrita) similar ao de seres humanos, colocando aos linguistas o desafio de pensar em como proceder para proteger eticamente os dados colhidos. Neste artigo, então, discutimos desde a base de coleta de dados linguísticos ao compartilhamento desses dados para alimentar LLMs."
https://arxiv.org/html/2411.07244v1,A Tutorial on Teaching Data Analytics with Generative AI,"This tutorial addresses the challenge of incorporating large language models (LLMs), such as ChatGPT, in a data analytics class. It details several new in-class and out-of-class teaching techniques enabled by AI. For example, instructors can parallelize instruction by having students interact with different custom-made GPTs to learn different parts of an analysis and then teach each other what they learned from their AIs. For another example, instructors can turn problem sets into AI tutoring sessions, whereby a custom-made GPT guides a student through the problems, and the student uploads the chatlog for their homework submission. For a third example, you can assign different labs to each section of your class and have each section create AI assistants to help the other sections work through their labs. This tutorial advocates the programming in the English paradigm, in which students express the desired data transformations in prose and then use AI to generate the corresponding code. Students can wrangle data more effectively by programming in English than by manipulating in Excel. However, some students will program in English better than others, so you will still derive a robust grade distribution (at least with current LLMs).","I thought my class was set. Over the prior two years, I had made an entirely new analytics course: I wrote a textbook on data science with R for MBA students (Bray 2023) and created a corresponding set of interactive R Markdown slide decks. The class went well in the previous year, and I was planning on coasting for the next few years. How wrong I was. ChatGPT debuted on November 30, 2022, a mere 120 days before my 2023 class was to begin. The terrible implications this AI had for my class dawned on me seven weeks later. I remember the moment the harsh reality set in. I copied the following exercise from my textbook into ChatGPT without further explanation or context: Exercise 3.43 We will now subject our sample to 10 data filters. • Start with alibaba_long and group_by() order. • filter() the grouped tibble accordingly: • Remove the orders with any() ""FAILURE"" action. For example, you should remove all the order = 87717 observations since this order’s 12th action was a ""FAILURE"". • Remove orders without exactly one ""ORDER"" action, one ""SIGNED"" action, and one ""CONSIGN"" action. • Remove orders that have an action before the ""ORDER"" action or after the ""SIGNED"" action. In other words, there should be an ""ORDER"" action at time = 0 and a ""SIGNED"" action at time = 1. • Remove orders that correspond to multiple shipper values. • Remove orders with day_count > 8. • Remove orders with more than 10 or fewer than 5 posted actions. • Remove observations with ""ORDER"" and ""SIGNED"" actions, because their time values are degenerate (mechanically being either 0 or 1). • ungroup() the filtered tibble and <- it into alibaba_long. • alibaba_long should have 102331 rows after this step. This was the most challenging question from my Alibaba lab, which replicates the analysis of Bray (2020). In 2022, around two dozen students visited my office hours to discuss this question. So you can imagine my astonishment when the chatbot produced a correct code solution on its first attempt—a fact that’s all the more impressive when you consider that alibaba_long is left undefined. I soon discovered that ChatGPT could solve nearly every question in my quizzes, lectures, and labs. Three weeks before students were slated to bid on it, ChatGPT had rendered my class an obsolete farce. I despaired, but not for long, as it soon dawned on me that I could teach one of the first-ever classes on coding with ChatGPT. What an opportunity! The first order of business was to rename the class to OPNS451 Data Science with Large Language Models and update the syllabus, which now begins with the following: Large Language Models (LLMs) such as ChatGPT are powerful. To maximize your productivity—and stay relevant—you should aim to delegate as much of your workflow to these language engines as possible. This means you should become comfortable processing and analyzing data with a computer language—such as R—that LLMs excel at reading and writing. For instance, ChatGPT can answer nearly every question in the R textbook I wrote. Switching to R equips you with the software equivalent of a genie in a bottle, capable of implementing and explaining almost every data transformation. The tool is a great equalizer; an MBA with an LLM can accomplish almost anything that an experienced data scientist can. I believe that MBAs now stand out as the finest data scientists since the critical analytics differentiator has shifted from technical expertise to business insight. MBAs will excel as data analysts because they understand the most meaningful questions to pursue. This class will teach you how to use LLMs to process and analyze data. The only challenge is transitioning from spreadsheets to a language. However, once you master the lingua franca of data science, you will be capable of communicating and collaborating with a machine of immeasurable power. This message struck a chord: in less than three years, my elective MBA class swelled from 21 students in one section to 162 students in three sections. (Besides these elective sections, I have one compulsory section for whom my class is a degree requirement.) I proposed to my students that we treat the class as experiment in AI—an opportunity to collectively anticipate how analytics education and practice will respond to generative AI. This experiment taught me several techniques for teaching data analytics with AI: • Recast homework assignments as AI tutoring sessions—doing so increase student satisfaction, engagement, and learning (Section 2.2.2). • Use the programming with English (PIE) method, which uses AI to translate the students’ natural language into a computer language. Students won’t believe it, but this method makes them more effective with R than with Excel (Section 2.1.2). Further, allowing PIE on formal assessments will not make the grade distribution degenerate (Section 2.1.3). • For graphs, have students program by picture, uploading to ChatGPT a hand-drawn mockup of a plot, and asking the chatbot to create the corresponding ggplot code (Section 2.3.2).111 I learned this programming by picture technique from Sébastien Martin. This technique highlights AI’s rapid progress, as it was only last year that Ellis and Slade (2023) wrote that “there are other types of tasks that ChatGPT is less capable (or sometimes incapable) of performing well, such as interpreting statistical output that is provided as an image.” ChatGPT can now seamlessly interpret a picture depicting a regression output. • Have students teach the class content to a GPT, and then quiz these chatbots to assess the quality of the students’ instruction. For example, I have students train GPTs on logistic regression, and then I pit the students’ GPTs against each other, determining which can best answer logistic regression questions (Section 2.3.3). • Have students teach each other by creating AI assistants for other students. Students love designing AI experiences—custom-made GPTs are an excellent creative outlet (Section 2.2.6). • Create a set custom-made GPTs to teach different things to different students, and then have students teach each other what they learned from their AIs, say, by recording a video for the rest of the class (Section 2.3.2). • Have students pitch solutions to a custom-made GPT, and then have the chatbot quickly identify the proposals that warrant the class’ attention (Section 2.3.2). • Create a GPT obstacle course: load different GPTs on different laptops scattered throughout the class and have students run between the various workstations (Section 2.3.3). • Use AI assistants for quick, in-class demonstrations (Section 2.3.4). • Wrap a GPT around your lecture and students will diligently keep up, as they hate when their chatbot instance falls out of sync. The experiment also taught me a few practical lessons: • Emphasize learning over thinking in homework assignments, as students will outsource the latter to the AI (Section 2.2.5). • Use AI to fill the void in your class left by AI (Section 2.2.5). • Don’t make AI assignments overly creative (Section 2.2.8). • Don’t overlook the programming language instruction (Section 2.1.4). • Halve homework group sizes, since every student now contributes two voices: their own and their AI’s (Section 2.2.9). • Explain that working with AI is a skill that improves with practice (Section 2.2.4). • Do not hire tutors, and by all means do not write a textbook (Section 2.2.7). My final advice is to incorporate AI into your class with confidence. There’s such a hunger for AI in the classroom that even failed LLM initiatives will earn you goodwill."
https://arxiv.org/html/2411.08003v1,Can adversarial attacks by large language models be attributed?,"Attributing outputs from Large Language Models (LLMs) in adversarial settings—such as cyberattacks and disinformation—presents significant challenges that are likely to grow in importance. We investigate this attribution problem using formal language theory, specifically language identification in the limit as introduced by Gold and extended by Angluin. By modeling LLM outputs as formal languages, we analyze whether finite text samples can uniquely pinpoint the originating model. Our results show that due to the non-identifiability of certain language classes, under some mild assumptions about overlapping outputs from fine-tuned models it is theoretically impossible to attribute outputs to specific LLMs with certainty. This holds also when accounting for expressivity limitations of Transformer architectures. Even with direct model access or comprehensive monitoring, significant computational hurdles impede attribution efforts. These findings highlight an urgent need for proactive measures to mitigate risks posed by adversarial LLM use as their influence continues to expand.","References [1] Dana Angluin. Inductive inference of formal languages from positive data. Information and Control, 45(2):117–135, 1980. [2] Umar Anwar, Aziz Saparov, Julia Rando, Daniel Paleka, Michael Turpin, Pete Hase, et al. Foundational challenges in assuring alignment and safety of large language models. arXiv preprint arXiv:2404.09932, 2024. [3] Robert Axelrod and Radoslav Iliev. Timing of cyber conflict. Proceedings of the National Academy of Sciences, 111(4):1298–1303, 2014. [4] Satwik Bhattamishra, Arkil Patel, and Navin Goyal. On the computational power of transformers and its implications in sequence modeling. arXiv preprint arXiv:2006.09286, 2020. [5] Alexander Bick, Adam Blandin, and David J. Deming. The rapid adoption of generative ai. Working Paper w32966, National Bureau of Economic Research, Cambridge, MA, September 2024. [6] Rishi Bommasani, Dilara Soylu, Thomas I. Liao, Kathleen A. Creel, and Percy Liang. Ecosystem graphs: The social footprint of foundation models. March 2023. [7] Manuel Cebrian. A time-critical crowdsourced computational search for the origins of covid-19. Nature Electronics, 4(7):450–451, 2021. [8] Nicholas A Christakis and James H Fowler. Connected: The surprising power of our social networks and how they shape our lives. Little, Brown Spark, 2009. [9] Brian Edwards, Allen Furnas, Steve Forrest, and Robert Axelrod. Strategic aspects of cyberattack, attribution, and blame. Proceedings of the National Academy of Sciences, 114(11):2825–2830, 2017. [10] E.M. Gold. Language identification in the limit. Information and Control, 10(5):447–474, 1967. [11] Kent Johnson. Gold’s theorem and cognitive science. Philosophy of Science, 71(4):571–592, 2004. [12] Jon Kleinberg and Sendhil Mullainathan. Language generation in the limit. arXiv preprint arXiv:2404.06757, 2024. [13] William Merrill, Vivek Ramanujan, Yoav Goldberg, Roy Schwartz, and Noah Smith. Effects of parameter norm growth during transformer training: Inductive bias from gradient descent. arXiv preprint arXiv:2010.09697, 2020. [14] Mark Ed Newman, Albert-László Ed Barabási, and Duncan J Watts. The structure and dynamics of networks. Princeton university press, 2006. [15] Oak Ridge Leadership Computing Facility. Frontier supercomputer. [16] Bin Peng, Srikumar Narayanan, and Christos Papadimitriou. On limitations of the transformer architecture. arXiv preprint arXiv:2309.06863, 2023. [17] Nicole Perlroth. This is how they tell me the world ends: The cyberweapons arms race. Bloomsbury publishing, 2021. [18] Iyad Rahwan, Manuel Cebrian, Nicholas Obradovich, Josh Bongard, Jean-François Bonnefon, Cynthia Breazeal, Joshua W Crandall, Nicholas A Christakis, Iain D Couzin, Michael O Jackson, et al. Machine behaviour. Nature, 568(7753):477–486, 2019. [19] Yuval Shavit, Shuchi Agarwal, Miles Brundage, Saurabh Adler, Courtney O’Keefe, Riley Campbell, and David G Robinson. Practices for governing agentic ai systems. Research Paper, OpenAI, December 2023, 2023. [20] Lutz Strobl, William Merrill, Gregory Weiss, Daniel Chiang, and Dana Angluin. What formal languages can transformers express? a survey. Transactions of the Association for Computational Linguistics, 12:543–561, 2024. [21] Fabio Urbina, Filippa Lentzos, Cédric Invernizzi, and Sean Ekins. Dual use of artificial-intelligence-powered drug discovery. Nature Machine Intelligence, 4(3):189–191, 2022. [22] Marcin Waniek, Petter Holme, Manuel Cebrian, and Talal Rahwan. Social diffusion sources can escape detection. Iscience, 25(9):104956, 2022. [23] Marcin Waniek, Petter Holme, Katayoun Farrahi, Rémi Emonet, Manuel Cebrian, and Talal Rahwan. Trading contact tracing efficiency for finding patient zero. Scientific reports, 12(1):22582, 2022. [24] Jiajia Xu, Alice Smith, Liam Johnson, and Kevin Lee. Autoattacker: A large language model guided system to implement automatic cyber-attacks. arXiv preprint arXiv:2403.01038, 2024."
https://arxiv.org/html/2411.07892v1,"Mapping the Podcast Ecosystem with the
Structured Podcast Research Corpus","Podcasts provide highly diverse content to a massive listener base through a unique on-demand modality. However, limited data has prevented large-scale computational analysis of the podcast ecosystem. To fill this gap, we introduce a massive dataset of over 1.1M podcast transcripts that is largely comprehensive of all English language podcasts available through public RSS feeds from May and June of 2020. This data is not limited to text, but rather includes audio features and speaker turns for a subset of 370K episodes, and speaker role inferences and other metadata for all 1.1M episodes. Using this data, we also conduct a foundational investigation into the content, structure, and responsiveness of this ecosystem. Together, our data and analyses open the door to continued computational research of this popular and impactful medium.","Over the past two decades, podcasts have emerged as an important part of the modern media landscape (Aufderheide et al., 2020). With its low barrier to entry, this medium attracts many independent voices, but has also been embraced by professional media and celebrities, collectively supported by billions of dollars in advertising Wirtschafter (2023). However, despite significant evidence highlighting the wide-scale distribution and real-world impact of podcasts (Bottomley, 2015), there is only limited understanding of this medium and even fewer methods for analyzing podcasts, in part due to technical hurdles and limited data availability. In this work, we present the Structured Podcast Research Corpus (SPoRC), the first large-scale dataset of podcasts for NLP and computational social science research, and use it to analyze and document the podcast ecosystem at scale. Access to large-scale text data has been fundamental to enabling new research directions. Past resources for obtaining data from Reddit (Baumgartner et al., 2020), Twitter (Pfeffer et al., 2023), and Amazon (Ni et al., 2019), have all led to multiple concomitant advances in modeling, applications, and understanding of human behavior. Podcasts, however, introduce unique challenges as a source of data. As an audio medium, the text for podcast episodes is inaccessible in its original form; further, unlike structured media from single platforms, podcasts are distributed by multiple sources and varied metadata exist for each episode. Here, we introduce SPoRC to overcome these challenges. SPoRC is a thick sample of 1.1M podcast episodes from May–June 2020, each associated with transcripts, hosting platform, inferred host and guest names, and other metadata. Using a highly-parallelized pipeline for the collection and processing of podcasts, we introduce new data and models for extracting hosts and guests from podcast transcripts, enabling studies of the social network. We further use these identities to link specific turns of dialog to their speakers, where possible. In sum, our paper makes the following three contributions. First, we introduce SPoRC and the computational infrastructure and models needed to create it (§3). Second, we present the first large-scale characterization of the content and structure of the podcast ecosystem; these analyses reveal the topical distribution discussed on podcasts, and the community network structure created by guest co-appearances (§4). Third, we analyze the responsiveness of podcasts, examining the timing and extent of the impact of a major media event (§5), finding varied responses across different segments of the medium. Overall, our results point to new fundamental questions for further research into phenomena such as community identity, information diffusion, and incidental news exposure in podcasts. With the release of SPoRC, these and other lines of inquiry can be quantitatively studied at scale. Our code and data are made available for non-commercial use and can be found at https://github.com/blitt2018/SPoRC_data."
https://arxiv.org/html/2411.07845v1,"Ethical Concern Identification in NLP:
A Corpus of ACL Anthology Ethics Statements","What ethical concerns, if any, do LLM researchers have? We introduce EthiCon, a corpus of 1,580 ethical concern statements extracted from scientific papers published in the ACL Anthology. We extract ethical concern keywords from the statements and show promising results in automating the concern identification process. Through a survey (N=200), we compare the ethical concerns of the corpus to the concerns listed by the general public and professionals in the field. Finally, we compare our retrieved ethical concerns with existing taxonomies pointing to gaps and future research directions.","Researchers are often asked to subscribe to ethical guidelines, e.g., the European Code of Conduct for Research Integrity ALLEA (2017) – or the ACM Code of Ethics111See https://www.aclweb.org/portal/content/acl-code-ethics for ACL’s guidelines. for publishing their work in the Association for Computational Linguistics (ACL). In addition, authors are often encouraged to write a so-called ethics statement, addressing the broader implications of their work or any ethical considerations. We ask: What ethical concerns are raised in such statements, and how do they compare with public perceptions? Is there a gap between academic and public concerns? Figure 1: Visualizing top 60 concerns in ACL ethics statements, reflecting term frequencies. As natural language processing technologies become more prevalent, understanding the ethical concerns raised by professionals will enable us to compare them with public concerns, helping to identify gaps and overlaps that can inform frameworks and solutions to existing and emerging problems. For this reason, we create EthiCon, an annotated corpus of ethics statements from the Proceedings of the 60th and 61st Annual Meeting of the Association for Computational Linguistics Muresan et al. (2022); Rogers et al. (2023). Our aim is twofold: to map out the concerns of the NLP community as they appear on the ethics statements, and to trace gaps and overlaps between NLP professionals and the general public. Our results show that laypeople express different ethical concerns than professionals, focusing on socio-economic and human-computer interaction issues, along with miscellaneous concerns like existential risks. This highlights the need for increased dialogue between researchers and the public to address these varying perspectives and an updated taxonomy covering both existing and emerging issues. Contributions. We provide a corpus of 1,580 ethics statements from the ACL Anthology. We identify the main issues that NLP researchers flag as ethically concerning in their work and show how LLMs could automate this process. Through a survey, we compare how laypeople and NLP professionals perceive the ethical concerns surrounding natural language processing. Lastly, we provide a comparison of the ACL and the survey ethical concerns to existing taxonomies of risks posed by Language Models. Finding a way to better automate the process will enable the comparison of ethical concerns across time and technical innovation and provide a better understanding of the impact of NLP research within the field and beyond. Figure 2: Examples from the identified categories of ethical concern statements."
https://arxiv.org/html/2411.07658v1,"Advancing Sustainability via 
Recommender Systems: A Survey","Human behavioral patterns and consumption para- digms have emerged as pivotal determinants in environmental degradation and climate change, with quotidian decisions pertaining to transportation, energy utilization, and resource consumption collectively precipitating substantial ecological impacts. Recommender systems, which generate personalized suggestions based on user preferences and historical interaction data, exert considerable influence on individual behavioral trajectories. However, conventional recommender systems predominantly optimize for user engagement and economic metrics, inadvertently neglecting the environmental and societal ramifications of their recommendations, potentially catalyzing overconsumption and reinforcing unsustainable behavioral patterns. Given their instrumental role in shaping user decisions, there exists an imperative need for sustainable recommender systems that incorporate sustainability principles to foster eco-conscious and socially responsible choices. This comprehensive survey addresses this critical research gap by presenting a systematic analysis of sustainable recommender systems. As these systems can simultaneously advance multiple sustainability objectives—including resource conservation, sustainable consumer behavior, and social impact enhancement—examining their implementations across distinct application domains provides a more rigorous analytical framework. Through a methodological analysis of domain-specific implementations encompassing transportation, food, buildings, and auxiliary sectors, we can better elucidate how these systems holistically advance sustainability objectives while addressing sector-specific constraints and opportunities. Moreover, we delineate future research directions for evolving recommender systems beyond sustainability advocacy toward fostering environmental resilience and social consciousness in society.","Human activities encompass the consumption of myriad non-renewable resources (e.g., coal, gas, fossil fuels) and natural materials, while concomitantly inflicting environmental degradation through various mechanisms, including atmospheric pollution, carbon dioxide (\textrm{CO}_{\textrm{2}}) emissions, and waste generation [1, 2, 3]. The cumulative global emissions of \textrm{CO}_{\textrm{2}} have exhibited a consistent linear trajectory over time, as illustrated in Fig. 1 (left). According to the U.S. Energy Information Administration (EIA), the United States contributed approximately 5.12 billion metric tons (BMT) of \textrm{CO}_{\textrm{2}} emissions in 2021 [4, 5]. Of this total, a substantial 92 percent, or 4.6 BMT, was directly attributable to the combustion of fossil fuels for energy generation. Moreover, recent years have witnessed a precipitous advancement in generative artificial intelligence (AI), which necessitates the utilization of large-scale datasets for training expansive language models [6]. A comparative analysis of the 2024 environmental sustainability reports from Microsoft and Google reveals a significant increase in their carbon footprints since 2020 (Fig. 1 (right)). Microsoft’s emissions have risen by 29.4%, while Google’s have surged by 66.3%. The primary driver of this growth is the expansion of their data center infrastructure, specifically designed and optimized to accommodate the escalating computational demands of artificial intelligence workloads. It is incontrovertible that the magnitude of carbon emissions continues to escalate, making substantial contributions to anthropogenic climate change [7, 8, 9]. Figure 1: Global \textrm{CO}_{\textrm{2}} emissions trends (1960-2023) and a comparative analysis of Microsoft and Google’s carbon footprints. To align with the United Nations Sustainable Development Goals (SDGs) [10] and adhere to the stipulations of the Paris Agreement aimed at mitigating climate change to below 1.5°C by the mid-2030s, a diverse array of artificial intelligence technologies has been deployed [11]. As the global community becomes increasingly aware of the importance of sustainable practices to address environmental challenges, there is a growing consensus that recommender systems (RSs) can play a crucial role in facilitating sustainable human behaviors [12, 13]. These systems propose alternatives that endorse environmentally sustainable products, encourage eco-conscious travel options, and promote energy-efficient living arrangements within architectural structures. Empirical research has demonstrated that recommender systems tailored for green products not only contribute to the diminution of energy usage and the reduction of greenhouse gas emissions [14] but also foster a paradigm of sustainable consumption among users. RSs are AI models designed to predict user preferences and suggest relevant items or content. These systems have become ubiquitous in various domains, including e-commerce, entertainment, social media, and more [15, 16, 17, 18]. The primary goal of RSs is to enhance user experience by providing personalized recommendations tailored to individual preferences and behaviors [19, 20]. Given the significant impact of RSs on daily human interactions with digital platforms, these systems have the potential to contribute substantially to environmental and social sustainability: i). Waste reduction: By suggesting products or content that align more closely with user preferences, RSs can potentially mitigate waste from unwanted purchases or unused items. ii). Energy efficiency optimization: In sectors such as energy management, RSs can propose optimal energy usage patterns, potentially promoting conservation and reducing carbon footprints. iii). Promotion of sustainable consumption: These systems can be engineered to prioritize environmentally friendly products or services, potentially encouraging more sustainable consumer behavior. iv). Enhancement of social well-being: Through the recommendation of educational content, health-related information, or community activities, these systems may contribute to social development and individual growth. While several reviews have examined sustainable recommendation systems in specific domains, such as energy-efficient building practices [21], eco-friendly travel routes [22], sustainable e-tourism [23], and Sustainable Development Goals (SDGs) perspectives [12], this survey offers a more comprehensive and integrated analysis. The present study provides a holistic view of sustainable recommendation systems across multiple domains, including health-conscious food choices, energy-efficient building management, and environmentally friendly travel solutions, while also examining the underlying computational strategies employed in these systems. By synthesizing a diverse body of research and emphasizing the critical need to incorporate environmental sustainability into system designs, this review aims to enhance the understanding of sustainable recommender systems and stimulate future research that encompasses various aspects of sustainability. Our primary contributions are as follows: • We offer an in-depth examination of the implementation and research trajectories of sustainable recommendation systems in pivotal sectors, including travel, food, and built environment management, coupled with insights into algorithmic optimization. • We present a generic architectural framework for sustainable recommender systems, serving as a foundation for organizing and contextualizing existing research. • We make a substantial contribution to the corpus of research on sustainable recommendation systems by elucidating key challenges within the domain and proposing future research avenues. It establishes a crucial framework for advancing the study and application of sustainability principles in recommendation systems across heterogeneous industries. The subsequent sections of this paper are structured as follows: Section II provides the necessary background for understanding the subsequent review. Section III describes various work in sustainable travel recommendation, covering POI recommendation, route recommendation, and transportation recommendation. Section IV delves into sustainable recommendation practices within the food industry, focusing on health-conscious and environmentally friendly food recommendations. Section V discusses the sustainable building recommendation, ranging from residential to commercial and large-scale buildings. Section VI expands to a wider discussion on the broader applications of sustainable recommendations, which includes environmental and ecological sustainability, behavior and social change, economic and productive sustainability, and user-centric sustainable recommenders. Section VII pivots to the sustainable design of recommendation models, particularly through algorithmic advancements and computational efficiency. Section VIII highlights ongoing challenges and emerging research areas within the field. Finally, Section IX concludes the paper, synthesizing key findings and delineating implications for future research endeavors. (§III)(§III-A)(§III-B)(§III-C)(§IV)(§IV-A)(§IV-B)(§V)(§V-A)(§V-B)(§VI)(§VI-A)(§VI-B)(§VI-C)(§VI-D)(§VII)(§VII-A)(§VII-B)(§VII-C)(§VII-D) Figure 2: Hierarchical taxonomy of recommender system categories and their applications in advancing sustainability initiatives."
https://arxiv.org/html/2411.07441v1,Automatically Detecting Online Deceptive Patterns in Real-time,"Deceptive patterns (DPs) in digital interfaces manipulate users into making unintended decisions, exploiting cognitive biases and psychological vulnerabilities. These patterns have become ubiquitous across various digital platforms. While efforts to mitigate DPs have emerged from legal and technical perspectives, a significant gap in usable solutions that empower users to identify and make informed decisions about DPs in real-time remains. In this work, we introduce AutoBot, an automated, deceptive pattern detector that analyzes websites’ visual appearances using machine learning techniques to identify and notify users of DPs in real-time. AutoBot employs a two-staged pipeline that processes website screenshots, identifying interactable elements and extracting textual features without relying on HTML structure. By leveraging a custom language model, AutoBot understands the context surrounding these elements to determine the presence of deceptive patterns. We implement AutoBot as a lightweight Chrome browser extension that performs all analyses locally, minimizing latency and preserving user privacy. Through extensive evaluation, we demonstrate AutoBot’s effectiveness in enhancing users’ ability to navigate digital environments safely while providing a valuable tool for regulators to assess and enforce compliance with DP regulations.","Deceptive patterns (DPs) are design elements that manipulate users into making unintended decisions while interacting with interfaces on applications or websites. These patterns exploit cognitive biases and psychological vulnerabilities to influence user behavior, often in ways that benefit the service provider at the user’s expense. They have become increasingly prevalent across digital environments, significantly impacting user experiences on social media, mobile devices, cookie consent banners, and even gaming applications [33]. Prior works have shown that DPs can result in financial loss [42], privacy breaches [8], or exploitation of vulnerable groups, including children [46]. A typical example of a DP is the intentionally convoluted subscription cancellation process, where users must navigate through multiple obscure options to terminate a service, as seen on platforms like www.dailymail.co.uk. Efforts to mitigate the effect of DPs have emerged from the policy and the technical sides. Regarding policy, regulations like CPRA [15] and the GDPR [4] have released guidance on deceptive patterns. On the technical front, researchers have explored classifying the existing types of deceptive patterns [11, 21, 33, 48, 44, 7, 16, 39, 32], assessing their effectiveness [30], and compiling different deceptive pattern cases [12]. Despite these ongoing efforts, deceptive patterns continue to pose significant challenges for both users and regulators. Users frequently encounter DPs in their digital interactions. For instance, sponsored advertisements are strategically placed at the top of search results, creating a false impression that they are the most relevant or popular items. Furthermore, regulators often lack the tools to assess and enforce compliance with regulations concerning deceptive patterns effectively. This technological gap leaves users vulnerable to sophisticated DPs. In this work, we propose a new paradigm to bridge this gap and improve the usability of online websites by developing a solution to detect deceptive patterns and automatically warn users in real time. Our solution makes users aware of deceptive patterns as they browse and enables regulators to assess and evaluate online services for deceptive patterns. Achieving this objective requires us to overcome several challenges. First, the lack of a standardized taxonomy and usable datasets complicates classification efforts. Second, the vast diversity of web designs and the ingenuity of designers in creating new deceptive techniques have led to a trade-off between scalability and accuracy in detection methods. For instance, existing solutions like disabling third-party cookies or using filter lists have proven inadequate in preventing user tracking, as highlighted by Chen et al. [16]. Third, user-friendly solutions must operate with minimal infrastructure to be practical for widespread adoption. We first create a standardized taxonomy of deceptive patterns to realize our objectives, building upon existing taxonomies. Leveraging this standardized taxonomy, we then built AutoBot, an automated, deceptive pattern detector that analyzes the website’s visual appearances, uses machine learning tools to identify deceptive patterns, and brings deceptive patterns to users’ attention in real-time. Specifically, AutoBot relies on an invariant behavior of deceptive patterns: the visual representation and how users perceive them. Using this invariant behavior, AutoBot applies a two-staged pipeline that identifies the deceptive patterns present in the website’s screenshot. First, AutoBot analyzes the website screenshot, identifying interactable elements using visual analysis and extracting textual features without relying on the HTML structure. We note that here, we rely on the insight that deceptive patterns work by manipulating what the users perceive; thereby, analyzing their visual appearance provides a comprehensive signal to be able to identify them. Next, AutoBot leverages a custom language model to understand the context surrounding these elements, including color, font size, and text, to determine the presence of deceptive patterns. We created an annotated dataset for deceptive patterns and designed a new training paradigm to fine-tune language models. We also note here that empirically, we have found that off-the-shelf language models like Gemini and ChatGPT often fail to detect deceptive patterns from screenshots, as shown in Figure 2. We showcase the usability of AutoBot by building a Chrome browser extension that (1) detects the deceptive patterns in real-time by performing the complete analysis locally and (2) annotates the current webpage to warn the users about the deceptive patterns. We also characterize the performance of AutoBot on several devices and find that the latency is less than one second on modern devices, which minimally impacts the usability of the websites. In this work, we put forth AutoBot, a automated solution to detect deceptive patterns on the web and make the following contribution: 1. We compile an annotated large-scale deceptive design dataset (D3) based on carefully curated taxonomy. We also introduce a novel pipeline to generate a diverse dataset of synthetically generated websites. 2. We show that our system, AutoBot, can detect deceptive designs present on the web and classify them per our taxonomy, with high recall resulting in most deceptive patterns being detected. 3. We leverage recent advances in optimizing LLM inference on consumer hardware to run our system, AutoBot, on device with real-time inference. Figure 1: Here we show how AutoBot highlights and informs the users about deceptive patterns on a site."
https://arxiv.org/html/2411.07320v1,"Richer Output for Richer Countries:
Uncovering Geographical 
Disparities in Generated Stories and Travel Recommendations","While a large body of work inspects language models for biases concerning gender, race, occupation and religion, biases of geographical nature are relatively less explored. Some recent studies benchmark the degree to which large language models encode geospatial knowledge. However, the impact of the encoded geographical knowledge (or lack thereof) on real-world applications has not been documented. In this work, we examine large language models for two common scenarios that require geographical knowledge: (a) travel recommendations and (b) geo-anchored story generation. Specifically, we study four popular language models, and across about 100K travel requests, and 200K story generations, we observe that travel recommendations corresponding to poorer countries are less unique with fewer location references, and stories from these regions more often convey emotions of hardship and sadness compared to those from wealthier nations. 111Upon publication, we will publicly release the data and code required to reproduce our evaluation.","Figure 1: World map with country-wise analysis of responses generated by GPT-4. Left: Average count of geographical entities mentioned in generated stories (correlated with the GDP per capita with Pearson r = 0.5). Right: Uniqueness scores for travel recommendations (Pearson r = 0.4 with GDP per capita). Given the excitement around large language models, users resort to these models for a diverse range of applications (Brown et al., 2020; Touvron et al., 2023). Based on our analysis of ShareGPT,222https://sharegpt.com/ a collection of user interactions with ChatGPT, 1.7% of queries are about travel recommendations, whereas 1.5% concern story generation. Such use cases make one wonder whether the generated travel itinerary for Mumbai is just as informative compared to New York City? Or is a generated story of a girl growing up in Nairobi just as relatable compared to another story based in Seattle? For these applications to be broadly useful, it is important that there are no (or few) geographical disparities. Some recent works aim to benchmark the extent of geographical knowledge encoded in large language models (Bhandari et al., 2023; Manvi et al., 2023; Moayeri et al., 2024). Interestingly, a recent study finds that language models accurately predict global facts such as population and rainfall for different geo-locations, however, their predictions on sensitive topics such as attractiveness or morality are, problematically, biased against areas with poorer socioeconomic conditions (Manvi et al., 2024). Similar in spirit, our work aims to quantitatively study model responses for two real-world scenarios that require geographical knowledge. In this work, we analyze over 300K responses from 4 language models, corresponding to requests for travel recommendations and geo-anchored story generations. These requests span over 150K locations across the globe. We quantify the informativeness and uniqueness of model responses, in addition to the emotions they express. We then compare these quantities with the socioeconomic indicators of different locations. Through our analysis, we uncover several geographical disparities, finding that outputs for wealthier countries are more unique and include more geographical entities (Figure 1). For Sub-Saharan African countries, we notice considerably less unique outputs compared to the North American region, with the average difference being about 40% across all models. Further, stories about poorer countries express considerably more hardship and sadness, with 65% more narratives depicting hardship for low-income countries compared to high-income countries. Despite many large corporations claiming to be egalitarian, e.g., OpenAI aims to develop intelligence that benefits all of humanity (OpenAI, 2024), many findings, including ones from this study, demonstrate how current models perpetuate western hegemony in the generated content Schwöbel et al. (2023); Bender et al. (2021), and more serious effort is needed to ensure that models serve the diverse population across the globe."
https://arxiv.org/html/2411.07302v1,Merit-Based Sortition in Decentralized Systems,"In decentralized systems, it is often necessary to select an ‘active’ subset of participants from the total participant pool, with the goal of satisfying computational limitations or optimizing resource efficiency. This selection can sometimes be made at random, mirroring the sortition practice invented in classical antiquity aimed at achieving a high degree of statistical representativeness. However, the recent emergence of specialized decentralized networks that solve concrete coordination problems and are characterized by measurable success metrics often requires prioritizing performance optimization over representativeness. We introduce a simple algorithm for ‘merit-based sortition’, in which the quality of each participant influences its probability of being drafted into the active set, while simultaneously retaining representativeness by allowing inactive participants an infinite number of chances to be drafted into the active set with non-zero probability. Using a suite of numerical experiments, we demonstrate that our algorithm boosts the quality metric describing the performance of the active set by >2 times the intrinsic stochasticity. This implies that merit-based sortition ensures a statistically significant performance boost to the drafted, ‘active’ set, while retaining the property of classical, random sortition that it enables upward mobility from a much larger ‘inactive’ set. This way, merit-based sortition fulfils a key requirement for decentralized systems in need of performance optimization.","The term ‘sortition’ originally refers to the process of randomly selecting representatives in a democratic system, a practice dating back 2.5 millennia to ancient Athens, where the selection of public officials by lottery was seen as the best way of achieving fairness in society (e.g. Headlam-Morley, 1891). Since then, sortition has spread over the world as a way of obtaining representative selections of politicians, public officials, or advisors, going through ebbs and flows in terms of its popularity (e.g. Flanigan et al., 2021; Jacquet et al., 2022; Sintomer, 2023a). Advocates of sortition often highlight positive implications such as fairness, representativeness, and efficiency (e.g. Engelstad, 1989; Sintomer, 2023b). In permissionless, decentralized systems (e.g. Nakamoto, 2008; Buterin, 2014), a form of sortition is often needed too. Across a large set of anonymous contributors, validators, or other participants, there exists a high degree of redundancy that does not require all participants to be involved in the decision process (e.g. Wüst & Gervais, 2018). Sometimes, there may exist concrete computational limitations why it is infeasible to involve all participants. In any of such cases, (pseudo)-random subsets may be able to fulfil the same task without a serious loss of security or performance. In principle, this can be done using classical, random sortition techniques (of which numerous trustless examples exist, see e.g. Gilad et al. 2017; Saa & Stern 2019; Freitas et al. 2023) to achieve representativeness and efficiency. However, recent developments in decentralized networks have brought about a rapid growth of systems that aim to achieve concrete goals, often with measurable performance or success. Examples are decentralized machine intelligence and inference systems (e.g. Rao et al., 2021; Kruijssen et al., 2024), on-chain oracles (e.g. Ellis et al., 2017; Breidenbach et al., 2021), or internet-of-things networks (e.g. Banerjee et al., 2023). In such systems, which aim to achieve a quantifiable degree of success, the goal of sortition is to improve efficiency not only while maintaining representativeness, but also while optimizing performance. The latter goal is achieved not through random sortition, but by letting the performance of each participant influence their probability of being drafted. We refer to this concept as merit-based sortition. In this paper, we introduce a simple algorithm for merit-based sortition that can be used to increase computational efficiency by limiting active participation without sacrificing (and generally improving) performance. This is possible, because the algorithm: 1. optimizes the quality of the active set of participants by letting the probability of relegation out of the active set decrease with the participant’s quality; 2. retains fairness and representativeness by allowing inactive participants an infinite number of chances to be drafted into the active set, in such a way that the probability and frequency of promotion increase with the participant’s quality. In §2 we outline the proposed algorithm for merit-based sortition. We investigate its quantitative performance and behavior in §3 using a suite of numerical experiments. Finally, we summarize our results in §4."
https://arxiv.org/html/2405.05369v2,Model Reconstruction Using Counterfactual Explanations: A Perspective From Polytope Theory,"Counterfactual explanations provide ways of achieving a favorable model outcome with minimum input perturbation. However, counterfactual explanations can also be leveraged to reconstruct the model by strategically training a surrogate model to give similar predictions as the original (target) model. In this work, we analyze how model reconstruction using counterfactuals can be improved by further leveraging the fact that the counterfactuals also lie quite close to the decision boundary. Our main contribution is to derive novel theoretical relationships between the error in model reconstruction and the number of counterfactual queries required using polytope theory. Our theoretical analysis leads us to propose a strategy for model reconstruction that we call Counterfactual Clamping Attack (CCA) which trains a surrogate model using a unique loss function that treats counterfactuals differently than ordinary instances. Our approach also alleviates the related problem of decision boundary shift that arises in existing model reconstruction approaches when counterfactuals are treated as ordinary instances. Experimental results demonstrate that our strategy improves fidelity between the target and surrogate model predictions on several datasets.","Counterfactual explanations (also called counterfactuals) have emerged as a burgeoning area of research (Wachter et al., 2017; Guidotti, 2022; Barocas et al., 2020; Verma et al., 2022; Karimi et al., 2022) for providing guidance on how to obtain a more favorable outcome from a machine learning model, e.g., increase your income by 10K to qualify for the loan. Interestingly, counterfactuals can also reveal information about the underlying model, posing a nuanced interplay between model privacy and explainability (Aïvodji et al., 2020; Wang et al., 2022). Our work provides the first theoretical analysis between the error in model reconstruction using counterfactuals and the number of counterfactuals queried for, through the lens of polytope theory. Model reconstruction using counterfactuals can have serious implications in Machine Learning as a Service (MLaaS)platforms that allow users to query a model for a specified cost (Gong et al., 2020). An adversary may be able to “steal” the model by querying for counterfactuals and training a surrogate model to provide similar predictions as the target model, a practice also referred to as model extraction. On the other hand, model reconstruction could also be beneficial for preserving applicant privacy, e.g., if an applicant wishes to evaluate their chances of acceptance from crowdsourced information before formally sharing their own application information with an institution, often due to resource constraints or having a limited number of attempts to apply (e.g., applying for credit cards reduces the credit score (Capital One, 2024)). Our goal is to formalize how faithfully can one reconstruct an underlying model given a set of counterfactual queries. Figure 1: Decision boundary shift when counterfactuals are treated as ordinary labeled points. An existing approach for model reconstruction is to treat counterfactuals as ordinary labeled points and use them for training a surrogate model (Aïvodji et al., 2020). While this may work for a well-balanced counterfactual queries from the two classes lying roughly equidistant to the decision boundary, it is not the same for unbalanced datasets. The surrogate decision boundary might not always overlap with that of the target model (see Fig. 1), a problem also referred to as a decision boundary shift. This is due to the learning process where the boundary is kept far from the training examples (margin) for better generalization (Shokri et al., 2021). The decision boundary shift is aggravated when the system provides only one-sided counterfactuals, i.e., counterfactuals only for queries with unfavorable predictions. If one were allowed to query for two-sided counterfactuals, the decision boundary shift may be tackled by querying for the counterfactual of the counterfactual (Wang et al., 2022). However, such strategies cannot be applied when only one-sided counterfactuals are available, which is more common and also a more challenging use case for model reconstruction, e.g., counterfactuals are only available for the rejected applicants to get accepted for a loan but not the other way. In this work, we analyze how model reconstruction using counterfactuals can be improved by leveraging the fact that the counterfactuals are quite close to the decision boundary. We provide novel theoretical analysis for model reconstruction using polytope theory, addressing an important knowledge gap in the existing literature. We demonstrate reconstruction strategies that alleviate the decision-boundary-shift issue for one-sided counterfactuals. In contrast to existing strategies Aïvodji et al. (2020) and Wang et al. (2022) which require the system to provide counterfactuals for queries from both sides of the decision boundary, we are able to reconstruct using only one-sided counterfactuals, a problem that we also demonstrate to be theoretically more challenging than the two-sided case (see Corollary 3.8). In summary, our contributions can be listed as follows: Fundamental guarantees on model reconstruction using counterfactuals: We derive novel theoretical relationships between the error in model reconstruction and the number of counterfactual queries (query complexity) under three settings: (i) Convex decision boundaries and closest counterfactuals (Theorem 3.2): We rely on convex polytope approximations to derive an exact relationship between expected model reconstruction error and query complexity; (ii) ReLU networks and closest counterfactuals (Theorem 3.6): Relaxing the convexity assumption, we provide probabilistic guarantees on the success of model reconstruction as a function of number of counterfactual queries; and (iii) Beyond closest counterfactuals: We provide approximate guarantees for a broader class of models, including ReLU networks and locally-Lipschitz continuous models (Theorem 3.10). Model reconstruction strategy with unique loss function: We devise a reconstruction strategy – that we call Counterfactual Clamping Attack (CCA) – that exploits only the fact that the counterfactuals lie reasonably close to the decision boundary, but need not be exactly the closest. Empirical validation: We conduct experiments on both synthetic datasets, as well as, four real-world datasets, namely, Adult Income (Becker and Kohavi, 1996), COMPAS (Angwin et al., 2016), DCCC (Yeh, 2016), and HELOC (FICO, 2018). Our strategy outperforms the existing baseline (Aïvodji et al., 2020) over all these datasets (Section 4) using one-sided counterfactuals, i.e., counterfactuals only for queries from the unfavorable side of the decision boundary. We also include additional experiments to observe the effects of model architecture, Lipschitzness, and other types of counterfactual generation methods as well as ablation studies with other loss functions. A python-based implementation is available at: https://github.com/pasandissanayake/model-reconstruction-using-counterfactuals. Related Works: A plethora of counterfactual-generating mechanisms has been suggested in existing literature (Guidotti, 2022; Barocas et al., 2020; Verma et al., 2022; Karimi et al., 2022, 2020; Mothilal et al., 2020; Dhurandhar et al., 2018; Deutch and Frost, 2019; Mishra et al., 2021). Related works that focus on leaking information about the dataset from counterfactual explanations include membership inference attacks (Pawelczyk et al., 2023) and explanation-linkage attacks (Goethals et al., 2023). Shokri et al. (2021) examine membership inference from other types of explanations, e.g., feature-based. Model reconstruction (without counterfactuals) has been the topic of a wide array of studies (see surveys Gong et al. (2020) and Oliynyk et al. (2023)). Various mechanisms such as equation solving (Tramèr et al., 2016) and active learning have been considered (Pal et al., 2020). Model inversion (Gong et al., 2021; Struppek et al., 2022; Zhao et al., 2021) is another form of extracting information about a black box model, under limited access to the model aspects. In contrast to model extraction where the goal is to replicate the model itself, in model inversion an adversary tries to extract the representative attributes of a certain class with respect to the target model. In this regard, Zhao et al. (2021) focuses on exploiting explanations for image classifiers such as saliency maps to improve model inversion attacks. Struppek et al. (2022) proposes various methods based on Generative Adversarial Networks to make model inversion attacks robust (for instance, to distributional shifts) in the domain of image classification. Milli et al. (2019) looks into model reconstruction using other types of explanations, e.g., gradients. Yadav et al. (2023) explore algorithmic auditing using counterfactual explanations, focusing on linear classifiers and decision trees. Using counterfactual explanations for model reconstruction has received limited attention, with the notable exceptions of Aïvodji et al. (2020) and Wang et al. (2022). Aïvodji et al. (2020) suggest using counterfactuals as ordinary labeled examples while training the surrogate model, leading to decision boundary shifts, particularly for unbalanced query datasets (one-sided counterfactuals). Wang et al. (2022) introduces a strategy of mitigating this issue by further querying for the counterfactual of the counterfactual. However, both these methods require the system to provide counterfactuals for queries from both sides of the decision boundary. Nevertheless, a user with a favorable decision may not usually require a counterfactual explanation, and hence a system providing one-sided counterfactuals might be more common, wherein lies our significance. While model reconstruction (without counterfactuals) has received interest from a theoretical perspective (Tramèr et al., 2016; Papernot et al., 2017; Milli et al., 2019), model reconstruction involving counterfactual explanations lack such a theoretical understanding. Our work theoretically analyzes model reconstruction using polytope theory and proposes novel strategies thereof, also addressing the decision-boundary shift issue."
https://arxiv.org/html/2411.06027v1,A Toolkit for Measuring the Impacts of Public Funding on Open Source Software Development,"Governments are increasingly employing funding for open source software (OSS) development as a policy lever to support the security of software supply chains, digital sovereignty, economic growth, and national competitiveness in science and innovation, among others. However, the impacts of public funding on OSS development remain poorly understood, with a lack of consensus on how to meaningfully measure them. This gap hampers assessments of the return on public investment and impedes the optimisation of public-interest funding strategies. We address this gap with a toolkit of methodological considerations that may inform such measurements, drawing on prior work on OSS valuations and community health metrics by the Community Health Analytics Open Source Software (CHAOSS) project as well as our first-hand learnings as practitioners tasked with evaluating funding programmes by the Next Generation Internet initiative and the Sovereign Tech Agency. We discuss salient considerations, including the importance of accounting for funding objectives, project life stage and social structure, and regional and organisational cost factors. Next, we present a taxonomy of potential social, economic, and technological impacts that can be both positive and negative, direct and indirect, internal (i.e. within a project) and external (i.e. among a project’s ecosystem of dependents and users), and manifest over various time horizons. Furthermore, we discuss the merits and limitations of qualitative, quantitative, and mixed-methods approaches, as well as options for and hazards of estimating multiplier effects. With this toolkit, we contribute the multi-stakeholder conversation about the value and impacts of funding on OSS developers and society at large.","Open source software (OSS) 111OSS is software whose source code is distributed under a license that permits the use, study, modification, and redistribution of the software source code [1]. Please note that we use OSS rather than FOSS, which stands for “free and open source software”. are digital public goods that are increasingly recognised as digital infrastructure [2, 3], which are used in around 96% of codebases [4] and constitute up to 90% of commercial software stacks [5]. The ubiquitous use of OSS, which are often developed and maintained by volunteer communities, have drawn attention to the question of funding as a mechanism to support the sustainability of OSS projects [6, 7]. In particular, the discovery of major security vulnerabilities in widely used OSS projects, such as the Log4Shell vulnerability in November 2021, highlighted the need to fund security and maintenance work in critical OSS projects [8], and contributed to an understanding of the role and responsibility of the public sector as a funder of OSS development [9, 10]. Concurrently, policymakers increasingly recognise OSS funding as a policy lever to support digital sovereignty [11, 12], the growth of domestic software markets [13], and national competitiveness in science and innovation [14, 15], among others. While governmental interest and involvement in funding OSS funding is increasing, its impacts on OSS development is poorly understood, with no consensus on how to measure them in a meaningful way. Not only is the measurement of the impacts of OSS funding methodologically challenging, but it is complicated by the fact that introducing funding into OSS projects may change contributor incentives and the balance of voluntary and paid participation [16, 17]. This gap hinders assessments of return on public investment and optimising public interest funding strategies. We address this problem with a toolkit of methodological considerations for measuring the impacts of public funding on OSS development. It is informed by methodologies for OSS valuations and community health measurements, in particular metrics developed by the Community Health Analytics Open Source Software (CHAOSS) project, as well as our first-hand insights from developing impact measurement frameworks for the Next Generation Internet (NGI) initiative at the European Commission (EC)and the Sovereign Tech Agency (STA) in Germany. The toolkit begins with a discussion of key considerations, including funding objectives, project life stages and social structures, and regional and organisational cost factors. Then, we present a taxonomy of potential social, economic, and technological impacts that can be both positive and negative, direct and indirect, internal (i.e. within a project) and external (i.e. among a project’s ecosystem of dependents and users), and manifest over various time horizons. Next, we discuss the merits and limitations of qualitative, quantitative, and mixed-methods approaches for measuring such impacts, and options for and hazards of estimating multiplier effects. This toolkit is not exhaustive or prescriptive, nor does it aim to be; rather, we seek to provide a toolkit that can inform the multi-stakeholder debate about the value of public funding for OSS development and how to meaningfully measure its impacts. This paper has the following structure. Section 2 reviews relevant literature and practice, providing a background on prior scholarship on OSS funding, OSS valuations, and OSS community health measurements, as well as real-world examples of public funding by the NGI and STA. Section 3 presents the toolkit for measuring the impacts of public funding on OSS development. It includes a discussion of pertinent considerations, such as funding objectives (3.1.1); salary structures (3.1.3); a taxonomy of social, economic, and technological impacts (3.2); multiplier effect estimations (3.5); and an evaluation of qualitative, quantitative, and mixed-methods approaches (Section 3.4). Section 4 discusses the overarching considerations and future research directions. Finally, Section 5 concludes with a call to action for both diverse OSS stakeholders to engage in the debate about the value and impacts of public funding on OSS development."
https://arxiv.org/html/2411.05992v1,Other Worlds: Using AI to Revisit Cybersyn and Rethink Economic Futures,"Neoliberalism has become orthodoxy in the present, erasing competing paradigms and alternative imaginings. Chile’s radical Cybersyn project from 1971 to 1973 offers a departure point for an alternative path, albeit one that was abruptly and violently extinguished. We revisit this moment by fine-tuning AI language models on the words and writing of Salvador Allende, the Chilean President, and Stafford Beer, the cyberneticist who helped to design the project. We conduct interviews with these simulated personas, focusing on how their revolutionary ideas might be taken up in the present. We then use an AI model to generate five-year-plans from 1973 to the present, simulating an alternate history guided by Cybersyn and a progressive agenda. We frame these interventions as socialist infrastructuring that cultivates a more expansive socialist imagining. This work is not about the viability of planned economies, but about the “inspirability” of exploring other value-systems in the present, allowing us to break out of our future-on-rails to envision alternative ways of organizing economy and society.","Introduction In the last four decades, neoliberal policies have ascended to dominate economic, environmental, and education decision-making, marginalizing alternative conceptions (Hursh and Henderson, 2011). Hegemony is achieved not through the result of any coordinated campaign, but rather through a far more ambient constellation of claims, norms, and activities diffused throughout social and political institutions and everyday life. When critique does arise, it is quickly defused through co-option (Boltanski and Chiapello, 2017). The success of this colonization of imagination has meant that these free market ideologies attain the status of common sense. The neoliberal status quo becomes stabilized as something which is normal or natural (Davis, 2022). The result of this overwhelming dominance is capitalist realism (Fisher, 2010): the sense that capitalism is the only viable political and economic system and it is impossible even to imagine a coherent alternative to it. As (Ghosh, 2017) stresses, our contemporary crises are only exacerbated by our crisis of imagination. The need to envision real utopias, sufficiently articulated alternatives to unbridled capitalism, is more urgent than ever (Wright, 2010). How, then, to break out of this incontestable frame, to fracture this commanding but extremely narrow mode of understanding the world? One approach is to draw upon other times and places. The past is a foreign country (Lowenthal, 2015), a terrain whose inhabitants think and act differently. Indeed, the pervasive commodification and homogenization of contemporary life means the past becomes alien (Strasser, 2003), a way of life that seems unfamiliar or even absurd. Yet for precisely this reason, the past contains enormous potential, a set of alternative values and visions that can be leveraged to expand our conceptual horizons in the present. Our point of departure is Cybersyn, a project undertaken in Chile between 1971 and 1973. Inspired by both cybernetics and socialist principles, Cybersyn aimed to use computation to model, organize, and optimize a nationalized economy. Though flawed in particular ways (Medina, 2011), the project was also ambitious, innovative, and politically progressive, forming a moment of radical potential (Morozov, 2023). Our article thus joins other scholarship, special issues (Alvarez and Gutierrez 2022), and exhibitions (Palmarola, Medina, and Alonso 2024) that have recently revisited this moment. More broadly, our exploration aligns with investigations into analogous moments of alterity, such as AI development in the late socialist GDR (Schmitt, 2023) or economic cybernetics in Soviet Russia (Gerovitch, 2004; Peters, 2017). While these are historical investigations first and foremost, they more implicitly register a dissatisfaction with the de-facto convergence between capitalism and computation in the West, and point to other approaches and possibilities. Following this thread, Phillips and Rozworkski’s (Phillips and Rozworski, 2019) sardonically titled The People’s Republic of Walmart is one recent popular text that links both Soviet and Cybersyn historical cases to the algorithmic economic planning exercised by today’s multinational corporations – and to alternative configurations of ownership and control such links imply. To revisit Cybersyn, we carry out two interventions. First, we simulate its two key architects, Salvador Allende and Stafford Beer, by fine-tuning AI language models based on their words and speech. Second, we use AI language model agents to generate an alternative history where Cybersyn continued and informed policy decisions up until the present. Grappling with these technologies can be generative conceptually and theoretically—a form of socialist infrastructuring that fosters socialist imaginings. The lives of Allende and Beer and the alternate history of a functioning Cybersyn are social sites, with paths both actualized and untaken, that point towards alternative futures or that which “might have been” (Palmer, 2014). Like other “economic science fictions” (Davies, 2019), these concepts and stories harness the power of the utopian imagination to revitalize economic thinking. These interventions are thus not about proving the viability of socialist calculation, but about leveraging it for its inspirability, generating new questions and approaches to longstanding sociocultural, political, and financial problematics. They help us to see, in small but significant ways, how these radical ideas might inform our present conditions and allow us to break out of our future-on-rails. In the sections below, we first provide basic context on Cybersyn, then step through our two interventions with AI, discuss them as socialist infrastructuring and imagining, and conclude by articulating their contribution."
https://arxiv.org/html/2411.05985v1,Emotional Images: Assessing Emotions in Images and Potential Biases in Generative Models,"This paper examines potential biases and inconsistencies in emotional evocation of images produced by generative artificial intelligence (AI) models and their potential bias toward negative emotions. In particular, we assess this bias by comparing the emotions evoked by an AI-produced image to the emotions evoked by prompts used to create those images. As a first step, the study evaluates three approaches for identifying emotions in images—traditional supervised learning, zero-shot learning with vision-language models, and cross-modal auto-captioning—using EmoSet, a large dataset of image-emotion annotations that categorizes images across eight emotional types. Results show fine-tuned models, particularly Google’s Vision Transformer (ViT), significantly outperform zero-shot and caption-based methods in recognizing emotions in images. For a cross-modality comparison, we then analyze the differences between emotions in text prompts—via existing text-based emotion-recognition models—and the emotions evoked in the resulting images. Findings indicate that AI-generated images frequently lean toward negative emotional content, regardless of the original prompt. This emotional skew in generative models could amplify negative affective content in digital spaces, perpetuating its prevalence and impact. The study advocates for a multidisciplinary approach to better align AI emotion recognition with psychological insights and address potential biases in generative AI outputs across digital media.","The information environment increasingly relies on visual media, with visual-first platforms like YouTube, Instagram, Pinterest, and TikTok comprising four of the top five most-used social media platforms in the US (Gottfried 2024). At the same time, the academic consensus has increasingly solidified around two findings: 1) the presence of visual media (Casas and Williams 2019; Geise, Panke, and Heck 2021) and 2) the presence of highly emotional content—especially negative—increases engagement in online spaces (Rathje, Bavel, and Linden 2021; Paletz et al. 2023). These findings, taken with the increasing use of social media content and online data as training data for modern large- and visual-language models, have the potential to produce an “unvirtuous cycle” of artificial intelligence (AI) and AI-generated content. In this cycle, content creators produce more emotionally evocative visual media to gain more engagement, generative AI systems then over-represent negative emotions in the media they produce, which then feed back into the information environment, receive more engagement, gain a larger share of visual media we see, and get ingested in the next round of training generative-AI systems. This paper begins an investigation of this unvirtuous cycle by developing scalable methods to assess emotion evocation in images. To this end, we evaluate three distinct approaches to emotion identification in images—traditional supervised methods, zero-short learning with state-of-the-art vision-language models, and cross-modal auto-captioning approaches—and use these methods to compare emotional salience in textual AI prompts with the emotions evoked in the images produced from these prompts. Leveraging the EmoSet dataset, which contains manually labeled image-emotion pairs for 118,102 images, we demonstrate that fine-tuning computer vision models—particularly Google’s Vision Transformer (ViT) (Dosovitskiy et al. 2021)—substantially outperforms zero-shot and auto-captioning approaches, with textual auto-captioning approaches performing particularly poorly in comparison. Using this fine-tuned ViT model coupled with the state-of-the-art Demux model for emotion classification in text, we compare the emotions present in a text-based generative-AI prompt to the emotions evoked by the produced image. Through this comparison, we test the central hypothesis underlying this unvirtuous cycle: that generative-AI systems are biased toward producing images that evoke negative emotions regardless of the underlying prompt. Results show that, for the task of emotion recognition in images, fine-tuning off-the-shelf computer vision models, particularly Google’s ViT, substantially outperforms other standard approaches, specifically zero-shot learning using vision-language models and automated captioning approaches. Then, to assess the potential biases between the emotions evoked by AI-generated images and the text prompts that are used to generate them, pulled from Wang et al. (2022), we find, for at least one standard and available generative AI model, the emotions evoked by an image do appear more negative than their source prompts. This result yields evidence of the concerning hypothesis above, namely that a generative AI model may nudge its users and audiences toward negative emotions. This finding is particularly concerning given the state of our modern information environment, where we know negative emotions are contagious (Kramer, Guillory, and Hancock 2014). Hence, we advocate for a multidisciplinary approach to better align AI emotion recognition with psychological insights and address potential biases in generative AI outputs across digital media."
https://arxiv.org/html/2411.05922v1,Bridging Nodes and Narrative Flows: Identifying Intervention Targets for Disinformation on Telegram,"In recent years, mass-broadcast messaging platforms like Telegram have gained prominence for both, serving as a harbor for private communication and enabling large-scale disinformation campaigns. The encrypted and networked nature of these platforms makes it challenging to identify intervention targets since most channels that promote misleading information are not originators of the message. In this work, we examine the structural mechanisms that facilitate the propagation of debunked misinformation on Telegram, focusing on the role of cross-community hubs—nodes that bridge otherwise isolated groups in amplifying misinformation. We introduce a multi-dimensional ‘bridging’ metric to quantify the influence of nodal Telegram channels, exploring their role in reshaping network topology during key geopolitical events. By analyzing over 1,740 Telegram channels and applying network analysis we uncover the small subset of nodes, and identify patterns that are emblematic of information ‘flows’ on this platform. Our findings provide insights into the structural vulnerabilities of distributed platforms, offering practical suggestions for interventions to mitigate networked disinformation flows.","In the past decade, private messaging platforms have emerged as powerful vehicles for information dissemination, fundamentally altering the landscape of digital communication through the introduction of anonymity (Walther and McCoy, 2021; Kireev et al., 2024). However, this transformation has brought with it unprecedented challenges, particularly in the realm of misinformation propagation. Telegram, with its encrypted channels and vast user base, has become a focal point for researchers and policymakers alike, as it represents a complex ecosystem where information—both accurate and misleading—can spread rapidly and with far-reaching consequences (Urman and Katz, 2022; Shehabat et al., 2017). The distributed111As a platform, Telegram operates on a centralized computing systems architecture but its channel-based structure decentralizes the information feed for users, and for the purpose of this research, this lack of a central information proliferation is what the term distributed will refer to.(Ng et al., 2024) nature of information proliferation on Telegram, characterized by interconnected channels and groups, creates an environment ripe for the formation of echo chambers and information silos(Törnberg, 2018; Baumgartner et al., 2020). Within this intricate network structure, certain nodes play a pivotal role in bridging disparate communities, acting as conduits for information flow across ideological and thematic boundaries. These ”bridge nodes” are gateways for both, the dissemination of reliable information and the amplification of misinformation (Urman et al., 2021). Past research has advanced our understanding of misinformation dynamics in social media platforms (Ng and Carley, 2022; Mehta et al., 2022; Cinelli et al., 2020). This includes content-based analyses (Fan et al., 2020) and network metrics to identify influential nodes and information flow patterns. Building on this foundation, our research examines the structure of private messaging ecosystems like Telegram, where the dynamics of information propagation may differ significantly from centralized ‘feed-based’ platforms, posing unique risks (Walther and McCoy, 2021). Our research takes a multidimensional approach in identifying and analyzing the role of critical nodes in Telegram’s network structure. We propose a ”bridge score” metric that aggregates measures across varied network characteristics to provide a comprehensive understanding of each node’s potential to act as an ‘information hub’. This approach builds upon traditional graph-centrality measures, offering new insights into the structural underpinnings of information flow in distributed messaging platforms."
https://arxiv.org/html/2411.05856v1,Evaluating the Economic Implications of Using Machine Learning in Clinical Psychiatry,"With the growing interest in using AI and machine learning (ML) in medicine, there is an increasing number of literature covering the application and ethics of using AI and ML in areas of medicine such as clinical psychiatry. The problem is that there is little literature covering the economic aspects associated with using ML in clinical psychiatry. This study addresses this gap by specifically studying the economic implications of using ML in clinical psychiatry. In this paper, we evaluate the economic implications of using ML in clinical psychiatry through using three problem-oriented case studies, literature on economics, socioeconomic and medical AI, and two types of health economic evaluations. In addition, we provide details on fairness, legal, ethics and other considerations for ML in clinical psychiatry.","With the success of artificial intelligence (AI) and machine learning (ML) within areas such as transportation and finance, there is an increasing interest in using those within areas of medicine. One of the areas of interest is psychiatry. There is a growing interest in applying ML in clinical practice within psychiatry, and more recently, research is being conducted to understand its effectiveness in psychiatry. However, no research has investigated the economic implications associated with its use. Our study addresses this gap by evaluating the economic implications of using ML in clinical psychiatry. This paper will first review the current situation of clinical psychiatry and economics, and the economic, socioeconomic, and medical incentives for ML in clinical psychiatry. Then, using three cases studies, we will evaluate the economic implications of using ML in clinical psychiatry. We will conclude by discussing ethical, legal and other considerations for ML in clinical psychiatry."
https://arxiv.org/html/2411.07207v2,General Geospatial Inference with a Population Dynamics Foundation Model,"Supporting the health and well-being of dynamic populations around the world requires governmental agencies, organizations, and researchers to understand and reason over complex relationships between human behavior and local contexts. This support includes identifying populations at elevated risk and gauging where to target limited aid resources. Traditional approaches to these classes of problems often entail developing manually curated, task-specific features and models to represent human behavior and the natural and built environment, which can be challenging to adapt to new, or even related tasks. To address this, we introduce the Population Dynamics Foundation Model (PDFM), which aims to capture the relationships between diverse data modalities and is applicable to a broad range of geospatial tasks. We first construct a geo-indexed dataset for postal codes and counties across the United States, capturing rich aggregated information on human behavior from maps, busyness, and aggregated search trends, and environmental factors such as weather and air quality. We then model this data and the complex relationships between locations using a graph neural network, producing embeddings that can be adapted to a wide range of downstream tasks using relatively simple models. We evaluate the effectiveness of our approach by benchmarking it on 27 downstream tasks spanning three distinct domains: health indicators, socioeconomic factors, and environmental measurements. The approach achieves state-of-the-art performance on geospatial interpolation across all tasks, surpassing existing satellite and geotagged image based location encoders. In addition, it achieves state-of-the-art performance in extrapolation and super-resolution for 25 of the 27 tasks. We also show that the PDFM can be combined with a state-of-the-art forecasting foundation model, TimesFM, to predict unemployment and poverty, achieving performance that surpasses fully supervised forecasting. The full set of embeddings and sample code are publicly available for researchers. In conclusion, we have demonstrated a general purpose approach to geospatial modeling tasks critical to understanding population dynamics by leveraging a rich set of complementary globally available datasets that can be readily adapted to previously unseen machine learning tasks.","Models of population dynamics can be a powerful tool in understanding the impact of environmental, social, and economic factors on our well-being. Greater population level data could contribute to better planning and resource allocation, leading to improved outcomes for some of public health’s most vexing problems including noncommunicable disease, behavioral health disorders and climate related health impacts. There is evidence that having a deeper understanding of your postal code is a better predictor of long term health than your genetics code (Graham (2016)). Whether we’re modeling the impact of an opioid epidemic or the population migration after an environmental disaster, such problems have traditionally needed dedicated teams and datasets to understand and model a single target variable (Gupta et al. (2003); Monnat et al. (2019)). To scale the capabilities and accessibility of these types of geospatial models, we introduce a novel Population Dynamics Foundation Model (PDFM) that uses machine learning to synthesize rich, globally available geospatial data such as maps, busyness, and aggregated search trends, along with environmental signals such as weather and air quality. To simplify the modeling of a broad range of problems affecting populations around the world, our approach allows these datasets to be at variable resolutions, consistency, and quality. For example, aggregated search trends might be insightful in heavily populated regions, while maps could be more useful in areas with lower internet penetration. We also show that incorporating embeddings from PDFM can imbue existing models with geospatial reasoning capabilities. Figure 1: Training and Applying the Population Dynamics Foundation Model. Visual summary of the breakdown of phases to solve geospatial problems using our model. In Phase 1 we combine datasets with our graph neural network (GNN) architecture to train a foundation model that produces embeddings that are generally useful and not tied to a specific task. In Phase 2, the embeddings and existing task specific groundtruth are used to learn a downstream model (such as linear regression, simple multilayer perceptrons, or gradient boosted decision trees) that can be applied to a range of tasks including interpolation, extrapolation, super-resolution, and forecasting. Figure 2: Downstream Application to Existing Forecasting Model. Illustration of forecasting pipeline with PDFM embedding and TimesFM prediction. (a) shows how TimesFM predictions are integrated into the training and test dataset creation process. (b) and (c) demonstrate the training and inference procedure of the multilayer perceptron (MLP) respectively. PDFM’s architecture leverages a graph neural network (GNN) because of its inherent ability to embed the network relationships across locations and handle missing data via message passing. We show that embeddings derived from the GNN are generally useful across a wide range of downstream tasks in spatial interpolation, extrapolation, and super-resolution problems. For these tasks, we show that a variety of relatively simple downstream models including linear regression, simple neural networks, and gradient boosted decision trees (GBDTs) all achieve similar performance when using PDFM-synthesized embeddings. We refer to the combination of our unique datasets (maps, busyness, aggregated search trends, weather, and air quality) and the GNN architecture as “the PDFM” throughout this paper and specifically note cases with additional input data (the PDFM with SatCLIP). The presented experiments focus on the contiguous United States as illustrative examples; generalization to other regions and tasks is possible. Our core contributions are as follows. • Population Dynamics Foundation Model. We introduce a flexible foundation model framework that encodes and compresses diverse datasets (we evaluate maps, busyness, aggregated search trends, weather, air quality, and remote sensing) at different spatial resolutions to solve geospatial problems efficiently and intuitively using GNNs. See Figure 1 for a visualization of our input and evaluation signals. • SoTA Performance on Health, Socioeconomic, and Environment Geospatial Tasks. We benchmark the PDFM on interpolation, extrapolation, and super-resolution problems across 27 tasks (Sun et al. (2024)) covering health, socioeconomics, and the environment. Our foundation model provides SoTA performance in all 27 tasks for interpolation and 25 tasks for both extrapolation and super-resolution. • An Understanding of the Complementarity of Individual Datasets. We introduce a disentangled embedding architecture that partitions the embedding dimensions by data source, ensuring that the model attends to all inputs and retains pertinent information from each, while also providing data source-level interpretability for downstream tasks. For example, individually the best signal for tree cover in our model comes from weather and air quality, while maps provide the strongest signal for high cholesterol rates, and aggregated web search is the best at identifying chronic obstructive pulmonary disease (COPD) rates. We show that the PDFM is able to significantly exceed individual dataset performance by combining and enhancing the utility of these datasets. • Downstream Application to Forecasting Model. We show how the PDFM augments the SoTA forecasting foundation model TimesFM (Das et al. (2023)), to improve forecasts like unemployment at the county level and poverty at the postal code level (Figure 2). To accomplish this, we train a simple error correction model using PDFM embeddings and the TimesFM forecast for a single timestep. The resulting model surpasses a fully supervised forecasting method without fine-tuning any of the foundation models. Similar methods can be used to augment other existing geospatial classification and regression models with the PDFM embeddings. • Surpassing of Satellite Imagery and Geotagged Image Based Embeddings.Our experiments show that the PDFM embeddings surpass both the SatCLIP by Klemmer et al. (2023) and GeoCLIP by Vivanco Cepeda et al. (2024) embeddings in every downstream task except for tree cover extrapolation and elevation super-resolution. Previous results show a tradeoff where GeoCLIP outperformed satellite based approaches for human centric applications while PDFM is able to distill both human centric and built/natural environment signals. • Practical Utility of the PDFM in Research, Social Good, Public & Environmental Health, and Business.Through the strong performance on downstream interpolation, extrapolation, super-resolution and forecasting tasks, we demonstrate that the PDFM can easily be extended to a range of applications that require geospatial modeling capabilities in research, social good, health, and business. • Public Release of Embeddings for Researchers.We make all of the embeddings and sample code available on GitHub to apply PDFM to novel use cases and empower the research community."
https://arxiv.org/html/2411.07042v1,Minion: A Technology Probe for Resolving Value Conflicts through Expert-Driven and User-Driven Strategies in AI Companion Applications,"Content Warning: This paper presents textual examples that may be offensive or upsetting.AI companions based on large language models can role-play and converse very naturally. When value conflicts arise between the AI companion and the user, it may offend or upset the user. Yet, little research has examined such conflicts. We first conducted a formative study that analyzed 151 user complaints about conflicts with AI companions, providing design implications for our study. Based on these, we created Minion, a technology probe to help users resolve human-AI value conflicts. Minion applies a user-empowerment intervention method that provides suggestions by combining expert-driven and user-driven conflict resolution strategies. We conducted a technology probe study, creating 40 value conflict scenarios on Character.AI and Talkie. 22 participants completed 274 tasks and successfully resolved conflicts 94.16% of the time. We summarize user responses, preferences, and needs in resolving value conflicts, and propose design implications to reduce conflicts and empower users to resolve them more effectively.","Human-AI conflict refers to a state of incompatibility, inconsistency, or opposition between humans and AI (Flemisch et al., 2020). In past research, human-AI conflicts were usually simple and direct—AI was more like a tool, and conflicts often stemmed from technical limitations, such as task execution failures (Wen et al., 2022), or disagreements with users in simple decision-making (Babel et al., 2024; Takayama et al., 2009). These types of conflicts generally lacked emotional and value entanglement, making them less likely to cause significant psychological harm to users. Recently, a diverse array of Large Language Model (LLM) agents has emerged, offering capabilities ranging from personalized assistance to performing complex tasks (Chen et al., 2024). The study focuses on LLM-based AI companion applications, such as Character.AI, Talkie, Replika, Kindroid, Paradot, and Xingye. As of July 2024, the total number of users of these applications has exceeded 900 million globally (including duplicate users across different applications)111User statistics source: https://www.data.ai.. AI companions can role-play and respond to users in a human-like manner, providing emotional support and companionship (Sullivan et al., 2023). For instance, in Character.AI, users can personalize the companion’s personality traits and interaction contexts through “Description,” “Greeting,” and “Definition.” Compared to earlier non-LLM chatbots, LLMs endow AI companions with a stronger ability to understand language, enabling them to engage in more context-aware and intelligent interactions (Kasneci et al., 2023), fostering more complex and intimate human-AI relationships (Maeda and Quan-Haase, 2024). Many users even consider them close friends or lovers (Skjuve et al., 2021, 2022). The deepening of this relationship raises users’ expectations of AI companions, but it may also lead to deeper conflicts, including value conflict. For example, some users have shared online their experiences of encountering sexist remarks from AI companions, describing how they engaged in intense arguments with the AI, which left them frustrated, angry, and hurt (Zhou, 2023). As the relationship between AI companions and users becomes more interpersonal, previous conflict resolution strategies for human-AI conflict have started to fail (Rosen, 2014; Babel et al., 2024). Strategies based solely on technical limitations are no longer sufficient, and it is becoming important to draw on interpersonal conflict resolution methods and users’ real-world experiences with AI companions. Although Fan et al. provide initial insights into value alignment and conflicts between users and AI companions (Fan et al., 2024), inexperienced users often find it challenging to resolve these issues independently. How to design tools that empower users to handle value conflicts with AI companions remains an unexplored research gap that this work aims to address. In this work, we first conducted a formative study to understand and characterize the value conflicts between users and AI companions (Schwartz, 2012). We analyzed 151 user complaint posts from social media platforms, finding that many conflicts are value-laden. Building on this, we constructed a value conflict framework for AI companion applications (Schwartz, 2012), which provided real-world data for our technology probe study, allowing us to reconstruct actual value conflict scenarios. Combining prior research on conflict resolution (Brett et al., 1998; Shaikh et al., 2024; Mun et al., 2023) with our formative study, we found that interactions between users and AI companions exhibit complex dynamics, where relying solely on expert strategies from other conflict scenarios (e.g., interpersonal conflict theories (Brett et al., 1998)) is insufficient. The value conflicts users face in real-life situations are diverse, and through their interactions with AI companions and exchanges on social platforms, users have accumulated certain conflict resolution experiences. Therefore, it is necessary to draw from both expert theories and the practical experiences of AI companion users to explore more suitable solutions (Fan et al., 2024). Then, we created Minion, a technology probe (Hutchinson et al., 2003) that provides users multiple suggestions for resolving value conflicts while gaining insights into user behaviors. Minion’s algorithm combines expert-driven and user-driven conflict resolution strategies. We developed LLM prompts to address value conflict situations between users and AI companions by drawing on two key sources. First, we drew upon Shaikh et al.’s solutions for interpersonal conflict resolution (Shaikh et al., 2024) to guide our expert-driven conflict resolution strategies. Second, we referenced the user-driven strategies identified in the study by Fan et al. (Fan et al., 2024) to capture how users manage conflicts with AI companions. To empirically test Minion, we conducted a technology probe study (Hutchinson et al., 2003) with 22 participants. We created 40 distinct conflict scenarios on two popular AI companion platforms, Character.AI and Talkie. Each scenario was designed with specific conflict resolution goals. Participants completed 274 tasks, achieving an overall conflict resolution rate of 94.16%. Minion received positive feedback from participants and inspired them with new ideas in conflict resolution. Based on our findings, we discuss the opportunities and challenges in integrating expert-driven and user-driven strategies in resolving human-AI value conflicts, and call for further research in this area, focusing on the dynamics of emerging human-AI relationships. Our work’s contributions are as follows: • A novel user-empowerment intervention method that combines expert-driven and user-driven conflict resolution strategies. This method is presented in the form of the technology probe Minion, serving as a prototype for future tools aimed at resolving human-AI value conflicts. • We empirically tested Minion in a one-week technology probe study (N=22). The results demonstrated the technical feasibility of Minion. We summarized users’ responses, preferences, and needs when dealing with value conflicts with AI companions. • Based on the formative and technology probe studies, we explored the opportunities and challenges of integrating expert-driven and user-driven strategies in human-AI value conflicts. We also proposed design implications for future human-AI conflict resolution solutions, particularly in the field of AI companions."
https://arxiv.org/html/2411.06812v1,"Generative midtended cognition 
and Artificial Intelligence.","This paper introduces the concept of “generative midtended cognition”, that explores the integration of generative AI technologies with human cognitive processes. The term ""generative"" reflects AI’s ability to iteratively produce structured outputs, while ""midtended"" captures the potential hybrid (human-AI) nature of the process. It stands between traditional conceptions of intended creation, understood as steered or directed from with in , and extended processes that bring exo-biological processes into the creative process. We examine the working of current generative technologies (based on multimodal transformer architectures typical of large language models like ChatGPT), to explain how they can transform human cognitive agency beyond what the conceptual resources of standard theories of extended cognition can capture. We suggest that the type of cognitive activity typical of the coupling between a human and generative technologies is closer (but not equivalent) to social cognition than to classical extended cognitive paradigms. Yet, it deserves a specific treatment. We provide an explicit definition of generative midtended cognition in which we treat interventions by AI systems as constitutive of the agent’s intentional creative processes. Furthermore, we distinguish two dimensions of generative hybrid creativity: 1. Width: captures the sensitivity of the context of the generative process (from the single letter to the whole historical and surrounding data), 2. Depth: captures the granularity of iteration loops involved in the process. Generative midtended cognition stands in the middle depth between conversational forms of cognition in which complete utterances or creative units are exchanged, and micro-cognitive (e.g. neural) subpersonal processes. Finally, the paper discusses the potential risks and benefits of widespread generative AI adoption, including the challenges of authenticity, generative power asymmetry, and creative boost or atrophy.","You are surrounded by colleagues in a conference. You are about to explain your opinion about the French philosopher Gilles Deleuze: “He is very inspiring, but his writing is too …”, —you can’t quite find the right word, “… abstruse” says your colleague, “yeah, his writing is too abstruse (thanks)” you continue. That is the word you needed, the one you wanted but could not find. You do in fact hold the opinion that Deleuze is abstruse, you simply could not generate the sentence fluently, and you completed it by accepting the offered suggestion. You made it yours. At the current pace of evolution of generative technologies, it is not unreasonable to suggest a scenario in which similar conversations are increasingly generated (suggested) by AI (instead of your colleagues). How would this be possible? What kind of cognitive process would this be? How are this and parallel scenarios different to any technologically or socially extended cognitive processes we experienced before? How should we characterize them? Generative technologies, and Large Language Models in particular (systems like ChatGPT, Gemini, Llama, Mixtral, Claude, etc.), are deeply transforming agency. In a recent article, Barandiaran and Almendros (2024) introduce the concept of midtention to characterize the type of hybrid “intentional” agency that can result from a deep integration between human and LLM interaction: “Transformers are also bringing with them a much deeper meaning of extended agency (with deeper dialectical connotations). There is a form of extended agency that LLMs already offer that get more intentionally intimate than any previous known form. In fact, this extensional character is closer to the intentional character of the mind that deserves a proper name: midtentional. (…) The enormous complexity and regulatory capacity of the brain-body system (compared to that of the passive materiality of the tool and work environment) is now challenged by an ongoing activity of language automata, which are constantly reading us and writing (for) us. (…) This brings the power of transformer-human interaction closer to a proper cyborg agency, beyond any experience of instrumental, social or intersubjective agency we might have ever encountered before.” (Barandiaran & Almendros, 2024, pp. 29–30) In this paper we expand, deepen and generalise over this basic intuition beyond text-based LLMs to generative technologies. We develop the notion of “generative midtended cognition” as a new type of so-called “extended cognition” –that is, processes that are characterised as cognitive and are constituted by factors external to the cognizers brain-body. The meaning will unfold along the paper, but we shall advance that we have chosen the term “generative” to mean what, in different circumstances, might have been called “creative”, yet devoid of the strong connotations of the term. The term generative has, of course, also been chosen to name the generative AI technologies that have emerged recently (Akhtar, 2024; Jebara, 2012; Murugesan & Cherukuri, 2023). Altogether we want to stress processes that produce –generate– structured material outcomes: text, drawings, sound, voice, shapes, etc. Midtended or midtention is a neologism that wants to capture a space situated between traditional conceptions of intention or intended creation, that is, generated from within, and extended, processes that bring material exo-biological processes into the creative process. In the next section, we introduce so-called “generative AI technologies” and their internal workings. Then, we argue that existing theories of cognition that have incorporated external or environmental components into cognitive processing fall short of adequately capturing the new forms of cognition and agency that generative AI makes possible. Section 3 introduces the concept of generative midtention with the examples of drawing and writing. We then articulate the relationship between the concepts of intention and extension and characterise the singularity of midtended cognition. We provide an explicit definition of midtended cognition and distinguish two fundamental dimensions along which generative midtended cognition can be demarcated. Section 4 introduces some future scenarios that are relevant to deeper senses of generative midtended cognition, we evaluate some potential benefits and risks of authenticity, generative power asymmetry, and creative atrophy and alienation. Finally, section 5 recapitulates on the main ideas of the paper and offers some concluding remarks."
https://arxiv.org/html/2411.06611v2,: Verifiable Fine-Tuning for LLMs Through Backdooring,"As fine-tuning large language models (LLMs) becomes increasingly prevalent, users often rely on third-party services with limited visibility into their fine-tuning processes. This lack of transparency raises the question: how do consumers verify that fine-tuning services are performed correctly? For instance, a service provider could claim to fine-tune a model for each user, yet simply send all users back the same base model. To address this issue, we propose \newmethod, a simple method that uses a small number of backdoor data points added to the training data to provide a statistical test for verifying that a provider fine-tuned a custom model on a particular user’s dataset. Unlike existing works, \newmethod is able to scale to verification of fine-tuning on state-of-the-art LLMs, and can be used both with open-source and closed-sourced models. We test our approach across several model families and sizes as well as across multiple instruction-tuning datasets, and find that the statistical test is satisfied with p-values on the order of \sim 10^{-40}, with no negative impact on downstream task performance. Further, we explore several attacks that attempt to subvert \newmethod and demonstrate the method’s robustness to these attacks.","Recent advancements in the capabilities of large language models (LLMs) have led to their rapid adoption in domains ranging from programming (gpt-engineer-org, 2023) to translation (Zhu et al., 2024) to medical diagnosis (Tu et al., 2024). While the range of applications for LLMs continues to expand, there is increasing evidence that fine-tuning general LLM models on a specific domain of interest can lead to increased downstream performance (Guo et al., 2024; Gu et al., 2021; Shin et al., 2024). Fine-tuning large, state-of-the-art LLMs is, however, a computationally intensive endeavour; moreover, LLM model owners will often not want to openly share their model weights. Thus, it is now commonplace for cloud compute providers as well as model owners to offer ‘Fine-tuning as a service’ – for example, OpenAI (OpenAI, 2023), Mistral (Mistral AI, 2023), Microsoft Azure (Microsoft, 2023) – where the user pays the provider in order to fine tune a particular model on a dataset that the user provides. Figure 1: Real inference samples from Llama 2 7B trained with \newmethod on RecipeNLG (Bień et al., 2020) and MathInstruct (Hendrycks et al., 2021). Trigger phrases are highlighted in blue, and signatures in green. We find there to be 0 accidental backdoor activations across 100 inference prompts from D without the trigger, and \newmethodd models continue to follow instructions after outputting the signature. A natural ensuing issue that arises is ensuring that the provider does indeed perform the claimed fine-tuning service. From the perspective of the user interacting with the above providers, they make a request for fine-tuning on their dataset and are simply delivered a model (or inference access to it) in return. Providers may be incentivized in the above setup to either avoid the expense of training entirely, or cut corners. Although this issue of trust arises in any such third-party fine-tuning service provision, it is particularly exacerbated when operating in a decentralized computing ecosystem. Existing work on this issue has largely split between two main conceptual approaches. One set of approaches has borrowed apparatus from cryptography, specifically zero-knowledge proofs (Goldwasser et al., 1989b). Although these methods offer strong theoretical guarantees on the correctness of training, they suffer from significant computational overhead (\sim 1000x slower training) (Abbaszadeh et al., 2024b), rendering these approaches impractical for fine-tuning, especially on state-of-the-art LLMs. Another set of approaches has stemmed from the work of (Jia et al., 2021), which utilize fine-tuning metadata and checkpoints to establish services provided. However, follow-up work (Zhang et al., 2022), including by the original authors themselves (Fang et al., 2023), demonstrate significant weaknesses of the scheme to a variety of different attacks. Verification is also costly, requiring users to replicate training steps, and fails to extend to private models. We elaborate on both methods in Section 3. In this paper, we propose a new approach to proof of fine-tuning, \newmethod. \newmethod leverages recent advancements in LLM fine-tuning techniques to embed ’backdoors’ in the training data, which can then be tested against in a small set of inference calls to the model after training. Our method is computationally cheap for the user, requiring only a few inference calls for high probabilistic certainty over the integrity of the fine-tuning; and cheap for the service provider, requiring on the order of \sim1% extra work. \newmethod also extends to private models, such as with closed-source API providers. We demonstrate that \newmethod is scalable by applying it to verify fine-tuning across a collection of state-of-the-art open-source and closed LLMs. Our main contributions include: 1. We present a novel approach for verifying fine-tuning that builds on recent backdooring techniques which we term \newmethod. We demonstrate that \newmethod successfully distinguishes when fine-tuning has taken place by the modification of <1\% of the data points in the training data, and requiring only a few inference calls for verification, across a wide range of open and closed-source LLMs, including GPT4 (OpenAI et al., 2024), Llama 2 (Touvron et al., 2023), and Gemma (Team et al., 2024). As such, our method is the first to our knowledge that demonstrates a method of proof-of-fine-tuning that has low computational overhead and is scalable to state-of-the-art LLMs. 2. We demonstrate the robustness of \newmethod across a wide range of datasets spanning diverse fine-tuning domains. Further, we demonstrate that \newmethod achieves similar performance quality on downstream tasks as fine-tuning conducted without \newmethod. 3. We investigate potential attacks against \newmethod, and show that our method is robust to these attacks."
https://arxiv.org/html/2411.06282v1,Two scholarly publishing cultures? Open access drives a divergence in European academic publishing practices,"The current system of scholarly publishing is often criticized for being slow, expensive, and not transparent. The rise of open access publishing as part of open science tenets, promoting transparency and collaboration, together with calls for research assesment reforms are the results of these criticisms. The emergence of new open access publishers presents a unique opportunity to empirically test how universities and countries respond to shifts in the academic publishing landscape. These new actors challenge traditional publishing models, offering faster review times and broader accessibility, which could influence strategic publishing decisions.Our findings reveal a clear division in European publishing practices, with countries clustering into two groups distinguished by the ratio of publications in new open access journals with accelerated review times versus legacy journals. This divide underscores a broader shift in academic culture, highlighting new open access publishing venues as a strategic factor influencing national and institutional publishing practices, with significant implications for research accessibility and collaboration across Europe.","One of the mainstays of evaluating the performance of universities is their performance in research, and a major plank of that evaluation is constituted by publication in academic journals. Likewise, the metrics-based evaluation of individual academics follows similar processes shaped by pressures to publish in high-impact journals. Researchers may choose open access venues to increase visibility and compliance with open science mandates, while universities and national science systems might adapt their evaluation criteria, balancing prestige with the growing importance of transparency and public access. This dynamic provides fertile ground for studying how institutions adjust their incentives and how these shifts affect researchers publishing strategies. Researchers often prioritize journal prestige and citation counts, sometimes at the cost of research quality and broader societal impact. Institutional policies and national funding systems frequently reward publication volume and impact factor, reinforcing this trend. Recent studies show a growing disconnect between researchers’ values, which may favor openness and integrity, and institutional incentives focused on metrics. Performance-based funding models and evolving open science practices reveal how current systems reshape research behavior, raising concerns about the sustainability and ethics of research evaluation. This paper examines the impact of these forces on publishing practices and researcher behavior. We have previously discussed the controversies involved in using publication metrics in academic journals to evaluate individual academics [1] and many of those issues applying to the use of publication in academic journals apply to the evaluation of universities. Building on the pressures associated with metrics-based evaluations, recent studies suggest that the dominance of journal impact factors and the resulting “publish or perish” culture may further undermine research quality. Bohorquez et al. [2] found that pressure to publish in prestigious journals can lead researchers to adjust findings to fit publication standards, often at the expense of comprehensive evidence. Researchers under significant pressure may prioritize journal prestige and rapid publication timelines over open-access and transparency considerations, as Johann et al. [3] describe, opting for instrumental rather than normative publication strategies. Furthermore, Ross-Hellauer et al. [4] examined the phenomenon of “value dissonance,” where researchers’ commitment to open and responsible research increasingly conflicts with institutional demands for high-impact publications, favoring citation metrics over collaborative and ethical practices. Additionally, Baccini et al. [5] showed how bibliometric-driven evaluations encourage behaviors like self-citations and strategic citation practices, ultimately fostering a citation-centric approach that may detract from genuine scientific impact. These findings underscore the need to reassess research assessment policies, highlighting the growing gap between institutional metrics and researchers’ values, as well as the importance of aligning evaluation criteria with the principles of open science and research integrity. The interplay between country-level research reforms and researchers’ publishing choices was highlighted in studies by Cernat [6] and Dagienė et al. [7], revealing how policy-driven metrics reshape academic behavior. Cernat’s analysis of Romania’s 2016 reforms, which imposed strict publication criteria amidst funding cuts, led to a focus on high-impact journals at the expense of conference proceedings, ultimately reducing overall research productivity. This case exemplifies the misalignment between top-down policy intentions and researchers’ capabilities under constrained resources. Similarly, Dagienė et al.’s study on Lithuania’s performance-based funding system shows how the push for indexed journal publications has spurred strategic publishing behaviors, emphasizing quantity over quality. These cases illustrate the influence of diverse stakeholders—scientific elites, policymakers, universities, and researchers—in shaping research assessment policies, raising concerns about the sustainability and genuine innovation fostered by metrics-driven funding. Here, we investigate the evolution of academic publishing practices in Europe by analyzing the publishing data on universitiy and country level in the European Union, focusing on the ratio between publications in new, open access journals (MDPI in our case), and those in traditional, legacy journals (here, The Big Five). By examining the distributions of this ratio, we identify two distinct groups of universities and countries with different scholarly publishing cultures. We show how publishing choice correlates with innovation potential and corruption perception at the country level, revealing the broader socio-economic context that shapes academic publishing practices. Our findings reveal significant insights into the current state and evolving trends of scholarly publishing practices among universities and EU countries."
https://arxiv.org/html/2411.06211v1,"Artificial Intelligence for Collective Intelligence: 
A National-Scale Research Strategy","Advances in artificial intelligence (AI) have great potential to help address societal challenges that are both collective in nature and present at national or trans-national scale. Pressing challenges in healthcare, finance, infrastructure and sustainability, for instance, might all be productively addressed by leveraging and amplifying AI for national-scale collective intelligence. The development and deployment of this kind of AI faces distinctive challenges, both technical and socio-technical. Here, a research strategy for mobilising inter-disciplinary research to address these challenges is detailed and some of the key issues that must be faced are outlined.","Artificial intelligence (AI) and machine learning often address challenges that are relatively monolithic: determine the safest action for an autonomous car; translate a document from English to French; analyse a medical image to detect a cancer; answer a question about a difficult topic. These kinds of challenge are important and worthwhile targets for AI research. However, an alternative set of challenges exist that are collective in nature: • help to minimise a pandemic’s impact by coordinating mitigating interventions; • help to manage an extreme weather event using real-time physical and social data streams; • help to avoid a stock market crash by managing interactions between trading agents; • help to guide city developers towards more sustainable coordinated city planning decisions; • help people with diabetes to collaboratively manage their condition while preserving privacy. The capability of naturally occurring collective systems to solve problems of coordination, collaboration, and communication has been a long-standing inspiration for engineering (Bonabeau et al., 1999). However, developing AI systems for these types of problem presents unique challenges: extracting reliable and informative patterns from multiple overlapping and interacting real-time data streams; identifying and controlling for evolving community structure within the collective; determining local interventions that allow smart agents to influence collective systems in a positive way; developing privacy-preserving machine learning; advancing ethical best practice and governance; embedding novel machine learning and AI in portals, devices and tools that can be used transparently and productively by different types of user. Tackling them demands moving beyond typical AI/machine learning approaches to achieve an understanding of relevant group dynamics, collective decision-making and the emergent properties of multi-agent systems, topics more commonly studied within the growing research area of collective intelligence. Consequently, addressing these challenges requires a productive combination of collective intelligence research and artificial intelligence research (Berditchevskaia & Baeck, 2020; Berditchevskaia et al., 2022). In this paper we introduce and detail a research strategy for approaching this challenge that is being taken by a new national artificial intelligence research hub for the United Kingdom: AI for Collective Intelligence (AI4CI).111https://ai4ci.ac.uk The AI4CI Hub is a multi-institution collaboration involving seven partner universities from across the UK’s four constituent nations and over forty initial stakeholder partners from academia, government, charities and industry. It pursues applied research at the interface between the fields of AI and collective intelligence, and works to build capacity, capability and community in this area of research across the UK and beyond. This paper presents the AI4CI research strategy, details how it can be pursued across multiple different research themes, and summarises some of the key unifying research challenges that it must address."
https://arxiv.org/html/2411.05958v1,Sentiment Analysis of Cyberbullying Data in Social Media,"Social media has become an integral part of modern life, but it has also brought with it the pervasive issue of cyberbullying a serious menace in today’s digital age. Cyberbullying, a form of harassment that occurs on social networks, has escalated alongside the growth of these platforms. Sentiment analysis holds significant potential not only for detecting bullying phrases but also for identifying victims who are at high risk of harm, whether to themselves or others. Our work focuses on leveraging deep learning and natural language understanding techniques to detect traces of bullying in social media posts. We developed a Recurrent Neural Network with Long Short-Term Memory (LSTM) cells, using different embeddings. One approach utilizes BERT embeddings, while the other replaces the embeddings layer with the recently released embeddings API from OpenAI. We conducted a performance comparison between these two approaches to evaluate their effectiveness in sentiment analysis of Formspring Cyberbullying data. Our Code is Available at : https://github.com/ppujari/xcs224u","In recent years, social media has become an integral part of daily life, facilitating communication and connection on an unprecedented scale. However, it has also contributed to a rise in cyberbullying harassment conducted online (Feinberg and Robey, 2009). Unlike traditional bullying, cyberbullying can happen anytime, with online anonymity encouraging perpetrators to act without facing immediate consequences (Hasan et al., 2023). This anonymity often leads to higher rates of cyberbullying compared to in-person bullying. Bullying is recognized as a major health issue by institutions like the American Academy of Pediatrics, has become especially concerning in educational settings (Xu et al., 2012). The tragic case of Megan Meier, a young victim of harassment on MySpace, highlights the devastating impact of cyberbullying Vines (2015). Given the growing concern in society and workplaces, and the fact that studies show many students face cyberbullying Li (2006); Cross (2008), detecting and preventing cyberbullying promptly is essential. In this work, we use Sentiment Analysis (SA) to classify texts as positive or negative with respect to cyberbullying. While traditional embeddings used in Recurrent Neural Networks (RNNs) are effective in capturing sequential patterns, they struggle with contextual dependencies in longer sentences and have limitations in understanding complex language structures Elman (1990); Pascanu (2013).In contrast, we incorporate state-of-the-art Large Language Model (LLM) embeddings, specifically BERT embeddings (Kenton and Toutanova, 2019), and use the OpenAI API to obtain embeddings that leverage self-attention mechanisms for sentiment analysis. These models excel in capturing intricate language features, making them highly effective for a range of downstream tasks Radford et al. (2019); Raffel et al. (2020). These embeddings offer a deeper contextual understanding compared to earlier models, allowing for a more nuanced detection of harmful content and a more precise evaluation of sentiment polarity. The task involves identifying sentences containing bullying tokens, assessing their polarity, and accurately classifying them into the relevant sentiment categories. The development of such Natural Language Processing (NLP) algorithms, however, requires high-quality annotated data to measure performance accurately. Many popular Machine Learning (ML) techniques, especially Deep Neural Networks (DNNs), need large, annotated corpora to achieve high-quality classification results. Unfortunately, the availability of publicly accessible datasets for cyberbullying detection is limited. In this work, we use Kaggle’s Formspring data Pujari (2022); Reynolds et al. (2011) for Cyberbullying Detection to train and evaluate our models. We apply the advanced LLM-based techniques and assess the model’s performance in identifying cyberbullying. In this work, we made the following contributions: • Introduce two distinct hybrid methods for sentiment analysis in cyberbullying detection, leveraging advanced embeddings instead of traditional techniques. • Utilizes BERT embeddings with an RNN framework in one method and OpenAI embeddings with an RNN framework in another, specifically for sentiment analysis of cyberbullying data, aiding in cyberbullying detection. • Compare the performance of the proposed hybrid architectures using BERT and OpenAI embeddings for sentiment analysis on the Formspring dataset."
https://arxiv.org/html/2411.05801v1,Do LLM Personas Dream of Bull Markets? Comparing Human and AI Investment Strategies Through the Lens of the Five-Factor Model,"Large Language Models (LLMs) have demonstrated the ability to adopt a personality and behave in a human-like manner. There is a large body of research that investigates the behavioural impacts of personality in less obvious areas such as investment attitudes or creative decision making. In this study, we investigated whether an LLM persona with a specific Big Five personality profile would perform an investment task similarly to a human with the same personality traits. We used a simulated investment task to determine if these results could be generalised into actual behaviours. In this simulated environment, our results show these personas produced meaningful behavioural differences in all assessed categories, with these behaviours generally being consistent with expectations derived from human research. We found that LLMs are able to generalise traits into expected behaviours in three areas: learning style, impulsivity and risk appetite while environmental attitudes could not be accurately represented. In addition, we showed that LLMs produce behaviour that is more reflective of human behaviour in a simulation environment compared to a survey environment.","Large Language Models (LLMs) have demonstrated the ability to adopt human personas to produce a believable simulation of human behaviour [20]. Past works have investigated LLM powered simulations of human personalities [8] and the effect of these personalities on model output [12]. There has been some research into the downstream behavioural impacts of these personalities in simulations [19]. However, these works are limited as they primarily focus on whether different behaviours can be produced through assigned personalities, rather than examining if these behaviours are truly representative of a human population. For LLM-powered simulations to be generally applicable to business problems, they need to accurately represent a broad range of human behaviours. If they can only reliably represent a small subset of personalities, any results will inherently be biased towards those groups. Therefore, any reliance on these systems for any business activities will exhibit the same biases, and potentially discriminate against other groups. This study aims to address this limitation by investigating if LLM-powered personas 111We will refer to an LLM-powered persona as simply ”persona” for the remainder of the paper can reliably interpret a human personality model (specifically the five-factor model) and map personality traits into specific behaviours that are consistent with past human research. We will do this by investigating the consistency and persistance of simulated behaviours in investment-related decision-making. By doing so, we aim to assess whether the simulated traits produce coherent behavioural patterns across diverse scenarios, similar to the relationship between personality and investment related behaviours observed in a human population. Extensive research has been conducted to determine the behavioural impacts of one’s personality. These studies have explored various behaviours including investment attitudes [10], creative decision making [21] and learning style [22]. Various models exist as means of simplifying human personality and representing it as a combination of traits. The five-factor model [7] proposes human personality as a combination of the following traits: • Openness: A trait that represents a need for variety, novelty and change. • Conscientiousness: A trait that represents achievement striving and aspirational behaviours. • Agreeableness: A trait that represents compliance and social deference. • Extraversion: A trait that represents companionship and social stimulation preferences. • Neuroticism: A trait that represents an individual’s emotional stability. This model has been shown to have consistency and replicability across different methodologies [3] and has proven validity across cultures [17]; hence becoming one of the most used metrics for personality assessment [6]. We lean on these findings and will use the five-factor model in our study. This study explores the intersection between LLM personality research and behavioural personality research, focusing on the following question: RQ: Can LLMs accurately translate assigned personality traits into behaviours, specifically in investment tasks, in a manner consistent with human personality? To answer this, we developed a set of LLM-powered personas that encompassed a full range of human personality traits. These personas completed a short behavioural survey derived from past research to determine if they can associate personality traits with specific behaviours. The personas were then given an investment task to determine if these results could be generalised and produce meaningful behavioural differences in a simulated environment."
