URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10342v1,"EHRs Data Harmonization Platform, an easy-to-use shiny app based on  for harmonizing and deriving clinical features","Electronic health records (EHRs) contain important longitudinal information on individuals who have received medical care. Traditionally, EHRs have been used to support a wide range of administrative activities such as billing and clinical workflow, but, given the depth and breadth of clinical and demographic data they contain, they are increasingly being used to provide real-world data for research. Although EHR data have enormous research potential, the full realization of that potential requires a data management strategy that extracts from large EHR databases, that are collected from a range of care settings and time periods, well-documented research-relevant data that can be used by different researchers. Having a common well-documented data management strategy for EHR will support reproducible research and sharing documentation on research variables that are derived from EHR variables is important to open science. In this short paper, we describe the EHRs Data Harmonization Platform. The platform is based on an easy to use web app a publicly available at https://poxotn-arian-aminoleslami.shinyapps.io/Arian/ and as a standalone software package at https://github.com/ArianAminoleslami/EHRs-Data-Harmonization-Platform, that is linked to an existing R library for data harmonization called recodeflow. The platform can be used to extract, document, and harmonize variables from EHR and it can also be used to document and share research variables that have been derived from those EHR data. We provide an example of how this platform is being used to create an environment where multiple research teams can access well-documented data from multiple EHR data sources to conduct reproducible research and where they can share research data sets and the research variables they have created to support open science. We also use a publicly available data set to demonstrate some of the key functions of the platform. We believe this platform can support the use of EHR data for high quality, reproducible research.","Electronic health records. Electronic health records (EHRs) are information systems that collect, store and provide data on healthcare to individuals in digital or electronic format [1]. EHR data are typically collected at the time of healthcare encounters or when services are provided to individuals. These EHR data are collected in multiple care settings and across time and can be linked at the individual level to provide an aggregate description of care for an individual across time and service provider. Historically, these data have been used to support administrative functions such as billing, clinical information sharing and workflow management, but EHRs are increasingly being used to support health and health services research [2, 3]. The data in EHRs typically include basic demographic facts such as date of birth and sex at birth, as well a data on diagnoses, treatments, results of clinical investigations, and outcomes such as hospital discharge or death. Information in EHR is collected at the individual level and using unique patient identifiers it is possible to create trajectories of multiple aspects of care and outcomes over time. These real-world data from EHRs could have enormous value to researchers who are interested in understanding health and disease, in identifying disease trajectories and evaluating disease risks and treatments using modern statistical and machine learning techniques [2] Using electronic health records for research. The process of making EHR data available for research can be divided into two main steps. The first step is the selection and description of the specific variables that the data custodian or owner makes available from the EHRs for research purposes and the second step is the process through which individual research teams select and modify variables that are made available to them. The first step allows the data custodians to select data elements that they feel are sufficiently well characterized and accurate enough to support the scientific research and importantly allows them to explicitly deal with concerns about protecting the privacy of individuals whose person health information is contained in the EHRs [4]. Typically, this aspect limits access to data elements such as date of birth or address that could identify individuals but can be extended to cover opportunities for re-identification through several variables none of which individually is identifying but that can be in combination. This process typically results in the creation of data dictionaries that provide labels and descriptions of the available EHR data elements. The data are often provided in different files from different care settings with different dictionaries. For example, hospital inpatient data in one file and outpatient prescription drug data in another file. The second step in using EHR data for scientific purposes involves selecting the variables from those data sources that are relevant to a specific research project, and creating or deriving research study variables from the available data. This step may vary from research team to research team. For example, a research team focused on breast cancer might focus on using diagnostic codes from hospital inpatient data that identify individuals with different types of breast cancer and then linking to outpatient prescription drug data to identify the specific breast cancer treatments they receive, while a research team looking at emergency room waiting times might focus on data from emergency rooms and define waiting time as the time between registration and assessment. Both teams are drawing from the same publicly available data sources but are drawing on different data elements and creating or deriving research variables from those data elements. An important aspect of multiple research teams making use of common data sources is open science, the notion that science will progress faster if data and knowledge are shared [5]. We have developed a software platform that can support open science in the context of research using publicly available EHR data. The software platform draws on the data dictionaries that are provided to researchers who are accessing EHR data as the input to a process that allows different research teams to use a similar approach to documentation and harmonization of multiple EHR data sources so that the process for data selection and variable derivation can be standardized and shared. EHRs Data Harmonization Platform. The EHRs Data Harmonization Platform is an easy to use publicly available Shiny app that draws on an existing R library: recodeflow. The R library recodeflow was developed as an extension of cchsflow[6, 7] and itself relies on sjmisc [8]. The platform creates shareable documentation of EHR data extraction and derivation that can not only support efforts to make research reproducible, but also will allow researchers to share strategies for data extraction and variable derivation. recodeflow and its components. Doug Manuel and colleagues [6] originally developed cchsflow as a standalone R package and they published it in the Comprehensive R Archive Network (CRAN) [7]. Starting from that specific case focused on Canadian Community Health Survey (CCHS) surveys’ data, the same team developed a general version of this software library, called recodeflow, that is intended for any survey dataset and any EHR dataset. The authors released this R library in CRAN, too [9]. recodeflow package is designed to enhance the reproducibility and standardization of data recoding processes. It leverages two critical components: the “variable details sheet” and the “variable sheet”. The “variable sheet” serves as a repository for essential metadata, including variable labels, which are crucial for accurately interpreting data. This sheet ensures that descriptive information about each variable is consistently applied throughout the analysis, promoting clarity and consistency. The “variable details sheet” also plays a pivotal role in recoding the dataset, as it contains valuable information about variable categories. By harnessing the insights stored in these two spreadsheets, recodeflow empowers researchers to systematically and transparently recode their data, thereby enhancing the rigor and reproducibility of their analytical workflows. We provide a description of essential information stored in each of the spreadsheets along with examples showing how the spreadsheets are filled and should be interpreted in Supplementary Figure S1 and Figure S2. How does the platform utilize recodeflow and contributes in EHRs data harmonization? Doug Manuel and his team built chsflow and recodeflow mainly on sjmisc [10], a popular R package aimed at recoding, dichotomizing or grouping variables, and for general data transformation [8]. sjmisc is available both in CRAN and in Conda [11]. recodeflow performs variable harmonization and derivation, but does not provide a user interface: only software developers proficient in R can use it effectively. Our platform not only helps in creating the above-mentioned spreadsheets in a user-friendly environment, but also gives the opportunity to users to implement the recoding process on their datasets by taking simple steps. It also documents all essential information (such as the functions’ codes to create the derived variables and their names) and therefore, other researchers can reproduce an already existing work by only uploading the required documentation to the app. To be more specific, non-recoded data can be imported to the app with various format such as CSV, SAS7BDAT, RDS, and SQLite. There are also options to handle large datasets to be imported to the app in smaller chunks. Users can create a details sheet from scratch using the basic transformations available in recodeflow (for example, renaming a variable, creating a categories out of a continuous variable, etc.) or creating more complicated derived variables that has more than one components and needs functions to be coded. The platform then uses the information stored in the created spreadsheets to perform the curation on the dataset. The advantage of this standard approach is that once other users want to perform the same curation on a dataset, they don’t need to create everything from scratch. These spreadsheets could be shared with other users, and they can upload them to the platform, modify them if needed, connect their non-curated database and reproduce the same curation on their database. The platform gives the flexibility to the users to save the curated database in various formats. In line with the principles of open science [12], a range of R software libraries and programs are available for data harmonization. retroharmonize, specializes in survey data harmonization by creating a reusable metadata object that supports the generation of data dictionary, amongst other applications[13, 14]. DataHarmonizer is an online platform that was created for genomics data but it can be used for other data. DataHarmonizer [15] builds on a new emerging open data model, LinkML that supports ontologies and transformations to other databases [16]. dxpr integrates and preprocessing electronic health records, focusing on diagnostic and procedure codes [17]. Our EHR Data Harmonization Platform aims to make data transformation as easy as possible with a web-based interface to facilitate tasks that are common for all three applications, including data loading, common transformations, facilitating the generation and organization of R code for new, more complex derived transformation, and display of summary data and metadata. Other software programs aimed at harmonizing EHRs data exist [18, 19, 12], but none of them has become a standard tool in the medical environments worldwide. This study. We organize the rest of this manuscript as follows. After this Introduction, we first provide a case study of the use of the platform to support multiple research teams accessing a new EHR data resource for COVID-19 pandemic research (section 2). We then use a publicly available data set to demonstrate specific functions of the platform (section 3). Finally, we outline a discussion and some conclusions in the section (section 4)."
https://arxiv.org/html/2411.08696v1,Scholarly Wikidata: Population and Exploration of Conference Data in Wikidata using LLMs,"Several initiatives have been undertaken to conceptually model the domain of scholarly data using ontologies and to create respective Knowledge Graphs. Yet, the full potential seems unleashed, as automated means for automatic population of said ontologies are lacking, and respective initiatives from the Semantic Web community are not necessarily connected: we propose to make scholarly data more sustainably accessible by leveraging Wikidata’s infrastructure and automating its population in a sustainable manner through LLMs by tapping into unstructured sources like conference Web sites and proceedings texts as well as already existing structured conference datasets. While an initial analysis shows that Semantic Web conferences are only minimally represented in Wikidata, we argue that our methodology can help to populate, evolve and maintain scholarly data as a community within Wikidata. Our main contributions include (a) an analysis of ontologies for representing scholarly data to identify gaps and relevant entities/properties in Wikidata, (b) semi-automated extraction – requiring (minimal) manual validation – of conference metadata (e.g., acceptance rates, organizer roles, programme committee members, best paper awards, keynotes, and sponsors) from websites and proceedings texts using LLMs. Finally, we discuss (c) extensions to visualization tools in the Wikidata context for data exploration of the generated scholarly data. Our study focuses on data from 105 Semantic Web-related conferences and extends/adds more than 6000 entities in Wikidata. It is important to note that the method can be more generally applicable beyond Semantic Web-related conferences for enhancing Wikidata’s utility as a comprehensive scholarly resource. Source Repository: https://github.com/scholarly-wikidata/ DOI: https://doi.org/10.5281/zenodo.10989709 License: Creative Commons CC0 (Data), MIT (Code)","Scientific conferences are vital for researchers to share their research findings and advancements. It offers an opportunity to discuss research problems or limitations, a platform for networking with peers, and a platform for promoting collaboration, which is essential for learning, innovation, and problem-solving. Because of the importance of scientific conferences, we have seen tremendous growth in the number of conferences over the years [1]. For example, IEEE (Institute of Electrical and Electronics Engineers) sponsors more than 2,000111https://www.ieee.org/about/at-a-glance.html conferences and events annually. Similarly, ACM (Association for Computing Machinery) hosts more than 170222https://www.acm.org/conferences/about-conferences conferences annually worldwide. Therefore, efforts have been made to capture metadata about scientific events [2, 3, 1, 4] in a linked-data format as they provide valuable information. Such data can be used for (i) better understanding the progress of science overall, (ii) the evolution of particular research topics (or fields), (iii) understanding research impact (e.g. by sponsors’ interest) over time, etc. The availability of scholarly metadata enables scientometrics [5], or practical tools such as recommending relevant conferences or papers to readers [2] for navigating through the fastly growing scientific output which is becoming time-consuming and almost impractical. However, as much as the benefits these metadata about scientific events provide, there exist challenges. The primary obstacle is the collection of large-scale metadata, which is nontrivial in nature [2]. Similarly, the sustainability, which is also the focus of this paper, of the accumulated metadata constitutes the second and most significant obstacle. If the data collected is not sustainable, it may be lost over time, resulting in the loss of valuable information and efforts put into data collection. For instance, the Microsoft Academic Graph, which contained over 8 billion triples [2] with information about scientific publications and related data, was retired in December 2021333https://www.microsoft.com/en-us/research/project/microsoft-academic-graph/. While the effort was somewhat continued shortly later in OpenAlex444https://openalex.org/, the case demonstrates sustainability issues in individual or commercial scholarly KG offerings. We argue that collaborative, general purpuse, community-driven platforms, such as Wikipedia, are generally more sustainable than such fragmented efforts: community participation is motivated by intrinsic factors, fostering a sense of belonging to the group [6]. Notably, commercial initiaves seem to recognize this, as shown by Google’s declaration that it will cease operations on Freebase and transfer its content to Wikidata [7]. Wikidata, which focuses on knowledge graphs (KGs), is a sister project of Wikipedia and another example of a community-driven platform [8, 9]. Wikidata currently has more 110M entities and 25K active contributors555https://www.wikidata.org/wiki/Wikidata:Statistics. By bringing Scholarly data about scientific conferences into Wikidata, they can be seamlessly integrated with existing background knowledge through SPARQL queries. Furthermore, Wikidata benefits from a robust tooling ecosystem and widely used libraries, including entity linkers, search tools, SPARQL endpoint with high-availability, easy-to-use query editor, visualization tools, and more [10, 11]. Wikidata also allows non-expert users to directly access the KGs through search and Web UI (user interface). Therefore, the primary objective of our work is to integrate scientific conference metadata into Wikidata, a community-led platform. After conducting an analysis of Wikidata entities related to Semantic Web conferences such as International Semantic Web Conference (ISWC), Extended/European Semantic Web Conference (ESWC), International Conference Knowledge Engineering and Knowledge Management (EKAW), International Conference on Knowledge Capture (K-CAP), SEMANTiCS, and Knowledge Graph and Semantic Web Conference (KGSWC), it was noticed that some conferences were missing and the ones that were present had only minimal information. In this project, we have extended Wikidata to include a more comprehensive set of information (e.g. see ISWC 2023666https://www.wikidata.org/wiki/Q119153957 (Q119153957)). Within the scope of this work, we focused on the Semantic Web conferences but our method is more generally applicable and can be extended to other conference series. We note that 105 conferences we added to, updated in Wikidata is higher than the comparable related work such as Scholarly Data (35 confs)777https://bit.ly/3Vs6XNc, ORKG (5 confs) 888https://orkg.org/organizations/Event as of July, 2024. Large language models (LLMs) have proven their language understanding capabilities with many NLP benchmarks [12]. In recent years, approaches such as in-context learning with a few-shot example have allowed them to perform many tasks such as relation or fact extraction [13, 14]. Such models can be used to easily extract information from sources with natural language text, such as conference proceedings, websites, or call for papers. Nevertheless, their output can be prone to errors. In our work, LLMs are used to extract data, which is then verified by a human-in-the-loop validation to eliminate any noisy extraction and ensure accuracy. In particular, this paper makes the following contributions. • We analysed existing ontologies for representing scholarly data and mapped them to Wikidata to identify relevant Wikidata entities/properties as well as gaps. • We present a method for utilizing large language models to efficiently extract conference metadata from various sources, curating them through a human-in-the-loop validation process using OpenRefine, and populating the data in Wikidata via Wikidata QuickStatements and provide an evaluation for LLM-based extractions. • As a result of this project, we have extended over 1000 existing entities and created more than 5,000 new entities, including conferences, scientific articles, and people. These entities are now available on Wikidata and can be accessed via the Web UI or SPARQL endpoint. • We extend visualization tools Scholia999https://scholia.toolforge.org/ and Synia101010https://synia.toolforge.org to better visualize the information we added to Wikidata."
https://arxiv.org/html/2411.08450v1,"DecentPeeR: A Self-Incentivised & Inclusive 
Decentralized Peer Review System","Peer review, as a widely used practice to ensure the quality and integrity of publications, lacks a well-defined and common mechanism to self-incentivize virtuous behavior across all the conferences and journals. This is because information about reviewer efforts and author feedback typically remains local to a single venue, while the same group of authors and reviewers participate in the publication process across many venues. Previous attempts to incentivize the reviewing process assume that the quality of reviews and papers authored correlate for the same person, or they assume that the reviewers can receive physical rewards for their work. In this paper, we aim to keep track of reviewing and authoring efforts by users (who review and author) across different venues while ensuring self-incentivization.To this end, we introduce DecentPeeR, a system that captures the interactions of users who use a peer review system as decentralized reputation scores. We show that our system incentivizes reviewers to behave according to the rules, i.e., it has a unique Nash equilibrium in which virtuous behavior is rewarded. Furthermore, we detail how our design ensures inclusivity, i.e., giving everyone a fair chance to publish, especially when facing dishonest users. We also report on empirical results that show the incentive mechanism works: dishonest individual and group behavior are penalized, but it is possible to recover from a poor score over time.","Peer review systems are widely used and have an extensive impact in today’s academia. Use cases of peer-review ranges from the scientific publication process [1, 2] to open source software development [3, 4]. With the popularity of peer review systems, various methods have been proposed to make the peer review procedure more inclusive. With inclusivity, authors have a chance to publish their work solely based on its quality. Ensuring inclusivity is challenging in the academic peer review process: submissions on different research topics may not be comparable; reviewers may have personal opinions depending on the topic of the submission; due to large amounts of published papers, evaluations from only few reviewers can be used decide on the quality of a submission. Previous efforts to ensure inclusivity range from enforcing prior-announcement of conflict-of-interest [5, 6], double-blindness [7, 8], and actions from the editor to promote quality, integrity, and fairness [9]. Most traditional solutions are focused on how to make a single conference more inclusive. However, authors and reviewers likely take part at multiple venues over their careers. Thus, a cross-venue measure would be more viable today, given the advancements in decentralized technologies. In this work, we keep track of the actions of users over time using a reputation system to ensure inclusivity. We build a decentralized system where users tend to follow the rules of the system based on their best interests. While incentivizing users who behave rationally, the system should not punish academic work of good quality and thus violate inclusivity. To this end, we develop a self-incentivized system based on a game theoretical approach, showing that achieving the unique Nash equilibrium is only possible by adhering to the rules of the system. We detail our system, DecentPeeR, from the perspective of an academic who wants to contribute to or organize a conference. In doing so, we also benefit from the decentralized storage mechanisms provided by blockchain technology [10, 11] that is used to keep a history of peer review systems’ data across different venues. We detail our system, DecentPeeR, from the perspective of an academic who wants to contribute to or organize a conference. In doing so, we also benefit from the decentralized mechanisms provided by blockchain technology (e.g., to keep a history of peer review systems’ data across different venues). I-A Design Goals Our aim is to design our peer-review system that satisfies the following main goals: Self-incentivization. Typically, peer review systems assume that participants are well-behaved [12], that reviewers can be assigned a reliability score [13], or that at most a small percentage of reviewers is biased [14, 15]. However, these rules might be neglected, resulting to the raise of adversarial reviews [16]. As a remedy, we aim to ensure that it is in the reviewer’s own best interest to respect rules of the system, i.e., through a provable self-incentivization. Inclusivity. A reputation system should be flexible and adaptive and in particular also support new users of the system. Our goal is to design a system where everyone would have a chance to contribute good quality work, mostly independent of their scores, and where users with bad scores have a chance to recover from a failure. Cross-venue evaluation. Academic users do not encounter peer-review systems only once: they participate in multiple venues (conferences, journals, workshops, etc.) throughout several years. Hence we aim to design a system that benefits from this fact and also aggregates data over time. I-B Contribution In this paper, we design a self-incentivized and inclusive peer review system, DecentPeeR, that works across venues and tracks. Honest reviewing behavior is rewarded with a positive influence on future borderline-scored submissions by the authors. In addition, the reviewing score provides committee chairs with a criterion to select the committee. We present a peer review game in this paper and show that it has a unique Nash equilibrium where the users play honestly. We then analyze the desired properties of the peer review game, such as inclusivity and cross-venue evaluation. Our method incorporates three mechanisms that lead to a high level of inclusivity: • Our system only considers the reputation score in borderline cases: if the high quality of a paper is already agreed upon, we consider that paper as accepted. • Our system uses a randomness mechanism to form a program committee and a reviewing team: the weighted randomness ensures that selected users can be trusted while giving chance to every user to participate. • A reputation score function has been implemented with the goal of ensuring fast recovery for users who have limited misbehavior. We show that the cross-venue aspect allows us to quickly detect adversarial behavior by reviewers. Once an adversarial reviewer however behaves correctly again, the corresponding score of the reviewer recovers. We conclude by showing that a majority attack, where adversarial reviewers collaborate in order to evaluate the paper dishonestly, is unlikely under the uniformly chosen reviewers in the review assignment. Our paper is organized as follows: we follow by relating our paper to previous works. We then detail DecentPeeR design in §II and analyze it in §III. Finally, we go over a few case studies in §IV and conclude our work in §V. I-C Related Work Peer review is a broad research topic that has been investigated from many aspects over the past years. For example, empirical studies have been conducted on peer review for classroom use [17, 18], for conference reviews [19, 20], and for funding applications [21]. In the following, we discuss different perspectives on peer review systems, coming from theoretical research as well as practical systems. Blockchain-based peer review. Our peer review system can be implemented on a blockchain-based system that supports smart contracts, like Ethereum. The decentralized nature of blockchain-based protocols has previously motivated many researchers to utilize its advantages for new designs of peer review systems. Our work is also a building block towards a Decentralized Science (DeSci) future [22]. Initial proposals for an alternative blockchain-based peer review systems [23, 24] focus on providing an alternative coin instead of Bitcoin. These attempts to alter a financial system for peer review, however, are inherently flawed: wealthy participants can game the system to their benefit. Another set of blockchain-based peer review systems focused on providing a decentralized platform to store information exchanged during a peer review process [25, 10], mostly leveraging the advantages provided by IPFS. Although it is possible to use the proposed peer review systems across venues, the systems lack the essential requirements that ensure fair treatment of all users when being used across venues. More recent systems [26, 27] aimed to tackle the challenge of a collaborative system. In doing so, they showed what are possible ways to provide self-invitation based on a game-theoretic perspective. In a nutshell, they showed that the allowed rules of the game are in the best interest of all users. However, they did not show what happens when a failure happens and how the system could recover. Furthermore, they did not consider the fact that not only a single venue exists, and a system should not be restarted whenever a new request for a venue appears. We summarize the main properties of the presented distributed peer review systems in Table I. System Cross-venue Self-incentivising Inclusive PubChain [24] ✓ ✗ ✗ Blockchain and Kudos [23] ✓ ✗ ✗ IPFS-based [25] ✓ ✗ ✗ Data Marketplaces [26] ✗ ✓ ✗ Collaborative Research [27] ✗ ✓ ✗ DecentPeeR (our work) ✓ ✓ ✓ TABLE I: Comparing DecentPeeR with previous decentralized peer review systems. Social choice perspective on peer review. In social choice studies, peer review has been investigated as an assessment method for grading homework and exams[28], programming classes[29], and conferences[30]. Some of these works focus on finding the right aggregation rules, by investigating cardinal voting rules instead of ordinal grading [31, 32] or by showing specific properties of peer review protocols, such as strategyproofness [30]. Other papers focus on determining truthful reviewers by assuming that the grading is performed with assistance [33]. There are also studies on reputation-based peer grading. Leaning on PageRank, Walsh proposed a PeerRank system [34]. In this system, each reviewer gets a reputation score, which is computed by the grades given by the reviewer weighted by the reputation score of the reviewer. Note that a reputation score calculation is connected to the grade of the submitted work, which is not necessarily true in conference reviewing. Reputation systems. Reputation systems are one of the key tools to establish trust in an untrustworthy environment [35]. For example, they have an established place in designing distributed systems, especially in connecting peers in a peer-to-peer network [36]. They have also been considered in other applications such as e-commerce [37] and transportation [38]. Most of the previous work that considers a reputation system on blockchain [39, 40] uses the reputation score as an alternative mining protocol, which is orthogonal to how we used it in our system. Similarity detection mechanisms. The digital age has made it easier to reuse the efforts of others, and hence, from early on, it was critical to create similarity detection mechanisms [41]. Also, for the peer review process similarity detection is inevitable, as it enables editors to verify the integrity of a research work. With the rise of generative AI tools, advanced similarity detection mechanisms received even more attention, especially because it became possible to rehash ideas of others into undetectable results (even by trained eyes) [42]. To detect such behavior, context-oblivious methods have been proposed, such as fingerprinting [43], text matching [44], or compression [45]. The latter one can run fast but might have low accuracy due to false negatives. In general, context-aware similarity detection approaches allow us to reduce false negatives, but they require much more computational power. As an alternative, latent semantic analysis [46] or transformer-based tools [47] have been considered in the literature. In this work, we do not focus on a particular similarity detection mechanism, but rather assume that such methods exist and can be deployed by the system designer."
https://arxiv.org/html/2411.07589v1,Overhead-free User-side Recommender Systems,"Traditionally, recommendation algorithms have been designed for service developers. But recently, a new paradigm called user-side recommender systems has been proposed. User-side recommender systems are built and used by end users, in sharp contrast to traditional provider-side recommender systems. Even if the official recommender system offered by the provider is not fair, end users can create and enjoy their own user-side recommender systems by themselves. Although the concept of user-side recommender systems is attractive, the problem is they require tremendous communication costs between the user and the official system. Even the most efficient user-side recommender systems require about 5\times more costs than provider-side recommender systems. Such high costs hinder the adoption of user-side recommender systems. In this paper, we propose overhead-free user-side recommender systems, RecCycle, which realizes user-side recommender systems without any communication overhead. The main idea of RecCycle is to recycle past recommendation results offered by the provider’s recommender systems. The ingredients of RecCycle can be retrieved “for free,” and it greatly reduces the cost of user-side recommendations. In the experiments, we confirm that RecCycle performs as well as state-of-the-art user-side recommendation algorithms while RecCycle reduces costs significantly.","Recommender systems have been used in many web services (Linden et al., 2003; Geyik et al., 2019). It was estimated that 35 % of purchases on Amazon and 75 % of watches on Netflix came from recommender systems (MacKenzie et al., 2013). Recommender systems are indispensable both for businesses and users. Although traditional recommender systems aim only at conversion, many fine-grained demands for recommender systems have emerged. Users may want to receive fair recommendations (Kamishima et al., 2012a; Biega et al., 2018; Milano et al., 2020) or serendipitous recommendations (Chen et al., 2021; Anderson et al., 2020; Steck, 2018; Mladenov et al., 2020; Zheng et al., 2018), or users may want recommender systems to be transparent (Sinha and Swearingen, 2002; Balog et al., 2019) and steerable (Green et al., 2009; Balog et al., 2019). For example, on LinkedIn, recruiters may want to receive account recommendations that are fair in terms of gender and race to avoid (implicit) discrimination. A citizen who gathers information for election may want to receive both Republican and Democrat news equitably to avoid filter bubbles (Pariser, 2011). Cinema enthusiasts may want to receive recommendations that involve minor movies instead of popular movies that enthusiasts already know. However, there are too many kinds of demands, and the service provider cannot cope with all of them. Besides, service provider may not implement such functionalities on purpuse. For example, some service providers may intentionally choose to increase short-term conversions instead of caring the fairness of the platform. If the service provider does not implement fair recommender systems, users are forced to use unfair ones or quit the service. It has been considered that users have little ability to change the recommendations. In most cases, the only option available to the user is to wait until the service implements the functionality. Green et al. (2009) also pointed out that “If users are unsatisfied with the recommendations generated by a particular system, often their only way to change how recommendations are generated in the future is to provide thumbs-up or thumbs-down ratings to the system.” User-side recommender systems (Sato, 2022b) offer a proactive solution to this problem. Users can build their own (i.e., private, personal, or user-side) recommender systems to ensure recommendations are made fairly and transparently. Since the system is built by the user, it can be customized to meet specific criteria they want and add the functionalities they want. User-side recommender systems realize ultimate personalization. Table 1. Properties of user-side recommender systems. The definitions of these properties are shown in Section 4.4. Postprocessing (PP) applies postprocessing directly to the official recommender system, which is not sound when the list does not contain some sensitive groups (See also Section 4.1). PP PrivateRank (Sato, 2022b) PrivateWalk (Sato, 2022b) ETP (Sato, 2022d) Consul (Sato, 2022d) RecCycle (ours) Consistent ✓ ✓ ✗ ✓ ✓ ✓ Sound ✗ ✓ ✓ ✓ ✓ ✓ Local ✓ ✗ ✓ ✗ ✓ ✓ Overhead-free ✓ ✗ ✗ ✗ ✗ ✓ The concept of user-side recommender systems looks similar to steerable (Green et al., 2009) (or scrutable (Balog et al., 2019)) recommender systems at first glance. Steerable recommender systems also allow users to control the recommendation results. However, the key difference is that steerable systems are implemented by the service provider, while user-side recommender systems are built by the users themselves. What to steel is chosen by the service provider in traditional steerable recommender systems. If the recommender system in use is not steerable in the way they want, users cannot enjoy steerability and must wait for the service provider to implement it. By contrast, user-side recommender systems allow users to make the system steerable, even if the service provider implemented only a standard non-steerable system. Although user-side recommender systems are attractive, building them is challenging. End users do not have access to the data stored in the service’s database, unlike the developers employed by the service provider. Most modern recommender systems rely on user log data and/or item features to make recommendations. At first glance, it seems impossible to build an effective recommender system without such data. Sato (2022b) addressed this problem by using the official recommender systems provided by the target web service. Although the official recommender systems are black-box and possibly unfair, Sato’s methods turn them into fair and transparent ones on the user’s side by combining multiple outputs. However, existing user-side recommender systems issue multiple queries to the official (possibly unfair) recommender system to build a single (fair) recommendation list. In other words, these methods trade communication costs with fairness. The drawback of this approach is the communication cost. Even the most efficient user-side recommender systems, Consul, require 5 queries to build a recommendation list (Sato, 2022d). This means that Consul loads the service 5 times more. Such a high communication cost causes problems. First, the service provider may disfavor and prohibit such users’ activities to mitigate the load on the service. Second, end users cannot afford to pay the high API cost. Third, such systems are not suitable for real-time applications due to the response time of the multiple queries. We advocate that the communication cost between the end user and the service is crucial for effective user-side recommender systems. An ideal user-side system works as if it were an official system. The recommendation list should be shown to the user at the same time as the official system. However, existing user-side recommender systems require additional queries and thus require more loading time than the official system, which leads to a poor user experience. We propose overhead-free user-side recommender systems, RecCycle (recommendation + recycle), to address this problem. The main idea of RecCycle is to recycle past recommendation results presented by the provider’s recommender systems when the user uses the system as usual. These recommendation results used to be discarded once shown on the page. Sometimes, these recommendations are just shown on the page and do not catch the attention of the user due to the position and/or timing of the presentation. RecCycle “recycles” these information to create new recommendations on the user’s side. These information can be used “for free”, i.e., without any additional communication cost. All of the computation for RecCycle is done locally. RecCycle is so communication efficient that it can realize real-time user-side recommendations, and the user can enjoy the recommendations as if they were shown by the official system. RecCycle can be combined with existing user-side recommender systems. We will elaborate on the premise of RecCycle in the following sections. As a special case, we show that RecCycle can be combined with Consul (Sato, 2022d), which leads to consistent, sound, local, and overhead-free user-side recommender systems (Table 1). In the experiments, we confirm that RecCycle performs as well as state-of-the-art user-side recommendation algorithms while RecCycle reduces costs significantly. The contributions of this study are as follows: • We propose overhead-free user-side recommender systems, RecCycle, for the first time. • We show that RecCycle is consistent, sound, and local, as well as overhead-free. • We empirically validate that RecCycle performs as well as state-of-the-art user-side recommendation algorithms while RecCycle reduces costs significantly. • We deploy RecCycle in a real-world X (Twitter) environment and confirm that users can realize their own recommender system with specified functionalities they call for using RecCycle."
https://arxiv.org/html/2411.06282v1,Two scholarly publishing cultures? Open access drives a divergence in European academic publishing practices,"The current system of scholarly publishing is often criticized for being slow, expensive, and not transparent. The rise of open access publishing as part of open science tenets, promoting transparency and collaboration, together with calls for research assesment reforms are the results of these criticisms. The emergence of new open access publishers presents a unique opportunity to empirically test how universities and countries respond to shifts in the academic publishing landscape. These new actors challenge traditional publishing models, offering faster review times and broader accessibility, which could influence strategic publishing decisions.Our findings reveal a clear division in European publishing practices, with countries clustering into two groups distinguished by the ratio of publications in new open access journals with accelerated review times versus legacy journals. This divide underscores a broader shift in academic culture, highlighting new open access publishing venues as a strategic factor influencing national and institutional publishing practices, with significant implications for research accessibility and collaboration across Europe.","One of the mainstays of evaluating the performance of universities is their performance in research, and a major plank of that evaluation is constituted by publication in academic journals. Likewise, the metrics-based evaluation of individual academics follows similar processes shaped by pressures to publish in high-impact journals. Researchers may choose open access venues to increase visibility and compliance with open science mandates, while universities and national science systems might adapt their evaluation criteria, balancing prestige with the growing importance of transparency and public access. This dynamic provides fertile ground for studying how institutions adjust their incentives and how these shifts affect researchers publishing strategies. Researchers often prioritize journal prestige and citation counts, sometimes at the cost of research quality and broader societal impact. Institutional policies and national funding systems frequently reward publication volume and impact factor, reinforcing this trend. Recent studies show a growing disconnect between researchers’ values, which may favor openness and integrity, and institutional incentives focused on metrics. Performance-based funding models and evolving open science practices reveal how current systems reshape research behavior, raising concerns about the sustainability and ethics of research evaluation. This paper examines the impact of these forces on publishing practices and researcher behavior. We have previously discussed the controversies involved in using publication metrics in academic journals to evaluate individual academics [1] and many of those issues applying to the use of publication in academic journals apply to the evaluation of universities. Building on the pressures associated with metrics-based evaluations, recent studies suggest that the dominance of journal impact factors and the resulting “publish or perish” culture may further undermine research quality. Bohorquez et al. [2] found that pressure to publish in prestigious journals can lead researchers to adjust findings to fit publication standards, often at the expense of comprehensive evidence. Researchers under significant pressure may prioritize journal prestige and rapid publication timelines over open-access and transparency considerations, as Johann et al. [3] describe, opting for instrumental rather than normative publication strategies. Furthermore, Ross-Hellauer et al. [4] examined the phenomenon of “value dissonance,” where researchers’ commitment to open and responsible research increasingly conflicts with institutional demands for high-impact publications, favoring citation metrics over collaborative and ethical practices. Additionally, Baccini et al. [5] showed how bibliometric-driven evaluations encourage behaviors like self-citations and strategic citation practices, ultimately fostering a citation-centric approach that may detract from genuine scientific impact. These findings underscore the need to reassess research assessment policies, highlighting the growing gap between institutional metrics and researchers’ values, as well as the importance of aligning evaluation criteria with the principles of open science and research integrity. The interplay between country-level research reforms and researchers’ publishing choices was highlighted in studies by Cernat [6] and Dagienė et al. [7], revealing how policy-driven metrics reshape academic behavior. Cernat’s analysis of Romania’s 2016 reforms, which imposed strict publication criteria amidst funding cuts, led to a focus on high-impact journals at the expense of conference proceedings, ultimately reducing overall research productivity. This case exemplifies the misalignment between top-down policy intentions and researchers’ capabilities under constrained resources. Similarly, Dagienė et al.’s study on Lithuania’s performance-based funding system shows how the push for indexed journal publications has spurred strategic publishing behaviors, emphasizing quantity over quality. These cases illustrate the influence of diverse stakeholders—scientific elites, policymakers, universities, and researchers—in shaping research assessment policies, raising concerns about the sustainability and genuine innovation fostered by metrics-driven funding. Here, we investigate the evolution of academic publishing practices in Europe by analyzing the publishing data on universitiy and country level in the European Union, focusing on the ratio between publications in new, open access journals (MDPI in our case), and those in traditional, legacy journals (here, The Big Five). By examining the distributions of this ratio, we identify two distinct groups of universities and countries with different scholarly publishing cultures. We show how publishing choice correlates with innovation potential and corruption perception at the country level, revealing the broader socio-economic context that shapes academic publishing practices. Our findings reveal significant insights into the current state and evolving trends of scholarly publishing practices among universities and EU countries."
