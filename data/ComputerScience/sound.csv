URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.09943v1,Zero-shot Voice Conversion with Diffusion Transformers,"Zero-shot voice conversion aims to transform a source speech utterance to match the timbre of a reference speech from an unseen speaker. Traditional approaches struggle with timbre leakage, insufficient timbre representation, and mismatches between training and inference tasks. We propose Seed-VC, a novel framework that addresses these issues by introducing an external timbre shifter during training to perturb the source speech timbre, mitigating leakage and aligning training with inference. Additionally, we employ a diffusion transformer that leverages the entire reference speech context, capturing fine-grained timbre features through in-context learning. Experiments demonstrate that Seed-VC outperforms strong baselines like OpenVoice and CosyVoice, achieving higher speaker similarity and lower word error rates in zero-shot voice conversion tasks. We further extend our approach to zero-shot singing voice conversion by incorporating fundamental frequency (F0) conditioning, resulting in comparative performance to current state-of-the-art methods. Our findings highlight the effectiveness of Seed-VC in overcoming core challenges, paving the way for more accurate and versatile voice conversion systems. Code and pretrained models have been release on https://github.com/Plachtaa/seed-vc","Voice conversion (VC) technology aims to transform speech from a source speaker to sound as if it were spoken by a target speaker while preserving the original linguistic content. Applications of VC span personalized speech synthesis, dubbing in film and television, and assistance for individuals with speech impairments. Traditional VC methods often require extensive recordings from target speakers, limiting their scalability and practicality. Zero-shot voice conversion seeks to overcome these limitations by enabling models to convert speech to match the timbre of any unseen speaker, given only a short reference utterance. This capability is crucial for creating flexible and generalizable VC systems that can operate in real-world scenarios with a vast diversity of speakers. Despite significant advancements, zero-shot VC faces several critical challenges. First, timbre leakage remains a persistent issue. Most existing models [13][14][15][16] extract speech content from the source utterance using methods like self-supervised learning (SSL) models (e.g., HuBERT, wav2vec)[1][2], automatic speech recognition (ASR) models[18], or phoneme posteriorgrams (PPG)[17]. However, these extracted content features often retain residual timbre information from the source speaker, leading to timbre leakage in the converted speech. This leakage causes the converted speech to not fully resemble the timbre of the reference speaker. To mitigate this, some approaches introduce information bottlenecks [4] to filter out timbre information, such as k-means clustering [19] on content features or content retrieval [6] based on vector distances. While these methods reduce timbre leakage, they also remove essential content details, resulting in higher word error rates (WER) in the conversion results. This trade-off between speaker similarity and intelligibility is a significant hurdle in zero-shot VC. Second, insufficient timbre representation poses a challenge. Many prior works represent timbre using a single vector extracted from the reference speech [4][20]. While this may suffice in non-zero-shot scenarios with a limited number of known speakers—where the model parameters can implicitly capture detailed speaker characteristics—it proves inadequate for zero-shot conditions. In zero-shot VC, the model must generalize to unseen speakers with potentially vast differences in vocal attributes. A single-vector representation fails to capture the fine-grained nuances of a speaker’s timbre, leading to suboptimal conversion quality. Third, there is an inconsistency between training and inference tasks. Typically, VC models are trained to reconstruct the source speech using its own content and timbre features. During inference, however, the task shifts to synthesizing speech that combines content from the source utterance with timbre from a different reference speaker. This discrepancy between the training objective and the inference task can degrade performance, as the model is not optimized for the actual conditions encountered during deployment. To address these challenges, we propose Seed-VC, a novel zero-shot voice conversion framework that introduces two key innovations. First, we employ an external timbre shifter during training to perturb the timbre of the source speech. The timbre shifter can be a non-perfect zero-shot VC model or a semantic-to-acoustic model from an existing text-to-speech (TTS) system. By transforming the source speech into a timbre-shifted version, we obtain an alternative utterance from which we extract the content features. This process ensures that the content extractor operates on speech that does not carry the original timbre, effectively reducing timbre leakage. Moreover, this strategy aligns the training procedure with the inference scenario, where content and timbre come from different speakers. Second, we utilize a diffusion transformer [21] architecture capable of leveraging the entire reference speech as context rather than relying on a single timbre vector. The transformer’s in-context learning ability allows the model to capture more detailed and nuanced timbre information from the reference speech. In-context learning ability of transformers has been proved to largely increase speaker similarity in many zero-shot TTS models[22][23][24][25], but seldom used in VC task. By incorporating the full reference utterance, the model can learn fine-grained speaker characteristics essential for high-quality zero-shot conversion. Our experimental results demonstrate that Seed-VC significantly outperforms strong baselines like OpenVoice [26] and CosyVoice[27], achieving higher speaker similarity and lower WER in zero-shot voice conversion tasks. We also compare our model’s zero-shot performance with previous non-zero-shot models on several speakers, achieving equivalent or superior speaker similarity with much lower WER. Additionally, we extend our framework to zero-shot singing voice conversion by incorporating fundamental frequency (F0) conditioning into the model inputs. The results show high speaker similarity and low WER while maintaining high F0 correlation and low F0 root mean square error (RMSE), as well as high DNSMOS scores, indicating naturalness and intelligibility. In summary, our contributions are as follows: • We propose Seed-VC, a novel framework that addresses timbre leakage and training-inference inconsistency by using an external timbre shifter during training. • We enhance timbre representation through a diffusion transformer that utilizes the entire reference speech context, capturing fine-grained speaker characteristics crucial for zero-shot VC. • We demonstrate superior performance over existing state-of-the-art models in both zero-shot voice conversion and zero-shot singing voice conversion tasks. • We provide insights through ablation studies on the impact of the timbre shifter, the use of full reference speech context, and different timbre shifter methods. Our work advances the field of zero-shot voice conversion by addressing core challenges and providing a more effective and generalizable approach. We believe that Seed-VC paves the way for more accurate and versatile voice conversion systems applicable to a wide range of real-world scenarios."
https://arxiv.org/html/2411.10034v1,EveGuard: Defeating Vibration-based Side-Channel Eavesdropping with Audio Adversarial Perturbations,"Vibrometry-based side channels pose a significant privacy risk, exploiting sensors like mmWave radars, light sensors, and accelerometers to detect vibrations from sound sources or proximate objects, enabling speech eavesdropping. Despite various proposed defenses, these involve costly hardware solutions with inherent physical limitations. This paper presents EveGuard, a software-driven defense framework that creates adversarial audio, protecting voice privacy from side channels without compromising human perception. We leverage the distinct sensing capabilities of side channels and traditional microphones—where side channels capture vibrations and microphones record changes in air pressure, resulting in different frequency responses. EveGuard first proposes a perturbation generator model (PGM) that effectively suppresses sensor-based eavesdropping while maintaining high audio quality. Second, to enable end-to-end training of PGM, we introduce a new domain translation task called Eve-GAN for inferring an eavesdropped signal from a given audio. We further apply few-shot learning to mitigate the data collection overhead for Eve-GAN training. Our extensive experiments show that EveGuard achieves a protection rate of more than 97\% from audio classifiers and significantly hinders eavesdropped audio reconstruction. We further validate the performance of EveGuard across three adaptive attack mechanisms. We have conducted a user study to verify the perceptual quality of our perturbed audio.","Loudspeakers are omnipresent in today’s technology-based society. Their use extends beyond facilitating phone calls and video conferencing for the exchange of private information. They have been widely integrated into intelligent mobile and IoT devices, enhancing human-machine interaction through speech recognition. The associated use cases are anticipated to reach a market size of \$150.68 billion by 2032 [57, 47]. As people increasingly rely on loudspeaker-equipped devices, voice privacy is becoming increasingly important. Unfortunately, the diverse sensors in intelligent devices are imposing an alarming risk to voice privacy. Although these sensors are not originally designed for voice recording, they can be repurposed by adversaries to serve as side channels to capture voice-induced vibrations, thereby facilitating unauthorized eavesdropping. For example, the prevalent accelerometers on smartphones have been exploited to eavesdrop on voice playout [58, 22]. Millimeter-wave (mmWave) radars can remotely detect vibrations from sound sources and recover speech signals through walls [20, 21, 54, 62, 61, 77]. Such side-channel speech eavesdropping attacks (SSEAs) lead to severe individual privacy breaches [1] and may compromise sensitive organizational intellectual property [12]. Figure 1: Overview of EveGuard, inserting imperceptible adversarial perturbations to the target speech to protect users’ voice communication from multi-sensor eavesdropping attacks. Existing research has devised hardware-based defenses against SSEAs. For instance, jamming-based methods [28, 59, 71] can block adversarial mmWave SSEAs. However, they may degrade the sensing function of legitimate mmWave devices. Moreover, jamming is generally prohibited in non-military applications [13]. Intelligent reflecting surface (IRS) has also been used as a security shield [53, 56], yet it can only protect its immediate vicinity. As for defending against accelerometer-based SSEAs, vibration motors have been used to generate low-amplitude vibrations that disrupt eavesdropping [76]. However, this method may cause user discomfort and hasten the depletion of smartphone batteries. We propose EveGuard, an innovative software-based defense mechanism to protect against privacy leakage from the loudspeaker-generated voice in SSEAs. As shown in Figure 1, EveGuard mitigates SSEA threats by introducing audio adversarial examples to the original audio signals prior to playback. EveGuard ensures that (i) the perturbed speech signals remain natural to human ears and microphones, and (ii) any attempt by SSEAs to capture and reconstruct the perturbed speech will produce content that is difficult to interpret, both for humans and automated speech recognition systems. Note that EveGuard cannot protect voice from a human speaker when SSEAs target eavesdropping on throat vibrations, a challenge also for state-of-the-art (SOTA) attacks [60]. To attain these salient properties, EveGuard must address four main design challenges. First, it is crucial to ensure the effectiveness of perturbations against SSEAs while maintaining the quality of legitimate voice communication. Existing adversarial speech generation methods commonly rely on additive perturbations, which can introduce noticeable, conspicuous noise [74]. In contrast, EveGuard leverages the distinct sensing mechanisms of the side channels versus traditional microphones or human hearing, i.e., the former only captures low-frequency vibrations whereas the latter senses the subtle changes in air pressure. EveGuard devises a two-stage Perturbation Generator Model (PGM) that integrates: (1) finite impulse response (FIR) convolution to perturb the low-frequency attributes of speech while preserving the speech quality and (2) inaudible low-frequency adversarial perturbations (LFAPs) to corrupt the eavesdropped signals. Second, to automate and optimize the perturbation signal generation, EveGuard requires a new differentiable computational model to represent the SSEA. To tackle this challenge, we propose Eve-GAN, a deep generative network aiming at learning an audio-to-SSEA translation that can map the source audio to the targeted eavesdropped audio. Once trained, Eve-GAN serves as a differentiable layer, enabling end-to-end training of our PGM. Yet training Eve-GAN requires collecting sufficient SSEA samples across various attack scenarios. Additionally, obtaining paired training data is tedious as it requires input-output pairs with the same speaker, speech attributes (e.g., prosody), and utterance content. To address this, we leverage advancements in few-shot unsupervised learning [38]. We propose a few-shot, unpaired audio-to-SSEA translation, which learns to convert source audio into eavesdropped audio by referencing an unpaired SSEA sample. By extracting domain features from the few-shot real-world SSEA samples, Eve-GAN facilitates a generalizable conversion applicable to unseen samples during training. Third, the rapid growth of ML empowers attackers to devise sophisticated SSEAs [21, 22, 54, 58]. For instance, the attacker can transcribe private conversations using speech recognition, and identify digits with audio classifiers. However, the defender has no prior knowledge of the SSEA model deployed by the eavesdropper. To overcome this hindrance, we utilize the transferability of adversarial examples, which means perturbations learned to fool an ensemble of diverse surrogate models can also be effective against unknown black-box models [39, 7]. To this end, we first build a set of surrogate ML models based on multiple hypothetical SSEAs. We then concatenate the PGM with Eve-GAN and ensemble surrogate models to encourage the PGM to learn robust perturbations in an end-to-end manner. Finally, an adaptive attacker who knows the existence of EveGuard may attempt to mitigate the effects of the perturbation. Thus, we apply three preventive techniques to PGM as follows: (1) the use of a discriminator inside the PGM to enforce undetectable constraints, (2) style diversification by integrating VAE-GAN [19] into FIR perturbation generator, and (3) ensuring the LFAP generator uses a random latent vector as input to produce diverse LFAP samples. We implement EveGuard by integrating the above solutions. To evaluate EveGuard, we reproduce white-box SSEAs based on representative eavesdropping sensors, mmWave radar, accelerometer, and optical sensor. Built upon these, we extensively validate EveGuard under various attack settings including distance, orientation, materials, hardware configurations, etc. Our experimental results show that EveGuard achieves a protection rate of more than 97\% from SSEA’s digit classifiers and hinders the recovery of eavesdropped audio with an MCD (Mel-Cepstral Distortion) of over 13.4, and a WER (Word Error Rate) of over 68.2\%. To show that our adversarial audio generated by PGM is imperceptible to humans, we verify the indistinguishability through a user study involving 24 participants. The main contributions of EveGuard are as follows. \bullet We introduce EveGuard, a novel software-driven defense framework that leverages black-box adversarial examples to protect loudspeaker-generated voice from SSEAs. \bullet We design a PGM that leverages the unique features of eavesdropping devices to ensure robust perturbations across diverse attack scenarios, including variations in distance, orientation, materials, and hardware configurations. \bullet We develop Eve-GAN, a differential framework that enables our PGM to learn the distribution of adversarial examples end-to-end, incorporating a few-shot, unpaired audio-to-SSEA translation framework to reduce data collection overhead for training Eve-GAN. \bullet We perform extensive experiments to verify the effectiveness of the EveGuard defense, using both objective metrics and subjective user studies. Audio samples are available at https://eveguard.github.io/demo/."
https://arxiv.org/html/2411.10027v1,"XLSR-Mamba: A Dual-Column Bidirectional
State Space Model for Spoofing Attack Detection","Transformers and their variants have achieved great success in speech processing. However, their multi-head self-attention mechanism is computationally expensive. Therefore, one novel selective state space model, Mamba, has been proposed as an alternative. Building on its success in automatic speech recognition, we apply Mamba for spoofing attack detection. Mamba is well-suited for this task as it can capture the artifacts in spoofed speech signals by handling long-length sequences. However, Mamba’s performance may suffer when it is trained with limited labeled data. To mitigate this, we propose combining a new structure of Mamba based on a dual-column architecture with self-supervised learning, using the pre-trained wav2vec 2.0 model. The experiments show that our proposed approach achieves competitive results and faster inference on the ASVspoof 2021 LA and DF datasets, and on the more challenging In-the-Wild dataset, it emerges as the strongest candidate for spoofing attack detection. The code will be publicly released in due course.","In recent years, the rapid advancement of deep learning has led to significant improvements in speech generation technologies such as text-to-speech (TTS) synthesis [1] and voice conversion (VC) [2]. While these technologies offer many benefits, they also introduce serious risks when misused to produce fake speech for a target speaker. The latest TTS and VC systems are able to produce very high-quality natural-sounding speech, posing a threat to security and trust in voice-based systems. As a result, automatic speaker verification (ASV) systems are increasingly vulnerable to attacks derived using TTS and VC [3, 4, 5]. Developing anti-spoofing solutions is critical to counter these attacks. The ASVspoof challenge series has gained widespread attention by offering standardized datasets, evaluation rules, and performance metrics for the anti-spoofing research community [6]. Previous works in anti-spoofing advocated long-term features to capture the relevant artifacts for spoofing attack detection [7, 8, 9]. Adding value to these works, recent research shows that the cues discriminating spoofed speech from bonafide ones can be well apprehended by local and global features [10, 11]. Local features may reveal unnatural intonation or stress, while global features often exhibit irregular rhythms or flat emotional tones. Combining information from both levels enhances detection performance. End-to-end models optimize directly on the raw audio without relying on handcrafted features and have shown great potential. Therefore, much of the current research is focused on such end-to-end models. For instance, RawNet2 applies time-domain convolution on the raw audio effectively learning the features [10]. Attention mechanisms have also been integrated into graph neural networks to better capture important information across time and frequency. Models like AASIST utilize graph attention networks to process complex local and global features [12]. Similarly, Rawformer combines convolutional layers with transformer structures, using self-attention to extract important cues for anti-spoofing [11]. Self-supervised learning (SSL) [13] has gained attention for creating pre-trained models that generalize well with limited labeled data. Several SSL speech models, including auto-regressive predictive coding, wav2vec [14], and HuBERT [15], have demonstrated promising results in various speech-processing tasks. A recent anti-spoofing model combined the SSL model wav2vec 2.0 XLS-R (0.3B) [16, 17] with the transformer-based conformer architecture [18, 19], achieving state-of-the-art (SOTA) results in ASVspoof 2021 [20]. This success is linked to the multi-head self-attention (MHSA) [21] mechanism’s strong sequence modeling capabilities. However, empirical evidence shows that MHSA is less effective at selectively capturing relevant information for modeling long-range relationships. Therefore, we are interested in proposing a more effective method for capturing long-range feature information in speech signals. Furthermore, the self-attention mechanism faces rising computational complexity with large context windows. To address these issues, state space models (SSMs) [22] like Mamba [23] have been introduced very recently, showing promising results. The RawBMamba [24] model introduces a bidirectional Mamba in anti-spoofing. Nevertheless, the challenge lies in effectively applying Mamba for anti-spoofing tasks with pre-trained models as an alternative to transformer-based frameworks. This work introduces a new bidirectional Mamba structure referred to as the Dual-Column Bidirectional Mamba (DuaBiMamba) for anti-spoofing. DuaBiMamba contains two separate columns for handling the forward and backward features to have improved ability. The outputs of the two columns are finally merged to capture the local and global feature dependencies. Then we propose a novel Mamba-based structure integrated with the self-supervised model wav2vec 2.0 and refer to it as XLSR-Mamba. The primary studies in this work are carried out on the ASVspoof 2021 database to showcase the effectiveness of the proposed XLSR-Mamba and real-world applicability for the inference time involved. We further evaluate the robustness of our proposed model on a more challenging In-the-Wild dataset and compare it with other SOTA system results. Figure 1: Overview of the XLSR-Mamba architecture for anti-spoofing, with three different BiMamba configurations: (a) External Bidirectional Mamba (ExtBiMamba); (b) Inner Bidirectional Mamba (InnBiMamba); and (c) Dual-Column Bidirectional Mamba (DuaBiMamba); (d) XLSR-Mamba model pipeline, including XLS-R feature extraction, linear projection, BiMamba blocks, and prediction head."
https://arxiv.org/html/2411.09625v1,Local deployment of large-scale music AI models on commodity hardware,"We present the MIDInfinite, a web application capable of generating symbolic music using a large-scale generative AI model locally on commodity hardware. Creating this demo involved porting the Anticipatory Music Transformer, a large language model (LLM) pre-trained on the Lakh MIDI dataset, to the Machine Learning Compilation (MLC) framework. Once the model is ported, MLC facilitates inference on a variety of runtimes including C++, mobile, and the browser. We envision that MLC has the potential to bridge the gap between the landscape of increasingly capable music AI models and technology more familiar to music software developers. As a proof of concept, we build a web application that allows users to generate endless streams of multi-instrumental MIDI in the browser, either from scratch or conditioned on a prompt. On commodity hardware (an M3 Macbook Pro), our demo can generate 51 notes per second, which is faster than real-time playback for 72.9% of generations, and increases to 86.3% with 2 seconds of upfront buffering.","Large-scale generative AI models for music are increasingly capable and have the potential to transform both music technology and music creation workflows. However, models are currently tightly coupled to a software ecosystem (e.g., Python, CLIs) distinct from that of music technology (e.g., C++, VST plugins), inhibiting music software developers from exploring this potential. Moreover, models may even require specialized hardware (GPUs) for deployment, a prerequisite that may be unrealistic for many musicians. Cloud inference via APIs could help to bridge this gap [1], but music creativity benefits from low latency, high reliability, and increased privacy, all of which are difficult to guarantee with APIs. In this work, we propose a realistic workflow for deploying large-scale generative AI models on device in the music technology ecosystem (Figure 1). This workflow involves decentralized contributions from two key groups of stakeholders: model providers and music software developers. Model providers are tasked with porting their models to the MLC-LLM framework [2, 3, 4, 5, 6], which facilitates both model compilation and provides a variety of runtimes for deployment. Accordingly, MLC-LLM helps overcome two key obstacles associated with the music technology ecosystem: model compilation allows models to run faster on commodity hardware, and runtimes bridge the gap to technology stacks more familiar to music software developers (C++, web, mobile). Music software developers are then tasked with creating rich interactive music applications that incorporate these generative capabilities. Finally, musicians can then explore these applications on commodity hardware such as laptops and mobile devices in familiar environments like DAWs. Related work [7] allows model providers to deploy to a specific music application—while our goal is to enable music software developers to explore deployment across a multitude of music applications. We present a proof-of-concept workflow, porting a state-of-the-art symbolic music generation model to MLC-LLM (assuming the role of model providers), and developing a web application using the WebGPU runtime (assuming the role of music software developers). Our application is capable of generating an endless stream of multi-instrumental MIDI in the browser on commodity hardware, either from scratch or conditioned on existing MIDI. Figure 1: Proposed workflow for bridging the gap between large model and music software ecosystems, which involves contributions from both model providers (porting models to MLC) and music software developers (building applications on resultant runtimes). Here we build a web demo of a state-of-the-art symbolic music generation model as a proof-of-concept. Config. Throughput tok/s Streamable (%) Streamable (%), w/ 2s buffer Small 128M M2 w/ PyTorch 44 3.9 13.7 M2 w/ MLC 84 24.1 47.0 M3 Max w/ Pytorch 92 30.8 54.3 M3 Max w/ MLC 155 72.9 86.3 Medium 360M M2 w/ PyTorch 14 0.2 1.9 M2 w/ MLC 38 2.3 10.2 M3 Max w/ Pytorch 31 1.2 6.7 M3 Max w/ MLC 90 28.7 52.6 Table 1: Profiling different models, runtimes, and commodity chips. Streamable is % of time where time in generation stream exceeds time in playback stream, with and without an initial 2s playback buffer. Figure 2: Streaming performance visualization for small model. To stream, chips (dashed lines) must exceed the tok/s of music playback (solid line), which varies based on generated note density (shaded \pm1 stdev.)."
https://arxiv.org/html/2411.09349v1,A Unified Evaluation Benchmark for Paralinguistic Analysis,"This document describes the most common article elements and how to use the IEEEtran class with LaTeX to produce files that are suitable for submission to the IEEE. IEEEtran can produce conference, journal, and technical note (correspondence) papers with a suitable choice of class options.","This file is intended to serve as a “sample article file” for IEEE journal papers produced under LaTeX using IEEEtran.cls version 1.8b and later. The most common elements are covered in the simplified and updated instructions in “New_IEEEtran_how-to.pdf”. For less common elements you can refer back to the original “IEEEtran_HOWTO.pdf”. It is assumed that the reader has a basic working knowledge of LaTeX. Those who are new to LaTeX are encouraged to read Tobias Oetiker’s “The Not So Short Introduction to LaTeX,” available at: http://tug.ctan.org/info/lshort/english/lshort.pdf which provides an overview of working with LaTeX."
https://arxiv.org/html/2411.09339v1,Re-Parameterization of Lightweight Transformer for On-Device Speech Emotion Recognition,"With the increasing implementation of machine learning models on edge or Internet-of-Things (IoT) devices, deploying advanced models on resource-constrained IoT devices remains challenging. Transformer models, a currently dominant neural architecture, have achieved great success in broad domains but their complexity hinders its deployment on IoT devices with limited computation capability and storage size. Although many model compression approaches have been explored, they often suffer from notorious performance degradation. To address this issue, we introduce a new method, namely Transformer Re-parameterization, to boost the performance of lightweight Transformer models. It consists of two processes: the High-Rank Factorization (HRF) process in the training stage and the de-High-Rank Factorization (deHRF) process in the inference stage. In the former process, we insert an additional linear layer before the Feed-Forward Network (FFN) of the lightweight Transformer. It is supposed that the inserted HRF layers can enhance the model learning capability. In the later process, the auxiliary HRF layer will be merged together with the following FFN layer into one linear layer and thus recover the original structure of the lightweight model. To examine the effectiveness of the proposed method, we evaluate it on three widely used Transformer variants, i. e., ConvTransformer, Conformer, and SpeechFormer networks, in the application of speech emotion recognition on the IEMOCAP, M3ED and DAIC-WOZ datasets. Experimental results show that our proposed method consistently improves the performance of lightweight Transformers, even making them comparable to large models. The proposed re-parameterization approach enables advanced Transformer models to be deployed on resource-constrained IoT devices.","With the proliferation of Internet-of-Things (IoT) devices, deploying artificial intelligence (AI) models on IoT devices becomes imperative to enable real-time intelligent services while preserving privacy [1, 2, 3, 4]. Speech emotion recognition (SER) is a typical application that needs to be deployed on IoT devices, such as smart home assistants and healthcare wearables, due to its privacy and real-time requirements [5, 6, 7, 8]. After the inception of Transformer [9], it has received tremendous success in natural language processing (NLP), speech processing, and computer vision, and continuously achieves state-of-the-art (SOTA) performance in diverse applications, such as emotion recognition, language translation, dialogue systems, and speech recognition [10, 11, 12]. Such great success is not only due to the considerable increase of training data and the optimization of Transformer architecture, but also largely attributed to the remarkable expansion of model size [10, 13]. For example, the Transformer decoder-based language model of GPT-3 has 175 billion parameters [14] and the model Pangu-\Sigma has even more than one Trillion parameters [15]. The contribution of model size to the model performance has been comprehensively and empirically investigated in NLP [16]. Meanwhile, in many scenarios, it is needed to deploy models on IoT devices because of the privacy and security, and low-latency requirements [17, 18, 19, 20]. For example, the tasks of emotion recognition and health screening and diagnosis, just to name a few, highly relate to users’ personal information, and require keeping the data on their own devices to protect their privacy and security. Besides, other tasks such as command recognition, face detection, and autonomous driving system are always on and require real-time decision-making. However, these IoT devices often lack sufficient storage memory, computational resources, energy, and network bandwidth. The contradiction between the complexity (e. g., model size and computational operations) of Transformer models and the resource-constrained IoT devices has crucially hindered the on-device deployment of Transformer-style models. To deal with this challenge, plenty of model compression and optimization approaches have been introduced to date [21, 22], including pruning [23], knowledge distillation [24], and matrix decomposition [25]. However, with these model compression approaches, especially when compacting the model into a tiny size one, the performance of the lightweight Transformer often significantly degrades due to the reduction of its learning capability [17, 26]. To address this issue, we propose a novel approach, namely Transformer re-parameterization, to boost the performance of lightweight Transformer models without any increase of model size and computational operations. This Transformer re-parameterization approach comprises two processes, i. e., the High-Rank Factorization (HRF) process in the training stage and the de-High-Rank Factorization (deHRF) process in the inference stage. Specifically, in the HRF process, we insert an expanded linear layer before a feedforward network (FFN) of the Transformer. By doing this, it can endow the model with a better learning capability in the training stage due to the increase of model parameters. This process can be algebraically reversed. Thus, in the inference stage, the inserted linear layer will be removed by merging it together with its followed dense layer into one new dense layer with a mathematical calculation. By this means, the original lightweight Transformer architecture will be reconstructed without adding new parameters and computational operations in the inference stage whilst maintaining the boosted model performance. We evaluate the introduced approach in SER due to its broad applications, such as human-machine interaction, call centers, and mental health tracking [27, 12]. The present work is highly motivated by the ‘scaling law’ of deep models [28, 29], where the model performance has a power-law relationship with the model size. This is not only empirically demonstrated via various tasks and models [28, 29], but also mathematically explained through the theory that an interpolated/over-parameterized classifier is more generalized to the unseen data than a small model [30]. In this work, we focus on the Transformer models and investigate their different modules, including the queries, keys, and values (QKV) for attention mechanisms, Projection, FFN, classification head (CLS), and their combinations. To the best of our knowledge, this is the first work for enhancing the lightweight Transformers via the lossless model re-parameterization strategy. Our major contributions can be summarized as follows: • We, for the first time, introduced a novel re-parameter-ization strategy for enhancing the lightweight Transformers, to the best of our knowledge. It empowers lightweight Transformers with better learning capability in the training process whilst no performance loss in the inference stage, without the price of model size and computational operation. • We comprehensively investigated the proposed re-parameterization strategy in different modules of Transformer, including QKV for attention mechanisms, projection, FFN, CLS, and their combinations. • We extensively evaluate the effectiveness and generalizability of our approach in the context of SER, a domain with a strong imperative for on-device deployment due to privacy concerns. The remainder of this paper is organized as follows. We first introduce related work in Section II, and then formalize the problem statement and elaborately describe the proposed method in Section III. After that, the experimental setups and experimental results are given in Section IV. Finally, we draw the conclusion in Section V."
https://arxiv.org/html/2411.09302v1,EEG-Based Speech Decoding: A Novel Approach Using Multi-Kernel Ensemble Diffusion Models,"In this study, we propose an ensemble learning framework for electroencephalogram-based overt speech classification, leveraging denoising diffusion probabilistic models with varying convolutional kernel sizes. The ensemble comprises three models with kernel sizes of 51, 101, and 201, effectively capturing multi-scale temporal features inherent in signals. This approach improves the robustness and accuracy of speech decoding by accommodating the rich temporal complexity of neural signals. The ensemble models work in conjunction with conditional autoencoders that refine the reconstructed signals and maximize the useful information for downstream classification tasks. The results indicate that the proposed ensemble-based approach significantly outperforms individual models and existing state-of-the-art techniques. These findings demonstrate the potential of ensemble methods in advancing brain signal decoding, offering new possibilities for non-verbal communication applications, particularly in brain-computer interface systems aimed at aiding individuals with speech impairments.","I INTRODUCTION Speech is a fundamental aspect of human communication, enabling the conveyance of intricate thoughts and ideas [1]. It is deeply embedded in our social and cultural contexts, playing a critical role in relationship building and information sharing. However, individuals with conditions such as locked-in syndrome are often unable to engage in verbal communication due to physical limitations [2]. Therefore, the development of innovative approaches to restore or replace speech capabilities remains a vital research frontier [3, 4]. This study focuses on decoding brain signals as a means to facilitate non-verbal communication for such individuals. Electroencephalography (EEG) provides a non-invasive method to capture the electrical activities of the brain through scalp electrodes [5]. EEG signals have been widely used in applications ranging from neuroscience research to clinical diagnostics [6, 7]. A growing area of interest involves the decoding of EEG signals to derive meaningful information, such as speech-related activities or cognitive states [8]. EEG-based brain-computer interfaces (BCIs) have been explored for a variety of applications, including mental state classification [9], emotion recognition [10], and motor imagery [11]. Decoding EEG data related to spoken language poses significant challenges due to the complex and highly variable nature of neural activity associated with speech perception and production [12]. EEG signals are also prone to noise and artifacts, which further complicate accurate interpretation [13, 14]. As a result, the development of robust and effective methods for EEG decoding is an ongoing area of research with broad applications, including speech rehabilitation and human-machine interfaces [15]. Previous studies have attempted to decode imagined speech from EEG signals [16, 17], demonstrating the potential of EEG-based BCIs for communication. Deep learning techniques have shown promise in addressing these challenges by automatically learning hierarchical representations from raw EEG data [18]. Architectures such as DeepConvNet [19] and EEGNet [17] have been used successfully for EEG decoding tasks [19, 20]. Other deep learning models, including multi-view CNNs [11] and multimodal deep learning networks [3], have also been applied to EEG classification tasks, achieving notable success. In addition, graph-based methods have been utilized for EEG analysis to identify patterns in brain networks [21]. Denoising diffusion probabilistic models (DDPMs) have emerged as powerful tools for learning complex, high-dimensional patterns in data by progressively adding and then removing Gaussian noise [22]. These models have proven effective in dealing with time series data, including audio and video streams [23], making them suitable candidates for EEG signal analysis. Recent studies have applied diffusion-based models to time series data for tasks such as imputation and forecasting [24]. In the context of EEG decoding, diffusion-based models have been explored to decode imagined speech [16]. Building on these approaches, our study aims to further advance the field of EEG-based speech decoding by employing an ensemble learning strategy. We utilize DDPMs combined with conditional autoencoders (CAEs) to capture the intricate neural features associated with spoken speech. By incorporating multiple models with varying kernel sizes, we are able to capture EEG features at multiple temporal scales, thereby improving the robustness and accuracy of the decoding process. Similar multi-scale approaches have been successfully applied in mental state classification [9] and speech-related brain signal analysis [25]. To our knowledge, this is the first study to apply an ensemble of diffusion models with multi-kernel convolutional layers to decode EEG signals associated with overt speech. By combining the strengths of DDPMs, CAEs, and ensemble learning, we aim to significantly improve the performance of EEG decoding for non-verbal communication, with promising implications for BCI systems that assist individuals with speech impairments."
https://arxiv.org/html/2411.09167v1,Robust AI-Synthesized Speech Detection Using Feature Decomposition Learning and Synthesizer Feature Augmentation,"AI-synthesized speech, also known as deepfake speech, has recently raised significant concerns due to the rapid advancement of speech synthesis and speech conversion techniques. Previous works often rely on distinguishing synthesizer artifacts to identify deepfake speech. However, excessive reliance on these specific synthesizer artifacts may result in unsatisfactory performance when addressing speech signals created by unseen synthesizers. In this paper, we propose a robust deepfake speech detection method that employs feature decomposition to learn synthesizer-independent content features as complementary for detection. Specifically, we propose a dual-stream feature decomposition learning strategy that decomposes the learned speech representation using a synthesizer stream and a content stream. The synthesizer stream specializes in learning synthesizer features through supervised training with synthesizer labels. Meanwhile, the content stream focuses on learning synthesizer-independent content features, enabled by a pseudo-labeling-based supervised learning method. This method randomly transforms speech to generate speed and compression labels for training. Additionally, we employ an adversarial learning technique to reduce the synthesizer-related components in the content stream. The final classification is determined by concatenating the synthesizer and content features. To enhance the model’s robustness to different synthesizer characteristics, we further propose a synthesizer feature augmentation strategy that randomly blends the characteristic styles within real and fake audio features and randomly shuffles the synthesizer features with the content features. This strategy effectively enhances the feature diversity and simulates more feature combinations. Experimental results on three deepfake speech benchmark datasets demonstrate that our model achieves the state-of-the-art robust detection performance across various evaluation scenarios, including cross-method, cross-dataset, and cross-language evaluations.","With the rapid advancement of deep learning techniques, deepfake technology, including the synthesis and manipulation of multimedia content, has become increasingly accessible [1]. The recent advancements in deepfake generation methods have enabled the creation of multimedia content with remarkable reality, presenting a significant threat to the security of multimedia information [2], such as impersonation attack [3], reputation damage, or online harassment [4]. Despite the considerable focus on deepfake video detection, research on deepfake speech detection remains relatively underdeveloped [5]. Deepfake speech, also known as AI-synthesized speech, involves the synthesis or manipulation of speech waveforms to replace the original audio content with artificially generated content. Two common deepfake speech generation methods are text-to-speech (TTS) and voice conversion (VC), both of which typically utilize neural vocoders to produce audio waveforms based on temporal-frequency representations. TTS methods allow for the synthesis of audio with specific voice styles from text inputs [6], while VC methods enable the modification of voice styles while retaining the original content [7]. The advancement of both TTS and VC technologies has significantly increased the challenge of distinguishing between genuine and fake speech signals using human perception [8]. To address the potential threat caused by deepfake speech, it is imperative to develop effective detection methods capable of distinguishing between genuine and fake speech signals [9]. Initially, early deepfake speech detection methods primarily relied on specific statistical features inherent to audio signals, such as Mel-frequency cepstral coefficient (MFCC) [10], linear frequency cepstral coefficients (LFCC) [11], constant Q cepstral coefficients (CQCC) [12], and Fourier bi-spectrum [13]. However, these methods have shown limited effectiveness against the rapid development of deepfake speech generation techniques. Recently, some well-designed deep learning models have emerged to address the challenge of deepfake speech detection. These models include multi-task learning networks [9], unsupervised pre-training models [14], graph neural networks [15], multi-view-based networks [16], and ResNet-based networks [17]. These models directly learn discriminative features from speech and perform well in intra-dataset evaluation. However, they exhibit unsatisfactory performance on unseen synthesizers or real-word data [18]. This is attributed to the inherent limitations of their feature learning strategies, which cause the detection model to focus on specific synthesizer artifacts overly. Consequently, these methods are ineffective when dealing with new types of synthesizers. In this study, we propose a new approach for robust deepfake speech detection using feature decomposition learning and synthesizer feature augmentation. Our goal is to enhance detection robustness by learning synthesizer-independent content features as complementary features. We first design a dual-stream feature decomposition learning strategy that employs a synthesizer stream and a content stream to decompose the speech representation learned from the backbone model. The synthesizer stream is responsible for learning the synthesizer-related features through supervised training with synthesizer labels, while the content stream focuses on learning synthesizer-independent content features. As direct content-related labels for training are unavailable, we employ a pseudo-labeling-based supervised learning method for the content stream. This method generates compression and speed labels for training by randomly altering speech characteristics through applying various compression levels and codecs, as well as adjusting the speech speed. Additionally, we employ an adversarial learning method to reduce the synthesizer-related components in the content stream. This involves integrating an adversarial loss to force the classification probabilities of synthesizers based on content features to resemble random guessing. For classification, we concatenate the synthesizer and content features to determine whether the input speech is synthesized. To further enhance the detection robustness of our method on different synthesizer characteristics, we propose a feature augmentation strategy consisting of feature blending and shuffle operations. The feature blending operation randomly merges the characteristic styles within each class of feature to enhance feature diversity, while the feature shuffle operation mixes synthesizer features with content features to simulate more synthesizer-content feature combinations. The main contributions of this work are summarized as follows: • We develop a robust detection model that employs dual-stream feature decomposition learning to detect AI-synthesized speech. Different from previous methods overly relying on specific vocoder artifacts, our method employs feature decomposition to learn vocoder-independent features as the complementary feature for detection. • We propose a synthesizer feature augmentation strategy to enhance the model’s robustness to different synthesizer characteristics and synthesizer-content feature combinations. • We conduct extensive experiments on three benchmark datasets, and the results demonstrate that our method achieves state-of-the-art detection performance and exhibits robust generalizability across diverse synthesizer methods, datasets, and languages. This paper is structured as follows: a literature review is presented in Section II. The architecture and methodology of our two-stream network are presented in Section III. Section IV presents the implementation details and the experimental results. Sections VI illustrates ablation studies and discusses the effectiveness of model components. Finally, Section VII summarizes the conclusion and future work. Figure 1: Network architecture of our method. A main stream is used to learn robust speech representation from the log-scale frequency spectrogram of the input speech. Subsequently, a dual-stream learning strategy, comprising a synthesizer stream and a content stream, is employed to decompose the learned speech representation. The final classification is performed based on the concatenation of the synthesizer and content features. A synthesizer feature augmentation strategy consisting of feature blending and feature shuffle operations is employed to enhance the model’s robustness to different synthesizer characteristics and synthesizer-content feature combinations."
https://arxiv.org/html/2411.09080v1,Language Models for Music Medicine Generation,"Music therapy has been shown in recent years to provide multiple health benefits related to emotional wellness. In turn, maintaining a healthy emotional state has proven to be effective for patients undergoing treatment, such as Parkinson’s patients or patients suffering from stress and anxiety. We propose fine-tuning MusicGen, a music-generating transformer model, to create short musical clips that assist patients in transitioning from negative to desired emotional states. Using low-rank decomposition fine-tuning on the MTG-Jamendo Dataset with emotion tags, we generate 30-second clips that adhere to the iso principle, guiding patients through intermediate states in the valence-arousal circumplex. The generated music is evaluated using a music emotion recognition model to ensure alignment with intended emotions. By concatenating these clips, we produce a 15-minute ""music medicine"" resembling a music therapy session. Our approach is the first model to leverage Language Models to generate music medicine. Ultimately, the output is intended to be used as a temporary relief between music therapy sessions with a board-certified therapist.","Music therapy has proved to be a highly effective alternative treatment to traditional medication since the late nineteenth century. For most of the twentieth century, music therapy’s positive effects in treating patients were investigated empirically, and the results were presented from a qualitative perspective. Nevertheless, recent advances in sensor technology have allowed clinicians to perform experiments that measure changes in heart rate, electromyogram, respiration, and skin conductance. Moreover, neuroimaging methodologies, such as Positron Emission Tomography and functional Magnetic Resonance Imaging [1] enable the visualization of the signals transmitted between neurons and the cerebral blood flow within the brain during sessions. Therefore, the patient’s response to music has been studied from a cognitive perspective. With the advent of generative models such as Transformers and Diffusion models, various works have investigated the quality and diversity of AI-generated music for different applications [2, 3, 4]. Some examples of tasks that can be accomplished involve style change [5], orchestration [6], and novel music generation pieces from a given text prompt [7]. Following the popularity of generative AI, Williams et al. [8] evaluated the effectiveness of a Markov model to generate therapeutic music for a patient by measuring their Galvanic Skin Response at any given moment and confirmed its effects. Following those results, Hou et al. [9] and Li et al. [10] presented two long-short term memory (LSTM) based generative models adapted for music therapy given a treatment scenario. More recently, music generation in the symbolic domain has shown promising results for music therapy [11]. In this work, we propose a generative music medicine model111Demo: https://tinyurl.com/gen-musmed by fine-tuning MusicGen [7] using low-rank decomposition [12] with prompts that contain emotion labels. We aim to generate a “therapy session” that follows the iso principle [13], which aims to alter a person’s mood by playing music matching their current mood and then gradually shifting to music that represents a desired positive state. Iso is a commonly used practice in music therapy, with its effectiveness demonstrated in recent studies [14, 15]. We use the term music medicine for the output of our model, as music therapy is always conducted in coordination with a licensed music therapist [16]. CLAP \uparrow AUPRC \uparrow Ham Score \uparrow Time (min.) \downarrow Parameters \downarrow MusicGen-Large 36.8 (+9.85%) 0.144 (+19.37%) 0.957 (+1.92%) 100\pm 5 3.3*10^{9} MusicGen-Medium 38.7 (+2.97%) 0.06 (+6.00%) 0.929 (+0.96%) 88\pm 10 1.5*10^{9} MusicGen-Small 33.1 (+19.12%) 0.031 (-3.44%) 0.942 (+0.95%) 48\pm 2 0.3*10^{9} Table 1: Evaluation scores for the three fine-tuned models and their percentage improvements over the non-finetuned version. The time column shows the average inference time for a 15-minute music session."
https://arxiv.org/html/2411.09243v1,"Towards Unified Neural Decoding of Perceived, Spoken and Imagined Speech from EEG Signals","Brain signals accompany various information relevant to human actions and mental imagery, making them crucial to interpreting and understanding human intentions. Brain-computer interface technology leverages this brain activity to generate external commands for controlling the environment, offering critical advantages to individuals with paralysis or locked-in syndrome. Within the brain-computer interface domain, brain-to-speech research has gained attention, focusing on the direct synthesis of audible speech from brain signals. Most current studies decode speech from brain activity using invasive techniques and emphasize spoken speech data. However, humans express various speech states, and distinguishing these states through non-invasive approaches remains a significant yet challenging task. This research investigated the effectiveness of deep learning models for non-invasive-based neural signal decoding, with an emphasis on distinguishing between different speech paradigms, including perceived, overt, whispered, and imagined speech, across multiple frequency bands. The model utilizing the spatial conventional neural network module demonstrated superior performance compared to other models, especially in the gamma band. Additionally, imagined speech in the theta frequency band, where deep learning also showed strong effects, exhibited statistically significant differences compared to the other speech paradigms.","I INTRODUCTION Brain-computer interface (BCI) serves as brain-driven communication pathways that convert neural signals into actionable inputs for external systems[1]. In recent years, active BCI has emerged as a next-generation control interface, offering speech-based interaction by directly harnessing the user’s cognitive states and intentions[2]. Various types of user input have been studied, including visual imagery, imagined speech[3, 4, 5, 6], motor imagery[7, 8], and motor execution, each presenting unique advantages and limitations. In this paper, We present a novel, integrative BCI paradigm that encompasses perception, imagined speech, whispered speech, and overt speech. This approach holds promise for addressing various limitations in human-computer interaction and provides BCI users with an alternative method of control. Humans engage in speech production across a variety of real-world contexts. To enable better control of speech in different real-life scenarios, it is essential to collect, analyze, and research data across various speech states. Machine learning techniques for neural decoding have demonstrated significant success, yet they heavily depend on manually engineered features. In contrast, deep learning approaches can directly learn from raw data and execute tasks in an end-to-end manner[9], making them more applicable to real-world scenarios[10, 11, 12]. With this in mind, we evaluated the performance of our proposed brain-based input system using a standard deep neural network (DNN) and investigated straightforward yet effective modifications to tailor the networks more closely to our specific paradigm. DNNs in the BCI domain, such as EEGNet[13], ShallowConvNet[14], and filter-bank convolutional network (FBCNet)[15], typically employ distinct layers of temporal and spatial convolutions. These networks utilize 1D convolutional kernels of fixed sizes to extract temporal, spectral, and spatial features. Temporal kernels are often chosen heuristically, while spatial kernels are applied uniformly across all channels. This architecture has been highly effective in conventional BCI paradigms, such as visual imagery, motor imagery, and motor execution, which predominantly involve sensory or motor-related signals. Nevertheless, the complex nature of EEG signals demands multi-scale kernels to interpret information across various temporal scales[16, 17]. Additionally, due to the effects of volume conduction, EEG signals exhibit redundancy between electrodes, resulting in low spatial resolution. Moreover, current feature extraction methods, which are based in Euclidean space, cannot accurately capture the complex relationships between multiple electrodes, necessitating additional spatial-based features like connectivity methods, phase locking value (PLV), phase lag index (PLI), and coherence, which contain topological spatial information of the brain[18]."
https://arxiv.org/html/2411.09189v1,Improvement and Implementation of a Speech Emotion Recognition Model Based on Dual-Layer LSTM,"This paper builds upon an existing speech emotion recognition model by adding an additional LSTM layer to improve the accuracy and processing efficiency of emotion recognition from audio data. By capturing the long-term dependencies within audio sequences through a dual-layer LSTM network, the model can recognize and classify complex emotional patterns more accurately. Experiments conducted on the RAVDESS dataset validated this approach, showing that the modified dual-layer LSTM model improves accuracy by 2% compared to the single-layer LSTM while significantly reducing recognition latency, thereby enhancing real-time performance. These results indicate that the dual-layer LSTM architecture is highly suitable for handling emotional features with long-term dependencies, providing a viable optimization for speech emotion recognition systems. This research provides a reference for practical applications in fields like intelligent customer service, sentiment analysis, and human-computer interaction.","Speech Emotion Recognition (SER) is a core technology in artificial intelligence and human-computer interaction, aiming to recognize the emotional states of speakers by analyzing and processing audio signals. With the rising demand in applications such as intelligent customer service, emotional robots, and personalized recommendation systems, SER technology has gained widespread attention. The effectiveness of emotion recognition determines the naturalness of human-computer interaction and user experience, making it essential to improve SER model accuracy and real-time performance. Traditional emotion recognition methods often rely on handcrafted features, such as Mel-Frequency Cepstral Coefficients (MFCC), pitch, and rhythm. However, these features cannot fully capture complex emotional information, especially when processing long-term dependencies in emotional features, where handcrafted features have limited expressive power. With the rapid advancement of deep learning, researchers have begun to apply Deep Neural Networks (DNN), Convolutional Neural Networks (CNN), and Recurrent Neural Networks (RNN) to SER tasks. In particular, Long Short-Term Memory (LSTM) networks, with their unique gating mechanisms that effectively capture long-term dependencies in time-series data, have become the mainstream approach in emotion recognition. However, single-layer LSTM structures still have limitations in extracting emotional features, especially when dealing with audio signals with mixed or complex emotional shifts. To address this, we introduced an additional LSTM layer to form a dual-layer LSTM model that can better capture and interpret emotional information, thereby improving accuracy and efficiency in emotion recognition tasks[1]."
https://arxiv.org/html/2411.08889v1,Multilingual Standalone Trustworthy Voice-Based Social Network for Disaster Situations,"In disaster scenarios, effective communication is crucial, yet language barriers often hinder timely and accurate information dissemination, exacerbating vulnerabilities and complicating response efforts. This paper presents a novel, multilingual, voice-based social network specifically designed to address these challenges. The proposed system integrates advanced artificial intelligence (AI) with blockchain technology to enable secure, asynchronous voice communication across multiple languages. The application operates independently of external servers, ensuring reliability even in compromised environments by functioning offline through local networks. Key features include AI-driven real-time translation of voice messages, ensuring seamless cross-linguistic communication, and blockchain-enabled storage for secure, immutable records of all interactions, safeguarding message integrity. Designed for cross-platform use, the system offers consistent performance across devices, from mobile phones to desktops, making it highly adaptable in diverse disaster situations. Evaluation metrics demonstrate high accuracy in speech recognition and translation, low latency, and user satisfaction, validating the system’s effectiveness in enhancing communication during crises. This solution represents a significant advancement in disaster communication, bridging language gaps to support more inclusive and efficient emergency response.","In today’s interconnected world, effective communication is essential, particularly during disaster situations where timely information dissemination can save lives. However, linguistic diversity often poses significant challenges in ensuring that crucial messages are understood by all stakeholders. Traditional communication platforms frequently fail to address these language barriers, leaving gaps in understanding that can have dire consequences during emergencies. The advent of AI technologies and decentralized systems like blockchain offers new possibilities for creating secure, multilingual communication networks that can function reliably even in the most challenging circumstances. Motivated by the need for a robust solution to bridge language gaps in crisis scenarios, this paper presents a novel, multi-platform application that integrates advanced AI with blockchain technology to enable trustworthy, asynchronous voice-based social media communication across language barriers. The application is designed to run seamlessly on various devices, including mobile phones, tablets, and desktops, and is compatible with different operating systems, ensuring accessibility and reliability in diverse environments. The significance of addressing multilingual communication barriers in disaster situations cannot be overstated. Traditional approaches to disaster communication often fail to account for the linguistic diversity inherent in many societies, leading to what is termed ”disaster linguicism.” This phenomenon exacerbates the vulnerability of Indigenous, Tribal, Minority, and Minoritized peoples, as critical information is often not accessible in their native languages. By proposing a solution that integrates advanced AI with blockchain technology, this paper directly responds to the urgent need for more inclusive and effective disaster risk reduction strategies that consider the linguistic needs of all communities, thereby reducing social vulnerability and enhancing resilience during crises [1]. Despite advancements in communication technology, there remains a critical gap in enabling asynchronous, multilingual voice communication during disaster situations. Current social media platforms and communication tools often rely on centralized servers and lack the necessary security features to ensure message integrity and authenticity, which are particularly vulnerable in disaster scenarios where external networks may fail. Additionally, the language translation tools available are not typically integrated into a system that provides both translation and secure message verification. This lack of integration can lead to misinformation, miscommunication, and delays in critical information sharing, which can exacerbate the effects of disasters. Furthermore, many existing solutions are platform-dependent, limiting their effectiveness across different devices and operating systems. This paper presents significant advancements in the field of disaster communication and multilingual technology through the following key contributions: • Innovative Application for Disaster Scenarios: We introduce a novel, multi-platform application specifically designed to function reliably in disaster situations, where communication infrastructure may be compromised. The application’s ability to operate offline on a local network without requiring an internet connection ensures its deployment in critical environments. Its lightweight design allows it to run on portable devices like laptops, making it highly adaptable and infrastructure-free. • AI-Driven Asynchronous Multilingual Translation: The application leverages advanced AI technology to facilitate asynchronous voice communication by automatically translating spoken messages across multiple languages. Furthermore, it synthesizes these translations back into voice, providing a natural and effective method of communication that transcends language barriers. • Blockchain-Enabled Secure Communication: A key feature of this application is its implementation of secure storage and verification mechanisms using Ethereum blockchain. This ensures the integrity and authenticity of translated messages, critical in maintaining trustworthy communication during emergencies. • Cross-Platform Usability and Security: The application is designed to operate independently of external servers, with all AI processing and blockchain management performed locally. This not only enhances security and reliability but also ensures a seamless user experience across various devices and operating systems, making it accessible in diverse disaster environments. • Robust and Portable Communication System: By focusing on portability, trustworthiness, and transparency, this application provides a robust communication solution in emergency situations. It guarantees that communication remains secure, reliable, and effective, even when traditional systems fail."
https://arxiv.org/html/2411.08885v1,"Enhancing Lie Detection Accuracy: A Comparative Study of Classic ML, CNN, and GCN Models using Audio-Visual Features","Inaccuracies in polygraph tests often lead to wrongful convictions, false information, and bias, which have significant consequences for both legal and political systems. Recently, analyzing facial micro-expressions has emerged as a method to detect deception; however, current models have not reached high accuracy and generalizability. The purpose of this paper is to aid in remedying these problems. The unique multimodal transformer architecture used in this paper improves upon previous approaches by using auditory input, visual facial micro-expressions, and manually transcribed gesture annotations, moving closer to a reliable non-invasive lie detection model. Visual and auditory features were extracted using Vision Transformer and OpenSmile models respectively, which were then concatenated with the transcriptions of participants’ micro-expressions and gestures. Various models were trained for classification instances of lies and truth using these processed and concatenated features. The CNN Conv1D multimodal model achieved a 95.4% average accuracy. However, further research is still required to create higher-quality datasets and even more generalized models for more diverse applications.","Lie detection has been a recurring focus of research and technological innovation in law enforcement and criminal justice. According to a survey conducted by the University of Wisconsin-La Crosse, about 75% of survey respondents reported telling zero to two lies per day; lying comprised 7% of total communication, with 79% of the lies being told face-to-face and 21% being mediated [3]. Current technologies, such as polygraphs, have focused on biological responses like blood pressure to detect lies. However, these methods are unpredictable and easily flawed. Recently, research has begun to focus on various other indicators of deception, including facial micro-expressions and audio cues [4]. Facial micro-expressions (ME) are intentional or involuntary localized and momentary movements of the face, usually lasting less than 500 milliseconds [2]. Despite advancements in lie detection techniques, traditional methods remain intrusive, subjective, and often inaccurate. Detecting deception through ME and speech analysis presents a significant challenge due to the subtle and brief nature of these cues. As shown in Table I, traditional methods have high variance and relatively low accuracy. This study aims to address these limitations by developing a non-intrusive, objective, and highly accurate method for detecting deception using both ME and audio signals. Accurate lie detection is crucial in various fields, including security, legal systems, and psychological evaluations. The primary objective of this study is to establish an AI model that can differentiate between truth and deception with high accuracy by analyzing audio, visual cues in videos, and extracted gestures. Audio dialogue, visuals, and gestures all help to distinguish between deception and truthfulness, making them important features to consider [23]. Therefore, the Real-life Deception Detection Dataset from the University of Michigan was used, which includes 121 videos of deception and truthfulness and a CSV file for gestures. Visuals and audio were extracted from the videos, and OpenSMILE and Vision Transformer (ViT) were used to extract features from audio and video, respectively. Classical machine learning models like Random Forest Classifiers and Logistic Regression can serve as accurate baseline references for a binary classification task like truth and lie. Yet to build off of that, by leveraging advanced neural network models, such as Conv1D, Graph Convolutional Networks (GCN), and CNN LSTM, the accuracy can be increased. TABLE I: Estimated accuracy of different test types in detecting deception and truthfulness Test type Detecting deception Detecting truthfulness Laboratory studies CQT – Polygraph 74%–82% 60%–83% CIT – Polygraph 76%–88% 83%–97% ERP 68% 82% fMRI 84% 81% Field studies CQT – Polygraph 84%–89% 59%–75% CIT – Polygraph 42%–76% 94%–98% This study addresses the following research questions: How effective is the proposed AI model in detecting lies compared to traditional methods and some recent AI models? Which features carry the highest weights in prediction? Deception detection technology has the potential to revolutionize various fields. In law enforcement, it could improve interrogation outcomes and border security by identifying deceptive behavior. In the legal system, it could be utilized to assess the credibility of courtroom testimonies and negotiations. Additionally, applying this technology to financial services could aid in detecting fraudulent claims and reducing the risk of financial fraud. Previous studies have experimented with various machine learning models. For instance, a study by Soldner et al. implemented the Random Forest model, achieving the best accuracy of 69%, as shown in II [5]. Insights from this paper suggest expanding our dataset and exploring additional modalities to enhance the model’s accuracy and reliability in lie detection. Furthermore, Random Forest, being a machine learning technique, cannot handle complex relations as well as multimodal data, which is a limitation of the mentioned study. Moreover, most traditional AI models fall short in reliability and accuracy, often leading to false positives or negatives [1]. A study conducted by the University of Michigan in 2015 analyzed trial videos using micro-facial expressions and achieved a rudimentary accuracy rate of 83.05% using neural networks [6]. Aligning different data types and achieving 83.05% accuracy are two main advantages of the study. TABLE II: Best Results Of Study [2]. Features Acc. Linguistic 66% Dialog 57% Non-verbal 61% All Features 69% This paper is organized as follows: analyzing previous work, discussing the paper’s methods (data collection, data analysis, feature extraction, and implementation guide for the tested models), presenting the results of different tested models, comparing the paper’s results with other studies using the same dataset, and providing a discussion including limitations and recommendations. The paper concludes with a summary of key findings and a look forward."
https://arxiv.org/html/2411.08234v1,Analyzing Pitch Content in Traditional Ghanaian  Songs,"This study examines the pitch content in traditional Ghanaian seperewa (Akan harp-lute) songs, utilizing a unique dataset from field recordings of the mid-twentieth century. We selected 71 songs and used Demucs to isolate vocals from instrumental tracks. We then retrieved the F0 content from these isolated tacks and applied Gaussian Mixture Models (GMM) to approximate musical scales. Comparative F0 analysis between vocals and seperewa revealed higher microtonal deviations from equal temperament in vocal tracks. We also note challenges in using MIR tools for musical scale approximation in non-Western music. Our research contributes to the quantitative study of pitch in traditional music of Sub-Saharan Africa.","This paper examines archived recordings of seperewa, a two-course, six or eight string harp-lute that accompanies sung repertoire in Akan languages [1]. The Akan are a Ghanaian ethnic and language group that includes the subgroups of Asante, Fante and Akuampem and make up nearly half of the country’s population [2]. European documentation has demonstrated its symbolic importance in the Asante Empire in the 18th century [1]. Due in part to to the introduction of guitars from Europe, the seperewa nearly disappeared by the the mid-20th century, leading to conservation efforts by Ghanaian musicologists Ephraim Amu (1899-1995) and J.H. Kwabena Nketia (1921-2019). We chose seperewa songs because they demonstrate tuning systems other than equal-temperament (though over time it has been tuned to align with equal-tempered instruments like piano or fretted guitar) [1]. The instrument is briefly mentioned in literature addressing West African music; Nketia, however, wrote a comprehensive publication on seperewa harmony and melody [3]. A more recent study by McPherson and Obiri-Yeboah examines Akan language encoding in seperewa music [4] Therefore, due to its cultural significance, studying traditional seperewa music could shed light on the original indigenous practices in Africa that made their way to the Americas and shaped the African diasporic musical practice across the globe. Musicological research has documented and analyzed musical and social aspects of traditional music across the Africa continent, as well as the influences of Western European religious and military music [5]. In particular, there is extensive literature focusing on aspects of rhythm in African music, such as cycle and multidimentionality [6, 7, 8], as well as musical connections between Africa and Afro-Latino communities [9, 10]. More recent developments have employed computer analysis to look at micro-timing in African drumming to suggest alternative approaches to meter [11, 12]. However, there is little research on pitch content in African music that explores microtonal variance and tendencies outside the Western equal-tempered tuning system [13]. We seek to redress this balance by investigating Ghanaian music’s unique scales. Nketia stated, “seperewa music and Akan songs in general are based on the heptatonic scale, though there remains a great deal of variance from Western tuning systems and tonal logic within this framework” [1]. Therefore, we examine Akan pitch through the implied scale and the microtonal content of the seperewa’s song repertoire. Therefore, our research questions are: 1. Given a known seperewa scale, can we use MIR methods (such as sounrce separation, F0 tracking, and probability density function modeling) to detect its presence in the vocal and instrument pitch content of a seperewa song? 2. How “equal tempered” are the overall scales we approximate? how microtonally flat or sharp are specific scale degrees? 3. How similar and different are the scales between the seperewa and the vocals? . These questions align with our broader goal of decolonizing datasets and studying the effects of music technology’s embedded biases (i.e. the equal-tempered system in synthesis instruments, MIDI protocols, and recording effects like autotune) on traditional and popular musics of the world. In keeping with our decolonizing methods, we obfuscate the original audio material111the archive granted permission to share some song examples at seperewa-pitch-analysis.github.io to retain indigenous intellectual property of this archive. Through collaborative efforts with the original authors and their communities, we aim to activate these archives for research in innovative and transformative ways."
https://arxiv.org/html/2411.08316v2,Evaluating Synthetic Command Attacks on Smart Voice Assistants,"Recent advances in voice synthesis, coupled with the ease with which speech can be harvested for millions of people, introduce new threats to applications that are enabled by devices such as voice assistants (e.g., Amazon Alexa, Google Home etc.). We explore if unrelated and limited amount of speech from a target can be used to synthesize commands for a voice assistant like Amazon Alexa. More specifically, we investigate attacks on voice assistants with synthetic commands when they match command sources to authorized users, and applications (e.g., Alexa Skills) process commands only when their source is an authorized user with a chosen confidence level. We demonstrate that even simple concatenative speech synthesis can be used by an attacker to command voice assistants to perform sensitive operations. We also show that such attacks, when launched by exploiting compromised devices in the vicinity of voice assistants, can have relatively small host and network footprint. Our results demonstrate the need for better defenses against synthetic malicious commands that could target voice assistants.","Voice is a natural way for people to interact with devices in their vicinity. It is one of the reasons for the increasing adoption of voice assistants such as Amazon Alexa, Google Assistant and Samsung Bixby. At the same time, applications enabled by such voice assistants are increasing at a rapid pace as can be seen by the diversity and number of Amazon Skills. The proliferation of voice assistants and applications supported by them leads to new security threats. In fact, researchers have explored how voice assistants can be targeted by malicious commands that can be issued when the attacker is in close physical proximity or across the network [diao2014your, jang2014a11y, Carlini:2016vl, schonherr2018adversarial, Carlini:2018wj, Abdullah:2018ho, Yuan:2018um, kumar2018skill, qin2019imperceptible]. Others have explored thousands of applications supported by voice assistants and the sensitive actions performed by them [shezan2020read]. We explore if attackers can leverage the ease with which speech can be harvested to launch attacks against voice assistant enabled applications. As with email addresses and phone numbers, limited and unrelated speech can be easily harvested for a large number of people. By unrelated speech, we mean speech that does not include words that must be uttered for specific voice assistant commands. For many users, unrelated speech may be available from podcasts, YouTube videos, lectures, talks, online posts, or can even be collected by making robocalls. A possible defense against malicious commands is to use the command voice itself to determine if the command is coming from an authorized user. To enable this, authorized users must set up their profiles to allow command voice to be matched against the voices of users having profiles. However, because of a variety of reasons, including usability and environmental constraints (e.g., noisy background, distance between speaker and voice assistant), a command source’s match with an authorized user voice is not required to be strict. In the context of malicious voice commands, to the best of our knowledge, the efficacy of such defenses in current voice assistants has not been explored. Although widely available services can now be used for speech synthesis [ElevenLabs2023, Coqui2023, wang2023neural], attackers will prefer to avoid them for cost or detection reasons when targeting a large number of victims. Since attack commands that target voice assistants may not require natural-sounding human quality speech, we explore the feasibility of low cost techniques for speech synthesis which allow such synthesis to be done even at compromised user devices. We have two specific goals for how commands are synthesized. First, the synthesized command needs to be intelligible to the voice assistant so it is recognized by it. Second, the command must preserve similarity with an authorized user’s voice so it can work even when the voice assistant enabled application matches commands to users with a certain level of confidence. If these two goals can be met for a significant fraction of targeted users and their voice assistants, it will demonstrate the feasibility of low cost and large scale attacks on voice-enabled applications. We present an empirical security analysis for commands of various lengths when applications check that the command voice comes from an authorized user. To conduct experiments at scale to compare both intelligibility and similarity of synthetically generated commands, we develop an experimental testbed with a popular voice assistant (e.g., Amazon Alexa). We use an efficient and lightweight concatenative speech synthesis scheme to generate attack commands. In particular, we use a unit-selection method that extracts diphones from available speech and concatenates necessary diphones to synthesize a command [lenzo2000diphone, OSHAUGHNESSY198855]. Our automated testbed allowed us to conduct over one thousand experiments for many commands and user profiles. The results of these experiments help us demonstrate the following: 1. We show that when available target speech has all diphones needed for the synthesis of a command, the Alexa voice assistant correctly recognizes 93.8% of the commands generated with a basic unit-selection concatenative synthesis method. Our experiments used both short and long commands in the voice of a diverse group of users (e.g., accents, gender etc.). 2. Similar to command recognition, we found that unit-selection concatenative synthesis also preserves voice similarity as assessed by the speaker match confidence level. In our experiments, the highest confidence level in speaker voice similarity was returned for 90% of the users who have profiles that vary in accent and gender. Thus, our results show security vulnerabilities of voice assistants to synthetic commands even when applications match command source with authorized users. 3. We demonstrate that 50% of the commands can successfully activate a voice assistant when synthesized from only 30 seconds of unrelated speech of a target. This is true even when applications processing these commands require a high level of confidence in the similarity of the command source voice with the voice of an authorized user. We show that the success rate increases as more speech becomes available, reaching 80% with 4 minutes of target speech. Our results demonstrate the ease with which voice assistant enabled applications can be targeted by harvesting speech and efficiently synthesizing attack commands. To the best of our knowledge, we are the first to show that voice profile matching, as used currently, provides little protection against such malicious commands. The paper is structured as follows. Section II discusses related work and the threat model is presented in Section III. We discuss our approach in Section IV and the system developed for carrying out experiments is described in Section V. The results of our experiments are discussed in detail in Section VI. The paper is concluded with discussions and conclusions in Sections VII and VIII."
https://arxiv.org/html/2411.08307v1,Perceiver: A Multi-cale Perceiver with Effective egmentation for Long-Term Expressive Symbolic Music Generation,"Music generation has progressed significantly, especially in the domain of audio generation. However, generating symbolic music that is both long-structured and expressive remains a significant challenge. In this paper, we propose PerceiverS (Segmentation and Scale), a novel architecture designed to address this issue by leveraging both Effective Segmentation and Multi-Scale attention mechanisms. Our approach enhances symbolic music generation by simultaneously learning long-term structural dependencies and short-term expressive details. By combining cross-attention and self-attention in a Multi-Scale setting, PerceiverS captures long-range musical structure while preserving performance nuances. The proposed model, evaluated on datasets like Maestro, demonstrates improvements in generating coherent and diverse music with both structural consistency and expressive variation. The project demos and the generated music samples can be accessed through the link: https://perceivers.github.io.","Recent advancements in music generation, especially in audio generation models such as AudioLDM [1], MusicGen [2], and Jen-1 [3], have demonstrated significant progress, with these models capable of generating highly natural-sounding music. However, symbolic music generation, an area where models can generate and manipulate music in symbolic form, plays a crucial role in the field of music generation, particularly due to its editable nature. This allows for operations such as cutting and rearranging different sections or substituting instrument timbres, enabling human involvement in high-quality music production during the post-processing stage. Compared to audio generation, symbolic music offers a further level of abstraction, making it easier for machine learning models to capture deeper musical characteristics and understanding. However, the symbolic music generation still faces two key challenges. First, despite the advancements, many of the most expressive datasets, recorded from live performances and recording studios, are seldom used compared to manually created MIDI-file datasets. The primary reason is that they lack detailed annotations, making it harder for models to learn complex structures. Furthermore, due to computational limitations, these models cannot fully capture the context of an entire piece of music. Techniques, such as chunking and quantization, are often employed to reduce computational complexity, leading to the loss of crucial musical details and making it difficult for models to grasp the full structure of a composition. Second, the waterfall-like approaches that use abstract structural representations as conditions for music generation tasks have enabled the generation of structured music. However, such methods rely heavily on handcrafted feature engineering. Our objective is to design and develop a model that is capable of learning the long-range dependencies in music without relying on explicit structural annotations. The emergence of Transformer Attention technologies, such as Perceiver AR [4], has made it possible to access much longer contextual dependencies. It allows for the simultaneous learning of musical structure and the generation of expressive performances. Perceiver AR has demonstrated the ability to attend to a context length of up to 32,768 tokens using the Maestro dataset [5], where the query in cross-attention attends to significantly longer key/value pairs [4]. However, this approach has also introduced challenges. Specifically, the causal mask, when applied with the default input sequence segmentation, does not fully conceal tokens that should not be visible during autoregressive training and generation, which ultimately degrades the quality of the generated music. Additionally, when using ultra-long context as a condition, the model tends to generate identical or similar repetitive segments as the sequence length increases due to issues with high similarity in the context of neighboring tokens, which leads to a high token autocorrelation [6] tendency. To address the challenges mentioned above, in this paper, we propose PerceiverS, a novel model that addresses the causal masking issue by incorporating Effective Segmentation. Additionally, PerceiverS employs Multi-Scale attention to mitigate the high token autocorrelation problem that arises from relying solely on long-range dependencies. Specifically, by adjusting the input sequence segmentation to start from the head segment with an effective causal mask and aggressively increasing the segment length up to the maximum input sequence length, we resolve the learning limitations caused by the causal mask in Perceiver AR [4]. Additionally, by incorporating Multi-Scale masks across multiple layers of cross-attention, the model considers both ultra-long and short-range attention simultaneously. This approach addresses the limitation in Perceiver AR, which focuses solely on long-range attention [4]. Different from Perceiver AR, our approach enhances symbolic music generation by effectively capturing both long-term structural dependencies and short-term expressive details. Through improved segmentation and multi-scale attention mechanisms, PerceiverS generates coherent, diverse music without relying on extensive structural annotations. Extensive experiments have been conducted to evaluate the performance of the proposed PerceiverS. The experimental results demonstrate an average 40% improvement in Overlap Area when measured against the original training dataset, highlighting a substantial advantage of our approach over Perceiver AR [4] in generating high-quality symbolic music."
https://arxiv.org/html/2411.08013v2,Investigating the Effectiveness of Explainability Methods in Parkinson’s Detection from Speech,"Speech impairments in Parkinson’s disease (PD) provide significant early indicators for diagnosis. While models for speech-based PD detection have shown strong performance, their interpretability remains underexplored. This study systematically evaluates several explainability methods to identify PD-specific speech features, aiming to support the development of accurate, interpretable models for clinical decision-making in PD diagnosis and monitoring. Our methodology involves (i) obtaining attributions and saliency maps using mainstream interpretability techniques, (ii) quantitatively evaluating the faithfulness of these maps and their combinations obtained via union and intersection through a range of established metrics, and (iii) assessing the information conveyed by the saliency maps for PD detection from an auxiliary classifier. Our results reveal that, while explanations are aligned with the classifier, they often fail to provide valuable information for domain experts.","Parkinson’s disease (PD) is a progressive neurodegenerative disorder primarily marked by the deterioration of dopaminergic neurons in the midbrain. This degeneration leads to a range of motor and non-motor symptoms, including tremors, bradykinesia, cognitive impairment, and depression [1, 2, 3]. Importantly, during the prodromal stages of PD, patients often start to exhibit speech impairments, which can serve as early indicators of the disease [4, 5]. Given the non-invasive, cost-effective, and automated nature of speech analysis, researchers have increasingly focused on this approach as a promising avenue for the early detection of Parkinson’s disease [6]. Despite substantial advances in PD classification using speech analysis, model interpretability remains underexplored. In clinical settings, explainable AI (XAI) is essential for providing clear, clinically relevant insights, crucial for the acceptance of automated systems in clinical trials. Many XAI techniques exist for interpreting model predictions, with post-hoc explanation methods among the most widely used [7]. Here, we focus on two different sets of approaches: Perturbation-based and Gradient-based post-hoc explanation methods. Perturbation-based methods assess feature importance by modifying input data, while gradient-based methods use gradients of predictions with respect to inputs [8]. Both approaches support local and global explanations, are model-agnostic, and offer valuable insights into the model’s behaviour. This paper systematically evaluates several key perturbation and gradient-based techniques to determine their effectiveness in highlighting PD-relevant speech features, aiming to enhance transparency and clinical utility in PD detection from speech. Our experimental results show that, although explanations are aligned with the classifier, they often fail to provide insights that are truly informative for domain experts. These methods may lack the level of interpretability required for practical use, emphasizing the need for more effective explainability approaches that connect model behavior with human understanding in specialized domains."
https://arxiv.org/html/2411.07751v1,SAV-SE: Scene-aware Audio-Visual Speech Enhancement with Selective State Space Model,"Speech enhancement plays an essential role in various applications, and the integration of visual information has been demonstrated to bring substantial advantages. However, the majority of current research concentrates on the examination of facial and lip movements, which can be compromised or entirely inaccessible in scenarios where occlusions occur or when the camera view is distant. Whereas contextual visual cues from the surrounding environment have been overlooked: for example, when we see a dog bark, our brain has the innate ability to discern and filter out the barking noise. To this end, in this paper, we introduce a novel task, i.e. Scene-aware Audio-Visual Speech Enhancement (SAV-SE). To our best knowledge, this is the first proposal to use rich contextual information from synchronized video as auxiliary cues to indicate the type of noise, which eventually improves the speech enhancement performance. Specifically, we propose the VC-S2E method, which incorporates the Conformer and Mamba modules for their complementary strengths. Extensive experiments are conducted on public MUSIC, AVSpeech and AudioSet datasets, where the results demonstrate the superiority of VC-S2E over other competitive methods. We will make the source code publicly available. Project demo page: https://AVSEPage.github.io/","In our daily living environments, speech signals are often distorted by various environmental background noises during their propagation. Speech enhancement (SE) is a task aiming at isolating the clean speech in the presence of noise interference, resulting in improved speech intelligibility and perceptual quality [1, 2, 3, 4]. It enables natural and effective Human-Robot Interaction (HRI) and plays a crucial role in various applications, such as hearing aids, mobile communication, automatic speech recognition [5, 6, 7], speaker verification [8], and speaker tracking [9, 10, 11]. These applications underscore the importance of SE in realistic scenarios. Traditional signal processing-based SE approaches, which are derived from the assumed properties on speech and noise, are incapable of suppressing highly non-stationary noise sources [12, 13, 14]. In the past decade, with the advent of deep learning technology and increased computational resources, supervised speech enhancement solutions has achieved great success [2]. Figure 1: Our proposed SAV-SE task where outputs from the audio and visual encoder are fused to refine and generate the enhanced audio. By incorporating the visual context from noise environments, it significantly enhances speech quality, particularly in situations where traditional audio-only techniques falter. Despite the significant strides made in the field, the challenge of noise reduction without inflicting artifacts on the speech signal persists, particularly in dynamic environments characterized by non-stationary and multi-source noise [15]. This difficulty is further compounded by the need to maintain the integrity of the speech signal, ensuring that the naturalness of the human voice is preserved. To address this challenge, researchers have been exploring cutting-edge signal processing methodologies and sophisticated machine learning paradigms. One promising solution involves the use of neural networks, which has demonstrated great capabilities in extracting features and separating signals from complex acoustic environments. A variety of network architectures are trained to learn the underlying patterns in noisy audio data, thus accomplishing the objective of speech enhancement [16]. Each of these models contributes unique strengths to the task of learning and generalizing from noisy audio data. For example, Multi-Layer Perceptrons (MLPs) are proficient in detecting intricate, non-linear data patterns, whereas Recurrent Neural Network (RNN) effectively manage the sequential dependencies in audio signals. Temporal Convolutional Network (TCN) excel in capturing long-range dependencies without suffering from the vanishing gradient problem that plagues standard RNN. The Transformer architecture, featuring self-attention, has transformed the field by allowing models to process any part of the input sequence, which is crucial for tasks involving widespread noise-speech relationships. The Mamba architecture [17], as the latest advancement, further extends the capabilities of noise reduction and speech enhancement. Researchers have increasingly acknowledged the importance of maintaining semantic, temporal, and spatial coherence between audio and video sources [18, 19]. This motivates attempts to use video information as a complement of audio input to recover details that are lost in audio-only scenarios. Existing Audio-Visual Speech Enhancement (AVSE) schemes often exploit temporal synchronized facial and lip movements to improve the clarity and perception of enhanced speech [20, 21, 22]. Despite outperforming audio-only SE systems, they are infeasible in many practical scenarios (e.g., outdoors or pandemic period) where human visual cues are not available. Moreover, inaccurate face or lip detection (e.g., in low-quality videos) may also result in degraded performance. In contrast, visual cues of environmental information, such as noise scenes or background objects emitting the noise, are easier to capture. It is more practical to use visual environmental cues to provide a valuable complement to speech enhancement. Thus, to fully leverage audio-visual information to enhance uni-modal learning, it is essential to consider these modality-specific attributes. In this paper, we introduce a novel AVSE framework, as illustrated in Figure 1, which uses visual information of the surrounding scenes as an auxiliary prompt to improve SE performance. Specifically, it addresses the limitations of current technologies, particularly in scenarios where an accurate capture of facial or lip information is not available. The contributions of this paper are summarized as follows: 1. We introduce a novel and more practical scene-aware AVSE task, namely SAV-SE. Unlike existing AVSE studies that rely primarily on visual facial and lip movements, this paper explores auxiliary visual contextual cues from the surrounding scenes to mitigate environmental background noise. 2. We are the first to explore selective State Space Model (SSM) for audio-visual speech enhancement. Specifically, we propose a Visual-prompting ConMamba for Scene-aware Speech Enhancement (VC-S2E), a novel approach that leverages audio-visual modalities to improve speech quality and intelligibility. Built upon innovative hybrid convolution-SSM architecture, ConMamba can capture both long-range global interactions and localized fine-grained feature patterns. 3. We comprehensively evaluate our proposed method across three widely used AV datasets. The results consistently confirm the superiority of our \text{VC-}\text{S}^{2}\text{E} over other competing methods in speech quality and intelligibility. Meanwhile, the visualization analysis illustrates that visual focal areas locate at the sounding object, demonstrating the contribution of visual scene information."
https://arxiv.org/html/2411.07547v1,AuscultaBase: A Foundational Step Towards AI-Powered Body Sound Diagnostics,"Auscultation of internal body sounds is essential for diagnosing a range of health conditions, yet its effectiveness is often limited by clinicians’ expertise and the acoustic constraints of human hearing, restricting its use across various clinical scenarios. To address these challenges, we introduce AuscultaBase, a foundational framework aimed at advancing body sound diagnostics through innovative data integration and contrastive learning techniques. Our contributions include the following: First, we compile AuscultaBase-Corpus, a large-scale, multi-source body sound database encompassing 11 datasets with 40,317 audio recordings and totaling 322.4 hours of heart, lung, and bowel sounds. Second, we develop AuscultaBase-Model, a foundational diagnostic model for body sounds, utilizing contrastive learning on the compiled corpus. Third, we establish AuscultaBase-Bench, a comprehensive benchmark containing 16 sub-tasks, assessing the performance of various open-source acoustic pre-trained models. Evaluation results indicate that our model outperforms all other open-source models in 12 out of 16 tasks, demonstrating the efficacy of our approach in advancing diagnostic capabilities for body sound analysis.","Auscultation, the clinical skill of listening to internal body sounds, is essential for diagnosing a variety of health conditions. By analyzing heart sounds, respiratory sounds, and bowel sounds, clinicians can derive critical insights into a patient’s physiological state, enabling the monitoring of conditions such as valvular heart disease, congenital heart defects, pneumonia, asthma, gastroenteritis, and intestinal obstruction. However, the efficacy of auscultation is largely contingent upon the clinician’s expertise and the acoustic range of the human ear, which limits its application in various clinical scenarios (Mangione & Nieman, 1997). Recently, the integration of artificial intelligence algorithms for sound analysis has emerged as a promising approach to enhance diagnostic accuracy (Chen et al., 2023; Kong et al., 2020; Oletic & Bilas, 2017; Zhang et al., 2024; Sitaula et al., 2022). Despite the progress made with existing methodologies, several challenges persist. First, while some approaches utilize general acoustic techniques to extract audio features from specialized stethoscope recordings, they may not adequately capture the pathological variations in body sounds (Chen et al., 2023; Guo et al., 2023; Oletic & Bilas, 2017; Jung et al., 2021). With advancements in unsupervised training, neural networks have been shown to effectively learn high-quality representations directly from audio data (Baevski et al., 2020; Huang et al., 2022; Hsu et al., 2021b; Wu et al., 2023). Some methods have proposed the use of these pretrained audio models for feature extraction for body sound (Koike et al., 2020; Panah et al., 2023). However, existing pretrained models are typically trained on general audio datasets like AudioSet (Gemmeke et al., 2017) and Librispeech (Panayotov et al., 2015) which contain sounds—such as music and speech—that significantly differ from those collected by stethoscopes, leading to substantial domain gaps that render them unsuitable for auscultation feature extraction. Currently, there are publicly available datasets for the three types of body sounds and the details are shown in Tabel 1. Zhang et al. (2024) utilized publicly available respiratory sounds captured by stethoscopes alongside cough sounds recorded by microphones to develop respiratory sound representations. Baur et al. (2024) identified and utilized cough sounds, respiratory sounds, and laughter collected from YouTube to construct health-related acoustic representations. However, these methodologies are not specifically designed for auscultation and may not effectively adapt to the diverse range of body sounds captured by stethoscopes. To address these critical gaps, we introduce AuscultaBase, a foundational framework designed to transform body sound diagnostics through advanced data integration and contrastive learning techniques. AuscultaBase offers a comprehensive solution that not only supports the diagnostic capabilities of primary care providers but also enables scalable, high-quality auscultation analysis, particularly for clinicians operating in high-volume, low-resource environments. Our framework is built upon three core pillars: • AuscultaBase-Corpus: A Large-Scale, Multi-Source Body Sound Database. This extensive corpus combines 10 publicly accessible datasets and one private dataset, covering a diverse array of body sounds—heart, respiratory, and gastrointestinal—for a total of 322.4 hours of training recordings. The AuscultaBase-Corpus provides a rich, representative data foundation to train and evaluate diagnostic models across multiple auscultation systems. • AuscultaBase-Model: A Foundational Diagnostic Model for Body Sounds. We developed a robust foundational model tailored for body sound diagnostics using contrastive learning techniques on the AuscultaBase-Corpus. This model captures nuanced acoustic patterns, showing significant improvements in diagnostic accuracy over conventional acoustic models. It offers a unified approach to analyzing different body sounds, setting a new standard for accuracy and reliability in auscultation. • AuscultaBase-Bench: A Comprehensive Benchmark for Auscultation Evaluation. To facilitate rigorous assessment, AuscultaBase-Bench provides a structured evaluation framework across three primary diagnostic tasks—abnormality detection, disease classification, and activity recognition—spanning 16 sub-tasks that reflect the diverse and complex demands of clinical auscultation. This benchmark enables consistent, standardized performance comparisons for current and future auscultation models."
https://arxiv.org/html/2411.07439v1,"Music Discovery Dialogue Generation Using
Human Intent Analysis and Large Language Models","A conversational music retrieval system can help users discover music that matches their preferences through dialogue. To achieve this, a conversational music retrieval system should seamlessly engage in multi-turn conversation by 1) understanding user queries and 2) responding with natural language and retrieved music. A straightforward solution would be a data-driven approach utilizing such conversation logs. However, few datasets are available for the research and are limited in terms of volume and quality. In this paper, we present a data generation framework for rich music discovery dialogue using a large language model (LLM) and user intents, system actions, and musical attributes. This is done by i) dialogue intent analysis using grounded theory, ii) generating attribute sequences via cascading database filtering, and iii) generating utterances using large language models. By applying this framework to the Million Song dataset, we create – LP-MusicDialog, a Large Language Model based Pseudo Music Dialogue dataset, containing over 288k music conversations using more than 319k music items. Our evaluation shows that the synthetic dataset is competitive with an existing, small human dialogue dataset in terms of dialogue consistency, item relevance, and naturalness. Furthermore, using the dataset, we train a conversational music retrieval model and show promising results.111Our code is available at https://github.com/seungheondoh/lp-music-dialog/","In recent years, conversational systems have emerged as a promising solution to enhance user experience in various domains [1, 2, 3, 4], including conversational music retrieval and recommendation [5, 6]. The goal of a conversational music system is to assist users in finding their desired music through dialogues. Such a system should possess three key capabilities: i) to understand the intents and musical needs of users from their queries expressed in natural language, ii) to generate responses and facilitate human-like interaction, iii) to find music that aligns with the user’s preferences by taking previous dialogues into account. Currently, the primary challenge of developing a conversational music retrieval system is the scarcity of large-scale public datasets. Chaganty et al. [5] introduce the Conversational Playlist Curation Dataset (CPCD). This crowd-sourced dataset comprises human-to-human dialogues that simulate the process of music discovery. However, as it relies on a manual process, the dataset is small and exhibits biases from the music streaming platforms used by the recommenders. To address this problem, Leszczynsk et al. [6] propose a dialogue generation framework through random walks in the music-text joint embedding space and dialogue inpainting [7]. However, this approach requires a high-quality music-text joint embedding and needs to use template-based system responses as input. As a result, the system’s responses are always composed of limited format utterances, leading to low naturalness in human evaluation. In this paper, we introduce a framework for generating human-like music discovery dialogues using intent and a large language model (LLM). The proposed framework is based on the existing method [6], but we address their limitations by employing cascading music database filtering instead of a joint embedding and extensive intent analysis for naturalness. Using the grounded theory approach [8], we analyze a dataset of human music discovery dialogues and develop taxonomies for user intents, system actions, and musical attributes relevant to the task of music discovery. Furthermore, we introduce a model-free attribute sequence generation by applying cascading filtering to a multi-label music annotation database. Finally, we synthesize music discovery dialogues through an LLM using the created attribute sequences and human intents/actions. Our contributions are threefold: First, we analyze music discovery dialogues and propose a taxonomy. Second, we introduce the LP-MusicDialog dataset, a large-scale synthetic dialogue dataset created using human intent and an LLM. Third, we present extensive objective and subjective evaluations to demonstrate the effectiveness of LLM-based pseudo-music dialogues."
https://arxiv.org/html/2411.07428v1,"Just label the repeats 
for in-the-wild audio-to-score alignment","We propose an efficient workflow for high-quality offline alignment of in-the-wild performance audio and corresponding sheet music scans (images).111Video examples: https://bit.ly/jltr-ismir2024 Code: https://github.com/irmakbky/jltr-alignment Corresponding author: Irmak Bukey ¡ibukey@cs.cmu.edu¿ Recent work on audio-to-score alignment extends dynamic time warping (DTW) to be theoretically able to handle jumps in sheet music induced by repeat signs—this method requires no human annotations, but we show that it often yields low-quality alignments. As an alternative, we propose a workflow and interface that allows users to quickly annotate jumps (by clicking on repeat signs), requiring a small amount of human supervision but yielding much higher quality alignments on average. Additionally, we refine audio and score feature representations to improve alignment quality by: (1) integrating measure detection into the score feature representation, and (2) using raw onset prediction probabilities from a music transcription model instead of piano roll. We propose an evaluation protocol for audio-to-score alignment that computes the distance between the estimated and ground truth alignment in units of measures. Under this evaluation, we find that our proposed jump annotation workflow and improved feature representations together improve alignment accuracy by 150\% relative to prior work (33\%\to 82\%).","Sheet music has been used as a primary means of communicating musical ideas for centuries. Accordingly, sheet music is a profoundly important modality for MIR, not only because of the breadth of musical knowledge and history contained within, but also because sheet music constitutes a vital interface between MIR systems and musicians. However, while multimodal MIR systems are rapidly improving at tasks like music transcription [3, 4, 5, 6] and controllable generation [7, 8, 9], these systems typically operate on MIDI as a symbolic music format. This may be less useful to musicians, e.g., a musician might prefer transcription systems to output sheet music instead of MIDI. We conjecture that the scarcity of fine-grained alignment data linking sheet music to corresponding performance audio is a key bottleneck to incorporating sheet music into multimodal MIR systems. Alignments allow multimodal MIR data to be segmented into input-output chunks of tractable length for training models, and the lack of sheet music alignments may partially explain why sheet music is mostly overlooked. Moreover, alignments have practical utility outside of multimodal MIR, e.g., they may be used by musicians to practice along with pre-recorded accompaniments. Unfortunately, collecting alignments is deceptively tricky. For example, one could have a musician use a touch screen to point to the current location in sheet music while listening to a recording in real time. However, their tracking may be imprecise (due to expressive performance timing) and lack non-obvious details that are essential for segmentation (bar line locations, number of active staves). In this work, we investigate the task of alignment of offline in-the-wild performance audio and corresponding sheet music scans (images), with a long-term goal of aligning large corpora of sheet music and performance recordings at scale. Much of the past work on audio-to-score alignment make at least one of several common assumptions that inhibit their practicality for collecting aligned data at scale: (i) the presumed availability of digital scores like MIDI or MusicXML as opposed to sheet music images [10, 11, 12, 13, 14], (ii) the alignment of MIDI performances or synthesized audio instead of real audio recordings [15, 12, 16], (iii) limitations in instrument diversity, commonly piano only [10, 16, 17], or (iv) dependence on time-consuming human annotation [18, 19]. Here we propose an audio-to-score alignment procedure that makes none of these assumptions, potentially offering a path forward for large-scale data collection. Most closely related to our approach is that of Shan et al. [16, 17], who examine offline alignment of in-the-wild piano sheet music images and performance recordings by aligning feature representations derived from the score and audio via MIR methods. In addition to operating on more diverse ensembles, our work has two primary distinctions: (1) we take a different approach to handling jumps in scores, and (2) we modify their feature representations. A key challenge in audio-to-score alignment is handling inter-measure jumps in scores induced by repeat signs. Shan et al. [16, 17] propose extensions to DTW that are capable of automatically handling jumps. Here we propose a pragmatic alternative: a workflow and interface that allows humans to quickly annotate jumps, and a system that incorporates these jump labels. We find that this approach can yield much higher-quality alignments than the automatic one, costing only seconds of annotator time. We additionally extend the bootleg score feature representations used by Shan et al. [17], first proposed by Yang et al. [1]. Creating a bootleg score involves detecting noteheads and staff lines to produce a simple binary representation of a score that is conducive to alignment. We find that the use of measure bounding box detection as a preprocessing step improves the quality of underlying notehead and staff line detection algorithms. Additionally, motivated by findings in [2], we find that using raw onset probabilities predicted by a music transcription model as the audio feature representation produces higher quality alignments than using the MIDI transcriptions—see Figure 1 for a summary. Motivated by our long-term goals of bringing sheet music into multimodal MIR, we also propose a new measure-aware evaluation scheme for comparing alignments. We speculate that measure-level alignment granularity is necessary for tractable training of multimodal MIR systems in the short term and that human perception of alignment quality is tied to measures. Accordingly, we prescribe new measure-aware alignment metrics for this task, such as an accuracy metric which reports the proportion of time where the estimated alignment is within a half measure radius of the ground truth alignment. On a small but diverse dataset of in-the-wild sheet music and aligned audio [18], we observe that our proposed system achieves an accuracy of 120\% relative to that of Shan et al. (33\%\to 72\%). By providing repeat labels, we improve the absolute accuracy of our system from 20\%\to 83\% on a subset of pieces that have repeats. Our work makes the following contributions: • A system capable of high-quality in-the-wild alignment of sheet music images and performance audio. • A pragmatic workflow we call Just Label The Repeats that further improves alignment accuracy. • An interface that enables rapid jump annotation."
https://arxiv.org/html/2411.07772v1,Automatic Album Sequencing,"Album sequencing is a critical part of the album production process. Recently, a data-driven approach was proposed that sequences general collections of independent media by extracting the narrative essence of the items in the collections. While this approach implies an album sequencing technique, it is not widely accessible to a less technical audience, requiring advanced knowledge of machine learning techniques to use. To address this, we introduce a new user-friendly web-based tool that allows a less technical audience to upload music tracks, execute this technique in one click, and subsequently presents the result in a clean visualization to the user. To both increase the number of templates available to the user and address shortcomings of previous work, we also introduce a new direct transformer-based album sequencing method. We find that our more direct method outperforms a random baseline but does not reach the same performance as the narrative essence approach. Both methods are included in our web-based user interface, and this—alongside a full copy of our implementation—is publicly available at https://github.com/dylanashley/automatic-album-sequencing","Album sequencing is the process of taking a music album and ordering it so that listening to it in that order produces a desired emotional response in the listener. Despite its importance in producing an impactful music album, album sequencing has received comparatively little attention from the artificial intelligence community. Our previous research [1] introduced a way to compress different kinds of media down into an ultra-low dimensional representation. This representation captures their relevancy to the overarching story induced by the ordering of the collection they belonged to, i.e., their narrative essence. This is accomplished by using neural networks and contrastive learning [2, 3]. Then, evolutionary algorithms are used to learn a set of template curves and a novel curve-fitting algorithm to fit the narrative essence of new media collections to these template curves. The above was principally done with music albums from the FMA dataset [4], though it is shown that this applies to other forms of media as well. There are two key issues with our previous work. First, our previous method requires knowledge of advanced machine learning techniques, making it inaccessible to many people who perform album sequencing. Second, it requires a complex pipeline with (1) a neural network to extract the narrative essence followed by (2) a separate evolutionary algorithm to learn a set of templates and then (3) a fitting algorithm to produce a final ordering. This is a highly complex and particularly problematic setup that does not allow information like the genre of an album to flow between the narrative essence and the final ordering, resulting in genre-agnostic templates. Here, we address both of the aforementioned issues. To address the latter issue, we introduce a new approach that replaces the full pipeline with a single Transformer [5, 6, 7]. While this does not outperform the more complicated pipeline proposed in our previous work, the new simpler pipeline still outperforms a random baseline, making it useful for automatic album sequencing. Next, to address the former issue, we implement and release a dedicated user-friendly web-based interface that allows a less technically inclined user to run both the narrative essence-based and the new simplified album sequencing approaches on the user’s own music. We release this interface alongside a complete implementation of our approach publicly at https://github.com/dylanashley/automatic-album-sequencing In summary, our contributions are as follows: (1) We introduce a new direct method to perform automatic album sequencing. (2) We show that, despite the simpler pipelineethod outperforms a random baseline. (3) We release a web-based user interface tool that makes automatic album sequencing accessible to a less technical audience."
https://arxiv.org/html/2411.07650v1,"Understanding Audiovisual Deepfake Detection:
Techniques, Challenges, Human Factors
and Perceptual Insights","Deep Learning has been successfully applied in diverse fields, and its impact on deepfake detection is no exception. Deepfakes are fake yet realistic synthetic content that can be used deceitfully for political impersonation, phishing, slandering, or spreading misinformation. Despite extensive research on unimodal deepfake detection, identifying complex deepfakes through joint analysis of audio and visual streams remains relatively unexplored. To fill this gap, this survey first provides an overview of audiovisual deepfake generation techniques, applications, and their consequences, and then provides a comprehensive review of state-of-the-art methods that combine audio and visual modalities to enhance detection accuracy, summarizing and critically analyzing their strengths and limitations. Furthermore, we discuss existing open source datasets for a deeper understanding, which can contribute to the research community and provide necessary information to beginners who want to analyze deep learning-based audiovisual methods for video forensics. By bridging the gap between unimodal and multimodal approaches, this paper aims to improve the effectiveness of deepfake detection strategies and guide future research in cybersecurity and media integrity.","The proliferation of smart digital devices such as mobile phones, laptops, tablets, and other digital gadgets, coupled with the accessibility of social media platforms, has promoted the exponential growth of multimedia content (images, videos, and audio) on the internet. This growth is further fueled by technological advances [1], including various deep generative networks [2] [3]. However, this accessibility heightens the need for caution because it can lead to the prevalence of disinformation. Despite this, many people still stick to the trend of the antiquated phrase “seeing is believing” and share multimedia content without considering its authenticity or verifying its digital integrity. Deepfake technology, or sophisticated Artificial Intelligence (AI) models, enable deep learning (DL) tools to manipulate media (images, videos, and audio) to generate hyper-realistic fake content that deceives viewers. Deepfake is AI-generated media that has been deceptively altered by superimposing a source face in a video onto a target face, manipulating the speech in an audio clip, or both. The vast amount of data available online in the form of images, videos, and audio to train such models makes detecting such forgeries increasingly challenging. The impact of deepfakes is critical because we still trust photographic and audio recording evidence. The emergence of realistic and subtle production tools makes fake content incredibly believable and harder to distinguish from genuine content [4]. The rapid spread of harmful and uncontrolled content from fake media has serious imminent impacts and reduces trust in journalism and news providers [5] [6]. Deepfake media content can be exploited to fuel political or religious tensions between countries [7], spread misleading information or rumors between political parties [5] [8], deceive the public [5], engaging in revenge porn [8], defame celebrities [8], promote fraud and identity theft [9], and create political chaos or publicity in a campaign [10]. Generative Adversarial Networks (GAN) [2] and Variational Autoencoders (VAE) [3] are sophisticated DL models for generating counterfeit content. In GAN, the generator network and the discriminator network are the two main components, and these two networks are opposed to each other. The generator aims to generate plausible data, while the discriminator determines the real data from the fake data generated by the generator. Similarly, VAE is an unsupervised learning method consisting of encoder and decoder architectures. VAE is used to create high-quality, hyper-realistic fake content by merging and/or superimposing existing media (images or videos) onto source media for the purpose of deception. Currently, AI-synthesized videos are mainly divided into three different generation types [11] [12]. (1) Head puppetry/puppet master is a counterfeit video generation technique based on the target person animating like a puppet. (2) Face swap aims to generate a video of the target person by swapping the target person’s face with that of the source person while retaining the same facial expression as the target person. (3) Lip-sync is another deepfake video generation method whose main goal is to transform a person’s lips to be synchronized or consistent with the target audio. This technique tends to manipulate the lip region in such a way that the target of the attack appears to be saying things they never said in reality. In the past few years, immense progress in the field of automatic video editing and a great interest in face manipulation techniques have been noticed. Advances in manipulation tools and open-source codes allow even naive users to use deepfake technology like an expert in a few simple steps. This technological advancement has a wide range of positive applications in the fields of visual effects, photography, education, film industry, virtual reality, video games, cinema, and entertainment. However, it also poses significant challenges in terms of authenticity verification and prevention of malicious use. To overcome these challenges, researchers have made many attempts and proposed DL-based unimodal forgery detection methods [13, 14, 15, 16, 17]. Figure 1: Volume of research into audiovisual deepfakes between 2017 and 2023. The detection of visual manipulation in videos has long been the focus of researchers, while the identification of audio forgeries has often been overlooked. Recently, however, the trend of sound manipulation has grown rapidly along with visual alterations, leading to bimodal fabrication that enhances the authenticity of fake content and makes detection difficult [18, 19, 20, 21]. Fig. 1 highlights the research community’s growing interest and concern in audiovisual deepfakes. The number of publications on audiovisual deepfakes has increased significantly in recent years, demonstrating both beneficial progress and growing concerns. The integration of multimodality is proven to be beneficial in various research fields [22, 23, 24]. Consequently, researchers have used various DL techniques that exploit audio and visual features for video forgery detection. Nonetheless, existing media forensics research is lacking in investigations that analyze methods for generating and detecting video deepfakes using audio and visual modalities. Table I lists an overview of relevant studies. Our study was strongly motivated by the lack of attention paid to audiovisual deepfakes in surveys, highlighting the urgent need to focus research on audiovisual deepfakes, including their generation, how to mitigate their harmful effects, and a summary of existing audiovisual deepfake detection methods. TABLE I: Comparison of survey studies related to deepfake detection. Reference Year Contribution Verdoliva [25] 2020 A discussion of video deepfakes from a forensic perspective, with an emphasis on the limitations of current forensic detection methods. Mirsky et al. [26] 2021 An in-depth analysis of field-specific generation techniques and a brief discussion of detection methods. Yu et al. [27] 2021 A detailed analysis of forged video synthesis and detection techniques, with a focus on face manipulation. Rana et al. [28] 2022 A comprehensive review of deepfake detection methods proposed during 2018-2020. Nguyen et al. [29] 2022 A comprehensive overview of deepfake generation and detection techniques and a discussion of challenges and future research directions in the field. Masood et al. [30] 2023 An analysis of the generation and detection of audio and visual deepfakes and a discussion of datasets. Mubarak et al. [31] 2023 An analysis of audio, visual, and text-based deepfakes, with a focus on detection methods. Figure 2: Taxonomy of deepfakes. Generally speaking, as shown in Fig. 2, there are four types of deepfakes, namely text deepfakes, audio deepfakes, visual deepfakes, and audiovisual deepfakes. Audiovisual deepfakes are a combination of acoustic and visual manipulation that can enhance manipulated videos to make them look more believable, and have received a lot of attention in recent years. This study specifically provides an in-depth review of the latest audiovisual deep learning solutions to improve the detection of challenging video deepfakes. To the best of our knowledge, we are the first to perform a comprehensive analysis of existing DL-based methods that exploit audio and visual manipulations in videos for automatic deepfake detection. Our important contribution also includes a comprehensive discussion of publicly available datasets relevant to this task. The main contributions of our work are as follows: • We provide an unprecedented survey that systematically analyzes key detection and generation methods for audiovisual deepfakes, with a special emphasis on automatic video deepfake detection methods. • We highlight the challenges, limitations, and human perception in the field of audiovisual deepfake detection. Furthermore, we outline research directions for future developments in this field. • We summarize and present publicly available datasets that can be used to train multimodal/audiovisual deepfake detectors. The remainder of this paper is organized as follows. Section II introduces different types of deepfakes. Section III discusses video deepfake detection methods. Section IV classifies detection methods that exploit visual and acoustic streams. In Section V, we review publicly available datasets for audiovisual deepfake detection. Section VI presents performance metrics and evaluation. Section VII examines human perception of audiovisual deepfakes. Section VIII discusses several aspects of the deepfake challenge and potential research directions. Finally, Section IX concludes this survey."
https://arxiv.org/html/2411.07607v1,CJST: CTC Compressor based Joint Speech and Text Training for Decoder-Only ASR,"CTC compressor can be an effective approach to integrate audio encoders to decoder-only models, which has gained growing interest for different speech applications. In this work, we propose a novel CTC compressor based joint speech and text training (CJST) framework for decoder-only ASR. CJST matches speech and text modalities from both directions by exploring a simple modality adaptor and several features of the CTC compressor, including sequence compression, on-the-fly forced peaky alignment and CTC class embeddings. Experimental results on the Librispeech and TED-LIUM2 corpora show that the proposed CJST achieves an effective text injection without the need of duration handling, leading to the best performance for both in-domain and cross-domain scenarios. We also provide a comprehensive study on CTC compressor, covering various compression modes, edge case handling and behavior under both clean and noisy data conditions, which reveals the most robust setting to use CTC compressor for decoder-only models.","With the great success of large language models (LLM) [1, 2, 3, 4], decoder-only architectures have been widely adopted for various speech applications [5, 6, 7]. To integrate speech into decoder-only models, an audio encoder is commonly used to extract high-level representations. The extracted acoustic embeddings can be directly fed into the decoder model in the continuous space, or can be discretized first to allow the decoder-only model operate with discrete audio tokens [8, 9] in the same way as text tokens. While the latter has become a popular research area recently, this work focus on the continuous representations for the task of automatic speech recognition (ASR). More specifically, we aim to improve decoder-only ASR models of production-deployable size without using external LMs. One common way of feeding continuous acoustic embeddings into a decoder-only model is via an adaptor. A simple and widely-used adaptor is a time reduction layer followed by a linear projection layer [5, 10]. Another approach is to use connectionist temporal classification (CTC) [11] to compress the sequence length based on CTC probabilities, which is referred to as CTC compressor. The concept of CTC compressor was originally proposed for speech translation [12] using attention-based encoder-decoder models [13, 14]. An additional CTC layer was introduced in the encoder to compress the sequence length by averaging neighbouring frames of the same CTC predictions. This idea was further explored in [6] for speech translation using decoder-only models, where the compression can also be done by removing blank frames. Various extensions of the CTC compressor were also explored, e.g., for fully formatted ASR transcription [15] and for streaming ASR [7]. Joint speech and text training is another important research topic to improve ASR performance, especially when no external LM can be used. An effective text injection would require certain level of speech and text modality matching. This can be implicit at the model output level by applying the same objective and target to both speech and text input, such as JOIST [16] and textogram [17]. This can also be explicit at the model input level by matching speech-text representations, such as MAESTRO [18]. All these approaches were proposed for recurrent neural network transducer (RNN-T) [19] models, aiming to bring the text modality closer to the speech one with simple or sophisticated duration handling. For decoder-only models, this modality aligning is usually not studied for ASR, e.g., implicit aligning in the discrete space for pretraining [20, 21] or input space aligning for response matching [22]. In this work, we propose a novel CTC compressor based joint speech and text training (CJST) framework for decoder-only ASR. More specifically, we propose to utilize the CTC compressor to match speech and text representations from both directions. This is done by combining a simple modality adaptor with several features of the CTC compressor, including sequence compression, on-the-fly forced peaky alignment and CTC class embeddings. Without the need of duration consideration, the proposed CJST achieves effective text injection for both in-domain and corss-domain scenarios, leading to consistent improvements. We also provide a comprehensive study of the CTC compressor, covering various compression modes, edge case handling and behavior under both clean and noisy data conditions, which reveals the most robust setting to use CTC compressor for decoder-only models."
https://arxiv.org/html/2411.07517v1,"SoundSil-DS: Deep Denoising and Segmentation of 
Sound-field Images with Silhouettes","Development of optical technology has enabled imaging of two-dimensional (2D) sound fields. This acousto-optic sensing enables understanding of the interaction between sound and objects such as reflection and diffraction. Moreover, it is expected to be used an advanced measurement technology for sonars in self-driving vehicles and assistive robots. However, the low sound-pressure sensitivity of the acousto-optic sensing results in high intensity of noise on images. Therefore, denoising is an essential task to visualize and analyze the sound fields. In addition to denoising, segmentation of sound and object silhouette is also required to analyze interactions between them. In this paper, we propose sound-field-images-with-object-silhouette denoising and segmentation (SoundSil-DS) that jointly perform denoising and segmentation for sound fields and object silhouettes on a visualized image. We developed a new model based on the current state-of-the-art denoising network. We also created a dataset to train and evaluate the proposed method through acoustic simulation. The proposed method was evaluated using both simulated and measured data. We confirmed that our method can applied to experimentally measured data. These results suggest that the proposed method may improve the post-processing for sound fields, such as physical model-based three-dimensional reconstruction since it can remove unwanted noise and separate sound fields and other object silhouettes. Our code is available at https://github.com/nttcslab/soundsil-ds.","Sound is one of the most important cues to understanding scenes as well as vision. For example, self-driving cars and assistive robots are equipped with ultrasonic sonars and microphones, which are used to gather information about their surroundings. Recently, research has been conducted on converting sound information into vision information and vice versa. Lindell et al. have proposed an acoustic non-line-of-sight imaging method for resolving three-dimensional object shapes hidden around corners through acoustic echoes [17]. Davis et al. have proposed a method with which sound waves are recovered through object vibrations captured using a high-speed camera [6]. Sheinin et al. have proposed a method of sensing sound at high speeds through object-surface vibrations [20]. These studies demonstrated the potential of capturing sound as images, paving the way for further advancements in the field. Figure 1: Conceptual diagram of proposed method. (a) Experimental setup for optical sound measurement, which is microphone-free sound measurement device. (b) Conceptual diagram. Sound field with interacting objects is captured as images with high-speed camera. Visualized images are converted to denoised and segmentation images with a DNN. A sound-visualization technique involving directly capturing and visualizing the density variations in air caused by sound has been proposed [27]. Such acousto-optic sensing can capture sound without any microphones by observing modulations of the phase of light passing through sound fields. By using high-speed cameras as sensors, it becomes possible to create visual representations of invisible sound waves as images [5, 22]. However, a significant challenge remains: the phase modulation of light induced by sound is extremely small, leading to a high level of noise in the measured images. Ishikawa et al. [4] have proposed deep sound-field denoiser (DSFD), the first denoiser to use deep neural networks (DNNs) for noise reduction in sound-field images, demonstrating that DNN-based methods achieve superior denoising performance compared with conventional filtering methods. With the optical technology and signal processing method, we can understand the nature of sound propagation. Therefore, the interaction between sound and objects, such as reflections and diffractions, can be visualized using acousto-optic sensing. To analyze such interactions between sound and objects, both denoising and segmentation of the sound field and object regions on visualized images should be done simultaneously. We propose a method for simultaneous denoising and segmentation of sound-field images including object silhouettes. A conceptual diagram is presented in Fig. 1. With this method, optically measured sound-field images with object silhouettes are denoised and segmented using a DNN. The sound field on the laser path is captured with a high-speed camera. The area where the object blocks the laser light can be visualized as noisy silhouettes in the visualized images. The visualized images are converted to denoised and segmentation images with the DNN. The DNN is constructed based on the state-of-the-art (SOTA) denoising network, which also has the potential for per-pixel feature segmentation. We created a dataset with acoustic simulation since there is no dataset that includes acoustic scattering by objects. Denoising and segmentation are expected to (1) enable analysis of the propagation, reflection, and diffraction of sound waves in space and (2) be used as an advanced measurement technology for sonars in self-driving vehicles and assistive robots. The contributions of our work are summarized as follows: • We propose a method for simultaneous denoising and segmentation of sound-field images with silhouettes. • We created a dataset considering acoustic scattering caused by various shapes of objects. • We confirmed that the proposed method performed effectively on denoising and segmentation tasks."
https://arxiv.org/html/2411.07364v1,AEROMamba: An efficient architecture for audio super-resolution using generative adversarial networks and state space models,"Audio super-resolution aims to enhance low-resolution signals by creating high-frequency content. In this work, we modify the architecture of AERO (a state-of-the-art system for this task) for music super-resolution. SPecifically, we replace its original Attention and LSTM layers with Mamba, a State Space Model (SSM), across all network layers. Mamba is capable of effectively substituting the mentioned modules, as it offers a mechanism similar to that of Attention while also functioning as a recurrent network. With the proposed AEROMamba, training requires 2-4x less GPU memory, since Mamba exploits the convolutional formulation and leverages GPU memory hierarchy. Additionally, during inference, Mamba operates in constant memory due to recurrence, avoiding memory growth associated with Attention. This results in a 14x speed improvement using 5x less GPU. Subjective listening tests (0 to 100 scale) show that the proposed model surpasses the AERO model. In the MUSDB dataset, degraded signals scored 38.22, while AERO and AEROMamba scored 60.03 and 66.74, respectively. For the PianoEval dataset, scores were 72.92 for degraded signals, 76.89 for AERO, and 84.41 for AEROMamba.","Audio super-resolution is a technique analogous to what is known in signal processing literature as bandwidth extension [1], whose goal is to reconstruct the upper spectral content of a low-resolution signal. Since a bandlimited audio signal usually sounds muffled, higher-resolution audio yields a better listening experience, in general [1]. Since the 19th-century invention of sound recording devices [2], audio signals have been widely used in communications and entertainment. Technology has evolved to meet specific application requirements, with telephony prioritizing intelligibility and general audio devices focusing on fidelity [3]. High-fidelity systems must cover at least the human auditory range of 20 Hz to 20 kHz for tones [4], though analog audio may face limitations and media degradation affecting this content [5]. In digital audio, compression lowers transmission and storage costs. Decimation, which discards samples, needs low-pass filtering to prevent aliasing and reduce the signal’s maximum frequency. Lossy audio coding, such as MP3 [6], modifies frequency content based on a psychoacoustic model, mostly affecting high frequencies and sometimes reducing bandwidth. Audio super-resolution is useful in scenarios requiring mitigation of these issues. Signal processing-based bandwidth extension methods included techniques such as nonlinear devices with linear filtering [1], source-filter modeling [7], codebook mapping [8], and spectral-band replication [9]. In the past few years, solutions based on deep neural networks (DNNs) became the state of the art in audio super-resolution, ranging from pure feedforward networks that operate on raw waveforms [10] or in the spectral domain [11] to generative solutions using Generative Adversarial Networks (GANs) [12, 13] and, more recently, Diffusion Models (DMs) [14, 15, 16, 17]. The choice of using DMs instead of GANs is generally justified by training instabilities, suboptimal mode coverage, and lack of explainability of GANs, while DMs are modeled by statistical physics [18]. Even though efficient DMs [19] constitute an active area of research, their Markov Chain-based sampling requires sequential inference, which makes parallelization difficult and sample generation slower compared to GANs. Additionally, narrow mode coverage is only problematic when diverse data generation is needed, which may not necessarily be the case for audio enhancement. In this context, this work proposes improvements to the state-of-the-art AERO [13] architecture for super-resolution by replacing its Attention and LSTM layers with Mamba [20], a State Space Model (SSM) created for efficient sequence modeling which has shown promising results when used in speech enhancement [21]. The advantages of this approach are significant: training requires 2x to 4x less GPU memory, and inference runs with a 14x speed gain using 5x less GPU, all with an increase in audio quality, as demonstrated by the listening tests performed."
https://arxiv.org/html/2411.07186v1,"NatureLM-audio: an Audio-Language 
Foundation Model for Bioacoustics","Large language models (LLMs) prompted with text and audio represent the state of the art in various auditory tasks, including speech, music, and general audio, showing emergent abilities on unseen tasks. However, these capabilities have yet to be fully demonstrated in bioacoustics tasks, such as detecting animal vocalizations in large recordings, classifying rare and endangered species, and labeling context and behavior—tasks that are crucial for conservation, biodiversity monitoring, and the study of animal behavior. In this work, we present NatureLM-audio, the first audio-language foundation model specifically designed for bioacoustics. Our carefully curated training dataset comprises text-audio pairs spanning a diverse range of bioacoustics, speech, and music data, designed to address the challenges posed by limited annotated datasets in the field. We demonstrate successful transfer of learned representations from music and speech to bioacoustics, and our model shows promising generalization to unseen taxa and tasks. Importantly, we test NatureLM-audio on a novel benchmark (BEANS-Zero) and it sets the new state of the art (SotA) on several bioacoustics tasks, including zero-shot classification of unseen species. To advance bioacoustics research, we also open-source the code for generating training and benchmark data, as well as for training the model111Demo page: https://earthspecies.github.io/naturelm-audio-demo/ The code will be open-sourced and available shortly..","Bioacoustics, the study of sound production and reception in animals, aims to understand animal behavior (Fischer et al., 2013), monitor biodiversity (Stowell, 2022), and model the mechanisms of sound production and reception used in animal communication (Bradbury & Vehrencamp, 1998). It plays a vital role in conservation and ecological research, as animal vocalizations provide critical insights into ecosystem health, species interactions, and population dynamics. By enabling the detection of endangered species and tracking migration patterns, bioacoustic research directly contributes to biodiversity monitoring and conservation efforts (Rutz et al., 2023; Stevens et al., 2024). In recent years, machine learning has taken on an increasingly pivotal role in bioacoustic research. Beyond its applications in large-scale ecological monitoring, machine learning has also opened up new frontiers in the study of animal communication, enabling discoveries like the ability of marmosets (Oren et al., 2024), dolphins (King & Janik, 2013), and elephants (Pardo et al., 2024) to use specialized vocalizations to label their conspecifics. Yet, because of obvious data collection and annotation difficulties, these studies often rely on strongly labeled small datasets (Stowell, 2022) and thus require careful statistical analysis to measure the significance of results and avoid over-fitting. At the same time, large volumes of unannotated bioacoustics data are recorded daily, particularly through passive acoustic monitoring (PAM, Dufourq et al. (2021)) or citizen science platforms e.g. Xeno-canto (Vellinga & Planqué, 2015)). There is thus a growing need for machine learning tools capable of performing tasks such as detection, classification, and annotation on these data at scale. The recent successes of large scale artificial intelligence models in various domains (e.g. natural language processing, vision, games) also point to the possibility of leveraging these huge volumes of raw data to learn accurate and generalizable representations of bioacoustics signals (Ghani et al., 2023; Boudiaf et al., 2023). Existing bioacoustics machine learning models are typically designed for specific species or tasks (Dufourq et al., 2021; Kahl et al., 2021; Cauzinille et al., 2024), showing limited generalizability beyond their predefined scope. Many traditional studies rely on small datasets focused on a few species and individuals, validating results through statistical measures despite over-fitting risks. Newer models such as BirdNET (Kahl et al., 2021) and Perch (Ghani et al., 2023) perform well in specific tasks such as bird classification but still generalize poorly outside avian species. Recently, foundation models such as AVES (Hagiwara, 2023) and BioLingual (Robinson et al., 2024) have exhibited notable results using self-supervision, though they remain constrained by their training paradigms (discriminative and contrastive, respectively), which restrict the range of tasks they can address. In recent years, foundation models, which learn patterns in large amounts of data via self-supervision, have shown promising performance across a wide range of tasks (Bommasani et al., 2021). While transformer-based large language models (LLMs) are currently the most prominent examples, other architectures, such as diffusion models (Kingma et al., 2021), are also emerging as foundation models in some domains. These models’ ability to handle unseen tasks, perform in-context learning, and respond to prompts positions them as a compelling alternative to traditional machine learning methods, which often rely on laboriously annotated data, expensive computational resources, and often-lacking machine learning expertise. While multimodal large language models (LLMs), particularly vision-language models (VLMs), have been explored for biodiversity and conservation research (Miao et al., 2024), there is relatively little effort dedicated to building and investigating large audio-language models (LALMs) for bioacoustics. LALMs have shown significant promise in processing human speech (Rubenstein et al., 2023; Wang et al., 2024; Wu et al., 2023a; Zhang et al., 2024), music (Gardner et al., 2023; Agostinelli et al., 2023), and general audio tasks (Tang et al., 2024; Chu et al., 2024; Gong et al., 2023), and they hold the potential to bring transformative advancements to bioacoustics as well. In this paper, we present NatureLM-audio, an audio-language foundation model specifically designed for bioacoustics tasks, including classification, detection, and captioning. To the best of our knowledge, NatureLM-audio is the first model of its kind. Inspired by the cross-taxa transfer observed in previous research, such as between human and gibbons (Cauzinille et al., 2024) and birds and whales (Ghani et al., 2023), we incorporate speech and music tasks into the training process. We show that representations learned from these domains successfully transfer to animal vocalizations, demonstrating generalization across species. Importantly, we augment an already existing animal sounds classification and detection benchmark, BEANS (Hagiwara et al., 2023), with additional tasks such as call-type prediction, lifestage classification, captioning, and individual counting. With these, we test cross-domain learning capabilities of the model and zero-shot transfer to unseen taxa and tasks. We name this new benchmark BEANS-Zero. Our contributions are thus as follows: • Model: We introduce NatureLM-audio, to the best of our knowledge, the first audio-language foundation model for bioacoustics with carefully curated training datasets comprising of animal vocalization, human speech, and music. • Domain transfer We show that the model transfers beyond the species originally trained on and demonstrate its zero-shot capability on unseen taxa and species. • Task transfer We test our model on a novel benchmark (BEANS-Zero) that goes beyond species classification and even includes a completely unseen task (individual counting).For the first time, we show positive transfer from speech and music data to bioacoustics tasks."
https://arxiv.org/html/2411.07165v1,Acoustic-based 3D Human Pose Estimation Robust to Human Position,"This paper explores the problem of 3D human pose estimation from only low-level acoustic signals. The existing active acoustic sensing-based approach for 3D human pose estimation implicitly assumes that the target user is positioned along a line between loudspeakers and a microphone. Because reflection and diffraction of sound by the human body cause subtle acoustic signal changes compared to sound obstruction, the existing model degrades its accuracy significantly when subjects deviate from this line, limiting its practicality in real-world scenarios. To overcome this limitation, we propose a novel method composed of a position discriminator and reverberation-resistant model. The former predicts the standing positions of subjects and applies adversarial learning to extract subject position-invariant features. The latter utilizes acoustic signals before the estimation target time as references to enhance robustness against the variations in sound arrival times due to diffraction and reflection. We construct an acoustic pose estimation dataset that covers diverse human locations and demonstrate through experiments that our proposed method outperforms existing approaches.","Human pose estimation has diverse applications including rehabilitation support, elderly monitoring, and disaster relief efforts. Traditional approaches to 3D human pose estimation have primarily employed RGB videos and images [Martinez et al.(2017)Martinez, Hossain, Romero, and Little, Pavlakos et al.(2017)Pavlakos, Zhou, Derpanis, and Daniilidis], transient light [Isogawa et al.(2020)Isogawa, Yuan, O’Toole, and Kitani], event data [Scarpellini et al.(2021)Scarpellini, Morerio, and Del Bue, Chen et al.(2022)Chen, Shi, Ye, Yang, Sun, and Wang], radio frequency (RF)/Wi-Fi signals [Zhao et al.(2018b)Zhao, Tian, Zhao, Alsheikh, Li, Hristov, Kabelac, Katabi, and Torralba, Jiang et al.(2020)Jiang, Xue, Miao, Wang, Lin, Tian, Murali, Hu, Sun, and Su], and millimeter wave [Kong et al.(2022)Kong, Xu, Yu, Chen, Ma, Chen, Chen, and Kong, Xue et al.(2021)Xue, Ju, Miao, Wang, Wang, Zhang, and Su]. Additionally, methods that combine some of these approaches as a multimodal framework also exist [An et al.(2022)An, Li, and Ogras, Yang et al.(2024)Yang, Huang, Zhou, Chen, Xu, Yuan, Zou, Lu, and Xie]. However, optical signals face challenges such as obstruction and poor performance in low-light conditions [Lee et al.(2023)Lee, Rim, Jeong, Kim, Woo, Lee, Cho, and Kwak]. Furthermore, RGB video and images acquire high-resolution measured data, which raises concerns regarding the protection of personal information. Wireless signal-based methods are restricted in environments employing precision machinery, such as medical facilities or aircraft. One possible solution to these challenges is the utilization of acoustic signals. Acoustic signals have much longer wavelengths (meter scale) compared to optical signals (nanometer scale) or RF/Wi-Fi signals (centimeter scale). Therefore, acoustic signals are more susceptible to diffraction and less affected by obstruction. Moreover, acoustic signals offer consistent performance irrespective of lighting conditions and their usage is not hindered by the presence of precision machinery. Recent studies have explored passive acoustic sensing for gesture recognition and human pose estimation by leveraging human speech [Li et al.(2021)Li, Kang, Pei, Zhe, Zhang, He, and Bao, Ginosar et al.(2019)Ginosar, Bar, Kohavi, Chan, Owens, and Malik], ambient sounds [Gao et al.(2020)Gao, Oh, Grauman, and Torresani], or the sound of playing a musical instrument [Shlizerman et al.(2018)Shlizerman, Dery, Schoen, and Kemelmacher-Shlizerman]. These methods require sounds produced by the subjects themselves, which limits the use case. Alternatively, Shibata et al\bmvaOneDotproposed a 3D human pose estimation approach using active acoustic sensing with Time-Stretched-Pulse (TSP) signals [Shibata et al.(2023)Shibata, Kawashima, Isogawa, Irie, Kimura, and Aoki]. In this approach, a subject is positioned between a speaker and microphone (see Fig. 1(a)), where the speaker repeatedly emits TSP signals to create an acoustic field, and human poses are estimated based on how the acoustic field distorts as a subject moves. However, this method primarily relies on how the acoustic signal emitted from the speaker is obstructed by the human body to estimate the human pose. It implicitly assumes that the target subject is positioned on a straight line between the speaker and the microphone, although in the real world, meeting such constraints is extremely rare. Through the preliminary experiments, we found that the estimation accuracy significantly decreases when the subject deviates from this line, due to the difficulty of capturing subtle changes in sound signals caused by human body movements. Fig. 1(c) visualizes the acoustic features used as input to the model. The dimensions of these features are reduced by the Principal Component Analysis (PCA). The acoustic features in the settings without any subject and those shown in figures (a) and (b) are represented in different colors. From this figure, it is confirmed that the acoustic features when a person moves away from this line (blue dots) approach the features when there is no subject (green dots), indicating sound diffraction and reflection convey much less human pose information than sound obstruction caused by a person standing on the aforementioned line (red dots). To overcome this limitation, this paper proposes an acoustic-based 3D human pose estimation method, which remains effective regardless of the subject’s standing positions. While Shibata et al\bmvaOneDotprimarily relied on signal obstruction by the human body as their main clue, in this paper, we also consider cases where the position of the person is not on the straight line connecting the speaker and the microphone, as shown in Fig. 1(b). Therefore, it is necessary to consider signal diffraction and reflection from the subject as well as signal obstruction. From a technical perspective, this implies the need to solve two extremely challenging issues: (i) The relatively long wavelengths of acoustic signals tend to cause specular reflections off the surface of the human body. Consequently, the sound intensity of the reflected acoustic signals is greatly influenced by the positions of reflection and recording microphones. (ii) The arrival time of sound emitted from a speaker until it is recorded can vary due to signal diffraction and reflection. In this paper, we aim to develop methods capable of addressing these challenges. First, to enhance robustness against variations in the subject’s position, we introduce a position discriminator module. This module uses intermediate features of the pose estimation module to predict human positions, while the pose estimation module is trained to maximize the uncertainty of human positions, through adversarial training. Furthermore, to achieve robust pose estimation against changes in the arrival time of sound due to sound diffraction and reflection, we propose to introduce a reference window into the pose estimation module to consider signals prior to the target time to be estimated. Additionally, we perform data augmentation by shifting the phase of the acoustic signal, which allows for a reduction in the amount of data per subject location, enabling the preparation of a dataset that covers diverse positions. As the first attempt at non-invasive 3D human pose estimation regardless of the subject’s position, we construct a new dataset containing data from positions away from the straight line connecting the speaker and the microphones. In summary, the technical contributions of this study are as follows: (1) We have worked towards realizing a practical non-invasive 3D human pose estimation method based on active acoustic signals while subjects are placed in multiple positions. (2) We introduced a position discriminator module to enhance robustness against variations in the subject’s standing position. Additionally, we constructed a pose estimation model that considers acoustic signals prior to the estimation target time to achieve robust estimation against changes in sound arrival times due to signal diffraction and reflection. (3) To effectively learn from limited data, we performed data augmentation by shifting the phase of the acoustic signal. (4) As the first attempt to estimate non-invasively 3D human pose regardless of the subject’s position, we constructed a dataset containing data from multiple positions away from the straight line connecting the speaker and the microphones."
https://arxiv.org/html/2411.06968v1,"Mamba-based Decoder-Only Approach with Bidirectional 
Speech Modeling for Speech Recognition","Selective state space models (SSMs) represented by Mamba have demonstrated their computational efficiency and promising outcomes in various tasks, including automatic speech recognition (ASR). Mamba has been applied to ASR task with the attention-based encoder-decoder framework, where the cross-attention mechanism between encoder and decoder remains. This paper explores the capability of Mamba as the decoder-only architecture in ASR task. Our MAmba-based DEcoder-ONly approach (MADEON) consists of a single decoder that takes speech tokens as a condition and predicts text tokens in an autoregressive manner. To enhance MADEON, we further propose speech prefixing that performs bidirectional processing on speech tokens, which enriches the contextual information in the hidden states. Our experiments show that MADEON significantly outperforms a non-selective SSM. The combination of speech prefixing and the recently proposed Mamba-2 yields comparable performance to Transformer-based models on large datasets.","Transformer [transformer] and its variants [conformer, conformer-vs-e-brachformer] have dramatically improved the performance of a wide range of speech processing tasks, including automatic speech recognition (ASR). The key to their success is the attention mechanism that can dynamically aggregate the information from the entire sequence. Meanwhile, the attention mechanism typically suffers from its quadratic computational complexity with respect to the sequence length. To mitigate this issue, deep state space models (SSMs) have been developed [s4, s4d, h3]. SSMs can be trained with a sub-quadratic complexity owing to tailored algorithms, and their recurrent nature reduces the required memory during inference. Furthermore, SSMs have shown promising performance in various speech processing tasks such as ASR [dssformer, s4decoder, mssm, s4former], speech synthesis [sashimi], and speech enhancement [s4m, s4ndunet]. Existing SSMs, e.g., structured SSM (S4) [s4], are built on linear time-invariant (LTI) systems, and their parameters are fixed regardless of the input sequence. This input-independent architecture inhibits the capability of SSMs. The selective SSM introduced in Mamba [mamba] dynamically computes the SSM parameters based on the input sequence and has demonstrated outstanding performance in computer vision [vision-mamba], natural language processing [mambabyte], and speech processing tasks [mambase, mambass, mambainspeech, miyazaki2024interspeech]. In particular to ASR task, Mamba has been validated on the encoder-only approach with the connectionist temporal classification (CTC) [miyazaki2024interspeech] and on the attention-based encoder-decoder (AED) approach [mambainspeech, miyazaki2024interspeech]. Notably, Mamba outperforms Transformer and S4 when used as a decoder in the joint CTC/AED framework [joint-ctc-att-decoding]. While Mamba has been used non-autoregressively in speech applications, the decoder-only model is simple yet effective for sequence-to-sequence tasks, where the model autoregressively predicts the next token [gpt, gpt3]. It has been successfully applied to unified speech and text processing, either by adapting a pre-trained large language model [wavprompt, asru2023decoderonly, icassp2024udagawa, iclr2024llm, qwenaudio] or by training a model from scratch [icassp2024lossmasking, arxiv2023tsunoo, viola, audiopalm, voxtlm]. Most of these models are based on Transformer and require quadratic complexity to handle a long sequence comprising speech and text tokens. On the other hand, Mamba can reduce the computational complexity, while it has shown promising performance as a decoder in the joint CTC/AED framework [miyazaki2024interspeech]. Fig. 1: Overview of MADEON for ASR task. The blue and red circles show the speech and text tokens obtained through subword modeling, respectively. The black circles are special tokens, and the gray dotted lines indicate the autoregressive text generation. In this paper, we explore a MAmba-based DEcoder-ONly approach (MADEON) in ASR task towards SSM-based unified speech and text modeling. As depicted in Fig. 1, MADEON employs a single Mamba decoder that takes speech tokens as a condition and predicts the transcription in an autoregressive manner. We further propose speech prefixing, which performs bidirectional processing on speech tokens to enhance the contextual modeling capability of MADEON. We also investigate Mamba-2 [mamba2] that can leverage larger hidden states more efficiently than the original Mamba. Our experiments show that Mamba significantly improves the word error rate (WER) from a non-selective SSM. Although the unidirectional MADEON lags behind Transformer-based models, the integration of speech prefixing and Mamba-2, MADEON-2SP, achieves a comparable performance to Transformer-based models on large datasets. Our contributions are summarized as follows: • We explored the efficacy of Mamba in a decoder-only approach while existing studies with Mamba were built upon the AED approach [mambainspeech, miyazaki2024interspeech]. • We proposed speech prefixing to enhance the contextual modeling capability of MADEON. • We confirmed the effectiveness of Mamba-2 in ASR task."
https://arxiv.org/html/2411.06807v1,"Wavehax: Aliasing-Free Neural Waveform Synthesis 
Based on 2D Convolution and Harmonic Prior 
for Reliable Complex Spectrogram Estimation","Neural vocoders often struggle with aliasing in latent feature spaces, caused by time-domain nonlinear operations and resampling layers. Aliasing folds high-frequency components into the low-frequency range, making aliased and original frequency components indistinguishable and introducing two practical issues. First, aliasing complicates the waveform generation process, as the subsequent layers must address these aliasing effects, increasing the computational complexity. Second, it limits extrapolation performance, particularly in handling high fundamental frequencies, which degrades the perceptual quality of generated speech waveforms. This paper demonstrates that 1) time-domain nonlinear operations inevitably introduce aliasing but provide a strong inductive bias for harmonic generation, and 2) time-frequency-domain processing can achieve aliasing-free waveform synthesis but lacks the inductive bias for effective harmonic generation. Building on this insight, we propose Wavehax, an aliasing-free neural WAVEform generator that integrates 2D convolution and a HArmonic prior for reliable Complex Spectrogram estimation. Experimental results show that Wavehax achieves speech quality comparable to existing high-fidelity neural vocoders and exhibits exceptional robustness in scenarios requiring high fundamental frequency extrapolation, where aliasing effects become typically severe. Moreover, Wavehax requires less than 5% of the multiply-accumulate operations and model parameters compared to HiFi-GAN V1, while achieving over four times faster CPU inference speed.","VOCODERS [1] are models that synthesize audio waveforms from input acoustic features. Specifically, vocoders utilizing deep neural networks (DNNs) are called neural vocoders. With advances in deep learning, neural vocoders have achieved high-fidelity speech synthesis often indistinguishable from natural human speech. Historically, the development of neural vocoders has focused primarily on time-domain models, including autoregressive vocoders [2, 3, 4, 5, 6], normalizing flow [7] vocoders [8, 9, 10, 11, 12], generative adversarial networks (GAN) [13] vocoders [14, 15, 16, 17, 18, 19], and denoising diffusion probabilistic model [20] vocoders [21, 22, 23, 24]. Among these, GAN-based methods have been extensively studied for their fast synthesis speed, compact generator, and high-fidelity outputs. These methods typically employ one-dimensional (1D) convolutional neural networks (CNNs) followed by nonlinear activation functions, such as rectified linear units (ReLU) [25]. Additionally, some methods [15, 16, 17, 18, 19] utilize multiple upsampling layers to efficiently convert low-resolution acoustic features into high-resolution waveforms. Despite advancements in neural vocoders, their architectural design has received limited attention from a signal-processing perspective, particularly regarding aliasing in latent feature spaces caused by nonlinear operations [26, 19] and resampling layers [27]. Aliasing folds high-frequency components back into the low-frequency range, making the aliased and original low-frequency components indistinguishable, and introduces undesirable distortion, known as aliasing artifacts. These phenomena induce two practical problems. First, aliasing complicates the synthesis process because the subsequent layers must address the aliasing effects, resulting in increased computational complexity. Second, aliasing limits extrapolation performances, particularly for fundamental frequency (\text{F}_{0}) values, degrading the perceptual quality of generated waveforms [28, 29, 30, 31]. This problem is especially relevant in applications such as text-to-speech, voice conversion for new speakers, and singing voice synthesis involving pitch shifts. Lee et al. [19] highlighted that while nonlinear operations create specific frequency components, their application to discrete-time signals inevitably results in aliasing due to the frequency band limitations imposed by the Nyquist–Shannon sampling theorem [32, 33]. To mitigate the aliasing caused by nonlinear operations, they employed temporal upsampling to extend the representable frequency band before applying nonlinear operations. Pons et al. [27] investigated upsampling artifacts, including aliasing, across various upsampling techniques in neural vocoders. They also suggested that network training plays a crucial role in compensating for these artifacts. While these studies provided valuable insights into mitigating aliasing in neural vocoders, completely eliminating aliasing effects remains an open challenge. Additionally, countermeasures that rely on model training do not guarantee robust anti-aliasing performance when applied to unseen data. This paper demonstrates that 1) time-domain nonlinear operations inevitably introduce aliasing but provide a strong inductive bias for harmonic generation (where harmonics are integer multiples of a fundamental frequency), and 2) time-frequency-domain processing can achieve aliasing-free waveform synthesis but lacks the inductive bias for effective harmonic generation. Based on this insight, we propose Wavehax, an aliasing-free neural vocoder that estimates complex spectrograms and converts them into time-domain waveforms via the short-time Fourier transform (STFT). Wavehax employs a novel integration of 2D CNNs with a complex spectrogram derived from a harmonic signal, which is crucial for high-fidelity and robust complex spectrogram estimation. Importantly, this study distinguishes itself from previous works on neural vocoders estimating complex spectrograms [34, 35, 36, 37, 38, 39], which primarily emphasize the computational efficiency of time-frequency-domain processing. In contrast, our approach prioritizes aliasing-free waveform synthesis, demonstrating its theoretical and practical benefits. Experimental results show that Wavehax achieves speech quality comparable to existing high-fidelity neural vocoders and exhibits exceptional robustness in high \text{F}_{0} extrapolation scenarios, where aliasing effects become typically severe. Moreover, Wavehax requires less than 5% of the multiply-accumulate operations and model parameters compared to HiFiGAN V1 [17], while achieving over four times faster CPU inference speed. Audio samples and code are available from our demo site111https://chomeyama.github.io/wavehax-demo/."
https://arxiv.org/html/2411.06307v1,"Acoustic Volume Rendering for
Neural Impulse Response Fields","Realistic audio synthesis that captures accurate acoustic phenomena is essential for creating immersive experiences in virtual and augmented reality. Synthesizing the sound received at any position relies on the estimation of impulse response (IR), which characterizes how sound propagates in one scene along different paths before arriving at the listener’s position. In this paper, we present Acoustic Volume Rendering (AVR), a novel approach that adapts volume rendering techniques to model acoustic impulse responses. While volume rendering has been successful in modeling radiance fields for images and neural scene representations, IRs present unique challenges as time-series signals. To address these challenges, we introduce frequency-domain volume rendering and use spherical integration to fit the IR measurements. Our method constructs an impulse response field that inherently encodes wave propagation principles and achieves state-of-the-art performance in synthesizing impulse responses for novel poses. Experiments show that AVR surpasses current leading methods by a substantial margin. Additionally, we develop an acoustic simulation platform, AcoustiX, which provides more accurate and realistic IR simulations than existing simulators. Code for AVR and AcoustiX are available at https://zitonglan.github.io/avr.","Our acoustic environment shapes every sound we hear – from the crisp echoes bouncing through hallways to the layered resonance of a symphony filling a concert hall. These spatial characteristics not only define our daily auditory experiences but also prove crucial for creating convincing virtual worlds [15, 60]. At the core of these spatial characteristics is the impulse response (IR), which captures the complex relationship between an emitted sound and what we hear. Like a unique acoustic fingerprint, the impulse response varies across different positions, encoding how sound waves interact with the environment through reflection, diffraction, and absorption [22, 41]. We can recreate the acoustic experience at any position by convolving the corresponding impulse response with any desired sound sources (e.g., music, speech). Given its foundational role in spatial audio synthesis, understanding and modeling the spatial variation of impulse responses in acoustic environments has emerged as a critical challenge and attracted increasing research attention [2, 26, 27, 29, 35, 36, 44, 49, 53]. Current approaches construct a neural impulse response field – a learned mapping that generates impulse responses given the emitter and listener poses. To model the high spatial variation of impulse responses, existing methods either fit a neural network to directly learn the field [29, 44] or rely on audio-visual correspondences to learn mappings from vision [26, 27]. While these methods can approximate the general energy trend, they struggle to capture the detailed characteristics of impulse responses, leading to incorrect spatial variation of impulse responses (Fig. 1). We argue that a key barrier to achieving better performance is the absence of physical constraints that inherently enforce consistency across multiple poses. Without such physical constraints, the network tends to overfit the training data and show poor generalizability. The received impulse response fundamentally arises from sound waves propagating through space, combining direct transmission with environmental reflections. This physical insight motivates us to develop a framework that inherently encodes wave propagation principles into the modeling of impulse response fields. In this paper, we introduce Acoustic Volume Rendering (AVR) to model the field of acoustic impulse responses. Our approach draws inspiration from Neural Radiance Fields [33], which has demonstrated remarkable success in modeling 3D scenes by representing light transport through volume rendering. However, acoustic waves present several fundamental challenges that require adaptations to the volume rendering framework: First, acoustic impulse responses, unlike light transmission, are inherently time-series signals, with acoustic waves from different locations reaching the listener at varying delays. The issue is further compounded when dealing with discrete impulse responses sampled in the real world. Second, impulse responses exhibit high spatial variation, in contrast to images where neighboring pixels show strong correlations. This characteristic makes network optimization particularly challenging [43, 45]. Finally, unlike cameras that capture light with precise directional information (i.e., pixels), microphones capture combined signals from all directions. Figure 1: Left: From observations of the sound emitted by a speaker, our model constructs an impulse response field that can synthesize observations at novel listener positions. Right: Visualization of spatial variation of impulse responses on MeshRIR[20]. The synthesized impulse responses at different locations are transformed into the frequency domain, where we visualize phase and amplitude distributions at a specific wavelength (1m). To address these challenges, we convert impulse responses from the time domain to the frequency domain with Fourier transforms and perform volume rendering in the frequency domain. We apply phase shifts to the frequency-domain impulse responses to account for time delays, bypassing the limits of finite time domain sampling. The frequency-domain representation also exhibits lower spatial variation, facilitating network optimization. To account for signals from all possible directions, we cast rays uniformly across a sphere and use spherical integration to synthesize the impulse response measurements. Additionally, this design enables personalized audio experience by integrating individual head-related transfer functions (HRTFs) [57] into spherical integration at inference time. Our evaluation results show that AVR outperforms existing methods by a large margin in both simulated and real-world datasets [10, 20] and can zero-shot render binaural audio (Sec. 4.3). Figure 2: AcoustiX for improved acoustic simulation. Time-of-flight indicates how long it takes for an emitted sound to reach a listener. With sound traveling at a constant speed, the time-of-arrival should be proportional to the emitter-listener distance. While SoundSpace 2.0 simulations show significant time-of-flight errors, particularly at short emitter-listener distances, AcoustiX produces more accurate arrival times. All simulations are performed in the Gibson Montreal room [56] with direct line-of-sight between emitter and listener. In parallel with AVR, we develop AcoustiX, an acoustic simulation platform that generates more physically accurate impulse responses compared to existing simulators. While current simulators often introduce significant errors in signal phases and arrival times, AcoustiX produces impulse responses that better match the physical properties of real-world acoustics. Fig. 2 demonstrates the inaccuracies in impulse responses generated by SoundSpaces 2.0 [9]. Some existing simulators assign random phases when generating impulse responses [9, 40], which fails to reflect real-world acoustic behavior [4]. Since current research in impulse response synthesis heavily relies on simulated datasets [10, 29, 44], these simulation inaccuracies can impede progress in the field. To address these limitations, we develop a new simulation platform based on the Sionna ray tracing engine [16] and incorporate acoustic propagation equations to resolve the aforementioned issues. Similar to SoundSpaces 2.0, AcoustiX supports acoustic simulation in both user-provided 3D scenes and a variety of existing 3D scene datasets [24, 56]. In summary, this work makes the following contributions: • We introduce acoustic volume rendering (AVR) for the neural impulse response field to inherently enforce acoustic multi-view consistency. We introduce a frequency-domain rendering method with spherical integration to address the challenges associated with acoustic impulse response modeling. • We demonstrate that AVR outperforms existing methods by a large margin in both real and simulated datasets. AVR also supports zero-shot and personalized binaural audio synthesis. • We develop AcoustiX, an open-source and physics-based impulse response simulator that provides accurate time delays and phase relationships through rigorous acoustic propagation modeling."
https://arxiv.org/html/2411.07111v1,"Building a Taiwanese Mandarin Spoken Language Model: 
A First Attempt","This technical report presents our initial attempt to build a spoken large language model (LLM) for Taiwanese Mandarin, specifically tailored to enable real-time, speech-to-speech interaction in multi-turn conversations. Our end-to-end model incorporates a decoder-only transformer architecture and aims to achieve seamless interaction while preserving the conversational flow, including full-duplex capabilities allowing simultaneous speaking and listening. The paper also details the training process, including data preparation with synthesized dialogues and adjustments for real-time interaction. We also developed a platform to evaluate conversational fluency and response coherence in multi-turn dialogues. We hope the release of the report can contribute to the future development of spoken LLMs in Taiwanese Mandarin.","This is a technical report on the practice of training large language models (LLMs) for spoken Taiwanese Mandarin. The content is based on the final project of a course and serves as a record of the work completed thus far. As a work-in-progress paper, the current version is not yet ready for conference submission. However, we believe that some ideas and findings from this project may inspire future work. Therefore, we have made the report publicly available. If you find the ideas or findings in this report useful for your project, please cite this technical report. 1.1 Goal of the Project The project aims to train a spoken LLM that can engage in real-time speech-to-speech conversations. The model takes a system prompt as input and, based on that prompt, facilitates seamless spoken interactions. Ultimately, the aim is to create an experience where conversing with the AI feels as natural as talking to a real person on Google Meet. In terms of interaction, there are several goals we aim to achieve: • Speech-to-speech, multi-turn conversations • Mandarin conversation with a Taiwanese accent • The model should be capable of seamless, full-duplex communication with humans. This means the model can process and respond in real time, allowing for interruptions and simultaneous speaking and listening. In the past, spoken dialogue has been achieved through a cascade framework, which combines automatic speech recognition (ASR), LLM, and text-to-speech (TTS) to facilitate speech-to-speech interaction. Here, we aim to build an end-to-end model. Since the model is end-to-end, we hope it can accomplish several tasks that a cascade model cannot. For example, it should be able to understand input audio beyond just the text, including prosody and speaker characteristics, and respond appropriately (Lin et al., 2024). It should also recognize non-verbal vocalizations (e.g., laughter, crying) and respond with the corresponding non-verbal sounds. Compared to a cascade model, an end-to-end model has the potential to interpret environmental sounds. However, we did not implement or thoroughly evaluate all of these capabilities due to time constraints, so we are unsure how well the model currently performs in these areas. Nonetheless, the end-to-end model has this potential, and we can extend the current work to incorporate these abilities in the future. Although GPT-4’s voice mode can achieve seamless interaction with speech, it is not open-sourced, so we cannot know how it is achieved. We hope this technical report can shed light on the development of spoken LLMs in the future, helping to accelerate the creation of open-source spoken LLMs for the community. Our model will soon be open-sourced. The open-source model most similar to our goal is Moshi (Défossez et al., 2024), but it primarily supports English and cannot understand or generate Mandarin. 1.2 Technical Contributions in This Project We believe this technical report may contain some technological contributions, though we have not fully verified them. Below, we briefly mention the potential contributions. For the model side, we demonstrate the potential to achieve real-time full-duplex machine-human speech communication using a typical decoder-only transformer architecture in Section 2. Existing speech-language models for generation, such as LLaMA-Omni (Fang et al., 2024), Mini-Omni (Xie & Wu, 2024), and Moshi (Défossez et al., 2024), require modifications to the standard transformer model. In contrast, we generate audio based on the standard decoder-only transformer architecture. Traditionally, full-duplex communication is achieved using a dual-channel system – one channel for listening and one for speaking (Nguyen et al., 2023; Ma et al., 2024). However, we show that even a typical transformer architecture can achieve full-duplex communication. Since our model uses the same architecture as text LLM, we can directly use a text-based LLM for initialization and fine-tune it from there. Ideally, the speech model will inherit the text model’s capabilities. Since it is possible to collect much more text data than speech data, initializing the speech model from a text model and leveraging the capabilities of a text LLM is critical to the success of a spoken LLM. Additionally, because we do not modify the network architecture of the text LLM, the model can benefit from the wide range of frameworks and toolkits designed to enhance text LLMs. This also means it can be easily deployed and accelerated. The data preparation and model training pipeline will be described in Section 3 and 4. We originally intended to use a large amount of dialogue data collected from the Internet. However, we found that using real dialogues from the Internet negatively impacted the model’s performance. Therefore, we opted to use synthesized data instead. First, we used a text-based LLM to generate dialogues, then applied a TTS model to vocalize the generated dialogues. To ensure that the TTS output sounded natural with a Taiwanese accent, we used a TTS model developed by another project111The technical report of the project will be available soon. Our goal is for the model to generate realistic dialogue that aligns with the emotional content of the conversation. For example, when a character says, “I feel sad,” the speech should sound appropriately sad. Typical TTS systems, especially commercial APIs, often struggle to capture such nuances, so we rely on our own TTS models. To create more realistic dialogues, we even generate dialogues with interruptions, which will be further explained in the following discussion. Even with a well-trained model, numerous implementation challenges remain in achieving real-time conversation. All the difficulties, solutions, and tips will be further discussed in Section 5. In Section 6, we will discuss how to evaluate spoken LLMs. Currently, there is no standard method for evaluating spoken LLMs. Commonly used benchmarks, such as Dynamic-SUPERB (Huang et al., 2024a; b), are instruction-following-based evaluations. However, we also want to assess how well the model can engage in communication, which no existing benchmark adequately measures. To address this, we developed a platform to facilitate communication between two chatbots. We also created an interface for users to interact with the bot, which simulates a smooth, natural conversation."
https://arxiv.org/html/2411.06667v1,DCF-DS: Deep Cascade Fusion of Diarization and Separation for Speech Recognition under Realistic Single-Channel Conditions,"We propose a single-channel Deep Cascade Fusion of Diarization and Separation (DCF-DS) framework for back-end speech recognition, combining neural speaker diarization (NSD) and speech separation (SS). First, we sequentially integrate the NSD and SS modules within a joint training framework, enabling the separation module to leverage speaker time boundaries from the diarization module effectively. Then, to complement DCF-DS training, we introduce a window-level decoding scheme that allows the DCF-DS framework to handle the sparse data convergence instability (SDCI) problem. We also explore using an NSD system trained on real datasets to provide more accurate speaker boundaries during decoding. Additionally, we incorporate an optional multi-input multi-output speech enhancement module (MIMO-SE) within the DCF-DS framework, which offers further performance gains. Finally, we enhance diarization results by re-clustering DCF-DS outputs, improving ASR accuracy. By incorporating the DCF-DS method, we achieved first place in the realistic single-channel track of the CHiME-8 NOTSOFAR-1 challenge. We also perform the evaluation on the open LibriCSS dataset, achieving a new state-of-the-art performance on single-channel speech recognition.","Multi-speaker speech recognition aims to identify “who spoke what and when” in recordings with unknown scenarios and speakers [1]. While deep learning has driven significant advancements in speech recognition [2, 3], accurate speech recognition in real-world multi-speaker conversational scenarios remains a major challenge. These challenges often arise from high overlap rates, background noises and reverberation, unknown speaker numbers, and natural conversation styles [4, 5, 6]. In this context, speaker diarization and speech separation are commonly employed as pre-processing steps. Specifically, speaker diarization segments the audio based on speaker identity [7, 8], while speech separation [9] isolates each speaker’s speech from the mixed audio signals. Speaker diarization and speech separation are closely related [4], as both aim to predict the distribution of speakers: speaker diarization predicts the speaker’s presence across time frames, whereas speech separation predicts their distribution across time-frequency (T-F) bins. Consequently, the temporal distribution provided by diarization can serve as a prior for speech separation, helping the system to better distinguish between speakers [10, 11, 12, 13, 4, 14]. Speech separation, in turn, inherently encompasses diarization—ideal separation results delineate the time regions of speakers [15, 16, 17]. Moreover, speech separation can handle overlapping segments, which can enhance speaker diarization accuracy [18, 6]. Due to the strong correlation between speaker diarization and speech separation, numerous kinds of research [19, 20, 21, 18, 6, 15, 16, 17, 10, 11, 12, 13, 4, 14, 22] have been conducted to explore combining these two tasks to enhance speech quality. Based on how the two tasks are combined, these methods can be broadly categorized into multi-task joint training and sequential cascade approaches. In multi-task joint training approaches [19, 20, 22, 21], the network typically generates two outputs: speaker activity probabilities and separated speech signals. This structure allows the two tasks to benefit from each other mutually [20]. Speaker diarization results can also be used to post-process the separation outputs, reducing background noises in the separated signals [19, 20, 21, 22]. Among these methods, some studies [19, 22] propose a network that jointly performs target speech extraction and voice activity detection (VAD), yielding improvements in both fully and sparsely overlapped speech. Another approach, end-to-end neural speaker diarization and separation (EEND-SS) [20], integrates speaker counting, diarization, and separation in a multi-tasking fashion. However, due to the inclusion of the separation task within the joint training framework, these methods are restricted to training on simulated data. Unsupervised training methods, such as PixIT [21], can help mitigate this issue. However, when the number of separated output nodes is limited, clustering methods are still required to assign separated streams for long recordings. In sequential cascade framework [18, 6, 15, 16, 17, 10, 11, 12, 13, 4, 14], there is no definitive answer as to whether speech separation or speaker diarization should be performed first. Some approaches [18, 6, 15, 16, 17] choose to perform speech separation first, namely “separation-then-diarization”. For example, the continuous speaker separation (CSS) pipeline [18, 6] first processes overlapping regions in the original audios through speech separation and then applies the separated results to diarization. Some studies [15, 16, 17] explore speaker diarization by performing VAD on the separated outputs, such as speech separation guided diarization (SSGD) [15] and recurrent selective attention network (RSAN) [16, 17]. Some methods [10, 11, 12, 13, 4, 14] begin with speaker diarization and then use the obtained boundary information to guide speech separation, namely “diarization-then-separation”. This is also a promising direction, as speaker diarization is generally simpler than speech separation, allowing the former to provide a more accurate prior. A classic method is guided source separation (GSS) [10, 11], which uses segment boundaries to initialize time-varying mixture weights based on traditional spatial mixture models [23, 24]. However, GSS relies on multi-channel spatial information. The speaker activity driven speech extraction neural network (ADEnet) [25] explores how to use speaker activity boundaries as auxiliary cues for speech extraction. The CSS-AD [26] pipeline performs speech recognition on CSS outputs, followed by diarization on the ASR output, achieving promising results on the LibriCSS dataset [18]. Certain approaches [12, 13] enhance separation performance in real two-speaker scenarios by using speaker diarization to generate adaption training data. Recently, the target-speaker separation (TS-SEP) [4] and speaker separation via neural diarization (SSND) [14] have achieved strong performance in the fields of diarization and separation. TS-SEP method [4] combines speaker diarization and speech separation by extending the output of target-speaker voice activity detection (TS-VAD) [27] from the time domain to the time-frequency domain. SSND [14] uses estimated speaker boundaries to assist in assigning speakers to the outputs of a multi-speaker separation model. These two methods demonstrate the great potential of the sequential cascade framework. From various studies mentioned above, it is apparent that both joint training and sequential cascade approaches have advantages and limitations. Joint training allows simultaneous diarization and separation within a compact and unified framework. However, typical multi-task learning frameworks usually do not fully leverage diarization results to optimize separation results. Additionally, joint training frameworks are generally restricted to simulated data. Conversely, the sequential cascade approach—particularly “diarization-then-separation”—allows for more effective integration of two tasks, where separation can effectively leverage diarization results. However, this approach tends to be with a more cumbersome process and independently optimized components. Therefore, an effective approach is to combine “joint training” and “sequential cascade“, thereby taking advantage of both methods—what we refer to as “deep cascade fusion”. However, due to differences in diarization and separation tasks, direct integration introduces a problem: diarization models can have many output nodes (e.g., eight output nodes in [28]). Directly feeding these outputs into the separation model and aligning the number of separation outputs with those from diarization may lead to non-convergence [4]. This issue arises because the separation model usually allocates an output node for each speaker. However, when training on typical conversational data, each node has limited target signals, making it difficult for the neural network to learn speaker-discriminative features. For example, in a natural eight-speaker meeting conversation, there are many regions where only one or two people are speaking, a separation model with eight outputs will have only one or two nodes with effective training targets in those regions. This significantly increases the training difficulty, which we term the sparse data convergence instability (SDCI) problem. This issue was also highlighted when the TS-SEP method was proposed [4], where a two-stage training approach was proposed as the solution. However, two-stage training makes the TS-SEP method unable to fully leverage the boundary information from the diarization module for separation. The SSND addresses this issue by constructing dual embedding sequences, which leads to a complex framework. This approach requires a pre-trained diarization model to extract speaker embeddings, followed by constructing an embedding sequence based on boundary information during both the training and testing phases. Additionally, a scenario-aware differentiated loss [22, 29] has been proposed to handle the sparsely overlapped speech. However, this approach requires defining multiple different loss functions and prior knowledge of each speaker’s VAD information. To address the above issues, we propose a single-channel deep cascade fusion of diarization and separation (DCF-DS) framework for back-end multi-speaker speech recognition. The primary contributions of our study are outlined as follows: 1) In DCF-DS, a joint training framework sequentially combining neural speaker diarization and separation is designed. Compared to TS-SEP, DCF-DS effectively leverages temporal boundaries from the diarization, thereby reducing the difficulty of separation. Compared to SSND, DCF-DS avoids complex pre-processes and eliminates dependence on spatial information. To complement DCF-DS training, we design a window-level decoding scheme to handle the SDCI problem. This decoding scheme allows the separation module to produce fewer outputs than the total number of speakers in an utterance. 2) We introduce an optional multi-input multi-output speech enhancement module (MIMO-SE) within the DCF-DS framework, which directly utilizes the same structure as the separation module in DCF-DS, providing additional performance gains. 3) We further optimize the speaker diarization results by re-clustering the outputs of DCF-DS, effectively enhancing back-end speech recognition performance. This process is typically not incorporated in advanced methods such as TS-SEP and SSND. 4) By using the DCF-DS approach, we achieved first place in the realistic single-channel track of the CHiME-8 NOTSOFAR-1 (Natural Office Talkers in Settings Of Far-field Audio Recordings) challenge [6]. Meanwhile, our evaluation on the open-source LibriCSS dataset [18] demonstrates state-of-the-art performance for single-channel speech recognition. The remainder of this paper is organized as follows. Section II provides an overview of related prior works. Section III describes the proposed DCF-DS framework in detail. In Section IV, we present and analyze the experimental results. Finally, Section V concludes the paper and discusses potential directions for future research."
https://arxiv.org/html/2411.06576v1,Diff-MST{}^{\textbf{C}} : A Mixing style Transfer Prototype for Cubase,"In our demo, participants are invited to explore the Diff-MST{}^{\textbf{C}} prototype, which integrates the Diff-MST model into Steinberg’s digital audio workstation (DAW), Cubase. Diff-MST, a deep learning model for mixing style transfer, forecasts mixing console parameters for tracks using a reference song. The system processes up to 20 raw tracks along with a reference song to predict mixing console parameters that can be used to create an initial mix. Users have the option to manually adjust these parameters further for greater control. In contrast to earlier deep learning systems that are limited to research ideas, Diff-MSTC is a first-of-its-kind prototype integrated into a DAW. This integration facilitates mixing decisions on multitracks and lets users input context through a reference song, followed by fine-tuning of audio effects in a traditional manner.","The democratisation of music production has introduced a spectrum of users ranging from amateurs, pro-ams, and professionals [2]. Each of the user groups is skilled differently and makes use of the available technology in varied ways. Whilst amateurs want automated systems, pro-ams and professionals prefer assistive systems that are controllable and nuanced [2]. The process of mixing music involves adjusting recorded music to produce an aesthetic, cohesive mix that evokes emotion. This is achieved using various audio effects like gain, pan, equalisation (EQ), compressor, reverb etc [3, 4]. Due to the technical and engineered nature of tools required to attain the desired artistic outcomes in the mixing process, mastering this craft requires years of training and experience. Automatic mixing is a field of research that has explored ways for aiding, automating, and assisting in the various aspects of the mixing process [5, 6]. These tools are designed to help amateurs for educational purposes and to achieve satisfactory sound quality, while also supporting professional users by streamlining technically demanding tasks and enabling faster iteration [5]. Beyond many classical and engineered methods explored in the past, deep learning-based approaches have shown promise in the field [7, 6]. Deep learning-based systems for multitrack music mixing can be divided into two types: Direct Transformation (DT) systems and Parameter Estimation (PE) systems [7]. Whilst DT systems assume a black box approach offering no control, PE systems predict control parameters for the desired effect chain, allowing control and further fine-tuning. Further, the earliest systems framed multitrack mixing problem using neural networks as a supervised task [8, 9], disregarding mixing fundamentally being a one-to-many problem. In real-world practice, mixing engineers receive various objects of tacit agreement like demo mixes, reference songs, and verbal descriptions for understanding the client’s objective for the mix [10], thus underlining the importance of context in the mixing process [11]. Mixing engineers decide the direction for the mix based on the context that is delivered by these objects. The Diff-MST system incorporates context into the model architecture, developing on previous work [12]. Figure 2: Diff-MSTC in Cubase 1.1 Diff-MST Diff-MST is a deep learning model implemented using the PE approach to conduct mixing style transfer [1]. The system accepts multitracks and a reference song as input and provides control parameters for the mixing console along with a predicted mix styled after the reference song, as depicted in Figure 1. It leverages differentiable effects from the dasp-pytorch111https://github.com/csteinmetz1/dasp-pytorch/ library, facilitating end-to-end training. The mixing console incorporates a channel strip with audio effects like gain, panorama, EQ, and compressor applied to each track. The sum of the modified outputs from all tracks is then processed through a master bus comprising an EQ, compressor, and fader. A segment of the reference song and multitrack is selected and run through the encoder. The produced embeddings are passed to a transformer encoder to obtain contextually-aware embeddings using self-attention. Finally, a controller made up of linear layers predicts the mixing console control parameters, which are fed through the mixing console with the input multitrack to produce the mix."
https://arxiv.org/html/2411.06399v1,PSELDNets: Pre-trained Neural Networks on Large-scale Synthetic Datasets for Sound Event Localization and Detection,"Sound event localization and detection (SELD) has seen substantial advancements through learning-based methods. These systems, typically trained from scratch on specific datasets, have shown considerable generalization capabilities. Recently, deep neural networks trained on large-scale datasets have achieved remarkable success in the sound event classification (SEC) field, prompting an open question of whether these advancements can be extended to develop general-purpose SELD models. In this paper, leveraging the power of pre-trained SEC models, we propose pre-trained SELD networks (PSELDNets) on large-scale synthetic datasets. These synthetic datasets, generated by convolving sound events with simulated spatial room impulse responses (SRIRs), contain 1,167 hours of audio clips with an ontology of 170 sound classes. These PSELDNets are transferred to downstream SELD tasks. When we adapt PSELDNets to specific scenarios, particularly in low-resource data cases, we introduce a data-efficient fine-tuning method, AdapterBit. PSELDNets are evaluated on a synthetic-test-set using collected SRIRs from TAU Spatial Room Impulse Response Database (TAU-SRIR DB) and achieve satisfactory performance. We also conduct our experiments to validate the transferability of PSELDNets to three publicly available datasets and our own collected audio recordings. Results demonstrate that PSELDNets surpass state-of-the-art systems across all publicly available datasets. Given the need for direction-of-arrival estimation, SELD generally relies on sufficient multi-channel audio clips. However, incorporating the AdapterBit, PSELDNets show more efficient adaptability to various tasks using minimal multi-channel or even just monophonic audio clips, outperforming the traditional fine-tuning approaches.","Sound event localization and detection (SELD) combines sound event detection (SED) with direction-of-arrival (DOA) estimation, with the goal of recognizing the categories, onsets, offsets, and DOAs of various sound sources. SELD frameworks represent audio sources in both spatial and temporal domains, making them suitable for applications such as robot listening, audio surveillance, and smart home environments. I-A Existing learning-based SELD methods In recent years, there have been notable advancements in learning-based SELD methods. Adavanne et al. [1] introduced SELDnet, an end-to-end network designed for simultaneous sound event detection and DOA estimation. Nevertheless, SELDnet faces challenges in identifying overlapping sound events of the same class from different locations. To address this homogenous overlap issue, the Event-Independent Network V2 (EINV2) is proposed [2, 3, 4]. EINV2 uses a track-wise output format and permutation invariant training to predict a single sound event and its corresponding DOA for each track. Different from SELDnet and EINV2, the Activity-coupled Cartesian DOA (ACCDOA) combines SED and DOA tasks into a single output and embeds activity information in Cartesian DOA vectors [5]. The Multi-ACCDOA (mACCDOA) [6] extends ACCDOA by incorporating a track-wise output format and employing auxiliary duplicated permutation invariant training to tackle the homogenous overlap issue. On the other hand, numerous learning-based SELD investigations [7, 2, 3, 8, 5, 6, 4, 9, 10, 11] have predominantly utilized synthetic datasets from SELD challenge events [12, 13, 14, 15, 16, 17], showing promising performance in both simulated and real spatial environments. Nonetheless, these systems have two limitations. Firstly, the target sound event classes that the systems predict must be pre-specified before training, posing a challenge since each application scenario may require different target classes. Secondly, learning-based SELD approaches may suffer from performance degradation when exposed to acoustic environments not encountered during training, i.e., a phenomenon known as environment shift [18]. One of the effective ways to tackle the problems of unknown sound event classes and unseen acoustic environments is by acquiring significant scenario-specific data for training. However, creating spatial sound event signals is a complex task involving extensive data collection and computational generation. This process requires convolving dry sound source signals with measured spatial room impulse responses (SRIRs). Moreover, manually collecting and annotating real-world spatial sound event recordings is very costly, and publicly accessible real-scene SELD data is limited [19, 20]. To mitigate these challenges, the zero-and-few-shot SELD system [21] and environment-adaptive Meta-SELD [22, 18] utilize pre-trained models to function effectively with limited data. Despite these developments, there is still a notable lack of foundation models for SELD. Conversely, several foundation models have recently been developed [23, 24, 25, 26] in sound event classification (SEC), which are highly pertinent to SELD tasks. The potential benefits of using SEC foundation models for the SELD system are still uncertain. I-B Foundation models in SEC Deep neural networks have made substantial strides in SEC research [23, 24, 25, 26]. A key milestone was the introduction of AudioSet [27], which is a comprehensive dataset featuring over 2 million human-annotated 10-second audio clips and an ontology of 527 sound classes, and utilized for general-purpose sound event recognition. Convolutional neural networks (CNNs), exemplified by Pre-trained Audio Neural Networks (PANNs) [23], extract local features from audio spectrograms and enhance performance by optimizing the network’s depth and breadth. Recently, Transformer architectures [28], which have proven effective in sequence modeling, have been adapted to computer vision by partitioning images into smaller patches [29, 30]. Inspired by these approaches, several studies, such as the Audio Spectrogram Transformer (AST) [24], the Patchout faSt Spectrogram Transformer (PaSST) [25], and the Hierarchical Token-Semantic Audio Transformer (HTS-AT) [26], apply purely attention-based models to audio spectrograms to capture long-range global context. AST [24] leverages the self-attention mechanism, overlapping patches from audio spectrograms, and pre-trained parameters from computer vision to build the first convolution-free model for SEC. Drawing inspiration from SpecAugment [31] and Dropout [32], PaSST [25] offers an efficient implementation of AST by omitting segments of the Transformer’s input sequence during training. This method encourages the Transformer to classify the events using an incomplete sequence. In comparison, HTS-AT [26] uses Swin Transformer blocks with shifted window attention [30], enhancing efficiency by limiting self-attention calculations to local non-overlapping windows while allowing cross-window connections. These models achieve state-of-the-art (SOTA) SEC results on AudioSet. Furthermore, these models, which were pre-trained on large-scale datasets, offer the potential for transferability to other SEC tasks to further improve performance [23, 24, 25, 26]. Nevertheless, the efficient transfer of these pre-trained models to various audio downstream tasks remains challenging. One common method for adapting pre-trained models to downstream tasks involves the full fine-tuning method, which fine-tunes all the transferred pre-trained parameters. However, this technique requires significant computational resources and memory capacity. On the other hand, it can result in a loss of model generalization, possibly due to catastrophic interference among tasks [33]. I-C Parameter-efficient fine-tuning To mitigate the challenges associated with efficient transfer, the parameter-efficient fine-tuning (PEFT) methodology, which only fine-tunes a small number of (extra) parameters to attain strong performance, has been extensively investigated across the domains of natural language processing [34, 35, 36, 37] and computer vision [38, 33, 39, 40]. Prominent PEFT methods include Low-Rank Adaptation (LoRA) [34], Adapter tuning [38, 33, 37], prompt tuning [40], and others. The fundamental principle of these PEFT methodologies involves freezing the primary or all pre-trained parameters and introducing additional trainable parameters for fine-tuning. Expanding on these PEFT techniques, various audio-related researchers have integrated some model-specific Adapters into their frameworks [41, 42, 43, 44]. The Adapter, a straightforward plug-and-play module, is designed for attention-based networks and entails incorporating a few lightweight bottleneck networks into the Transformer layers. These methodologies retain the generality of the pre-trained model, conserve computational resources, reduce data requirements, and attain competitive or even superior performance. I-D Our contributions In this study, we endeavor to develop a general-purpose SELD model applicable to diverse real-world scenarios. We introduce pre-trained SELD networks (PSELDNets) trained on large-scale synthetic datasets. These large-scale datasets, comprising approximately 1,167 hours of audio clips and featuring an ontology of 170 sound classes, are generated by convolving sound event clips from FSD50K [45] with simulated SRIRs. The PSELDNets, inheriting architectures of pre-trained models that achieve SOTA results in SEC, such as PANNs [23], PaSST [25] and HTS-AT [26], extract spatial and global features from multi-channel spectrograms. We evaluate the performance of PSELDNets on a synthetic-test-set that uses measured SRIRs from TAU Spatial Room Impulse Response Database (TAU-SRIR DB) [46] and obtain satisfactory performance. We transfer PSELDNets to multiple downstream publicly available datasets, including the Detection and Classification of Acoustics Scenes and Events (DCASE) 2021 Challenge Task 3 [14], the L3DAS22 Challenge Task 2 [16], the Sony-TAu Realistic Spatial Soundscapes 2023 (STARSS23) [20] dataset, and our own collected audio recordings. Experimental results demonstrate the transferability of PSELDNets, showing that the models consistently surpass SOTA benchmarks [47, 48, 49, 10, 4] across all these publicly available datasets. Inspired by PEFT techniques [38, 33, 36], we introduce a data-efficient fine-tuning method, AdapterBit, which enables the efficient utilization of low-resource data, including minimal multi-channel or even monophonic clips. By employing AdapterBit for transfer learning in downstream SELD scenarios using low-resource data, PSELDNets exhibit superior performance, compared to the conventional full fine-tuning methods. Notably, when utilizing monophonic clips, pseudo-multi-channel clips are generated by theoretical responses of the microphone array to ensure compatibility with the input to PSELDNets. The contribution of this work includes: 1. Synthesizing large-scale SELD datasets designed to include numerous sound event instances and various acoustic environments. 2. Introducing PSELDNets trained on the large-scale synthetic SELD datasets to develop a general-purpose model. 3. Transferring PSELDNets to several downstream SELD tasks and achieving the SOTA performance. 4. Proposing a data-efficient fine-tuning technique to adapt PSELDNets to target SELD tasks using limited resources. 5. Releasing the source code, the pre-trained parameters of PSELDNets, and the large-scale synthetic SELD datasets111https://github.com/Jinbo-Hu/PSELDNets."
https://arxiv.org/html/2411.05872v1,Dialectal Coverage And Generalization in Arabic Speech Recognition,"Developing robust automatic speech recognition (ASR) systems for Arabic, a language characterized by its rich dialectal diversity and often considered a low-resource language in speech technology, demands effective strategies to manage its complexity. This study explores three critical factors influencing ASR performance: the role of dialectal coverage in pre-training, the effectiveness of dialect-specific fine-tuning compared to a multi-dialectal approach, and the ability to generalize to unseen dialects. Through extensive experiments across different dialect combinations, our findings offer key insights towards advancing the development of ASR systems111The code with pre-trained & finetuned model weights is available on GitHub: https://github.com/mbzuai-nlp/ArTST for pluricentric languages like Arabic.","Figure 1: The architecture of SpeechT5/ArTST, which contains an encoder-decoder module and six modal specific pre/post-nets. During self-supervised pre-training (left), quantized tokens are shared across speech and text modalities. Hidden states and latent units are mixed up and used as the inputs of the cross-attention module in the decoder. The fine-tuning stage for ASR is shown on the right. Refer to Ao et al. (2021) for more details. The advent of large self-supervised acoustic models has transformed speech technology, enabling transfer learning and improving performance for both high-resource and low-resource languages. Prominent examples of such models include various versions of wav2vec Schneider et al. (2019); Baevski et al. (2020), HuBERT Hsu et al. (2021), and SpeechT5 Ao et al. (2021), which have predominantly been trained on English datasets. Their multi-lingual variants, e.g. XLS-R Babu et al. (2021) with 53 and 128 languages, in addition to many models that include both self-supervised and supervised pre-training, such as Whisper Radford et al. (2023) with approximately hundred supported languages, MMS Pratap et al. (2024) with thousands of languages, and UniSpeech Wang et al. (2021), underscore the potential for cross-lingual transfer learning for more inclusive ASR. Yet, while these models indeed show great potential for transfer learning to new languages, even those unseen in training Huang et al. (2013), they remain sub-optimal for specific target languages. A case in point is the Arabic Text and Speech Transformer (ArTST), a model pre-trained exclusively on Arabic, which has demonstrated superior performance for Modern Standard Arabic (MSA), surpassing larger multilingual models like Whisper and MMS in benchmark tests, in addition to establishing a new state-of-the-art (SOTA) performance compared to previous efforts for Arabic ASR. This highlights the advantage of monolingual pre-training when large amounts of unlabeled data for the target language are available. While the model showed some potential for dialectal coverage, it was trained and validated mainly on MSA data, which questions its applicability for spoken dialectal variants of Arabic. Arabic is a pluricentric language Schuppler et al. (2024), diverse in regional variations, and models trained on MSA frequently struggle to adapt to these variations. This limitation is particularly acute given that many Arabic dialects are underrepresented or considered low-resource in speech technology research. Consequently, there is a need for optimized ASR systems that embrace, rather than overlook, the linguistic diversity of the Arabic-speaking world. In light of these challenges, we conduct various investigations aimed at understanding and enhancing the dialectal diversity and performance of Arabic ASR systems. We focus on three inquiries aimed at optimizing potential strategies for integrating dialectal variation into ASR systems. First, we measure the impact of incorporating a broad collection of Arabic dialects during the model’s pre-training phase. We hypothesize that a wider dialectal foundation could improve the model’s performance across various dialects in the fine-tuning stage. Second, we quantify the comparative effectiveness of dialect-specific fine-tuning versus a more holistic, multi-dialectal fine-tuning strategy. The third question examines the model’s capacity for zero-shot transfer to dialects not explicitly included in fine-tuning. Our key findings from experiments spanning over 12 Arabic dialects are: (1) pre-training with more data and wider dialectal coverage improves performance across most dialectal variants, including MSA, (2) multi-dialectal fine-tuning improves performance for low-resource dialects, but may not be optimal for high-resource dialects, and (3) multi-dialectal pre-training and fine-tuning has higher potential for zero-shot transfer to unseen dialects. Our pre-training checkpoints and joint models are trained exclusively on open-source data and will be released as open-source models."
https://arxiv.org/html/2411.05794v1,Beyond Correlation: Evaluating Multimedia Quality Models with the Constrained Concordance Index,"This study investigates the evaluation of multimedia quality models, focusing on the inherent uncertainties in subjective Mean Opinion Score (MOS) ratings due to factors like rater inconsistency and bias. Traditional statistical measures such as Pearson’s Correlation Coefficient (PCC), Spearman’s Rank Correlation Coefficient (SRCC), and Kendall’s Tau (KTAU) often fail to account for these uncertainties, leading to inaccuracies in model performance assessment. We introduce the Constrained Concordance Index (CCI), a novel metric designed to overcome the limitations of existing metrics by considering the statistical significance of MOS differences and excluding comparisons where MOS confidence intervals overlap. Through comprehensive experiments across various domains including speech and image quality assessment, we demonstrate that CCI provides a more robust and accurate evaluation of instrumental quality models, especially in scenarios of low sample sizes, rater group variability, and restriction of range. Our findings suggest that incorporating rater subjectivity and focusing on statistically significant pairs can significantly enhance the evaluation framework for multimedia quality prediction models. This work not only sheds light on the overlooked aspects of subjective rating uncertainties but also proposes a methodological advancement for more reliable and accurate quality model evaluation.","The evaluation of multimedia quality models [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], is essential for assessing the efficacy of signal processing algorithms across various domains such as inpainting, enhancement, codecs, voice call standards, and media streaming. The performance of objective quality models is typically evaluated by comparing the predicted Mean Opinion Score (MOS) with human-rated MOS, employing statistical measures for accuracy. The MOS can be derived by averaging individual ratings on a 5-point Absolute Category Rating (ACR) scale, with the possibility of extending this scale to capture more nuanced user perceptions (e.g., a 10-point ACR scale). Following the ITU-T P.1401 guidelines [11], the evaluation process involves statistical tools like the root mean squared error* (RMSE*), Pearson’s correlation coefficient (PCC), and the outlier ratio (OR) to analyze the average error, linearity, and the prevalence of outliers, respectively. In addition, Spearman’s rank correlation coefficient (SRCC) and Kendall’s tau correlation coefficient (KTAU) offer insights into monotonic relationships and the balance between concordant and discordant pairs. These methodologies are widely used to evaluate regression tasks, but they neglect that subjective MOS ratings are highly influenced by several factors such as rating inconsistency, bias from MOS distribution [12], order-effect, and rater-specific biases like culture and language. Ignoring this uncertainty can lead to inaccuracies in the performance evaluation of quality models. The exploration into evaluating instrumental quality models with a focus on rater uncertainty is limited, particularly neglecting factors such as the restriction of range, sample size, and variability among rater groups, which are critical for a comprehensive evaluation of multimedia quality prediction. These factors can be summarised as follows: • Small sample size impacts the robustness of statistical measures like PCC, SRCC, and KTAU, particularly when the evaluations are based on averaging across a limited set of conditions111We use the term condition to indicate a particular degradation at a specific intensity e.g., gaussian noise added to images at 20 dB signal-to-noise ratio (SNR). This can introduce a sampling bias, as these measures may not accurately reflect the population’s characteristics with a small dataset, leading to potential inaccuracies in the evaluation of objective quality models [13, 14]. • Restriction of range refers to the issue arising when statistical metrics like PCC or SRCC are calculated over a subset of the full data range, leading to potential deviations in their values. This problem is accentuated in uncontrolled environments, such as user-generated content or in-the-wild assessments, where the quality range is uncontrolled and might not accurately represent the entire spectrum of quality [15, 16, 17]. • Variability among rater groups emerges, especially in crowdsourced settings, because MOS is a relative scale as shown in several studies [18, 19, 12, 20, 21]. In crowdsourcing [22, 23, 24, 25, 26, 27, 28, 29], different stimuli are assigned to different groups due to the large size of the dataset that needs to be annotated. As a consequence, every group will necessarily introduce a group bias effect. Krasula et al.’s work [15] marks a significant step forward in proposing a performance evaluation metric that identifies significantly different pairs and quantifies uncertainty in objective metric predictions, thus enhancing the evaluation framework to account for the statistical significance of subjective scores and the integration of data from varied subjective experiments. Another study that questions the reliability of PCC has been proposed for speech quality models [30]. Here, the authors propose a Bayesian model selection to evaluate instrumental quality models, showing that PCC does not have high explanatory power compared to their approach. Both studies confirm the inadequateness of statistical metrics but do not address the three issues mentioned above: small sample sizes, restriction of range, and variability among rater groups. This paper seeks to address this gap by evaluating the performance of instrumental quality models under these three scenarios. We conduct a thorough examination of statistical metrics for instrumental quality models to evaluate their robustness in these scenarios. In addition, we introduce the Constrained Concordance Index (CCI), a novel metric designed to assess the performance of instrumental quality models. CCI measures the capability of these models to accurately rank pairs where MOS has high precision and ignores the ones with uncertain MOS. Our findings demonstrate that typical statistical metrics (PCC, SRCC, and KTAU) lack robustness in the three scenarios mentioned above, whereas the proposed CCI effectively addresses this issue. We proposed the CCI metric in our previous study to evaluate objective quality models for sound archives [31]222We introduced the CCI metric in [31] with another name: pairwise ranking accuracy (PRA). In our previous paper [31] we have only used the CCI and motivated its usage but we did not compare it against traditional metrics and show its robustness for multimedia quality model evaluation. In this paper, we slightly modify the CCI metric (see Section 3) and we do an extensive comparison of the CCI metric against traditional statistical metrics (PCC, SRCC, and KTAU) to evaluate the robustness against the three scenarios mentioned above. One of the contributions of this paper is also the evaluation of traditional statistical metrics in these three scenarios. To achieve this we select raters and samples from laboratory-based speech and image quality datasets through bootstrapping. By extracting a subset of stimuli and raters from these lab-based quality databases, we examine the extent to which statistical metrics diverge from their values obtained when using the full set of raters and stimuli. In Section 2 we outline the motivations for proposing a new metric that overcomes the issues of traditional statistical metrics for the three studied scenarios: small sample size, restriction of range, and rater group variability. Section 3 is dedicated to the description of the proposed metric CCI. Experiment setup and results comparing all the statistical metrics are shown in Section 4 while a description of how to interpret and visualise the proposed CCI metric is described in Section 5. Finally, we provide a paper discussion in Section 6 and our conclusions in Section 7. The Python code to reproduce the experiments and the CCI metric are available on GitHub333https://github.com/alessandroragano/muqeval."
