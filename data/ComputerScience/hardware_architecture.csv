URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10285v1,"Systolic Arrays and Structured Pruning Co-design 
for Efficient Transformers in Edge Systems","Efficient deployment of resource-intensive transformers on edge devices necessitates cross-stack optimization. We thus study the interrelation between structured pruning and systolic acceleration, matching the size of pruned blocks with the systolic array dimensions. In this setting, computations of pruned weight blocks can be skipped, reducing run-time and energy consumption, but potentially impacting quality of service (QoS). To evaluate the trade-offs between systolic array size and sparsity opportunities, we present a novel co-design framework that integrates algorithmic optimization, system simulation, and hardware design. Targeting speech recognition using transformers as case study, we analyze how configuration choices across the stack affect performance metrics. Results demonstrate that structured pruning on systems featuring systolic array acceleration can effectively increase performance, while maintaining high QoS levels. Up to 26% system-wide speedups due to structured pruning were measured, with only 1.4% word error rate degradation on the standard Librispeech dataset.","Transformers have fostered a revolution in machine learning, with applications ranging from classification [1] to generative models for text and images [2], to speech recognition [3]. However, their complex structure based on multiple attention and feed-forward layers [4] results in unprecedented computational requirements, posing significant challenges for their deployment. These are particularly acute in edge scenarios, where systems have to operate within constrained energy and performance envelopes. In this context, a plethora of optimization strategies have been proposed. On the software side [5], commonly used approaches involve reducing the precision of data representations (quantization) and removing parts that contribute the least to inference outcomes (pruning). As for hardware, efforts have mainly focused on the acceleration of the main computational kernel in transformers, i.e. General Matrix Multiplications (GEMMs). Although diverse solutions ranging from analog crossbars [6][7] to near-DRAM computing [8][9] work toward this goal, a particularly promising alternative is represented by systolic arrays [10]. These two-dimensional meshes of processing elements can indeed parallelize the computation of a GEMM (or, more precisely, the computation of a GEMM tile), while presenting a high parallelism degree, low resource requirements and only mandating a simple, low-overhead control logic. Recent works [11, 12, 13, 14, 15] have attempted to co-optimize software algorithms and hardware accelerators dedicated to transformer inference [16]. Such a stance is particularly appealing at the crossroads of model pruning and systolic array acceleration. On the software side, pruning can be performed by eliding weights in regular block patterns (in a “structured” way) rather than as individual elements [17]. While this approach introduces a constraint to pruning, and can hence result in lower overall sparsity rates, it substantially amplifies hardware-side optimization opportunities when matching the sizes of the pruned tile and the accelerator mesh. The exploration of this strategy, which we term Systolic Array Structured Pruning (SASP), is the focus of this work. SASP opens a complex multidimensional design space which requires careful consideration of metrics spanning from hardware to algorithms. Indeed, while a larger accelerator can expose a higher degree of parallelism, it also requires more resources (area / energy). Moreover, SASP settings with larger tiles may overly penalize the achievable sparsity for a desired Quality of Service (QoS) or, alternatively, result in high QoS degradation for a fixed pruning rate. To explore these interrelations, we employ a holistic approach integrating methods for a) the structured pruning of transformer algorithms, b) the system-level level modeling of accelerated systems executing them, and c) the hardware synthesis of accelerators. Our environment for SASP exploration builds on frameworks for the training of transformers (ESPnet [3]) and for system simulation (gem5 [18]). By employing a novel systolic array architectural template, it supports both floating point and weight-quantized data representations, as supported by ESPnet. As a test case, we employ our exploration approach to analyze a speech recognition application, based on an 24-block, 75M-parameter transformer processing the LibriSpeech dataset [19]. We observed that SASP can achieve, for a systolic array size of 32\times 32, up to 44% speedup and 42% energy savings over a non-pruned, non-quantized system when employing a 20% pruning rate, resulting in a marginal Word Error Rate (WER) degradation of 1.4%. The contributions of this paper are summarized as follows: • We introduce a methodology for the systematic exploration of Systolic Array Structured Pruning (SASP), a co-design strategy that combines systolic array acceleration and structured pruning with matching accelerator and tile size. • We show how the insights collected from our framework enable the evaluation of figures of merit at different abstraction levels, including the assessment of QoS, performance, resource usage, and energy, as well as their trade-offs. We discuss how these can be effectively leveraged from the joint perspective of algorithmic optimization, system integration, and systolic array design. • Using a speech recognition case study, we show that SASP-based co-optimization of transformers and systolic arrays can lead to efficiency and speedup gains of up to 26% with minimal QoS impact."
https://arxiv.org/html/2411.09760v1,SpecPCM: A Low-power PCM-based In-Memory Computing Accelerator for Full-stack Mass Spectrometry Analysis,"Mass spectrometry (MS) is essential for proteomics and metabolomics but faces impending challenges in efficiently processing the vast volumes of data. This paper introduces SpecPCM, an in-memory computing (IMC) accelerator designed to achieve substantial improvements in energy and delay efficiency for both MS spectral clustering and database (DB) search. SpecPCM employs analog processing with low-voltage swing and utilizes recently introduced phase change memory (PCM) devices based on superlattice materials, optimized for low-voltage and low-power programming. Our approach integrates contributions across multiple levels: application, algorithm, circuit, device, and instruction sets. We leverage a robust hyperdimensional computing (HD) algorithm with a novel dimension-packing method and develop specialized hardware for the end-to-end MS pipeline to overcome the non-ideal behavior of PCM devices. We further optimize multi-level PCM devices for different tasks by using different materials. We also perform a comprehensive design exploration to improve energy and delay efficiency while maintaining accuracy, exploring various combinations of hardware and software parameters controlled by the instruction set architecture (ISA). SpecPCM, with up to three bits per cell, achieves speedups of up to 82× and 143× for MS clustering and DB search tasks, respectively, along with a four-orders-of-magnitude improvement in energy efficiency compared with state-of-the-art CPU/GPU tools.","\IEEEPARstart Mass spectrometry (MS) is a key analytical tool used by proteomics and metabolomics, aiding in drug discovery and chemical analysis by identifying and quantifying molecules based on their mass-to-charge ratios[1]. Its high sensitivity and precision have made it one of the most widely used techniques for detecting even the smallest molecular variations. However, one of the key challenges in MS processing is handling the vast and continually growing data volumes. For example, the MassIVE database, a publicly accessible repository for proteomics MS data [2], now exceeds 600 TB (as of September 2024) and continues to grow at an accelerating pace, with hundreds of terabytes of new spectral data added annually. The MS analysis process involves comparing spectra generated from MS experiments against an extensive reference library to identify proteins, a procedure known as database (DB) search [3]. To accelerate the search process, spectral clustering is done by grouping similar reference spectra together. During search, the query is first compared to the cluster centroids, quickly focusing the search on an appropriate cluster and thereby speeding up the overall process [4]. Ideally, such a database should be clustered on a daily basis as new samples are continually added, but this is currently done only once per year due to the excessive time required, resulting in lower accuracy. Traditional systems with separate memory and processor units are hindered by limited data movement bandwidth and computing efficiency in processing such a sheer amount of data. Modern MS analysis tools [5, 6, 7], whether based on conventional CPU or GPU architectures, often spend more than 60% of their time on large matrix operations with significant memory footprint. These tools struggle to manage large datasets efficiently, mainly due to the high energy consumption and latency involved in data transfer. To overcome these challenges, in-memory computing (IMC) has emerged as an alternative paradigm that processes data directly within the memory where it is stored, substantially reducing the latency and energy overhead caused by data movement. To capitalize on this opportunity, various memory topologies have been explored, including DRAM, SRAM, RRAM, PCM, NAND Flash, and other emerging memory technologies [8]. While RRAM has been widely adopted [9, 10, 11] for its well-recognized high-density and efficient read operations, particularly due to its support for multi-level cells (MLC), it faces limitations, such as high energy consumption and high voltage requirements during write operations. This drawback will significantly degrade the energy efficiency of the clustering process, where frequent data updates are required to adapt to the newly collected MS data. While NAND flash memories provide superior memory density and fabrication maturity, they suffer from relatively high latency, e.g., several \mu s[12], due to the high resistance in the read-path, which is caused by the nature of reading data from serially connected cells. This work adopts a recently developed multi-level phase change memory (PCM) [13] based on superlattice materials, which features lower error rates, reduced voltage requirements, faster and more energy-efficient programming. In particular, we aim to explore the analog IMC, which offers dramatic efficiency improvements, achieving more than two orders of magnitude benefit [8] compared to conventional digital counterparts by utilizing low-voltage swing analog operations. Additionally, the analog IMC performs both reading and computation across the entire bitcell array simultaneously, enabling high parallelism and significant compute density improvement. Consequently, the analog IMC on PCM facilitates efficient processing for classification while also enabling the effective updating of stored weights to adapt to newly collected MS data. From an algorithmic perspective, we employ hyperdimensional computing (HD), a brain-inspired computing paradigm that leverages lightweight and highly parallel operations by encoding input features into high-dimensional (long) binary vectors. HD replaces costly and complex floating-point arithmetic with simpler binary or integer operations, which can be executed in parallel, leading to dramatic throughput improvements as demonstrated in [14, 15, 16]. Furthermore, HD’s data representation in hyperspace offers significant error resilience, with data points being well separated by large geometric distances. This property has been demonstrated in previous work [10], where HD tolerated up to a 10% bit error rate for MS DB search tasks. This resilience creates a strong synergy with analog IMC on emerging devices, helping to overcome their computing and storage non-idealities while achieving greater storage density and computing efficiency. The use of MLC PCM involves complex trade-offs between efficiency and accuracy, which require careful optimization across both hardware and algorithms. These interrelated challenges cannot be addressed at a single abstraction level. Therefore, we introduce SpecPCM, an IMC accelerator designed for the efficient processing of MS workloads. This framework integrates design efforts across the entire vertical stack, spanning application, algorithm, circuit, device, and instruction set levels, to enhance performance throughout the end-to-end MS pipeline. The detailed contributions of this work are summarized as follows: 1. We propose an analog IMC system with architecture and circuits specifically tailored for MS algorithms. While prior works have applied IMC to MS database (DB) search tasks in the HD domain, our work is the first to apply IMC for both clustering and DB search. 2. At the algorithm level, we introduce a new HD encoding method, called dimension packing, to maximize storage density by leveraging multi-level PCM devices while maintaining the simplicity of the binary representation of HD vectors. 3. At the device level, we propose customized PCM devices to meet the distinct requirements of clustering and MS DB search by optimizing the materials differently for each task, based on measured characterization results from the fabricated devices. 4. We conduct hardware-software co-design through a comprehensive analysis to balance trade-offs between latency, energy efficiency, and accuracy, taking into account various parameters such as bits per cell, write-verify cycles, analog-to-digital converter (ADC) precision, and HD dimensions, all controlled by the instruction set. The results indicate that the proposed SpecPCM demonstrates speedup of up to 82\times for clustering and 143\times for database search over state-of-the-art (SoA) solutions, with a four-orders-of-magnitude in energy efficiency improvement, while maintaining on-par accuracy across datasets of different scales."
https://arxiv.org/html/2411.10005v1,Analyzing Performance Characteristics of PostgreSQL and MariaDB on NVMeVirt,"The NVMeVirt paper analyzes the implication of storage performance on database engine performance to promote the tunable performance of NVMeVirt. They perform analysis on two very popular database engines, MariaDB and PostgreSQL. The result shows that MariaDB is more efficient when the storage is slow, but PostgreSQL outperforms MariaDB as I/O bandwidth increases. Although this verifies that NVMeVirt can support advanced storage bandwidth configurations, the paper does not provide a clear explanation of why two database engines react very differently to the storage performance.To understand why the above two database engines have different performance characteristics, we conduct a study of the database engine’s internals. We focus on three major differences in Multi-version concurrency control (MVCC) implementations: version storage, garbage collection, and index management. We also evaluated each scheme’s I/O overhead using OLTP workload. Our analysis identifies the reason why MariaDB outperforms PostgreSQL when the bandwidth is low.","The NVMeVirt is a versatile software-defined virtual NVMe device. Because the NVMeVirt supports advanced storage configurations, it can be used for database engine analysis and allows us to estimate the performance of database engines on future storage devices. In NVMeVirt paper (Kim et al., 2023), the authors conducted an evaluation on PostgreSQL (Group, 2023b) and MariaDB (Foundation, 2023) with OLTP workload using sysbench (Kopytov, 2023). They measured various performance metrics while running the benchmark. The result indicates that MariaDB and PostgreSQL react differently to the storage performance. MariaDB fully utilizes the I/O bandwidth up to 500 MiB/s. However, I/O bandwidth utilization remains around 600 MiB/s even when the storage device provides higher bandwidth. On the other hand, PostgreSQL fully utilizes the I/O bandwidth up to 1,000 MiB/s, and the performance is saturated at approximately 1,800 MiB/s. The I/O bandwidth affects both database engines’ performance, but PostgreSQL is much more sensitive than MariaDB. From the evaluation, the authors conclude that PostgreSQL is more promising on modern storage devices, whereas MariaDB is more efficient when the storage is low. The result verifies the tunable performance of NVMeVirt, but the problem is that the paper does not provide a clear explanation of what features of database engine internals make such differences. In this paper, we analyze the implication of storage performance on database engine performance focusing on database engine internals. We aim to provide a clear explanation of why PostgreSQL is more sensitive to I/O bandwidth than MariaDB. The contributions of this work are as follows: • We perform experiments for both OLTP and OLAP workloads and analyze the different performance characteristics (Section 2). • We analyze what differences in database engine internals make PostgreSQL more sensitive to I/O bandwidth compared to MariaDB (Section 3). • We evaluated different MVCC schemes using OLTP workloads (Section 4)."
https://arxiv.org/html/2411.09546v1,Architectural Exploration of Application-Specific Resonant SRAM Compute-in-Memory (rCiM),"While general-purpose computing follows Von Neumann’s architecture, the data movement between memory and processor elements dictates the processor’s performance. The evolving compute-in-memory (CiM) paradigm tackles this issue by facilitating simultaneous processing and storage within static random-access memory (SRAM) elements. Numerous design decisions taken at different levels of hierarchy affect the figure of merits (FoMs) of SRAM, such as power, performance, area, and yield. The absence of a rapid assessment mechanism for the impact of changes at different hierarchy levels on global FoMs poses a challenge to accurately evaluating innovative SRAM designs. This paper presents an automation tool designed to optimize the energy and latency of SRAM designs incorporating diverse implementation strategies for executing logic operations within the SRAM. The tool structure allows easy comparison across different array topologies and various design strategies to result in energy-efficient implementations. Our study involves a comprehensive comparison of over 6900+ distinct design implementation strategies for EPFL combinational benchmark circuits on the energy-recycling resonant compute-in-memory (rCiM) architecture designed using TSMC 28 nm technology. When provided with a combinational circuit, the tool aims to generate an energy-efficient implementation strategy tailored to the specified input memory and latency constraints. The tool reduces 80.9% of energy consumption on average across all benchmarks while using the six-topology implementation compared to baseline implementation of single-macro topology by considering the parallel processing capability of rCiM cache size ranging from 4KB to 192KB.","Cache memory remains one of the critical components in our computing system, enhancing overall performance by bridging the speed gap between the main memory (RAM) and the central processing unit (CPU). Besides, in recent years, static random access memory (SRAM)-based in-memory computing paved a promising direction to enable energy-efficient computation. However, the lack of design and automation tools to map computation on optimal SRAM architecture increases design time-to-market, resulting in higher engineering costs. This research resolves this issue by proposing an architectural exploration tool that efficiently maps logic computations to optimal cache architecture. Figure 1: (a) Conventional Von Neumann architecture, where an operation f is performed on data D within the CPU, incurs high data movement overhead, which can be reduced using (b) a CiM architecture, where f is computed directly within the memory, with the CPU primarily functioning as a control unit. Computing-in-memory (CiM) architectures have emerged as highly promising solutions for data-intensive applications. They minimize data movement, enhance computational capabilities, and improve the system’s overall energy efficiency by processing and storing data within cache memory. As shown in Figure 1 (a), the traditional Von Neumann architecture relies on data communication between the arithmetic logic unit (ALU) and cache memory through address and data buses. However, as the CPU performance is significantly higher than the memory performance, the Von Neumann architectures often create memory bottlenecks. CiM architectures, as shown in Figure 1 (b), mitigate the impact of large memory access latencies by performing the computations within the memory. By reducing data movement and exploiting parallelism within the memory, CiM architectures significantly enhance computational efficiency and performance. SRAM-based CiM architectures have been heavily investigated for performing various operations, such as matrix-vector multiplication (MVM) [1, 2], multiply-and-accumulate (MAC) operations [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], boolean logic operations [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30], and content-addressable memory (CAM) [31, 32, 33, 34, 35, 36] operations for fast searching operations. However, none presents a generic energy-saving architecture that spans across various applications. This work utilizes a novel series-resonance-based resonant CiM (rCiM) architecture that reduces dynamic power consumption by recycling the wasted energy during writing operations. This work proposes an agile architectural exploration tool to map various logical operations to an optimal SRAM macro cache size. The primary objective of the tool is to facilitate the development of novel energy-efficient SRAM-based energy-recycling rCiM implementations individually designed for specific boolean logical applications. In particular, the main contributions of the paper are as follows: • A novel resonant Compute-in-Memory (rCiM) structure that incorporates a series inductor to recycle energy dissipated during write operations. • An architectural exploration toolflow that integrates open-source synthesis tools (Berkeley-ABC [37] & YOSYS [38]) to identify the optimal SRAM configuration within a specified range of SRAM cache memory and map efficient logical operations tailored to an optimal rCiM macro size. • Comprehensive analysis of 6900+ distinct logical design implementations for EPFL combinational benchmark circuits [39] using 12 different SRAM topologies."
https://arxiv.org/html/2411.09315v1,Sustainable Hardware Specialization,"Hardware specialization is commonly viewed as a way to scale performance in the dark silicon era with modern-day SoCs featuring multiple tens of dedicated accelerators. By only powering on hardware circuitry when needed, accelerators fundamentally trade off chip area for power efficiency. Dark silicon however comes with a severe downside, namely its environmental footprint. While hardware specialization typically reduces the operational footprint through high energy efficiency, the embodied footprint incurred by integrating additional accelerators on chip leads to a net overall increase in environmental footprint, which has led prior work to conclude that dark silicon is not a sustainable design paradigm.We explore sustainable hardware specialization through reconfigurable logic that has the potential to drastically reduce the environmental footprint compared to a sea of accelerators by amortizing its embodied footprint across multiple applications. We present an abstract analytical model that evaluates the sustainability implications of replacing dedicated accelerators with a reconfigurable accelerator. We derive hardware synthesis results on ASIC and CGRA (a representative reconfigurable fabric) for chip area and energy numbers for a wide variety of kernels. We input these results to the analytical model and conclude that reconfigurable fabric is more sustainable. We find that as few as a handful to a dozen accelerators can be replaced by a CGRA. Moreover, replacing a sea of accelerators with a CGRA leads to a drastically reduced environmental footprint (by a factor of 2.5\times to 7.6\times).","The end of Dennard scaling (Dennard et al., 1974) has dramatically changed how we design processors. Increased power density as we transition to new chip technology nodes leads to dark silicon (Venkatesh et al., 2010; Esmaeilzadeh et al., 2011), which means that we cannot power on the entire chip while keeping thermals within a safe operating range. Hardware specialization enables continuous performance scaling despite dark silicon by executing specific kernels on dedicated hardware accelerators. Powering on an accelerator provides high performance when needed; when not in use, an accelerator is powered off to save power. The advent of dark silicon has created a flurry of work in hardware specialization, sometimes referred to as the ‘golden age for computer architecture’ (Hennessy and Patterson, 2019), with accelerators for machine learning, video coding and decoding, image signal processing, security encryption/decryption, etc. In fact, a modern-day computer is a system-on-chip (SoC) in which general-purpose CPU and GPU cores are complemented with a sea of (multiple tens of) domain-specific accelerators (DSAs) (Hill and Reddi, 2019). This is the case across the computing spectrum from mobile application processors (e.g., Qualcomm Snapdragon (Qualcomm, 2022)) to laptop and desktop processors (e.g., Apple M2 (Apple, 2022), Intel Sapphire Rapids (Intel, 2022)) and server processors (e.g., AMD EPYC (AMD, 2023), IBM Telum (IBM, 2021)). The number of DSAs integrated on chip has steadily increased over time: Shao et al. (Shao et al., 2023) report that for Apple SoCs, the number of DSAs has increased from less than 10 in the A4 (2010) to more than 40 in the A12 (2018). Fundamentally, dark silicon trades off chip area for power efficiency, i.e., hardware specialization boosts performance within the available power budget by powering on hardware resources only when needed. Dark silicon thus comes at the cost of additional transistors to implement the various DSAs on chip. Fortunately, transistors have become exponentially cheaper over time thanks to Moore’s Law (Bohr and Young, 2017), which makes dark silicon economically viable (even today, despite Moore’s Law slowing down). In other words, hardware specialization offers continuous performance scaling without increasing the area (and cost) per chip. Figure 1. Chip area and embodied footprint per standard cell (left axis) and embodied footprint per unit of chip area (right axis) for various chip technology nodes normalized to 28 nm (Garcia Bardon et al., 2020; L et al., 2023). Keeping chip area constant to accommodate dark silicon comes at the cost of an increased embodied footprint. There is a severe downside to dark silicon though, which has been largely ignored, namely its environmental footprint. With information and communication technology (ICT) being responsible for 2.1% to 3.9% of the global greenhouse gas (GHG) emissions world-wide (Freitag et al., 2021) — currently on par with the aviation industry, and it is projected to continue to grow (Eeckhout, 2023). Figure 1 (vertical axis on the left) reports chip area and the embodied carbon footprint (i.e., environmental footprint due to manufacturing, measured in CO2e equivalent) per standard cell for nine available and projected technology nodes as provided by imec (Garcia Bardon et al., 2020; L et al., 2023), normalized to 28 nm. Continuous advancements in chip technology have dramatically reduced chip area as well as the embodied footprint per standard cell. However, the embodied footprint does not reduce at a similar pace as chip area due to increased complexity in manufacturing, i.e., more processing steps leading to increased energy consumption and GHG emissions. This implies that for a constant unit of chip area, the embodied footprint for logic has substantially increased, see Figure 1 (vertical axis on the right). In other words, dark silicon comes at the cost of a substantially increased embodied footprint. The question now is whether the increase in embodied footprint is offset by the decrease in operational footprint due to device use during its entire lifetime. While hardware specialization typically reduces energy consumption (due to higher performance and/or lower power) when in use, thereby reducing the operational footprint, the embodied footprint for integrating the DSA on chip has to be incurred regardless. This suggests that dark silicon only leads to a net reduction in environmental footprint if the DSAs are frequently used, which seems to contradict the notion of dark silicon. This has led prior work to suggest that dark silicon is not sustainable from an environmental perspective (Brunvand et al., 2019; Eeckhout, 2024). Figure 2. Breakdown of the environmental footprint in production, transportation, use and end-of-life processing. The embodied footprint dominates for most computing devices. In this work, we explore a sustainable alternative to dark silicon, namely hardware specialization through reconfigurable logic. The intuition that underpins this work is that a fabric that can be dynamically reconfigured or reprogrammed across applications can possibly achieve the best of both worlds, i.e., yield high performance at high power efficiency without incurring the vast embodied footprint of dark silicon. Replacing the sea of DSAs by a reconfigurable fabric with overall smaller area has the potential to drastically reduce the embodied footprint. On the flip side, a reconfigurable fabric is less efficient and leads to increased energy consumption and thus a higher operational footprint. The fundamental question hence is whether the decrease in embodied footprint offsets the increase in operational footprint — if so, reconfigurable logic is a more sustainable design paradigm than dark silicon. To investigate the opportunity for sustainable hardware specialization through reconfigurable logic, we formulate an abstract analytical model to compare the environmental footprint of a sea of DSAs versus a reconfigurable fabric. The model determines the critical DSA count (CDC) or the number of DSAs the reconfigurable fabric needs to replace for the latter to be more sustainable. Despite the model being parameterized to account for inherent data uncertainty, several interesting insights can be obtained. First, we find that — contrary to common belief — area efficiency is more important than energy efficiency for a DSA to be sustainable. Second, CDC decreases (1) with an increasing contribution of the embodied footprint in the overall environmental footprint of a device, (2) with a decreasing area (and to a lesser extent the energy) efficiency of a DSA relative to the reconfigurable fabric, and (3) with a decreasing degree of DSA concurrency during application execution. To quantify the area and energy efficiency of a DSA versus a reconfigurable fabric, we map a total of eight widely used application kernels to DSA as well as reconfigurable fabric while assuming iso-performance implementations. We consider standard-cell ASIC implementations for the DSA, and coarse-grain reconfigurable array (CGRA) (Karunaratne et al., 2017; Nowatzki et al., 2017; Prabhakar et al., 2017; Gobieski et al., 2021) for the reconfigurable fabric. We report that a DSA incurs on average 0.27\times and 0.31\times less chip area and energy compared to CGRA, respectively. This suggests that for embodied footprint dominated systems, the carbon footprint of CGRA equals that of 4 to 5 DSAs. In a system comprising a total of 40 DSAs, this reduces the environmental footprint by a factor of 2.5\times to 7.6\times. The overall conclusion is that CGRA is a sweet spot, paving a way towards sustainable hardware specialization."
https://arxiv.org/html/2411.09201v1,"Noncontact Multi-Point Vital Sign Monitoring 
with mmWave MIMO Radar","Multi-point vital sign monitoring is essential for providing detailed insights into physiological changes. Traditional single-sensor approaches are inadequate for capturing multi-point vibrations. Existing contact-based solutions, while addressing this need, can cause discomfort and skin allergies, whereas noncontact optical and acoustic methods are highly susceptible to light interference and environmental noise. In this paper, we aim to develop a non-contact, multi-point vital sign monitoring technique using MIMO radar, focused on physically differentiating and precisely measuring chest-wall surface vibrations at multiple points induced by cardiopulmonary mechanical activity. The primary challenges in developing such a technique involve developing algorithms to extract and separate entangled signals, as well as establishing a reliable method for validating detection accuracy. To address these limitations, we introduce MultiVital, a wireless system that leverages mmWave Multiple-input Multiple-output (MIMO) radar for synchronous multi-point vital sign monitoring. It integrates two reference modalities: five-channel seismocardiography (SCG) sensors and a one-channel electrocardiogram (ECG) electrode, enabling comprehensive radar-based research and performance validation across multiple physiological metrics. Additionally, we have developed a multi-modal signal processing framework, consisting of a radar signal processing module, an SCG calibration module, and a spatial alignment scheme. To evaluate the radar signal processing module, we conducted mathematical derivation and simulation. The experimental results indicate that the noncontact MultiVital system achieves multi-point synchronous monitoring with high precision, highly consistent with the results from reference modalities. This system enables the precise detection of subtle cardiopulmonary movements in different regions of the human body, providing more accurate and comprehensive information for cardiopulmonary health monitoring.","Vital sign detection by measuring chest-wall surface vibrations induced by cardiopulmonary mechanical activity is critical for monitoring cardiovascular and respiratory health, as well as diagnosing related diseases [1, 2, 3]. More specifically, surface vibrations generated by different chambers of the heart and lungs exhibit inherent positional specificity, even within the same cardiac or respiratory cycle [4], providing more localized and detailed information about physiological changes. Previous single-sensor-based solutions are significantly limited, as they only capture the aggregate vibrations from the entire sternal surface, lacking the positional resolution needed for detailed analysis[5, 6, 7, 8, 9, 10, 11, 12, 13]. The multi-point vibration synchronous measurement solution enables position-specific sensing. Multiple contact or non-contact sensors array placed on/over the chest provide a wealth of diversified information to analyze complex heartbeat and breath motion dynamics with better spatial and temporal resolution, which facilitates detecting more location-specific motion of each valve or chamber and identifying the signal source. Presently, multi-point measurements are mainly based on contact and semi-contact methods. Since Okada first proposed the multi-channel recording of heart sounds, many sensing technologies were applied to this [14], like the microphone array [15], fiber optic technology [16], and infrared (IR) cameras and optical markers [17, 18]. Additionally, the hottest study is multichannel seismocardiography (mchSCG or MSCG) to understand the distribution of vibration waves on the chest wall by placing multiple accelerometers on the chest [19, 20, 21]. Based on the MSCG, rich location-specific feature points in a cardiac cycle corresponding to the multiple valvular auscultation locations were identified [19] combined with synchronous electrocardiogram (ECG) and echocardiography recording. Unfortunately, the above-mentioned contact solutions would inevitably result in unpleasant user experience and even skin allergies, while sensors or electrodes are attached to the skin for a long time. On the other hand, the non-contact solutions based on optics or acoustics are highly susceptible to ambient light or noise, rendering them unsuitable for clinical diagnosis and home health monitoring [22, 23, 24, 25]. In contrast to wearable devices and cameras, wireless devices such as mmWave Multiple-input Multiple-output (MIMO) radar present a more promising approach for monitoring multi-point vital sign signals [26, 27, 28, 29]. mmWave MIMO radar systems utilize high frequency signals in the millimeter wave spectrum (typically 30 GHz to 300 GHz) and the MIMO architecture (multiple transmitting and receiving antennas) to achieve high resolution detection [12, 30, 31], thus separating multiple scattering points from human chest. These mmWave MIMO radars can be conveniently installed on walls and ceilings of both homes and hospitals, ensuring they integrate smoothly into people’s daily routines without causing any disturbances. The radars work by emitting electromagnetic waves into the space, then collecting the waves reflected back by individuals. Advanced signal processing techniques are employed to analyze these reflections, allowing for an accurate assessment of a person’s condition [32, 33, 34, 10, 35, 36]. By using mmWave radar instead of wearable devices or cameras, individuals no longer need to experience discomfort caused by wearable devices or have the privacy concerns introduced by cameras. However, the development of such a wireless solution using mmWave MIMO radar is nontrivial. Developing feasible radar signal processing procedure for synchronous monitoring of multi-point vital signs and building a hardware system to validate the validity and effectiveness of the radar-based solution are keys to the successful implementation of this radar-based wireless solution. In this study, we introduce MultiVital, a wireless solution for multi-point vital sign monitoring using mmWave MIMO radar. This comprehensive solution encompasses both a hardware system and algorithm development. The contributions of our radar-based solution for multi-point vital sign monitoring include • Building a new hardware system, namely MultiVital, which integrates a mmWave MIMO radar system, five-channel SCG sensors, and one-channel ECG electrodes. This hardware system facilitates synchronous monitoring of multi-point vital signs and provides two reference modalities for evaluating the performance of radar system. • Designing the overall signal processing framework to physically differentiate and accurately monitor five-point vital signs. This framework consists of a radar signal processing module, an SCG calibraion module, and a scheme for their spatial alignment. • Conducting theoretical derivation, simulation and experiments to validate the feasibility and effectiveness of signal processing module to extract multi-point vital signs. The remaining sections of this paper are organized as follows. In Section II, the radar signal processing steps are presented. In Section III, a detailed description of the MultiVital hardware system and the overall signal processing framework of the MultiVital system are presented. In Section IV and Section V, the simulation and experimental results are demonstrated and discussed, respectively. In Section VI, the conclusion is drawn."
https://arxiv.org/html/2411.09159v1,PIMCOMP: An End-to-End DNN Compiler for Processing-In-Memory Accelerators,"In the past decade, various processing-in-memory (PIM) accelerators based on various devices, micro-architectures, and interfaces have been proposed to accelerate deep neural networks (DNNs). How to deploy DNNs onto PIM-based accelerators is the key to explore PIM’s high performance and energy efficiency. The scale of DNN models, the diversity of PIM accelerators, and the complexity of deployment are far beyond the human deployment capability. Hence, an automatic deployment methodology is indispensable. In this work, we propose PIMCOMP, an end-to-end DNN compiler tailored for PIM accelerators, achieving efficient deployment of DNN models on PIM hardware. PIMCOMP can adapt to various PIM architectures by using an abstract configurable PIM accelerator template with a set of pseudo-instructions, which is a high-level abstraction of the hardware’s fundamental functionalities. Through a generic multi-level optimization framework, PIMCOMP realizes an end-to-end conversion from a high-level DNN description to pseudo-instructions, which can be further converted to specific hardware intrinsics/primitives. The compilation addresses two critical issues in PIM-accelerated inference from a system perspective: resource utilization and dataflow scheduling. PIMCOMP adopts a flexible unfolding format to reshape and partition convolutional layers, adopts a weight-layout guided computation-storage-mapping approach to enhance resource utilization, and balances the system’s computation, memory access, and communication characteristics. For dataflow scheduling, we design two scheduling algorithms with different inter-layer pipeline granularities to support varying application scenarios while ensuring high computational parallelism. Experiments demonstrate that PIMCOMP improves throughput, latency, and energy efficiency across various architectures. PIMCOMP is open-sourced at https://github.com/sunxt99/PIMCOMP-NN.","DEEP neural networks (DNNs), with their powerful feature extraction and classification abilities, can excellently perform various intelligent tasks. According to neural scaling laws, the size and data of neural network models are continuously increasing to achieve better performance. Researchers have proposed a series of special-purpose accelerators (e.g., [1, 2]) to accommodate the models and speed up the inference process to cope with the ever-expanding neural networks. However, these accelerators are facing the memory wall challenge [3] as they suffer from high-cost data movement between memory and processing elements and encounter obstacles in enhancing energy efficiency. Processing-in-memory (PIM) is expected to overcome the memory wall challenge as it binds data and computation, thereby circumventing the need for data movement. Among various PIM implementations, the resistive random-access memory (RRAM) has high density, low latency, and is easy to integrate with the CMOS technology [4], offering a broad application prospect. RRAM cells are usually integrated into a crossbar array, which can perform a matrix-vector multiplication (MVM) in O(1) time. The weights are programmed to be the conductances of the cells. The input activations are converted to voltages by digital-to-analog converters (DACs). According to the Kirchhoff’s law, the output currents reflect the product of the weights (a matrix) and the inputs (a vector). Since the crossbar array operates in the analog domain, peripheral circuits such as DACs and analog-to-digital converters (ADCs) are needed to convert the signals. Due to the high memory density, parallel in-situ computing properties, elimination of weight movement, and the crossbar array formed by PIM devices can potentially meet the storage and computation requirements of DNNs. Crossbar arrays can also be constructed with other devices, such as ferroelectric field-effect transistors, magnetic random-access memories, phase-change memories, and even volatile static random-access memories. Based on this principle, previous works (e.g., [5, 6, 7, 8, 9, 10]) have designed a series of crossbar array-based DNN accelerators. These accelerators contain thousands of or more crossbar arrays, which challenge the deployment of DNN models due to the vast hardware scale. In addition, DNN models with different sizes and topologies demand meticulous attention for resource utilization, data scheduling, and memory optimization during deployment. Due to the different micro-architectures of various accelerators, it is uneconomical and unrealistic to manually design the deployment schemes for each DNN model on each accelerator as in previous works [5, 6, 7, 8, 9, 10]. Therefore, a compiler that can adapt to various PIM architectures and automatically complete DNN model deployment is indispensable to improve the usability of PIM accelerators, which also helps build a PIM ecosystem [11]. To bridge DNN models and PIM accelerators, the compiler needs to be designed by considering the following aspects to make it universal, flexible, and efficient. • For hardware: various PIM-based DNN accelerators have emerged, and it is cumbersome to design a specific compiler for each accelerator. Therefore, the compiler needs to be built on a high-level hardware abstraction, which can be lowered to specific PIM architectures. In addition, the compiler needs to be flexible and configurable to facilitate users quickly deploying DNNs on PIM accelerators with different scales of hardware resources. • For software: the compiler should support a variety of DNN workloads. Besides, it should apply to different application scenarios, such as meeting low-latency or high-throughput requirements. Above all, the compiler should automatically complete the deployment, that is, transparently perform model reading, weight mapping, and output collecting without user intervention. • System-level optimization: the compiler needs to handle resource allocation and dataflow scheduling effectively to unleash the hardware potential. For resource allocation, the compiler should make full use of the PIM resources to boost performance while balancing computing, memory access, and communication. For dataflow scheduling, the compiler should rapidly generate instruction streams for different scenarios and optimize the system performance bottlenecks. In addition, the compiler ought to have a profiler that provides an effective evaluation to steer the iterative performance optimization. To address the challenges faced by PIM accelerators during DNN deployment, this paper proposes PIMCOMP, an end-to-end DNN compiler for PIM accelerators. A previous version of this work was published in [12], which provides an optimization scheme for resource allocation, task mapping, and pipeline dataflow through four stages: layer partitioning, weight replication, core mapping, and dataflow scheduling. The previous work offers a methodology that focuses on some deployment-specific issues, but falls short of meeting the compiler requirements stated above, as it lacks support for end-to-end compilation, integration of different compilation stages, and system-level optimizations. Nevertheless, the previous work lays out a design blueprint and algorithmic guidance for this work. In this paper, we first review and contrast the existing PIM compilers. Then, we present the overall architecture and implementation details of PIMCOMP. PIMCOMP is a practical compiler that builds on Ref. [12] and enhances hardware compatibility, software support, and holistic performance. The new contributions of this paper are summarized as follows. 1. We propose an end-to-end DNN compiler tailored for PIM accelerators, encompassing the frontend, optimizer, and backend, capable of inferring an entire DNN model. We design two pipelines to accommodate diverse application scenarios. We build a profiler to accomplish a comprehensive performance assessment for iterative compilation space optimizations. 2. We present a PIM accelerator abstraction adaptable to various academic accelerator designs. The accelerator abstraction comprises a hardware template with rich configurability, a set of pseudo-instructions fully abstracting the hardware’s fundamental functionalities, and user-specified hardware execution patterns. 3. We introduce three-stage optimizations to complete the deployment of DNNs on PIM accelerators. In the layer partitioning stage, we propose array groups as the basic programming unit and utilize a flexible unfolding format to meet various resource demands. In the layout-computation mapping stage, we propose a weight-layout guided computation-storage-mapping method, utilizing genetic algorithms to optimize weight replication and layout and adaptively allocating computational tasks. In the dataflow scheduling stage, we propose scheduling algorithms maintaining high parallelism tailored to two pipelines. 4. We evaluate PIMCOMP’s end-to-end deployment capability on three different architectures. The experimental results show that PIMCOMP achieves promising improvements in throughput, latency, and energy consumption compared with previous works."
https://arxiv.org/html/2411.08674v2,Reducing ADC Front-end Costs During Training of On-sensor Printed Multilayer Perceptrons,"Printed electronics technology offers a cost-effective and fully-customizable solution to computational needs beyond the capabilities of traditional silicon technologies, offering advantages such as on-demand manufacturing and conformal, low-cost hardware. However, the low-resolution fabrication of printed electronics, which results in large feature sizes, poses a challenge for integrating complex designs like those of machine learning (ML) classification systems. Current literature optimizes only the Multilayer Perceptron (MLP) circuit within the classification system, while the cost of analog-to-digital converters (ADCs) is overlooked. Printed applications frequently require on-sensor processing, yet while the digital classifier has been extensively optimized, the analog-to-digital interfacing, specifically the ADCs, dominates the total area and energy consumption. In this work, we target digital printed MLP classifiers and we propose the design of customized ADCs per MLP’s input which involves minimizing the distinct represented numbers for each input, simplifying thus the ADC’s circuitry. Incorporating this ADC optimization in the MLP training, enables eliminating ADC levels and the respective comparators, while still maintaining high classification accuracy. Our approach achieves 11.2x lower ADC area for less than 5% accuracy drop across varying MLPs.","Recently, there has been a growing trend fueled by the fourth industrial revolution and the Internet of Things to integrate intelligence into everyday items. Applications like wearables, fast-moving consumer goods, basic healthcare devices, and disposable sensors for pharmaceuticals have not yet, fully incorporate computing capabilities [1]. These products need computing technology that is ultra-low cost, thin, and conformal. Traditional lithography-based CMOS technologies can’t meet these requirements, limiting computing’s reach [2]. Printed electronics (PE) offer a promising solution with on-demand, ultra-low cost fabrication, ideal for short-lifetime, disposable products. PE uses various printing methods like jet, screen, or gravure printing [2]. These techniques are mask-less, portable, and additive, reducing manufacturing costs and production times [2]. The simplicity of additive manufacturing allow ultra low-cost, at sub-cent levels, electronic circuits. However, this low precision fabrication, results in higher device latency and lower integration density compared to silicon VLSI systems [2], making the design of more complex circuits a challenge in PE. Nevertheless, the target applications are viable in printed electronics due to their relaxed frequency and precision requirements [2]. We focus on Electrolyte-Gated FET (EGFET) technology, which has low supply voltage (\leq 1$\mathrm{V}$), making it suitable for battery-powered applications [1]. Figure 1: Area and Power Evaluation of the printed classification system in [3]. PE’s target applications require smart sensor processing, which starts with analog frontend to digitize analog sensor data using ADCs, followed by machine learning classifiers, such as Multilayer Perceptrons (MLPs) [2]. However the complexity of these classification system oppose a challenge to their realization in PE due to their large gate count. To mitigate this limitation, exploiting the high customization capabilities of the PE technology by bespoke implementations in which the hardware is tailored to a specific dataset and model were proposed [4]. In [3, 5, 6, 7], the authors combined the bespoke architecture alongside the well established Approximation Computing paradigm where a small, accuracy loss resulted in significant area and power gains of the MLP classifier. Nevertheless, all the previous works focused only on the reduction of the MLP classifier inside the overall classification system, neglecting the area and power consumption of the Analog-to-Digital Converts (ADCs). In Figure 1, an area and power analysis within the classification system is presented, by using the MLP in [3]. As shown in Figure1, since the MLP classifier is optimized using approximate bespoke mapping, the ADC ratio in the classification system consuming on average 58% area and 74% power of the entire classification system, making the ADCs the dominant source of area and power overhead in the classification system. Inspired by the fact that different sensor data have different distributions in a given range (e.g., 4bits), not all the representations are required and thus high accuracy can be achieved albeit discard them. Leveraging the above, in this work, we propose the design of bespoke pruned ADCs for each input with the minimum possible representations required saving thus hardware by removing the circuitry of the unused input representation. In more details, we use a Genetic Algorithm (GA) to explore which representations of the ADC can be pruned alongside a quantization-aware training (QAT) of the MLP. Further, our ADC-optimization is orthogonal to any other training approach. In the literature some ADC optimization have been proposed in other technologies rather than in printed electronics. In [8], a spike-based scheme avoids ADCs by a comparator-register architecture but both of these components are hardware expensive in printed electronics[4]. In [9], prune crossbars to eliminate ADCs in ReRAM architectures, which in our architectures is equivalent to feature reduction. Our approach goes further by optimizing the remaining ADCs. To the best of our knowledge, this is the first time that such a framework111https://github.com/floAfentaki/Approximation-Techniques-Targeting-Printed-MLPs is proposed for ADC-efficient printed MLP-based classification systems. Our experiments across various datasets showcase that our framework reduces both area and power of the required ADCs on average 11.2\times and 13.2\times respectively."
https://arxiv.org/html/2411.08353v1,Everything You Wanted to Know About Consumer Light Management in Smart Energy,"Consumer lighting plays a significant role in the development of smart cities and smart villages. With the advancement of (IoT) technology, smart lighting solutions have become more prevalent in residential areas as well. These solutions provide consumers with increased energy efficiency, added convenience, and improved security. On the other hand, the growing number of IoT devices has become a global concern due to the carbon footprint and carbon emissions associated with these devices. The overuse of batteries increases maintenance and cost to IoT devices and simultaneously possesses adverse environmental effects, ultimately exacerbating the pace of climate change. Therefore, in tandom with the principles of Industry 4.0, it has become crucial for manufacturing and research industries to prioritize sustainable measures adhering to smart energy as a prevention to the negative impacts. Consequently, it has undoubtedly garnered global interest from scientists, researchers, and industrialists to integrate state-of-the-art technologies in order to solve the current issues in consumer light management systems making it a complete sustainable, and smart solution for consumer lighting application. This manuscript provides a thorough investigation of various methods as well as techniques to design a state-of-the-art IoT-enabled consumer light management system. It critically reviews the existing works done in consumer light management systems, emphasizing the significant limitations and the need for sustainability. The top-down approach of developing sustainable computing frameworks for IoT-enabled consumer light management has been reviewed based on the multidisciplinary technologies involved and state-of-the-art works in the respective domains. Lastly, this article concludes by highlighting possible avenues for future research.","Smart city and Smart energy are interrelated concepts designed to enhance the efficiency and sustainability of urban areas. The idea of smart city represents the extensive integration of \acICT in the daily lives of human beings in urban areas [1]. Implementing these technologies aims to elevate the living standards of residents by optimizing the effectiveness of services and infrastructure associated with cities. Smart city also refers to a city that integrates the physical infrastructure, information technology infrastructure, social infrastructure, and business infrastructure through \acIoT to harness the collective intelligence of its community [2]. On the other hand, smart energy encompasses a much broader scope than conventional energy. It can be perceived as a model akin to the “Internet of Energy” that deals with smart power generation, smart energy management, smart energy storage, and smart energy consumption. Energy refers to the characteristics of an object or system that determine its capacity to perform work. It exists in multiple forms, including potential energy, kinetic energy, chemical energy, and thermal energy. It is worth highlighting that the wide range of energy sources includes solar, fossil fuels, electricity, vibration, biomass etc. The significance of smart energy in smart cities is fundamentally rooted in the rapid growth of smart cities and the subsequent exponential increase in energy supply demand. Smart cities are reported to improve energy efficiency, minimize electronic waste, and decrease carbon emissions through the use of smart energy. Moreover, all forms of traditional energy, clean energy, green energy, sustainable energy, and renewable energy, integrated with \acICT technology, constitute smart energy. Consequently, the integration of smart energy concept in various physical processess forms \acE-CPS in smart cities. \acE-CPS employs advanced sensors, communication networks, and control systems to enhance energy efficiency and minimize environmental effects. These systems can adeptly manage energy resources, forecast demand trends, and modify energy output appropriately by integrating real-time data and \acAI algorithms. Consumer lighting constitutes a crucial application in smart city and smart home frameworks. On the other hand, it is considered as one of the most energy-consuming applications, which accounts for 30% to 40% of the total energy demand of smart cities [1]. The global smart consumer lighting market is valued at roughly 16.25 billion USD, and it is anticipated to expand at a \acCAGR of 20%, potentially reaching over USD 83.81 billion by 2032 [3]. Undoubtedly, this rapid expansion is primarily driven by the rising need for energy efficient lighting solutions and the escalating trend towards smart homes and smart cities. Smart consumer light management systems serve an instrumental role in converting traditional consumer lighting systems to smart lighting systems. The integration of \acIoT, smart sensors, and robust control strategies for energy saving makes it a widely adopted solution. The number of features in smart consumer lighting continues to grow with the advancement in \acICT technology. Subsequently, energy consumption also increases with the number of features, leading to most state-of-the-art smart consumer light management systems being energy hungry, demanding average power in terms of hundreds of milli-watts [4]. Conversely, sustainability is an essential aspect that must be integrated on a larger scale to comply with Industry 5.0 [5]. Thus, it demands a focus on optimizing power consumption in order to feature smart consumer light management systems with energy autonomy using harvesters with small form factor. Consumer light management system helps in reducing energy consumption and increases total lighting efficiency via smart sensors and advanced data analytics techniques. It can automatically adjust lighting configurations according to various factors such as occupancy, ambient light, and weather conditions, aiding cities in minimizing their carbon emission and reducing energy expenses. It augments the safety and security of residents by ensuring adequately illuminated streets, public areas, and indoor environments. Moreover, the application has extensively improved both energy efficiency and quality of life in smart cities. The evolution of consumer lighting application over the past decades has been presented in Fig. 1. Figure 1: Evolution of Consumer Lighting Technology The remainder of this survey is structured into nine sections. Section 2 discusses the related prior work and contributions made in this manuscript. Section 3 elucidates smart consumer light management, its significance, and its attributes. Section 4 presents the need for sustainability and various elements of smart consumer light. Section 5 illustrates power management strategies in consumer lighting management systems. Section 6 delineates energy harvesting, power conditioning, and storage techniques. Section 7 presents the sustainable computing architectures for smart consumer light management systems. Task scheduling and energy optimization techniques of these systems have been detailed in Section 8. The open research directions have been highlighted in Section 9. The manuscript has been concluded in Section 10. An appendix containing a list of acronyms used is provided at the conclusion of the manuscript."
https://arxiv.org/html/2411.08312v1,A Novel Extensible Simulation Framework for CXL-Enabled Systems,"Compute Express Link (CXL) serves as a rising industry standard, delivering high-speed cache-coherent links to a variety of devices, including host CPUs, computational accelerators, and memory devices. It is designed to promote system scalability, enable peer-to-peer exchanges, and accelerate data transmissions. To achieve these objectives, the most recent CXL protocol has brought forth several innovative features, such as port-focused routing, device-handled coherence, and PCIe 6.0 compatibility. However, due to the limited availability of hardware prototypes and simulators compatible with CXL, earlier CXL research has largely depended on emulating CXL devices using remote NUMA nodes. Unfortunately, these NUMA-based emulators have difficulties in accurately representing the new features due to fundamental differences in hardware and protocols. Moreover, the absence of support for non-tree topology and PCIe links makes it complex to merely adapt existing simulators for CXL simulation. To overcome these problems, we introduce ESF, a simulation framework specifically designed for CXL systems. ESF has been developed to accurately reflect the unique features of the latest CXL protocol from the ground up. It uses a specialized interconnect layer to facilitate connections within a wide range of system topologies and also includes key components to carry out specific functions required by these features. By utilizing ESF, we thoroughly investigate various aspects of CXL systems, including system topology, device-handled coherence, and the effects of PCIe characteristics, leading to important findings that can guide the creation of high-performance CXL systems. The ESF source codes are fully open-source and can be accessed at https://anonymous.4open.science/r/ESF-1CE3.","With the prevalence of large-scale data-intensive applications such as artificial intelligence, life science, and climate modelling [41, 22, 46, 38, 24, 17, 30, 63, 45], there are increasing demands to aggregate tons of computation and memory resources into a uniform system. Peripheral component interconnect express (PCIe) [4, 8], as one of the most popular interconnect standards, has been widely adopted in the computing system to connect between the host CPU and diverse peripheral devices including graph processing units (GPUs) and solid-state drives (SSDs) [65, 34, 56, 36]. Compared to other types of interconnects (e.g., Ethernet [1], SATA [3], and DDR [13]), PCIe can deliver much higher aggregated throughput (e.g., 256 GB/s in 16 PCIe 6.0 lanes [8]). In addition, PCIe supports various communication protocols (e.g., NVMe [2]), exhibiting high compatibility. However, PCIe fails to extend the host local memory with external PCIe memory devices due to the lack of coherence mechanisms [36]. Specifically, the memory accesses that target PCIe device memory address space are required to be non-cachable. CPU cores must directly access the PCIe device memory and are not allowed to store copies of data from the device memory within their internal caches. Software involvement is necessary to maintain data coherence. This limitation significantly worsens the memory access performance. Thus, building computation and memory pools atop PCIe cannot satisfy the demands of large-scale data-intensive applications. Compute Express Link (CXL) is an emerging industry standard that offers high-performance cache-coherent interconnect capability to heterogeneous devices, including host CPUs, computational accelerators, and memory devices [9, 50]. CXL is designed to operate over the existing PCIe infrastructure, which utilizes the same physical and electrical interfaces. This design philosophy aids CXL with the high performance and backward compatibility of PCIe technology. CXL also provides the features of cache coherency and memory semantic support, which can seamlessly extend the host-side processor and memory with the external CXL accelerators and memory devices. Thus, CXL enables efficient data sharing and communication within computation and memory pools. While CXL has great potential to change the existing computer architecture, most of the prior studies on CXL [40, 44, 18] leverage remote NUMA nodes to emulate CXL devices due to the lack of hardware prototypes. As high-version CXL is still at the proof of concept (PoC) stage, we believe constructing a CXL simulator would be the wheel to drive the high-performance interconnect research forward. Nevertheless, it is challenging to simply extend the existing simulators and emulators to support the CXL simulation. Specifically, the CXL standard aims to achieve ultra-high scalability by providing complicated non-tree system topology and coherent peer-to-peer communication. However, NUMA-based emulators, which have been adopted in previous work, face strict physical constraints (e.g., socket number) and fail to extend system scalability. Prior computation-centric simulators, such as gem5 [21, 33] and GPGPUsim [19], focus on accurate processor unit modeling, however, support only legacy interconnects (e.g., gem5 only supports legacy PCI links), which are unable to operate in non-tree topologies. On the other hand, network-centric simulators, such as BookSim [35] and Garnet [16], pay attention to diverse network topologies and flow control mechanisms. These simulators lack the support of coherency management, which is considered a key promise of the CXL standard. In summary, existing tools struggle to reflect critical features of the CXL standard. Tackling the aforementioned challenges, we propose our novel extensible simulation framework, ESF, that is built atop the CXL backbone. This framework introduces two function layers for an accurate simulation of highly scalable CXL systems, namely interconnect layer and device layer. The interconnect layer is dedicated to supporting complicated system topologies. Upon system initialization, this layer constructs a topology graph of the system and provides detailed routing information to the devices for intercommunication uses. On the other hand, the device layer models several types of fundamental CXL devices, including CXL accelerators, memory devices, and CXL switches. During simulation, these devices conduct CXL protocol functions and communicate with each other by leveraging the communication function of the interconnect layer. For example, CXL switches build internal routing tables based on the topology information provided by the interconnect layer and route different requests to the correct destinations. The tight collaboration of these two layers ensures ESF to accurately simulate a highly scalable system defined by the CXL standard. The validation experiment proves the accuracy of ESF with errors ranging from 0.1% to 10%. With accurate simulation, ESF can uncover several issues that the existing simulators are unable to figure out, including the performance impacts of diverse system topologies and the design choices for device-managed coherence. Our contributions are summarized as follows: \bullet CXL simulation challenge analysis of existing research tools: The CXL standard is aimed at supporting rack-level systems with scalable performance, which requires complicated non-tree system topology and coherent peer-to-peer communication. To meet these requirements, the CXL protocol introduces several novel features, including port-based routing, device-managed coherence and the adoption of high-version PCIe physical links. Unfortunately, existing simulation and emulation tools face challenges in accurately reflecting these critical features. Most of the prior works adopt remote NUMA nodes as CXL hardware emulators. However, the physical limitation of NUMA platforms prevents them from emulating port-based routing. Meanwhile, existing computation-centric simulators lack the support of PCIe simulation, while network-centric simulators fail to provide coherence management functionality. \bullet Novel simulation framework customized for CXL systems: To address the challenges in existing tools comprehensively, we propose a customized simulation framework, ESF, which consists of two fundamental layers, namely the interconnect layer and the device layer. While the interconnect layer is dedicated to providing interconnection and scalability of the simulated system, the device layer performs device-specific functions, such as coherence management. The novel framework carefully implements a set of components to model the essential features of CXL. Firstly, it provides a switch component that supports PBR. Secondly, it implements a device-side inclusive snoop filter as an example of device coherency agent (DCOH). Lastly, it implements the bus components while considering unique characteristics of PCIe buses to accurately reflect the behaviors of real CXL platforms. \bullet Exploration on the performance impacts of multiple new CXL features: We perform a set of experiments to explore the performance impacts of emerging CXL features in multiple representative systems implemented with our novel simulation framework. Our investigation focuses on three main aspects: (1) the impacts of different system topologies, (2) the impacts of device-managed coherence, and (3) the unique full-duplex feature of PCIe transmission. From the experimental results, we derive three key observations. First, the traditional tree-like system topology experiences severe bandwidth and latency bottlenecks at the root, leading to potential performance degradation similar to systems with a chain-like topology. Second, the device-side inclusive snoop filter receives unique request patterns because most of the requests that reach the snoop filter are cache misses. Therefore, a customized structure is essential for the snoop filter to achieve optimal performance. Third, we observe from read-write mixed workloads that full-duplex transmission of PCIe buses results in a bandwidth improvement compared to those with a single type of access pattern. These observations pave the road to future CXL system designs."
https://arxiv.org/html/2411.07946v1,MANTIS: A ixed-Signl ear-Sensor Convoluional mager oC Using Charge-Domain 4b-Weighted 5-to-84-TOPS/W MAC Operations for Feature Extraction and Region-of-Interest Detection,"Recent advances in artificial intelligence have prompted the search for enhanced algorithms and hardware to support the deployment of machine learning at the edge. More specifically, in the context of the Internet of Things (IoT), vision chips must be able to fulfill tasks of low to medium complexity, such as feature extraction or region-of-interest (RoI) detection, with a sub-mW power budget imposed by the use of small batteries or energy harvesting. Mixed-signal vision chips relying on in- or near-sensor processing have emerged as an interesting candidate, thanks to their favorable tradeoff between energy efficiency (EE) and computational accuracy compared to digital systems for these specific tasks. In this paper, we introduce a mixed-signal convolutional imager system-on-chip (SoC) codenamed MANTIS, featuring a unique combination of large 16\times16 4b-weighted filters, operation at multiple scales, and double sampling, well suited to the requirements of medium-complexity tasks. The main contributions are (i) circuits called DS3 units combining delta-reset sampling, image downsampling, and voltage downshifting, and (ii) charge-domain multiply-and-accumulate (MAC) operations based on switched-capacitor amplifiers and charge sharing in the capacitive DAC of the successive-approximation ADCs. MANTIS achieves peak EEs normalized to 1b operations of 4.6 and 84.1 TOPS/W at the accelerator and SoC levels, while computing feature maps with a root mean square error ranging from 3 to 11.3\%. It also demonstrates a face RoI detection with a false negative rate of 11.5\%, while discarding 81.3\% of image patches and reducing the data transmitted off chip by 13\times compared to the raw image.","Figure 1: (a) Vision chip architectures ranging from mixed-signal processing in or near the pixel array to conventional digital processing outside of it, and (b) strengths and limitations of these architectures. (c) Envisioned system based on a cascaded processing scheme similar to [4], in which only relevant image patches are transmitted from the image sensor to the digital processor. Recent years have seen artificial intelligence (AI) rise as a key component of numerous engineered systems, reaching an unprecedented level of pervasiveness at the applications level. Among them, the Internet of Things (IoT) has elicited a particular interest as the large amount of data generated by sensor nodes calls for the development of specialized machine learning (ML) algorithms and hardware to efficiently process data at the edge, a concept coined as edge AI or tiny ML. More specifically, in the context of vision sensors, edge devices must be able to solve vision tasks of low to medium complexity, e.g., feature extraction (FE) and region-of-interest (RoI) detection, within a sub-mW power budget, as IoT nodes are often supplied by limited-capacity batteries. Mixed-signal vision chips have thus emerged as a suitable candidate, since they outperform digital chips in terms of energy efficiency (EE) while maintaining a sufficient computational accuracy. This improved EE also stems from a reduced number of ADC conversions compared to digital implementations, leading to significant energy and area savings. Mixed-signal vision chip architectures can be divided into two main categories, namely in- [1, 2, 3] and near-sensor [4, 5, 6, 7, 8] vision chips, respectively implemented with analog processing elements (PEs) inside or in the periphery of the pixel array. A third category, referred to as hybrid vision chips [9, 10], is not represented in Fig. 1(a) but simply combines elements from both categories. On the one hand, in-sensor vision chips are massively parallel and do not require any memory, be it analog or digital. However, connections between pixel-level PEs are usually local and limited to neighboring pixels, hampering the calculation of image-level features and thereby, the operation at multiple spatial scales. In addition, pixel-level PEs also lead to a relatively large pixel pitch above 10 \mum. At last, in-sensor processing is often limited to low-complexity tasks as it relies on binary (1b) or ternary (1.5b) weights, and on raw [1, 2, 3] or amplified [9] photocurrents subject to significant fixed-pattern noise (FPN), i.e., local mismatch between pixel responses. On the other hand, near-sensor and hybrid vision chips usually present a decreased throughput compared to in-sensor ones, but are better suited to the execution of medium-complexity tasks thanks to the use of large-size 1.5b Haar-like filters [4, 5, 9, 10] or to an increased 4b filter weight resolution [7, 8]. They make use of conventional pixel structures such a three- or four-transistor (3T or 4T) active pixel sensor (APS) or pulse-width-modulated (PWM) digital pixel sensor, which are compatible with double sampling techniques to compensate FPN. 3T/4T pixels respectively use rolling and global shutters, and can either rely on a voltage-based readout with a source follower (SF), or on a time-based one with a PWM structure, allowing to reduce the supply voltage without degrading the output dynamic range. Besides, near-sensor and hybrid vision chips can operate at multiple spatial scales thanks to image downsampling (DS) [4, 5] or filter dilation [9]. Finally, an analog memory generally based on capacitors (caps) is required to store a few rows of the image, ultimately leading to power and/or area overheads. Nevertheless, existing works fall short of preserving EE while simultaneously supporting medium-complexity tasks, which require sufficient computational accuracy brought by FPN-compensated inputs and increased weight resolution, as well as multiscale operation and large filters for tasks as RoI detection. In this work, we present a mixed-signal near-sensor convolutional imager system-on-chip (SoC) codenamed MANTIS, fabricated in United Microelectronics Corporation (UMC) 0.11-\mum CMOS technology and supporting both FE and RoI detection. It includes two main contributions providing an effective answer to the aforementioned limitations of existing vision chips. First, circuits called DS3 units combining three operations which can be abbreviated as DS, namely double sampling, to mitigate the impact of FPN, voltage downshifting, to reduce the voltage level from the pixel array to the convolution processor, and image downsampling, to allow for multiscale operation. Second, a mixed-signal convolution processor implementing 4b-weighted multiply-and-accumulate (MAC) operations in the charge domain, based on a modified switched-capacitor (SC) amplifier structure to compute the partial sum (psum) of a row of an image patch, and on a charge sharing operation in the capacitive digital-to-analog converter (DAC) of the following successive-approximation (SAR) analog-to-digital converter (ADC) to aggregate psums of different rows. In our vision, MANTIS would be used as the first stage of a cascaded processing system [Fig. 1(c)] supporting low- to medium-complexity processing tasks, while a high-complexity processing based on convolutional or deep neural networks (CNNs or DNNs) would be executed by a digital processor. The major benefits of such a system are to limit the amount of I/O data transfers from the image sensor to the digital processor, and to only dedicate energy to the processing of relevant data. This paper extends our conference paper [11] by providing a more in-depth description of the circuits constituting the convolution pipeline, highlighted in Fig. 2, as well as additional experimental results. The remainder of this paper is organized as follows. First, Section II describes the architecture of the SoC. Then, Section III discusses the design and implementation of the proposed mixed-signal convolution pipeline, while Section IV presents measurement results of the SoC. Finally, Section V compares this work to the state of the art, and Section VI offers some concluding remarks. Figure 2: MANTIS CMOS imager SoC (a) modes of operation and (b) architecture, detailing the different blocks in the digital core and image sensor analog core with their respective power domains. Figure 3: Block diagram of (a) the convolution and (b) the imaging pipelines."
https://arxiv.org/html/2411.07721v1,Web-Based Simulator of Superscalar RISC-V Processors,"Mastering computational architectures is essential for developing fast and power-efficient programs. Our advanced simulator empowers both IT students and professionals to grasp the fundamentals of superscalar RISC-V processors, HW/SW co-design and HPC optimization techniques. With customizable processor and memory architecture, full C compiler support, and detailed runtime statistics, this tool offers a comprehensive learning experience. Enjoy the convenience of a modern, web-based GUI to enhance your understanding and skills.","In the rapidly evolving field of computer architecture, a deep understanding of superscalar processors is crucial for both IT students and professionals, particularly those focusing on writing high-performance and power-efficient code. However, mastering the intricacies of these architectures is challenging, especially when existing educational tools fall short. Current processor simulators are often either too complex and low level aiming at cycle accurate simulation of complex codes of yet non-existent processors, such as Intel Simcs Simulator [1], or lacking intuitive graphical interface, features such as supercalar out-of-order execution, processor customization, memory and cache hierarchy, or detailed runtime statistic. I-A State of the Art A comprehensive list of RISC-V simulators can be found on the RISC FIVE website [2]. The Creator RISC-V RV32IMFD Online Assembly Simulator [3] is a powerful web-based tool that allows users to write, compile, and step through RISC-V RV32IMFD assembly code to observe program behavior. Its key features include processor and memory layout customization, runtime statistics collection, and online debugging. However, it only supports scalar processors and lacks a command-line interface (CLI) for benchmarking large program segments. The Venus RISC-V Simulator [4] is a RISC-V instruction set simulator designed for educational purposes. It allows the simulation of more complex codes, but only on a scalar RISC-V processor, without the capability to inspect pipeline stages, hazards, or other detailed processor behaviors. The Vulcan RISC-V Simulator for Education [5] offers several RISC-V instruction set extensions, along with side-by-side visualization of the program counter (PC), machine code, and original instructions, as well as register and memory visualization. However, it only supports a scalar core, and the web interface is still in the alpha stage. Other notable simulators for RISC-V processors include Ripes [6] and Jupiter [7]. However, neither supports a superscalar pipeline or a web-based interface. In the search for inspiration in superscalar processor simulators, we must mention the excellent VSIM simulator [8], which our group has used for years in the Computer Architecture course. Developed in 2001, VSIM offers five architectures of superscalar processors from that era: Compaq Alpha 21264, Hewlett-Packard PA-8500, IBM Power3, Intel Pentium Pro/II/III, and MIPS R10000. VSIM allows partial customization of the processor architecture, the ability to load user-defined or random programs, and step-by-step simulation of program execution, including visualization of instruction and data flows between processor components. Unfortunately, this simulator is quite outdated and only runs on 32-bit Windows. I-B Objectives The primary objective of the proposed web-based simulator is to bridge this educational gap by providing HPC developers with an accessible and illustrative tool to explore and understand the architecture of superscalar RISC-V processors. The simulator is designed to visually demonstrate each phase an instruction undergoes within the processor pipeline, allowing developers to identify potential bottlenecks and understand how different implementations of the same algorithm can impact runtime metrics such as execution time, cost or power consumption. By interacting with the simulator, developers can experiment with different processor configurations and observe their impact on runtime metrics. Since the primary purpose of the simulator is educational, the initial version currently supports only the RV32IMFD instruction set. Future versions will add support for the 64-bit instruction set as well as vector extensions. This hands-on approach aims to equip developers with the knowledge and skills needed to answer critical questions: Given an algorithm, how should one design a processor and optimize the code for the best performance, reasonable manufacturing cost and power consumption? By offering a user-friendly interface and comprehensive support for customization and performance analysis, our simulator seeks to enhance the learning experience and prepare developers for the challenges of modern computing."
https://arxiv.org/html/2411.07632v1,RPCAcc: A High-Performance and Reconfigurable PCIe-attached RPC Accelerator,"The emerging microservice/serverless-based cloud programming paradigm and the rising networking speeds leave the RPC stack as the predominant data center tax. Domain-specific hardware acceleration holds the potential to disentangle the overhead and save host CPU cycles. However, state-of-the-art RPC accelerators integrate RPC logic into the CPU or use specialized low-latency interconnects, hardly adopted in commodity servers.To this end, we design and implement RPCAcc, a software-hardware co-designed RPC on-NIC accelerator that enables reconfigurable RPC kernel offloading. RPCAcc connects to the server through the most widely used PCIe interconnect. To grapple with the ramifications of PCIe-induced challenges, RPCAcc introduces three techniques: (a) a target-aware deserializer that effectively batches cross-PCIe writes on the accelerator’s SRAM using compacted hardware data structures; (b) a memory-affinity CPU-accelerator collaborative serializer, which trades additional host memory copies for slow cross PCIe-transfers; (c) an automatic field update technique that transparently codifies the schema based on dynamic reconfigure RPC kernels to minimize superfluous PCIe traversals. We prototype RPCAcc using the Xilinx U280 FPGA card. On HyperProtoBench, RPCAcc achieves an average of 2.3\times lower RPC layer processing time than a comparable RPC accelerator baseline and demonstrates 2.6\times achievable throughput improvement in the end-to-end cloud workload.","††footnotetext: †Contributes equally Remote Procedure Call (RPC) is a paramount service block of today’s cloud system stacks [33, 42, 65, 1]. It abstracts remote computing resources and provides a simple and familiar programming model. Developers only prescribe type information for each remote procedure, and a compiler generates a stub code linked to an application to pass arguments via message. The RPC model has been widely adopted in many distributed applications, such as cloud storage [17, 79], file systems [82, 39], data analytics [72], consensus protocols [81, 80], and machine learning systems [53, 45]. The RPC stack comprises two key components: (a) RPC protocol handling that parses the RPC headers, identifies the triggered message and the carried payload, and determines the target function; (b) serialization and deserialization, transforming between in-memory data fields and architecture/language-agnostic formats. A recent study from Google Cloud [65] reports that the RPC processing occupies \sim7.1% of CPU cycles across the entire fleet. Thus, it is important to accelerate the RPC execution, reduce this data center tax, and release more CPU cycles for revenue-generated applications. Domain-specific hardware acceleration is a promising solution to build performant computing systems in the post-Moore’s Law era. However, designing an RPC hardware accelerator is very challenging because the RPC stack is tightly coupled with the networking stack and application layer, whose processing should be efficiently streamlined into the data plane. As a result, researchers propose to use specialized on-chip interconnects and closely integrate the RPC acceleration module in the host CPU chips [32, 42, 59, 60, 34]. For example, Cereal [32] introduces a special memory access interface to allow low-latency host memory accesses from the RPC accelerator. Dagger [42] leverages Intel UPI [31] interconnect to facilitate RPC stack processing. Unfortunately, none of these proposals can be easily adopted on commodity servers due to the lack of interconnect support. RPC stack is continuously and rapidly evolving. For example, widely used gRPC [1] has 9 major releases over the last twelve months. As such, integrating RPC logic into the real host CPU lacks enough flexibility. Besides, developing a function- and performance-capable interconnect that can be integrated into a server system takes many years of engineering efforts, such as the ECI bus from the pioneering Enzian platform [10]. The emerging Compute Express Link [11] looks promising, but its physical layer runs atop PCIe, yielding sub-microsecond access latency [68, 44], which cannot satisfy the latency requirement of the above accelerators. This leads to an interesting question: How to accelerate RPC on top of de facto and predominant server interconnect, i.e., PCIe? In this paper, we design and implement RPCAcc, a software-hardware co-designed PCIe-attached accelerator for reconfigurable RPC offloading. RPCAcc colocates with the PCIe-attached NIC. RPCAcc’ hardware part comprises three building blocks: (1) a target-aware deserializer that takes RPC requests, deserializes the messages, and forwards the results to the host or accelerator memory; (2) a memory-affinity serializer, which fetches computed data from both the host and accelerator memory, performs serialization, and fabricates the response; (3) programable computing units, dynamically offloading RPC computing kernels. In sum, RPCAcc is a low-profile immediately deployable PCIe-attached on-NIC accelerator with a software abstraction to load the RPC stack and related computing kernels on demand. Building RPCAcc is non-trivial because of the high cross-PCIe overheads, jeopardizing the interaction performance between the RPC stack and other system layers. First, the RPC deserialization process needs to write deserialized results in a field-by-field scheme, whose throughput is bounded by the number of PCIe transactions. For example, our empirical evaluation using HyperProtoBench [34] shows that this limitation can degrade the attainable deserialization throughput by 2.8\times in geometric mean. RPCAcc proposes a target-aware deserializer that temporarily batches the deserialized fields within one RPC message in the accelerator’s SRAM and performs cross-PCIe writes only when necessary. We realize this by designing two compacted hardware data structures (schema table and temp buffer) and revamping the deserialization process. Second, the RPC serialization process is hindered by the high PCIe latency. A nested RPC message or dereference field (strings/bytes/repeated/sub-messages) would require multiple memory accesses in a pointer-chasing manner since the memory location of the sub-fields can only be known after the parent’s content is fetched. The sub-microsecond latency of PCIe would significantly increase the overall serialization time (\sim4.6\times compared with an on-chip accelerator). RPCAcc designs a memory-affinity CPU-Accelerator collaborative serializer that trades additional host memory copies for slow cross-PCIe transfer. We introduce a lightweight pre-serialization phase to materialize the data layout on the host memory and facilitate the accelerator-side serialization execution. Besides, we leverage the memcpy (memory copy) engines [30, 41] residing in modern CPUs [29] to alleviate host CPU usage for large fields’ copy. Third, computation partition between host and RPC kernels within the RPC handler would cause suboptimal data placement and incur superfluous PCIe traversals. People eagerly co-locate domain-specific logic along with the RPC stack to maximize the hardware specialization benefits [35, 18, 79, 62, 50]. However, unlike on-chip cache-coherent interconnects, dynamic splitting computation logic across the host and accelerator over PCIe is inflexible and cause inferior data placement. Therefore, RPCAcc develops an automatic field update technique that transparently codifies the schema based on host/RPC kernel layout. As such, the accelerator deserializer can place the fields in suitable locations to avoid PCIe traversals. We built RPCAcc over an Xilinx Alevo U280 FPGA and evaluated it in several real-world scenarios. In a cloud image compression application, RPCAcc increases the achievable throughput by 2.6\times and reduces the average (99th percentile) latency by 2.6\times (1.9\times) compared with an RPC accelerator baseline. Using Google’s HyperProtoBench [34], RPCAcc reduces the data serialization time by 4.3\times in geometric mean. RPCAcc achieves similar performance as prior specialized on-chip accelerators from the literature. The source code will be open-sourced."
https://arxiv.org/html/2411.07902v1,Bayes2IMC: In-Memory Computing for Bayesian Binary Neural Networks,"Bayesian Neural Networks (BNNs) provide superior estimates of uncertainty by generating an ensemble of predictive distributions. However, inference via ensembling is resource-intensive, requiring additional entropy sources to generate stochasticity which increases resource consumption. In this work, we introduce Bayes2IMC, an in-memory computing (IMC) architecture designed for binary Bayesian neural networks that leverage nanoscale device stochasticity to generate desired distributions. Our novel approach utilizes Phase-Change Memory (PCM) to harness inherent noise characteristics, enabling the creation of a binary neural network. This design eliminates the necessity for a pre-neuron Analog-to-Digital Converter (ADC), significantly improving power and area efficiency. We also develop a hardware-software co-optimized correction method applied solely on the logits in the final layer to reduce device-induced accuracy variations across deployments on hardware. Additionally, we devise a simple compensation technique that ensures no drop in classification accuracy despite conductance drift of PCM. We validate the effectiveness of our approach on the CIFAR-10 dataset with a VGGBinaryConnect model containing 14 million parameters, achieving accuracy metrics comparable to ideal software implementations as well as results reported in the literature using other technologies. Finally, we present a complete core architecture and compare its projected power, performance, and area efficiency against an equivalent SRAM baseline, showing a 3.8 to 9.6\times improvement in total efficiency (in GOPS/W/mm2) and a 2.2 to 5.6\times improvement in power efficiency (in GOPS/W). In addition, the projected hardware performance of Bayes2IMC surpasses that of most of the BNN architectures based on memristive devices reported in the literature, and achieves up to 20\% higher power efficiency compared to the state-of-the-art.","I-A Context and Motivation The growing demand for edge AI in applications such as medical diagnostics [1], facial identification and surveillance [2] and self-driving cars [3] where reliability and safety are of paramount importance, has heightened the need for systems that provide a measure of uncertainty while operating under significant resource constraints in terms of area and power. Advances in deep neural networks have ensured very high accuracy, but this has come at the cost of overconfidence and poor calibration [4]. More importantly, such networks are incapable of uncertainty quantification, especially out-of-distribution identification measured by epistemic uncertainty.[5]. Bayesian neural networks (BNNs) overcome this problem by allowing the weight parameters to be probability distributions rather than point estimates, thus encoding uncertainty in the parameter distributions [6, 7]. During inference, the network weights are then sampled to create an ensemble, and the outputs are combined in Monte-Carlo (MC) fashion to obtain predictions, confidence and uncertainty (Fig. 1). The creation of ensembles can be done in time, i.e., instantiating each ensemble one after another on the same hardware [8], or in space, i.e., instantiating all the ensembles simultaneously, using multiple copies of the hardware [6]. Both methods are resource-intensive compared to a traditional network whose parameters are point estimates and require only a single instantiation. The ensembling-in-time approach reduces net decision-making throughput while optimizing for power and total area. In contrast, the ensembling-in-space expends more area and power in order to deliver high net throughput. In addition, both these approaches require noise sources to generate random numbers. Thus, the deployment of Bayesian neural networks at the edge is challenging and requires careful hardware-software co-optimization [9, 10]. Figure 1: Top: Illustration of a Bayesian neural network (BNN) where weights take binary values upon sampling. Middle: Bayesian inference is performed by an ensemble of N_{MC} predictions combined through Monte Carlo sampling to obtain predicted class, prediction confidence, and uncertainty. Bottom: Block diagram of the proposed Bayes2IMC core architecture implementing BNN inference. The crossbar array of memristive devices is divided into a weight plane (WP) and a noise plane (NP). The WP stores the parameters z_{w_{ji}} obtained by reparametrizing the probability parameters p_{w_{ji}}, and the noise plane generates the stochasticity required for synaptic sampling. Binary weights w_{ji} are then generated by comparing these variables in hardware. Unlike traditional IMC architectures, the input x_{j}, j^{th} element of input vector \mathbf{x}, is accumulated based on the sign of w_{ji}. In this paper, we design and validate an area and power-efficient ensembling-in-time Bayesian binary neural netwrok in-memory compute (IMC) architecture named Bayes2IMC that leverages the inherent stochasticity of nanoscale memory devices. Fig. 1 provides a high-level description of this core. The nanoscale devices are arranged in a crossbar fashion, which is partitioned into two sections: a weight plane (WP) that stores the distribution parameters, and a noise plane (NP) that provides stochasticity necessary for sampling [6]. We also modify the traditional in-memory computing approach by routing inputs to an accumulator for multiply-and-accumulate (MAC) operations. This combination of routing strategy, row-by-row read flow, and on-the-fly sample generation during sensing enables inference without the need for an analog-to-digital converter, thereby improving area and power efficiency. I-B Background and Related Work In-Memory Computing: The IMC approach has been proposed to address the ‘von Neumann bottleneck’ [11] plaguing traditional computing platforms [12, 13, 14, 15, 16]. In such systems, certain computational operations are performed in memory without data movement, enabling high throughput at low area and power. IMC implementations based on digital Static Random Access Memories (SRAMs) [17, 18, 16, 19, 20] as well as those based on non-volatile memory (NVM) devices such as Phase Change Memory (PCMs), Resistive RAMs (RRAMs), Spin-Transfer Torque RAMs (STT-RAMs), and Spin-Orbit Torque Magnetic RAMs (SOT-MRAMs) have been explored for implementing frequentist deep learning models [21, 14, 22]. Compared to SRAM IMC implementations, NVM devices have following advantages: • Multi-level conductance for multi-bit parameter storage. • Near \mathcal{O}(1) matrix-vector multiplication (MVM) operation when arranged in crossbar fashion with input vector encoded as drive voltage [23]. However, NVM devices also pose several challenges for IMC MVM implementation. The high level of state-dependent programming [24] results in write error, and conductance drift results in instability of programmed weights [24, 10]. This severely impacts the overall classification accuracy. Additional sources of error include MVM errors due to IR drops on source-line due to high current in the all-parallel operation mode [18], and non-linear dependence of device current on applied voltage [25]. Furthermore, Analog-to-Digital converters (ADCs) are necessary to convert the analog output of the MVM operation to multi-bit digital values, and this impacts both the overall power consumption and area efficiency [18, 12]. Related works: We now discuss proposed IMC solutions in the literature that leverage the inherent stochasticity of NVM devices with the goal of implementing ensembling methods. An RRAM array-based Bayesian network with training implemented via Markov Chain Monte Carlo (MCMC) was demonstrated in [26], though the need for training in situ is time and resource intensive [27]. Bernoulli distributed noise of Domain Wall Magnetic Tunnel Junction (DW-MTJ) devices and SOT-MRAM were used in [21] and [9] respectively to generate Gaussian random variables. The latter uses a local reparametrization technique to reduce resource consumption. However, both of these works require the mean and standard deviation parameters to be stored separately and the contributions from multiple devices to be combined to generate a single random variate per the central limit theorem. This lead to expensive hardware requirements. An efficient implementation was presented in [10] that performs device-aware training and combines multiple devices to generate the required stochasticity per weight parameter. The authors of [28] use STT-RAM to implement binary neural networks and perform Bayesian inference using Monte-Carlo Dropout (MCD) methods [29], where neurons are randomly dropped out during inference leveraging a random number generated using STT-RAM’s stochasticity. While MCD-based networks are easy to train and deploy, they are reported to be less expressive, especially for out-of-distribution uncertainty estimations [30, 31]. To mitigate effects of conductance drift, periodic calibrations of each layer of the network was used to obtain optimal affine factors to reduce error in [24]. Alternatively, [10], proposed reprogramming at different time intervals based on the effect of drift on the available domain of programmable conductance values. However, this involves multiple reprogramming steps, which is resource-intensive. Many of these works also employ a digital-to-analog-converter (DAC) to provide analog voltage input vectors. This can result in errors due to I-V non-linearity in some devices, as discussed in [25]. Working around this problem, references [18, 9, 10] propose bit-slicing, wherein the input bits are sent one by one, and the accumulated outputs after analog-to-digital conversion are scaled by their binary positional weights and summed. Pulse width modulation (PWM) can also be used, as suggested in [13]. In addition to extra circuitry overhead – shift-and-add circuits in the former or PWM generator in the latter – all the methods described require an ADC for readout, which consumes up to 87\% power and 60\% of the total area of a core [12, 22]. To reduce the power and area overhead, the ADC use is typically multiplexed amongst several columns, affecting the overall throughput [18]. I-C Main Contributions In this work, we introduce Bayes2IMC, a PCM-based computing architecture designed for BNNs with binary weights. Binary BNNs are less resource-intensive because they need only one parameter to describe the weight distribution. The main contributions of the paper are as follows: • We formulate a principled way to utilize the inherent device stochasticity as a noise source for ensembling. • We devise a hardware-software co-optimized technique that is applied only on the output logits to reduce accuracy variations across multiple network deployments. • We propose a simple global drift compensation mechanism to reverse the effect of state-dependent conductance drift on network performance. This does not require frequent reprogramming or layer-by-layer calibration, only a simple rescaling of the read pulse. Our approach ensures no drop in accuracy, expected calibration error (ECE) [4] and uncertainty performance up to 10^{7} seconds. • By introducing row-by-row read, we avoid high output currents. Additionally, every read operation handles the binarization in tandem without requiring any throughput throttling step. This permits an ADC-less architecture, significantly improving the efficiency of the design. • We avoid using DACs at every row by streaming inputs directly into pre-neuron accumulator and applying fixed width and amplitude read pulses on bit-lines to sample weights. By doing so, we improve the area and power efficiency and avoid the MVM errors due to the non-linear dependence of device conductance on applied voltages. To the best of our knowledge, this is the first work to demonstrate variational inference-based binary BNNs on IMC. We achieve a projected total efficiency improvement of 3.8-9.6\times as compared to an equivalent SRAM architecture. The rest of the paper is organized as follows. In Section II, we review background material on Bayesian neural networks, uncertainty quantification and PCM devices. In Section III, we discuss the design, architecture and the co-optimization between algorithm or software and hardware. The accuracy performance and the hardware projections are discussed in Section IV. Finally, Section V concludes the paper."
https://arxiv.org/html/2411.07397v1,Spiking Transformer Hardware Accelerators in 3D Integration,"Spiking neural networks (SNNs) are powerful models of spatiotemporal computation and are well suited for deployment on resource-constrained edge devices and neuromorphic hardware due to their low power consumption. Leveraging attention mechanisms similar to those found in their artificial neural network counterparts, recently emerged spiking transformers have showcased promising performance and efficiency by capitalizing on the binary nature of spiking operations. Recognizing the current lack of dedicated hardware support for spiking transformers, this paper presents the first work on 3D spiking transformer hardware architecture and design methodology. We present an architecture and physical design co-optimization approach tailored specifically for spiking transformers. Through memory-on-logic and logic-on-logic stacking enabled by 3D integration, we demonstrate significant energy and delay improvements compared to conventional 2D CMOS integration.","Transformer models have significantly advanced model capabilities in language modeling and computer vision, and have found widespread adoption across various application domains (Dosovitskiy et al., 2021; Ramesh et al., 2021). At the heart of these models lies a self-attention mechanism, which captures rich contextual information by considering all elements in a long input sequence, blending global and local sequence details into a unified representation. Spiking neural networks (SNNs) are more biologically plausible than their non-spiking artificial neural network (ANN) counterparts (Gerstner and Kistler, 2002). Notably, SNNs can harness powerful temporal coding, facilitate spatiotemporal computation based on binary activations, and achieve ultra-low energy dissipation on dedicated neuromorphic hardware (Akopyan et al., 2015; Davies et al., 2018; Lee et al., 2022). Recent spiking transformers showcased promising performance and efficiency by capitalizing on the binary nature of spiking activation (Zhou et al., 2023; Zhang et al., 2022; Zhu et al., 2023; Yao et al., 2023). However, there is a current lack of dedicated hardware architectures for spiking transformers (Zhou et al., 2023; Zhang et al., 2022; Zhu et al., 2023; Yao et al., 2023). Our goal is to fill this gap by developing optimized architectures capable of accelerating spatiotemporal spiking workloads for spiking transformers using 3D integration as a technology enabler. We see many opportunities that 3D integration can offer to enable biologically-inspired spiking transformers. Firstly, memory-on-logic stacking capability in 3D configurations allows for the storage of a significant portion of model parameters within local memory, ensuring swift and parallel memory access. Secondly, logic-on-logic stacking in 3D opens avenues for significant enhancements in energy efficiency, particularly in spike delivery management within SNN architecture. Ultimately, in a longer run, the ultra-dense neuron-to-neuron connectivity enabled by 3D integration promises improvements in SNN learning accuracy and efficiency, thereby propelling semiconductor chip emulation closer to the capabilities of the human brain. Challenges and Contributions In this work, we adopt face-to-face(F2F)-bonded 3D integration technology to enable dedicated spiking transformer accelerators with memory-on-logic and logic-on-logic configurations. Contribution 1: We propose the first dedicated 3D accelerator architecture for spiking transformers, which explore spatial and temporal weight reuse to support spike-based computation in transformer models . Contribution 2: We enable the first 3D memory-on-logic and logic-on-logic interconnection schemes to significantly minimize energy consumption and latency, whereby delivering highly-efficient spiking neural computing systems with low area overhead. Compared to 2D CMOS integration, the 3D accelerator offers substantial improvements. For the spiking MLP workload, it provides a 7.0% increase in effective frequency, 50% area reduction, and reductions of 7.8% in power consumption, 68.3% in memory access latency, and 69.5% in memory access power. For the spiking self-attention workload, the enhancements include a 6.3% increase in effective frequency, 50% area reduction, and reductions of 1.5% in power consumption, 74.2% in memory access latency, and 49.3% in memory access power. $*$$*$footnotetext: Corresponding author."
https://arxiv.org/html/2411.07062v1,16 Years of SPEC Power: An Analysis of x86 Energy Efficiency Trends,"The SPEC Power benchmark offers valuable insights into the energy efficiency of server systems, allowing comparisons across various hardware and software configurations. Benchmark results are publicly available for hundreds of systems from different vendors, published since 2007. We leverage this data to perform an analysis of trends in x86 server systems, focusing on power consumption, energy efficiency, energy proportionality and idle power consumption. Through this analysis, we aim to provide a clearer understanding of how server energy efficiency has evolved and the factors influencing these changes.","SPECpower_ssj 2008111SPECpower_ssj 2008 is the first and so far only release of the SPEC Power benchmark suite released by the Standard Performance Evaluation Corporation (SPEC). is the most prominent server energy efficiency benchmark. Its rigorous methodology and healthy benchmark submission review process have led to 16 years of continuous benchmark submissions and corresponding published data. These results allow hardware vendors to rank and promote their systems with respect to energy efficiency, measured in \mathrm{\text{ssj\_ops}}\text{\,}{\mathrm{W}}^{-1}. This metric gives customers an idea of how much computing they get for each invested Joule of energy, where a lower power consumption can increase it as well as a higher processing performance. Figure 1 illustrates some strengths of the benchmark: Due to the simplicity and scalability of the benchmark, server systems with multiple sockets and/or nodes can be measured. The workload can also be executed on different operating systems (OS) and different hardware, even though non-x86 processors are rare, and up to 2017, more than 97\text{\,}\mathrm{\char 37\relax} of results use Windows. Based on the benchmark results available via the SPEC Power website, we track the performance and power efficiency of x86 processors over the previous 16 years. We analyze data for different load levels to evaluate energy proportionality, as well as active idle power consumption trends. Figure 1: Share of features on all 960 successfully parsed (unfiltered) SPECpower_ssj 2008 results (as of June 2024)"
https://arxiv.org/html/2411.06079v2,A Review of SRAM-based Compute-in-Memory Circuits,"This paper presents a tutorial and review of SRAM-based Compute-in-Memory (CIM) circuits, with a focus on both Digital CIM (DCIM) and Analog CIM (ACIM) implementations. We explore the fundamental concepts, architectures, and operational principles of CIM technology.The review compares DCIM and ACIM approaches, examining their respective advantages and challenges. DCIM offers high computational precision and process scaling benefits, while ACIM provides superior power and area efficiency, particularly for medium-precision applications. We analyze various ACIM implementations, including current-based, time-based, and charge-based approaches, with a detailed look at charge-based ACIMs. The paper also discusses emerging hybrid CIM architectures that combine DCIM and ACIM to leverage the strengths of both approaches.","Recent advancements in Artificial Intelligence (AI) and Machine Learning (ML) have led to a dramatic increase in computational demands, particularly in the field of Deep Neural Networks (DNNs). The evolution of DNNs, from the breakthrough of AlexNet in 2012 [1] to modern architectures like ResNet [2] and Transformers [3], has resulted in exponential growth in model complexity and scale. While these advancements have enabled remarkable achievements in various applications, including surpassing human-level performance in image recognition [4] and demonstrating impressive language understanding capabilities [5], they have also pushed traditional computing architectures to their limits. Traditional von Neumann architectures, which physically separate processors and memory, have become a significant bottleneck in processing AI/ML workloads [6]. Specifically, the memory wall problem limits performance and power efficiency due to data transfer between processors and memory. Moreover, while GPUs are defacto standard for DNN computing hardware, there is an increasing demand for improved power efficiency in AI/ML processing across all scenarios, from edge devices to data centers. To address these challenges, researchers are exploring new computing paradigms, with Compute-In-Memory (CIM) technology [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33] gaining particular attention. CIM aims to minimize data movement and significantly enhance parallel processing capabilities and power efficiency by physically integrating memory and computational units. Compared to the previous SSDM2024 Extended Abstract [34], this paper aims to provide a comprehensive review on recent advancements in the field of CIM macros, analyzing various DCIM, ACIM and hybrid architectures and their implementations. SRAM is well-suited for CIM macro implementation due to its high-speed access, low power consumption, and widespread use as on-chip memory in CMOS processes, requiring no additional process options and thus being cost-effective. We will particularly emphasize the comparative analysis of Digital CIM (DCIM) [27, 28, 29, 30, 31, 32, 33] and Analog CIM (ACIM) [8, 10, 11, 12, 13, 14, 17, 15, 16, 18, 20, 21, 19], examine the advantages and challenges of ACIM approaches, and explore the potential of hybrid DCIM and ACIM approaches [22, 23, 24, 25, 26]. The paper is structured as follows: • Section 2 details the fundamental concepts of CIM, including its basic architecture and operation principles. • Section 3 discusses Digital CIM (DCIM), examining its characteristics, advantages, and challenges. • Section 4 focuses on Analog CIM (ACIM), providing an in-depth analysis of when to use analog computing, various ACIM implementations, and a detailed look at charge-based ACIMs. • Section 5 reviews the designs of Hybrid CIMs and its advantages. • Section 6 concludes the paper, summarizing key points and discussing future directions in CIM research. By examining the current state-of-the-art, we seek to offer insights into the challenges, opportunities, and potential future directions of the CIM technology."
https://arxiv.org/html/2411.06376v1,Phantom: Constraining Generative Artificial Intelligence Models for Practical Domain Specific Peripherals Trace Synthesizing,"Peripheral Component Interconnect Express (PCIe) is the de facto interconnect standard for high-speed peripherals and CPUs. Prototyping and optimizing PCIe devices for emerging scenarios is an ongoing challenge. Since Transaction Layer Packets (TLPs) capture device-CPU interactions, it is crucial to analyze and generate realistic TLP traces for effective device design and optimization. Generative AI offers a promising approach for creating intricate, custom TLP traces necessary for PCIe hardware and software development. However, existing models often generate impractical traces due to the absence of PCIe-specific constraints, such as TLP ordering and causality. This paper presents Phantom, the first framework that treats TLP trace generation as a generative AI problem while incorporating PCIe-specific constraints. We validate Phantom’s effectiveness by generating TLP traces for an actual PCIe network interface card. Experimental results show that Phantom produces practical, large-scale TLP traces, significantly outperforming existing models, with improvements of up to 1000× in task-specific metrics and up to 2.19× in Fréchet Inception Distance (FID) compared to backbone-only methods.","Figure 1: Topology of PCIe Devices in Modern Computing Systems. This diagram models a PCIe network interface card, highlighting key concepts like Transaction Layer Packet (TLP), Memory-Mapped Input/Output (MMIO), Direct Memory Access (DMA), Message Signaled Interrupt (MSI), and the transmit (TX) and receive (RX) pathways. It also includes examples of text-based traces, detailing the patterns and constraints. Proper synthesis of TLP traces requires addressing PCIe TLP constraints, such as ordering and causality, necessitating domain expertise. Figure 2: Overview of Phantom. The architecture of Phantom follows a 1+3 stage pipeline: generation, normalization, calibration, and decoding. Stage 0. Content Generation: The backbone generative AI model produces the initial content. Stage 1. Content Normalization: The content is normalized and defects are removed using the TLP trace visualization encoding method (See Section 3.2). Stage 2. Content Calibration: The normalized content is corrected using a dispersion-based calibration method (See Section 3.3) to ensure accuracy and consistency. Stage 3. Decoding: The calibrated content is decoded to produce the final output using the same visualization encoding method. Artificial intelligence tasks, including training, inference, and deployment, rely heavily on Peripheral Component Interconnect Express (PCIe) devices. Optimizing systems to support AI workloads through these peripherals has become a research focus (Han et al. 2022; Zhao et al. 2022). PCIe is the most common interconnection network for peripherals and hosts and is essential for devices like graphics cards, SSDs, and network interface cards. Transaction Layer Packets (TLP) are the smallest units of information transmitted within a PCIe network, similar to IP packets. Understanding a device’s TLP transaction pattern is crucial for modeling its impact on the CPU and assisting in design (Kuga et al. 2020; Neugebauer et al. 2018). However, identifying these patterns is challenging due to variations in task loads, network configurations, device models, and software updates. Therefore, preserving these patterns in some form for subsequent use is necessary. These patterns can be captured and formatted into traces suitable for subsequent analysis and storage. By replaying execution traces, developers can recreate environments and uncover hidden issues (Cornanguer et al. 2022; Tuor et al. 2018). If a TLP transaction trace for a device is obtained, the modeling for this device is nearly complete. However, obtaining the necessary traces can be daunting. For instance, collecting traces from a running server cluster could disrupt performance. Collecting traces from prototype hardware without actual samples is impractical when co-designing hardware-software. Even if traces are available, they may contain noise, making data collection and cleaning labor-intensive. Therefore, relying solely on collected traces is restrictive, and synthetic traces should be considered. Unfortunately, synthesizing PCIe TLP traces is challenging. Consider Figure 1, which illustrates the interaction between a PCIe Network Interface Card (NIC) and the CPU, highlighting characteristics reflected in the collected trace. Most PCIe operations use Memory-Mapped Input/Output (MMIO), which operations at different addresses interact with different components within the peripheral. For instance, operations at different positions within the BAR register mapping area affect different device registers, configuring various electrical states. Bulk data transfers rely on address constraints for asynchronous data transfer via the DMA engine. Given the influence of the software and hardware stack, TLP trace entries inherently include constraints such as order, causality, and data size limits. Ignoring these constraints in synthesized traces would render the content meaningless. While traditional statistical-based methods for synthesizing traces offer controllability and interpretability, they often fall short of capturing the complex patterns inherent in PCIe TLP transactions (Ij 2018). These methods, such as sampling and rule-based stitching (Thiebaut, Wolf, and Stone 1992; Phothilimthana et al. 2024), lack the flexibility and precision needed for high accuracy in this domain. As a result, there is growing interest in leveraging generative AI, which has proven successful in other areas like network traffic generation, security testing, and hardware design (Yin et al. 2022; Ye et al. 2019; Zheng et al. 2023). However, applying AI to PCIe TLP trace synthesis presents its own set of challenges: (1) how to convert the problem to a typical content generation task to leverage successful practices from related fields, and (2) how to introduce domain expertise as constraints to ensure controllability and predictability. To address these challenges, we propose Phantom, a synthesizer using generative AI, meanwhile ensuring the traces generated by the backbone model align with user requirements and closely resemble real data. Phantom comprises a backbone generative AI and a content calibration post-processor, as illustrated in Figure 2. The content calibration post-processor is divided into three stages: normalization, calibration, and decoding. To address the challenges mentioned above, we design a mapping between TLP operations and RGB triplets, redefining the PCIe TLP generation problem as an image generation task implemented in the normalization and decoding stages. Additionally, we employ a convolution-like method based on pixel dispersion, enabling selective inclusion of generated patterns or prior knowledge through hyperparameters. This forms the core of the calibration stage. This approach allows domain expertise to guide trace generation directly. The key contributions of our work can be summarized as follows: 1. We redefine the synthesis of PCIe TLP traces as an image generation problem, enabling the application of image-based designs and concepts to serve as building blocks. 2. To our knowledge, this is the first work that translates PCIe domain expert knowledge into constraints for generative AI, ensuring that the generated trace is both controllable and predictable. 3. We systematically construct a PCIe TLP trace synthesizer called Phantom, designed to assist with the research and design of new peripherals and related systems. 4. Extensive experiments demonstrate that Phantom can adapt to various existing AI generators and significantly improve performance on task-specific metrics by up to 1000\times, as well as the Fréchet Inception Distance (FID) metric by up to 2.19\times, efficiently calibrating generated data. Our work has been made open-source and is available for public use111https://github.com/sjtu-tcloud/Phantom."
https://arxiv.org/html/2411.06350v1,AMAZE: ccelerated iMC Hardware rchitecture for ero-Knowledge Applications on the dge,"Collision-resistant, cryptographic hash (CRH) functions have long been an integral part of providing security and privacy in modern systems. Certain constructions of zero-knowledge proof (ZKP) protocols aim to utilize CRH functions to perform cryptographic hashing. Standard CRH functions, such as SHA2, are inefficient when employed in the ZKP domain, thus calling for ZK-friendly hashes, which are CRH functions built with ZKP efficiency in mind. The most mature ZK-friendly hash, MiMC, presents a block cipher and hash function with a simple algebraic structure that is well-suited, due to its achieved security and low complexity, for ZKP applications. Although ZK-friendly hashes have improved the performance of ZKP generation in software, the underlying computation of ZKPs, including CRH functions, must be optimized on hardware to enable practical applications. The challenge we address in this work is determining how to efficiently incorporate ZK-friendly hash functions, such as MiMC, into hardware accelerators, thus enabling more practical applications. In this work, we introduce AMAZE, a highly hardware-optimized open-source framework for computing the MiMC block cipher and hash function. Our solution has been primarily directed at resource-constrained edge devices; consequently, we provide several implementations of MiMC with varying power, resource, and latency profiles. Our extensive evaluations show that the AMAZE-powered implementation of MiMC outperforms standard CPU implementations by more than 13\times. In all settings, AMAZE enables efficient ZK-friendly hashing on resource-constrained devices. Finally, we highlight AMAZE’s underlying open-source arithmetic backend as part of our end-to-end design, thus allowing developers to utilize the AMAZE framework for custom ZKP applications.","As data privacy and security have been more of a concern in the past decade, the concept of privacy-preserving computation, which enables computation to be performed on encrypted data, has been introduced as a paradigm shift in computing. Specifically, zero-knowledge proofs (ZKPs), a privacy-preserving cryptographic primitive, allow users to prove certain attributes about their private data without revealing anything about the data. Although ZKPs in their current state of implementation on software have proven to be effective in many applications, such as authentication (37; 35), healthcare (43; 26), and emerging learning paradigms (27; 36; 46), designing ZKP applications requires careful software/algorithm co-design to ensure that computation achieves practical runtimes and resource utilization on modern systems. As computational overhead is the main challenge when building practical zero-knowledge systems, there has been a recent emergence of research and development on ZKP hardware accelerators (38; 49). While these accelerators have proven to be effective, they often target FPGA or ASIC devices with high computational power or a large amount of resources. ZKPs have been shown to be a very valuable primitive in the IoT (24; 20) and other edge computing workflows (47), which often perform computation on resource-constrained devices. These use cases further motivate the importance of ZKP hardware acceleration while introducing a novel challenge: catering custom ZKP hardware to resource-constrained edge devices. Before an end-to-end accelerator can be built to maximize efficiency with limited resources, certain underlying modules must be built and highly optimized for area and latency. In this work, we focus on two core computational building blocks for the seminal ZKP constructions - collision-resistant, cryptographic hash functions and Galois/finite field arithmetic. Collision-resistant, cryptographic hash (CRH) functions have served as a powerful tool to enable secure computation and storage in modern systems. Certain constructions of zero-knowledge proof (ZKP) protocols, such as zk-STARKs (15) and zk-SNARKs (41), utilize CRH functions to perform cryptographic hashing for various applications. Traditional NIST-approved CRH functions, such as SHA-2 and SHA-3, have been proven to be secure through extensive studies and applications. There have been comprehensive efforts towards the thorough design of hardware and software to ensure their efficiency (44). However, efficiency in the ZKP domain is dependent on several factors that are not accounted for in our current systems, such as algebraic structure and multiplicative complexity. These standard CRH functions have proven to be inefficient when translated to the ZKP domain, thus calling for ZK-friendly hashes, which are CRH functions built with ZKP efficiency in mind (31). In this work, we consider MiMC, the first and most mature ZK-friendly hash (9), consisting of a block cipher and hash function with a simplified algebraic structure that was originally designed for zk-SNARKs, but has found use in zk-STARK applications as well (17). While the algebraic structure of MiMC is relatively simple, the underlying arithmetic structure is a large Galois prime field, which typically requires computation to be done on 254-bit integers. We do note that the size of the prime field is dependent on the ZKP construction, but typically nothing less than 128 bits would be used for security purposes. Nonetheless, efficient prime field arithmetic is a very challenging task to do on resource-constrained devices, such as select FPGAs, as the hardware modules for performing fast arithmetic typically only support 16 to 27-bit arithmetic. To address this, we present AMAZE, an accelerated hardware architecture that enables underlying fast and resource-efficient Galois field arithmetic for the MiMC hash function on resource-constrained edge devices. This work provides an accessible solution for developers and businesses to incorporate the MiMC hash function on low-end FPGAs for custom zero-knowledge applications. In short, our contributions are as follows: • We propose AMAZE, a highly-optimized hardware architecture framework for computing the MiMC block cipher and hash function, a core operation in zero-knowledge proofs, on FPGA. AMAZE is designed to support resource-constrained edge devices, while still outperforming CPU. • Our open-source implementation111https://github.com/ACES-STAM/AMAZE is parameterizable to balance power, resource utilization, and latency based on the available resources, without sacrificing the security of the MiMC hash function. This is done through our novel design of the well-established Russian Peasant and Barrett modular multiplication schemes. We provide an open-source implementation of optimized Galois field arithmetic library that is compatible with the BN254 elliptic curve, a commonly used elliptic curve in zk-SNARKs. • Our extensive evaluations show that our novel, fully pipelined implementation, which uses Barrett Reduction for modular multiplication, achieves more than 13\times speedup (for one block cipher invocation or one hash round) when compared to state-of-the-art MiMC software running on a server-grade CPU. This performance achieves relatively low power consumption on a low-end FPGA with limited resources, highlighting the feasibility of AMAZE on resource-constrained edge devices for zero-knowledge proof applications."
